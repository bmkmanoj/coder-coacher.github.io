<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Algebraic Techniques for Multilingual Document Clustering | Coder Coacher - Coaching Coders</title><meta content="Algebraic Techniques for Multilingual Document Clustering - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Algebraic Techniques for Multilingual Document Clustering</b></h2><h5 class="post__date">2011-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/X3PZVtqX5bM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">there aren't that many places that it
can claim to be as cool as Google but it
does happen so maybe few for I've heard
of a sandia national laboratories they
have lots of cool harder in there they
have the biggest laser in the world and
it is a monster and we have a speaker
from their bread father but he is not
going to talk about the laser is it's
going to talk about something that's a
much closer to us for me surprisingly
close they are very imagined that they
will work a something like that
algebraic techniques for multilingual
document clustering so move the lingual
document clustering is very interesting
for us and we'll Here I what brett has
they to tell about that um actually I
met bread through a colleague of his
feel if Michael Myers sitting there ok
say you are wondering and they he is
already gave several talks at Google and
I was really impressed with his work so
here he brings us another boot thing
well thanks ma che so my name's Brett
Vader and so I've been at Sandia for
about a little over seven years and I've
done a number of different projects but
this worked out we talking to you about
today is actually a project that started
about four or five years ago under a p.i
principal investigator named Peter chew
he has since left sandhya but he was the
one who kind of kick started this work
in multilingual document clustering and
I started working with them for years
ago kind of bringing some computational
expertise looking at different
algorithms and how to apply different
algebraic techniques to the problem so
so Peter was involved principally with
he's a computational linguistics
computational linguistics background and
also worked with a lot of data Ron
Oldfield is another key person for the
work in this talk and he dealt a lot
with the high-performance computing and
architecture aspects of the work here
and then Philip nickel miner who also
somo she introduced him he did a lot
with the machine learning so those are
the three principal people involved as
collaborators but there are a lot of
other folks that were involved with
contributing ideas or helping with some
of the data analysis high performance
computing aspects and so forth so those
names are listed there as well so
Sandhya has developed some techniques
for analyzing documents in a number of
different languages and we're interested
in kind of translating these documents
into a concept space that's language
independent so we can then apply
different techniques like doing document
clustering maybe using some of the those
features to do cross-language text
analysis or classification and so what
we'll be talking about primarily is the
document clustering I guess the sub part
of that could be doing translation
triage where if you have some clusters
of documents you could look at those
those clothes clusters and see what
English Dawkins are the languages that
you know and you're interested in
something in that particular cluster you
can then go ahead and spend resources to
translate those other documents I'll
talk a little bit the varied about the
two topics in text classification so one
is ideological classification another
was is multilingual sentiment analysis
and so I'll go ahead and talk about
those at the end so I'm sure all of you
aware of this but just kind of frame the
rest to talk I just want to give an
introductory slide on the bag of words
and vector space model so we have a
number of documents we can pull out
terms of interest to that to us we then
have lexicon of those terms we can
create a bipartite graph consisting in
terms and documents and then take that
graph and then take the adjacency matrix
of that graph too so we have the number
of rows be the correspond to the terms
lived up in the documents of course want
to the columns and so this the vector
space model is basically saying that
each document can be represented as a
vector a column vector in this sense and
so in other aspect here is if we have
stopped words we may ignore some of the
work we're done and so forth ones that
don't really carry much meaning so we
can
ignore those now come up a little bit
later also stemming so the word bake and
baking are in this list here but we
don't include those as separate terms we
stem or semi as a common technique and
you'll see later that we try to avoid
that and let's see the last thing I
herrick is that while this example here
shows zeros and ones we actually may be
interested in you know including the
actual term count in our documents and
then do some sort of information scaling
and so this will come up just briefly a
little bit later so our design goals for
the work here is to allow as many
languages as possible and we don't want
to be constricted or restricted by
language expertise so we don't have to
hire some X lenguage experts who knows
some obscure language we want to go
ahead and just try to do some
statistical analysis of corpora and rely
on that knowledge so we don't want to do
any stemming because constructing a
stammer would require with some language
open their knowledge of the language and
we also don't constructive stop lyst so
we'll just go ahead and include all the
terms that we come across and so those
are the design goals that we've set out
in the very beginning and that's what
we've done so it's you might expect what
we do for dealing with multiple
languages we can construct a turn by
document matrix for each language and
then stack those on top of each other
this has been done before with pairs of
languages so landauer littman in 1990
and young in 1994 dealt with pairs of
languages where one of them was English
we we've also so I think some of the new
thing here is that may be applying or
stacking many more languages on top of
each other so we have a much larger
matrix and try to look at the
co-occurrences across many languages and
so this is one of our contributions that
we had early on
just a bit already but you know exactly
the language of the document or is the
thoughts on I'm sorry what's the name
the chocolate of 20 so yes we assume
that the languages known or do you know
what it is so that way we can classify
it and put it in here and so we're
starting from say here so we're using
the Bible's as our rosetta stone the
Bible's it's been carefully and widely
translated into a lot of different
languages so we actually know which
translation we're dealing with as we
simplify the problem that way station
also seem that each dome in the city yes
but it's it's easy at least I think we
can go ahead and use some techniques if
the document does have multiple
languages in it we can either look at
the individual words and those who fall
into you know some piece of that that
current vector and so that wouldn't be
too difficult but the case may be we
have some words that transliterations or
whatever that may follow different
languages and so we don't know which one
of those and so we can use maybe it
naive Bayes classifier or something to
help help with that but we haven't
explored that all right so the Bible is
the is one of the core corpora they'll
be used and so it's nice it's there are
a lot of different translations of it
it's also versa line so each verse is
roughly a sentence or two in length our
particular database has 54 languages
they're all listed here and roughly
color-coded by language family however
I'm not one to not work with all of
these language we typically just deal
with a few of these for doing our
training and testing
yeah excellent question so so the
question is how does the vocabulary
overlap with crawl the web and it
doesn't necessarily but we have you know
it does have a lot of terms and so this
table here shows you know the unique
terms or English so there's 12,000 just
in the English translation the king
james version of the bible and so we get
you know a good overlap with some of the
relatively common words but we're
missing things like technology and
politics and you know some of these more
modern ideas so we can go ahead and
extend that with corpora we haven't done
that to to a very large extent so far
but we have worked with Europe are also
the European Parliament proceedings and
so that was something that we've looked
at movie but our main focus has been
looking at threats that are kind of more
religious based and so that's why we
started with the Bible well why are
there so many more I'll just stick terms
and build languages that you're stunning
mister please it's because so the
question is why are there more terms
down here in Arabic for instance or
Russian as compared to English and the
answer is because languages can pay
different number different amount of
information their words and so linguists
have kind of classify languages on a
spectrum between an isolating language
and a synthetic language and I'll get
into a little bit more it deals with
language morphology and so that that
will come up a little bit later so just
for translating it is and we actually
use that here as well so we use the
Bible and Quran as our training
intestine alright so I don't know here's
just one example is this is just the
first verse of the Bible and you can see
that the word count you know difference
as you would expect really for that this
kind of sets the stage for an example of
what we're doing alright so so the
approach that we haves we take this turd
by verse matrix we're dealing with five
languages so English Spanish Russian
Arabic and French are the languages that
we've been using for training and
testing purposes and that's we published
on and so well multilingual sentiment
analysis means taking that matrix and
doing some sort of the dimensionality
reduction using the singular value
decomposition SVD and we truncate that
single value decomposition to retain the
lard I guess that the largest rank
matrix that kind of represents this
original term by document matrix we
actually keep 300 columns of this matrix
here which consists of with left
singular vectors and so that matrix has
300 concepts and we've looked at going a
little bit less than that but 300 is the
is the number that we stuck with and see
so we have this matrix here that's no
Padma matrix that relates the terms to
these different latent concepts that are
in the Bible the V matrix here is the
orthogonal matrix that relates the
documents back to these latent concepts
and then the Signet here is the diagonal
matrix of singular values
so how do you how do you spend the Bible
into documents how hope you get it up
chapter slice so vs so it's a verse
aligned and so it's about a sentence or
two in length and there's there's thirty
one thousand verses so we have thirty
one thousand columns that verses are
what we're considering is our document
when that's houses so just to clarify
this is this is your training step in
that you're trying to learn a transition
model effectively because you've already
got it it's not a transition model but
just a yeah we're just trying to learn
latent concepts and so you know latent
semantic analysis is trying to extract a
lower dimensional representation of the
original matrix but it's not looking
actual of transitions you word word type
transition investiture sure but at this
point your data is is you know that the
date emitter documents are transitions
of each other environment mater sorry
yeah yes yes yeah so we know that for
instance the first one and so maybe I
can go back to this so we know that you
know first one in English is this we
look at the vector space model of that
and we know that the vector space model
same thing in Arabic you know we can
look at the statistical co-occurrences
of terms and so we can see that God
created and cRIO deals you know those
things always a co-occur and so those we
can make that relationship explicit in
this matrix it's u matrix and so we have
large values for a particular column for
those particular terms we can see large
values there across languages that's
clear this is slowed still monolingual
like the sisters for one language to us
is this is multilingual so we have all
languages these five languages here and
so in the corresponding u matrix we have
english all the terms for english up
here the terms for spanish here terms of
russian so on does the folk structure
carryover projection space it should you
should expect to see certain terms like
so the word God appears a lot in the
Bible so you see God you know somewhere
here is the the row for God and so you
see large values in that row and then
for Spanish Dios corresponding word for
God so you'd see about large values
there as well and so it's that that's
where you get the the relationship the
explicit relationship between words make
sense I was writing just if you look at
you as enjoyed the side
alright so this first part what I just
described as a training training step so
we have a reduced representation of the
latent concepts in our corpus if we have
new documents we can go ahead and
project we can represent those new
documents in the vector space model and
project those documents is the space
span by the columns of you and so we get
then a small dimension dimensional
representation of this document here and
I've got here just 20 of them but you
know as I mentioned before we actually
have 300 columns of you but for all the
just for the sake of simplicity I'm
showing 20 so we have 20 features or
latent concepts that are language
independent okay and so those features
we can then use a number of applications
like computing pairwise similarities
across documents or looking at
cross-language retrieval or some machine
learning applications and so each so
this little figure here in the lower
right is actually taking this vector and
just representative now as a signature
and so we're saying that each document
has compass 300 dimensional feature
signature consisting of all of the
features
so we're using the Bible's or training
set and then we use the Quran as our
test sets we train on the Bible using
SPD and then we project the some of the
chapters of the Quran into that language
in the pen concept space and then use
since we know just like with the Bible
we know which chapters of the sera
related our translations of each other
we then have a ground truth we have 114
of those and so we can then produce some
computational tests and actually look at
the accuracy or the precision of how
well we're doing and I'll suit that last
bullet just to save a little time so the
performance metrics that we're looking
at these are all standard measures from
information retrieval but we've modified
them slightly to fit for this language
for multiple languages so the first one
is what I'll call p1 or the average
precision at one document and so this is
if we have on the far left we have a
query document that we want to find in a
specified target language which document
most closely matches that one query done
and if that document that that's return
is actually the the translation for
example if we had as a query document
each chapter one in English and the one
that have found us chapter one in
Spanish that we know that you know that
we basically got that one right if it
was chapter 2 that we returned that we
got that wrong as you got 0% if we can
average over all queries and just for
that one target language that gives us
than the average precision the other
thing that we're actually more
interested in is what we call mp5 for
this average multilingual precision at
five minutes so since we're dealing with
five languages will say five year but in
the general case it could be in
languages and so in this case we're
looking at we have a query document but
we don't specify a target language and
we allow that query we will try to find
matching documents for that query and it
could be an e language any of our five
languages including the the language of
the query document and if those five
matches
we're looking at the percentage of those
five being with the counterpart of the
query and so that gives us that
percentage and we want to go ahead and
look at all queries and averaged over
that so this is actually more of a
measure of success in doing multilingual
clustering and it's a much more
difficult retrieval problem than this
one over here as we're actually striving
for about ninety percent mp5 which
roughly says that 9 out of 10 of our
documents will cluster together
regardless of language so you're you're
trying to find all the documents or just
any language and so if we start from any
line with any language we have a query
document we want to say find wiese let's
say we start with chapter 1 in English
we want to find all the chapter ones in
all the other languages so do you as the
other I am pleased to have no
acquaintance no there's always yes
always going to have a correct answer so
because we're dealing with the Bible or
the Quran we know which one should be
matched up yep lido you can move just to
see with it yeah we haven't done that
okay so just some early results here
this chart shows a precision at one
document which is the case where we
specify the target language so we have
an English query and then we look in
Arabic and also look in French and so
these bars the the gray bar the far left
in each case is basically just training
on those two languages themselves so
training on your English and Arabic
translations that's what we start from
English
the rightmost bar in each of these cases
we have in our train set we have 47
languages but we still only test on the
to the response event here and so the
general trend is that as you include
more languages in your training set your
you're getting better precision okay
which is somewhat counterintuitive you
think that what do you think if you had
more data in your train set that's
irrelevant to your actual query that
your precision would go down but we're
actually finding that we're learning
some something else from the other
languages and that's kind of helping the
query or the precision here so you're
going slopes are getting saying your
concepts are getting better yes so
they're kind of getting more refined
because they're borrowing and leveraging
information from other languages and
making those relationships more explicit
and so that's that's house helping
intestine we encourage the English
government and the first up we return is
non arabic buchanan's hit-or-miss is
amiss for this case so even if you have
47 languages just prayer in English and
the first one in turn there's a big it
has me but we're specifying that that it
is here but but it has to be you know if
we have a chapter one query we want to
find chapter one in Arabic so so you'll
return maybe 47 but then one of them is
Arabic that's the way you know we're not
even just ignore the 47 and in this case
so while we train on all those work best
building up our latent semantic space so
that u matrix and then forget all the
other 45 languages now we only focus on
english and french or look at the
first-ever big government to retrieve a
affair so social Oh
what the distance measure used so we're
looking at cosine similarity so we
normalization no idea Sega was there a
novelization like idea for something oh
yes so that kind of goes back to the
very beginning so we started with a log
entry scaling of our original term
document atrix sorry Oh shouldn't you're
always using 300 independent of the
number of 300 concepts independent of
the number of languages included in
training but wouldn't you be able to get
away with more concepts with more
languages because you'd be able to
resolve perhaps yeah we went with 300
partly out of fact that when we started
out we were limited a little bit with
the computational power we had just
running on laptops and whatnot and also
there's evidence in the literature that
suggests that 300 that's kind of been a
historical number I think since the
mid-90s or so ik member which paper was
but you know they they used 300 concepts
and so we went with that we did trial
you know bearing it from like 100 all
the way up to 300 and 300 still
performed better than the ones with
fewer dimensions so that I can see how
300 would so the number so this is
though so Sigma is a square matrix of 3
300 by 300 as I would guess so I can see
why it would be data driven what that
should be but it should also be domain
driven like what book like for the Bible
300 might make sense for certain
purposes but if you're applying it to
the web then right yeah so that's a good
point yeah if if you have a particularly
rich set of documents or a very
expansive set and yeah maybe you would
need more that's something we haven't
explored it please I'm
but Scully the design completely agree i
mean-- heavy there's a handful of magic
numbers here and having a gated relation
deciding how to set those numbers or
needs to know that you've finally
tweaked holding chakras you can owe
these things is a ongoing area of
interest and it just takes a lot of
computational cycles to explore that
perimeter space but you know we have
those which is this for matter of having
of human time human time n funding all
right so this is now as a more complete
graph or plot of the results we had in
this early paper until I see more
language pairs like Russian and French
and so forth on the far right is just
the overall kind of aggregating all the
results together but the trend still
holds when you have few training
languages the precision is lower than
when you have more training languages so
so just to make sure I understand so the
way the sharing apples with the
different languages is because you doing
me up everybody irrespective of the
language so we've got it happens in
verse 3 please don't because the
language of them yes it's I understand
question I think you do I mean the each
column of that original matrix was
devoted to a different verse of the
Bible and the first chunk of each column
width length in the values the next
jumper that same columns the french
language and so forth so all the terms
are a single call relate to the same
verse and that's how true
think what column ideas is equivalent to
about verse 8 here to your question Fred
and Mercedes common across the factories
in our colleges looking at this it seems
that the blue language pair is
particularly good and if we will throw
out the one afterwards we'll probably do
better now I did you try something more
ambitious on these lines to choose no
not try anything more than this this was
something that was an interesting result
but then we moved on to more techniques
and so so I'll point out that this is a
precision in one document and so on my
next slide this is where we're getting
into the thing that we're really
interested which is multilingual
precision or mp5 and so this table this
is the store that we're actually looking
at and so we started out at twenty-six
percent and then this is when we need to
actually leverage some more
sophisticated techniques to get that
multilingual precision up and so we
tried a number of things to actually get
there and so this is where i was i
contributed some techniques and
different ideas along with some others
and so just one technique you know going
from 26 to 65 if we take our login
scaling that's what we scaled our matrix
with we can do some things to the global
term way to influence that so the
distribution of terms and so this chart
I'm not going to go into detail not but
that's kind of one simple technique that
we did get 65 and a half another idea is
now instead of having all these language
matrix you stacked on top of one another
we can separate them out into a set of
matrices where they all have the same
number of columns but because each
language have language has a different
number of terms we now have different
number of rows for each matrix and so
there are techniques out there for
looking at these collection of matrices
and analyzing them all together and so
this is an actual expertise that that I
have my pen brought to this where we
looked at this pair of fact to model to
basically do with the opposition
of these set of matrices to get this
individual you may treat matrices that
got us up to seventy eight and a half
percent but then we tried some more
techniques and one that we found that
worked pretty well is this one called el
SATA or the TA stands for term alignment
and so I'll go into that a little bit
more detail here but the idea is that we
have some we've included some term to
term of alignment information in our LSA
technique in this kind of stems from
there's a lot of different ways of
computing the SPD and so the SPD x
equals using the transpose can be
computed by just taking their car matrix
x multiplied by x transpose and then
computing the eigenvectors of that
matrix that square matrix that gives us
the matrix you here with you know augen
values would be the singular value
squared okay we can do the same thing
for X transpose X but there's a another
way of doing this whole thing where you
actually get you and me together and
that is it can take two bites you block
matrix and so we have X and X transpose
in our original matrix and it's kind of
represented out here so we have you know
if you turn by document matrix in the
upper right and then you take the
transpose of that as down the lower left
if you compute the eigenvectors of that
two by two block matrix you then get U
and V stacked on top of each other
appropriately scaled by 1 over root 2
but you can then you know take that
information out and get retrieve you and
B so this is what we did but we actually
we were inspired by a paper by Bruce
Henderson in 2007 that looked at how can
we take over again augments information
that upper block this term the term
information block similarly you can also
augment down here and it's document
document block what we just do that so
what you do is in the D 1 block the
upper left is we could maybe put some
binary increase one's one if the term in
each language is matches up with
term in another language so casa and
Spanish Matt matches with house in
English matric matches with Mason and
French so we can go ahead and put a one
in those cases in those entries where
house casa and they saw on match up okay
and so we can then go ahead and hit the
eigen vector decomposition that or get
those eigenvectors and then that use
those once we separate out the
individual you matrices that use those
that are latent concepts so this is a
little so as far as the number of
documents and number instead features
that's right okay you have a lot more
yes okay I think what we'll do is I'll
skip this in the interest of time but
there is a kind of representation I
maybe I can cover this briefly so the
idea is that if we have just in the
normal technique if we have house casa
anime song maybe the original method
this just using LSA would give us a
large value for a house there column but
a small value for cost of a small value
for Mason's based on so those are
represented by these the size of these
blocks here are these injuries so once
we multiply by this d 1 matrix that has
that information more explicit between
those terms that then makes those values
the same in magnitude okay so that's
that's kind of the rationale behind this
pretty clear method and with our
particular scaling that we used is
listen to here we actually getting up to
multiple precision of five documents
about eighty percent so we're on our way
to ninety percent but still not there
this is where then we turn to a
different technique or we looked at
language morphology and so there's a
question very early on about so why why
are there different number of terms for
english and arabic so the answer is that
languages are in the spectrum of between
isolating and synthetic language
so an example of an isolating language
is Chinese English is also largely
isolating so here's an example from
Wikipedia so he traveled by hovercraft
on the sea is the example of they give
there's mostly where there's generally
just one morphine per word morphine
being come be the smallest individual
unit of meaning so he is a more game
traveled actually contains two it has
traveled the base verb followed by the
suffix ed for the past tense marker ok
so then hovercraft is a compound noun it
has two morphemes and all the rest of
the words are just a single morphine so
that's one example on one end of the
spectrum synthetic languages on the
other hand have a lot of for each
forward and so there are some examples
like in German where he's very long
compound nouns my favorite is this one
in the very bottom Eskimo where this one
single word means he had not yet said
again that he was going to hunt reindeer
and so somewhere so if somewhere in that
had changed to be she have not yet said
again and so on somewhere in his word
that's going to change and so in this in
our late semantic analysis example or
that framework we now have an individual
term or a different row in our matrix
that would correspond to the to the word
or she had not yet said again ok and so
that would kind of break the
co-occurrence or being allowing us to
find the cool occurrence information in
the
tation 449 see how did I do the August
augmentation he's dead how big right
good i don't know i'm not an expert but
I grabbed this from a textbook that my
wife has and so I don't know maybe this
is the founder of a phonetic spelling
she made us they may not have had to
earn an additional spent so then
somebody came and said oh this is how
you should write it but maybe there was
absolutely like i said i'm not an expert
of these spaces so that then led us down
this path of looking at morphological
token asian tokenization so our
hypothesis is if the terms in our LSA
analysis were morphemes instead of words
or stems then our results of information
retrieval would be improved so two
approaches that we looked at for
tokenization based on mutual animation
of character in grams and the other one
is looking at mins description length
the technique used by goldsmith in a
open source tool called linguistica and
we've actually used this linguistic
approach more at least we've had it
easier to use but i'll go through does
kind of describe the first one just a
little bit to note at least motion A's
interested in language morphology salt
related to the second one I guess we
read a paper by schneider and barzilay
which follows up on the gulf stream
network i think it must be interesting
yeah talking efforts and so i can write
that down I be interested in that all
right so the nice thing is that these
approaches are generalizable to new
languages so if we come across the
language we have seen before there's no
language expert we can use these
unsupervised approaches to kind of do
this morphological tokenization
so that one approach tokenization from
Engram mutual information we take the
words we consider all possible to
organizations of the word so take for
example the word walked so walked you
just go ahead and consider that as a
full term it can maybe split it between
the W and the a or between the a and
they ll all the way down or maybe we
consider multiple splits in that word
and so we can split in many late
locations what we do then is we
calculate the point-wise mutual
information or PMI for each end grab
individually from the corpus so one
example is 20 ways mutual information of
walk is just the probability of lock in
our corpus divided by the probability of
all individual letters appearing by
themselves or I'm not a period but
actually most letters of periods in
corpus then we can take and some of
those those pointwise beautiful mutual
information scores for each tokenization
and then select the result that is
closest to 0 and so we'd have a whole
list of those and we can look at the one
that has the store that's closest to 0
maybe it's the one here in the middle
but most likely it's not the one at the
very end here when the split is kind of
awkward between the the a and the l-sit
justified
glue take the text and do all possible
the musicians you need for me with equal
probability and then we'll start content
to get your we see so we look at our
corpus we been having trouble
understanding your question i guess like
well you showed there is various ways of
splitting walk right yeah tricey walked
in my training data and i want to go to
calculate this for those particles
are we good we're sorry if we should
like and see all of them you're
considered you're considering all of
them and so you're looking at
probabilities of you know walk and Ed
for instance how often do those appear
in in your corpus and so then because so
not only have walk or walks past tense
but you have talked all the other ones
that ended ed and so because of that you
have a large if you more likely that
you'd pick walked you know split this
one here walking in the tacit monarchy
than any other one maybe it will become
more clear if you define the probability
so in the case we'll walk for example
what's the denominator in the case of Ed
was number are they the same or they do
per game so there so in this case walk
has just economic denominator are even a
probability itself PR is the total count
of the good question i think it's the
all of all of the instances of the word
I have to go back or maybe we could look
in this paper I can't remember exactly
what it is in the denominator again to
gum line here social complexity we're
not worried about that at the stage yeah
this is the research activity but
something later there in the PM 541 can
be many things right I mean it could be
mom are you talking about the
denominator this again yeah we might by
managers that the people stood you just
showed you can speed walk about kokanee
inmates what an excellent it seems to me
that those rooney crumbs they just drop
out officially yeah so actually the
denominator doesn't yeah so because we
were dealing with the same word so
imagine so the PMI have walked is going
to have wa lked the denominator here
because it's a logarithm it's when you
can split it up and you have log log of
the numerator minus the log of the
island on Maris okay and so and so once
you have that you can go ahead and all
the not denominators here in the bottom
you can just go ahead and ignore it so
you're only considering the numerators
for this PM I expression
make sense makes sense all right may
submit this nameless talk afterwards so
these are some sample tokenization and I
believe this is with mdl so these are
the first three words are from French
the second three second set of three are
from English as you can see cases where
you know it's slitting where you might
expect to see good splits so are you
sitting a hog limit on the number of
states so with mdl mvl only two splits
in in one place and then if you want to
split a second time okay so Rachel's a
cassava question but that's that's only
for mdl the other approach that we are
talking about you know it doesn't matter
that you can split multiple ways you can
go ahead and set it so you know if you
just wanted to look at one split you
know one tokenization we have two
morphemes you could do it that way to
limit to clump complexity complexity
setting it okay so I think the question
is you know are you worried about the
complexity of trying to find on the app
of these tokenization zand so if we were
to allow all of the tokenization you
know all splits that's why could be very
complex but we could go ahead and limit
it to just have one a single split in a
word okay and then if we want to redo it
we do the analysis to then find a second
split but so you do agree you basically
you find a one-state and then you say
okay it's coming to its peak 10 they say
we can so thou speaketh bring some value
and that's that's basically what you do
get some split first okay alright so now
we can go ahead and replace those terms
with morphemes in our turn by document
matrix now we have a morphine bye-bye
matrix and we call this technique latent
amor flow semantic analysis or LMS a and
because we will actually have fewer
morphemes so this will be a smaller
matrix fewer number of rows but the
matrix is actually be a bit more dense
and so that's this is the premise for
this technique the results for that if
we compare by language so LSA is a bar
on the left and then this new technique
dealing with language morphologies on
the right each of these cases you see it
across the board adding in Lima from
quality does help when you get done that
state you can do any Stefan's that's
right yeah each other just do some
simple step say take I didn't but I
think my colleague did when he first
started but I'm not so sure on that it
may have been because we didn't have the
stemmer for other languages justjust
daintiness this both had excessive each
would replace yeah we have looked at
like doing character n-grams and doing
doing an analysis there I can't remember
the results I'm published on that
network sort but I can get that to you
but I can't remember off hands what it
is all right so this is now kind of
revisiting that table i showed you
before so el SI ta got us up to 8.7 the
language morphology actually dropped us
down a bit to 70 three-point-seven
percent for mp5 score now let me
describe the final technique which is
combining the language morphology with
the term alignment to get us up to
eighty eight percent which is pretty
close to our goal and that is just we
want to use the language morphology so
we have morphine site by documents so we
have a marking the morpheme alignment
matrix there and then we have some
scaling in there and that gives us
ultimate precision at week eight percent
okay
so let me go ahead and write a seasoned
veteran even edge of this question where
the actual legs come from it was
reserved a standard lexicon of
translation yeah so that was actually
looking at mutual information once again
so we can we can look at the mutual
information of various words appearing
in the parallel corpus to determine what
those are I think I have a slide I
mentioned that we could actually have
ones and zeros in that matrix we've got
ones and zeros here just looking at a
cross language dictionary or it will go
back and look at the mutual information
of you know where those terms
co-occurrence to actually get a
numerical value that's not a one in 20
and so that's the approach that we that
we used so so this slide summarizes the
results for some of these techniques the
one that we've been going with release
gives us the best precision honestly
what we always use but because it does
require some extra work is this latent
morpho semantic analysis with turbo
alignment so you see the bar on the
right which is the multilingual
precision it generally pretty high so
we're at eighty-seven percent 88 present
there the bar on the left in each of
these cases is the precision at one
document the one where you're specifying
the language pair and that's the easier
of the two so you see that basically
prepare these methods across the board
so now we can just do a simple example
clusters and documents in this case is
just showing clustering the cost of
books of the Bible not really a good
example since we're training on the
Bible now we're trying to cluster them
really what we wanted to do is you know
train on some corpus and then try to
cluster documents from another corpus
but this kind of gives you a sense since
a lot of people knew some about the
Bible so the these nodes in this graph
are colored by language and the books
cluster first by their counterparts in
other languages and then cluster largely
by topic so the New Testament clustered
up on the top part of the graph Old
Testaments more on the right the Gospels
are down there so let me zoom in on the
Gospels so you see that the book of John
and the book of Acts actually have tight
clusters the red links indicate that
they have there's a strong similarity
between those two those documents but we
do see some mixing in Matthew Mark and
Luke and the Bible scholars call those
the synoptic Gospels and so there's
makes sense that they made cluster you
know kind of have some mixing because
they do share a similar perspective so
this house is picture made is it so I
kind of glossed over that so we're doing
some we have some visualization software
that does the graph layout and layout
algorithm for doing the clustering it's
it's a springs up force-directed layout
and so you give it the distances and
there's a surprise yes we have a
paradise similarity based on the cosine
similarity between documents and then it
goes into the algorithm treating those
those values as spring constants and so
with a higher value that's that's trying
to keep those documents closer together
and as the as the shape starts to take
place you know as things start to line
out the algorithm actually starts to
clip some of those springs if they get
too far stretched out and so then allow
us to kind of spread out a little bit
more and so that's that's this algorithm
your VX board it was developed at sandia
2005 do do anything special to the coast
on distances before you put it in there
or I know anything special about them
dude do anything so these are understand
both the ones of the edges are other
ones of the edges the cosine distances
or now there's no if you're not anything
like that you could so you could do that
where if you're to look at like ten
cents multi-dimensional scaling where
you want to do a lower-dimensional
embedding that tries to preserve the
distance between documents based on you
know the distance in the higher
dimensional space you could use MDS for
that but that's not what we're doing
here this is a different algorithm its
scales it's much faster than doing MDS
like that sort of layout but yeah buddy
applications those that you should we
look at the length of the loins as a as
more than just a locative measure oh
okay that was my main question but on
the next slide is that should give me so
they might work the color is a safer
that is to the tightness of the
connection with the implement like
lesson
alright so so you told me to finish up
about five till we're getting close so
so I've got a few things to show still I
can show some eight high performance
computing or these text classification
problems what's more interest in folks
thanks classification text
classification
alright i'll just show that the
highlight though for the
high-performance computing so we try to
cluster a million and a quarter
documents of the european parliament
proceedings and try to do this fast as
we could all the data resided on a on a
network pregs me on a high-performance
computer in this case it was jag or PF p
f stands for petaflop and so on third
two thousand cores it took just about
eight minutes do and so we're the whole
idea here was trying to do this in what
we call analyst time so that as you know
when analyst has a some idea in their
head they want to get the answer as fast
we don't know wait for a day to submit a
batch job and have the result come back
later so let me skip now to the the text
classification problems so we looked to
the first one is can we predict ideology
in documents and in this case we
specifically looked at can we look at or
can we find hostile ideology since we're
kind of fighting hostile ideologies as
those that are hostile to democracy and
so forth so our hypothesis that you know
there may be a link between religious
texts and threats and since we're
looking we've used the Bible as a lens
to look at these documents there may be
some regions in this document feature
vector that would indicate some amount
of ideological content so we constructed
a data set that had 250 documents of
different languages but they all can
consist of you know some documents from
Lenin Hitler's from the Middle East and
so forth and then we supplemented that
data set with nine times as many web
documents randomly sample from the web
the approach that is that we create some
feature vectors of all 2500 of these so
now we have some feature vectors to
train a classifier and then we used this
predictive model to predict ideological
verses on ideological so try to predict
these from the randomly selected set
and also looking at just the first two
can be distinguished between the first
two here and so the next slide we have
the results here so we have the hostile
ideology versus the non-hostile baseline
accuracy is nine percent we actually
were able to get to about ninety nine
percent and this is using a ensemble
decision tree classifier and then we
also have in the second category looking
at marxism-leninism versus the documents
written by hitler we're getting up to
about ninety five percent but we compare
those classification problems versus
when it's out there for doing sentiment
analysis again movie reviews where we
have positive and negative movie movie
reviews we're actually not doing as well
as what's published in the literature
they're getting up to like 75 percent
and where it's we're getting about 65
and I think it's because we're looking
at these documents through the lens of
the Bible and that's kind of relates to
a question that was early earlier about
you know is this really the right thing
to be looking at web documents lastly
we've explored some ideas looking at
multi-level sentiment analysis and so
the idea here is if we're starting
solely from an English sentiment lexicon
so these are you know like words like
happy and pleasant and fun these all
kind of a happy feeling pleasant feeling
and people versus ones that are not as
happy like funeral and death and so
forth so can we look at that use just an
English sentiment lexicon can we look at
the other documents and classify their
sentiment so our approach is we label
some chapters of the Bible we picked a
subset of those so I'm indicating those
just figured here red and blue positive
and negative we can then project those
particular chapters into our language
antenna feature space training
predictive model using avatar which is a
software that it's andy it we're doing
ensemble decision trees we can then
once we had that predictive model we can
then look at the other documents in the
other languages and then compare back
with the actual labels in our from our
from the ones in English and so when we
train on French Spanish and German
languages we're getting about seventeen
percent accuracy so certainly better
than random and but not as good as we'd
like alright so I think so i think that
kind of concludes my talk so what we've
done here is we've created a framework
for doing a number of things and so by
taking these documents and languages
project them into a language and append
feature space do a lot of things with
competing similarities and clustering
text classification problems and we
actually have a lot of other text
analysis work going on at Sandy I
believe it or not and these are just
some of the things that I've been
involved with them I can talk more
intimately about these topics if there's
any interest but I think due to time
I'll go ahead and finish here the last
slide is just a list of all the
references so right any other questions
yes Abdullah how crucial is to have
perfectly aligned to the documents yeah
that's kind of the heart of the matter
here for us because we're starting with
the Bible it's very fine-grained
parallelism you know what a sense or two
in length and we know that those those
sentences in other languages should
match up with what we have in English if
the if the parallelism gets much if the
chunks are much larger than that then
you know I don't think it would work as
well we actually haven't tried it but
that's an educated guess so
how would you get it away so we've
looked at a mansion yeah so we've looked
at you know I think it's simple you just
go ahead and add you can add more things
than just the Bible we add more corpora
so we'd have you know in fact we've done
this not where we've augmented the Bible
but we've replaced the Bible with the
European Parliament proceedings and so
now we are the lens that we're looking
at other documents is more of a
political context or you know some world
issues you could look at things if you
included textbooks for instance then you
get more technology in your concept
space so the idea here is if you take
all of that together Bible textbooks
you're you know Parliament proceedings
these sorts of things then you have a
large corpus to draw from you know to
get at your underlying leading concepts
so if you want to do that you're going
to be able to handle like missing
languages in 10 recovers let's say you
take the Bible plus with your pet
benefiting so you have an Arabic
translation table I imagine that so your
parentage proceedings I'm not ready to
marry how you're going to deal with it
that's a certainly a research question
I'm interested in and I think developing
some computational techniques that can
deal with those missing blocks you know
you've got missing data how do you deal
with that effectively night that's a
research question I'm interested in so
in your experimental setup for the
multilingual precision have you tried
using just the dictionary data or just
the PMI for the use for the term of
lemon yes I think we've I don't think we
use the dictionary information partly
because it was easier just to deal with
Trevor's we had and just run our studies
but I think it would be interesting to
actually include more explicit and
refined relationships as provided by an
outside source right but what could also
just try to use the mutual information
like metrics you have right now without
using any of the document any of the SVD
based concept space right you could take
the take the term vectors from one side
chop them up into smaller pieces and
send them through your
term by term similarity and see what
comes out in the end and whether it's
still try to still manages to find yeah
without using any SD this we do stuff so
we've done something similar to that if
I understand correctly just looking at
doing the competing I convectors of that
d 1 block that truck and turn block and
so that actually does fairly well all
right I don't notice the numbers on that
but it's it's very competitive the other
questions did you too i used to go to
last word alignment tools i don't think
we used to not aware of using these
words language schools it's basically
those numbers that you stick in your
term by term matrix ah i think we have
tools to you know better kind of
parameters for that than just the mutual
information that whenever using they
basically resolve through some
competitive analysis of what words in
each sentence aligned to what words like
which which things cooker right oh to be
fantastic you XO because they actually
belong together yes it might just help
you out yeah I think that'd be great so
let me know after it will come for this
one sure I'll give you a little more in
translation that's why not Filipino
something for reelection do machine
friends we should have always done any
we just do wish pediatric translation
tools we need to buy them
or people people results India had their
own translation resources but it's not
been a research line for us in fact even
this work came and you cindy has a need
to do this we're at the apartment
because part of what we do is just
technology surprise investigation look
at what are could you people of another
company cable up a projector 30 may be
able to do the future and to do that you
and the tractor scientific literature
art so for the multiple love which
discoveries before but we started off
with this work because we had skills
essentially in linear algebra and tensor
analysis and found a brutal field of
application rather than having a
direction to do language before
Selfridge anything so cool about it
so something really interesting was that
when you show the performance of
different language pairs some of them
were really bad summer really good I
mean in the kind of your earlier
experiment so Spanish Arabic was like
fifty percent the English French was
ninety-five percent and we're used to
seeing that difference but I've always
chalked it up to kind of we have more
data in English French which is in the
case here yeah or we have kind of it's
harder for generate friends it's harder
generator because it's more collectively
with but here I think like it's a little
bit more of an even playing field
doesn't just think that you saw that
picture so and that we've always
ascribed it to just the language family
so arabic is a more complex language
than English and so trying to get those
co currents relationships as latent
relationship out of that analysis has
been more difficult 19 and you still see
those big differences after you add the
kind of term by term yeah it's always
there I turned up yeah so we it improves
it certainly but Arabic Russian are the
ones that are typically lagging the
others because you know English and
French are very closely related and
Spanish isn't too far off and so those
are able you know when we look at the
statistical co-occurrences in those
languages it's easier to find them as
opposed to in arabic and russian because
they're just different language channels
what's kind of fooling about that is
that there was a question earlier about
you know why does it even helped to have
multiple languages the q and there's a
comment that was made about maybe that's
something to explore you're giving you a
bit more refined sense the concepts
involved and what I patha says I have
that we had is that multiple languages
help to implicitly the ideas that plus
me is it conserved across language
families so although it can be confusing
it's just French in English or something
it's not going to the same Pacific
parisar it can be existing other
languages and so maybe that's why any
languages hopes that's the case it's
confusing while I Russian any Russian or
Arabic causes things to take because
there's such a distinct language
that seems like that was a useful
contribution of the even more useful
there so there must be it that's not the
current hypothesis or there's a
compensating disadvantage or including
the MTW it isn't there a simpler
explanation that basically just says if
you're only run your RS CD on two
languages then a lot of the work of the
SPD is doing is finding synonyms within
each language because it's kind of
finding sure exciting whereas if you put
kind of more languages there then kind
of your what you're going to learn has
to be more feeling bored yeah I think so
I mean I think I think it's pointless to
me as a way of looking at that i mean
it's it's it's the fact that poets
listen isn't conservative or languages
that causes I think SPD does sort of
spread it this its modeling strength you
know one thing we've been looking at it
i mean i'd be interested in reading this
room is thought about this is that you
know essentially what talked about was
with ls8 methods for campus analysis but
LD lDA's of the popular technique for
the campus center stuff and there's been
claims at least that LD is you know more
suited for dealing with polysemy because
we may attribute on its weights and
we've been thinking about but haven't
yet constructed an experiment to
actually do a head-to-head of LSA early
place to this purpose and but i'm
interested in doing that because i think
it might tease out some of the LSA vs el
dÃ­a influence on this kind of concept
model is it another family
right yes a non-native matric
factorization is a technique but it
would it doesn't work as well for
projecting new documents into the space
it's good for just doing an unsupervised
clustering of what you have right but I
think that the main complaint here is
that you couldn't awesome point this
particular accounts when you
and there's not what marginalization our
posse that's right yeah and that's why
this they do the scaling you have like
login troops in ginger scaling or term
frequency and we're talking and
frequency scaling these things try to
account for that well thanks for you
time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>