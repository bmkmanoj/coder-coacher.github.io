<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Speculative Parallelization of Applications on Multicores | Coder Coacher - Coaching Coders</title><meta content="Speculative Parallelization of Applications on Multicores - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Speculative Parallelization of Applications on Multicores</b></h2><h5 class="post__date">2008-10-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IgKYhGa8O6k" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">from everybody thank you for coming here
I'm very happy to have my advisor
professor rajiv gupta who's faculty at
University of California Riverside to be
here with us today he's going to be
talking about his current research on
speculative paralyzation for multi cops
okay thanks okay so if I don't sound
enthusiastic about my work is probably
because i'm sitting down and giving the
talk not because i don't like the word
okay so shriram said i'll be talking
about speculative paralyzation and
clearly the goal here is as follows that
you may have programs where when you run
them you observe that there is peril ism
but you're not able to you but you're
not able to statically necessarily
paralyze the programs and so this could
be due to number of reasons so for
example you could have the you know part
of the code which is very infrequently
executed the dependencies could arise
from there but in most cases when you
don't execute it you may observe
parallelism it could be because there
are dependencies but those dependencies
are very harmless so if you ignored them
in parallelization it wouldn't really
hurt the program or it could be because
the dependencies are there because you
read and write memory but actually you
read and you you overwrite a value with
the same value so those dependencies
actually don't have any impact so so for
various reasons you have pal ISM but you
can't do it statically so the goal of
this work basically is to you know what
we did is we developed a very simple
execution model which will allow you to
speculatively execute programs or parts
of programs and basically what it
consists of is you you divide your
computation into the non speculative
part in a speculative part so if you
wouldn't wouldn't do any speculation it
would be like sequentially executing the
program through this non speculative
thread but if you can speculate
threads then you get things done sooner
and these threads are actually running
on the additional course that you have
and of course once these threads finish
you have to commit the results and we
will commit the results of the
speculation is successful otherwise we
simply throw away the results in other
words these speculative threads don't
overwrite results in the non speculative
state so you don't require any rollback
you simply throw away the information
that you computed and then we have in
our work we have taken this idea and we
had applied it to some paralyzation of
some loops which had coarse-grained
parallelism and this was implemented
entirely in software and so we have some
results there and then in more recent
work we are developing architectural
supports so we can apply the same idea
to do more fine-grained parallelization
and one example of that is you can
speculatively execute pass barriers
small amounts of code and speculation
doesn't fails then you are ahead ok so
this is the model so as I said so we
have one main thread and then any number
of parallel threads the main thread does
the non speculative competition the
parallels hair does all the speculative
computations the main thread controls
what these parallel residue so it
creates them initializes gives them work
takes back the results performs the Miss
speculation check and commits the
results in order but that I mean that if
you if you assign the work too many
threads one after another you will
commit the results also in the same
order and you know so so basically then
you are making sure you are following
the sequential semantics of the program
ok so this is just showing that visually
so if you have a loop we divided the
loop into three parts the prologue the
epilogue and this is the speculative
body so this is what's going to run in
parallel these parts are going to run
sequentially so it's going to be
something like this let's say the main
thread is it going to execute the
prologue and assign the work to the
threads they execute when one of them
finishes it returns the results to the
main thread it commits the results and
it assigns more work to it same thing
with the next one and when these finish
you know those results are committed so
you can see that the epilogue and
prologue are being executed in exactly
the same order as they were originally
in the program and it's the only the
speculative body which is executed in
parallel so I'm showing this main thread
and these two threads but this main
threat doesn't actually run on a
separate core so actually this main
thread since its job is to just add
these transition points deal with these
threads when one of these thread is done
then this thread wakes up takes care of
this work and then it goes back to sleep
so this main thread is just waking up at
this different course to do the
assignment of work and committing the
results okay so okay so this is what the
state looks like so we have the non
speculative state which is the memory
attached to the main thread so if you
are running the program sequentially
this is what you are running but if you
speculatively create parallel threads
then there will be separate memory here
and whatever this thread needs to read
or write it'll lead and right from here
and the point is that as I mentioned
before once you are done if there was
Miss speculation you simply throw away
these results and this is still intact
and you basically repeat the computation
again till you're successful and then
you commit their results and there's
another part and this is where your what
I say is coordinating state which is the
auxiliary information that you have to
keep around so you know whether my
speculation has occurred or not so for
example we keep these version numbers
I'll talk about more in detail but these
version numbers let us detect miss
speculation now let us detect miss
speculation but but these version
numbers do not allow us to do any roll
back or anything so there's no role
button' involved this is only for
detecting miss speculation
then there is this mapping table and the
reason we need this is because once you
are done you need to copy the results
back from there to here so it's just
telling you what to copy from where to
it okay so obviously if you do a lot of
copying like for example if you were to
copy this entire state from here to here
do everything and then copy it back
that's going to be very expensive so the
key thing is that we want to find out
what we need to copy and can we reduce
the overhead of that copy and so so we
don't want to just copy and copy out
everything but what we have is this is
where we are using profiling to figure
out what we need to copy and what we can
leave alone okay now since this is based
on profiling we cannot perfectly tell
what we need to copy okay there's not
based on static analysis so what you're
doing is you're saying actually these
things I think I need so I'm going to
copy these other things I don't think I
need I won't copy and if things work out
okay then everything is fine but you
have to provide a backup mechanism so
that if you touch something that you're
in touch during the profiling run then
on the fly you can bring it over so on
the fly you have to provide this
mechanism so the way to think of it is
the exception it's like I having an
exception mechanism if you don't have
the value and exception happens and it
brings the value back but once you
provide this then this can be
probabilistic you can you know you can
copy some things not other things and
you because you always fall back on this
to bring in the value at runtime and the
mapping table essentially no keeps what
you brought from where and so on but
also the version number now let me
explain a little bit about the version
number when you copy from the non
speculative state to speculative state
you're copying shared data each of the
shared data will have a version number
attached to it so every time you change
it you increase the version number now
you're copying that because when you are
done when the speculates that is done
with the work the version number should
not have changed if it has changed that
means it read the value too early and
earlier thread change the value so that
means we have a Miss speculation okay so
so
another way to explain this is that what
these speculative threads are doing is
they are reading values before they were
before all the earlier computation is
done so in a way we are doing a
prediction of those values but the
prediction is simply the current value
and then later on we just need so we
need the versions of numbers just to
tell that my speculation has occurred
not because we want to do anything more
elaborate than that so after that once
we have the DS version numbers you know
we will be able to when we commit that's
when we are checking if the version
numbers are the same or not and and so
so the Miss speculation is simply if any
version number doesn't match you are you
have a Miss speculation this is just a
side comment but you know you could do
the checks on addresses but if you have
some values where you think that usually
right to those variables but their
values don't change then you can do a
value-based check also but it'll be more
expensive because you have to remember
the old value then even though you think
it has been overwritten it's been
overwritten with the same value and
therefore you don't have a Miss
speculation but the key thing here is
that because of the way it is we do the
version number check a Miss speculation
occurs you throw away the results there
is no rollback or anything we simply go
back and repeat the competition you
basically set it up again and you repeat
the computation and if if one thread
miss speculates then the later threads
can't commit their results you know so
it's not that if one thread miss spec
list then all the later threads are
squash know they do their work they wait
for the first set to finish and then
they try to commit if they can't commit
then they repeat their competition so
it's possible that later threads are
finished earlier because then in Miss
speculate and if nothing went wrong they
could still come back there goes right
away this is just a example to show you
know how we are dividing up the loop
into three parts and this is the
prologue this is epilogue this is a
speculative part so what you would put
in the
prologue and epilogue are the things
which you know are dependent from
iteration to iteration so there's no
point speculating on them because even a
Miss piglet plus you put all the iOS up
there because you can't do I owe from
the speculative competition so what is
happening is if there are any i/o
operations there they are simply
buffered and only when you commit those
as you actually do the i/o so so this is
basically separating out the speculative
part from the rest of the loop body and
whatever goes up or down goes into the
prologue epilogue and prologue then so
so when you take so once you have
divided a loop like that into these
three parts and you transform it it
looks something like this that first you
are creating the threads and giving them
some work when you give them the work
you execute the prologues then you are
waiting I mean by you I mean the main
thread is waiting and when thread
finishes it checks if the speculation
has been successful if it has been
successful it's fine it'll go ahead and
give it more work but if it's been Miss
speculated it asks us to repeat the word
it copies the values again with the new
version numbers and the computation is
repeated otherwise you commit their
results and you assign more work to it
and then you repeat the whole thing
again when you are committing that's
when you're using the mapping table to
copy the values but yeah yeah sure so
where does the value gets loaded in the
example I showed from parser the line
variable is getting overwritten
conceptually every time you run the
prologue now if that were to be stored
in memory how would you go back and redo
the comput
particularly ok so the prologue is not
executing speculatively so so what
you're doing is basically you are not
really I mean you have executed the
prologue once you're not really getting
the prologue you're simply reacting I
guess I where's the standard memory
buffer like there's the state of memory
at the beginning of every specula thread
and that needs to be buffered somewhere
right so yeah oh yeah it was a cute yes
I so all that is in part of this space I
showed on the right hand side which is
with the thread ok so when you're
setting up the third you're allocating
the space you're giving it you know I
mean you have to make sure if you need
to save something like you said the line
you have to save it there so when you
repeat it you can repeat you so that's
one of the working stores as well as the
initial states exactly exactly so it's a
lot of copying yeah ok so this was so
this is so as far as the trail is
concerned actually it's very
straightforward all it does is it waits
it waits for a signal to start the work
it does the work its signals is done the
work and then next time when it does it
you know again the main thread will say
now you're set up again do it again
whether it's the old work you are doing
on new work you're doing it you know
it's basically the same thing you
automatically have all the rest of the
threads also develop them is speculated
or you wait to see individually whether
you wait to see because it's possible
that the dependencies are you know not
from not not like that so especially if
they have finished the work then might
as well check you know now then throw
them all away so yeah so basically what
we will do is will it's like you're
stuck on the earlier thread because you
miss speculated it repeats eventually
you will succeed once you succeed you go
to the next one you check with Miss
speculation occurs then you've lost it
to repeat so in the worst case you will
squash all of them but you don't a
priori squash them so I mean they're
couple of sort of other issues here that
obviously this is not a very scalable
thing you know because the main thread
is doing some parts of the computation
serially as it was in the original
program so of course this is no
scale forever but I think for small
number of threads and scale but even in
the small number of tests which is we
did it 48 course because that's the
number of course we had on our machine
you can run into this problem that if
you're giving very little work to each
of these threads while you're assigning
work to them you know the first one is
finished and now you're signing to the
fourth fifth so having more courses in
hell because you could have might as
well go back to the first threat of the
first court assigning the work to it so
here the I mean there's no magic
solution here for us basically what we
need to do is assign the big enough
chunk of work so that by the time you
have assigned to everybody and you come
back you know the first one has just
finished or it's not been sitting around
for a while so you need to make sure
that you know if you don't want this
idling to happen you you give it a
bigger chunk of work then the other
thing is that in in one of these
programs we saw that if you use more
threads the Miss speculation weight
increased and I think because if you're
doing more threads in parallel there are
more dependencies along which you're
speculating and if the application is
such you will have more problems so in
that case we found it to be beneficial
that if we for some values which were
causing this miss speculation we don't
we don't copy them in the beginning we
just leave them alone because if we copy
them later there is a chance that the
thread that was going to overwrite it
has already over it in it and you will
actually get the real value so you have
you can so you can tune what you copy
because you always have this mechanism
so earlier I was trying to say we want
to copy everything beforehand kind of
maybe I or implied that we don't copy
everything beforehand every really need
but among those there may be such that
you need them but there will be over at
answer we should delay their copy so our
implementation is basically we use the
pin for carrying the profiling
information but it's the
affirmations are being done in the
intermediate format for llvm and
essentially all of it is not fully
automated so we basically have the
template as I described where you put
the prologue where you do that belong
the overall structure of the program is
fixed and what the this lvm lets us do
is it let's separate the different parts
of the program what goes into the
speculative body what goes into the
epilogue and so on and once it
identifies those parts it's not fully
automated some of it we actually
manually plug into this template and
then then whatever back end we have we
generate the code from llvm intermediate
code and then the machine that we ran it
on was a so this has two quad-core so or
total of eight cores so it was a dell
machine so we learn so we were able to
run six program so again i want to point
out these six programs or not like we
just picked up six programs and have you
got good results no I mean there are
some programs which we picked up like
when we are looking at my bench they are
actually statically parallelizable so we
don't want to look at those because they
are sadly palledorous hope it's not
really a very valid test for this then
the other thing is these are the
programs we did there were some in
progress which we couldn't get through
because of you know problems but these
were the programs which we first studied
and we said okay I mean there is
parallelism here so let's put them
through the framework ok so I'm not
claiming that you know give me any
program I'll speed it up by a factor of
antoniette codes or something like that
judge so all the show all the results
show is that there are programs on which
you can get speed ups through the
speculative paralyzation this is simply
showing the number of areas you're
copying in copying out and so on and
this is just to indicate that this you
know the copying and copying out that
you are doing statically is not very
huge
and the cases where you do lot of
copying is usually the heap but the heap
we always handle on the fly and so in
those programs the heap copying
dominated but this is excluding the heap
so this is the one which we do in a
priori so these are the speed ups so
this is the number of course actually
when I'm saying threads thread well
there's one thread per core and the main
thread is just waking up everywhere so
this is it be Harriet code so we went up
till eight threads so we have nine here
this red so eight speculative threats
and so more or less at least it goes in
the right direction you know the slope
varies but it is somewhat linear in most
of these programs speed of the portion
paralyzes the entire event ok yeah
that's a good question so in among these
programs the mcf one this is only the
loop he paralyzed and in the others the
main loop was the sort of the big loop
which was paralyzed able so there it was
the whole whole execution and so the
speed ups on a threats varied from 3
point something to seven point something
and but you know some of these programs
are easy to paralyze like pass this one
visit but the but the thing is that you
you know at the same time there are
dependencies so of course as a user you
can rewrite these programs also but
there are dependencies so you cannot
just purely do using static analysis you
have to do speculative paralyzation
that's sort of the main point ok so this
is a showing that in four of the
programs this thread idling was a issue
once we reach for and you know it would
just flatten out make here here this is
maybe five six here over here at six no
flattened out and actually it was
because of this idling business so if we
assign bigger chunks of work we were
able to you know get the
that we would like to get ya the
situation improves you dedicated a core
two running the main thread rather than
having a context switch around the
idling will it improve no I think the
issue is not that because you because
it's not the main thread which is called
which is the problem the problem is that
you give the first thread some work so
let's say main thread we don't need it's
zero time let's forget about it we give
everybody work but we are giving it one
at a time and that handing out those
code that's running in the main threat
yeah that's exactly what i mean so the
main thread had more cpu cycles and be
able to hand out faster no because when
the main thread is executing it's
because that code is idle you know so so
in so initial implementation we put it
on the separate code then we moved it
here and it didn't make much difference
so the problem simply is that you're
right but so I we couldn't speed that
part up you know that's the way to look
at it yeah this is a showing that there
was there was only one program in these
programs in mcf where the speculation
rate keeps increasing as you do more
threads and I suppose that must be
because you know each thread is not is
dependent upon maybe iterations which go
much earlier in which case you will have
the more threads you have more miss
speculations we will have if he stirred
was only dependent upon just the
preceding threat for example then more
threads don't hurt you but instead can
be dependent upon the previous three or
two or one information then it starts to
increase so there are some of these so
so when we did the delayed copying where
we just do some of the variables on the
fly then we did reduce it somewhat of
course we can't get rid of the problem
because you know inherently these
dependencies are there if you spawn them
off early you are going to get miss
speculation but it did help out someone
I mean this may also be an indication
that this is probably the limit if we
had 16 probably if you do much better
you know if we had more course we know
you wouldn't do any better maybe this is
you know this is where
you would probably do the best okay so
this is a showing that if he had
different copy strategy we said why not
do everything on the fly allocate the
storage would copy everything on the fly
or yeah how do you protect this thing
for instance one writes from the state
level how do we protect it so basically
this the state which is for each thread
is not written by any other thread other
than the main thread it sets it up so
you allocate say you know brand news for
stack locals everything her cause
violence predictive value that is a
pointer and it goes you're missing
Antonius Lee no no so we don't know yeah
yeah so we don't have any protection for
that yeah we don't have any protection
for that so I mean we could put
something because see actually what
would happen I'm just thinking now what
will happen if that that situation arose
it would be a bug because if it's not a
bug I mean you will you always go back
to the main thread and main thread will
read the current value and give it to
you but but it overflows or something
causes an exception which otherwise or
maybe some yes also actually that's why
there is another piece of work with
somebody did in that no not in Rochester
with they did this using process level
speculation because then they can get
the protection from that but one of the
things which motivated us to do this was
with the process space the thing was
that when the speculation succeeded you
had to do a lot of copy right because
then now you say you know we speculate
it and this is successful it is further
ahead so whatever you didn't copy from
the previous process is a previous
process I don't need anymore you have to
copy so the common case I mean if you
are succeeding you do copy and here is
the other and there they kept you know
then pages
you know they had some information for
pet peeves level maybe to reduce it so
yeah so if you have a bug I think you're
right this can happen so this is showing
that if we if we are doing copying used
on the fly or we copy everything by
everything I mean like we copy the heap
and everything beforehand or optimum no
he p always copy on the fly I mean copy
all the variables beforehand all of them
on the fly and this is the sort of the
heuristic which is optimized one so it
does pay off a little bit but in some
cases but yeah we would have thought it
would have paid off much more but I
think the problem is that a lot of the
references are to the heap and the heap
all three techniques are the same folder
he so therefore for example when you
look here you know I mean if most of the
references are the heap references so so
copying all are on the fly they are all
pretty much the same because there's
only small file on the cost but it did
matter in some programs as you can see
this speed up is higher than that here
these two do well this one is higher
this one is higher so it does give you
know some speed up in some program like
this is 2.5 and this is like three so it
does it's not insignificant but in some
cases on the fly is good enough actually
so this is kind of summarizing why you
got good speed ups in these programs
this is showing that when we are doing
these copying or doing the Miss
peculation checks and so forth I mean
the instructions we executed there were
only small pile of the computation
because we doing a lot of work and then
doing little bit of checking so it
worked for these programs but it may not
work for other applications where the
PAL ISM is more fine-grain because here
it was like seven percent of the
instructions you are spending on doing
all this checking and stuff so therefore
you could afford to use this technique
but of course this is not gonna be true
for every application so I'll talk a
little bit more about that in a in a
slider too so this is simply showing
that of course your memory overhead goes
up okay so in the program's you know two
to three times and here almost six times
so the more threads you have the more
memory you can zoom because each of
these threads have their own memory so
that's the cost you are paying because
you're making use of much more memory
yeah so the thing is that so the key
thing is that in those programs we were
doing course Cal grain paralyzation so
it was fine but that's not always the
case so now we started looking at you
know can we provide architectural
support and this is what we are doing
these days so that we can do these
things that more finer grain level we
can use the same model but do more
fine-grained sort of parallel execution
and approximately how many instructions
actually I don't know but it was I mean
it was these instructions that we
executed for checking they were let's
say hundreds of instructions as opposed
to those were at least you know tens of
thousands more you know so it's a very
small fraction of the the work in the
case of parser memory serves me there
for reference input their tens of
billions or hundreds of billions of
instructions put in actuality not too
many loop iterations on the order of
hundreds so for parser each loop
iteration might be 10 million instruct
okay so they have any longer sir yeah
very long interesting i'm not sure what
loops you didn't know i mean visit their
friends here put yet it means the
parties if is a very similar situation
where does a lot of work yeah so there's
a lot of what in these and that's how it
works so in the fine-grained one so this
is an example this is an example
application we are looking at that if
you have barriers and you execute pass
then you're going to going to do a
little bit of work so now if you do a
checking and it costs you a lot then
it's not worth it so we try to apply the
same idea for the loop is running in
parallel you reach a barrier you don't
stop you continue but when you continue
you write in a separate memory and then
you have to copy bad same model here
what happened is when we try to do this
our program slowed down you know of
course we did some useful work but then
we did too much checking in to merchants
so it kind of motivates for me to have a
better you know hardware support so you
can be
down the cost so you know so the
question is what kind of solution should
we have for this then I mean what should
we put in hardware so the key thing for
us is that two things one is copying you
know so we haven't looked at how to
reduce that cost but the other cost is
then we do these checks that you know
has miss speculation happened or not and
so we so we have proposed some support
for that and the support we have say
let's see what we want to do is we want
to propose support which is not
specifically for one thing but hopefully
something which can you know you can say
is generally useful for many different
things if you were to provide it on
multi course so we didn't provide any
dedicated support for this but what we
are doing is we are using this idea that
see if you could expose these cash
events like invalidates and value
replies to the application then you can
identify the inter processor
dependencies when they are cut and Miss
speculation simply that's all you want
to know was there a dependence or not so
you know so if you have a mechanism so
you can expose this these events to the
application then application can say oh
if this even happened miss speculation
is occurred and i will just redo the
work so now it can be used for other
things also he can be used for other
things for example a lot of monitoring
applications so i should let me describe
the support then i will explain why that
that could be used for other things yeah
so i think probably like donation
because what
neutralization memory r1 this kind of
support and the hardware too but the
problem here is that you have to keep a
lot of state in the hardware and you
start earning out of the that state of
the or the hardware that you put in
France out of craziness oh okay no okay
so let me respond to that so you know
like obviously I don't want to go that
route I was so emphatic about pointing
out that I don't do recovery by
rollbacks so I I don't want to go there
so in our case I don't want all the
state I just want to know there was an
enterprise for dependents okay so that's
all I want the hardware to do just give
me the events now if I want to analyze
for them forever and figure out which
location cause the dependence and how
can I now say we can recover it like
which the transaction many people have
to do because you spec literally do
things then you have to roll back that's
a different thing but i'm not you know
so far for this word i would say that's
overkill I mean I'm I won't use most of
the scale you want on so in that sense i
agree with you i don't want all the
stage i have owned a simple mechanism
which will help me do the job so for
example like if you're doing raise
detection I want to know races occurred
and then you go you know maybe capture
the instructions which caused the races
and then you replay and then you figure
out what location it was and all that
but you don't need to really do all that
so dependent detection don't you run
into the same problems where if
something is evicted from the cache
before there's the actual into thread
dependent you now have two
conservatively assume that there will be
some dependency occasion yeah so in this
one in this one it is conservative to
some extent because for example even if
you find the dependence between I mean
it could be at the block level like
you're finding at a block level but
actually it's yet what I suppose about
was that the question earlier was
talking about how do you purchase all of
these
so that your transaction sizes or what
not being the arbitrary and you're
saying that you're not actually storing
extra state in the hardware which is
true but the flip side of that is that
the ability to track dependences relies
on you know it can basically only
dependences can be tracked between what
fits in the cache because beyond that
level you now no longer remember that
you locally even access to something so
from that standpoint if one task is
overflowing the cash regularly you have
no confidence about what dependent so
actually we don't have that issue
because really the way we are
maintaining these the information is if
you have a dependence it's like the
shadow memory concept so we are using
shadow memory and if you if you Rick
something I mean you're saving state but
you're not saving I mean you're saving
it in memory okay okay so yeah so
basically that part is still in software
you know so yes we have spend a lot of
time thinking about this so in this I
didn't mention but this relies on our
work on shallow memory where we have you
know how we implemented and so forth but
I'm not really talking about that I'm
just focusing on another part yeah so we
won't rely on the hardware for that
sighs yeah so the thing is basically so
once you have these events you know the
question is what you do with them after
that I mean whether you yeah exactly but
I think that can be done but then you'll
have to really do more work so the way
we look at it is that we expose these
events then the application should
decide what is good enough even if it's
in precise or the heuristic and that you
program you know whatever application
you are doing you program the handlers
and in that process you can do these
trade-offs you know so that's so the
basic support is very simple so this is
just to show how the barrier thing works
so this is for an example for the you
know the Jacobi where you go through you
know a bunch of points where each point
you replace with the average of the
neighboring points and then you do that
all over again so if you paralyzed this
you divide the work among different
course you basically divide the points
into strips and you give it a different
course and each code will compute its
strip then what will happen is once all
of them have done it once they will do
it again and then they will read the
values which are computed by other
strips before therefore you put a
barrier you know in between so all of
them do the work then they synchronize
then you do the work now what happens is
if you are waiting at the barriers we
can go ahead and execute pastevalues
that's the speculation and when you do
that so here is the same computers to be
and now you recompute strip p when Yuri
computes to be you'll be using these
rows to compute these values and those
may not have been computed yet so you
would be speculatively reading the old
values and so that's where it's the same
model you know as before we want to know
if the when we read these were they were
they already computed or not and you
know so and and what we do is basically
when you speculatively compute this to
be you don't write it into the place you
were going to write into you write it in
a scratch place and then you will copy
it back so
if we just do this in software as i said
we slow down the program but if we have
the hardware support we we don't slow
down we get modest you know improvements
in speed up so basically instead of ten
percent slow down we get ten percent
speed up for some of these programs so
but now let me talk a little bit about
the software support so okay so what we
want to do is you want to expose the
cash even so I think I already mentioned
this but let me explain what the support
is basically the cash events by fixed
you know the ones that can be exposed
and you as a user if you want you can
provide handlers for those events and in
addition to that when you once you
provide the handlers you have these
instructions now this specifies the
handler you know where the handler is
for a particular event but then you also
have these instructions which says where
these handlers should be active so these
handlers need not be all the time active
like you once you go past the barrier up
till seven point you say start handler
and handler that means the handler is
only active within this code and the
second thing is we have instruction
which says which events you want to keep
because you don't want to keep every
event you know every is for example
invalidate message you get you only want
to keep the invalid messages that matter
to you so you can say only if the lock
address is between this and this then
remember it so so you're so because you
are only interested in finding the
dependencies if they occur in the
speculatively executed code that's why
these first these instructions and only
if they are from the relevant part of
the memory that's why that instruction
okay and then you go ahead and you know
you program the handler like in this
example because this program the handler
to say you know you can do a set jump
and long jump and go back to the
beginning of the the code that you
executed speculatively and that's that's
good enough and but then you're
specifying which addresses matter to you
in the filter instruction and once you
leave this code then the handler is no
longer active it's not being considered
no none of these events are being
so it's not for all the time it's when
you need it to and these were few of the
programs where the barrier if he did it
in software we got you know slow downs
but if he had this support we got speed
ups now the these programs are spending
a lot more time in the barrier only part
of it we are getting as as a return in
speed up because the other part is due
to copying so we're not you know like if
there was this much time half the time
if this much time as we spend in the
barrier half of the time I'm using to do
copy from the other half i am doing some
miss speculation so maybe thirty
thirty-five percent i actually get back
as useful work so copying is still an
issue where we do you know we don't have
any support for this review software
simulation this is simulation on
simulator you know with you know so
because we have to implement it the
instructions and so forth so the same
could be you know so there's a lot of
recent work on record replay if a record
and replay based debugging and you have
many systems like bug nets that a PR to
basically you know do that so we can
simulate these schemes once you have
these using these handlers you can
essentially capture the information that
you were capturing there of course i
have to again go back to what I
mentioned before that you have to keep
some additional information which we
keep in shadow memory and that
information you know that part I haven't
talked about but but you need the shadow
memory also because and there is another
issue once you keep this Meredith so
let's say you have some days some real
data and you have some meta information
you keep it somewhere else and the issue
is that whenever the data changes you
change the metadata but then they should
be atomically updated or at least you
should see them as a topic so the way
these handlers are designed you know
because you can invoke them where you
want to you can also select where you
want to invoke them if you can invoke
them at a fixed points then you know
that DOMA city would be guaranteed
because if you update memory
you update member meli data and then you
invoke the handler then the drama city
would be given to you but if the if the
event would come up at any time and
invoke invoke the handler then the Tama
city wouldn't be there so we have two
versions of this handlers either you can
invoke them anytime a event happens or
you say where you want to invoke it so
for example you could have said at the
end of the shot handler and handler at
the end handler now invoke the hem and
then process all the events which you
have collected anyway so that's kind of
the summary of what I've talked about in
the recent what we are doing is you know
first just looking at more aggressive
techniques to do speculation then we are
looking to see if we can leverage some
of this work to do more fine-grain
optimization so for example here when I
suspect native a local its allocation
what I mean is that if there are many
threads and you think other threads will
touch them some part of the memory you
can't put it in registers what we want
to do is we want to put them in
resistors and we want to execute this
search speculatively but the end and
then realize that oh okay this was
speculatively done so basically leverage
what we already have to maybe
aggressively do resistor allocation
which you you know couldn't do if you
know if you were running the threads in
parallel and we are of course exploring
these monitoring applications or idea of
applications and to see the support we
are providing whether they can
conveniently with the support we can
program those applications so the work
is on board track some is on you know
pushing it further it would optimize the
program's more some for other
applications to see if this cache
support architecture support can be more
widely used for many applications we
think we can because we designed it that
way but actually going through the
applications doing them seeing what you
benefit that's what we are in the
process of doing okay so that's all I
have you
question about any effect
multiprogramming has on these results so
have you done any of those experiments
no no so err them when we are using that
machine we are running basically you
know our program I mean we can do those
experiments at least because you know we
can spawn off other things and then at
the same time run it but but I think
then that means you would have to figure
out how many cores you can use and when
to use them and so on so we haven't
looked at those issues yet when when
there are other applications running how
to kind of scale back or you know how
many calls you give to an application or
how many threads mean we haven't done
any of that so right now basically it is
like we run it Museum eight-thirds you
can run because we have it course and
nobody's using our machine right the
loops and to what extent to that
automatically be done with rules so
basically the we didn't rewrite anything
at source level the reload after at llvm
intermediate core level v rewrote after
llvm told us this part you know because
see one of the things is that some
information he has from profiling but
there are parts of the code which are
never executed so that information we
get from lv m once we have identified
which parts they are then we are you
know rewriting them these applications
of the number of such interactions were
fairly small numbers yeah most of these
applications because it's the outer loop
the rewriting is not that hard of course
I mean we hope we can automate that's
why we lead out at the intermediate
level because you want to automate it
you know not you know so it's not fully
done yet but you would like to do that
but but I was talking to the you know
you idea what we would also like to do
is the information we collect you could
give it back to the user and the user
could rewrite it because if you want to
rewrite it actually it's better to go
back to the source level I mean at least
for these applications probably is
better to go back to the source level
then to the lower level we were simply
doing at lower level because we only to
make
sure that what we are doing we think we
can do it you know he is using the
automation some of these benchmarks are
like Reggie said they're very easy at
that matter most Lublin phone so you
know having just gone through the
process of doing similar parallelization
for millions as I can say we can parser
for example one of the biggest obstacles
is they use a high watermark memory
allocator and so forever they're
processing a bunch of sentences in a
file and they just remember how many
blocks they've allocated the last time
it if you need more you allocate a few
more and then at the end they end up
popping off of the staff so if you do
dependence analysis use of all these
stack manipular shoes well it turns out
the stack is always empty at the
beginning of every liberation but so you
can either analyze your way to that
realization or speculate your way to
that realization either way that's the
the solution to the problem and so some
of the as part of my dissertation work
some of these things we were able to
automate and some of the things just
about this future yeah actually some of
the program is that from you use it so
we benefited from his work because then
we said these are the programs that he
has put through so let's see whether
this approach camera what what and I
think and some of those like you know
you should maybe go back to the user
level them do because like that things
that writing your own memory effects it
actually I put it another sort of
heading of harmless dependency that you
can ignore them for these harmless ones
are the ones that you would know them
you know but automating that part is is
pain it's interesting to see because
doing a software you got these loops and
turns over fine
thousand instructions or more and when I
did the same sort of stuff I did it with
hardware you know with the Camaro me
codons check price the hydrogen and our
loops run over 300 instructions and
their final instructions in a completely
different world I noticed that there's
almost no overlap between the
instructions we could do yeah yeah so on
the other hand I mean the reason is
because we knew we were doing everything
in software so we look for the coarse
grain winner and but that's why we are
doing the follow-up voice yeah we could
love buffer that sort of state and of
course we could take off these really
small loops because you know our
communication relays were so short I
think it actually turns out to be very
important to pick what the loop next
level because depending on whether you
go really deep or really far out you can
just find you know these tons and tons
of dependences that prevent
parallelization but there's generally a
sweet spot level or if you hit that
level there tends not to be a whole lot
of loot care dependences and you know
the parts that exist can be split into
this prologue and epilogue and whatnot
put it it does require short carefully
taking that next one back to the
hardware
so the only thing that you're interested
in is cash invalidate and they stay
calling a handler you have no stayed
inside the l1 or l2 that you have
invalidate coming in from a different
chip well actually okay so in the case
where I said we buffer the events then
you have to have some place to buffer
the events so if you want to buffer so
in this work since we are only using it
for a small chance of code we are
assuming you have a dedicated place
where you can buffer certain number of
events as opposed to but if you want to
scale it then you will have to go to the
cash or something that you can you know
every block maybe can carry some state
but we are not doing that right now yeah
because we are not using it sort of
using it for certain applications and in
those applications we know the code is
only this much and you know there's not
so many events would occur and you know
and we are only interested in very
narrow set of events anyway you know and
and and if even if that buffer would
overflow then you can just send miss
speculation and come over everything
have you thought about it one potential
things you could do if you
the information on the cache line that
it's been that it's been read that it's
been basically read between your start
handler and Hunter so basically it's
been read speculatively or not or it's
been written speculative yeah so right
now it's actually for us it is more
convenient because we have to then go
through this in the events it's better
to have them together as opposed to look
in the cache because then you will have
to say which blocks I care about then
look at all of them so I think it just
depends how much you want to scale I
mean what you're using it for if you
start scaling it to large number of
events you know but right now that
monitoring applications are the same way
yep you know we instrumented something
only for this much code we care about
this issue not outside it and so we
don't need a lot of events it's only to
you know like for example in some of
these when you're doing these metadata
updates and the data updates you want to
make sure nothing happens in between so
if somebody and so for that again you
know the number of instructions it's
just these few and so it's not so it's
not general arbitrary code like a loop
or something in there that's our pc now
if I kind of have a very crude version
of this where you can do load with
reservation and the store conditional
it's not intended for parallelism but
it's intended for atomic increments not
sort of thing grabbing on so ever so
it's are you basing proposing an
enhancement of that mechanism listen to
converge at some level well i mean i was
thinking more like this that may be so
what I'm trying to do here is basically
i want i wanted to do course level
things like this much competition and at
the end simply know if an even happen so
I'm not so much interested in all the
like example if Miss peclet will happen
all the ways in which it happen but
simply yes and no one said whether it
happened so in that sense is live
so the store the conditional store would
fail if anyone came and modified the
location before you did no but but the
thing is I want to monitor lot of memory
I mean Lord made a lot of memory means
not just one location many locations so
age level or multi multi page so maybe
page is too big and what you're saying
is too small it seems to be some very
nice what's a mile harbor today yeah
additionally a lot of YouTube monetary
very small number of locations before we
open it's one I think it's just one
reservation radio okay so that's very
small so we're saying is that you want
that expression extended to hundreds or
thousands of location well that would
probably be impractical and profitable
but perhaps so I think I'll sort of
initial tends to get the cache line
level maybe at the TLB level would it's
more similar to what we need is Jesus so
basil ever could work in some cases if
you made sure in that page you only put
the stuff you want to further not
something it so then you would take your
stuff and spread it across pages you
know I mean that's another way to use
something which is already there you
know
talk a little bit about how you dealt
with the shadow memory and the
efficiency of doing address translation
another thing oh yeah so what we are
doing with shadow memory is that so
there is some hardware support their to
foreshadow memory even the initial
results were from a simulation or okay
so in this one we didn't need the lab
rachelle of memory but what I'm saying
is that it for some other applications
like Vanessa replay and all that there
we needed it because you know for their
we need a lot of shell of everything so
what we are doing there is something
like this that every time you allocate a
page you actually allocate a shadow page
and when you write your code your
instrumentation code you are addressing
actually you address you don't
explicitly address the shallow page you
just address the original location but
but you communicated these this is like
this instrumentation code so you assume
that the hardware will translate it into
the other page so that way you are not
like creating multiple tlb entries
either 70lb entry and the shadow pages
you know simply just next to it the two
pages are allocated together so you can
compute the address in the translation
process and so that's the address
translation part and the other part is
is atomicity and that is related to this
in walking the handler if you invoke it
if you can choose when to invoke it then
you can avoid this problem so what about
for just the specular parallelization
that you didn't this work and in at the
beginning of the talk that they know
itself community that's all so you just
did like hash table normal yeah that is
purely so basically you didn't need much
shadow memory there actually it was i
was saying mapping and all that you know
so it's there but that's not shared I
mean it's per thread so you didn't mean
so I'm just imagining so for you I you
did parallel as mcf which has a lot of
link data struc yeah sure which means
this point they're stored in data
structures which are live into the
the code your paralyzing yeah so when i
get a pointer how do i guarantee that
i'm going to access my low thread local
copy of that pointer rather ok so you
what you do is basically with those you
go there and then you allocate the
entire object so basically you copy the
entire object that you allocated and
then you reference only that so you
never reference the global heap ok so
you only copy there just like you copy
the statics are copying the heap now and
but I i guess i just i don't understand
how the pointers work like how do you
remember which pointers are original
program original heat pointers like if i
copy a data structure with a pointer in
it then the copy is pointing to oh icic
and then you know now as i'm executing I
need to say oh that was an original key
point yeah I into translated yeah how
did we do that I'll have to ask my
students home I just because we're
copying the entire objects and of course
it doesn't make any sense if you copy
the objects and now they're still
pointing right there they should be
pointing I just this is a very big
problem and a lot of transactional
memory implementations and typically
people use hash tables around every load
or around every story yeah so I think
thing is down tremendously yeah yeah so
I think they did track what malik's and
this kind of stuff you know to see what
was allocated and then they make a copy
and but I have to make sure how they did
the pointer yeah I think that's one of
the one of the bigger problem saying
yeah whenever you make copies your
monitors all get mr. yeah why the
slowdown too snug big enough oh yeah
that's who you know this is almost no
slow down thank you much being slow yeah
yes maybe the teacher for example I know
my work we cheated and assume hardware
supports you can assume a lot of things
yeah yeah a Latino
ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>