<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Engineering Data Analysis (with R and ggplot2) | Coder Coacher - Coaching Coders</title><meta content="Engineering Data Analysis (with R and ggplot2) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Engineering Data Analysis (with R and ggplot2)</b></h2><h5 class="post__date">2011-06-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TaxJwC_MP9Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning welcome everybody we're
happy to have with us today
they wickham from Rice University to
talk to us about engineering data
analysis without that I'll just handed
over the ring have it thanks right so
today I kind of want to talk about one
of my sort of Carnage's which is Darren
Ellis so what do we do when we kind of
get data in and then we're trying to
turn it into understanding or knowledge
or insight and I think we want to be
kind of a little bit like engineers or
scientists really do this we're not
artists we're not craftsmen we really
want to have a reproducible process
which helps us go from our raw data to
what we really want some understanding
so talk a little bit about what their
own alysus is and then hopefully the
majority of you are already convinced
that you should be using a programming
language to do down Alice sesame Excel
users here or SAS users or anyone
willing to admit that so we're talking
about why you should be using a
programming language a little bit about
why you should be using are specifically
and then I want to talk about fewer few
of the things that I've worked on
specifically in our system
domain-specific languages for
visualization and transformation and
then I want to show those off on a
little case study of an interesting data
set that I've been working with lately
looking at Mick mortality in Mexico okay
so to me the process so data comes in
one side on the other side we get inside
understanding and knowledge and that to
me kind of I'm a I identify myself as a
statistician or a data analyst and to me
that's what we try to do so all of our
tools need to be towards that aim we
want to get raw data in and we want to
get understanding out so whatever we do
with working tools we want to we want to
keep thinking about that so I'm sort of
struggling to understand or come up with
a process which describes what we do in
data analysis this is my first attempt
so if you're going to do a data analysis
well the first thing you're going to do
is get some data so the first step is to
access the data so sometimes at eat
that's easy you know just pulling it out
of a big database sometimes particularly
if you're a scientist that's really hard
you're going to design some experiments
collect some data but you can't do data
analysis with
data then what you going to do is I'm
trying to understand that data and I
kind of think about that as an iterative
cycle of three sets of tools so you're
going to transform or rearrange your
data you're going to visualize it and
you're going to model it so each of
these tools have each of these
components of particular strengths and
weaknesses right visualization is great
for revealing the unexpected but it
doesn't scale very well you've got to
look at everything so if you've got a
hundred thousand variables then looking
at every scatterplot just isn't going to
work so that doing models comes in
models kind of give us a nice callable
approach to data analysis they allow you
to basically pass off a lot of the hard
work out of your head to a computer
computers are cheap we could have lots
of them we can do lots of stuff with
models but the problem is that a fairly
fundamental level models won't tell you
what you won't expect so unless you've
got you can your models because they are
so specific because they can scale so
rapidly typically they're not going to
show you something that you just had no
idea could be possible okay
so you know cycle through those things
your visualizations are going to help
you understand what you didn't expect
you'll capture that with models look
through that again typically as you
remove the bigger structure you'll find
finer a smaller and smaller details and
gets of lis-listen it's interesting
so some point you decide you've stopped
you've done the data analysis and then
the next step is to communicate your
results to others so you can kind of
think of this understanding processes as
building up an internal mental model of
what's going on the data and then the
next step is to help others gain that
same mental model but with helpfully not
quite as much heartache and harder if it
as it took you to get and then typically
the cycle is going to lead around once
you've communicated you resolves to
someone else they're going to want you
to do some more so you're going to have
to click some data and new data or we
can think about it similarly you know
you start with a question you get the
answer and then those answers are going
to lead to more questions so if you want
to try and do data analysis why do you
want to do it in the context of a
programming
language in my mind instead of four
major advantages so the first advantage
is reproducibility so particularly if
we're being scientists we want to ensure
that everything we do can be reproduced
in the future we I really like this idea
of provenance from art when you get a
visualization or result of analysis you
really want to be able to track that
back to exactly follow every step
backwards from the raw data so you can
say how do they go from the raw data to
this result and that that tends to be
really easy in a programming language
right because all you have to do is
follow the code that transforms the data
to your final visualization the flipside
of reproducibility is automations the
reproducibility is making sure you can
recreate the past and the present and
automation is preparing for the future
in the present so typically you're not
just going to get one static copy of the
data the data is going to change you're
going to need to redo your analysis make
sure you can update your conclusions as
new data comes in so for you guys that's
probably you know your data you collect
constantly collecting data even if
you're working in situations we think
you know there's a one experiment people
collect the data and as your job as a
statistician to help them analyze it
it's been really been my experience you
know you meet with them you get the data
and you say is this your final data and
they say yes and then a week later they
say oh hi I just discovered this other
log book on the bottle the bottom of the
filing cabinet there's actually a whole
lot of other data that I forgot to give
you here's a new version and then you do
a little bit of looking at the day and
discover this some really weird things
going on and you talk to them about and
they say oh yeah that's right I forgot
about that I'm going to give you yet
another version so if in the course of
an analysis you only need you know get
three or four versions of a data set I
think you're doing pretty well so no
trying to automate these kind of tasks
and a GUI like Excel it's just it's just
a nightmare you know you really when you
get a new data set you need to go to
rerun your analysis very very easy so
another kind of often overlooked
advantage I think of a programming
language is that it's just text so this
is some real big advantages because
we've got a lot of
great tools for dealing with tips so you
know if we want to see how our analysis
is changing over time we can use source
code control if we want to store our
code for the really long term we can
print it out if we want to reuse our
code in some way we can just copy and
paste it right and it's sort of and it
leads to another really important aspect
I think of code which is also really
supports communication so if you've got
a problem with code you can easily copy
and paste that chunk of code you can
make a little reproducible example and
you can stick it in an email if you
compare that to trying to diagnose
what's gone wrong in an Excel
spreadsheet your process in an Excel
spreadsheet that's really really
difficult and a GUI it's pretty much
impossible to capture what you're doing
in any other way except by making a
video and you know that's painful to do
your server takes a lot more setup than
just copying and pasting some code and
it's also much harder for people on the
other end to verify what's going so one
thing that kind of surprised me I used
to teach a data analysis class we had
components in both Excel and R and
surprisingly the students actually
preferred R because they could see
absolutely everything I was doing you
know in Excel it's very difficult when
you're showing someone something on a
big screen it's very difficult to
distinguish you know am I left clicking
or right clicking and just the tiniest
difference like you know pixel to
difference can make a big difference in
what action actually goes on so another
real big benefit of using a programming
languages that is so easy to communicate
what you've done to us
the disadvantage of a programming
language of course is that there's a
pretty big learning curve associated
with it so pretty much everything I'm
going to talk about today I'm not I'm
not aiming at kind of casual users of
data analysis you know if you're just
doing a data analysis once a month or a
couple of times a year none of what I'm
going to talk about really applies to
you right what I'm interested the people
I'm interested in what people are doing
data analysis everyday but it's really
worthwhile to make this investment to
understand deeply a programming language
so you do get these advantages of
reproducibility
the automation the fact it's just text
and that it's easy to communicate so if
you've decided you're going to use a
programming language or probably you're
already convinced before hopefully
already convinced before I told you but
these are the sort of the reasons that I
think are the most important well why
should you bother learning art so it's
open source of course I think that's a
pretty good start these days for any
kind of data analysis platform again
connected to this idea of
reproducibility that anyone can
reproduce it they don't need to spend
ten thousand dollars or fifty thousand
dollars buying the software just so they
can rerun your analysis connected to
that R has a really strong community so
even if R doesn't have what you need now
maybe in a few years at will there's a
fantastic variety of add-on packages I
keep losing count baiting is over 3,000
add-on packages per hour which give you
pretty much anything you can imagine in
terms of statistics or machine learning
the downside of the community is
sometimes it can be a little bit prickly
our help the mailing lists for our is a
fantastic resource but if you ask a
stupid question you can pretty much
guarantee that someone is going to tell
you that you're an idiot either politely
or not so politely
so in fact I've been told on our help
twice that I need to look up words in
the dictionary both times I was correct
but another big advantage of our unit
runs anywhere Windows Linux Mac people
have run it on their Playstations and
the Xbox's and so on pretty handy in
this idea you know because it is
open-source because you can add on to it
it's really build it yourself so even if
R doesn't have what you want it's a real
programming language you can build on
whatever you need and again you know so
many other people have built on other
packages that even if it isn't there in
base R and while the chances are it
isn't in base R you can download a
package so sort of an interesting
corollary of this is it's almost a
disadvantage so at the moment I'm
working with a student reviewing density
estimation so there's about 20
five different packages and our to do
some kind of density estimation so now I
was getting to the point where if you
want to try some statistical methodology
it's not does I have it but which of the
ten different up to up ten different
options should you use it's on the
downside R has a reputation being slow
and I think compared to a lot of other
languages it is it is slow but then
typically we're not really interested in
absolutes is it is a programming
language slow or fast what we really
care about is it fast enough for what we
need and the big advantage of R is that
it really is fantastic connecting to
other programming languages so why in
some sense the our language itself is
pretty slow R is not because so much of
the high-performance computing stuff and
base R is written in C and Fortran and
so this is a really big advantage of of
our it's trivial to connect to C or
Fortran it's pretty easy to connect to
Java people have written connections to
Ruby and Python and oh camel and all
sorts of other things and you can read
and data from just about any format
imaginable connect to all sorts of
databases the other kind of the last
downside I want to talk about for our is
that to me it's still sort of lacking
some programming infrastructure so the
focus of our really has been on
statistics and data analysis and and
that's what people who contribute to the
our community tend to be good at by and
large that people who want to understand
that data they're not programmers and
and this still isn't a sort of a culture
like and many other languages of of how
do you do things the best possible way
how do we develop programming best
practices and I've kind of found this
fairly frustrating myself that I've sort
of had to build a whole lot of
infrastructure myself you know there's
uni very good unit testing packages in
our there's no good packages for
understanding your profiling data
there's not some of the documentation
stuff has only come to the scene
relatively early so one of the things
that I've kind of spent a lot of my time
doing not so much because I'm interested
as I want to be able to develop good our
code is develop a lot of this
infrastructure so for the course I'm
teaching later in the week is one
example we help kind of people become
our programmers and I'm really hopeful
as people as more people who have you
know more programmers more people
experience in other programming
languages as they come to our they don't
say oh hey you know our is missing
feature exit sucks what I want them to
say is hey are as much missing feature X
this is an opportunity where I can
really contribute to the community and
make a big difference so I think there's
still a huge number of possibilities for
that sort of contribution or not okay so
those are the kind of the pluses and
minuses for our I mean I haven't really
compared it specifically to any other
languages but I think at the moment at
least the sort of well what well so it's
a little bit hard to talk about Big Data
so it is as the sort of statistician and
me thinks you know fifty thousand
observations is a big data set but the
computer scientist in me rejects that as
being big so so our I think is in a
sweet spot for analyzing data up to
about a million observations and I think
even if your data is bigger than that
the limitations aren't really on are so
much once you get data once your data
gets bigger but this is the cognitive
limitations I think it's possible to
work with about a million observations
interactively kind of understand what's
going on there and then in some way
scale up so think about how you're going
to scale up your models or all sorts you
never even though then those million
data points you know might represent ten
billion original observations that have
been aggregated in some way the trick is
figuring out how do you do that
aggregation and how do you make sure
that when the aggregation is no longer
appropriate that you still detect that
problem but what I'm going to talk about
next is sort of within our how can we
develop domain-specific languages for
specific tasks of data analysis so so so
some people describe as a DSL for status
which i think is is wrong our isn't a
domain-specific language it is a
general-purpose programming language I
mean it has traditionally been applied
by and large in the fields of statistics
and data analysis but but there's
nothing really built into the the
language at a fundamental level that
implies that so top of are we really new
do need to develop these DES ELLs that
help us help us kind of automate and
understand and think about the common
problem that we have when during during
a data analysis and to provide a little
bit more motivation for why we need to
do this here's senders remorse
so if any number of magnitudes are each
the same multiple of the same number of
other magnitudes then the sum is that
multiple of the sum so can anyone tell
me what kind of simple mathematical
principle algebraic principle he is
describing yeah I think yeah you're
pretty close but slightly simpler than
that so it's just this idea of that
addition multiplication distributes over
addition right so here what he's been
doing is trying to express this idea
while not in English but in Greek
obviously it's pretty clumsy right so we
really have a domain-specific language
which is math basically right so it
makes it's much much easier I think to
reason and to understand what's going on
with this domain-specific language than
it is when we're just trying to use
natural languages so the same thing kind
of happens when we're doing a data
analysis what we want to do is make sure
we've got domain-specific languages for
each of these three components that's
going to make it easier easier for us to
think about the problems so you know
there's a component of any problem is
how do you actually represent on the
computer but there's a big program a big
problem first of all just how do you
understand what you're trying to do and
I think domain-specific languages are
really helpful there because they sort
of they sort of help you understand well
one of the key components of this type
of problem how can I break it out
I take these general ideas and apply
them to my specific problem so I want to
talk about each of these just briefly
and I'll show them off in a case study a
little bit more detail so the modelling
I'm kind of domain-specific language in
our is this is a formula interface how
many of how many of you guys have seen
this before
okay so basically we're saying you know
in this one we want to predict Y based
on X or we want to predict Y based on
the variables X 1 and X 2 or we instead
of using addition we can use
multiplication which means we don't want
X 1 and X 2 you kind of act
independently we also want to have their
interaction included in there so this
formula interface this domain-specific
language for describing how you expect
responses are connected to predictors
can be extended in various ways and here
we're kind of thinking about instead of
using X directly maybe a smooth function
of X or maybe we want to predict two y's
from two x's and there's also sort of
illustrates what I mean by that a model
is never going to tell you something you
don't expect it's some little right if
the this model is never going to tell
you hey you know you've forgotten to add
x2 that's actually a really important
predictor this model is never going to
tell you a x1 and x2 aren't actually
linear they've got some other smooth
forms so use the models really constrain
what what they're going to tell you
about the data so that that's both a
real strength and a weakness you have to
be aware of that you have to know that
the models not going to tell you what I
can tell you about the data is
fundamentally constrained so we're going
to talk a little bit more later about
the domain-specific language I developed
a visualization ggplot2 but the basic
idea is of visualization is a mapping
between data set and things that we can
perceive so the basic idea of ggplot we
give it a data frame we say how we want
to connect
a year standard short for aesthetics how
we want to map the data to aesthetics
things that we can perceive so here
we're saying we want the exposition to
be given by variable Y and the Y
position by variable 2 and then the
color by variable 3 and then we need to
specify how we're going to represent
each observation so in this case we're
going to use points to create a
scatterplot and then we're going to add
on a layer which is a smooth curve so
not only do we want to plot raw data
typically we often want to put lot some
statistical transformation of the data
we want to plot some model that predicts
Y based on the X mm a smooth form of X
and see what's going on so talk about
this in much more detail shortly
transforming so when we think about
transform and dad I'm not just thinking
about the kind of the classical
statistical transformation where we
might you know log one variable or
square root one or arc sine square root
another one I'm thinking about is
generally how do we kind of aggregate
our data how do we rearrange it how do
we reshape it to get into the form
that's most suitable for analysis so
typically this you need to do the step
because when you get your data it's not
in the right form for you to feed to
your modeling and visualization tools
and I mean my my experience working with
data from all sorts of people is just
absolutely astounding how creative
people are with the way that they store
their data and a huge part of any real
data analysis is just getting the data
out of you know whatever crazy format
they've stored it into something
standardized something useful that that
you can work with and I have a few
examples of that coming up but so I
think there's kind of four key verbs
associated with transforming your data
so sub sitting or filtering mutating or
transforming where you're adding on new
columns that are functions of the
existing columns arranging or reordering
so typically you know just changing the
order of the rows doesn't make much
difference to the modeling but it's
pretty useful when you're looking at
day to yourself and then finally
summarizing where you're condensing
multiple values down into a smaller
number of values so those are all useful
by themselves but really they need to be
combined with some kind of group by
operator so I want to say summarized by
treatment group or our transform by
patients so we need some kind of
combination of these basic verbs with a
group I operator and then we need a few
other things when we're working with
multiple data sets we need some way of
combining them together like an SQL type
join or maybe sometimes we just need to
match them up so we want observations
and data frame a that and some way are
included in data frame P so we'll see
some more examples of these sure okay so
what I wanted to finish with to kind of
make this try and make this as concrete
as possible to show how you know I've
used all of these tools in a data
analysis and I've just been exploring
that's interesting dare i've got so it's
every single death that occurred in
mexico in 2008 and it's at the
individual death level so i've been
interested in data like this for a long
time there's all sorts of interesting
relationships between like temperature
and deaths from heart disease and so on
but unfortunately in the u.s. you can't
get any data on this because americans
are very concerned with data privacy and
seems so far that Mexican government
statisticians are not so concerned so if
you want to get this data for the US you
can fly to the CDC and you can work on
it a near computer lab and you're not
allowed to bring anything out of that
that isn't approve by them and Mexico
they just put it all on the web so it's
a little bit easier to analyze there's a
whole lot of variables in this data see
I'm going to look at three so Co D the
cause of death hid the hour of death DoD
the date of death and in the location
where they died and what I'm going to
try and focus on this analysis is look
at how do those DEA cells help us
understand this data so start by looking
at cause of death so here's a fairly
simple plot so on the x-axis
I've got the number of deaths on a log
scale and the y-axis I've got the cause
of death so just a very very high level
summary one thing that I think is kind
of interesting about this data is the
11th highest cause of death so when I
saw this as I was actually
other those pretty surprised write to me
that seems much much higher than I would
expect homicide to be in this ranking
but does anyone know what the 11th
highest cause of death in the u.s. is
suicide
so I'm not entirely sure what's better
so Mexico people are killing other
people in America the killing themselves
I guess so
neither of that's particularly appealing
but let's see how we got to this plot so
first of all so this is some our code so
how many of you have seen our code
before okay so even if you haven't you
know similar laughs to any kind of algol
derivative I guess that you should be
able to see what's going on and I'll
kind of point out the important
functions important things that we're
doing so I'm going to start by loading
ggplot to imply packages so ggplot2 the
DSL for visualization and ply out the
DSL for data transformation so one note
going back to the pro cleanness of our
help so you use the library function to
use our packages and should you ever
refer to an our packages in our library
and I help someone will very indignant
ly tell you that you're a moron and
you're using the wrong word somewhat
ignoring the natural confusion here
anyway we load those packages
load up the data which I've just stored
as a binary our data format
just because loading and a big CSV file
this is a little bit low quicker than
loading in the CSV file and then I'm
going to do some analysis I'm going to
start by counting the deaths broken down
by cause of death so this is going to
give me a new and new column a new table
one column I've got the cause of death
and the other column I've got the number
of deaths the frequency of deaths and
then I'm going to arrange that in order
of descending frequency just so I've got
a nice
when I look at it I see it nicely
ordered from the most common to the
least common diseases then I'm going to
join on another data frame which I
haven't talked about yet but in this one
so Co D is an icd-9 abbreviation it's
like K 17 which means they died of liver
cancer you know it's not very useful
when we were looking at the data to have
these opaque codes I'm just going to add
on basically a lookup table that gives
us a nice human readable description of
the disease next I'm going to plot it so
the idea of GG plot I'm setting up the
relationship between stuff and my data
and what I want to see so I'm going to
use the COS dataset which are just
created on the x-axis right I'm putting
the frequency on the y-axis the disease
I'm going to display the data with a
point so each observation will be
represented by a point and then I'm
going to modify the x-axis scale so
instead of a linear scale I'm gonna have
a log 10 scale so that so what I'm going
to do in this presentation it's going to
focus on the essence of the plot so if
you actually ran this code you get a
plot that was pretty similar but not
quite what I showed you so they actually
do the plot that I showed you there's a
little bit more work you know I just
actually looked at the top 20 causes and
I showed you the deaths divided by
10,000 and I'd reordered the disease's
by the frequency and then a few other
little bits and pieces so and what I
want to show you here is the essence of
what's going on with this DSL whenever
you're presenting data to anyone else
there's always a whole lot of fiddly
little tweaking you need to do to make
it understandable so then this is a it's
really useful to think about when you're
doing graphics this or the difference
between an exploratory graphic which is
just for you you understand what the
variables are you don't need fantastic
labels you're probably going to create
hundreds of thousands of these most of
them going to end up in the trash the
complement of that is expository
graphics when you're trying to explain
what's going on to someone else you want
to make sure your labels are correct
that you
got nice legends and so on so I'm going
to kind of show you the exploratory code
but the expository graphic so I just
kind of tweaked them to make it a little
bit easier for you guys to see what's
going on okay so that was cause of death
I mean again if we go back to this you
know nothing amazing most people are
dying of heart attack and and diabetes
and pulmonary disease and liver disease
and so on so what I want to look at I
wanted to look at next and more details
will win will people dying and
specifically could we find diseases
where people seem to have a different
time course of death so I'm going to
start with a very similar plot as before
I'm just going to show you the overall
what's going on by hour of day so here
we've got a line plot on the x axis the
hour from 0 to 24 and then again on the
y axis the frequency so the number of
deaths in each hour so how do we create
this well so first of all what can you
see here when do people die by and large
seems to be by and large during the day
right there may be fewer deaths at
nighttime in their eye during the day
you know you might wonder if this might
be some kind of reporting bias that
people only discover the bodies in the
morning maybe that's this 6 a.m. spike
when people wake up and discover person
excellence did okay so how do we create
that well first of all we need to do the
little manipulation of the data this is
pretty common well when I first
discovered hey there was an hour of
death which was 99 you know I know there
aren't 99 hours in the day so I figured
out but this is probably actually a
symbol for a missing value so what I'm
going to do is replace this with our
missing value na which stands for not
applicable so one of the things that's
really nice about our maybe one of the
things that does sort of make it a
little bit more on the statistic DSL
kind of languages is it has the support
for missing value so when we don't know
what the value is our is going to
propagate that in any computation so if
you have a number you don't know what it
is
you add on for you know you still don't
know what it is and it supports this at
a very very low level so I'm gonna
replace the 99th with a missing value so
if I take an average it's not going to
be skewed and so on and then the other
thing I'm going to do is I'm also going
to you know there's only one minute at
2400 hours that's 2400 so for the
purposes of this I'm just throwing that
out you know it's a little bit lazy but
it's quick and dirty so they're going to
do the same thing again I'm going to
count up the deaths this time by hour of
day then I'm going to subset it to
remove all the missings I'm not
interested in those and then again I'm
going to plot it in a very similar way
here's my data set hid on the x-axis I
want the hid variable and on the y-axis
I want the frequency variable and this
time I'm going to represent it with a
line so giome here it's ggplot2
abbreviation for geometric object that
kind of determines like the type of plot
as a points or lines or bars or polygons
or so on so if I put G on point here I
get the same thing but I just have
points instead of lines okay so then
what I wanted to do is figure out which
of the diseases which diseases don't
follow that overall pattern so I'm going
to show you here of a kind of eight
common causes of death that don't seem
to follow that same pattern we'll look
at how I managed to find these which
shows a nice kind of cycle of a little
bit of modeling a little bit of
visualization a little bit of
transforming and then we'll look at some
other ones so what sort of causes of
deaths do we have here
any seemed to break down into a couple
different groups kind of looks like
we've got murders right so murders are
interesting right you're more likely to
get murdered at night during the day got
a couple of drowning ones right people
drown in the afternoon
this one's sort of interesting this
exposure to electrocution basically
people go electrocuted during the day so
it's reasonable right it's hard to
electrocute yourself while you're asleep
and then we've got these basically
traffic accidents which you can see kind
of peak maybe it is some suggestion that
they peaked during rush hours so how do
they find those we've got I forget but
there's about it was about ten thousand
causes of death right so there's no way
I can go through and look at ten
thousand causes of death and pick out
the plots that are interesting what I
need to do is come up with some way of
modeling what do I mean by interesting
applying that to all the causes of death
and then picking out the interesting
ones there's quite a lot of code here
I'm not going to explain it all I'm just
going to kind of talk about the general
idea but basically what I'm going to do
is come up with a distance metric
between the overall time course and the
time course for each individual day so
I'm going to do that by first of all
counting up the deaths broken down by
both cause and by hour and here I'm
going to use DD play which is my kind of
by operator so this is saying take this
data frame of frequencies of counts
break it down by cause of death within
each cause of death I'm going to mutate
it or transform it to add a new column
which is the proportion so the
proportion within a disease is just the
frequency divided by the total frequency
right I want to try and get rid of that
that denominator which is how many
people died from that disease now I need
to do the same thing for the overall
write to get the frequency of the
percentage of people dying in each hour
over all diseases
and then I join them back together so in
one data frame I've got both the person
the proportion dying at each hour for
each cause and the kind of corresponding
frequency overall diseases so then what
I'm going to do is again break it down
by cause of death this time I'm going to
summarize it some summarizing each cause
of death with two numbers so the total
number of people who died of that
disease and then this distance which I'm
basically looking at the average squared
distance between the frequency for that
disease at that hour and the overall
frequency so just kind of a quick way of
picking out ones that are far away
should have high values ones that are
close to the overall rate should have
small values so one thing I need to be
aware of here right is this basic
statistical idea that if you don't have
much data your estimates are going to be
more variable so I need when I'm looking
at this distance of this deviation I
need to be aware of how many points were
used to calculate it so that's why I'm
computing the N and then I also decided
hey if I've got any chance of picking up
departures from the overall trend I
probably need at least two deaths per
hour right at a very at the minimum
right I've only got 48 people dying of a
disease in the day my ability to pick up
departures from the overall trend is
going to be very very very small so what
I did next is this plot where so each
point represents a disease on the x-axis
the number of people with that disease
and the y-axis with the distance so what
do you notice here
Yeah right we've got a huge amount of
very a small much smaller variation
wieners big so once one way I kind of
drive this home when I'm teaching
classes I do a plot like if you look at
all of the basketball players in the NBA
last year you plot the number of baskets
they shot and their act the proportion
that they made right so it's pretty easy
to think hey if I'm going to pick
players my NBA team I'm going to play
pick the people who have the highest
accuracy the problem with that is though
that you end up picking like 15 people
100 percent accuracy but they're 100
percent accurate only because they made
one shot and they succeeded in it so
whenever you're looking at any kind of
statistical summary you always need to
be aware of you know what what's how
much variation there is so you know I
could do some kind of nice mathematical
statistics and figure out what the
theoretical properties are I'm just
doing something very crude I'm also
going to I'm going to mix them in a log
transform it so exactly the same plot
but now on a double log scale and
basically what I'm interested in these
guys right and this upper triangle so
these for a given in they have much
higher values than average so my kind of
expectation is you know that most
diseases are going to follow that
basically the same pattern there are
going to be a few that are unusual I
want to pick out those unusual ones so
this is a pretty kind of crude technique
but basically I'm going to chop off all
of these guys here and examine them in
more detail so to do that basically you
know it's hard to select a triangular
region it's a kind of easier to subtract
that line off I'm using a robust linear
model I don't want my straight line
affected by these guys up here so I'm
using a robust robust model but
shouldn't be so affected by them and
then I'm just going to pick off the ones
that have a fairly large residuals of
more than some distance away from that
line you know I just kind of picked that
with a little bit of trial and error and
then divided that up so then I'm going
to pick out all of the diseases which
are unusual by my
characteristics or residual bigger than
0.5 and then I'm going to use basically
select go back to the original time
courses and then create those plots so I
split them up again sort of just a
little bit of trial and error and the
ones add more than 350 deers and ones
that had less than 300 pts basically
because the scales are so different they
didn't work well online so these are the
five diseases that less than 350 deer so
it's pretty small right that's you know
ten twelve deaths an hour but I think I
am picking up something legitimate here
right so we see well bus crashes
lightning drowning again aircraft
crashes and and SIDS so even though they
seem to be pretty small these are these
guys up here right even though I've got
a small n I've got a much bigger
deviation from the overall pattern that
I might otherwise expect okay so what I
conclude with my kind of main challenge
from this which is which is give driven
by this plot so on the x-axis I've got
theme basically I've done the same thing
we've done again again and again I'm
just showing you for each day how many
people died on that day so we can see
lots fewer people are dying and June and
July more a dying and in December and
January
so what's probably causing this probably
weather right you might expect it's
colder during the winter and more people
die when it's comped so this would have
led me on a a mini Odyssey to try and
match up this death data to temperature
data so the first problem was to match
up this deaths data to temperature data
I needed to figure out where each person
died so there's a location code in the
field the the statistical department
also supplies a database that gives the
latitude and longitude for each of those
location codes so when I did that this
is just for each unique location I'm
showing how many people died in that
location so you see any problems with
this plot
so we've got a few locations off the
coast right so maybe you know it seems a
little unlikely that a whole lot of it
you know like maybe five thousand people
capsized in a boat and all drowned here
right we also see some sort of odd
striations right so suggesting that
there's something wrong with this
location database so what this you know
one of the ongoing challenges with data
analysis is matching up your data with
data that helps you make sense of it
with contextual data and in this case my
context the location of each death was
wrong but luckily there was a different
data set supplied by a different
government agency and when I use that to
capture that data choosing longitudes I
got this plot which is a little bit
easier to see if I just restricted the
locations which have more than 100
deaths so how do I create this part this
part interesting because it's got
multiple layers of data on it so again
I'm going to start off again by counting
I just want the number of deaths and
each location each combination of
latitude and longitude then I'm going to
have so here I've got two layers on my
plot so my first layer is a layer of
polygons I'm using it from a state's
database which basically has the
outlines of all of the states of Mexico
on it then I'm going to add on a layer
of points which comes from this
locations data and I'm going to map the
size of each point to the number of
deaths there need to do one little tweak
to that size we know that when we look
at dots we basically perceive the area
rather than their radius so we need to
adjust the scale that's used to map
values to radius I need to do a square
root transformation which is what scale
area does and then finally this is map
data you know points on a sphere it
isn't like the usual Cartesian
coordinate system we want to use a
special coordinate system which uses map
projection so it doesn't really do much
here in terms of so the fact that we've
got data over a sphere doesn't matter so
much because we're in a fairly small
region the Grob just kind of flattening
it out doesn't distort
too much but using that coordinate
system doesn't sure that we get the
right aspect ratio so this does look
like a map of Mexico doesn't look
squashed or expanded in any way okay so
the next thing I needed so now I've
managed to locate the deaths next I
needed to locate our weather station so
I'm going to truncate the largest part
of this which is managed to find this
data source that looked really promising
the global historical climate network
which has about 40 weather stations in
Mexico unfortunately is the most bizarre
data format you've ever seen so it took
me a while to pass that and figure out
how to work with them combined all the
weather station down into one file but
we finally got there and so this plot
shows the locations of all the deaths
there were in 50 within 50 kilometers of
a weather station so I guess I should
say like 30 miles but since we're in
Mexico and I'm from New Zealand will go
with kilometers again just sort of a you
know a heuristic right that probably you
know I'm sort of sweeping a whole of
stuff under the cover right I'm just
kind of saying well I think the forecast
from weather station is going to be
fairly reasonable for out 50 kilometers
around that you know there's all sorts
of arguments you could apply to that but
this is a good first start and then if
we they're just showing the number of
location the number of deaths by each of
those weather stations we can see
there's one really big one which is
pretty close to Mexico City but not
actually in it so Mexico Studio thing
accounts for about 20 percent of the
deaths it was about a hundred thousand
deaths here all up so I thought well
let's just start for a single location
just start with Mexico City that's got a
good number of deaths you know match
that up to the weather in Mexico City so
here's a plot of the climate data that
either Boreas Lee passed and collected
showing the minimum and maximum
temperature so temperature on the X X or
the Y axis the maximum and blew the
minimum of red and the date on the
x-axis
so can anyone anything interesting about
this plot yeah there's some missing data
and in fact 87% of the data is missing
so I just spent two days trying to pass
this awful data format I need to
discover out that when I finally get to
it any per se D 7% of the measurements
are missing so this was kind of the
cause for great wailing and gnashing of
teeth but luckily I managed to find
another dataset collected by you another
government agency just for me just for
Mexico study I'll kind of skip the
details of that yet again more cleaning
more messing around with the data you
know so much of a data analysis is doing
this just getting the day on the right
format so you can actually use it but
then I did get some interesting data so
each of these points now represents a
day of the year on the x-axis I've got
the minimum temperature and on the
y-axis I've got the number of deaths on
that day so you can see it looks like
this some I think we can kind of say
least along here that as it gets colder
more people seem to die and we get a
little maybe there's a little hint that
also when it gets really hot more people
start to die we can also look at the
maximum temperature so we don't see this
relationship so strongly down here kind
of suggesting that it's the minimum
temperature of the day it's cold it's
killing people not hot
unless it gets really really hot no kind
of looking at this again made me doubt
this data because so the maximum
temperature in 2008 was 30 degrees
Celsius which is like 90 Fahrenheit or
something which seems like it's to kind
of would think Mexico City would be
warmer than that given that Houston's
getting up to about 40 degrees from time
to time so I kind of you know at every
point of the data analysis you kind of
think hey what's gone wrong with my
context is they're really going to ruin
my ruin my results but the one plot that
really was interesting to me is this
plot so here we've got wind speed on the
x-axis and frequency of deaths on the
y-axis so I think this is
pretty interesting so it seems to be
showing as it gets windier fewer people
died so this is a little bit puzzling to
me but then someone mentioned hey Mexico
City has a lot of pollution so it may be
that on windy days the pollutions
getting blown away on still days it's
kind of gathering and more people are
dying from that so again you know this
kind of raises more questions and
answers I also looked a little bit to
see if I could pick out diseases that
seem to have particularly strong
relationship with the minimum
temperature none of these seem
particularly compelling except maybe
pneumonia which well we kind of have
some no prior belief that pneumonia may
get worse as it gets colder but
certainly it's hard to imagine why non
insulin-dependent diabetes would have a
relationship with whether something
directly and you know a lot of these
kind of optics look like it's just one
or two points that are making us think
that that that curve is going upwards so
it doesn't seem to be a lot of doesn't
seem to be much strength to kind of say
individual diseases are particularly
affected by the weather maybe if we had
more data maybe if I managed to find
more weather stations across all of
Mexico
I get more deaths and I'd have more
power they're going to pick up those
changes okay sort of conclude by showing
the plots for that the code I use for
that plot so again we've got this daily
data set daily data Scott for each cause
of death the temperature and the
proportion of people who died on that
day so temperature on the x-axis
proportion on the y-axis then I'm going
to use points to display that data I'm
going to specify alpha I'm going to make
the points transparent so to deal a
little bit with this over plotting
problems I have some idea where them a
majority of the points lie and then I'm
on top of that I'm going to overlay a
smooth so just these smooth lines which
I've said this time I don't want any
standard errors and I want to make it
thick I want the size to be big so I can
see it and then there's finally
one faceting another really important
component of the grammar so typically of
ggplot2 so typically we want to be able
to recreate this each plot for multiple
substitute to the data that's what
faceting does it says I want this plot
this plot that has points on a smooth
line for each disease and the facet the
wrapping basically means here I've got a
1d ribbon I'm going to wrap it around
into two dimensions
there's also facet grid where you think
about I've got want variables in the
columns and other another variable okay
so that sort of hopefully that gave you
a little bit of a flavor of how these
domain-specific languages can help kind
of Express common data analysis tasks so
if course the big problem with this is
that basically these domain-specific
languages are designed by me and the
design on the basis of the types of data
analysis that I do so one thing I'm
really interested in is trying to figure
out you know am I basically an outlier
or am i doing something common so what
are the kind of common tasks of data
analysis how can we figure out what they
are then how can we divide figure out
tools both kind of computational and and
cognitive how can we think about these
problems better
it's very more efficient and to do that
sort of I hope to you know find out more
about what other people are doing so
there's sort of some indication that's
not just me because you know people do
download my packages and they do use
them but it's kind of you know it's hard
to know what what are what really are
the common problems and data analysis
what what how can we develop tools to
best solve them so kind of overall
hopefully well I think you're probably
already convinced about why you should
use programming languages but
particularly for data analysis it's
reproducibility automation communication
on the downside it's going to learning
curve so ours pretty great it's a bit
slow the community can be a bit prickly
so lacking a little bit of
infrastructure but these are all things
that I think can be fixed there and
slowly in the process
of being fixed so one thing in the
latest version of our has a bytecode
compiler which yields you know nothing
amazing but for certain types of
constructs kind of two to four times
speed-up so things are slowly getting
better and then finally this of what I'm
most interested in is how can we help me
figure out what what the fundamental
problems of data analysis are how can we
devise tools domain-specific languages
to make those as easy to solve as
possible that concludes my talk I just
wanted to quickly say that I'm hanging
around Google until my meetup talk this
evening they all gave me that which is a
location I believe so I'm going to have
office hours so if you want to you know
stop by and talk to me I'll be there
three to four pm I'm really interested
in hearing kind of you know you're you
know both success and failure stories
for our what are you trying to do when
you do data analysis or you know at any
point just feel free to send me an email
I'm happy to chat about anything related
to our so thank you
yeah yeah be happy to talk about been
doing some other work recently sort of
thinking about tidy data or what you
know what do you what format you
actually want for your data when you're
doing down ah so it's a really important
challenge yeah so I'm I'm sort of still
you know kind of personally trying to
scale up the size of data that I work
with so I mean I try really hard in
class at least I've moved beyond like
these hundred observation datasets and I
think it's pretty I can get a class of
undergrads on their own computers
working with 50 to 100 thousand
observations and that's easy enough to
do in class a million I think is
achievable kind of there's all sorts of
problems with my tools in terms of like
ggplot to being god-awful this slow
which is one thing I'll talk about this
evening
so part of it I think is just as like a
programming challenge right my job I'm
not a software developer my job is a
assistant professor is to produce
research and new things not make old
things work pretty well so this sort of
a tension in my life between how do I do
kind of you know how do I keep doing new
things while making sure the things that
people actually need to do which we've
known about for years and years and
years possible so so I think just to
assume in some ways investment on the
programming side there's nothing
anything fundamentally wrong with our
which means it couldn't scale up but I
do think we do kind of need different
tools at four different scales of data
that we need I just don't see like this
kind of interactive exploration I'm not
sure how well that scales up to like 10
billion observations when no matter how
much computing power you have it's still
going to take you know a couple of
minutes to get a result back and how
that sort of effects this iterative
cycle of transforming and
and modeling and visualizing so I think
that's an open question you know
obviously I've sunk a lot of time into
our so I'm biased but that respect but
yeah I think it's interesting question
I'm doing a class in San Francisco on
Wednesday Thursday but it's sold out so
but I'm teaching the class I've called
on our development master class so one
day is like advanced our programming I
was developing our packages so one thing
that I'm trying to resolve personally is
this kind of complete lack of material
about how you become a good art
programmer or if you are a programmer in
another language how do you move to our
how do you understand kind of the things
I mean there are lot of things of you
know I did a kind of a traditional CES
program I learned a jar a little bit of
scheme and stuff and then I come to our
and you just look anything crazy this is
ridiculous how does it possibly work
this way so some of that is legitimate
complaints that some of there is a lot
of craziness in our and some of it is
just trying to understand that you know
our looks at the world and a very very
different way from a you know a
functional a lot more functional
perspective than object oriented and
just trying to figure out how can get
people up to speed on that writing kind
of idiomatic our code that makes use of
our strengths rather than trying to
force it to be like Java or C sharp</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>