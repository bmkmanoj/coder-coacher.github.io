<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Keeping Up With The Human Genome | Coder Coacher - Coaching Coders</title><meta content="Keeping Up With The Human Genome - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Keeping Up With The Human Genome</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ryc5bD814k8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright folks well it sounds like it's
about time to get started so
morning ladies and gentlemen we are
pleased to have with us here this
morning dr. Tim Hubbard who is the
gentleman at the Welcome Sanger trust
Institute who is responsible for the
group that what does the correct term
has tights the human genome very much
and has it was responsible for
annotating one-third of the human genome
sequence so without further ado I will
turn the mic over to dr. Hubbard who
will give us his presentation on Keeping
Up at the human genome thanks very much
so it's nice to be back at Google this
is interesting my mouse is dying I was
here for the Google food camp in august
and i didn't really talk about this
particular thing I talk more about
openness things then so I thought since
I'm in the area for the casp competition
which has just finished yesterday where
I come and drop in and talk about this
so this is about genomes genomes are
very very recent occurrences as far as
having the sequences concerned since
just 1995 when we had the first one and
then since about 2,000 we've had the
human genome which is huge and problem
is that lots of people want to look at
this data use this data integrate it so
I'm going to talk about how we do that
in ensemble which is one of the big
three browsers that provides access a
bit of background since I'm in a the US
and a company and you probably don't
know some of this background so welcome
to our Sanger Institute we sequence one
for the human genome as part of the
public international partnership that
put it out in the public domain we're a
big center by pretty much any standards
not as big as you but biggish so we have
a thousand square meter computer center
for example which we're doing rotation
farming on filling it up with nodes each
year killing off one of the nodes and
replacing it that sort of approach we're
funded by the Wellcome Trust Wellcome
Trust used to be the richest charity in
the world until the Gates Foundation
overtook it which they're kind of upset
about but anyway it's a very big charity
and we spend around fifteen percent
the there spend each year so we're doing
big science high-throughput data and all
of our data is put out into the public
domain all the large-scale projects
we're based on a campus just outside
Cambridge in the UK and we have another
Institute on that campus the European
bioinformatics Institute that's the
equivalent to ncbi so you've got Sanger
informatics and ebi informatics next
door baby for 500 implementations as
well as another for 500 experimental
scientists and the Sangha so one of the
big projects I'm involved in I'm one of
the leads on is ensemble this started up
dealing with just the human genome it's
now got around 30 genomes and it 30 big
genomes it's got references going right
down to yeast but most of what it's got
is the large tree geka based genomes so
a lot of the important stuff about
having these genomes is the sequence
relationships between them some things
are extremely distance there's a family
tree of genomes some things are very
related such as you know right at the
top you've got chimpanzee many what are
mammals then you've got more distant
things going down to other model
organisms colored in blue which have
been completely sequenced most the other
ones are in a fairly rough state now in
terms of informatics the natural way of
looking at these things is a continuous
coordinate system you know here's a
that's the carrier type of human so
you've got these 22 chromosomes and x
and y they vary in length from around
250 million letters down to around 60
million letters so that's how we'd like
to address them one to n with a
coordinate now in actual fact with the
way these things have been sequenced is
as thousands of pieces or millions of
pieces depending on the technique and so
the informatics challenges organizing
that handling it's quite a lot of the
problem is sequence transformations
being able to handle between these
different coordinate systems it's not
just within a genome it's also between
genomes that are related to each other
and also more exotic transformations
you've got the genes which are any sub
parts of those genomes now we delivered
more than just the sequence so here's a
piece of sequence this is tiny piece of
sequences less than a millionth of the
human genome but most of what we do is
is serve information on top of that so
here's one of these views it's a small
piece of sequence the boxes represent
things like jeans and other evidence for
those jeans being there may be up to 80
different types of information layered
on top and you know what's the reasons
coming here well it's not completely
unlike when I get through this under ten
percent of what's in ensembles sequence
it's not completely unlike what you do
here putting annotation on top of maps
we're putting annotation on top of the
sequence you're in a 2d coordinate
system with Google Maps we're in a 1d
coordinate system with ensemble so we
add gene annotation and for gene
annotation all i'm going to say in this
talk is that it's hard still hard we
still don't know where all the genes are
gold standard right now is its manual
curation because the automatic
algorithms produce too much uncertainty
and although we rely on experimental
information from sequencing the the
fragrance of genes that are actually
turned on in cells they're very noisy so
having humans check that is the most
reliable thing so that's one thing we do
is produce sets of genes which are used
you know pretty much around the world
this is why it's hard because in
bacteria you've got a continuous just a
jeans made up of one unit in higher
organisms it's fragmented and that makes
it much harder to identify where the
jeans are so once anymore about gene
building in this talk you want to ask me
later the other thing we do is
comparative genomics I've also I've
already said that we have our different
genomes in there and so we can calculate
the relationships between those look at
the rates of evolution between the
jeans and other parts of the genome we
can look at ad information such as the
variation within a genome so there's one
human genome but there's six billion of
you and you've all got individual
mutations within you and the population
structures of those and those can be
related to disease so it's interesting
to store that information and make it
available to other researchers and then
for all this it's you know
infrastructure in some ways we're just
organizing this data into a way wherever
people can get access to it and some of
that we do that with the website some of
its far API for an open source
environment so the API just means that I
said you'd like to see it be able to
address the thing one to n so our coding
API means you can do that though it's
made up of lots and fragments you can
address the whole thing here the middle
thing is saying fetch the whole of
chromosome 22 as a virtual fragments
which you can then do calculations on
that API is now quite flexible quite
extensive it allows you to compute
across different organisms you can
project information for one coordinate
system on one genome onto another one
you can also layer information we've
just put in a lot of data from different
mouse strains people do experiments on
different mice and so it's relevant to
be able to look at we haven't got the
complete sequence of these different
mice but we can project between them and
you can access that information either
via the web displays or programmatic
claim so conceptually behind ensemble
complete openness codes available of our
BSD we can make all the data dumps
available we code as a single common CVS
that's about 30 people there's another
maybe 20 30 people who put code in
internally and some external people it's
based on MySQL with pearl and Java api's
different layers of objects so that you
have objects or things like jeans but
how we handle that underneath can be
changed and continuous improvement so
this site chain
gets updated once every two months and
there's nearly always a schema change at
that point because we've added some new
data and we've had to extend something
or maybe completely refactor the way we
store things because the quantity of
data has changed we split up into little
sub teams guess this is not that
interesting to you but I think and we
have a process of handling that release
cycle within the organization we have a
healthy paranoia because we are you know
still we'd work with in terms of usage
we're probably the leading one in the
world of the big three ncbi that there's
a lot of things but it's browsers
arguably less competitive UC santa cruz
just up the road they have a simpler
browser engine they're very much our
direct competitors so we're worrying
about you know is what we're doing still
relevant how can we tell if it's
relevant we're interested in ways of
counter looking at usage of our API
accesses and web pages and I guess
that's pretty similar for all
bioinformatics resources it's becoming
relevant if you're going to were
supported by the Wellcome Trust we need
to justify our existence and in fact all
web source resources need to justify now
that's what we are right now but there's
lots of interesting things happening and
that's connected with data so this is
sequence growth so people may think you
know we sequence the human genome and
then it all went away it's not sure at
all the assembled sequence is following
pretty much a 13-month doubling time
it's been doing that for a long time
it's kept on doing it in the meantime
we've constructed a new archive for the
raw unassembled sequence because there's
a lot of things that come out of
sequencing machines should never get
assembled into full genomes but it's
still useful data to search that's
doubling every 11 months and the
database for that is currently thirty
five terabytes it's one of the bigger
oracle databases out there so this this
complete sustained behavior that we're
seeing the very few things which really
work in an
exponential way here we've got we've got
obviously Moore's law information
processing computers that's been
following an exponential growth for a
long time this is another technology
which is doing the same thing and
basically the point is that it's just
information and there's the moment
there's really unbounded amounts of
information in terms of sequencing we've
just scratched the surface of what could
be sequenced out there in the sort of
natural world and up to now we've mainly
been collecting a representative of a
sequence of an individual but the next
thing is going to be collecting that
across many individuals and there's a
current revolution in technology going
on so new technologies available now we
have development machines in house
through a number of companies using new
techniques for sequencing this means
that a single machine can produce
between 100 and 300 more per machine and
the costs already gone down by a factor
of 10 as a resolve this new technology
and I promise to come down much lot more
see you can do actually a first pass
across an individual one get you all the
pieces because you're collecting it
randomly but maybe for around thirty
thousand dollars and that's intent
there's a target to get that down to a
thousand dollars a genome and get a
higher quality than this so we're still
maybe a couple of all the magnitude away
but it you can see that it's within
sight now considering how much it cost
to do the first genome so we're going to
be collecting a lot more of this stuff
and we're going to have to organize that
and future human health research and
development is going to increasingly
depend upon this data and you can kind
of see this is you know sort of overview
I've stolen from somebody you see the
inputs here it's this annotation you
take the reference you layer the
variation on you try and interpret it
work out where the genes are and look at
the variations and those and relate
those to medicine and you end up with
this sort of complete sort of
understanding or at least set of things
that you understand and then you take an
individual sequence and that's pretty
much that's going to be achievable quite
soon and then you start relating those
to the database with the individual set
and you won't be able to interpret all
of it in fact we very little we really
understand properly at the moment but
the amount that we understand will
increase gradually over time as this
collective database increases you only
have to sequence the person once and
then that will allow you to start
understanding more about the medical
consequences and it's not all it's not
all projecting probabilities about
whether you're going to die of X or Y a
lot of it is quite practical things that
this person shouldn't be taking drug X
because it's it's going to kill you you
there's a fourth biggest killer in the
u.s. is adverse drug reactions allegedly
of course that's a somerville drug
reactions of course it's skewed towards
you treated a lot of drugs when you're
about to die but it's still very
significant number and so it's very
rather than to be working on this so we
have our database you know we're kind of
doing the preliminaries of this we're
starting to pulling in Ricci Qin Singh
data that's being shown on our websites
it's available we have to work on how to
do this in a compressed way but we're
already up in this massive 3 80
gigabytes our complete database and
that's changing every two months we
provide data mining interfaces to that
haven't really talked about and they can
be aggregated and integrated with other
resources over here but that's not
really the solution because we only put
things in the data mining interfaces
that where we know people want to ask
particular questions that's how we
denormalize the database in that way if
we really want to provide complete
flexibility then you can get at the data
using the api's but you have this
download problem so we provide access
now remotely through we host the
databases people can download the code
and talk to us that's becoming an
increasing way of people integrating
data with their own data
but what about beyond that because
that's still not completely neutral in
terms of the sort of democracy of
integration and this is the kind of way
I look at it so complete genomes provide
this framework that we can organize
stuff around but you know we're in a
strong position because we've got this
big database and resources we have a lot
of power then and in fact that's not
ideal you'd actually want to have no
monopoly on this because you like
anybody to contribute no matter how
small they are the more organization
provide data though the harder it
becomes for anybody to use the results
so how can you address this problem this
kind of fits into other sort of openness
issues which I just mentioned so just
the general issue of data sharing all of
the human genome because it's been open
has been quite a driver for this the
idea that you could release data
immediately make people available and
there's also this kind of idea which
came up of course of the Google camp
here but open models maybe you actually
require open models Vista was just
announced a couple of days ago people
are saying well maybe that suggests that
difficulties in doing that suggests that
centralized projects are just not very
scalable science has always been highly
cooperative but it's not been so data
rich as it is now not in biology we have
to find better ways of handling this and
then I'll just put up this this was in
berkeley i saw a few years ago all these
arguments about ownership patents things
like that you know i give another talks
about this but basically it's open it's
better it's going to be easier to share
things so the the various problems to
solve in the community of scientists
we're having to get used to the idea of
sharing data more and allocating credit
appropriately but there's also these
practical issues of data sharing so we
have this camp here and they were it was
kind of similar issues how do we
increase integration and processing
bandwidth
now bioinformatics so we have these
databases that kind of link together but
there's a lot of databases in fact
there's a danger that we have we have
too many databases and too many diverse
interfaces it actually removes reduces
impact because scientists just get lost
in different ones so can we find a way
of splitting data and presentation so we
get competition between those two
mechanisms and find you know optimal
tools for visualization so I'm going to
introduce that this dad's idea because
we're now using this very heavily and
this seems to fit within this paradigm
so what is DARS so this is the idea so
we're a big data provider and there's
lots of people viewing our data for our
servers and that's kind of a
monopolistic view because somebody else
has got some data it's hard they've got
to persuade us to put that data in our
system and that's not always we you know
we don't always know that the quality of
us somebody else's annotation it should
we include it or not so external
contributors they might set up their own
database but then of course that's a big
overhead to set up something competing
with us and probably it means you know
not many people go and look at their
data even though it might turn out to be
valuable so the dad's idea is you just
serve the little bit of extra stuff that
you've got you use some infrastructure
that make sure the coordinate systems
are synchronized and then you make the
viewers cleverer so they can integrate
things on the fly and once you've done
at once of course you can have as many
service as you like and users can
control this they can turn things off so
this is the opposite to the links model
linking takes you into different
websites but they've got different
interfaces it becomes harder to compare
the scientific data the Dow's approaches
the opposite standardized servers and
viewers which just can integrate and
users can choose which ones to turn on
so it's kind of like a very simplified
web services web services are fine but
if they all have their own
protocols then it's a programming
nightmare to use those here we've got
very status small set of standardized
ways to pull these things together and
the kind of ways we're using this here's
an example to link all the way from
sequence to protein structure so there's
this viewer for structure it's got a
protein structure there it can pull all
the stuff out from existing down sources
and integrate it on the fly and allow
you to see things like variations in
genomes mapped onto protein structure or
pretty transparently so I think that
what it what's possible using these
approaches you've got a set of data from
different providers moment we have a set
of services the big ones are you know
big integrators like us but you have
these problems like like this one here
which you know a small group that's
serving its own data it's not going to
get much usage because it's only got the
resources to serve this you know dad's
environment maybe it doesn't need to
provide anything at all because it's
data can just be integrated and then
down here here's somebody who was just
using their own data now they can pull
in other people's data and maybe they
can be a full competitive client by
themselves so we have quite a lot of
that infrastructure in place which we're
using we have around 200 different but
that different servers in this registry
quite a lot of different coordinate
systems and you can start thinking about
servers that build on servers so you
have consensus servers that process
other servers provide simplified views
where there's duplication
yeah so that's that's where I think the
annotation world is going right now at
least one sub strand of it how can you
pull this stuff in together so that then
scientists really can compare different
results and try and work out which ones
to believe because the moment if they're
sitting on different sites it becomes
very hard to do that now I want to say
just something about prediction because
this is where I've just come from this
meeting so I'm that's my background as a
predictor trying to compute biology
directly and you can look at it in terms
of these extremes pragmatic versus pure
so in protein structure prediction this
is trying to work out how the protein
structure folds up one end you've got
comparative modeling pragmatic basically
saying we don't know the structure but
we know that the sequence is related to
a sequence where we do know the
structure and they're close enough that
we can maybe in further thing it's not
real prediction that's kind of cheating
in a way but it's quite a practical
solution the other end you'd like to
just do some pure physics just take the
thing we know it folds up by itself make
it simulate but simulation doesn't
really work a few years ago there was
this thing called fold recognition which
was really comparative modeling at a
long distance and in fact what's
actually proving practical is
fragment-based assembly which is not
quite pure physics but it's it's much
closer to real proper simulation and
that's being popularized by the Baker
group who were again successful a cast
this year so in my genome annotation
thing it's kind of pretty similar one
and we have ab initio gene prediction it
kind of works but is problematic in
vertebrate genome annotation then we
have the evidence-based approaches which
i mentioned before which can be
automated with lower accuracy
things we hoped that comparing genomes
would make things easier but in fact it
doesn't because there's a lot of genomes
which is actually similar even if
outside the gene regions because they're
involved in regulatory regions but now
we're beginning to learn but one of the
reasons why we're so bad at this is
because there's lots of other factors
influencing the structures of genes lots
of other binding factors which turn
things which look like fragments of
jeans off make them inactive and so
probably to do this we have to predict
predict motifs we have to predict these
little signals and build genes that's
the way to do it properly but maybe that
involves a lot of compute so if you look
at what happened a cast this year it's
kind of interesting the different
approaches you've got to two groups that
came out very well one of which is not
using very much CPU and is using
evolutionary information a lot different
models different examples and merging
them but limited CPU and then you've got
the baker approaches which which is it
directly it's fragment-based and it's
quite costly in terms of CPU it's very
costly if you want to refine things but
the refinement accuracy can be very good
right getting quite close to
crystallography for at least a small set
so it's definitely making progress but
the cost is enormous so 1.8 angstrom
structure for if one example took they
quoted a million CPU days I'm not quite
sure how precise that is but its burst a
distributed model on 100,000 nodes so
where are we going to go with the genome
because we're collecting this data we're
integrating all these different bits of
evidence but ultimately we're going to
understand it at the level where we can
interpret individual mutations we're
probably going to have to do similar
sorts of very high CPU intensive things
so that's where I think ultimately we'll
have to go at the moment the value comes
from integrating data
so I just acknowledge all other people
these are all the different groups the
people involved in setting Ensemble and
the different groups the annotators and
people support that operation at sang it
so I'll stop there thanks very much ok
yep you mentioned the continuum proteins
between the computer modeling approach
and molecular simulation yeah under kind
of thing you mentioned is two of the
public record strategies where you're
like you can imagine I mean it's hard to
find another protein where the entire
sequence is similar but you can find
other proteins were know the structure
and little pieces of each of them are
similar
and so as a really good area where the
metric was pieces and then chose each of
the continuum close like that and then
they do an optimization and early fewer
that's kind of the fragment approach
although the fragments can be quite
small yeah you repeat other awesome okay
so so this was a question about the
about assembling protein structures so
the question was whether there's whether
there's an intermediate approach of
taking Frank matching fragments of
structure and putting them together and
that that kind of is the fragment
approach i mentioned although the
fragment approach goes to very small
fragments three months three residues
pieces nine most few pieces different
different groups to use slightly
different approaches and some of the
little hybrid strategies yeah
to the dash models based on an
agreed-upon xml the option yes so how
did you how did the larger community
arrive at that XML description is has
I've been agreed upon you talk about
some of the product some of that so the
this was proposed by Lincoln Stein so
just rebuttal yeah sorry so there's a
question about the dalles protocol how
it was agreed upon so it was basically
proposed by Lincoln Stein who you have
the first grant to set up the you know
the first of the client server libraries
for this and specify the protocol and
it's been basically evolved its protocol
we've extended it various ways to handle
other data types but it's still quite
compact there is a effort to make a dash
to specification which will be more
flexible will handle more metadata
properly and handle things like
searching because one of the things it's
distributed searches across these
sources so most of the models you know
things like Google map I think it's
based on the idea that you deposit your
annotation and central place where's
this model is of course you couldn't set
it up like that but you have servers
scattered around the world and each
Center hosts that server and it's up to
them to keep it up we with our central
registry we do monitor whether servers
are still up and warn them if they've
gone down and things like that but it's
much more fine-grained distributed model
so in terms of the evolution basically
was imposed and you know it's gradually
being adopted these eating providers
image really do not use so ncbi
certainly doesn't do this right now UC
Santa Cruz does have some does
capabilities now although they have
another protocol for uploading data
directly which we thought we have as
well we can you can upload data and
we'll run the dance server for you so
that you can it's integrated but it also
appears externally as does yep Laurel
question you mentioned earlier that
you've read back through your entire
data representation at sometimes if
something goes on multiple times
multiple times so how do you do this
without interrupting your users work and
it has the API from the users or clients
perspective from a pc alright so this is
a question about ensemble in its
development strategy so the refactoring
yes the refactoring goes on continuously
but there have been major points so we
have a number which gets associate the
release number within the original
release number in fact always used to be
the the schema version number and the
problem was that this it changed so many
times it's kind of become the version
number so that's where we are we're
version 41 right now so there was a big
change between version 9 and eight and
nine and between 19 and 20 where we
really restructured some things and a
lot of the other stuff has been a
relatively minor addition of columns and
things like that how do we handle it
well you get into this culture of that
you have 30 genomes and we don't know
update the data in these genomes every
month but quite a lot of them change and
some of the common databases that the
comparative genome databases have to be
recalculated every time so in every in
the cycle either the data gets updated
all the date this the data gets cum the
schemer gets converted so in the
distribution there are
SQL files for you know schema conversion
for every point so if you go through our
CVS tree you'll see a whole load of
patch files and so people have
downloaded stuff could also patch their
systems because other people externally
also putting using on psalm using the
ensemble framework to store genomic data
and of course they want to keep
migrating and keep up to date as well so
yeah it's quick it's I think it's a
cultural thing if you and in terms of
supporting these sort of things you know
we have 30 engineers working on this
there's a kind of feeling that that a
lot of a lot of infrastructure isn't
well enough funded and it's only got
enough funding to just keep going we
have enough where we can keep going but
we can also be re-engineering things on
the side and that's what you need to
keep things moving you have to have
enough resources for those two things
and we do get pressures we used to do a
one month cycle and that just became
unworkable because everybody was running
the cycle rather than doing any new
development work but two months seems
more sustainable
is there a standardization effort for
which one say so it's the dance protocol
is kind of standard and but it's it's
because it's XML it can be extended so
the we've added extensions and other
people can proposed extensions as a
proposed extension for interactions
between objects biological objects so
there's lots of lots of cases where
people propose a list of proteins which
interact with each other and that's
another thing where you'd like to
integrate in this framework be able to
see all these different opinions
integrated in one client so it's an
ideal thing to work this way where you
have rather than relying on people
publishing their own integration which
of course only takes the data that's
available at that particular moment so I
and if you said we're tracking a lot of
user data you're like a suspect you
didn't show anything any of it okay so
user data access as well we're not trip
web you know we have our web logs
basically and we know that the usage
goes up and we have user surveys and we
look at publication records where we've
been cited but that's all right now and
the question one question for us is
exactly is how much more should we do
and in a wider infrastructure question
you know I'm on various European
infrastructure committees you know over
here things are quite well funded
actually the NCBI structure of funding
events VI and other resources it's quite
stable and we don't have this in Europe
right now it's not as stable but
governments are always worried about
funding things on an ongoing basis you
know perpetually without having some
measure of value and so working out what
the level what the value is of these
resources and whether they're still
valuable where they should be merged or
closed down or things like this it's a
real question and so working out how to
not only how to integrate data but then
work out
who contributed what is I think a quite
an important question in terms of this
sort of long-term funding yep I had
protein structures can I you've asked
so you could set up so if your structure
that I was talking with that some people
were Casper about this so there's quite
a few databases of models for example
protein structure around and most of
them can be related to uniprot sequences
which are standard protein sequences and
so there's a matar all kinds of
annotation out there against uniprot
sequences so if you make the connection
between the two yes you can use those to
you know display that annotation on any
of these models that have been
constructed you know using pretty
structure prediction no but they were
talking quite enthusiastically about
some of the people were talking about
doing that so yeah I think you know
within Europe this thing called bio
sapiens which is a you eu-funded project
to link a load of different by automatic
people may working on protein sequences
and they've been the data adopted as as
their mechanism for exchanging data and
so that's been quite seem to be quite
successful and it actually is providing
an interoperability layer between all
those groups so you know a lot of the
sources are coming for those groups
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>