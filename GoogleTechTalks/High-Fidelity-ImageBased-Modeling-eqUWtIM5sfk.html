<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>High Fidelity Image-Based Modeling | Coder Coacher - Coaching Coders</title><meta content="High Fidelity Image-Based Modeling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>High Fidelity Image-Based Modeling</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eqUWtIM5sfk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you
yeah so Raquel fish the students at
University of Havana champagne with
gemphones now is in France yeah he left
you are you see whoever I right yeah
through almost two years ago now yea
once again once my outfit go after the
selected magic it was doing a no-show
there and we collaborated on a state of
the construction program from problem
hearing a lot more work in this area so
it's gonna tell us stuff about it yeah
thanks Franco thanks everybody for
coming I'm just stuck a few cava and
i'll be talking about our image based
modeling or multi-view studio and to
joint work with the jump on who is my
advisor so what is image based modeling
well it's about reconstruct 3d geometric
information from images then are there
are all different kinds of computer
vision algorithms that use images to the
3d reconstruction but our problem is our
problem called multiple views two-year
or multi-view studio which is kind of
just one our category of image based
modeling methods and images can contain
your various objects are like see is my
pointer obvious of some like a toy like
depicting our Roman soldier could be
some sculpture it could be some face or
some real human skull it's for the
anthropologists who are interested in
analyzing the shapes of skulls and it
could also be in images of some outdoor
scenes this in water and could also are
images of some buildings we are you see
pedestrians in front of it so in this
example you want to do 3d reconstruction
on this building while ignoring
pedestrians see also by the way I feel
free to stop me and ask questions at any
time so more formally are we focus on a
problem that takes set of images with
camera meters so we assume that the
camera parameters already given so
cameras are calibrated and you're going
to reconstruct 3d geometric information
of objects or scenes that are rigid with
respect to the cameras so that's kind of
key an idea so in the previous slide you
see the people in front of building and
if you take pictures picture the
building like from multiple viewpoints
those people probably have moved
so those people are not rigid in the 3d
scene so those are whatever object which
is not rigid is considered as outlier
and I'll be circle so we get ignore them
so our purpose is to reconstruct rigid
structure in the 3d scene and you see
just one example you take a pictures of
my face for four different viewpoints
and this is our pitch taken at the same
time so my face is rigid and we're going
to do some 3d reconstruction and you can
get a model like this and I like to
mention why this problem is important ah
first of all I in comparison to laser
range scanner which is going to popular
for complete graphics applications we
would like to address a couple of other
advantages over later a nice kind of
based methods which is on first of all
wheels just cameras are for data
acquisition which is cheaper down there
in the scanner and if you count the
number of pixels in the cameras or if
you copy ha lateral resolutions of the
devices cameras have much more pixels
around later in the scanner you know
they don't it's going to give you some
dips map but the number of pixels are
much fewer and also the cameras are fast
or higher frame rates that's akin to key
advantages over the is a range scanner
but of course the disadvantage is that
depth accuracy or vertical resolution
according to some quantitative
evaluations of this multiverse Theory
algorithm you see right here that if you
use our kind of latest one of the state
of the art image building methods the
accuracy of reconstruction is about
point five millimeter for an objective
about 15 centimeter in diameter so about
these objects and about 25 millimeter in
accuracy if you use our kind of set of
low-resolution images 640 by 480 images
well if you pick one laser in the
scanner brand here which costs about
fifteen thousand dollars then according
to the aspect the accuracy of depth
measurement is point one millimeter and
for an object about 15 centimeter but
the couple of things to note is for this
result for this image space moon result
is really low resolution images 640 by
480 you can just use higher resolution
and camera maybe 16 megapixel are 12
megapixel then this result should just
become much better and also for this
later in the scanner typically you want
to get a single nice model you have to
scan from multiple viewpoints by using
later in the scanner they merge them
into a single 3d model so typically
there's a manual work to register those
tips map and additional errors can
introduce in that post processing so i
guess the image based molding method is
kind of improvement or a good competitor
for is there any scanner based methods
and also like mention other advantages
more like advertisement but are here are
some situations where we can capture 3d
geometry by using cameras but not by
laser engine scanner for example are if
you go to NASA website you can download
some images of an asteroid like rotating
in the space and ultimately we can just
use our standard chemically beijing
program like structure from motion and
get camera parameters around our system
to get to the reconstruction but if you
want to capture 3d geometry with the
asteroid in the space we're using later
in the scanner you have to you know ship
your scanner to the space which is
expensive and you don't want to do and
another situation is our if you want to
capture like fast motion like facial
motions or more non rigid object because
our cameras of has are we can kind of do
for visit example frame before every
construction but later in the skin is
too slow and cannot capture like facial
motion and last extreme example is are
some people injured in reconstructing
objects like hundred years ago which
these are showing this picture this is
the kind of famous kind of culture
heritage our steps it has been in there
for like hundred years and actually they
are some people interested in
reconstructing structure hundred years
ago because this search has been eroded
for years like winter rains and for
those people the actual images taken
hundred years ago set black and white
images but actually we can do use those
images to 3d reconstruction actually
want to microwaves has done that but you
can't use laser range a scanner to
capture this structure 100 years ago
because you don't have later than scan
about that time and you don't have a
time machine to the ship laser scanner
back hundred years ago so there are
situations where you want to use cameras
instead of the ax later in the scanner
so here's the overview are for the rest
of my talk first I'm going to talk about
the function called photometric
consistency function which
is a really general fundamental idea in
multi-view studio its kind introduction
to for people who doesn't really know
about reconstruction today I'm going to
talk about the many ideas so algorithm
then talk about the algorithm in detail
show some results and differences some
future work any questions so far yeah so
far so good so first I'm going to talk
about the automatic consistency function
which is really key an idea in seed
reconstruction the real basic of how
testament 3d depth from multiple camera
is really simple it's just what you do
with your two eyes if you know close one
are you can really estimate depth
information while you have two eyes you
can kind of estimate depth information
and but we look at this property are we
I mean by people working a multi-view
studio in a totally different way so
basically we are trying to come up with
a function which takes a 3d coordinate
and resented some scholar value and if
that 3d point is on the surface of an
object then that's colabello should be
small so basically we have some a 3d
space and trying to evaluate this
function value all over the space then
we have a function value is small that
is probably on the surface of an object
and I'll give you more intuition by
traveling some easy example so this is
one example of such photo constituency
function which takes a 3d coordinate and
return some scholar value suppose you
have our three chambers one two three
here and because kemah parameters are
calibrated you know where these three
cameras are this is the one object
source of knowledge of course you don't
know this surface is something you are
now trying to reconstruct so what we do
is to suppose you're trying to is our
evaluate a function value at this 3d
coordinate so what we do is we just take
this 3d coordinate and project into each
camera we can do this because we know
the camera parameters and we look at
pixel colors of these image projections
so we expect that these three pixel
colors should be different because this
pixel colors coming from this point on
the surface well this pixel color is
coming from this point on the surface so
those values should be are not
consistent well if you pick this 3d
coordinate oops they evaluate the
virtual is the function value in the
same way you know these pixel values
values come
from the same place so this pixel color
should be consistent so for example by
computing the variance or pixel colors
of these image projections you expect
that the function value be small for the
3d coordinate but high for his 3d
coordinate so we can do is just sample
point in 3d space and evaluate the
function value what we should expect is
smaller values here which actually on
the surf zone object and higher value in
the other places so essentially our
problem is to evaluate this function in
the 3d space and trying to find a 2d our
surface where this function values small
I hope this makes sense so now I'm going
to talk about there are many ideas of
our approach framework and they actually
more than three many ideas but I just
fix major three ideas and first our is a
patch based service model on typically
when you talk about like 3d
reconstruction people interested in
getting a 3d mesh model or triangles in
a mesh model but we use our set of
smaller rectangle patches to represent
our at the surface of an object so in
this movie is actually pretty hard to
see but this is just our tipsy mapped
smaller planet patchi's density covering
the surface of an object so we use our
suitability rectangle patties to
represent the surface of an object and
we use our this representation for
various reasons which I am going to
refer later and for each patch are
basically we just need to know the 3d
coordinate of the center of the patch
and also orientation which basically we
want to estimate these parameters for
each patch covering the surface of an
object and observe before getting to the
second idea are in the year I just
defined for the consistency function for
3d point but similarly we can define for
the consistency function for 3d patch so
that is really simple suppose you have
some 3d patch in the 3d space and you
want to compute some function and if the
function value is small dispatch it
probably on the surface of an object nor
either is simply just project this patch
into you know multiple images then
sample colors of these 3d how can agree
three by three grid locations so if this
patch is actually correct then
these colors my nine colors this line
color should be consistent or to some
similar distribution so we can just use
any like a measurement consistency
measurement like some obscure
differences or normalized cross
correlation to compute how image texts
are consistent with each other or not so
this level problem is you want to find a
3d playing where this function value is
good and our here's a second idea Apache
expansion which is basically making use
of special consistency so for example
you have some surface of an object which
is typically smooth so if you can
reconstruct a single patch here then we
expect that there should be another
patch nearby so you can kind of keep on
expanding those packages instead of
exhaustively searching will hold 3d
space so we have this kind of smart
expansion procedure to efficiently cover
the surface of an object and the last
idea is how to handle or collusion such
as like pedestrians and in this example
for example if you want to reconstruct
this structure you won't use these two
images while ignoring this one to the 3d
reconstruction but how do you do that
and it turns out that a really simple
trick can do the job and the trick is as
follows so suppose you have some 3d
patch which are going to project into
these three regions what you want to do
is to use only these two images you can
know the third one to the 3d
reconstruction then what we do is simply
we just compare textures so if you
compare this texture this texture by
using the photo constituent function I
just are defined so these two textures
look similar well this is to actually
look different also these two types you
look different we can basically do that
oh this must contain some obvious about
lawyer so what really simple just
compare textures and messing with some
threshold we can detect that this is
outlier and use only the in light
emerges to add your 3d reconstruction
it's really simple trick by turns that
works very well and I'm going to show
you some results later so I'm going to
talk about the details of the algorithm
by using these ideas and the algorithm
consists of our 3 steps and 1st we
detect features and I use the kind of
two popular
future detector in the computer vision
which is a Harris corner detector you
supposed to take like corner like
features and the second future sector is
our difference of gaussians which is
supposed to detect blob-like features so
here are some one sample input image and
we run future detector and we get some
result and also another image of the
same object from different viewpoint and
run future detector and get some output
so in the second step we try to match so
the features to generate a spot set of
like initial purchase which are going to
be expanding the in the last step so the
in the second step we try to match
features to generate patches and the
idea is I think I guess pretty straight
forward in this toy example you have
three images and are so this grid is
basically you can think of a parameter
to control the others resolution with
output so you we are trying to get at
least a single patch in every cell so if
you really want to hide identity
reconstruction you just put better to
one so you want to get at least one
single patch in every single pixel or
you can asset this way that 24 which
means you want to get a single patch in
every 4 by 4 pixels squares but anyway
our goal is to get until it's a single
patch in every single cell and in the
second step what we do is we just pick
one feature you want image then trying
to match this feature in another feature
in the other two images so if you know
Gemma parameters then we know that our
this feature must correspond to some
future along this line there's a special
ankle a polar line is I have to match
this feature again select for example
this feature you know that the
corresponding fusion must be along this
line so we pick this feature and find
the feature along with a pole line in
another image suppose you find one so
you're trying to do is now trying to
match these two features to generate the
patch then if you assume that these two
features match you can actually estimate
the center of the patch very simple
triangulation you have two images you
know Cameron parameters if you assume
that these two features match then just
shooting rate and compute intersection
we can roughly initialize the location
of the center of the patch
so we initialize the center of the pads
are by simple triangulation and we
initialize the normal over the patch the
orientation of a patch by simply taking
our the optical axis of one camera so
this is a really rough initialization
now we are going to refine these
parameters in our current simple
optimization so before fertilization are
we like to use as many images as
possible to define these parameters so
because we have not good initialization
we also again project this 3d coordinate
into what the other images so these
three features could possibly contar
correspond to the same 3d feature and
what we do is the adjusting I explained
before assuming that these three
features match we did it outliers by
simply compelling image regions and by
using only image in layers to refine
these parameters and for the
optimization are we have some function
to in optimized we i define the photo
consistency function which is defined by
a 3d patch we basically want to find out
this parameter I feel the location of a
pear to orientation of a patch so that
this function value is going to be
optimized so this is a simple
optimization problem we have some number
parameters to parameterize the patch or
initial location you have some single
function for the consistency function to
be maximized so you put into this
function and available to the optimizer
and you get the optimized value and I
just use the conjugate gradient method
for the optimizer again ah I have some
initialization in a sea of P NOP 3d
coordinate of the center of a patch and
fashionable and put into the optimizer
to maximize photo consistency function
and I got the result so if you after in
optimization if your photo constituent
function is more than some threshold all
the patches successfully generated so i
take this patch and kind of mark this
cell as gray the patch has be
reconstruct already so i'm not going to
do any reconstruction for this cell
anymore so you keep on doing until all
the cell has been our marked or all the
future has been tried a matching till we
keep on doing the same thing
so after this are we have a some initial
set of sports are patches but they only
corresponding to the features on images
and we want to dance reconstruction so
we are going to apply some patch
expansion and also filtering because
this naive our future matching generate
some false positives so we need to fill
out some bad false positives while
making the pass reconstruction dense by
expansion so a patch expansion is just
explaining the main idea it's really
simple so this is something we expect
will help some surface and the past you
lined up nicely I think each other so
suppose you have reconstructed one patch
here then we expect that there should be
another patch along the same in a plane
so we initialize again the patch the
location and orientation of a patch by
simply placing no neighboring patches on
the same plane then again optimize this
allocation orientation by maximizing on
photo consistency function so like this
the suppose are again both parties have
succeeded in optimization as we just
take next patch assuming that the famous
via our neighboring patch here so you
place it initialize it and put it into
the opportunity then you get some
results so you keep on doing kind of
smart about expanding patches on the
surf zone object instead of searching in
whole 3d space so are the another are
think we have to do in this step is
filled out bad patches or encrypt
Apaches and here's a typical situation
where bad patches occurs if we have four
cameras then this is a red patch is that
incorrect the patch and we just kind of
reconstructed here by mistake and you
have a set of like four pages which are
correct path leads and example hook up
this patch p1 when you optimize the past
few one for example you use this image I
for as an in lawyer optimize pad the
past p1 thinks that I should be visible
in this camera and also for patch p to
Apache the things that I should be
visible in camera I three so are there
are some visibility inconsistencies
happening so what we do is for each pads
for example for patch p 0
vehicle at the set of pages which are
inconsistent invisibility information so
in this example for past p0 all these
four pages are inconsistent or breaks to
visibility consistency so there are some
our collision here so I just our compare
the photo consistency function value at
p0 and all the photo Constitution
function value at these four parties and
obviously because there are so many
pachitea hear conflicting the
invisibility so p0 should be an outlier
so there's you can kind of guess that it
should be linked web to patch so we can
reject these encrypts patches and here's
another situation we have the inquest
patch it again p 0 is inside of the true
surface again same thing happens so p0
cunning are inconsistent invisibility
information for camera i won against
patch p1 so there are two pages that are
conflicting invisibility information for
dispatch so again now we can reject this
p0 as an outlier the typical of utilize
this filtering process and expansion
process a couple of times we can expand
futile bad patches and we expand our two
or three times you get a final 3d
reconstruction I'm going to show our use
some results here is our sample image of
our an object data sets where there's an
object and I amateur taken it and over
around and typically there are many
images from like 15 or 20 or 30 to cover
it really from all around and for these
example are only four images can only
from the front and here's our stop it
it's not powerful enough to show playing
both this is just showing the texture
map reconstructed Apaches so this is you
are now looking at a bunch of tiny
rectangular patches with a fiction
mapped on but because they are so dense
it doesn't look like our set of purchase
but this is just a real output of the
algorithm again for both datasets are
just coming from for images and our
before showing more results are of
course you're just getting a bunch of
patches so this is not doesn't have any
connectivity information and people
interested in math model we have to kind
of convert our past model to mesh model
and there are something
existing work for example are the cousin
he has some algorithm and kind of three
windows are software binary which going
to convert set of packages into to the
manifold automatically and there are
some another method which are was
originally your president bye bye uncle
is and live Michael avoid which is
actually for the laser range is kind of
based methods but there are couple
methods to kind of merge convert touch
base model into our self the base model
and this work I pick this approach I'll
call the interactive deformation which
is actually simple so first we somehow
build a polygonal mesh model as an
initialization so if you know are some
program background segmentation you can
be the visual whole model which gives
you an initial mesh and if you don't
have our segmentation information or
then you can for example compute a
convex hull of centers of reconstructed
practice at the initial kind of surface
and there's another way you have kind of
initializing the surface this is the
kind of visual hall for one data set
this is the convex hull of centers of
all the reconstructed patches and here's
some another way of initializing the
mesh what we do is basically trying to
snap the vertices on this match until
reconstructed pages so you have
initialized you know your mess somehow
they have a detailed reconstructed part
is basically trying to snap those in
services on the reconstructive hatches
at one by one so our keys the results of
all the object data set so you see for
each data set you see a sample image
then these are reconstructed our patches
and these are the final mesh model and
from different to different viewpoints
if you a couple of things to notice for
this example you don't see much texture
basically kind of white and are
basically using conceding information to
the 3d reconstruction and it works out
really well and this is a one example
with our kind of complicated the
geometry for example this eye socket is
a really deep concavity and the typical
it's really hard to get this a high deep
concavity by image based moving
algorithm but because he is going to
patch flexible passion model and if you
notice that we don't have any good
regularization in the algorithm so you
can easily handle
a huge concavity a huge depth the app in
the reconstruction and also this is
another interesting example with a skin
texture this is really weak you don't
see much texture but we can actually
match those textures to get a nice
reconstruction and here they are the
movie of the reconstruction mesh model
so this is example with our big
concavity here at the eye socket is
typically difficult to obtain in the the
right hand side you see our this is kind
of massive data said about I guess 48
images and high resolution images and
are I guess the reconstruction almost
accurate except a few places for example
here this actually a garbage our whole
structure and this part cannot be
reconstructed simply because this is
kind of cape and seeing the structure
behind it so there are not many cameras
probably only similar KML looks at the
surface and to the 3d reconstruction
have to at least two cameras to look up
the other structure so this replace
we'll really we couldn't do anything but
for the other parts I think we got to do
a pretty good job of reconstructing the
structure here's our some another
example this is the movie i should
actually at the beginning the only for
images but i guess you can get lots of
sharp with details and like my facial
like a features also some buttons you
can get really high detailed structure
from only from four images yeah here's
again from four images on my whole body
and so really i thought it's a really
tough example because of the jeans the
jeans you know black which means absorbs
lights so it's kinda hard example pretty
much based on building methods because
you don't see much photons coming from
the jeans but we can get lots of details
and here's our results are sample image
for our scene data sets see means can be
helped some structure and picture taken
from like one side and our this is
interesting data set in terms of water
because water in
looks different from different
viewpoints so in this example we expect
the algorithm to kind of you know these
water region and just do 3d
reconstruction here and our for this
example I can manually pain to this go
read our human are to this data set to
kind of add I obstacle up later so this
is a real example with the opposite of
outlier and I have some movies to show
the first of all just aesthetic images
of the result and so for this example as
I mentioned earlier you know there are
pedestrians in front of it so you want
for example you want to get our
destruction then you want to use the
second or third image but even loading
the first and as a result that are you
can make some fiction matched patch
model and when do you texture-mapping
you can use images which does not count
the outliers so when this visualization
you don't see any pedestrians in this
are rendered our image because we can
kind of vino which image region contains
pedestrians we can just use the right
image texture 2d texture mapping so we
can magically remove people from the
input images and this is another example
without lawyers and so these seven
images and this is a result again are we
can kind of statically identity of this
region as outlier and you know those
region just do 3d reconstruction where
are those people don't appear and for
this result one thing I did is to
replace against the fifth image by the
third without changing camera parameters
which means the whole foods image
becomes a garbage because general
parameters different so just replace
this image by this so the algorithm has
to identify the whole fifth image as a
garbage and ignore the outlier and also
ignore these red of image original layer
then do 3d reconstruction and we can
still do now of course we are missing
some part of the structure because
probably they are not enough cameras
looking at the structure so we cannot do
anything but instead of creating garbage
we just reconstruct structure and
leaving leaving ok empty space we are
there are not enough information I think
which is a good thing and another are
interesting example r is for this data
set this is our just a building
structure but one interesting thing
about this data set these are for some
of the image
is our pictures are taken really close
so you have a four hour before for kicks
you're taking from far from this
structure then you have a couple of
images are taking this structure really
close so if you remember our algorithm
the number of patches or the resolution
of the reconstruction depends on images
if you have the close-up images we are
trying to reconstruct more pages for
those places so the resolution without
put is kind of adaptive automatically so
if you have some images close up then
forage for that image we are trying to
reconcile our number of the Apaches so
this is one advantage with algorithm
that the ultimate resolution is going to
operative so you have close-up images
you get more patches and this is
shoulder reconstructed patches for some
of the other results this is example if
you remember with some water behind it
so actually you can see how the movie
has marched back but from this reason I
actually asked up already and this
contains some fake patty's here it's
probably corresponds to alter region but
our this is the only mistakes and we
didn't create garbages for the rest of
the yellow image and for this example
again because we know this example with
the pedestrians in front of it and
because we know which image region
contain obstacle which image does not so
we just use the maturation which does
not count lb circle to the texture
mapping so that we can kind of remove
pedestrians in rendering this movie and
also follow you can see this poll i
standing here I guess it is difficult to
get this structure if you like mesh
model best methods because we use patch
flexible fashion model we can get this
kind of structure quite easily and our
he'll just our final 3d mesh model which
you converted from the set of patches
so our I've shown you pictures and
movies and of course people are
interested in asking so how are they
accurate and are so fortunately there's
are some quantitative evaluation of
multi vista algorithms features are
published last year cvpr so basically
provide some datasets then people run
their own algorithm then submit result
they're going to evaluate you accuracy
of the results are compared with a
ground truth and show you the how much
accurate your results are for example if
you can see maybe I'm now connected I
can go to the website
yeah so if you take multi-view steel
gets the first link is going to be the
evaluation which is done by a Steve site
too risky so apparently they are
providing terraces for two objects about
total six data sets for each object they
provide three datasets they are
different in the number of input images
then are you know people just run you
algorithm and submit results and you're
going to see our results by clicking
main table here so there a whole
different our algorithm you can see and
the algorithm I just explained these are
going to be listed like here and are you
see like two numbers so this table is
our so you have one day to the temple
dataset three different data sets they
are just different in the number of
images so you see other sparse data set
which consists of only 16 images they
have some intermediate data set 47
images and really dense 1 312 images and
you have another data set the Hawaii
dinosaur without much texture again
three different addressed with the
different number of images and for the
green number of the balloon number
basically says accuracy so they use
laser range scanner to capture that kind
of ground to a 3d model basically
computing distance from the ground truth
model to your reconstruction so are the
kind of says that on the average your
reconstruction within point 65
millimeter from the ground truth so the
smaller the better for this blue number
and the green number is kind of
completeness so this number is higher
the better basically that how many
percentage of the reconstruction with it
is within how many are a certain number
of distance from the ground truth model
so basically that if the blue number is
small which is good if the green number
is high then which is good and so our
algorithm these are just full of color
to actually there's a free call at the
top and our our result is actually the
best in terms of both these blue numbers
green numbers and in these four
categories so all the dinosaur data sets
and one sparse temple data set if you
compare for example are so our result is
I point 42 millimeter accuracy which is
better than all the others again 99.2
average and which is better than all the
others and one thing I found interesting
is our results for this sparse data set
only 16 image is actually better than
any other algorithm on the dense data
sets so these people use more than 300
images to the reconstruction but our is
l only 16 image is actually better so I
guess you can tell that al are going to
really good at handling and processing
the accurate not fitting model yeah so
here's some of the pictures of the
reconstruction model by four different
kind of top performing algorithms this
is our method and these are the other
three you can top-performing algorithm
and you can see that even qualitatively
you can see I guess different so we can
reconstruct this kind of sharp structure
here or there while many other methods
going to fail in getting structures and
even now a qualitatively I guess I'll
result is probably better than are the
others and I but of course you know
getting good numbers in this kind of
evaluations oh it's a great thing
because people pay attention but I guess
it's really are i miss kind of confusing
because people use become numbers and
think that all this algorithm is good
this algorithm is bad but i will like to
emphasize that there are whole different
kinds of properties of the algorithm
which is superior to the other
algorithms and are in addition to the
accuracy just mentioned i guess we can
list a couple of things which a
logarithm is better than the others for
example on a logarithm just takes images
and the camera parameters and the
multiverse theory is supposed to work
with just imaging camera parameters but
typically there's something like hidden
inputs are in addition to the stream hmm
parameters and images and are those are
children inputs are with visual hull or
bounding box or valid depth ranges and
our new algorithm we do not use any such
information but many other algorithm are
needed this information to kind of start
their process but we do not need any
such addition
input so which is one advantage and of
course we can handle a variety of data
sets so I showed your results of the
object that like data sets or seemed
like data sets but many of the algorithm
in the evaluation you kind of focused
only object like data sets so our
algorithm really general framework and
can handle variety of jealousy but still
is I guess the shoulder kind of better
results or best results in an object
like data sets so our okay this is one
advantage it's really general algorithm
we can handle different kinds of objects
and ours the other advantage is using a
patch based model because there is no
regularization or smoothness we can
handle a guest complicated structure
like a deep concavity another thing is
our collision handling this is just one
simple trick but we can handle I
pedestrians in front of the building and
etc this is the difference from the
others and the last thing is our this is
also one thing I mentioned that if our
if take pictures and film like different
viewpoints and if some images are really
closed up then our resolution is going
to be higher I'll fold those images so
you can just take take picture of random
and the algorithm knows that focus for
this structure you have the close-up so
the resolution that is going to be
higher and it cetera so our elective
mention about the huge our kids what you
can do are after this and so one thing
is to obviously to extend this model to
incorporate some temporal a component so
if you're going to capture some motion
so so far even just looking at static
frame reconstruction but suppose you
have some moisture like facial motions
or some no movement then we like to
catch the structure at each frame but
also estimates emotion which can be done
by adding some temporal component to a
touch base model and our which already
have some results to show see although I
either have time to explain details but
are what you were trying to do is for
example you see some eight synchronized
video cameras think a multiple
synchronize and calibrated video streams
what you want to do is basically our
reconstruct structure frame by frame but
also SME motion frame by frame
and this actual data set are which is
not present at siggraph are this is for
our fourth motion capture and me just go
back and play so for this data set but
they use the fact that they know how
those triangles laid out on the pant so
for example if you look up this triangle
like red triangle blue triangle yellow
triangle for their method they know that
if you know the patterns they know that
where this triangle should be for
example vehicle left leg has some
special patterns so they remember how
those colorful triangle laid out on the
patch but in this framework we just want
to show that you don't have to remember
those you know textual areas you just
use my multi-view serio constraints to
the motion capture and reconstruction
and you see the results here oops here
so here again we basically reconstruct
set of patches for one frame the initial
frame you keep on track in patches frame
by frame this is the direct output of
the purchase this is just our with the
texture mapped on and are we can
estimate the velocity field actually
translational velocity and rotational
velocity field and so these actually
blue arrows show translational velocity
field actually it's really hard to see
because so this green one is the
rotational velocity and we can can
animate the mesh according to this
velocity field so you see kind of
dynamic mesh model on the right hand
side sorry I so our spoil you reckon i
initialize a pageant first frame so you
know that how the texture should look
like in the first frame so basically
you're trying to find the optimal three
widgets transformation a frame by frame
so it's kind of purely tracking same as
an old widows in a 2d tracking but we do
tracking 3d
so another thing which I think we can do
is to remove obstacles from movies and
movie editing so this is one kind of
proof of a constant movie which I share
with you earlier so this example you
have only three images as an input then
you see like pedestrians in front of
building so we can make this movie
without showing any pedestrians so I
guess you text a movie which you know
container people in some of the building
what we can do is to identify imagery
which Conte obstacle and kind of
magically remove those people from the
movie so kind of lots of things should
be able to be done by using the fact
that we can detect like imagery region
containing obstacle an outlier so this
could be another interesting application
oops I stopped it before showing the are
who acknowledgement section see I see
this the end of my talk but just want to
show the names are people who contribute
to this work and I thank you very very
poor Allah attention I mean I happy to
answer any questions yet Oh in two
different field so that's actually good
point are so for all the data is pretty
much I really carefully kept a set up
the camera so everything is in sharp and
are so actually there's wonders I don't
have any results but our I have example
where the Sun load the image are
surfaces kind of out of focus and but
actually our algorithm worked and
actually it's kind of hard to of course
the subsidy cuts to become so noisy and
our mother we can still the autumn
didn't fall apart and i will say so if
you have limited depth of field we can
get structure that the quality of
repossession degrades of those places if
things become too kind of blurred out of
focus i guess are going to falls apart
and you do not get any reconstruction
the filter and accessibility facts begin
yeah yeah yes you mean like errors in
kemah parameters I also that's also a
good question up so i would say that
I'll going to actually without handling
bad cameras so I've showed you one
example where I can manually change the
one image by the other with a changin
camera around which means Cameron
parameter for this camera is totally
screwed up but because our algorithm has
a system to detect light image which is
bad so I always when i whenever i
compute this photo constitute a function
i identify image which looks bad so you
can all those images yeah oh also is
inaccurate yeah yeah yeah so I guess in
that situation for a structure if you
see for example two cameras which is
somehow consistent we use that to camera
to reconstruct and for this surface we
have another set of cameras which are
consistent we use that camera to
reconstruct but because these two
cameras are have some errors so you see
some bumps or noisiness in the
reconstruction so I assume though
because I haven't experimented with such
data set but I assume that you get some
structure which is just noisy but
doesn't fall apart
for let's see for some of the sequences
i use the exhibition grid for some of
the sequences i didn't use any
television grid just use structure from
motion and insect features in the image
and match them and kind of computer
video algorithm to automatically give
you camera parameters yes estamos less
optimized ionic compounds as soon as we
do not know where the cameras in our
countries if your mileage somehow from
the images we have 120 of them I mean it
makes features somehow because now I
feel that there are many applications
which we when we just have an image or a
movie and we didn't know the exact
position yeah so our and say so heels
are in computer vision structure from
motion so for our side I believe that
the camera pans are given and this is
not our problem so except those people
working a source promotion so they have
a similar way of matching features from
like different viewpoints and do some
robust is like matching algorithm j some
internal parameters but i guess one
thing I can say is are so if once you
have a sort of chem impromptu which are
good enough then we can run come this
algorithm to get some matches then by
using those matches I can refine camera
parameters because matching features are
difficult as getting channel parameters
so I guess we can help each other in
your estimation of parameters and doing
density construction to help each other
kind of ETA right between those steps to
you are defined command parameters
further nice ash missus
I mean speeding up all those they'll
actually no because I don't really care
much about engineering efforts so this
is actually kind of slow when optimizing
those Apaches I guess I did just use a
numerical derivative is around other
analytical so for I didn't care much
about our speeding to speeding a lot but
I guess there's opportunity of the
speeding things up because one work by a
young Philip owns here's a GPU to do in
multi-view stereo to run really fast I
guess we can kind of i'll just mention
optimize patch in parallel by using gpus
detect something really fast but the i
didn't care about those engineering a
fourth yes point samples that's why I'm
going from point samples to patches from
one to the other
we have a positional information of the
pensions only to include the normal
vector as well sorry about the question
I the way I did things in ages ago was
primarily based on just extracting
t-shirt points
3d and treating them as poets rather
than the Centers of patients I see you
can go that route oh I see so this one
answer is our I'd like to use some edges
and points first first of all you got an
expansion if you know patch orientation
you know that there should be a patch
Jason patch nearby here because we know
the normal direction but he have a
30-point alone you don't know where the
adjacent points should be kept going to
sergeant all 3d space and the other
reason is our if you have a 3d point so
basically you're compelling pixel by
pixel all you have a pad some region
your computer image region by imogen
region and I guess it is kind of true
that if you image by image image region
compared to things become more robust so
I guess that's a two main reason why I
want to use patches and Sylvia points
yes will be excellent witnesses Oh easy
so for the first and second power for
the repeated textures lr so on the sea
they may not have a good example but one
I guess answer is so if you have focus
on three cameras they have some repeated
texture maybe like this and I actually
it is really difficult to to match this
line with this line if you have three
cameras so maybe not into so you have
only two cameras a you take picture from
two viewpoints then there is your like
3d coordinate where this paramedics to
this game a pattern we have three
cameras it's actually really less likely
to happen there's such a coincidence we
have more cameras that even though your
textures you can repeat it I think it's
less likely that our kind of different
different a similar patterns from
different position actually matching in
3d space now possible I'm sorry but it
was the first point I know text you I
see so actually lots of K she asked me
that question and I guess that's a good
point on so I guess the data set which
does not contain good texture I guess
here is this object and if you look up
this region looks really homogeneous and
our I guess I can just go to the website
and other case it turns out
also for our eyes you don't see much
texture on those homogeneous region but
I guess actually there are textures see
oh here so this is the R this Hawaii
dinosaur data set and are here is my
reconstruction to view one yeah so
actually we are doing a pretty good job
which means there's a slight graduation
in shading and we can actually capture
those the patterns the maximum occurs
multiple multiple images but their
situation we actually there are no
texture at all really just quite and you
can really distinguish between the other
regions and which actually occurred in
one part of this data set which from the
different viewpoint actually I think
this is hard to see but for this region
you see some bumps which is a fake
structure and I guess for this data set
this region doesn't have any containing
useful information and all the other
methods failed in getting this structure
for example some of the good ones are
see so this is a good algorithm so again
basically messed up here so I guess this
is a situation where there's a really no
good information to extract placing
multiple constraint and things fail our
current regularization for our work I
haven't really done anything but what
happened these are we didn't get any
patches here so there's no information
we so in getting a mesh model or somehow
an interpreter added some legalization
to get a nice surface but many other
methods help some realization so the
information is noisier than you get
summa summa the surface oh yeah i'll
show you my lady this city
more speaking Isaac
like this RPC save your all do you like
metal same as regular doesn't work and
for some objects which has some speaker
I guess we may not have a good image to
show here there's one object which is
not really special but you see some
sweet girl highlights for one image but
are sweeter holidays we considered
almost about liar same as pedestrian so
in our framework if you have you know
some more service about a particular
multiple viewpoints if you see people LT
one in which we educate us out by image
they know it and use all the other
veggies 20 constructions it is obvious
not to speculum you can handle no
similar transparent that's even tougher
well I guess it doesn't work at all and
of course if you obvi semi-transparent
there should be another complete vision
algorithm to the 3d reconstruction so
this is assuming that should be some
math like property on the assumption
material oh now I see yeah I guess I
haven't tried like glasses or windows I
guess if you use like windows the field
reconstruct your system reflection for
example have some window you reflection
with a structure so I should be able to
get the structure which is merely
affected on the other side see the app
if you see clear texture to guess yeah
no other questions yeah thank you very
much folder they're coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>