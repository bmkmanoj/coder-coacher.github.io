<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Emerging World of ECoG Neuroprosthetics | Coder Coacher - Coaching Coders</title><meta content="The Emerging World of ECoG Neuroprosthetics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Emerging World of ECoG Neuroprosthetics</b></h2><h5 class="post__date">2010-10-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mFWnTONOvVo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">eric is a neuro scientist and a
neurosurgeon in particular it is also an
entrepreneur it's actually a novelists
as well and he has some interesting
perspectives about the intersection of
the brain and information that we can
gather from the brain and information
what we can do with that information for
both people who are inside of what we
would traditionally think of as
healthcare and the needs of healthcare
but also people outside of that so with
that Eric we look forward to your talk
well first off let me thank Astro for
inviting me and also thank Google for
inviting me out it's a real pleasure to
be here so the title of my talk is the
emerging world of econ or prosthetics
and what I'd like to talk about first is
I'm going to define what a neuro
prosthetic or a brain computer
interfaces what signals do we use for
brain computer interfaces ye cog and
that's kind of the signal that I've been
studying a lot for its application and
what are some of the near-term
applications and what are the future
implications of this so first off what
is a neural prosthetic well a
neuroprosthetic often it's and it has
multiple names brain computer interface
mother named hands or brain machine
interface neural robotics basically it's
a device that can monitor and decode
electrical signals from an individual's
brain and convert that information to
some type of machine control so
essentially again if you're taking some
type of signal here from the brain we're
going to talk about that in just a
moment digitizing it processing it and
then analyzing it such that you can
interpret the intentions of this brain
and convert it to some type of device
control whether it be something simple
like controlling the cursor on a screen
to something more intermediate in
complexity such as controlling a
wheelchair or some other type of machine
to even controlling your own limbs or
some robotic limbs and the idea is and
the reason you know I come at this from
a neurosurgical standpoint is that for
people with spinal cord injury stroke
neuromuscular disorders who have severe
motor impairment by accessing the brain
too
we can potentially create a shortcut
around that disability and give them and
an ability to interact with their
environment in a better way and in a non
biologic way so kind of before I start
kind of talking about kind of some of
the ways that we do brain computer or
faces I also want to talk about there's
different types in broad categories of
different types of brain computer
interfaces there's an input BCI where
basically you're taking something from
the outside world and putting it in as a
perception that's called an input BCI
now alternatively and kind of what I'm
going to focus my talk on is you have an
intention basically your brain wants to
do something and you convert that to an
overt real-world action such as moving a
cursor on a screen okay now that is when
we talk about intentions where we have
several different brain signals that we
can utilize to infer those intentions
and again just here's a little brief
anatomy lesson so again here's kind of a
side view of the brain frontal lobe
temporal lobe occipital lobe parietal
lobe and the majority of neurons are
actually on the surface of the brain
that's called cortex and over over top
of the cortex we have kind of a leathery
membrane and that's called the dura the
long term is called dura mater which is
Latin for a tough mother and I don't
know why they call it that but anyway
and so then outside of that is the skull
and then outside of that is the scalp
now again there's we can put our
electrodes in different spots to get
signals from the brain and they each
have different characteristics first off
and kind of ones that you may be
familiar with we can put electrodes on
the scalp
that's a eg or electroencephalography
relatively non-invasive and easy to use
we can put electrodes in the brain and
these are called single unit systems and
these are hair like electrodes that
monitor single neurons firing
and again these tend to be more invasive
or we can go somewhere in between and we
can put electrodes on the surface of the
brain and that's called electric
choreography or ACOG for short and again
that's intermediate and it's
invasiveness so first off let's start
off with a single neuron and talk
that and work our way up so in terms of
single unit systems kind of the way
these work is again we're putting single
small hair like electrodes into the
brain surface and we're having these
small electrodes again this is an
example what's called a Michigan probe
which is monitoring the action potential
firing rate of a neuron basically what
that is neurons will change their
voltage and kind of in a span of around
five milliseconds that's called an
action potential basically you know it
activates and discharge is an electrical
signal down the axon and now the rate
and the timing of these action
potentials actually give information and
that's a I kind of almost get it from a
nervous person I just called the Morse
code of brain activity basically the the
dot dot dot dot of activity can give us
information about what that the
information that neurons trying to
convey and so some of the earliest
examples of this were presented by this
guy named George Ovilus in 1982 and what
he did is he did what's called a center
out task and what that is is here's this
little particulated arm it's called a
manipulandum and basically what he did
is he had a monkey manipulate that
manipulandum from the center of a table
to a peripheral target kind of see
around here and here's kind of the
tracing that the monkey made and you can
actually see the little monkey arm right
there and concurrently he recorded
neuronal activity while the monkey was
moving his arm and let me show you an
example of that and so what he found was
that depending on the direction that the
monkey moved his arm affected the action
potential firing rate so that if he
moved it up in to the left in this
example there's a larger action
potential firing rate than if he moved
it in a different direction that there
was a lower action potential firing rate
and you're seeing that graphically here
and so each of these lines again that's
technically called a raster plot is
again remember that little dot dot dot
and that each of these dots represents
an action potential fire and again each
of these lines represents a different
time that the monkey moved his arm and
so what you see here is when you moved
up and to the left we see that there's a
high degree of firing
and to the left here but when you moved
down to the right much lower and when
you actually regret yep there's a
question in back oh roughly anywhere
between 20 to 60 20 per second that's
right yep
so I'm sorry
oh that's sorry I thought I said it's me
I said a question so in any case so when
you try to regress that activity to
align it regresses to a cosine tuned
curve and the the term has been coined
since that time cosine cosine tuning so
that a neuron essentially has a
preferred direction that it likes to
fire and so if you put if you record
from a number of different neurons you
can actually predict where that monkey
wants to move his arm in
three-dimensional space and so I'll show
you a graphic representation of that so
this is actually data taken from a real
monkey and around I think 16 it's been
recorded from around 60 neurons the
length of the yellow line represents the
firing rate how fast is the neuron
firing and the direction is the
preferred direction of that neuron and
so essentially it's a kind of a summed
vector and what you'll see here is that
and this is kind of the actual movement
of the monkey arm again all in a
stylized fashion here but basically it's
it's a vector sum of that direction of
those multiple neurons that actually
perceives the actual movement of the arm
movement and really this is one of the
first demonstrations that we can take
brain activity and decode it so that we
could know the user in this case the
monkeys intentions before they actually
did it by around 60 milliseconds okay
and now now since we know the intention
from the brain it doesn't mean that we
actually have to control an arm we we
can actually control
I think we're getting some noise here
please mute your microphones if you're
on the bc bridge Thanks
we good okay great so what so in any
case it doesn't mean that we actually
have to control a real arm we can
control a robotic arm
as long as we know those intentions we
can take that and we can control
something else and so let me show you an
example this is this is work done by
Andy Schwartz at an University of
Pittsburgh but again there's a implanted
array in the monkey's head here and for
privacy reasons we've covered his eyes
for healthcare privacy these days that
we have to protect his identity so that
his monkey buddies don't know that it's
in participating but in any case so here
you can see that he's controlling this
robotic arm from around 16 neurons so
that he's able to feed himself
again it doesn't take a lot of
imagination to think that if we can
decode this information that and we can
control some type of external robotic
for people with severe motor
disabilities this could potentially
augment their capabilities okay now
there are however problems I'm not
saying they're never going to be sold
but currently one of the problems with
micro electrode arrays is that after a
period of time scar forms around these
electrodes and it's due to a number of
different reasons have been proposed
micro motion of the implant the brain
actually moves a fair bit in our heads
basically you know as we walk down steps
or really anything that we do our brain
moves by several millimeters it's kind
of just look if you ever see it you know
it basically it's kind of like cold
comments like cold jello so kind of it
kind of jiggles a bit there's a foreign
body response that the neuronal
processes tend to retract from the shank
of the electrode as well as penetration
of the microvascular leads to
inflammation and the formation of a
chaotic sheath or a scar around the
electrode so oftentimes we're losing
signal after around six months so from a
neurosurgical standpoint I'm not going
to want to be putting a new one in every
six months surgically and that's been a
problem with these because you get very
good control but the durability of these
constructs right now is a problem so
then kind of kind of broadening out now
if we look at these signals and
populations we can also get different
types of information and they have
different features to them for from a
brain computer interface standpoint so
now we're going to talk about econ and
EEG basically again larger populations
of neurons acting in synchrony and again
we're either getting putting our
electrode
the scalp we're putting on the skull and
we see again we're classically known as
going to brain waves you know kind of
oscillations of signals and we look at
how those oscillations change relative
to various cognitive activities and so
when you actually see kind of the a
signal traits whether it be from EEG or
from E cog it looks relatively chaotic
and I guess the best example that I
would give you here is that can you hear
that no no that well actually it's
several different things playing all at
the same time you know but together it
sounds like noise and so we actually
have talking to the mic here Beethoven's
Symphony number one and see again it
more subtle sound kind of hard to hear
Mozart's Symphony number 25 and G
little bit folder and then something a
little bit more modern with thievery
corporation holographic universe again
different rhythm different and each of
these rhythms has an intrinsic kind of
value to it it conveys information and
has an intrinsic order but when compiled
together
sounds like junk and noise and so we
have to be able to separate those
different rhythms to get the most
information out of that and the way that
we do it is that again once again
looking at these this initial junky
rhythm here is that it's really the
supper is kind of superposition of a
number of different frequencies and each
of those frequencies really represents
different neural sources that give us
different information about cognitive
intentions and so broadly some of those
sources are these lower frequencies and
again they have a number of different
names theta alpha mu beta and higher
frequencies above 30 Hertz which are
collectively referred to as gamma
frequencies and again these low
frequencies tend to represent what are
called flama cortical modulation
basically how deeper brain structures
modulate cortex kind of it's almost like
the spotlight that coordinates different
areas of brain working together
cortney's gamma rhythms these higher
frequencies are thought to represent
more focal constrained cortical circuits
now there's a number of different ways
that we look at these these rhythms we
look at how they change in their power
or amplitude and that's kind of a and
we're going to return to this quite a
bit
phase you know how do different areas
coordinate between each other
band-limited covariance how did
different areas change their power
together and phase power correlation how
does one frequency affect the power of
another correlation okay now one of the
most common ways that we as our signals
that we use for brain computer interface
control is a changes in amplitude or
changes in power again here's kind of
amplitude versus frequency this is from
an EEG signal and this right here is non
active sit scenario and this here is the
active scenario where basically
somebody's
imagine motor movement and you can use
that activation versus rest the to
distinguish kind of two different
targets so for instance you could if you
wanted to be something to move to the
bottom target you imagine moving your
hand it creates a suppression in the
amplitude and what you see here and that
will move you towards the bottom target
and if you basically don't do that
action you go to rest that will go
towards the rest rest state and that
will move you in a different direction
let me show you an example of that and
so now what you're going to see is
here's a guy with an EEG calf and he's
you just can imagine motor movement and
when he wants it to go in one direction
he imagines moving his hand when he
wants to go to the other direction he
just basically sits and rests and he
doesn't do anything and the
distinguishing between the active and
the rest scenarios can allow him to move
a cursor in a very simple direction
again very simple control but it's a
demonstration that you can use your
brain signals for in a non-invasive way
to achieve simple levels of control
now again EEG also has problems because
these electrodes are on your scalp
they're really susceptible various forms
of noise noise whether it be from your
body such as a EMG kind of muscle
artifact or from the external
environment a noisy
light bulb for instance and so here's an
example looking at a raw trace from an
EEG and somebody blinks their eyes and
you can see how much it distorts the
signal afterwards and the problem with
that is ultimately that makes the
control erratic even if it's simple you
want it to be consistent and so and
that's been a problem with the EEG
derived control systems and that really
brings us to you know what is in my
opinion kind of a happy medium in
between and that's electric or
topography basically again signal
acquired from the surface of the brain
and I'm going to talk a lot about econ
its context of gamma rhythms that we
have access to these high frequencies
and that gives us some very unique and
important information about calling
they're processing and so again our goal
is at least from a neurosurgical
standpoint to create a brain-computer
interface that will that facilitates the
control of X external elements in the
world and what I've told you about so
far is EEG
limited degrees of freedom it functions
somewhat erratically and it takes a long
time to train to use these signals so
oftentimes for one one one dimensional
and two dimensional control it can take
up to anywhere from weeks to months to
achieve some level of functional control
single unit systems again we've seen
that they can achieve a high level of
control but right now there has been
problems with the durability of the
signal the electrodes will cancel out
and that's where ich odd may be the
sweet spot for the signal the signal is
much more robust than EEG millivolts
versus micro volts with EEG has much
better regional discrimination meaning
that you can get find independent
signals much closer together and also
because you're not penetrating the brain
the electrodes will look much less
likely to have problems with scarring
and so one thing that I'm definitely a
highlight in addition to the space and
the time is again this access to
frequency this these higher frequencies
give us a lot of information about
cognitive intent which I'm going to walk
you through both in regards to motor
function as well as speech function so
once again EEG here's what you see
basically can the low these low
frequencies upwards to around 30 to 40
Hertz and again we refer to it often at
time as a low frequency band and when
cortex becomes active power decreases
and here you can see it's broadly
represented over cortex when you look at
eeka we can see much higher frequencies
and it tends to be anatomically
constrained so much smaller area of
cortex becomes active with these high
frequencies and that's important I'll
show you an example of that in a more in
a moment and once again when cortex
becomes active at the high frequencies
the power increases so it's different
from between the low frequency band in
the high frequency band now the model
that I use to study this is again it's
not like you can just kind of grab
something off the street and put
electrodes over the surface of the brain
you know that that might run into human
studies violations so the context that
we study this in is in patients with
intractable epilepsy essentially there's
a certain subset of patients who have
intractable seizures who require the
placement of electrodes placed over the
surface of their brain to identify where
their seizures are coming from and they
they usually are monitored for upwards
of around a week and so that once we put
their grid in
this is an example of the electrode
array that I've implanted here here you
can see an x-ray with the electrodes
over the surface of their brain that
kind of they're monitored for a week to
essentially have receivers but also do
brain mapping and during that time it
allows us a unique opportunity to test
them so we can have them do various
cognitive tasks we can have them do
brain computer interface experiments
while they're essentially waiting to
have seizures and then once they have
their seizures of they're then gonna go
for surgery get the grid taken out and
have their definitive surgery to remove
that seizure focus okay but also again
as I said it really gives us unique
access to human cortex but also unique
access to human cognitive intentions
because certain things like you know
human language really doesn't exist in a
motor and a monkey model for instance so
now let's talk about some of the high
frequencies and kind of how they're
important so one of the things when we
first started doing a BCI experience
once again we did very similar stuff to
a eg we basically took the raw signal
looked at the power versus frequency we
looked at for changes but now we looked
at the changes in high frequencies again
because we did access to those and again
we looked at an active condition versus
arrest condition and we used the the
difference between those two signals to
control a cursor in one or two
dimensions now the nice thing is since
we were able since gamma rhythms tend to
be much more cortically constrained
meaning that so for instance this is an
activation here associated with the
tongue movement here is an activation
associated with the hand movement again
one thing I want to point out is that
regardless of whether you actually do
the movement or you imagine doing the
movement so this is somebody imagining
moving their tongue this some of you
imagine moving their hand the signals
look very very similar so that and
that's important from a BCI standpoint
because you don't want to actually have
to do your movement especially if you're
paralyzed to create the signal to
control the device right so kind of real
and imagined motor physiology are very
very similar and so now since we can
separate these areas topographically
meaning kind of where and cortex we can
use those different signals for device
control and let me show you an example
of that so here's one of our subjects
he's hit he had surgery several days
prior and and it's a little bit bright
in here so it's kind of hard to see the
cursor moving but basically he's moving
the cursor towards the red target when
he hits it it turns yellow
and this is you know roughly after
around 20 to 30 minutes of training he
really kind of knocks the lights out and
the interesting thing is when you talk
to him again he's using real he's using
imagined hand movements an imagined
tongue move to make it go up and down
left and right and when you ask him
after a while he says well what are you
doing here like well I just wanted to go
up and I just wanted to go down so it's
almost kind of like training wheels you
know that you give him some cognitive
tasks to train on but then they just
kind of figure out how to do it on their
own and again for both one dimensional
control and two dimensional control we
we achieve control really quickly using
these high frequencies really effective
control and around 20 to 30 minutes
which for us was very exciting was at
the time
EEG again for one-dimensional controls
taking several weeks for two-dimensional
controls taking about you know several
months to even other words as long as
two years so 30 minutes pretty good now
he got bored actually with just doing
kind of you know controlling a cursor in
the two dimensions so you actually when
to place in video games and this is
actually a nice example so what you're
going to see is now he's using actual
movements remember it doesn't matter
whether you actually imagine and you're
gonna see him he controls now this is
actually demonstrating my age I remember
Space Invaders but so he's controlling
this little cannon to shoot the the
different spaces that he controls
whether it goes left and right you can
kind of see matched up to his hand
movement here again we're just decoding
the movements from his brain and as an
aside he has the Guinness Book of World
Records highest score cheat on a video
game using his brain there's not that
many people have played so it's pretty
easy yet but so anyway so but again it
doesn't matter whether you're actually
doing it or imagining doing it and
here's an example where he's now using
imagine hand movements playing a video
game and he got roughly about the same
score
now again once you have a control
feature kind of what do you want to
control it becomes an app right whether
it's a cursor on a screen playing space
invaders or as to show you another
example here's one of our subjects
controlling a robotic arm you can see
can you know it opens and closes when
the robotic hand opens and closes when
he opens closes his hand and so we did
this in a little kid he thought this is
the most fun I was and he had just
watched the movie Terminator so he loved
it so but one of the things I've shown
you is kind of our early stages was
using activation versus rest right an
activation a task versus a rest test get
control and one of the things that we
became very interested in is can we go
beyond just cortical activation can we
start to get at true motor intentions
and so this is an example here and we're
looking at the data from a single
electrode over a motor region and if you
remember the monkey test the center out
task well now what our monkeys are not
our monkey our patient is doing here is
they're doing a similar task to the
monkey using a joystick they're moving a
cursor from the center of the screen to
a peripheral target on the screen and
what we see here is what's called a time
frequency plot frequency here time here
and the color you see is an increase or
decrease in amplitude red as an increase
blue as a decrease and when he in this
example when he moves to the left we see
an increase in amplitude in this
particular location roughly between 17
163 Hertz whereas when he moves to the
left it goes down so we were able to
distinguish whether we wanted to move
left or whether you wanted to move right
from the single electrode okay and when
we put a number of different electrodes
together we could actually start to see
very similar tuning curves kind of from
what you saw with the single unit system
so on the micro on the single neuron
level as well as in the large cortical
population level we could start to see
tuning to given directions based on
different cortical populations and now
I'm going to show you an example of that
and so here's this is kind of a busy
kind of little slide movie here but this
is kind of you're going to see cortical
changes associated with horizontal
movement and cortical changes associated
with vertical movement and this
basically changes from
the same brain but we split them up to
make it a little bit easier to interpret
and what you can see appear here is the
actual movement in black of the person
moving a cursor around at the joystick
and the predicted movement in green and
what you see is that it's certainly not
perfect but it's pretty good and that we
can actually predict kind of where that
person wants to move their cursor based
on their brain signals alone from these
larger cortical populations so using
these large cortical populations we can
infer intentions from the brain surface
so in kind of some more recent studies
it's not only can we do kind of you know
proximal arm movements like you're
moving a joystick but actually people
have recently just shown that you can
also individually separate out
individual finger movements and this has
been demonstrated in a couple of
different studies now so we can we can
now know whether you move where what
direction you want to move and which
fingers you want to move using econ
signals so again the nice thing about
our model is we can start to do stuff
that the monkeys can't do such as speech
and so we're starting to look at kind of
what is some of the coracle physiology
associated with the different components
of speech because what I've shown you so
far is kind of this continuous style
move moving a cursor around in space and
one of the things that we're very
interested in is kind of just what we
don't write a paper using our mouse
right we have we have different
abilities to select things we have
choices and that's our keyboard and so
we thought that speech might be a unique
physical or physiologic substrate to
give us additional control features for
discreet selection and so if we can just
get one phoneme for instance or one
component of speech out reliably that
could give us a click function if we can
do multiple phonemes or different
components of speech that could
potentially give us multiple choices or
right and left click for instance and if
we got really good and we can
discriminate multiple different phonemes
perhaps we can even create a cortically
based language where somebody could
directly communicate with a computer
using their imagine speech alone and so
again I'm going to show you an example
here again we're returning to our time
frequency plot frequency on the y-axis
time on the x axis here and this is the
grid position on one of our patients and
what you're going to see here is us
airing the cortical activations between
different phonemic classes of words we
had them say words like that had a goo
an e type of words in them so feed bad
food bad something like that
and so an a versus you here produces a
specific frequency band change in this
particular electrode here that wasn't
there when they said ad verses are in a
different frequency change here in this
particular location versus zoo again a
different frequency change in E versus a
and a different frequency change and we
can now use those different signal
changes between these different phonemic
classes of words that can allow us once
again to perform some device control and
here I'm going to show you an example of
one of our subjects again in this
particular example she's using around 75
to 100 five Hertz and she's using
imagined she's imagining ah or imagining
do to control whether the cursor goes
left to right okay and so now and when
you ask her what she's doing she's
saying to the left boo to move it to the
right and again this is without any
training her performance essentially was
kind of between I think 97 and 100%
right off the bat so we can use speech I
think this is her one mistake here kind
of include that for honesty sake so that
speech does become a viable option now
the interesting thing is that when we
looked at that was a single electrode
where we used a single frequency band to
distinguish between AA and o and that
very specific example but the
interesting thing is that these
different frequencies really can give us
a lot of different information about
what they're doing and so now I'm going
to show you another example of that
again this is kind of a busy slide this
is what's called a kind of a log plot of
the power but basically so okay this is
all data taken from a single electrode
kind of over kind of a speech area here
and what we see is that when they were
hearing a specific word that we saw an
increase in the frequency band at around
100 Hertz which wasn't there when they
were reading the same word when they
were preparing to say that word after an
auditory cue we saw a broader frequency
band here but quite a different response
when they were preparing to say the same
word after reading
first again down at 100 Hertz and up at
roughly 250 Hertz and then when they
spoke the word we see this broadband
change here but it's quite different
again they're speaking the same word but
they have different frequency changes
between whether they heard it or whether
they saw it so we can infer a lot of
information about kind of what's going
on from a very small location so these
electrodes measure 2.3 millimeters in
diameter so we can get a lot of
information just by looking at the
different frequencies so then the
question is how small can we go
you know how light and so we started
implant them in addition to our classic
clinical arrays we started putting micro
grids in and so again here's just an
example we put a micro grid over motor
cortex and here's its size relative to a
penny right there and there was 16
electrodes under space one millimeter
apart and we measured the EMG or kind of
motor signals from their arms as they
would flex and extend their wrists and
so kind of what they would do is again
here's kind of the grid here's the set
up and they would see a screen that
they'd you know move your right hand
they would flex and extend their right
hand move your left hand they flex and
extend their left hand and we wanted to
look at this how these frequencies
change relative to these distinct
movements and what we found was that
contralateral meaning the the arm
opposite side of where the micro
electrode array was had a specific
pattern for flexion extension which was
quite different from ipsa lateral for
flexion extension so from this little
four millimeter by four millimeter area
we could figure out whether you're
flexing extending both arms or excuse me
both wrists that's a lot of information
out of a small area of cortex and we
also did that for in a different patient
and this is an example of a microwave a
microarray that we implanted over speech
again we wanted to distinguish ah versus
ooh and again a these are these
electrodes are about 70 microns spaced
one millimeter apart we see very focal
changes around 400 Hertz that help us
distinguish whether the person was
saying ah versus ooh now let me show you
an example of her using these signals
for control
if you can hear that so again we can
achieve this control from a very small
area of cortex I think oh well if I go
back to that okay yeah no no we uh we
basically placed it on an anatomic
coordinates basically but part of the
problem is we know when I'm putting
these grids in the the patients are
under anesthesia so I can't use
stimulation to find out where their
speech area is so we did under under
anatomic stereotactic coordinates so
actually they go back here for a moment
so now the question is okay we can use a
small area of cortex now how invasive do
we need to be because and I'm going to
return kind of to a brief anatomy lesson
here is again we have our scalp here
where we get our EEG we have the skull
and then we have that membrane that I
talked about the dura can we put the
electrodes above or below the dura
having it above the door there is some
advantages because again the dura is
again it's just leathery membrane that
really is kind of our last barrier from
the outside world so that if we could
put these sub or excuse me epidural II
but that would be a much less invasive
case than if we put them subdural E
neither is incompatible with the
clinical approach but epidural certainly
would be that'd be an outpatient
procedure for instance and so what we
did is we have we had specialized arrays
where we would put one above and below
the dura over the same gyrus and this is
kind of an example here here you see the
array and we would basically look at the
the baseline brain signals to see how
different they were and basically what
we looked at was the magnitude of the
signal as well as the correlation
between the electrodes we went to
Bagnall to see the magnitude of signal
to be high and we want the correlation
to be low and so here we're seeing is
again here's the voltage of the signal
we have as our subdural electrodes our
epidural electrodes these are the micro
electrodes and then the macro electrodes
these are the other 64 electrodes
terms of the signal signal size and what
we see is again subdural and we first
saw this you know looks like it's a
little bit better as a higher signal
than the epidural but both are about in
the same range as the macro electrode in
terms of the absolute size of the signal
now but it gets a little bit more
interesting in that when we looked at
kinda low frequencies versus high
frequencies once again low frequencies
tended to have an improved voltage
compared to epidural signals here but
against in about the same range but once
we get to the high frequencies they were
roughly about the same so that's that
was encouraging because again if high
frequencies were kind of where the money
are then that we could we could in terms
of the signal size potentially use these
epidural signals we also looked at
correlation and again kind of a similar
trend in that the low frequency seem to
have a higher level of correlation here
but once we got down to the higher
frequencies that the correlation is much
similar subdural is still a little bit
better than epidural but they were much
closer here in terms of D correlating as
we got up there so that that again those
results are stills I'd say somewhat
preliminary so I think we have to do
more of that but if it was epidural we
can imagine relatively easy
straightforward surgery yeah I'm sorry
didn't hear that question that's a in
general we would be looking for either
whether it's subdural or epidural we'd
be looking at the crown of the gyrus not
the sulcus for instance that and that's
true for all econ signals and in part
because let me see if I can show you an
example here
in general the sok is this what you're
talking about kind of the deep portion
you the the there's an actually another
membrane which I kind of didn't talk
about here but there's a thin membrane
that really kind of glosses over here so
you really never get electrodes into
this deep sulcus right here into those
valleys in general you're looking at the
the crown of the gyrus for any ACOG
signal
does that answer your question forward
in microgrids well for a micro grade
you're probably looking at around one
kind of to one to two millimeters worth
of cortical signal for the macro grid
you're probably looking at a little bit
that the size of that electrode is
around two point three millimeters so
you're probably looking at something a
little bit bigger than that around three
or four millimeters worth of cortical
change does that get at your question
okay
so that's kind of going forward here for
a sec and so again I'm actually gonna
walk you through a virtual procedure of
what this would potentially look like so
again with the when we're putting the
grids and that's a big surgery excuse me
we're taking a big wind to a bone off
we're putting a large grid array but
again if we're using micro grids and we
can imagine what's called a burr hole
and again I basically doctored up some
pictures of a procedure where I made a
burr hole for a different reason but
anyway we're making a linear incision
here that takes around three minutes to
shave we're doing what's preparing the
area for surgery
we're drape it up that takes several
minutes then we make the incision again
that takes about five minutes to cut
there's the surface of the bone there we
use a drill drill a hole and that's the
dura down there at that point we would
kind of screw in our implants which
would be kind of like putting screwing
in a thimble into that hole and then we
would close up and I'll take about ten
minutes so the surgery would take
program 30 to 45 minutes to implant a
brain-computer interface so what I think
we can expect from a near-future
standpoint is again if we can get I
think we can reliably get
two-dimensional control and with the
utilization of speech certainly we can
get a click function so in the questions
well what kind of functionality can you
get out of two-dimensional controls with
a click well I'll show you an example of
two-dimensional control with the click
it's a tonight I phone or an iPad
basically kind of moving around in XY
space and then tapping
so really it becomes what what kind of
app do you want and so I think for the
for people with severe motor
disabilities whether they be
quadriplegic patients patients with ALS
that you can imagine simple
communication devices controlling a
Windows operating system which can
potentially substantially facilitate
their lives now but I really do think
that is just the beginning and that I
think we're really starting to see that
a lot of changes and they're happening
rapidly and I think that it's kind of
the what I call the tip of the iceberg
of something kind of more substantially
this idea that we can decode information
from the brain and that that has larger
and more substituted implications and
this idea and I kind of draw an analogy
to mapping out the human genome it's an
it's a problem of information it's a
problem of scale but similar to the
genome just as basic all biologic
diversities governed by four nucleotides
that there are some simple rules that
govern how information is encoded into
cortex and we're just starting to learn
those well I certainly don't know that I
don't even think we know all four
nucleotides of brain signals but we're
starting to learn that they exist and
we're starting to we're starting to
decode that information and I think the
implications are just as big and I think
that it's similar to kind of you know
previous kind of grand projects in
science with a B lunar exploration which
led to kind of computers internet jet
propulsion and modern materials kind of
like we see with the human genome today
that we really start to see an unfolding
of a lot of different things happening
whether it be novel molecular
therapeutics and diagnostics for cancer
and infections biotechnology
bioinformatics individual risk
assessment and personalized medical
treatments I think we're gonna see
similar grand changes occur with
decoding of neural information that you
can see you can imagine seamless human
machine interactions more efficient
person-to-person communication treatment
and understanding of a number of
different psychiatric neurologic
disorders and fundamentally if you can
understand how the brain processes
information that you can also
artificially recreate it and let me
finish kind of with a quote from a
newspaper from 1972 phase seven quarters
later they were having extended volleys
and a constant pong noise was attracted
it was attracting the curiosity of
others at the bar before closing every
in the bar had played the game the next
day people were lined up outside and
ecaps at 10 a.m. to play pong around 10
o'clock that night the game suddenly
died the Machine coin container was full
so people were lining up out of out of a
bar to play pong in 1972 and these kind
of you know thirty years later we have
video games like this which basically
take for granted we don't I don't this
is certainly not going to have anybody
lining ups outside of a bar for it's
just a part of our lives and I would
argue kind of here we are again in 2006
where people get excited about playing a
video game with your brain and I think
that we're gonna see an equal level of
dramatic change over the next several
decades that are gonna alter the way we
interact with machines so again thank
you very much for taking the time to
listen to me talk and I'd be happy to
answer any questions we've set up a
microphone in between disease okay two
questions first have you measured bit
rates that you can actually get it's a
great question so right now the highest
bit rate of any human BCI is around 50
bits per minute so it's it's pretty slow
so if I think away from like people who
are severe disabilities but people like
like you and me I have some do but you
might not I think we all seems to be
this trade-off between invasiveness
versus good rates sometimes that's Korea
we won't be that I question whether we
can actually really surpass it which is
if you stay non-invasive there might be
a maximum bit rate we might be able to
get and that's it and then you have to
get closer and close into the brain to
the point that at some point you have to
make lasso the brain the electrodes I'm
actually might not be willing to accept
now that certainly may be true for video
games you didn't have to break all our
skull and get into it
because it becomes a cultural question
to be quite frank so for instance if you
could have a bit rate equivalent of say
moving a mouse and a joint or a joystick
which is today is around 300 bits per
minute if I remember correctly for a
mouse would people be willing to undergo
a surgical procedure which quite frankly
is
less invasive and less risky than
current surgical procedures whether it
be a deep brain stimulator for the
treatment of a trauma or less risky from
a than a breast augmentation a would
rather go to price and get a joystick do
the same thing or who would undergo
surgical procedures well but let me
finish let me finish my art let me
finish my pitch
so again if you have kind of if you
could say get 300 bits of control with a
small implant the size of a thimble kind
of would you know what that could allow
you to control your environment that
could allow you know it's not just
simply controlling just like moving a
mouse is not just moving a mouse it
facilitates to control all sorts of
technologies you can imagine controlling
your environment you can imagine
controlling all sorts of things and
again 30 years ago kind of you know the
reason people got breast augmentations
is for the purposes of mastectomies so
culture has changed that has allowed
people to adopt those type of
technologies and I you can almost think
of this it's kind of the augment you
know for the Information Age so do we
want to do a poll I'd be happy to okay
well not yet so it's then it becomes a
with this his question is is he's
talking about augment you know would
people do it for an augment reason I
mean they could do texting and driving
at the same time
you could text at any moment in time I
see Astor raising his hand okay
okay fair enough you know so if you're
if you're implanting and then covering
up what are the what are the ways that
you're looking at to do that
communication through the through the
skull through the skin and what do you
see on the horizon as better techniques
so for instance so I think your question
is what would I see the implant looking
like in terms of its kind of nuts and
bolts like the end I guess I assume
you're not going to you know have an
actual wire sticking out of people's
heads you know that's what one would
imagine again it's gonna be a
microelectrode array with an integrated
chip that will have the amplification
capacity and wireless power and
transmission capability all in in that
kind of symbol sized format that can fit
within the hole of a skull so basically
what you would see kind of what I showed
you in that little clip that would be
the entirety of the device and that the
scalpel closed over and that would well
it would it would have battery power
that would be kind of inductively
powered probably in the evenings so for
instance like you know they they would
basically it's in their pillow and it
wirelessly powers that you know while
they sleep couple of questions in a
comment first one you seem to imply by
some of the things you were saying that
for people that have neurological
disorders white can't control their
extremities that the brain is okay and
the problem is in the nerves that
connect the brain to say the hand that's
right therefore if you could pick up the
intense then the brain is perfectly fine
that's exactly a week okay yeah that's
exactly right that's exactly right
second thing is all of the sensor
techniques that you described were
we're voltage sensors and I imagine that
the current is so low in the brain that
field effect pickups probably couldn't
work is that true or is there any or is
because that you know and it just has a
totally different approach to getting
the information out I'm not sure I
understood your question so basically
it's on the order of millivolts well if
there is a voltage change there must
have been a current change that's right
and anytime there's a current simply
where there's an Associated B field a
magnetic field and you can pick that up
if you're close enough or sensitive
enough you know like say I think I like
magnetic yeah yeah like a non-invasive
magnetic signature correct right all
right but I'm guessing that the reason
we it's really sort of FEM to Tesla it's
quite small and it's incredible
certainly do use em eg you know to look
at some of these type of signals the
signals are quite small and you have to
be in an isolated room so if you're in
normal environment you probably couldn't
use yeah that's what I thought yeah and
because of that I think it's probably
also true that the brain and voltage
sensors stuck in the brain are probably
fairly immune to EMI interference in
other words other fields know there's
there's not that much of a feel there
it's operating on these chemically
generated voltage swings and therefore
the light bulb interference that you
talked about is affecting the sensor but
not affecting the brain that's correct
that's correct yeah absolutely and if
something like this were to become
commercially you know available you go
down to fries and buy your implant at
300 bits a minute since I'm a drummer
I could become an octopus of a drummer
and just do all kinds of things that
more than I could do with just two hands
so I'm all for it I okay well all right
well first let's help the spinal cord
injury patients let's help that let's
help the stroke patients and write your
next my question is about the scaling of
the recording to the electrode technique
so thirty years ago people would record
from a few electrodes and I wonder how
what's the doubling time how many years
more would
to have double the number of channels
etc that is a good question I there's
right now we can certainly with I think
in our current form factor we could get
16 to 32 channels within kind of what
we're talking about the the honest
answer is you know again my background
tends to be more physiologic and and
neurosurgical I'm not sure I can answer
the question in terms of Canada rate a
change of amplifiers and sensor
technologies and how compact can you get
it in general I think since most of the
stuff moves pretty quickly and it's
certainly a kind of our amplifiers even
from five years ago have changed
dramatically so I think that you
probably could get increased number of
channels in there how many and at what
time it's harder for me to answer so
what's the past short history what was
the doubling time until now say it again
when was the so we at some point we
double the number of electrodes you will
look back how many years did it take us
to double to get to this point so well I
the single unit experience is the
longest that's around thirty years and
they've gone from single neurons
recording even back in the 80s they were
monitoring you know up to ten to thirty
now they can do they can certainly do 60
to 100 the issue is you know how many
electrodes do you need you know and so
right now it looks like sixty from my
kind of a single you know electorate is
enough 16 to 32 maybe a hundred would be
great from that service so I think you
know from electrode sensors we're pretty
good it's just more creating the implant
an integrated implant yeah hey this
question is about the cognitive load so
those patients are thinking really hard
about ours or is to do this and of
course we'd like them to not think so
hard about it and still have the
response and be able to drive and eat
sandwich and text all episodes yeah
looked into that cognitively yeah and
and the the honest answer is I think
that that's something that we still are
working on testing right now we're just
showing that we can get the level of
control
we want to but can cognitive load
prohibit or getting away I think I think
we still have to test it out okay do you
feel that the modality EEG II cog and
then you know the single electrodes do
you think if you became more invasive
you would overcome that easier or you
don't believe that limitation comes from
that I'm guessing but I think that's
likely only because for instance with
EEG you have to engage a broader area of
cortex to get the signal that you need
so if you're using a broad array of
cortex chances are that if you have
other stimuli or attending the different
Ang's it's going to disrupt that larger
swath of cortex my question is related
to his octopus drummer huh scenario that
how likely would it be for us to be able
to control forearms this is this
actually gets back to a question Astro
and I were talking about earlier how
many independent degrees of freedom can
you achieve so for instance if I
weakened and to give Astro credit he is
like well what if I you know created
this kind of totally artificial thing
that I'm in happier with my answer or as
an analogy what if I gave you just two
robotic arms could you learn to control
them in addition to your two arms I
think the psychic of physics of that
still needs to be tested I don't know
the answer so so far we have been
talking about being able to detect
signals from brain and you mentioned at
the very beginning the you know you
could have also feedback in the opposite
direction I'm just curious if people
right now are doing anything that would
be like they are into the future yeah
they are there's several sensory brain
computer interfaces that people are
working on kind of closest to the motor
field is kind of sensory feedback so
that you stimulate the sensory cortex so
that not only do you can say for
instance control a robotic arm but that
the robotic arm gives you feedback to
your brain so that you can feel what
you're touching alternatively people are
also working on visual prosthetics
stimulating occipital cortex or visual
cortex to create artificial visual
perceptions it's still again relatively
simple in that basically when you
stimulate a visual cortex you get what
are called phosphenes basically kind of
pixelated bright dots no formed images
yet but
enough pixelated dots that people can
for instance a blind person can
potentially navigate a room like that's
it thank you Eric yeah thank you very
much everybody thank you all for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>