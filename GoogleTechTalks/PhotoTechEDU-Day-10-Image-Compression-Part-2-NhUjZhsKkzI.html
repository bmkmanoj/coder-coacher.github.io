<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 10: Image Compression Part 2 | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 10: Image Compression Part 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 10: Image Compression Part 2</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NhUjZhsKkzI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome everybody to the second
half of my image compression talk
reviewing the basic problem we'd like to
represent an image using as few bits as
possible but we'd like to do it in a way
that doesn't give up too much coding
quality so some of the questions that
we've asked and will continue to ask are
how well can we expect to do in
principle or in theory how might one
compress images if we're willing to
accept some loss and then finally and
we'll get into that today a little bit
what should we do if we want an exact
bit for bit identical replica to the
original so today I'll briefly review
some of the main points from the
previous talk and then I'd like to make
some final points about lossy
compression and then move on to the
lossless compression so with lossy
compression we have to know how well
we're doing and it's not a perfect
measure but it's broadly used it's
unambiguous and it's useful within a
coding scheme to assess different
versions of parameter settings etc it's
called a peak to peak signal-to-noise
ratio so let's say we adopt the mean
square error measure such as what is
what P SNR is based on we could ask well
what's the best we could ever do in a
lossy compression setting and so last
time we mentioned that there's this
theoretical curve called the rate
distortion bound the important point
here is that if you believe in the model
that is we believe in the probabilistic
description of your source so now this
is a very abstract setting the source
could be an image it could be speech
samples what have you if you believe
that probabilistic model then and for
with respect to some meaningful
distortion measure D there exists a
theoretical curve so this is the rate ax
that's the number of bits per
observation typically and this is a
distortion axis the rate distortion
function tells you how you could do
using the best possible coder coding in
very long blocks of observations and
basically you can't be down here
so that's the theoretical result last
time we went over probably the what I
consider to be the main framework for
lossy compression it's transform coding
and some of the techniques that we call
by different names are actually closely
related to transform coding so it's
worth briefly reviewing we start off
with a block of input observations so in
the case of image compression this would
be the result of tiling the image into
non-overlapping blocks typically 8x8
blocks so we have a 64 a vector if we're
thinking on one dimensions or if we're
doing two-dimensional separable
processing or even non-separable
processing we'd have a 2d block we apply
an orthogonal transform which is
invertible and preserves energy we by
doing that we've compacted the energy
into relatively few components we can
then allocate bits to the components
that need the most we may or may not try
to transmit some information aside
information we reconstruct the transform
coefficients at the other end inverse
quantized them inverse transform them
rather and come up with an approximate
replica of the original so this was kind
of the thing we talked about most last
time yeah
oh so the question is do horrible things
happen if there are blocks that overlap
transforms where the kernels overlap and
in fact I'm glad you asked no good
things can happen people sometimes do
that on purpose and I think we may have
mentioned a little bit about that last
time but we'll get into more of that now
yep okay so one of the things we talked
about was a choice of transform we
restricted consideration to linear
transforms to in fact transforms with
the orthonormal basis we asked well if
we want to optimize the energy
compaction is there something we can do
that really explicitly optimizes that
and then under certain assumptions we
saw that the car horn and web transform
also called principal components
analysis is a way of doing that but
that's data dependent and if you were to
use that in a coder you'd have to then
have some means of synchronizing with
the receiver to use the same transform
and that would probably mean
transmitting all the information needed
to specify that transform so we saw that
the discrete cosine transform is kind of
a one-size-fits-all approach that has
pretty good energy compaction we took a
look at that also in the frequency
domain to try to interpret that in terms
of a spectral decomposition ah it means
when in the simplest setting the
one-dimensional setting if I have
adjacent samples all they could depend
on each other and if they depend on each
other in the sense that if one value is
higher than the one next to it is high
and when one value is low the one next
to it is low and that's positively
correlated you can imagine a situation
where they alternate that's a different
kind of correlation
okay so we settled on the DCT as a
reasonable choice of transform we also
talked a little bit about generalizing
it in the framework of filter banks but
then once we've got these transform
coefficients we've got the energy
compacted into a few components as
possible now we're going to actually
convert these transform coefficients
into bits so this is where the
digitization happens and we consider two
different approaches one where the
quantizers have a fixed coating at the
output so every quantizer output gets a
certain fixed number of bits and then
another approach is when the quantizer
outputs have a variable number of bits
so that's an entropy coded quantizer
which if you look only at the quantizer
performance and distortion versus rate
will work better than the fixed rate
scalar quantizers but more importantly
there's a simplification that happens if
we were to use a simple scalar fixed
step size quantizer with the same step
size for all of the sub bands for all of
the transform coefficients and be
careful when we code the output of the
quantizer that the entropy coder knows
that the different coefficients can have
different activity levels okay but we
use the same step size for all the sub
bands and if we do everything else right
then we get an implicit bit allocation
implicit in the sense that the
coefficients that have a lot of
information or a lot of energy that is
coefficients that are exercising a lot
of the quantizer levels will have a
greater output entropy and therefore
will be allocated more bits implicitly
by the entropy coder so if you account
for all of that and you add up how many
bits are being used by which
coefficients then we saw last time that
that's a nearly optimal thing to do and
that was the Lagrange multiplier
argument
they're to rate allocation you might get
or right so if I were the principal
component analysis right so the question
is how does the rate allocation here are
compared to what one might do if you're
using PCA so with PCA per se that that
speaks to the transform so that speaks
to what you're doing to allocate to
compress to compact the energy don't you
still have the same questions I mean
isn't it
right so the extra that you spend
upfront and communicating the PCA
transform um
do you get some of that back because you
don't have to transmit them as much
right that's a good question I don't
know okay we could interpret the
transform as a critically sampled filter
Bank and we observed last time that
first of all this is a one-dimensional
process and typically when we think
about applying this to images we might
do it separately so first do all the
rows and then do all the columns for
example so each of these filters has
some a convolution kernel and we're
gonna subsample the output of those
filters and we can think in the case
where the lengths of these filters are
less than or equal to all the
subsampling factor then this can exactly
express the application of that
transform matrix the block transform
it's interesting to ask though what
happens when the filter length is
greater than the subsampling factor or
equivalently in this case the number of
sub bands and in that case we have
situation where the kernels are
overlapping all to the adjacent blocks
and one version of doing that is called
the lapped orthogonal transform and it
was proposed as a means of one getting
better energy compaction but more
importantly to try to reduce the
blocking artifacts that people typically
get when they're doing transform coding
okay oh this is still by way of review
which is why I'm going kind of fast
through these first slides here we also
said that while besides thinking in
terms of D correlating a covariance
matrix or a correlated source we can
again think in terms of this multi rate
signal processing framework where we
have different filters processing
different parts of the frequency
spectrum so here's a hypothetical power
spectral density for a one-dimensional
signal it's a discrete signal so the
spectrum goes from zero to PI and then
it has its mirror image the idea here is
I've got a non flat spectrum which which
corresponds to a correlated source I
analyze that into these different sub
bands resulting in B I can then sub
sample these sub bands and the effect of
subsampling is then in the transfer in
the hizzy transform domain as to is to
map remap where 0 and PI are so it
effectively stretches these little
pieces to fill up the full band and
since we started off with a pretty small
band if the spectrum didn't change a
whole lot in that small band when you
stretch it to the full band it'll kind
of look flat and if you have a flat
power spectral density that corresponds
to an uncorrelated source this is a
slightly different argument from the D
correlation we talked about in the
carbonyl of transform in the sense that
there we were talking about D
correlating different coordinates in the
in lobster basin vector here we're
talking about D correlating coefficients
in the same position in your transform
but over different blocks so this is the
correlation over time if you like or
over space and they came a case of image
process
and when we optimized with the karun
left transformer approximated that with
the DCT the D correlation we're talking
about there was D correlating among
different transform coefficients so
there's two types of D correlation that
we're going on okay there are a couple
of different ways we can improve this
basic scheme one which we just mentioned
is to allow the convolution kernels to
extend beyond the block boundaries to
try to get at some of those blocking
artifacts that we sometimes see and
another thing we can do again appealing
to this idea that we want to compact
energy into a few components as possible
we could try to do a non-uniform
spectral decomposition where we put
greater frequency selectivity or
frequency resolution in those parts of
the spectral density where there's a lot
of detail where things are changing very
quickly one way of doing that is to
apply a simple tube and a critically
sampled filter Bank in a tree structure
so each block here each saw so the
analysis block here has incited a
low-pass filter and a high-pass filter
and then each is followed by a sub
sampling by 2 and then we cascade that
so at the end we'll see that this band
right here gets processed using only a
single stage so it's a high pass filter
followed by a sub sampling the lowest
band has several stages and each time we
apply the subsequent processing the
effect in the original time domain is
much longer so the equivalent filter
that was used in this lobe and if we
were to express this as a parallel
structure and we in their identities
that let us do that we'd see that the
equivalent filter if we were to express
this as a parallel structure would be
very very long would have a long spatial
region of support for the low low pass
filter and that makes sense because we'd
like to be able to have that low pass
filter then I'll be very active in the
interior of large regions but when we're
at edges we'd like to be able to adapt
very locally to those that
so we're not smearing the effects of
those edges far away from the edges
right so there's a practical reason for
doing this when we arrange a
decomposition in this form historically
I think that was first done with
quadrature mirror filters um back in the
70s and then later which only sought to
eliminate aliasing distortion to cancel
aliasing for one band to the other but
then later on people discovered that you
could have perfect reconstruction
filters and they tried to get smart
about the exact shape of the filter
kernels and their relationships from one
band to another put it all in a nice
mathematical framework to talk about
optimal localization in in space and
spatial frequency and and there's a
large mathematical theory around
wavelets that talks about that okay so
if I apply it separably in the
horizontal and vertical dimensions then
I could get a decomposition that looks
something like this so in the first
stage I do a low and high decomposition
horizontally and I do it vertically also
then I take the low low band and I
recursively subdivide that into an
arrangement looks like that okay any any
questions so far on this set up oh so
last time briefly we mentioned that we
don't have to we don't have to have the
same acuity or resolution in the color
domain so what people typically do is
they'll decompose into the luminance and
chrominance space like Y IQ or something
like that and they'll work real hard on
the luminance and then they'll do
something a little sloppier on the
chrominance and from a perceptual
standpoint that works pretty well
is it just arbitrary that you don't do
any recursive decomposition on the low
high and high low squares in this
picture oh right I and okay so the
question is is is there a reason why we
don't do any decomposition on these
components so I guess the I guess you
could argue that you might gain a little
bit from doing that but I mean
experience has shown that this is a
reasonable thing to do and ultimately
the decomposition that you're going to
use is the one that in practice gives
you the best energy compaction okay so
before I go on to segue into it I want
to reflect on transform coding a little
bit because there's a lot of little
pieces that go into transform coding
that kind of all worked together in a
nice coincidental way to make the thing
reasonable but when we really try to
look at its foundations I think it it
illustrates some some questions it
brings up some questions so basic
question does the transform make the
input more compressible and that's an
unanswerable question I guess in
principle it probably doesn't but in
practice it does so what do I mean by
that so given that the transform is
invertible it hasn't really changed the
information in the original input so in
particular the rate distortion function
of the input if if it exists you know
with respect to the right models etc
whatever that is at the input that's the
theoretical bound on how well you can do
well that's got to be the same right
here because you haven't changed the
information in the in the signal so in
light of that observation what is the
role of the transform right so as we
observed last time the transform
actually doesn't compress it simplifies
the subsequent compression and to say it
simplifies as always with respect to
something that you think is simple so it
simplifies compression if what you're
going to do is the things we talked
about and more than that it allows you
to kind of get away with the simple
processing we talked about the scalar
quantization etc and still do pretty
well so it kind of all works well
together it puts the energy in
predictable predictable places so you
can take advantage of that statistical
regularity that's latent in the illness
in the image but it brings up a question
well what if I'm willing to do something
more complex than simply treat each
transform coefficient as a separate
entity that I'm going to scale or
quantize right well that brings up the
general question of vector quantization
and I'll get back to the question of
applying vector quantization to a
transform coder let's talk for a minute
about vector quantization in the
abstract so we already have this idea of
taking your input image and tiling it up
into different blocks and then trying to
encode the different blocks separately
so let's hold on to that idea but now
we're going to take each input block as
a point in a high dimensional space so
if it's an 8 by 8 block then I've got a
point in the 64 dimensional space
I'm going to build a codebook of
representative vectors in that space and
to code an image every time I get one of
these input vectors I'm going to map it
to the closest entry in my code book
yeah right so a very simple minded
approach each block is independent we're
not carrying data from one block to the
next so of course in practice we'd put
in refinements that understands that
these different blocks came from the
same image and things like that but
conceptually we can imagine this
scenario where now if we have a set of n
reproduction vectors to transmit which
one it is I need a log base 2 of the
number of vectors if I'm using a fixed
rate code or I could do the same thing I
did with scalar quantization I could
have a lot of different representative
code book entries and it doesn't hurt if
I have code block entries where we're no
one put point will ever be as long as I
use the variable rate code which would
then on not allocate any bits to those
things because they never get used right
I mean it would have a long code word
for those things that never get used but
they'd never get used hence you'd never
pay that price
so as long as the entropy of the code
book output is constrained you're okay
so you can have a lot of you can have a
big code book okay so how do you build
one of these code books so let's say I
gather a bunch of images that I think a
representative of the type of images
that I want to compress so if I have
natural scenes at a certain resolution I
collect a lot of them if I have document
images I collect a lot of those I
prepare I tiled them all up so that I
have a lot of vector observations and I
then initialize the code book with some
fixed set of entries let's say I have 50
code book entries to start out with I'm
going to iterate this procedure then
first I take every training vector that
I observe and I assign it to its nearest
code book entry for each resulting set
of for each resulting group then so each
code book entry has its own group
I then recompute the centroid of that
group
and I use that as a new value of the
codebook entry right and then I iterate
I return to step two and I keep going
until this thing converges okay that's
called the Lloyd algorithm it's also
called k-means it's also used for
clustering okay okay so that's basically
vector quantization in a nutshell for
whatever reason it doesn't figure
prominently into existing standards
although it does seem to be used in some
proprietary schemes so unlike transform
coding vector quantization in principle
can approach the rate distortion bound
so if you were to just ask well how good
can transform coding be in the best-case
scenario in the best-case scenario
transform coding is ultimately limited
at least the version that we describe is
ultimately limited by the fact that
you're using scalar quantizers and
scalar quantizers aren't optimal even in
the cases where you've really even in
the case where you've really done a good
job of D correlation vector quantizers
can fill up space more efficiently with
representative code points scalar
quantizers are kind of stuck with the
rectangular lattice and vector
quantizers can use a fancier lattice
there's another advantage that vector
quantizers have which is that if you
haven't done that great a job in d
correlating well that's where they
really shine that you can really do well
because then you can take advantage of
the nonlinear dependence that may exist
between these coefficients okay so let's
go back to transform coding and let's
ask well why do we use scalar quantizers
after doing the transform instead of
vector quantizers
okay so the transform coefficients are
uncorrelated both spatially and across
frequency bands spatially because of the
spectral argument that we gave and
across frequency bands because of the
original argument behind principle
component analysis so if there's not a
whole lot of correlation the main
advantage of vector quantization is is
usually in the statistical dependence
between these things that make up the
vector and if there's not a lot of
dependence and there's not a lot of
advantage in using the vector
quantization over the scalar there's
still as these other advantages but most
of the advantages in the dependence so
we can therefore ask is there any role
for VQ in this type of coding and I want
to do a little thought experiment here
so here's some two-dimensional data it's
a scatter plot of x and y pairs and let
me ask first all so this is X and this
is y right and they're just these are
the points that are kind of observed
okay let's assume that successive pairs
are independent so each point is an
independent observation of this vector
source our x and y correlated in the
mathematical sense of correlation the
expected value of x mon it's - its main
times y minus its mean any takers yeah
right there they're not they're not
correlated they were logging on so does
that mean that I wouldn't gain anything
by coding them jointly so if I if I know
that X is 0.5 do I really not know
anything about Y I know a great deal
about Y right in particular Y had better
be either here or there so here's an
example of a contrived artificial signal
that has no correlation at all but has a
great deal of statistical dependence
nonlinear dependence and vector
quantization would work pretty well here
so going back to the transform coding
example for transform coding of images
do is there some phenomenon that's
analogous to this going on among the
transform coefficients and the way I
want to visualize that well let me just
quickly add the orthogonal linear
transforms that we've considered you
know with these orthonormal bases can be
thought of as rotations of the
coordinate system and so if I rotate
this coordinate system it doesn't help
me at all in this example ok so you can
actually visualize transform
decomposition in this case it's a sub
band realization so that multi-rate
signal processing realization and I've
done a 3x3 splitting where I use
bandpass filters followed by a sub
sampling factor of 3 and I'm just
putting them in the positions that
correspond to their space of their own
spatial frequency locations okay so
that's the low-pass signal and that's
the high-pass signal and I'm visualizing
these by scaling the amplitudes and
putting them on the range 0 to 255 so I
can see them so
if we look at this band for example does
that tell us anything about this band is
there some sort of dependence going on
there visually it looks like there is
and so it'd be nice to be able to
exploit that it's kind of interesting if
you were to actually estimate the
correlation by taking the average value
of the products of these coefficients
and averaging that over the right things
that you want to average over whether
you're doing spacial D correlation or
spectral any correlation you'd see that
that is actually very close to zero so
it really is uncorrelated in the
mathematical sense but in the intuitive
sense there's a lot of dependence so
this suggests that we could benefit by
joint coding of the transform
coefficients or in other words vector
quantization of the transform
coefficient okay so one way to approach
that that statistical dependence is if I
were to go back to this picture of doing
this hierarchical decomposition a course
to find decomposition or rather I should
say tree structure decomposition it's
been empirically observed that if there
are regions in a location in the
low-pass image that don't have much
energy that the corresponding regions in
the other bands also don't have much
activity so that's the basis of a
technique called embedded zerotree z'
and that's a way of exploiting the
dependence in this case in activity not
an actual pixel value but in the
activity of the local region across the
bands and so that has been exploited to
advantage that's that's sort of a
refinement on transform coding that
happened in the 80s that or early 90s in
this case that really made a difference
so if you want to look that up it was
described in a paper by Jerry Shapiro
and there are a whole bunch of things
that that paper did that I think are
really neat from the point of view of
this subject though that this talk
the most important contribution is that
have provided an effective way of
getting at this nonlinear dependence and
exploiting it to pretty significant
advantage
okay I want to almost there any final
questions on lossy image compression I
want to move to lossless by popular
request actually from last time okay
because normally the order in which you
to explore these would be reversed you
might first start with lossless and then
go to lossy as a generalization okay so
the problem now is that the decompressed
image we insist has to be exactly equal
to the original okay quick question why
would anybody want lossless compression
these are these CCD sent and what the
photons that happen to go on a CCD
sensor bin are they so sacrosanct that
we've got to preserve those precise
values at all costs or oh okay all right
and then I'm also gonna ask why might
lossless compression be hard but
seriously design maybe you were serious
medical imaging okay so maybe there's
some legal reasons that you want things
to be lossless yep you don't want
generation loss from subsequent code
yeah
where you don't know you don't know that
your subject interesting alright
so if you just want to eliminate the
variable so if you're doing scientific
imaging and you just don't want to have
to guess about whether the compression
is making a difference in your results
or not yeah good point
okay I'm going to give an example of
lossy compression again this is a a
bitmap from a document image a binary
binarized document image and that's the
original and that's what we would get if
we were to losslessly compress it now
for document images like this there's a
special technique of lossy compression
that tries to figure out what these
shapes are and replace these little
connected components by their idealized
shapes and if we were to do that we'd
get something like this so this is a
lossy compression version of the top
image so which the people prefer if you
have to read a document probably the
bottom one right so but lossless
compression would insist on doing that
and let's say this let me ask you a
different question suppose this is my
original image in case a and this is my
original image in case B from a lossless
compression standpoint which would be
easier to losslessly compress now
treating this as an original in one case
or this is original in one case B right
B is somehow simpler it has less strange
artifacts it looks more regular in some
way okay okay let me also ask why might
lossless compression be hard so here's a
seat fed scan of a thin page from a book
and you can see a lot of bleed through
from the other side if I look at the
least significant bit of that image so I
take the least significant bit and plot
it out as an image scale it from 0 to
255 that's what I get
right so if I do a lossless compression
of this image I've got a lossless Lee
compress everything in it including the
loss we significant bit and so some of
my code rate is going to this image and
if I were to then look at the two least
significant bits well it looks a little
bit better but there's still there's
still a lot of noise there and if I were
to do a good job in binarize in it then
this would be the thing that I start
with and I don't mind losslessly
compressing this guy I could do pretty
well oh and I probably from a practical
standpoint get most of the information I
care about okay so let's go into
lossless compression basics and the
basic idea is we've got some discrete
source and we want to represent that
source without any ambiguity or loss
using a few few as a few number of bits
as possible the way I'm going to do it
is the more probable images or more
generally observations get short code
words and the less probable ones get
longer code words okay that seemed
reasonable ah but why not use short code
words for everything there's not enough
of them good so let's try to formalize
that we'll also ask how long is the
ideal code word and actually I won't try
to argue it I'll just present it the
best code length is the minus log of the
probability with respect to some
probability model and on average if you
were to use that code length you'd get
the entropy as the number of bits that
you spend on average so then another
question is given a probability model
for images one that we believe so that's
kind of a hard thing to get a handle on
but imagine hypothetically we had one
well could we design a coder that
achieved the best rate possible with
respect to that probability model so
that's an interest interesting question
to ask so these are all questions here
except the best : 'the is kind of an
assertion rather than a question
okay so to get at the notion of lossless
compression from a slightly different
angle let's look at kind of a
statistical argument that says that
compression to a certain amount is
possible and so I want to consider this
idea of typical sets or typical
sequences the basic idea is that if you
have some discrete random variable and
you see a bunch of observations of that
discrete random variable if I were to
treat those observations in blocks so
now I have a new set of observation
would each of which consists of a long
sequence of observations in the original
space there are a lot of different
values of that block that are possible
so if the alphabet size is K and the
blocks that length is n then it's going
to be K to the N different possible
values for that whole block taken as an
aggregate of all the values of that for
that block that are possible a much
smaller set usually can actually happen
with any significant probability and
that's kind of one of the key ideas
behind how information theory in general
works and noiseless source coding in
particular works so this effect becomes
stronger when we let the block lengths
get larger and larger and the idea here
is that the law of large numbers is at
work here so let's consider an example
let's say I have a biased coin with the
probability of heads being point one and
I'm gonna flip it a thousand times
intuitively we would expect that I would
get about a hundred heads in that
experiment if I ask what is the
probability of the set of all sequences
of all all sequences I could have gotten
that have about 100 heads plus or minus
two or three or something like that okay
that set the set of sequences with about
100 heads has the overwhelming majority
of the probability if you were to just
add add up the if you were to do the
binomial distribution and simply add up
the probability masses associated with
each of all those values of of K if if
this is like an n choose K problem
also if there if you consider the set of
sequences with about 100 heads they all
have about the same probability namely
there's going to be point 1 raised to
the hundred times 0.9 raised to the 900
that's the probability of any particular
sequence in that set so I could use for
that set I could for that typical set I
could use a fixed rate code I could
simply take the log base 2 of the number
of sequences and the number of sequences
in this case that have exactly 100 would
be the log base 2 of a thousand choose
100 okay so in general I could apply
Sterling's approximation to the log base
2 of n choose K and I'll get something
that looks like that and that looks like
the entropy the end times the binary
entropy of n over K of K over N rather
okay so well what happens if I get a
sequence it's not in the typical set so
this this coding scheme works for
everything that's in the typical set and
in this extreme case everything that had
exactly hundred exactly 100 successes
but let's what some relax out a little
bit and let's say we'll use a similar
scheme for things that have about 100
head a head so it really does cover the
typical set pretty well well if I get
something that's not in the typical set
its probability is almost zero so I kind
of don't have to worry about it well
that's not lossless though so what do I
do
I could transmit a bit up front that
says whether or not it's in the typical
set if it's in the typical set I do
something really efficient like this if
it's not in the typical set I do some
sort of brute force method but the
probability of having to do that is so
small that it doesn't it doesn't hurt me
on average
okay so let's get back to this question
why not use short code words for
everything and the answer of course is
because there's not enough of them we
can try to make that a little bit more
formal by considering first a set of
prefix-free codes so for a code to be
useful it has to have some properties
one is it has to be uniquely decodable
that's pretty obvious it's also nice if
by looking at a long code bit sequence
if I can just look at it and go through
sequentially and sort of parse out
successive code words a code that has
that property is called a prefix-free
code no code word is a prefix of another
and this is an example of a prefix-free
code so normally if I put up a a listing
of code words and ask is this code any
good usually the answer is well you have
to tell me what the probability
distribution is before I can tell you if
it's any good but I claim that you can
tell me that this code is no good just
by looking at it
any takers for why
there's no series Arowana yeah exactly
um and in another way of saying that is
I don't need I don't need this last zero
if I wanted to have if I wanted to have
that then I might as well have the zero
zero one because there's no there's no
reason not to okay so I can any
prefix-free code I can represent as
terminal nodes you know you know tree in
an initial tree of some hypothetical
full tree so imagine taking the longest
code word and finding the depth of the
tree corresponding to that and then sort
of sketching in the full tree has a
hypothetical entity all the code words
would correspond to leaves in some
initial tree of that full tree so if L
sub I is the length of code word I then
this is a Kraft inequality and to see
the Kraft inequality note that for any
node here the contribution to the Kraft
inequality can be decomposed into what
it would have been for the descendant
leaves right so the less than or equal
to comes from the fact that we may not
use all of the leaves in this initial
tree so for example the 0 0 0
corresponds to this guy but it's sibling
isn't used so we could have just gotten
rid of it and use that node instead
while using this I'm actually meeting
the Kraft inequality with a strict
inequality so it's not quite efficient
so this is a pictorial representation of
why we can't have all of the code words
be simultaneously short because if they
were or than these things would be too
big right
okay so there are a bunch of different
techniques that people actually use in
lossless compression deciding which one
to use often requires detailed insight
into the physical nature of the source
that you're dealing with there's no
compression at all that's a special case
there's Huffman coding has anybody here
not heard of Huffman coding okay
there's sort of the inverse Huffman
coding where instead of trying to reduce
the number of alphabets for a fixed
number of input symbols you do the
opposite you try to maximize the number
of input symbols that get swallowed up
in producing a fixed number of output
bits that's all called Tunstall coding
there's also variable two variable so
remember in jpg the encoding step
involves exact scanning through the
quantized DCT coefficients run-length
encoding that zigzag scan and then
putting those runs into a Huffman table
and using a Huffman code for the runs so
that's kind of a variable two variable
all length coding then there's
arithmetic coding which I'd like to talk
about now because that's kind of a nice
general technique that works that allow
separating the coding from the modeling
problem and works over a broad range of
entropies and can be adaptive and all
kinds of nice things so the way to
understand arithmetic coding is to think
of taking the whole sequence of things
that I'm trying to encode so in case of
images I'm going to do some sort of
scanning of the pixels I have a sequence
now of discrete observations and I'm
going to encode that entire sequence
into a single number and I'm gonna send
that number and that's the arithmetic
code and I'm gonna go over the details
of that now so I start off with the unit
interval I'm gonna divide it up into sub
intervals I'm going to reserve a sub
interval for each possible value of x
one the width of the sub interval is
going to equal the probability of seeing
x one in that context once I see the
value of x one I'm going to then commit
commit to the sub interval that
corresponds to it
I then have some new subinterval that
corresponds to that value of x1 I'm
going to subdivide that one subinterval
for each possible value of x2 given that
previous value of x1 and I continue
recursively subdividing these
subintervals
well they get smaller and smaller as I
get farther and farther along in the
sequence and at the end of the whole
sequence I'll have a very tiny
subinterval that corresponds to the
probability of the whole sequence ok I'm
gonna have a picture that maybe just
reiterates that the key observation for
arithmetic coding is that this final
subinterval will contain at least one
number in it which when you represent it
as a binary fraction you know one bit
for the half splays another bit for the
quarters place etc can be represented
exactly using L bits which is about the
minus log base two of the width of that
sub interval so if you just look at the
spacing of binary fractions that can be
represented in L bits you'll see that
the spacing is such that we're
guaranteed to fall into one of these sub
intervals of that with okay we're
guarantee that one of them will fall
into the sub interval of that way okay
and we can build up we can we can
actually deterministically figure out
what what the number representative
numbers should be and there are a couple
of different ways of doing it
one proposed by IBM and another one
proposed all all later in the 80s by I
want to say folks in Bell Witten Cleary
and others I think I think they're from
Australia or New Zealand or someplace or
maybe Canada there are two different
ways that are popular for doing that and
here's a picture so let's say I'm
encoding the sequence BAC start with the
unit interval it's with this one I then
observe the value B I commit to the
middle interval the width is now P of B
I subdivide that I get an a I commit to
all the first third of that middle
interval and the width now is this chain
rule probability
subdivide that and I continue in now
icsc and I commit to that third of that
new subdivision so so and then at the
end when I'm done with my whole sequence
I'll pick a number in there that has a
compact binary representation and that
unambiguously describes which sub
interval I'm in and therefore the
decoder knows what all of the letters in
the sequence most of them
great question how do you transmit the
probabilities in order to do this are
they assumed to be known by the encoder
and decoder so the usual method is to
agree in advance that whatever
probabilities are used have to be
deducible from what's already been sent
plus any prior knowledge that's known
about the domain yep okay so there's
some practical issues importantly it
separates the modeling part from the
coding part so when I have a Huffman
code all the probability model has kind
of implicit in the code here's a way
that I can kind of really separate the
two problems I can say okay there's the
job of the modeling unit that whose job
it is to give me a sequence of
predictive distributions for successive
observations and the arithmetic coding
unit is going to take those probability
mass functions along with the actual
observations and it's going to produce
bits so it's a nice separation of
responsibilities arithmetic coding was
invented I guess in the late 70s there
were a couple of theses I think Frank
Rubin and somebody named Pasco and then
folks at IBM really tried to make it
practical and really made it practical
Glenn Langdon and you're Marissa Ninh
most of those patents now have expired
there's some patents that came
afterwards for variants of the basic
technique which may still be in force
oh cool okay some somebody named Pasco
used to work at Park Xerox PARC rich
Pasco good okay
I don't know that okay so schematically
we've got an input sequence we've got a
probability model in this speaks to your
point we can change this probability
model as we go along but to prevent
having to transmit site information we
agree in advance that we'll condition
that only on proceeding information okay
okay so here's the question um the earth
meta coder will give me minus log P bits
can i gain anything by using something
other than the true probabilities let's
assume the true probabilities P and the
assumed probability by the coder is Q
the average code length will be the
expected value of the minus log of Q and
this is greater than what I would get if
I'd use the true probabilities this
comes from the non negativity of
relative entropy or you could use
Jensen's inequality okay how do I apply
this to images so one approach that one
could use and has been used for binary
images at least is to simply code every
pixel and raster order and we're going
to use a probability model that depends
on nearby previous pixels previously
decoded pixels pixels that are available
at the decoder so if I'm encoding X I
could use a through G as my conditioning
all values and why not use very large
contexts so by context I mean everything
to the right hand side of the
conditioning bar well I have a data
sparseness problem if I use very large
contexts right because I want to
estimate these probabilities probably
using occurrence counts so every time I
see a particular configuration for the
context I count how many times I got the
different values of X and I build a big
table
if I have a lot of conditioning pixels
then I've got a big table with the big
indexed into the table and there's two
problems one is a practical problem
storing the table but then more
fundamentally how do I ever see enough
example data to fill that table in a
meaningful way
it's a sparse data problem curse of
dimensionality problem on the other hand
why don't I use very small contexts any
ideas
yeah they don't give you any leverage
they don't have much predictive power
right in the extreme case I don't use
any context at all and then I'm at the
marginal entropy which for typical
images 8-bit images might be around five
bits so you don't you don't get much why
not use non contiguous Conditioning
pixels and there actually are people
sometimes do use non contiguous
condition pixels and you could you could
gain something by using different
configurations okay here's a variant
called two-level context coding so I've
got a smaller neighborhood like the one
I already showed and then I've got a
larger augmentation to that neighborhood
so the idea is the encoder tries the
bigger neighborhood first if it's got
good statistics in the table for that
big neighborhood it uses it otherwise it
backs off to the smaller neighborhood
yep
Oh No ah right so the question is would
it be better more efficient more
sensible to implement that in a
multi-resolution way where you have a
coarse resolution and a finer resolution
etc yeah I think maybe there are there
ways of doing that um so how does the
decoder know when to switch while it has
exactly the same information that the
encoder used it can look at the table
and see if there's enough counts to make
these probabilities reliable and it can
on its own be synchronized by making the
same decision that the encoder made okay
so that brings us to the end of part 2
so we said that the nonlinear
statistical dependence if there's
statistical dependence it's probably
nonlinear between these sub bands or
most of it's on linear and we can
exploit that using the embedded zerotree
x' idea and there are other variants of
of that lossless compression can be
challenging arithmetic coding is an
efficient way of achieving lossless
compression it is good for even low
entropy sources it's adaptive it
separates the modeling and the coding
problems and the context coding is is
one approach to lossless compression and
there's a variant of that there's
something called je big one which
essentially uses a form of context
coding using an arithmetic coder
proposed by IBM and I don't know if je
big one is broadly used people use
usually use ccitt group for or other
techniques dan would now
anyway IBM
the pet yeah right so right so we agree
that it's Jay big one never really
caught on and probably one of the
concerns was at the time the the
proprietary interests of IBM okay thanks
a lot
right so for the transform part or what
you do after you get the transform coded
you know people have tried to come up
with lots of different approaches to
transform coding where you try to have
transforms that are specific to patches
and the patches can have funny shapes
and in the 80s and 90s there was a lot
of work in that direction but I yeah I
haven't actually tracked the most recent
but the last I looked there wasn't
wasn't much
oh people use jpeg2000 it's it's got a
bunch of different parts to it so
there's kind of the part that relates to
what I've talked most about is kind of
the wavelet coding part night I don't
know a whole lot about it but I think
it's related to the embedded zerotree
approach or a derivative of the embedded
zerotree approach but I don't actually
know but but people do use JPEG 2000 oh
right there was some email about that
right was that that you're talking about
the thing where there's some email about
it through HD photo yeah I don't know
anything about it
okay well Henrique Mel var was the main
force behind the left orthogonal
transform in the 80s and he's now
prominent at Microsoft and he's a very
smart guy and they've got other very
smart people in that group I'll fill
Chao and others and I wouldn't be
surprised that they have something very
good are they gonna displace JPEG with
it probably not not I guess don't know I
mean maybe yep
or is there more oh they do
I'll take advantage of that but they
also want to get perfect reconstruction
and they want to try to simultaneously
localized in space and spatial
frequencies so there's a little bit of
uncertainty the uncertainty principle
trade-off going on okay thanks a lot</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>