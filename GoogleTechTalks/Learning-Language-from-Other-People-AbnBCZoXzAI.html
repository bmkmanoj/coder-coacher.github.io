<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning Language from Other People | Coder Coacher - Coaching Coders</title><meta content="Learning Language from Other People - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning Language from Other People</b></h2><h5 class="post__date">2015-04-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AbnBCZoXzAI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm very pleased to introduce Mike
Frank so Mike got his PhD at MIT and is
now associate professor at Stanford he's
an expert on how kids acquire language
and how they you know interpret it in
and use just a reason one of the things
i think you know his findings will be
very useful for us but also his
experiment design and how he works with
young tennis it's really interesting to
see great thanks thank you very much
jessica for the invitation thanks
everybody for having me so I'm be
presenting on one particular line of
work on children's reasoning about
communication in context I chose this to
err on the side of coherence but I
understand there are a lot of diverse
interests and I'd love to take questions
on any aspect of this work or really
anything about children's language
Department I figured to be more useful
for me to present my own work but then
we can kind of veer off into the broader
context and the broader kinds of
methodological and topical issues that
you guys are thinking about as we go and
I'll say now that I'm going to start by
presenting some theory and theoretical
framework that supported by work with
adults and then we'll get to kids kind
of for its part two okay so here's Hagar
the horrible and he says to his
dim-witted friend any luck and his
bongos any luck with what and have our
says any luck with fishing friend says
wouldn't you say so I'm not a
mind-reader you know similarly Jeremy
says can borrow ten bucks mom sighs no
prob 20 bucks Jeremy if I wouldn't lend
you ten dollars i'm certainly not go in
g 20 okay how about 40 okay so thank you
very much for that laughs the extent
that these are funny and I think that's
debatable whether they're funny I
they're funny because they show the
errors made by two individuals who don't
know that language interpretation in a
particular context is going to require
reasoning about the other person with
whom you're communicating it's going to
require inferring their intentions I and
trying to figure out what they're trying
to tell you in this particular context
all right so people of course in
thinking about language and how it's
used have been considering this problem
for a long time
I in particular the kind of dominant
view and thinking about language
understanding is that in order for
language understanding to really work
you have to proceed from an assumption
of cooperativity that you've got a
partner who's trying to tell you
something about the world and they're
doing that in it as helpful away as
possible so one view on this is that you
think okay speakers are assumed to obey
a certain set of rules be relevant
truthful informative clear concise and
so forth and then if you're a listener
then you get to make inferences about
what somebody meant by assuming that
speakers followed those rules this
framework is due to philosopher Ian
price I he had a kind of broader view
point here though so his idea was that
you could see talking as a special case
of rational behavior in other words
language juice is just like any other
kind of action you take to achieve a
goal you say some stuff to try to get a
particular thought in some of these mind
but you can also do some things to try
to achieve a and and in both cases
you're going to want to be maximally
efficient do this so thinking about
Grace's framework you he was able to
make a set of predictions that explain a
lot of seemingly incoherent phenomena
about language so for example you get
this recommendation letter and the
recommender says I recommended for your
position he has really great penmanship
he shows up on time all the time you
know hiram I and you've course don't
want to hire this person um but the
reason you don't want to hire them is
because you have to assume that the
recommender was being maximally
informative given the constraint of
being truthful and if they're maximally
informative and truthful then you have
to assume that other things they could
have said are not the case they could
have said smart you know hard-working
creative and no that's that's not going
to be what happens here or in a much
smaller scale if I tell you that you
know I gave an exam last week and some
of the students passed the test you
might infer the exam it's hard why well
because I could have said that all of
the students passed the test I didn't
and so all of them probably didn't some
but not all of them probably passed so
these kind of inferences they sound kind
of like edge
cases but in fact they're just all over
the place in language understanding I
you know from kind of classic examples
like time flies like an arrow these
ambiguities of that sort all the way
through kind of complex discourses that
we see in natural situations basically
everything in language understanding
really requires this kind of reasoning
about speakers intentions in that
particular context I just one one thing
that makes computer speech recognition
commuters to each other standing I very
very difficult is that typically the
task is decontextualized so I putting
contextual information in the mix even
very very simple pieces of contextual
information can dramatically increase
performance for automated systems ok so
in this talk what I'm going to be doing
is formalizing a model of this kind of
language understanding and context and
the reason why I think this is important
just because typically people have
understood that language in context is
governed by this kind of rational
goal-directed action but it doesn't end
up formalized into a lot of models
instead it ends up and what people have
called the pragmatic wastebasket
pragmatics being all the rest of
language that happens in context it's
hard to reason about the day I complete
it's terrible so let's ignore it let's
think about the syntax and semantics the
phonology the morphology but let's not
think about pragmatics that's kind of
overarching like well people are trying
to do stuff so I'm going to try to give
you some tools to think about this
question and I'm going to do this by try
to capture the conflict between two
opposing principles to kind of basic
tensions in using language I want to be
informative I want to tell you about
what I want to tell you that but I also
want to save money and by saving money i
mean not talk a lot not use hard long
words not introduce a lot of complexity
these fundamental tensions I get summed
up in a lot of different ways throughout
the history of linguistics but they're
kind of essentially like a fight between
trying to get what I want done but
minimizing the bird and that's when I
say rational that's what i mean by
rationals oh we've got to maximize my
utility tell you stuff without talking
all day well i'll try to do that in this
talk with you
locals on success so um folks in
computer science actually had recognized
this kind of tension for a long time I
here's to folks who work on generating
natural language they said Grice's
Maxim's taken collectively mean don't
include elements that don't do anything
our position has been under a
goal-oriented view of language no need
to explicitly follow such a directive at
all this thing just falls out of the
mechanism in other words try to get the
maximal amount done with the minimal
amount of money okay so on to an example
here so suppose I tell you that my
friend has glasses which one of these
guys is my friend thoughts mm thank you
yeah see you and you know I'm
seventy-seven percent people on the
Internet I go maybe you're sort of
similar to um I and folks that we've
proved about this I'm sort of more like
that some but not all idea that I was
giving you good one describe the guy on
the right you would have said hat but
you didn't since the guy in the middle
of a glasses but not so that's that same
kind of counterfactual reasoning well
you know it would have been more
informative you use the word hat if you
had been talking about that dude over
there on the right but you you weren't
so you couldn't so you didn't and so
it's not him it's the guy in the middle
this kind of counterfactual reasoning
sounds kind of hard I so but you know
it's something we can try to formalize
and test we can ask do people do this
kind of reasoning and what do they get
any sorts of scenarios so let me show
you now our model for formalizing these
kinds of inferences and this is joint
work with my colleague Noah Goodman was
also at Stanford so um we posed a
rational speech act model this is a kind
of probabilistic model uses
probabilities as the language for
keeping score keeping track of between
alternate hypotheses and uses beijing
inference on ballistic inference to try
to make sense of competing hypotheses
I'll present this in a little bit of
detail your but if you have questions
about it feel free to interrupt me or we
can come back to at the end so
the fundamental question we're trying to
answer here is for a listener which
reference did the speaker intend given
the word glasses so we're trying to
compute what you call the posterior
probability of a particular reference
given the word that was honored and the
context that's the listeners posterior
probability we're going to use rule to
factor this into two terms I a
likelihood in a prior so the likelihood
we're going to call speakers likelihood
that's the probability that some word
would have been honored when the speaker
had they been talking about this thing
in the context and then so that's the
speaker like good and then they'll call
a reference prior the probability of a
particular face being the reference in
this particular context do we want to
talk about guys with hats and glasses
just glasses or just nothing at all so
now I'm how do we get that speaker
likelihood um well we think about a
speaker this is the speaker's
probability of saying a word given the
particular reference so that's where
informativeness comes in because if
we're we were a listener we're thinking
about a speaker who's trying to inform
us about which thing talk about okay but
how does the speaker choose how does the
speaker know what's informative well
they're going to think about a listener
in turn okay so clearly you got a
recursion there this recursion could be
endless let's ground it out let's say
speakers don't think about real
listeners they think about kind of fake
listeners will call them l0l0 isn't what
we call a literal listener literal
listener just says I don't you know i'm
not going to think about speakers
anymore i'm just going to choose a
referent that's consistent with what you
said so if you say glasses it's one of
the guys with glasses and that's all I
know so the literal listener is the base
of the recursion the speaker thinks
about the literal listener and then the
eye we call it el one of the pragmatic
listener thinks about the speaker
thinking about the lower listener so
that's how you get a pragmatic listener
who is actually considering what a
rational agent would have done to get
the job done and these speakers are
rational utility maximizers I haven't
shown you all the mechanics here they
want to be informative relative to the
cost so there's a little bit of
relationality with
back to you know how long the utterances
um so yeah there's some kind of I think
decently nice math you can do here which
shows that being informed of it all else
being equal you can define that as
transferring the largest number of bits
and then you get this kind of recursive
formulation the information theoretic
optimal in terms of transferring it's
actually kind of boils down to this sort
of same probabilistic model just kind of
fun okay so that's the abstract set up
here let's apply it to an example these
hats and glasses let's actually try to
compute these probabilities which turns
out to be remarkably easy and sort of a
paper-and-pencil model at this point
okay so um you could call one of these
hat and glasses games here a signaling
game these signaling games have been
studied in philosophy and linguistics
for many years I you know going back to
David Lewis and others um so um just
waiting for this it's kind of quick over
so you can think about I this guy this
this set of three objects as a matrix of
reference or meanings and uh words that
you could say so um right the top row is
on the three objects and you could say
hat about the guy on the only on the
right and the bottom row is glasses you
could say that about both the guy in the
middle in the battle right so now the
literal listener defined over that
matrix is a set of conditional
probabilities on if I say happen it has
to be the guy with the hat if I said
glasses then it could be either of the
guys with glasses now taking about a
speaker who's considering that literal
listener we could just transpose the
matrix and derive those conditional
probabilities for a speaker what should
they say if they want to talk about each
of these guys just going to build this
full slide so that we don't have to wait
so um the informative speaker thinking
about the guy who has a hat would
probably want to say hat rather than
glasses because littered the literal
listener would be most likely to get
that guy if you said hat
in contrast with the guy with glasses
you have to say glasses there's nothing
else you can say and so you get a 1 in
that color so then when we build to the
pragmatic listener I then glasses now
strongly means guy with glasses that's
supposed to guy with the hat because of
that competition both pragmatical
sinners thinking about the informative
speaker and what that informative
speaker would have done so that's the
kind of intuition and that leads you to
then converge on the guy with glasses
and not a hat so um you know for those
of you with an inclination towards these
sorts of matrices you can kind of see
that this is actually you might think of
as a matrix whitening ski we are taking
kind of off diagonal elements and
getting rid of them by this recursion
and the degree to which we get rid of
them is Peter corresponds roughly to the
depth of recursion here so in some game
theoretic versions of these sorts of
models what you do is you iterate until
you find a fixed point in the matrix
where all that kind of quote off
diagonals doesn't have to actually be
diagonal but all of the non equilibrium
points get white and that they go to
zero and so those are the those are the
fixed point solutions for these
signaling games but they're actually you
know you don't tend to go all the way to
the fixed point they tend to have some
fun boys over ok um so that's the basic
model of course wait there's a prior in
here on that we would use I've left it
out for you know to simplify the example
here um ok so in context these rational
speech act models are a special class of
probabilistic models in which you do
nested inference it's not just inference
is in a standard probabilistic model you
actually have to reason about somebody
else who's reasoning about somebody else
I this framework provides a way of
applying pragmatics arbitrary signalling
systems on but in order to do it over an
arbitrary signalling system you have to
give me both my quota syntax and
semantics and so that would be hard
computationally in a really big system
it's but it's trivial as you see in here
in a very easy system we have some kind
of fun approximations which if you're
interested in I can talk to you about
you can do this using kind of neural net
style models that are trained
with this sort of recursive pragmatic
input and sorry I'm kind of talking
forward for the slides because it's
taking about a minute to switch over um
so these are related to game theoretic
models that allow you to study
equilibrium behavior and so the goal
here is these guide your intuitions and
they become a tool for describing
empirical data that's the framework and
within that framework on the central
argument i want to make today is that
pragmatic reasoning and language
comprehension can be described by these
sorts of rational goal inference models
ok so the outline here is I that I'm
going to show you some initial tests of
this model then give you some evidence
that kids pragmatic reasoning and
they're worth learning which I think was
inspired by and really captured fairly
well by this kind of model and then you
know if we get a little chance at the
end I'll tell you about some ways that
this kind of pragmatic reasoning
actually informs the structure of
languages um there's some google data
and which is fun okay so this is the
first experiment that we did to study
this kind of stuff um we stripped things
down even further than the hats and
glasses we told speakers imagine you're
talking to somebody in you want to refer
to the middle object would you say blue
or circle and this is a less nice but I
saw morphic matrix representation of the
same matrix you know um so if you wanted
to talk about the middle object you
should probably say circle because
circle is unique descriptor of it um so
then if a listener is thinking about
this and somebody uses the word blue
then they should it should be the square
because if you've been talking about the
circle it would have been blue eye so um
the speaker then you know was asked to
do this across a number of different
conditions some cases where the there
were there was kind of one blue thing
and or one circle and two blue things
one circle and three blue things two
circles and three blue things and so
forth and throughout all of these and of
course i won't mention this but in all
these experiments we don't just do the
experiment of the blueant circle
everything is randomized there's also
pink stuff and
angles so forth and so on I'm and this
is done with 300 people on Mechanical
Turk Amazon's crowdsourcing marketplace
so we did these experiments by trying to
kind of strip out everything that would
matter an average across lots and lots
of participant judgments and what we see
is the more on the greater the ratio of
blue things to circles or vice versa I
the stronger the judgment that you
should say circle relatively so I'm the
kind of informativeness of the of one
predicate over the other I increases the
the speaker's judgment and here we use
to measure I where we asked people to
bet they have a hundred dollars and
they've been whether a speaker should
say blue or circle or square or blue or
whatever um and that produces graded
judgments you can reproduce the same
finding by just ask them to choose which
one they would say doesn't this week's
Patrick okay so now we want to compute
the listener condition um so there's a
problem with computing that because as I
mentioned to you there's a prior
distribution that we need to take into
account people talk about different
things different amounts um so we got it
that by actually asking people to
comprehend what a speaker said but then
not telling them what the speaker said
we said the speaker has one word to tell
you about this display and they say
mumble mumble mumble oh you didn't hear
what they said which one do you think
they were talking about so that's
literally the definition of a prior
distribution right we tried to query
their distribution without giving them
any data to change their distribution um
and what we get is you know as you might
expect people don't talk about stuff
that's as much that some kind of you
know not unique so the blue square here
shares a feature with the blue circle
and with the green square and so it gets
a lower ranking in terms of whether
people think I the the speaker would
have talked about it so we take that
empirically measured prior distribution
we multiply it by the model prediction
for informative uses of the predicate
blue on and what we get is a pretty good
fit to listeners judgments
so um in in some sense what we're trying
to do is connect a bunch of different
measurements from people we could even
sub in the speaker data that I showed
you and you know we're sort of using the
model to compute the relationship
between the prior elicitation task the
speaker task and the listener task we
get pretty good fit to this particular
case where it turns out that the prior
distribution actually cancels out the
inference about the informative use of
blue that kind of work against one
another and you get a pattern that kind
of is relatively flat but across a lot
of examples we get a very nice fit to
the empirical data um so there's really
a kind of nice correspondence between
the model predictions in this first test
at least in these very restricted cases
and I you know stress that the reason
why this works so well is because we
randomized out every other possible
factor that could have affected these
results so this is a little bit of
experimental trickery we just made sure
that we counter balance two randomized
all the stuff that people care about
except for the pragmatic arrangement in
the context um so um this is a kind of
first proof of concept which showed a
nice correspondence to the data um but
we kind of didn't really deal with a
bunch of the issues in this model we set
the level of recursion artificially we
measured the prior and we experimentally
control the speaker costs so we've done
a number of follow-ups to the study on
testing parts of the model independently
I'll mention those briefly because I
think important to understand that these
parameters can be brought under
experimental control before you go on
and try to test more exotic things
that's me okay so um first we looked at
whether people could actually do deeply
recursive reasoning I so this is the
cited same kind of result I was showing
you before these are data from the hats
and glasses now um so you can see in
green here I've boxed the hat on so hat
is what we call at level zero inference
because it that's just the meaning of
the word hat you don't have to think
about anybody to figure that out um then
I box from blue the glasses because
that's a level one inference need to do
one recursive cycle in order to figure
that out
I so um uniform 75% for the level one
now here's a matrix that's a little more
complicated there's actually a level two
inference so we've got um hats don't get
to hat glasses uh is hard mustache get
still get you know is sort of like the
glasses was but glasses if I say glasses
well there are two guys with glasses and
each of them have another thing in order
to figure out the glasses one you need
to say oh well if you wanted the guy in
the mill you would have said mustache
and you didn't you know if you wanted
the guy on the left to decide hat and
you didn't say that either and so
glasses must refer to this guy on the
right and that's a much harder inference
for people to make we're still working
on exactly why people strikeout on this
and whether under some circumstances
they can get it we think maybe we can
kind of scaffold them into it but this
feels a little bit more like Sherlock
Holmes than like language comprehension
in context so recursion appears to be
limited to you know kind of one or max
kind of one and a half levels in
practice at least in these simple
experiments um second thing I want to
show you here is um this issue of the
prior so as I mentioned what I did in
the previous experiment was measure the
prior empirically I but you know that's
actually you know technically that's a
correlational measure that's not
actually establishing any kind of
causality i just measured one thing
about people a measurement know anything
about people and showed that they
correlate with one another use them to
correlate or the third thing on what I
actually want to do is manipulate the
prior and show that that affects human
behavior I'm via the route that I care
about so I'm just reminding you the
formulation here the listener is a
function of the speaker times the prior
so that's the thing that we measured but
didn't manipulate so here we manipulate
it by just telling people about
interlock your name Bob Bob visits a
friend every week and here's the friend
that he visits every p 0 for the last
nine weeks okay so that gives you an
empirical prior distribution and now we
do the same two measurements we both
check on the prior uh and then we also
do that my friend has glasses thing and
what we find is that you know if you
plug that
prior measured prior into the model you
get a good prediction of the listeners
inference and listeners inference is
changed by the strength of the prior so
it just shows us that we can actually
push around people's judgments by
manipulating this term of the model
which is a nice piece of evidence right
okay so we've got graded sensitivities
so okay now that puts us in a place
where we've got this theoretical
framework it shows some fit to adults
data now we can actually start to apply
this to the situation that I'm primarily
interested in my in my own work which is
children's pragmatic reasoning and there
were look how these two things go
together because a little bit of
background on word learning probably the
primary finding and word learning but in
some sense is how deeply grounded in the
pragmatic context it is so we're
learning is very very much in mesh in
social interactions with caregivers it's
hard as you're just going to be talking
about hard to learn from kind of static
passive viewing of media it's much
easier to learn when you have somebody
who's giving you this rich set of
interactive social cues saying this is
you know a pen yeah have you seen a
plane before look at this we capture the
listener child's attention elaborate oh
you see the cap it goes on and off this
kind of you know classic parent talks is
designed to capture attention bring it
to a referent elaborate provide
repetitions I'm you know kind of narrow
the world of hypotheses that the child
considers so that's that's all well and
good um but then when you think about
pragmatics and language learning kids
are actually terrible at pragmatics
right they can't lie um they constantly
say decontextualized things I'll rock
you and be like you know it broke it
broke it broke an you're like hold on
stop well then what broke you're not
thinking about like what I know here
you're just talking right so there's
this kind of fun puzzle on where kids
learn a lot from social contextualized
language use but they also strike out in
some interesting and thought-provoking
ways so we've been using this model to
try to get a handle on that kind of
mismatch um so let me show you first
just a really really simple
demonstration that this basic phenomenon
is present in kids and
so I'm gonna mute my I'm gonna undo my
computer and mute the on video here I'm
gonna meet here s then great are you
muted I see ya okay we're getting
feedback from abroad oh yeah this is
great natural experiment is the larger
our friend set less drama I'm you per
second okay please mute on BP
okay there we go um all right I'm
unmuted and and safe now okay I'll play
this video without sound and I so here
here's a here's a child nor these
experiments there's a puppet
interlocutor who's you know very dumb
you have to help him find his friend his
house his car his plate of pasta so we
can't hear the sound uh unless say can
you show me my friend awesome so we've
got this puppet interlocutor here how do
you want any good and puppet I you know
you have to help guide him through his
day help him find his friends help him
find his glasses help him find his um
you know his car and so forth uh and um
then we ask kids to make judgments
within this set based on this puppets
under informative utterance glasses like
it's a cover story for why the thought
that would be saying such silly stuff
and what we find is that the rate of
choosing glasses the one feature option
goes off from h 22 h for four and a half
I'm we start to see it pull apart from
the responses to hat on glasses I'm
really by three or maybe three and a
half um so I this you know this seems
like not that hard a task but it stands
in striking contrast to the kinds of
failure that people have seen with kids
before in these sorts of tasks so for
example on if you show kids three horses
and all three jump over this fence then
you say some of the horses jumped over
the fence they're like okay that's
totally fine that's not under
informative at all which is of course
different from what an adult would say
that you know the adults say that that's
a terrible sentence to describe three
out of three horses jumping over the
fence um so there's this real mismatch
and that's five year olds maybe even six
year olds I have problems with the some
of the horses jumped over the fence task
yet three three-and-a-half year olds are
doing okay on this particular hat and
glasses task so this suggests some kind
of earlier pregnant
reasoning them we would have expected in
a congruent with this basic framework
where social pragmatics early on is
helpful in language learning it also
provides a tiny little clue and some
follow-up data sets I'm kind of support
that about what's going wrong with the
two-year-olds you see there's actually
kind of a cross over there it's pretty
small in this data set but it's bigger
than some others where we use eye
tracking they actually uh you know they
choose the guy with the hat and glasses
more because hats and glasses together
are cooler than just glasses so this is
kind of a important signal for us of
what's challenging for kids in these
sorts of situations in particular the
thing that's challenging for the kids I
is tearing themselves away from the
prior from the thing that's cooler
because they have a lot of trouble at
this age of inhibiting a lady called
prepotent response a response to the
kind of coolest things sort of most
natural thing to to respond to um so
it's not that they can't make the
inference here we'd argue I'm we're
starting to against an evidence to
support that I'm it's that I the
inference is blocked by the cooler thing
that I'm with you know otherwise attract
our attention okay so um the kids can
make this sort of inference let me show
you a case where I this inference
actually then helps them learn a word so
imagine I show you these two dinosaurs
and I point to this guy on the right and
sort of a nonspecific way and I say this
is a dinosaur with attacks now what do
you think add accents just come thank
you great it could plant to the audience
here um so when I when I ask you this
test question which of these has a dax
you would then answer the guy on the
right as a scarf as opposed to the guy
on the left as what I've now learned is
called a fascinator English royalty
where these I'm so I in the language of
these kind of ratios we would call this
a one to trial on this is a display
there is flipped but it doesn't really
matter what the feature is it matters
how many of each feature there is um so
there's one fascinator there in two
bandanas I so if I point to the guy with
the fascinator and the bandana you'll
think it's the fascinator in that case I
could do it where there are three
bandanas and then really why would you
be talking about
you should be talking about the
fascinator and adult judgments about
what dax means track really nicely with
that so you get this very nice
quantitative relationship between the
ratio of been damaged fascinators on the
one hand and the average support for gas
being the kind of a more unique thing
but more importantly we can look at kids
these are three year olds and four year
olds again I'm plotting the kind of
proportion correct I there are two types
of trials here there's filler trials and
inference trials inference trials are
what i'm showing on the left till the
trials are the same thing except the guy
would point to doesn't even have a
fascinator it's just completely
unambiguous all you have to do is
remember what i said and what you can
see the thing that i found striking in
this is that actually the photo trials
are not that easy for three-year-olds
because if to remember a lot of been
Devas and fascinators and stuff across
the experiment and the inferences are
basically no worse they're like only a
tiny bit worse so I'm what this shows is
that three and four year olds are
actually able to use this kind of
inference to learn and for the meaning
of a word um I also think this is
consistent with that sort of salience
explanation I was given because here the
three-year-olds are pretty good you know
why for your old standards at least on
and what you see further is that in this
case the guy who I'm pointing to is
actually cooler than the other guy so
there's no conflict between salience and
informative miss here I'm guy I'm
pointing to by virtue of this task
design actually you know provides you
know the kind of best thing to look at
and I'm pointing to and so the like
there's no conflict and and the
three-year-olds are winning I'm so
that's kind of another little piece of
evidence suggesting that it really is
this tension between okay it's hard to
rip myself away from something awesome
I'm minorly more awesome they're not
going to similar in battles um it's hard
to rip yourself away from something
awesome to focus in on something that's
kind of informative or useful okay so um
here's one more experiment that suggests
that this kind of pragmatic reasoning
actually can be a very powerful wait not
just to learn words but to learn about
the world so this is the work of Ali
Horowitz user tries to be my lab I'm so
suppose Ali says to a kid I this here
this is a broken TV
okay which is what do you think t-bo's
usually look like uh so hopefully you
agree that they more and more likely to
look like the thing on the left the
thing on the right but suppose I had
told you it was a small t blue instead
then maybe you generalize do the thing
on the right so you can see here is I
the inference that we're testing is that
because I used a word contrastive Lee
that is to be informative about the
thing relative to some context ah you
can now infer what that implied context
is what T boos usually look like I and
you can see this sort of um inference
all over the place in the real world
both in positive contexts like maybe I
say hey let's go to the art museum and
you're like oh there are the test amazia
me wonder what those museums show so
you're kind of learning by my
contrastive Yusuf and modifier what the
world is like or you can see that in
very negative contexts like gender
stereotypes like I say oh you know I'm
you know I went to you know see the male
library like why are you saying male
librarian you know either you have there
is some expectation in the world or at
least you have that expectation that
there's some gender differential there
that you need to you know kind of work
around so this kind of implicit learning
from the implied contrast set then
becomes a mechanism for you to get all
these kind of social expectations that
are out there as well as knowledge about
categories and what we find is again
that three year olds are able to do this
kind of inference and by for four and a
half they're really kind of getting up
to adult levels here I the two different
bars here are the broken versus small so
broken we called feature these are these
opposite features in size is the small
large kind of features um so let's
provide some evidence taken in some that
children make pragmatic inferences
earlier than previously supposed and
they can use these inferences to learn
about words and to learn about the world
more generally okay um so I'm close to
the end but I want to spend just a few
more minutes I'm telling you about some
cool new work that we're doing on how
this kind of pragmatic reasoning can
lead to language change so there's a
little bit of extra theory that I need
to inject here so the basic model that i
described does a
a pragmatic work but it doesn't do all
of the pragmatic work that we need and
in particular here's an inference that
it can't account for so this is a
classic example I say John got the car
to stop and you're like oh oh what
happened because he didn't use the
brakes maybe the brakes were not broken
you how to use the emergency brake um
why is why do we get that inference well
the kind of idea here is that I John got
the car to stop is a weird way of saying
John stopped the car the weirder the
thing is the more likely it maps onto a
weird kind of outcome in the world low
frequency way of saying something maps
onto a low frequency way of talking
about the world or state of the world
these are called by convention because
of this guy Larry horn who's a linguist
to describe them horn implicatures and
reasoning about them actually requires
uncertainty about the lexicon for some
kind of technical reasons that I won't
go into um but you have to set up the
pragmatic model to say I the listener is
actually reasoning about the speakers in
a world where they're not totally sure
what all the words need um so I you know
basically they consider the possibility
that kind of got the car to stop means
just this kind of special case or it
could reply to you know kind of the
broader set of cases and they kind of
average across those it's a way of
breaking the symmetry between these two
statements got the car to stop and stop
the car which actually mean the same
thing um so they consider a world in
which they don't mean the same thing and
in that world where they don't mean the
same thing it's actually kind of cheaper
and better to have it work that way and
that breaks the symmetry and allows the
kind of pragmatics to go forward as
usual um this is sort of a has been a
little complicated and so I'm happy to
talk a little bit more about that um but
for my purpose is actually the kind of
model that averages across what words
mean that's actually precisely what i
was talking to you about with kids right
that's a model of language learning
under pragmatics that doesn't know what
words mean and figures them out so
that's kind of fairly familiar territory
um so I skipping forward I this kind of
model that doesn't know what word means
towards me and actually allow for the
emergence of like
so you know if you can kind of have a
model that does pragmatics while sort of
learning words you can actually kind of
figure out what words mean in a context
where you don't know anything you could
talk to somebody you have nothing in
common with and figure out a kind of
signaling system that works for you this
is a simple simulation from a paper we
wrote about this where you can see the
kind of two agents trying to converge on
the signaling system and here they
converge on sort of one signaling system
on your they converge on it kind of
potentially a different one I they say
they kind of you can kind of just make
up a sort of diagonal along this matrix
and go with it doesn't really matter
which one it is um and you can kind of
see that the probability of
understanding in these sorts of very
very simple iterated games where you
just signal at each other and then try
to learn from that actually kind of
works pretty well and there's sort of a
bunch of recycle linguistic findings
that are related to this where you put
people in situations where they really
have to like make weird symbols to each
other and they figure out how to use
those to play games together um but ok
so this kind of emergence finding I
allows us to study these kinds of horn
implicatures these I implicatures where
they kind of length of a statement
actually maps onto its complexity or its
unusualness got the car to stop type of
thing and what we find in these thin
this theoretical model is that these
horn implicatures actually get
lexicalized I you end up learning that a
you know the longer statement actually
means the sort of more unusual meanings
so you actually sort of fossilized that
into the lexicon which makes a kind of
fun prediction that we have been testing
which is that in natural languages more
complex meanings should map to longer
words in general in the previous thing
or you said good luck ciccone inside
lexicon what is good in what um so II I
this is a yeah I'm sort of skipping over
the simulation just to get to the data
here on the good lexicon is one where
got the car to stop that sort of longer
expression maps to the more infrequent
meaning so
this is a the simulation here is about
how often the models learn that's
actually what the word means um so the
good lexicon is one that SAT or the long
expression maps to the unusual meaning
that lexicon is one that's vice versa or
the long expression maps to a usual
meaning and that mapping doesn't get
learned because it's just too weird it
doesn't it violates the pragmatics into
the models don't encode it as part of
their kind of state for the lexicon
whereas if it if you do have this kind
of asymmetry where the long expression
goes with the on a more complex or more
unusual meaning than those things get
fossilized in and become part of the
actual lexicon um so we tested the
prediction this kind of prediction that
you'd get lexicalized horde implicatures
so kind of a complicated way of saying
long words should be more conceptual ii
complex um so our strategy was we asked
people how complicated the meanings of
particular words were we took a sample
of 500 words that have been studied by
other folks and we just compare these
word meaning ratings in terms of
complexity to word length across
languages so two things to say here
methodologically one is that these
complexity ratings that's like a very
weird task to be like a brick what's the
conceptual complexity of this meaning if
people give really consistent answers on
this kind of task you can ask two
totally different groups of people and
you'll get very very similar ratings we
have some intuition about complexity
which I'll get to in a moment okay where
do we get word lengths across languages
well you guys nicely pretty provide us
with Google Translate which allows us to
to look at you know 83 odd languages
this is the correlation between lengths
of those 500 words Foreman I'm Edwards
in each of those a languages and there
um and their conceptual complexity so
English is the longest words sorry no
English has the highest correlation
between uh complexity and length and
that's probably because it was English
speakers that came up with the
complexities yeah um so but okay a bunch
of things to notice about this
complicated plot I'm first thing first I
all above zero I'm for you know all 83
of the languages it's above zero um so
that the mean is about point three it's
lower than it is in English with a
bigger than it is um you know by chance
you would not expect these correlations
I second thing to notice here there's
some pink bars some red bars pink bars
are Google Translate red bars are hand
check languages we went back and check
the translations yet turns out
translated quite well I'm like you know
high 80s low 90s for all the 12
languages that we have linked with check
for so didn't make any difference to the
correlations third thing here is length
is also correlated with word frequency
and that's a big confound the study so
those triangles actually show the
decrease in the correlation when you
partial out the word frequency and it
actually doesn't do all that much I mean
it does something very reliably across
languages but the relationship is still
overall I'm very positive and you said
all of these ratings are done by English
speakers yes all the meaning ratings are
done by English speakers we're in the
process of trying to get meaning ratings
by non-english speakers can you
determine length by a number of letters
I orthographically in this study in
English yeah Tunak number of letters in
English you can do it with number of
phonemes or number of syllables um these
are of course very highly correlated
phonemes actually does better on that's
the more kind of psychological measure
and so you'd expected to do better we
just don't have kind of accurate you
know phone IAM dictionaries for any of
these other languages otherwise we won't
do that but yeah when you can do
phonemes in english it works much better
so i shouldn't say much it works
substantially better okay so um this is
you know kind of okay uh Chinese and
Japanese are very very far apart even
though they use the same a lot of the
same characters yeah that's true um um
you know I'm not a speaker V those
languages so i can't check that but my
intuition would be that a bunch of these
words are like kind of fun you know
they're relatively abstract not all of
them but the number of them and so they
may actually have different forms in
chinese
because nice lips everything into two or
three punji yah not always Japanese also
has a an actual alphabet that 30 by
overseas Chinese technically stips
sticks kanji slide if I kupit google
translate doesn't do that very well
romaji yeah yeah um so I would have to
check exactly what's going on here but
my guess I know that vocabulary and
character overlaps are greater for kind
of historically older words and I
suspect there's a good chunk of our
words that are not historically older
and so there's less form for meeting
alignment there is it also depend on
which like the worst that you pick from
English don't really map with other
languages as being the most frequent
word so a good question so what
frequency distributions do align
decently well across languages um so
this is the same set of 500 words which
were in part chosen because we thought
that they had some chance of translating
across languages um right now we're
using English word frequencies because
of that correlation we're using the most
work for concedes as a proxy here I'm so
you know an ideal version of this study
would be that we got complexity ratings
all of the 83 languages and then for
frequency distributions at all behaving
three languages um that would be awesome
but corpus resources and you know
crowdsourcing or participant resources
don't exist for the vast majority of
these so I think what we're going to end
up with in the end is a couple of
languages where we check okay in this
language we have frequency distributions
and it looks the same yeah so to this
suggests that some of this structure is
kind of fossilized in two languages but
I haven't I haven't actually shown you
that kind of individuals make this sort
of inference in the moment that longer
words map to more complex meanings let
me show you that now um so um we've got
to bring bit things back into the land
of weird words and odd objects so
imagine you just heard somebody say fat
papalis below this I guess I haven't
pronounced that one or which object do
you think double locus refers to so we
asked people for this judgment it's not
as strong a judgment by any means as
many of the others I'm
these objects vary in their complexity I
in fact what we've done here is created
a set of objects that have one two three
four or five shapes and then we compare
one versus five shapes one versus four
shapes one verses 3 and so forth all the
way on down at all ratios that gives us
a complexity ratio and we're plotting
the that on the horizontal axis on the
vertical axis we get what you call
linguistic complexity bias that's the
effect size of the bias to map a longer
label on to the more complex thing and
does not also queerly with detective
sheep um we you know in this particular
study we randomized everything like
crazy so we don't have that I would
guess that you know kind of more unusual
shapes would end up with longer labels
I'm as well as them because if the in
other studies have found that certain
word sounds associated with round shapes
yeah yeah that's right so this is that
yeah that's that's a great point so
there's this kind of boba Kiki my bow
buzz around and all this and kikis or
you know sharply spiky um that effect
should be independent here because we're
kind of randomly generating these mounts
words with the same structure and we're
kind of randomly concatenating the
shakes the jion's um we've also done the
same study uh just sort of between
getting on the point in a different way
um with this set of natural images um
natural is it yeah these weird images
that we found on the internet somewhere
that we didn't have a name for uh we
asked people on the internet again to
rate these images on their complexity
and they're ranked you know there's
certain ordering there from this kind of
weird dog bully kind of thing on the
upper left to this very odd sort of like
weird shoe based letterpress thing on
the right low right there's this kind of
complexity gradient and when we use that
complexity gradient to do the same kind
of task we get exactly the same kind of
thing um there's a relationship between
the complexity ratings and the end to
the reports so um ok so the kind of up
shot here is that these horn
implicatures these cost implicatures and
maybe a pragmatic force that actually
shakes the emergence of natural
languages
so I you get this you know dynamic where
even young children are making these
sorts of pragmatic inferences that we're
describing here and the facts that
children and adults are making these
inferences then kind of feeds back into
the structure of the actual language
that they're using again so I've shown
you a couple of applications of rational
speech act models I'll just mention that
there are many others that I didn't talk
about today I I didn't talk more than
just giving the example about
quantifiers like some and all but my
evil nemesis neha franca um has done
some very nice studies on quantifier
meetings there's also some great work
from you know a Goodman's lab on non
literal language use looking at UM at
exaggeration and hyperbole looking at
metaphors and so forth they had a very
nice paper last year on hyperbolic use
of numbers this watch cost me a thousand
dollars did actually cost two thousand
dollars or am I just mad about how much
it cost I know it also has done some
work on epistemic inferences like if I
say some of the students test passed the
test but I've been finished grading the
papers and you shouldn't imply that
shouldn't imply that all of them didn't
I've done some simulations on scaling
these kinds of communication models to
multi listener states where you actually
are thinking about teaching a group of
people and you can derive things like
class size effects and kind of tracking
or ability grouping effect in teaching
from these sorts of models it's a
direction I've been going on recently
and I'm pretty excited about mmm Chris
Potts and the number of collaborators
and I have done some work on motor
neurons embedded implicatures these
complex and tactically based
implicatures that linguists have cared
about like I some of the basketball
players hit all of their shots um what
does that mean the others hit you know
not all you know you can kind of draw
out the implicature across the different
parses of the syntactic frame so that's
a kind of fun case study ok so in some
what I provided you with here is a
probabilistic goal inference framework
for pragmatic reasoning I it makes
quantitative predictions about speakers
listeners and learners
and it allows us at least to start
rethinking pragmatic influence David
Valley so thanks again from a couple
minutes of lunch any other questions
okay if you were using google translate
japanese if i use hear about our
katakana for a lot of modern or borrowed
works I'm instead of the country thanks
i will have to take a look at how that
was done you're a you know um there were
there was some attempt to kind of
normalize because occasionally translate
will actually spit out the English
version um you know because of parallel
corpora or whatever and so we did try to
filter those from all the languages
views I don't know what we didn't trip
the Japanese said f to check thanks so
do you find any um any results from my
training or I mean like mathematicians
are trained you know some does not being
not all there exist so do you have any
of those issues with with with older
kids or adults depending on the kind of
background they have you know it's
really interesting um this is this kind
of I think this kind of reasoning is our
default when we use language and I think
that's actually what makes some kinds of
formal training really really difficult
so the reason you have to go to three
years of Law School you know the best
law schools don't even teach you the law
they teach you how to reason about the
law and what they're really trying to do
in my view at least part of the time is
trying to talk you out of being a
pragmatic listener um the same thing is
true for kind of first-year logic
courses where they're trying to talk
about it applying your natural language
quantifiers to these cases where they're
clearly inappropriate um we actually
studied a corpus of people learning kind
of logic problems to try to get at that
look at sort of developmental effects as
they learn logic um the problem is that
really what what uh you know you don't
actually talk to bottom using natural
language and these bunch of courses you
teach them a set of rules by which they
can kind of translate natural language
you know if this problem says if and
only if then you just gotta go by
conditional you can't go with the
conditional if it you know if it's got
an if or as you know otherwise or
whatever
do this other translation and what you
see in their mistakes is that they miss
apply the translation not that they're
kind of just overly pragmatic so um I
think that your point is really well
taken that formal training is about
trying to overcome this sort of work but
it's actually very hard so if it doesn't
contaminate our results again this you
know most people can't overcome the sort
of desired use language in this way as
the age of which kids learn come to
proper context and inference very by
language that's a really great question
and something that I'd love to get at I
if you think about the logic of kind of
causal inference cross-linguistic
studies are thermally hard to do an
interpreter right because you can never
randomly assign kids to you know learn
indonesian which is a language with very
limited and morphological its detect
marking and so in which you'd expect to
see much more pragmatic skill
manifesting itself but say I run that
study and I don't find more pragmatic
skill maybe it's the Indonesian kids
were you know would have had they been
matched for vocabulary or kind of
educational experience but they weren't
or maybe they are more pragmatic but I
can't conclude anything because maybe
they just have more practice I think
they're just not a it's not a random
sampling this is like the problem of all
bilingual research that bilinguals and
non bilinguals differ in the gazillion
ways most of which have to do with their
bilingualism but aren't there my levels
of per se they grow up in bilingual and
bicultural families they have different
family structures they have different
you know socioeconomic status you know
in all of these ways you can control for
those in a regression but that's not
going to do the work for you so I really
want to do that study and I've been
trying to figure out what the natural
experiment is what the setup is that's
going to let me do it so if you have
missed you know assume that's a key
prediction</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>