<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>HCDF: A Hybrid Community Discovery Framework | Coder Coacher - Coaching Coders</title><meta content="HCDF: A Hybrid Community Discovery Framework - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>HCDF: A Hybrid Community Discovery Framework</b></h2><h5 class="post__date">2010-03-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/k_ML_rctUBY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm pleased to introduce today Tina le
hace rod from Lawrence Livermore
National Laboratory who's going to talk
to us about a framework that she's been
working on for Discovery Network or for
discovery framework for networks thanks
Jim for those of you who don't know me i
went to industry wisconsin i studied
official intelligence and machine
learning and i am moving to rutgers next
fall to join the faculty there and i'm
happy to be here to talk to you about my
work or one aspect of my work so the
problem at hand is that you're given a
graph with a set of vertices and edges
and edges can be directed and what you
want to do is you want to find
communities in them and a lot of people
have worked on this people refer to
communities as clusters or groups and
they're very ill defined and you are to
come up with the procedure that matches
the four properties listed one is a
scalability so it has to deal with very
large graphs as such it has to be
strictly sub quadratic with respect to
the number of nodes that you have number
two is you want it to be nonparametric
that is initially you don't know enough
about the network to say I want k
clusters and so your procedure should be
able to grow with the sides of the data
and find the right size the right number
of communities number three is you want
it to be consistent which is since we
have networks from very different
domains social networks biological
networks technological networks
informational networks you want it to be
able to work on all different types of
networks and not just a community
discovery procedure for social networks
and so the question is what do I mean by
works or effectiveness as I have it up
there and i define effectiveness
with two properties one is that the
committee structure that I find should
be able to accurately predict the
individual links back after all I'm
finding the communities just solely from
the structure of the network and number
two my communities should be able to be
stable should be stable with respect to
small perturbation to the structure of
the network so i would like to talk a
little bit more about why link
prediction is a good measure of
effectiveness of a community structure
in particular what we want is we want to
come up with factorizations that could
accurately model this this this
probability function that I have there
it doesn't go that far that's fun where
when I'm given a source node and a
target node and I've given the group
that the source node belongs to and the
target note that that the group that a
target of belongs to give me the
probability of a link due to from the
source to the target and so one way to
evaluate this would be you have your
graph you make an adjacency matrix you
randomly remove some of the entries in
the adjacency matrix that will be your
held out test set you build your model
on the on that type of the adjacency
matrix where there are some entries that
are taken away and you learn a model and
then you use that model to predict those
held out links back and then you can
measure it with any kind of regular
performance measures that we like for
example area under the ROC curve the
other one is based on a paper by Carrera
a towel which says for me to believe in
a community it should be stable to small
perturbations
if i were to randomly move links around
and rewire them while maintaining the
degree distribution in the graph then my
community shouldn't just fall apart and
they measure this by variation of
information which is an interview based
approach and i'll describe this more but
these are the two metrics that we will
use to judge how good we're able to
factor out the community structure out
of the networks so you may say why not
other community structures there's lots
of lots of them and in particular there
is a recent paper by earring at a lat
dub dub dub 2010 that lists a lot of the
metrics like conduction conduction and
expansion etc however most of those rely
on the notion that a good community is
one where you have better internal
connectivity than external connectivity
and the question is is that the right
notion or definition of a community
across all different types of networks
that can come across your desk so let me
give you some background about some of
the community discovery procedures and
algorithms that are out there i roughly
divided them into hard clustering and
then soft clustering so in hard
clustering you as it and know it gets
assigned to only one group in soft
clustering and node can belong into
multiple groups which matches social
networks more usually in a social
network you can belong into multiple
groups and not just one community or
group so the first one is is newman's
fast modularity or or actually it's it's
closets fast modularity where you
maximize modularity and it follows this
notion that internal connectivity is
denser than external connectivity and
you can approximate it with spectral
clustering the second one is based
compression approaches where you
minimize the total encoding costs and in
that one what you want to find are these
dense co clusters and in fact the
definition there is that members of the
community don't necessarily even have to
have links to each other they fall into
the same community because they all
point to the same set of people so if
you have a bipartite graph of let's say
users and the movies that they like I
and jam and Chicago will all belong into
the same group because we are all like
let's say Pixar movies even though maybe
we don't even have links to each other
so it breaks the other notion of what is
a good community and the other one is
soft clustering and there are many other
soft clustering approaches but the one
that I will focus on this talk is late
in dursley allocation for graphs where
we took David Bligh's Leander's lay on
location which is a topic modeling
approach and and tweaked it so it would
work on graph data and I'll talk a
little bit more about it and so here
what you find is for each source node
you find a multinomial distribution over
the groups that it can fall into and so
you have probabilities for that and so
what you have is you maximize the
likelihood and the notion of a good
community is one way you have accurate
multinomial distributions of source
nodes to these communities where they
are basically their latent variables
that then will tell you which target
nodes you will link to so three
definitions of you know what's a good
community what's the objective function
that is being optimized for those of you
who don't know about these three
approaches that are just highlighted I
will go a little bit into each of them
because we're going to be using them as
as constituents of our framework so the
first one is fast modularity and as I
said before in maximizes this
q metric called called modularity which
is a fraction of the edges within a
group as compared to my minus the
expected number of edges that you would
have in the randomized null model where
you still have the same set of degrees
and so it produces these hard groups
where a node belongs to one group and
the run time complexity for the fast
modularity version is order of the
number of nose times log squared of the
number of nodes the other one is cross
association and that's the one where it
came out of out of crystallises group at
Carnegie Mellon where you are trying to
minimize the total encoding cost which
is the code cost plus the description
cost and hear what you do is you produce
row groups and column groups so as a
source node I will belong into one group
and as a target node i will belong into
another group and this runs in order of
the size of the edges and so here is the
the matrices in the in the blue the
before is just a raw graph in the
adjacency matrix representation and be
the after is after you have run cross
association on it so it try what it does
is it swaps the rows and the columns so
you get those dents blah dense blue
blocks and the last one that we're going
to be using a choir framework is this
late in dursley allocation for graphs
that came out of our group and it's
based on david blyth work as i said on
topic modeling and what you have here is
basically what you say is I'm going to
learn to multinomial distribution one
that's going to map the documents to
topics and the other one that's going to
map topics two words and it's it's a non
parametric model in descent
in the photos so you can use a drill a
prior and it will grow with the size of
the data and give you the right number
of topics and here i'm just showing a
picture of the difference of one
difference between lda and LD AG so in
el dÃ­a you get these documents which
are your inputs and it treats each
document as a bag of words and what we
do in LD AG is we have a graph and we
say each document is one row in the
adjacency matrix so here a node can be a
document and it can be a word in the
document and some documents are totally
empty because they don't have any
outgoing links from them so here for
example in document to is empty because
document to only has incoming links but
no outgoing links so basically the
mapping that if you're familiar with ll
da the mapping is a document is a source
node a top pic is the community or
cluster and the target node is is the
word and here is the graphical model for
it where you define the prior and the
groups and the prior and your
observables which are the target nodes
and you learn the two multinomial
distributions and for this talk for the
verses this talk i will show results
where we used gift sampling we've also
used variational techniques and the
complexity of this is order e so they so
it's basically n log n so going back to
our problem to find the community
procedure why can I use one of these
right and one of the things that we
found is that they're not consistent
across all the different graphs that
could come your way so for example on
the chart which is various real world
graphs a times a is the co-authorship
graph a times kaiser bipartite author
knowledge graph
a times s is an autonomous systems graph
and the dub dub dub is a web graph and
as we see the performance on link
prediction so on the y-axis I'm showing
the average a a you see across five
trials on a held out set of links the
performance is very very much from for
example the author author graph which I
will show you which has a lot of
connected components to like an author
knowledge graph which is on which has
only one connected component and then on
the other side where I show the various
IP to IP graphs that's where the nodes
are IP addresses and you have a
connection from one IP address to
another when they talk to each other
when one sends packets to the other and
again for example we see that that fast
modularity doesn't do as well as as as
cross association for example this is
again a link prediction so none of these
approaches by themselves satisfy the
constraint that we had that I would like
a procedure that given a complex network
doesn't really matter from which domain
it comes that it does consistently well
across the different networks that can
come my way so can we do better and so
we introduced a Bayesian framework
recalling hybrid community detection
algorithm where you first run a hard
clustering technique like cross
association or fast modularity then you
use the groups that you get out of that
as hints to LD AG that as you grow the
LDA geographical model so that they can
take another set of observables call
that the attributes which are these
hints and it will learn on that and so
the this leads to better performance as
measured again by link prediction and
this robustness measure and the
intuitive reason just giving you the
take away message up front is that
hard clustering vice nature cannot
explain all of the links that you as a
node have in a network because it's
going to put you in just one group and
so if you belong into multiple groups
then it's not going to do well with link
prediction because it's going to put you
in one group and LD AG by itself if
you're a very heterogeneous kind of a
person and you kind of uniformly belong
to a lot of groups then it kind of
becomes ambiguous and it doesn't know
which group you really belong to and so
with the hint that it gets from the hard
clustering it's able to disambiguate
itself so this combination works well by
putting these two together so that's the
takeaway and I'll come back to that so
the question is how what's the right way
of combining these hints that I'm
getting from cross association or fast
modularity or if you like ncl or any
other approach that that you like that
produces a hard clustering and which
threat we tried three different
approaches one we call seed which is I
just use the group information that I'm
getting from my hard clustering to see
Gibbs sampling to start at a better
location in some sense um yes
stop it
birth and rock
you just gave an example
the situation where the class of these
appear for
that
so then use this hard clustering some
some assignment
that this heart of mine
stable
the ground that part of assignment
unless I
this
fine
so the key is how much do you perturb
right it was small perturbation if I
perturb it a lot oh yeah the question is
how can I do well with the the metric
that was robustness to small
perturbations to the graph especially
with hard clustering and so the answer
is of course if you perturb the graph
enough then it will become a random
graph right so the key is small
perturbations and the other aspect of it
is that even though you're perturbing
the graph you're keeping the same degree
distribution and a lot of these
approaches inherently when you look
under the hood right there looking at
the degree distribution of the graph but
i will show you plots where obviously if
you perturb the graph enough then it's
not the same graph you got right
not me just ability but also there
should be certain parts of the girl
stability is not an issue because that
this specific notes of the draft that
you are and I I am yeah yeah so we could
perturb the graph um by no type where we
define some kind of no type but we
didn't feel that we needed to make this
more general robustness measure more
specific where I say you know I'm only
going to change the nodes that are on
the high end of the degree distribution
or in the low end or meet some specific
criteria right I hope to answer your
question as as we go along more um so
coming back to this so the idea is I
have hard clustering and have soft
clustering I want to somehow combine
them come up with a hybrid approach that
could take advantage of the hard
clustering and the soft clustering all
together and voila we are all good and
so and given that the second component
the soft clustering is a Bayesian
framework so one is I can use the hard
clustering assignments that I get to see
the gibbs sampling and then forget about
them that is just start at a better
location to do my search for Gibbs
sampling the other one is what we call
prior which is that I not only use it to
see the give sampling but that I carry
these hints along as I do the gift
sampling and the third one which we
found was the best one which is actually
introduced these hints as observables so
now the Bayesian framework the
observables are for it for the Bayesian
procedure that observables are no not
only the target nodes but also the group
assignments that i got from one of the
hard clustering approaches or multiple
hard clustering approach is so you can
have multiple hints from diff
experts in some sense so how do we
extend LD AG to include attributes very
simple so I get my my attributes a so
now my graph I supposed to it just being
vertices and edges its vertices edges
and attributes and what I do is I just
learn another multinomial which goes
from groups to these attributes and so
and i'm calling him link attributes
because each node so as a source node
you will have an attribute and as a
destination node you will have an
attribute and again the the prior is der
ich lay and for those of you who like
plate models this is the plate model so
the the gray nodes are the observables i
have three hyper parameters and i'm
going to learn distributions over the
attributes distribution over groups and
so on and so forth so i'm going to move
on to experiments so that was basically
to sum total of the framework any
questions on it before i move on how to
simplify it in what sense I have three
hyper parameters
I never really thought of simplifying it
I don't why would I want to simplify it
more like vulnerable okay sure well I
don't want to bias one prior and the
other prior I guess but we could talk
about maybe another way to say is how do
you elicit priors on your you can tune
for them you can tune for them do they
correspond to
um yes and no yeah I would say no but
but but they're not very sensitive in
the sense of you know you treat it from
one to two and it all goes to hell in a
handbasket so so I'm going to show
results on nine real-world graphs that
we tried their sizes vary from you know
about 11 and a half thousand nodes to
325,000 nose and the edges go from like
32 thousand to one and a half million
and just to show that these graphs are a
bit different the the author author
graph was extracted from PubMed so these
are co-authorship relationships that we
got from PubMed and in biology usually
you have these cliques of people so you
have a lab and you know one lab is not
published with another lab because
that's like religious war will break out
so so there's a lot of components right
and then we have like an author by not
knowledge bipartite graph which only has
one component or the autonomous systems
graph that we have on the top a s that
has one component and just some basic
characteristics of these graphs the the
world wide web graph has the biggest
diameter the author by autograph as
expected has a larger clustering
coefficient and the one of our IP to IP
communication graphs to the IP IP
communication graphs are actually across
five consecutive days at a meeting that
we tap the network and was able to
connect the data and that has the
largest average degree among the
different graphs that we tried followed
by another IP IP communication graph and
one of the things that I am also showing
here is the number of articulation
points and an arctic
asian point is one where if you were to
remove that node from the graph then you
increase the number of components in the
graph and of course the dub dub dub has
the largest one there so one is how do
we go about measuring link prediction
and for LD AG and the hybrid approaches
is very simple because I have my two
multinomial distributions that I learned
so what's the probability that a node
you will have a link to a node B and
then that's i will just sum over all the
groups and i would say okay i'm giving
you what's the group for you and now
given that group what's the chance that
i have a link from that group to be and
for the hard clustering approaches fast
modularity and cross associations it's
based on density so the number of edges
from the group that you belongs to to
the group that B belongs to over all the
possible edges that we could have from
these two groups and so on the
experiments that i'm going to show you
we given the adjacency matrix we
randomly selected 500 what we call
present links so in dÃ©jÃ  c matrix
these are nonzero entries and 500 zero
entries these are absolutely and this is
the held out test set and we run the
remaining links through our community
strawberry procedure and then at the end
what we get is we use the community
structure to then estimate the
probability of the the links that we
took out actually being present and we
compute the area under the ROC curve and
we repeat this five times so those are
the results that I'm going to show so um
here we have the on the x-axis are the
various IP to IP communication graphs on
the y-axis is the average a you see
you already saw the results were fast
modularity LD AG and across association
so that's in green blue and purple and
now on the top you see the results for
the hybrid approaches HC d dash M is
where you use fast modularity first and
then you use lb AG second HDD alone is
where you use cross associations first
and then use LVAD afterwards and as we
can see the HDD so the combination of of
cross Association and LD AG is
consistently able to stay above 0 point
95 in average AAA you see and the bars
on the LD AG is what if i run i ll deg
LLL daj first and then give its results
as a hint to itself and we see that that
doesn't really improve the results as
much so the end this combination of hard
clustering and soft clustering and
combining them is what is giving you the
boost here and this is the results for
the other graphs so they co-authorship
graph their author by by knowledge graph
the autonomous systems graph and the
world wide web graph and again we see
that HDD and HDD F R are on the top or
at least they don't perform no worse
than the others so this part is trying
to show you the difference between the
various ways of combining the hard
clustering with the soft clustering and
the seed is where as I said you use the
hard clustering to just see your give
sampling to just start at a better
location and forget about it and that
doesn't really do very well so as the
plot shows we're arguing that you should
use this attribute based approach where
you use the output of a hard hard
clustering as observe
herbals I supposed to just using it to
to either see your your gibbs sampling
process or seed and follow through with
with that information as as you make
your move in the sampling space and this
blood here is trying to show you the
worst-case performance across all graphs
that we tried right and the y-axis is
again the worst-case average average HD
so what does this mean so for example
with HDD what that means is that the
worst average a you see we got when we
ran it across all the different graphs
across all the different domains that we
tried was a little bit above point 9
verses for example with fast modularity
it's a little bit below point 7 so this
is to us as I'm as our money shot for
this approach and the same with HH TDM
as we see so going to how do we measure
the robustness because that's another
measure that we don't want the community
structure that we're factoring out to
fall apart when you do small
perturbations to this graph and so
basically what you have is you have this
rewiring parameter C from 0 to 1 which
which decides what fraction of the links
you're going to rewire and the links are
rewired so that you preserve the
expected degree of each node in the
graph to the degree distribution remains
the same and so what you do is you find
your communities on your original graph
then you perturb on your graph and you
find in other set of communities and
obviously when C is one you have
perturbed the graph fully and now you
basically have a random graph and so the
value of information is the symmetric
entropy based approach where you measure
the information needed so H of C given C
Prime you measure the information that's
needed for first to describe see given C
prime yes
we did not we did not but that's a good
good idea and I think it goes back to to
your calling up front about maybe
selecting or maybe perturbing specific
links right like if I if I were to take
and no doubt and that note happened to
be an articulation point then it will
have a much bigger impact on the graph
than not so or i guess if i were to take
a link that has a very high between this
and if i were to take that link so it
too it's a weak tie then it will break
up the ground yeah good good point and
so this this variation of information
falls between zero and log of n and we
divided by log of n so it's normalized
on the results that i'll show you so
yeah question yes sure
the sea small which is the rewriting
parameter between zero and one yeah I
have a little bit i'm standing it so do
you just make do
remove these many edges from the graph
and then you see the expected value
which is the average of this
of the
so the way that you do is you basically
just cross right so what you're doing is
you're not taking any links out right so
I pick I I pick a link so I I guess the
question is are you having trouble with
how the rewiring is occurring so that
you maintain the same degree
distribution so so basically when you
when you cross the wires you you make
sure that you are you still maintain the
degree distribution of the big graph
right so it's just it's a simple
bookkeeping right it where you know i'm
not going to wire one node that has
let's say only one link to to a node
that has lots of lots of links but it's
just bookkeeping to make sure that the
degree distribution is the same it's
nothing special really yeah so here um
the the variation of of information is
on the y-axis and the lower the better
and the rewiring parameter is on the
rewiring probability is in the x-axis
and so when you have one you basically
have a random graph so as we perturb
this the rewiring probability you'll see
that for example let's pick a HC DX so
it's the same as HC d the the blue line
at the black line and then if you look
at its constituents which is X a and an
Lda G the point here is that the
variation of information for the hybrid
approach is lower than its constituents
so in some sense it's more it's
communities are more robust to to
changes in the structure of the graph
and for a CDM whose components are our
fast modularity and and cross and
and an Lda gee um it's at least you know
it at least matches um the other ones
now one of the points here is that
variation of information is a little bit
of a funny measure in that if I put
everything in one cluster then I get
very low variation of information right
I have one graph and I put everything in
in one cluster and then I perturb the
graph I'm Leah and again I put
everything in the same in one cluster
and then clusters haven't really changed
and so it's it does need a little bit of
work as a metric which has been pointed
out with different aspects before so
this is also variation of information
for a fixed point this is at twenty
percent wiring of the graph again the
y-axis is variation of information the
x-axis is the various IP to IP graphs
and again the point here is that the
hybrid method is doing better than it's
considered then its constituent which is
LD AG and and and either better or the
same as as fast modularity and in these
two plots both the previous one and this
one LD AG usually has very high
variation of information so this the the
soft clustering is because it generates
a lot of clusters right and so when you
perturb the graph a little bit the
clusters are likely to also perturb and
so this robustness measure so to just
pause and think about what I've shown
you and like what's going on here and
the way we see it is that there is this
for the link prediction there's this
trade-off between what we call low
entropy and flexibility so low entropy
means that if I'm able to compress the
graph they just see matrix of a graph
really well then I do well in link
prediction and
the multinomial distribution that i'm
getting for my source nodes are far from
uniform then again I'm able to do well
on on link prediction right because if
they're fairly uniform then it gets em
ambiguous and it doesn't know where I
belong to and so on and so forth and
then the flexibility part of it comes
that if I'm if I a very heterogeneous
behavior than the hard clustering
approaches only are able to put me in
one group and you know pick the the most
prominent behavior that I'm exhibiting
in the social social network so it's
that combination and again putting them
together helps both the hard clustering
and the soft clustering go ahead
so the question is if I were to get my
graph compress it and then run the same
approach will I get different results
what did you do
um I don't know my hunch would be my
hunch would be it depends on what parts
do you use because that's what basically
cross association is doing right it's
doing compression and and by giving the
hints in some sense I am making these
super nodes right because i'm saying
that i use cross association i compress
the graph I found that you and I belong
to the same group and I'm giving that as
a hint to the Bayesian part the soft
clustering part right the kind of
behavior then you would have a mantis
like Albert Benedict original graph
compress it we're really nice clustering
and do we find that have a very fast
algorithm for meeting with humongous
graphs yeah yeah I know I know actually
the work that you're talking about where
he uses Metis to separate the the
partition yeah I don't know we'll try it
we'll try it it's it's a it's a good
question and so um oh yeah here so so
this follows on what I was saying is
just trying to show you a picture of
this trade-off that I was just talking
about our low entropy and flexibility so
the matrix that's all read that that's
the raw input that you get for one of
our IP IP graphs and the rest of the
matrices that you're seeing our
community sorted and the takeaway from
here is that the more white space you
have the higher your link prediction
accuracy is going to be so HDD and a CDM
has have the most white space that is
you're able to compress the graph more
and so their area under our secure is
higher than then the constituents which
are listed above which have which have
more snow
so the other question is what about like
a super hybrid in the sense that I go
and ask multiple hard clustering
approaches to who clustered the graph
then use each of their hints into my
Bayesian model yes i'll have more
observables but you know maybe i'll do
better and we did try that and when the
different approaches disagree then you
do get a boost in performance but if the
different approaches don't disagree with
each other that is if fast modularity
and cross association put the set of
nodes the same set of nodes in to the
same community then you don't get a big
boost so to summarize today I pushed for
a hybrid approach to community discovery
um where you combine hard clustering and
soft clustering I talked about two
different metrics to measure the
goodness of a community one is link
prediction after all you're finding
these communities just by looking at the
structure of the graph so if your
communities hold any water they should
be able to predict individual links back
for you and the other one is robustness
that is if you perturb the graph
slightly while still maintaining the
same degree distribution you don't want
your communities to be totally
different-looking and I also pushed for
the fact of combining the hard
clustering and the soft clustering by
using the output of a hard clustering as
a hint into the bayesian approach the
soft clustering approach here are some
of our papers on this topic we also
extended LD AG to work on time varying
graphs we extended David Bligh's dynamic
topic models the difference there is
a topic usually a document does not
change here and know it can acquire more
links so there was a slight change to
the graphical model and here are some of
the references of the work that I talked
about and any questions yes sugar
suppose you have one graph of let's say
Facebook users and have not a graph of
Twitter
we're say be able to do mapping between
the nose as in you know which
motorsports would you be able to wrangle
them to actually do some kind of
prediction that if we know there's a
link via clustering facebook community
that you'll be able to predict a friend
yes I think that's a great question so I
the the question is I have two graphs
once a from facebook and another from
Twitter and I have a mapping from the
note so I know that Tina and the
facebook graph is this tina in the
twitter graph can I build a model of
communities on the facebook and use it
to predict lengths on the Twitter and I
think it will depend on the application
of the social network so if I'm not
quite sure for example that if I learn a
community structure on facebook and used
it on linkedin maybe I will not be able
to do as well because on facebook i have
a different set of friends and I exhibit
different behaviors that don't even show
up that maybe in a professional setting
they show up but I would say that if
it's all within like my social and hobby
since then then we should be able to do
well it would be I would love to try it
yeah that fountain yes
because they can only put a unit into a
single community that's true hard
clustering methods and is also
probabilities out of c'mon instead of
doing something like Lincoln Deerslayer
elevation
would it make sense in the type of
applications that you care
to think about objects belonging to war
and model the different community
memberships committee membership as a
set of attributes instead of a
yeah absolutely and we we have we have
tried that where you say where you
independently belong in two different
groups and so the probability of me
belonging to this group is like point
eight and probably mean not belonging to
the group is point two and that sums up
to one but it doesn't sum up across the
groups absolutely yeah good question yes
right
so the question is whether to add other
attributes that you may have on the
nodes for example if you know the
occupation of somebody and and use that
to cluster the graph and it's a good
it's a good suggestion and and we have
tried it in some domains I think in
those cases it's a bit harder to argue
for link prediction as it as a measure
because now you're taking attributes
into account and but it's a good it's a
good suggestion absolutely and people
have tried it and we have tried it yeah
yes
so I'm
that
so in what sense would you like me to
talk about the implementation of
algorithms that
I think the biggest one was for example
cross Association is written in matlab
and a lot of our code is written in Java
and we also wanted it to be extendable
to to work on Hadoop and just you know
initially we wanted to just like call
call matlab from Java and that gets a
little bit hairy and there's not really
a lot of good java libraries to do
numerical analysis and matrix
multi-mission I know at Google you guys
have one that that we actually use to
but
yeah it wasn't
what
what would be how do you make it easier
they come up with a model of computation
for four classic words
it's impossible
a model of computation that will make it
easier to do clustering on networks well
I guess one of the things I was
interesting is when Jim came and visited
and thinking of it with respect to to
like a node perspective view of
clustering was was interesting I haven't
really spent a lot of time thinking
about it but I like the approach of
computations with respect to the again
the vertex perspective and see how well
we can do there and that's actually what
we try to do with with it LD AG like one
of the good things about lda and why it
scales up its supposed to the other
approaches that people have done to do
probabilistic community discovery is
that it doesn't look at the zero links
right it only looks at the one link so
when you do topic modeling you don't
look at the words that are not on the
document right and so when we do LD AG
I'm just looking at the the links that
this note actually has I'm not looking
at all the possible links that it could
have that I have not seen right and so
by taking this vertex perspective then I
can just make a quote unquote document
or this independent container of data
that is centered around this vertex and
then I can send it off to multiple
processors and let them work on it so I
see a lot a lot of promise and in the in
the vertex point of view of computation
for for graph algorithms
stupid
The Young Turks were too small
to not be online drafted because one be
interpreted this is
a single machine will go to satisfy you
beyond
disposal
dissipating the difficult
paralyzed algorithms now don't went to
the model question what first of all
difficulties
what early
well address
so yes I should have started with a
disclaimer that large is relative and so
and I guess um I feel that if you can
break up the graph into these
independent containers and you have lots
of lots of processors and you could do
it where you can do the computation
locally like for example David Newman at
a university of california irvine has
shown that you could do distributed
gifts amping on models like an Lda then
you would be good to go but I guess I
have again I haven't really thought
about graphs that are billions and
billions of of nodes and I'm not even
sure that you should do community
discovery on a graph that's that big in
the sense that maybe you should do what
you got to was talking about where you
partition the graph and then you do your
community discovery on those smaller
partitions of graph of course of course
yeah yeah yeah and we have other work
that we're looking at now which I'm very
excited about and I can talk off camera
with you about time varying graphs and
how it's not necessary to work on a
really huge graph because you take
trunks of it obviously and work on it
that way
easier to express readership
many
yeah and and others have looked at that
as well yeah good questions yes you
extended indie game the graph the way
have you considered looking at the
relational model kind of setting because
the advantage of having one you again do
need the number of
people here is that instead of having
the membership
you can say that okay am i belonging to
one cluster yeah we did look at at Tom
Griffiths IRL model and the the problem
there is that he looks at both one and
so both present links and absent links
and so it doesn't scale right because I
have to look at the entire jason c
matrix i have to look at both the
nonzero entries and the zero entries and
and that's why all of those fail and and
the one thing that i should say that so
we came up with the LDA g model around
the same time as lee Jazz's group also
discovered the same thing which is why
don't we take a topic modeling approach
which only looks at the words that are
present in the document and turn it and
use it for graphs and they also extended
it for weighted graphs by using a
different prior so yeah and in fact all
of the scalable approaches the community
discovery that I know that use these
kind of probabilistic approaches are
based on lda because it doesn't look at
the zero entries in the adjacency matrix
yeah
thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>