<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ZuriHac2016: Parallelizing and Distributing Scientific Software in Haskell | Coder Coacher - Coaching Coders</title><meta content="ZuriHac2016: Parallelizing and Distributing Scientific Software in Haskell - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ZuriHac2016: Parallelizing and Distributing Scientific Software in Haskell</b></h2><h5 class="post__date">2016-09-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4py8BYIw1DI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so yeah this talk is about making some
simulations in some scientific software
go faster and we make it go faster
simply by 30 more cpus at them and I
think it's a nice example because it's
sort of the dream scenario in terms of
paralyzing something because it's a pure
function that you need to execute tens
of thousands of times like at the same
time and I think it's interesting
because it's not as easy as you might
think so I will not be able to tell you
exactly what this thing is about because
of NDA's and stuff but basically you
have this this piece of scientific
software scientific in the sense that it
does number crunching and it's nice
because it's really you have a pure
function that does some number crunching
and then you have to do some statistical
analysis basically on it and to do that
a bit like a Monte Carlo thing and if
you're familiar with that and to do that
you take this your function and you need
to execute it over thousands of days I
mean it depends what the exam number is
it depends on how precise you want to be
but it goes from a thousand to ten
thousand right and start this and and
yeah this is sort of like the classic
embarrassingly parallel problem that
everybody wants to paralyze right you
don't you're not waiting on any i/o
you're not interacting on any database
you're not you don't have locks you
don't have music says you don't have any
of these problems but but it turns out
it's not so easy to get good performance
in this scenario so so right you have
this pure function to run it tens of
thousands of times you want to run it in
parallel you have a machine with 18
physical cords you go rent it on AWS
it's kind of expensive but it's easy to
rent many of them and the first thing
that you might want to try is to use the
power primitive jeje gives you and this
is basically a pure facility during the
sense that it doesn't use I or M bars if
you're familiar with that
it just tells Jesse execute two things
in parallel and you know I let you do
the scheduling and everything I just
want to run to get to evaluate these two
expressions in parallel and so if you
have basically a vector of things that
you need to apply a pure function on
each of them you simply what we thread
first is that you split this vector in
18 parts since we have 18 cords and then
you do a map or these 18 chunks and
you'd expect roughly 24 this for each of
this piece of computation internal
separate core and you should be you
should get at 18 x 18 x speed up right
but by the way if you have any doubts or
anything just interrupt me like raise
your hand or shout or something and i'm
happy to take questions so you try that
you try that and the speed up looks like
this and this is actually an old plot I
didn't have time to regenerate something
you could read better but at the bottom
you've got the number of course so you
start with one and you end with 18 and
on the left you've got there on time of
a simulation and it starts out with a
nice speed up so going from one to two
at the scale is also weird it's not a
great plot but it's the only thing I had
that it starts with an i speed up it
actually goes almost twice the speed
going from one to two but then it slowly
but surely slows down right so 4 is not
half the time the two is and an eight
and then 12 and around 10 or 12 course
it just stalls and words in that past 12
cores it starts to go slower so you
don't get any improvement at all it gets
worse we actually thrive with a machine
with more than ending chords I feel like
a while ago and yeah I just gets worse
than words you basically do not get any
speed up past em chords so so why is
that well some of you I'm getting
guessing some of you already know why
but
the reason is basically a garbage
collection and the problem here is that
what happens when you have one core that
is executing this function 10,000 times
you're sort of serially going through
this list of inputs to process and
you're processing one as once at a time
you generating some some some garbage
right that the garbage collector needs
to collect and you've got this this
garbage like like these memories are
located and collected from one single
heat now when you add more computation
that it's suffering at once so for
example when you're running 18 things at
once you're always working on one single
heat right and the garbage collector
will like when when you're running
aiding things you're generating 18 times
the allocations and also 18 times the
garbage right so the garbage collector
basically needs to work much more when
you're running these many things at once
and then it doesn't like like it it's
almost it's not quite through like the
gabelli has your garbage collector is a
bit parallel and I'm actually not really
intimately familiar with it I'm sure
there are people in this audience that
notice better than me but the effect
that you get is that as you add course
you start allocating much more and
that's putting much more stress on the
garbage collector and so the garbage
collector needs to kick in more often
with longer poses that end up blocking
the work across all the course right I'm
again I'm using sort of simpler times
here but you get the idea that basically
you need to garbage collect more often
and for a longer amount of times and you
block the work across all the worker
threads and when this happens and this
causes like a progressive decrease in
the overall productivity of the program
where we productivity we mean the amount
of time we spend doing actual work
rather than doing average collection so
with one with one core the productivity
is at around ninety five percent so it's
really healthy a school program you sort
of doing real work most of the time but
then as you add course it goes down to
like forty percent like around 12 course
yes
there is a question that the nursery
yeah we've tuned all the parameters that
you can possibly tune and like also the
new I think it's called n and so it like
okay so I can get a bit more specific
with a higher nursery sighs you delay
the problem in fact I think that plot
was with a 64 megabytes north surry or
even bigger because otherwise it gets
worse quicker so with 512 case which is
the default and it like it like plateaus
much more quick much we can in fact i
think the default is too low because you
gained like five ten percent performance
with one thread but like it's really bad
with more threads so this is a like it's
a known problem right it's a known
problem that is so brilliantly well one
language in the mainstream which is this
language called airline and like it's
solved by airline by having multiple
hips like instead of having one big heap
you have multiple ones and in fact the
woods work yeah in this direction we ask
you but it didn't really come to
completion again i'm not entirely sure
why but Simon Milo was working on these
local heaps but that work was dropped I
hope some point it will be resumed so
yeah we have a problem right because we
cannot take advantage on of this big
machines and more importantly we cannot
just get faster by throwing more
resources at the problem which in some
projects is really what you want to do
because you have little time or you
cannot change the code and you have this
pure function and like it doesn't make
sense you're like I just want to run in
a lot of times why can't I do this so
did I skip no so how do we fix this
right how do we achieve this nice
scaling that we want there are roughly
two ways I would like two general ways
to fix it one is to make this single
function like make the single pure
function have less GC pressure right you
can make the single function to be
friendlier to the garbage collector so
if you're in it like once we already
know that the pressure is okay
but if you ran it 18 times at once the
pressure is unacceptable if you make it
18 times nicer right to the garbage
collector it ought to be go fast also it
if you run it 18 times at once so to
give you a bit of an idea of what this
would look like in this code base it's
basically vectors of doubles that's what
the state is and you manipulate a set of
splice them together you do stuff with
them but I think like my birth plan to
make this function nicer was to have a
single pinned state like a single block
of memory that you modify in place sort
of much more imperative if you will and
you will like drastically reduce this
sort of tiny locations of these tiny
vectors that happen all the time in the
function as it is now and and this is
the best solution I think like in terms
of like it's it's a net win like this
solution apart from the fact that the
code might be a bit uglier has just
advantages right it will go faster also
with one core you might get the scaling
for free just using your facilities like
the power Combinator so I mean it's a
really attractive solution the problem
is that at least in this code base it
would have required like a massive
refactor so this function is a single to
function but it's like they are like
70,000 nights of code and depending it
and we would have had to change the
memory layout of this whole thing so we
also are working like highly regulated
industry where code changes have a
really high burden of verification and
by the way this problem is not like this
is like a real word problem that we had
to solve so it's not like we really need
it to go faster quicker you know in a
you know you know yeah you know short
amount of time so yeah so this was sort
of discarded simply because it was too
risky in many ways the second solution
I'm really better switching slides today
is to use multiple historian times right
so as I hinted before the problem in
scaling this program is that you're
always using the same heap and this heap
is putting the single hip is putting a
lot of pressure on the garbage
but as I've said there is a solution to
this which is to have multiple hips
right a multi-part garbage collector for
a chips so this is what airline does
each our length trap basically has its
own little heap and garbage collects
it's it's own memory but it doesn't
really affect the rest of the threads
and and you can really do the same in
Haskell is just that the facilities are
not there to have you to do really easy
and really the only way to do this right
now and ask least use multiple processes
so you literally have to spawn multiple
worker processes like not threads and
each process will have a dedicated has
the RAM time with a dedicated heap and
then you need to communicate with this
processes communicate somehow right but
even if it's awkward to do and I mean I
will explain like also why is it awkward
right now I think it's really the only
safe way to do this sort of stuff and I
found it interesting that like a few
months later there was a guy reddit
which expressed exactly how I felt after
this problem i'm just going to it for
high scalability fixing GC is unlikely
to work with all the work put into it
the java grabber to lecture system is
still horrible while airline rules
easily in heaps amusing message passing
is more scalable i'd like to see more
work in making multi-process OS
processes program programs more fluently
i'm not exactly sure what fluently means
but anyway i think is totally right like
at least with the technology that we
have now garbage collection with one
heap and like 30 threads it just doesn't
doesn't work so I I also think it would
be totally great to have these sort of
facilities without having to spawn
different processes but as to now you
need to go to the pain of doing this
manually and another side effect of
going this route is that you get for
free scalability like mostly machine
scalability right if you if you're using
message passing and you basically there
isn't really much difference I mean
since we also use like sockets do it
which is the most immediate way to do it
there is no like impediment to
distribute it across
multiple machines right and and we do
that and and it's just it gives you like
much more power in terms of how much you
can scale right so the next like the
rest of the talk is basically why this
is sort of annoying to do in a school as
of now sort of what we had to do to make
this work but it does work so I'm pretty
happy with the result because I don't
think we would have been able to do it
otherwise so just to describe the
problem in a bit more detail you have
this update function which has a state
and input and it gives you back a new
state and an output rate so again this
is it's basically literally what it
looks like I'm not simplifying much it's
just a simple pure function and you have
a vector of states and you need to apply
this function to all these states right
with a fixed input so you have these
initial states a list of inputs you need
to repeatedly apply to the whole to do
all the states and you get a list of
like a vector of vector of outputs you
basically get for each input a list of
outputs and then these outputs are
processed and that's what we're
interested in am and given that the sort
of naive solution is to have worker
processes just processing basically just
embodying that function and all they do
is that they take a pair of the state
and the input and they give you back the
cylinder and the output right so this is
the simplest thing you can do that you
have the like and we had this at the
beginning you simply have each worker to
be a TCP server that accepts these
messages and replies and then you
basically implement this power map
instead of doing it locally you send the
receive messages to the remote workers
yes no no no it's like it's like Burma
but it's not prime I'm going to because
climate is actually not what it actually
excuse each element is violent which is
not good anyway
because you want to split the workload
you don't want to spark that many
computations so when you do this the
first problem you encounter is that
serialization in a skill as to now it's
like super slow it's it's it's way too
slow for this sort of stuff and like
with civilization in a Cell I mean
binary and cereal this is not like
unknown like it's known that it can be
faster but it was a complete like
showstopper for this application and and
for us by far the biggest problem was
actually pretty like it's like a pretty
weird legacy problem about doubles are
encoded that they're encoded like in a
pretty bizarre way that that yeah it's
just very slow and we fixed it actually
I'm not sure if like with binary the
patch was in marriage because they need
to preserve backwards compatibility
basically in the format with cereal I
don't remember if it was fixed not maybe
Nikolas does I'm not sure yeah I don't
know but this like after we did this it
was like five times faster for our
application when we fixed this double
problem and also like a lot of slight
problems like unnecessary allocations
and stuff like this but it was still
slower than it could be and so we ended
up writing our own library and also
because we had another client that
needed very fast serialization and this
library is called store and it was
announced and yeah and you can use it i
think there was a question yes yeah so
the question is why didn't you new type
your state if i understand correctly
that I think the question is your
serializing the serializing mostly
vectors of doubles why didn't you new
type these vectors and in and have like
specific instances
for those that would be very fast that
is a solution and the problem with the
solution is that it's sort of it has a
development overhead whereby you need to
be careful about this sort of new types
and instances everywhere and you need to
communicate efficiently to everybody and
it's like 25 person people team that you
need to be careful i need to new type
the right thing also because the double
instance itself is low right even after
that fix if you just dump what's in
memory it's much faster basically what
binary early do is that they they see
realized too big and ian don't to make
sure that it's compatible across
different architectures and stuff like
this we literally just wanted something
that dumped the thing in memory and i
mean you can do it with these instances
but it's just a bit inconvenient in
terms of developing in like this victim
the other thing is that there are some
inherent problems in the state that's
carried over by binary in serial 11
parsing for example we never a
backtracking store like this is no
backtracking there's no like there's no
accumulation of x sings or anything like
this is just forward i think anyway when
I first I I actually didn't write store
like Chris dawn and michael Stone wrote
it but they also made the basic
machinery faster for example when
serializing store and knows how big the
message would be before you circle eyes
it so you can allocate the buffer to
bits of exactly the right size before
you you start actually filling it so
stuff like this it was just yeah it was
just easier to to to start from a clean
slate so yeah after we switch to store
the position performance doubled and
also the memory consumption doubled like
like was like a porter what it was
before and yes sir wasn't just for this
effort we needed to do it also because
of another effort but it was helpful for
this tomb so you make serialization
faster the second problem is that if
you're delegating like if every time you
want to evolve the state you're
basically transferring the state and the
input to the work and then you're
waiting for it to come back with a new
state any new input
this is a problem if the state is big
and in our case the state is like in the
hundreds of kilobytes and we have
thousands of states so we have like
considering like the round trips if we
do it this way you have like gigabytes
of data to be transferred at each round
of updates and considering like the time
that it takes to actually do the a place
which is like sub-millisecond like
milliseconds it's just it just still
dominated the performance of the program
and so the way you solve this I mean the
way we solved it anyway was to build
this system that can persist the states
on the work so you when you have the
initial states you just upload them
evenly across the work except you have
and you assign a unique ID to each state
and then you do not need to transfer the
state each time you can just say you
know give this input to state 1 2 3 and
then give me back the output because the
only thing that you're interested in is
really the outputs right so you let this
workers handle the state and all and
then there is a master process the sort
of orchestras updates and for example or
it balances the states where some when
some workers are idling the other cool
thing about this is that it is about
desert balancing is that we can add
workers at a problem in the middle of a
simulation so if a simulation is ramming
you can sort of see that it's going slow
these we're talking about things that
can run for half an hour an hour right
you can sort of odd things while they're
running and and in fact since there is
like it like in the final product of
this thing there is a web server sitting
in front of this of this of this
simulator what we can do is that we can
very dynamically scale up and down the
simulation like regarding how much we
value fairness or a much value latency
for example we can say ok and ran up
maximum of five simulations at once but
then if three simulations finish the
other two will fill in all we will
subtract why are all the workers and
start going faster while the simulation
still running so it's it's pretty simple
but it works well because like yeah you
just have this bookkeeping either you
have to do
being stated these two actual states due
to work sorry and then you simply send
the inputs to the to the workers of
course this only works if the inputs are
very small which they are in our case
this dates are big because again they're
composed of these big vectors of doubles
that represents that curves and stuff
but yeah it's going to be relevant now
okay so results this is another bad plot
and I apologize but basically at the far
left you see the performance with eight
chords and a 10 and the one at the top
is the purple curve that you saw in the
previous plot so at eight chords they're
actually similar so the speed up is a
bit it takes a bit to gain momentum
right now it's it's it's not perfectly
linear but then as you can see with 12
cores and with 18 chords with aiding
courses like twice the speed and then
you can throw more machines at it for
example 36 is to machines and 72 is for
machines and this is also not linear
right it's clearly not a nice linear
curve we would like it to be linear and
we try to make it linear but the
performance bottleneck that we are
hitting now is that the loop that
dispatches work to the to the slaves is
not fast enough so it's just a problem
of making this the message passing up
yeah the sort of central orchestration
program loop to be really really fast
and this is like very common problem
sort of events that need to go really
fast and yeah it's just there are queues
in and Q's out but again we're pretty
happy because we had to change super
small part of the code itself to
actually speed up and we also got this
nice infrastructure where you can throw
as many machines as you want added Alto
past four machines is not worth it
really i mean past 72 past a hundred
workers we also plateau but we blow up
like a handle instead of a tensile and
i'm confident we can make it go almost
linear if we make that loop faster we
haven't really worked on it much so I
think I'm almost out of time but I have
two small asides so if there are
questions
we can answer the question otherwise I
have two little stories of two other
problems that i found surprising but if
there are questions i prefer a three
questions because they're kind of I
related yes Simon so the question is
that sounds like what spark does in
Scala did you consider I don't
understand what I should have considered
emigrating to scala or using spark as
like just to do the orchestration and
keep the Oscar layer intact if right
right we did not consider it now and
simply because I mean I did not consider
it and I was the one like figuring out
how to make this go faster um the reason
is that I felt I would be quicker doing
it this way in a school and I think yeah
I think I'm right but sort of i had to
know i mean did this is a fairly simple
program in the end so i sort of
programmed exactly what we needed and I
was confident that in the mountain house
two months that I had that I could
actually do it and I just saw less
unknowns in doing it in a school
directly but it's a valid question i'm
also not familiar with back so that's
another problem yes same answer really
that like the way this went is that okay
we have this thing we need to go faster
rate and sort of i look at it and like a
look at the productivity sort of
bottoming out and going like that and
then i program this prototype that does
this and that takes me like 10 days or
two weeks to to do and since we need to
to be sure that this would work again
like cloud Haskell I think solves also
slightly different problems right but
for me choosing to do it this way it's
mostly about the confidence of being
able to deliver it but it's a very valid
question like I also haven't talked
about another part of this work which is
actually there is a job to in front of
this right we cannot if we got
and accumulations we cannot process them
all at once and that part then I I'm hey
I'm not really sure how much I can talk
about this in general but like I think
in that area is much less clear whether
you want to invent your own thing or not
because that solved many times in
different circumstances but for this
specific program I see it more as like
yeah as a very targeted thing rather
than some generic facility and again I
had never used the club as collecting
anger so that's always a derp back I
think when you need to do something in
two months yeah yes like workers failing
and stuff like that yeah so this job
queue that I've talked about reschedule
failed jobs and a worker dying mid
simulation is a simulation failure so
basically you will just redo the whole
thing there is no recovering of law
states because a worker failed it's just
not worth like it's just not worth
trying to do it also because I don't see
how you could do it like because you
have the inputs that generate the
initial states but the states in the
middle of the computation if a worker
dies while it's sold in the states you
would have to replicate them somehow but
it's yeah is just better off retrying
well if the if the worker fails you get
you get an error when trying to send or
receive messages from it right so I have
basically two implementations of this
message passing one based on SDM and one
based on sockets and and I use the SDM
once only for testing the correctness of
the socket version basically but in
production we only use the socket
version so the failure would simply be
that you know when you do the read it
just fails now of course if there is a
fire will in the middle then you might
get a timeout but we control the deploy
the deployment of this so we know there
are no it's like the same data center
they shouldn't be firewalls between I
hope maybe there are some buddies an
expert on AWS maybe they can tell me but
basically I just like if the
communication with work has failed the
simulation has failed and it will be
returned
anyone else know ok I don't know if I
have more time but I don't think so ok I
will not talk about the other system
alright thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>