<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Carat: Collaborative Energy Debugging | Coder Coacher - Coaching Coders</title><meta content="Carat: Collaborative Energy Debugging - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Carat: Collaborative Energy Debugging</b></h2><h5 class="post__date">2012-10-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Rfxbyv1ZMoA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll introduce Adam all inner who got
his PhD last year at Stanford and he's
going to talk to about some research
that he's been doing as a postdoc at UC
Berkeley yeah take it away all right
thanks Phil so this is joint work with
Anand and yon at the amp lab at Berkeley
and a couple of people at the university
of helsinki emil and zazu so Mobile is a
hot topic as you know sometimes this is
literally true this is an article
someone well it's a letter that someone
sent in to lifehacker complaining that a
phone which they believe to be sitting
idle in their pocket was actually
getting hot so it was clearly doing
something even when it wasn't supposed
to be this user is asking questions like
how do I make this stop and why is it
doing this and so anyone who has a
smartphone has probably had a similar
experience so they start their day and
the phone is fully charged and they
maybe don't do some work for a while it
sits idle for a bit maybe they actually
get something done sits idle and at some
point they take it out of their pocket
and it complains that the battery's been
depleted this I think is a pretty much
universal experience for people who have
these kinds of devices and for the user
there I think three primary questions
they'd like to answer first is why the
battery is draining is it something that
they did is it an app that they're
running and then the second question 1
which carrot is uniquely positioned to
answer as you'll see over the course of
the talk is whether or not that drain is
normal so if the user does the same
thing tomorrow should they expect to see
similar battery drain if another person
who has a similar phone does the same
behavior should they expect to see
similar battery drain and so on and then
finally what can the user do about it
their behavior that they can change or a
setting that they can change to improve
their battery life so today I'll be
talking about a tool that we built
called carrot and a method for analyzing
data that enables us to answer those
three questions we were talking about on
the previous slide I'll describe how we
go from the samples of the state samples
that we take on individual devices and
how we aggregate that and use that to do
energy diagnosis for the same users I'll
talk about some of the ways that we deal
with uncertainty in practice so as you
measure things on devices there are lots
of sources of uncertainty and
imprecision and I'll describe some of
the ways that we deal with those I'll
talk about our implementation
mostly on the analysis side and then
finally describe our deployment and some
of our results okay so just to give you
a little bit of background prior
approaches had been pretty much strictly
ad-hoc so there was some work out of
Purdue that worked with looking for
specific kinds of misbehavior so in
particular on Android they were looking
at something called a no sleep bug this
was a violation of the locking
discipline that would prevent the screen
from going to sleep and so if your
energy problem was not related to a no
sleep bug that approach would not help
you to figure out what was going on many
of the previous approaches were also
intrusive they would require you to
instrument the operating system or have
access to the source code of a
particular app that you were trying to
debug and this is frequently prohibitive
especially for lately people and then
finally you can find a dozens and dozens
of apps on pretty much every app store
that claim to help you with your battery
life and almost universally these are
generic pieces of advice it'll tell you
you know kill all of your background
apps or dim the screen basically just
don't use your phone as much as
essentially the advice that they give
which is also not very helpful it's not
actually telling you what was going
going on and whether or not that was
surprising behavior so our approach
looks like this this is a kind of
overview of the infrastructure we
collect data rather than from a single
device we collect it from the crowd
aggregate that in the cloud and do a
statistical analysis on it there and
then return to the users information
about for their particular device in
their behaviors what seems to be causing
the energy problems and whether or not
that's normal so as far as we're aware
this is the first collaborative approach
for diagnosing these sorts of energy
problems so we built carrot as a mobile
app for both iOS and Android and it
provides personalized energy debugging
so what I'll tell you is what's
misbehaving whether that's normal what
you can do about it and those are things
we mentioned before and I'll furthermore
carrot will also try to quantify how
much it will help and I'll describe how
we do that so the design goals of this
project were to pick up to a particular
design point in the space and so we've
chose the space that was the most
invasive method that works on both iOS
and Android so the things that you can
do that are specific to I their platform
but we wanted to do what is the sort of
most you can do that works on both of
these and so the primary constraint
there was what was eligible for apple's
app store that was sort of the the upper
bound of what we were able to do and so
the goal here is to just investigate how
far we can take diagnosis given only
that sort of information so there are
lots of questions related to well what
if you measured this what if you
measured that and we're looking at those
two but that's not the goal of this
particular stage of the project and i'll
be talking today just about carat as a
method that works on both of these
platforms so this is what some of the
screens of carrot look like on the left
and on the center you see what's called
the action list for ios and android and
what this tells you is based on the
app's that you're running which ones you
kill or attempt to restart so that you
can improve your battery life so if it
says I say kill pandora but it's
suggesting is that pandora is running on
your device and that it seems to be
consuming a lot of energy and in
particular it's estimating that you'd
get about an hour and a half more
battery life if you killed him which is
significant on the right side you see
one of the other screens which is the
device screen and this gives you a
little bit of information about how long
you should expect your battery life to
last as you currently use it and finally
something called a J score and the J
score tells you you're sort of
percentile battery life relative to
other similar users so this particular
screen is from a device that was in the
64th percentile meaning sixty-four
percent of the other devices got better
got worse battery life than this
particular device so sometimes google
would complain that they were
experiencing battery problems but had a
really good J score and this essentially
meant that they thought they had better
bad battery life but relative to other
people they were actually doing pretty
well and vice versa there are people who
have extremely low J scores and seem
really happy with their battery life
which I guess is the merit of low
expectations so we had a lot of Prayer
were a lot of privacy concerns when we
were first developing the app so people
expressed a little bit of wariness about
well what sort of information or you can
be reporting there have been previous
incidents about things like carrier IQ
and so on that reported rather large
amounts of personal information and so
we went to great pains to make sure that
you were not we were not collecting
anything that was personally identifying
or otherwise endangering people's
privacy in the end it turned out that
people say that they care about privacy
but in practice it doesn't seem that
they do and actually we would have in
retrospect like to have collected more
personally identifying information so
that we could help individual users with
support so we would get emails from
people saying hey you know I want to
understand this particular set of
recommendations but we have because we
collected nothing that ties that person
to the data that we've collected from
them we have in a way of looking up
their data and telling them anything so
you mean what what is an example of a
personally identifying piece of
information I mean we could literally
just say hey could you type in your
email address and then we can tie that
to the records they'll be done and we'll
probably do that in a later version just
because it seems like almost everyone
would be willing to provide that
information only be helpful boss so at
any rate so this was something that we
were initially concerned about but it
turns out that we probably should have
been more liberal about what we collect
ok so now that's a kind of overview of
what carat looks like and I'll start to
dive in here and describe the process of
how we go from the data that we collect
on the device to the diagnosis so the
way care it works is that periodically
it will wake up and get some time to
write down information about the state
of the device so in particular it'll say
what the toe write down the time stamp
it'll write down the battery level and
then a feature vector that includes
things like what apps are running does
the device have Wi-Fi access what
version of the operating system is it
running and so on and it doesn't just
collect one sample that collect samples
over time so in this case you see that
the the battery in the second column
there is depleting over time certain
apps are being closed or opened I Wi-Fi
access is intermittent and so on so this
tells us what the battery level is
instantaneously but we'd actually were
actually interested in rates of battery
drain and so what we'll do is we'll look
at consecutive pairs of discharging
samples and convert this into a rate and
so we do this by looking at the
difference in time the difference in the
battery level this gives us a discharge
rate %
second then we take the two feature
vectors and combine these two to get a
feature vector for this discharge rate
and so we can condition the discharge
rate on some set of features like what
is the discharge rate look like when the
user has Wi-Fi access or is running an
iphone 4 or something like that so from
these we can build probability
distributions these conditional
distributions of energy drain so to be
so just to be clear this is the
probability distribution and to be
concrete about it let's say that this
feature vector is devices that are
running Facebook so this is going to be
the distribution of energy drain when
users are running the Facebook app so
one way that we can characterize this
distribution is by comparing it with
other distributions so in particular we
might be interested in the distribution
when the users are not running Facebook
so if you see a pattern like this where
the average energy drain when people are
not running Facebook is significantly
lower than when they are running
Facebook we describe this as an energy
hog this is a particular type of energy
anomaly that we look for and I'll
explain what I mean by significantly
more energy in a few slides for now ok
so this is a pretty straightforward
characterization this is just one pair
of type 2 types of distributions that we
compare another thing we might be
interested in is Facebook running on a
particular device so let's say we're
looking at Facebook running on yawns
phone now if we only had one device
there's no way for us to characterize
whether or not this distribution that we
see is normal yeah so this is a critical
difference in between carrot and other
approaches so if we have the crowd
however we can say something about what
about Facebook running on other people's
devices is it the case that when running
on yawns device it seems to be using way
more energy than on other devices so if
that's the case then we describe this as
an energy bug ok so this is just a
particular terminology that we use to
describe comparisons between an app
running on one device versus an app
running on other devices so if it's
using significantly more energy than we
call that an energy bug so now we'd like
to do more than just compare these two
distributions we'd also like to say
something about how confident we are in
this comparison so to do this rather
than comparing just the original
distributions and looking at their
expected values which is what we were
doing
for just looking at the distance between
these two instead we're going to look at
the distribution of these expected
values because these are computed from a
sampling of some true distribution and
it turns out that these are normally
distributed which is nice and so instead
of comparing just the expected values we
can now talk about the relationship
between these two distributions and a
particular we can quantify our error and
confidence so we have both an expected
value and given some confidence interval
we can say something about that expected
value being plus or minus some value so
we can say the expected value is X plus
or minus some error E with fifty percent
confidence and if we like to be more
confident we can increase the size of
those error bars to some larger E and
say with ninety-five percent confidence
the true expected value is somewhere in
this range and there are two factors
mathematically that contribute to this
confidence interval the first is the
variance of the original distribution so
the more variance you see in energy
drain of a particular app let's say you
watch Facebook running on a particular
device and sometimes it uses a lot of
energy sometimes it uses very little and
so on you'll end up with a very wide
spread of this distribution it'll have
high variance and that will increase the
size of these error bars and similarly
you have the size of the crowd so
basically how much data you have so if
you decrease the amount of variance or
and increase the size of the crowd you
do either of these two things you expect
your error bars to decrease basically
increasing your confidence because you
either have more data or the data that
you do have is more consistent about
what it says about the energy drain and
the opposite is also true if you have
higher variance or less data than your
error bars get bigger okay so when we
say that something is significant we're
talking about whether or not one
distribution is far enough away from the
other distribution in terms of this
expected value and the error bar is that
there actually is this sort of
separation between these two error
ranges so they're actually separated
like this then we can say with let's say
ninety-five percent confidence that one
is in fact on average experiencing
greater energy drain than the other we
call that significant and for those to
this talk it's implicitly ninety-five
percent confidence
okay so one of the ways one of the
things we'd like to do is not just
quantify our error in confidence but
also increase the confidence that we
have and the recommendations that we're
giving and one of the ways that will do
this is by doing classification so we
can take a distribution and we can split
it up into two other distributions
conditioned on the some feature so we
can say this is a distribution let's say
the original distribution was running
Facebook we can split it up and say
let's look at the energy drain when they
had Wi-Fi versus when they did not have
Wi-Fi now by doing one of these splits
we're decreasing the amount of data that
we have in each of these leaf
distributions and as I just said this
decreases our confidence in the
recommendation that we're giving but it
may also be the case that performing one
of these splits decreases the variance
of these distributions and it may do so
significantly enough that actually our
confidence our error bars are smaller on
these split distributions than on the
original one so if we if we see this
sort of behavior then we'll perform one
of these splits and you can build a
diagnosis tree like this so you can
split on various features sort of
digging down until you actually find the
the conditions under which energy drain
is significantly higher or significantly
lower and so on and these diagnoses
trees allow us to compute diagnosis of
the following kinds of forms you can say
something like killing some app a will
give you X plus or minus some error E
minutes of battery life with ninety-five
percent confidence as well as to supply
alternative diagnoses so just saying
well you could get a similar effect if
you upgraded the OS to some other
version v so I don't to pause right here
just are there questions about this part
of the talk yeah or saturday you mean no
the sampling interval yeah so this
depends on the operating system so on
iOS for example you can't schedule
regular intervals to wake up so all we
can do is subscribe to various
notifications and then we're at the whim
of when those notifications get
triggered these are things like when the
battery level hits 5 percentile marks
when someone plugs or unplugs the device
and so on so there's more flexibility on
Android in terms of when you can
schedule these sorts of things and so it
depends basically this answer
on Android it's still pretty infrequent
so once every five minutes or so and
i'll talk about what sort of overhead
this incurs a little bit later that's
right that's right so the granularity on
any individual device is very low and so
one of the reasons that carrot is able
to work at all is because we do this on
a large number of devices yeah ok so
this segues nicely into the question of
dealing with uncertainty so there are
lots of sources of uncertainty and I'll
talk about just a couple of those in
this talk so for 1 i'll talk about the
case of iOS and measuring the battery
level so the battery api on iOS is kind
of interesting so there's a particular
type of notification called battery
level changed and when that trigger is
the battery life that it tells you the
percentile is roughly accurate so if it
says 85-percent then the true battery
level is roughly around eighty five
percent but for all other notifications
under all other circumstances where you
ask the battery API what is the battery
level it can be as much as five points
above the true value so it may say
eighty-five percent and it could be
anywhere between eighty percent and
eighty-five percent so when we were
first starting to develop carrot and
discover that this was true it was a
little bit disheartening we thought it
might be a showstopper so but we came up
with this neat statistical trick that we
used to deal with this kind of
uncertainty what we're going to do is
we'll take the measurements that we
believed to be accurate these battery
level changed events and we'll use this
to build a prior distribution and this
prior distribution is what we'll use to
fill in the sort of uncertainty gaps in
the other information as I'll now
describe so you observe the battery
level over time and let's say this blue
line is the true battery level and
carrot every now and then is able to
wake up and take these samples and so
the green dots you're going to be
samples taken when the phone was
discharging and the red ones when the
phone was charging so we'll throw out
the red ones and now let's look at some
particular consecutive pair of these
discharging samples and as I said this
is how we compute these rate
distributions so if green is the value
that the battery API returned we know
that it can
be as much as as low as the extent of
this black bar thats hanging down from
it and so the actual rate of battery
drain between these two samples could be
as low as why meaning the low point of
the first value and the high point of
the second or as high as X the high
point of the first one in the low point
of the second one and so these are our
bounds below and high potential battery
drain and we use this to take a slice of
the prior and then once we have the
slice then we build a new probability
distribution and rather than account a
single value as the rate of discharge
between these two samples we instead
have a distribution of discharges and
this is based on the accurate
measurements that we took from these
other events these battery level changed
events so this is one of the ways that
we deal with uncertainty is by using the
statistics that we have from other
measurements both from that from and
from other phones to essentially fill in
these gaps okay so another question you
might ask is whether the sort of tricks
work does the the sampling that we we
get from carat match the reality of
energy consumption and also how much
does carat cost to run so if you have
this going on your device and taking
these samples are you joining your
battery you know just as badly as
whatever the offending app was so to
test this we did a couple of things so
one was that we got this monsoon power
monitor which is a neat little device
that you can use and actually the
previous slide shows it on the left here
and we rigged it up to an iPhone so that
we could measure the actual rate of
energy drain and we also partnered with
a company battery company called latent
energy down in the south bay here which
has a bunch of battery testing equipment
and so they can hook up devices to their
machines and run various usage scripts
and so for the iPhone and a galaxy tab 2
we did these experiments where we would
run eight to 10 our usage scripts on
doing things like browsing the web email
and so on these were repeatable scripts
and tested what the battery drain was as
well as looking at carat taking samples
and then a third experiment where Kari
was not running but we ran through the
same usage script and so we'll use these
to quantify how accurately carat is able
to measure the batter the energy
consumption and also how much energy
carat is too
hang up yeah it's how can you guys just
publish these badges but your question
is have we published the the stats the
stats being which stats oh just like
what what are the raw numbers of how
much energy gets consumed by various
things so so we mostly been focusing on
the app developers at this point we've
talked a little bit with folks at apple
and you know I'm here hopefully to talk
to the Android people but it seems like
they don't care about energy or
something I don't know if they care they
can talk to me I'm I did my part so the
customer service at monsoon was
fantastic so they we had some trouble
getting the iphone hooked up to the
device as it turns out Apple products
are not intended to have wires soldered
onto them and whatnot they're not
designed to make that easy and so they
actually dressed this hand diagram I in
the picture I guess that's like a TI 82
running Windows or something like that
but supposed to be an iphone I don't
anyway so it worked and that was pretty
exciting ok so just to give a little bit
of a feel for some of these numbers
obviously there's a lot of data here and
I'm not going to talk about all of it
but the the first high-level bit is that
the values that we see on the battery
indicator that's returned by the API
agree with what you see coming out of
the the end of the bat of the pop our
monitors okay so these are just numbers
that more or less track each other the
second high-level bit is that you get
pretty good accuracy in terms of using
the prior to compute what the energy
drain distribution is so this is this
orange curve is the discharge
distribution as estimated by carrot and
the green black ones are the
distribution as measured by the numbers
that we got out of the power monitor and
you know they don't match exactly but
the thing to recognize is that this is
from one device over a single day and
this is information this is data from
the the iphone and so over the course of
that experiment carrot took nine samples
it only measured the battery level nine
times but using the prior
was able to get this accurate of a
distribution the other high-level bit
here is that running with and without
carrot is basically the same amount of
battery and just to give a little bit of
quantities to these during the
experiment we did with the galaxy tab we
only carrot only miss has to miss
misunderestimated underestimated the
battery drain by point zero zero zero
one five percent per second on iOS it
was a little bit worse so these were
fairly good given that they only had
less than a dozen samples compared to
the thousands and thousands that the
power monitors were measuring and the
second is that carrot is extremely low
overhead so during the experiment where
carrot was not running in its place we
ran the standard weather app and carrot
uses less energy than the weather app
weather app uses three point five
percent more of the battery over the
course of this experiment than the
weather okay thing carrot okay so that
was the ground truth experiments I'll
talk now a little bit about the
implementation focusing on the backend
so as I said before this is what the
carrot infrastructure looks like you
have the crowd which communicates data
to a central server which is then sent
to the backend analysis and mostly today
I'll be talking about this back-end so
to implement the analysis we use the
language which was developed at the amp
lab at Berkeley called spark spark is a
cluster computing framework that uses a
structure called resilient distributed
data sets and the the merit of these
distributed data sets is that they
reside in memory and they persist across
runs of thing runs of jobs and so the
idea is that rather than like a
MapReduce job where you have to reload
everything into memory if you want to
run it again they instead stay resident
in memory and the intention here is that
you will run iterative jobs or
interactive workloads and so this is one
of the merits of the rd DS they also
provide various other features such as
the ability if a node crashes to
efficiently recompute the data that was
lost and so it's actually quite a cool
language and we've we've had a lot of
success with it and I'll show you some
of the the parallelization numbers and
subsequent slides so one of the main
challenges that we faced when trying to
paralyze this was the process of
computing the converting the samples to
rates and the rates to distributions and
to get the samples to rates one of the
main challenges there is that we had
this inter sample dependency we're in
order to compute a rate we needed these
consecutive pairs but then the
subsequent rate needs the second sample
from the first one and so on and so one
of the things that we had to do is
convert everything into these
consecutive sample pairs and use those
as kind of the unit of information so
there was some replication of data going
on there but it removed this dependency
that we had between samples and let us
paralyzed much more efficiently and then
this is a slide that's showing a little
bit about how we do the conversion
between rates and distributions just to
give you a sense of what a workflow
might look like in spark and it should
look very familiar to you its operations
like map and reduce and group by and so
this uses the same sort of language as
you see in MapReduce operations except
spark supports general graphs instead of
just specific kinds of graphs okay and
so we were able to leverage the spark
parallelism in order to do this and use
our dd's to good effect and I won't talk
too much about the details here is I
think it's fairly self-explanatory yeah
and if there are questions about it then
I can field those individually okay so
carrot as you might imagine is very low
traffic so any individual device is not
communicating very much information to
the servers on average it's way way less
than a byte per client per second and
this means that with relatively few
servers we can handle all of the both
incoming and outgoing traffic that all
of our users generate this is currently
being handled by five AWS instances
running with a load balancer in front
but really this is overkill ok so the
server itself is not a bottleneck for us
I'm just sort of showing you this to say
don't worry too much about the central
server the main issue is dealing with
the analysis so parallelizing the
analysis was absolutely essential we had
done a sort of naive parallelization of
it when we start
it out with our initial sort of lab
distribution of a few dozen users but as
you as well described later this fairly
quickly exploded and we had to do a
proper parallelization of it in order to
make it scale so just to compare here
this is a demise cereal implementation
and on the x-axis you have the number of
samples so the red is the optimized
cereal implementation and then the green
one is when you actually paralyzed this
in spark okay so essentially by the time
we have the number of users that we
currently have which is about 340,000
the analysis would have taken more than
a day to run if we stuck with a sort of
cereal implementation in spark we can do
the whole thing in about 45 seconds from
scratch okay all right so I'm gonna
spend the rest of the talk describing
the deployment that we have and
explaining some of the results just
giving some examples of some of the
anomalies that we've seen and so on yeah
yeah so we're changing how this works so
I'll explain the the process of how from
a user's perspective they interact with
the carrot app so currently the way it
works is you start up carrot and it
basically just doesn't have any
information for you it will start taking
samples and sending stuff as a server
but we'll just say that it has no
results yet and this was true initially
just because we had so little data that
we couldn't boot strap it on anything
but now that we actually have users the
intention is that you can immediately
report what apps that user is running
and give back to them information about
the hogs so the very least we can
populate the hogs list when they first
open the app and this is something that
we intend to do soon the bugs on the
other hand require us to observe that
particular device over time so it's not
the the speed of this is not bounded by
the analysis speed in terms of how
quickly can we tell them about bugs it's
really just a function of how quickly
can we get enough data from them to have
enough confidence that we can say
something about which of these apps are
misbehaving and so that's really the
limiting factor the reason that having
an analysis that runs quickly
is important is it first of all we were
not like it wasn't like 45 seconds
versus a minute and a half or something
that was the issue it was the do is
taking more than a day for people to
start getting any sort of results and
that was frustrating them and us another
thing that's nice about having it be
only 45 seconds is that these diagnosis
trees that we build we can go much
deeper and start to look at things like
combinations of features as opposed to
just individual features and splitting
based on those so all of these basically
give us more flexibility in terms of
what sort of situations we can look at
all right so initially back in i guess
late january early February we had a
sort of little iOS implementation and we
distributed it to a small number of
people we are allowed you're allowed to
basically get a hundred people to sign
up for this this is a cap that Apple
imposed of those people 75 installed the
25 who signed up and then did not
install unfortunately we couldn't reuse
their spots you can't recycle unused
spots for for these sort of beta
distributions which was frustrating but
so our this initial deployment with 75
people and from these 75 people over a
couple of weeks we collected about
10,000 samples so the first thing to
note here is this is a really small
initial deployment okay this is very
little data and so despite that fact we
were able to find 35 apps that exhibited
energy bugs anomalies in general but 35
in particular that exhibited energy bugs
and these were in popular apps so things
like Facebook and kindle and Flipboard
just a digital magazine app and we were
able to corroborate these things with
forum posts and news articles and even
in some cases the the implications of
some of these forum posts with there was
that there were particular features that
were triggering these miss behaviors so
interactions with a particular OS
version or issues when they didn't have
Wi-Fi access and so on and we were able
to actually corroborate those with the
data that we had so despite this
relatively small deployment we were
already starting to find some
interesting energy anomalies so this is
a very encouraging initial result we
also tried doing some synthetic bug
injection so we took the Wikipedia
mobile app and we wrote in some
behaviors miss behaviors that we could
manually trigger they
would cause the app to use a lot of
energy basically abuse a resource like
the GPS or the CPU and so on we
installed this on one of the devices in
the deployment and then tested whether
when we triggered that Miss behavior
whether carat came back and said it
looks like Wikipedia running on that
particular device is exhibiting an
energy bug and in all three cases did so
and in no other cases on any of the
devices running Wikipedia mobile did it
say that it was anomalous so essentially
it did the right thing in this injection
experiment so both of these initial
results were pretty promising and so we
developed an android version and
submitted both of these things to the
respective app stores and then after a
bit of a delay finally in mid-june they
were they both got into their respective
app stores and then we got featured on
techcrunch which kind of took us by
surprise and I was very exciting so we
were we were pretty thrilled that we
might go from like 75 users to some
larger number and then lifehacker picked
up the story from TechCrunch as did
dozens of other news sources so within
24 hours there were at least dozens of
articles and we rocketed to more than a
hundred thousand users in less than a
day so I basically woke up and then
checked flurry and said oh no that's a
lot and then within the 24 hours after
that we went to 200,000 users and so the
rest is history so we today we have
about 340,000 devices most of those are
iOS but Android is gaining quickly I
think mostly the reason that iOS started
out ahead is that the the TechCrunch
readership I think is a little bit
biased toward toward Apple devices I
can't back that up that's just my
impression of of the crowd and so far at
last count we have something like 20
million samples okay so this is a far
cry from the ten thousand that we used
to have and obviously there's a lot more
that we're finding in this data so to
give you a sense of what these numbers
look like what sort of energy anomalies
we're seeing about nine point four
percent of the reported EPS qualify as
energy hogs this is about 11,000 apps
these include the ones that you might
expect like Pandora and skype things
that you use a lot of resources what's
surprising is that some of the
energy hugs are not apps that you would
expect to be energy hogs so for example
there were a family of basically airline
search related apps so you could check
whether there were flights available and
so on we're using just far more energy
than they ought to be using compared
with other similar apps and also we
found lots of instances of energy bugs
so about five point three percent of the
instances of apps running on devices
qualifies as buggy instances these
include things like kindle and facebook
and youtube and i'll describe a couple
of examples of energy bugs and
subsequent slides okay and obviously i
mean can't even scratch the surface on
some of the interesting stuff here so
i'm just going to give a couple of
examples if you're interested in seeing
examples of energy hogs and energy bugs
I encourage you to run carrot on your
device and you'll probably find some so
one example of an energy bug that we
found was Kindle running on iOS I
particularly like this example I think
it was really interesting so this was
reported as a bug on three-point-nine
percent of the client so a relatively
small fraction of the people running the
kindle app we're seeing way more energy
drain than the other users and this has
already been sort of discovered by
frustrated users who were running the
kindle app and they were complaining on
on the forum and one of the theories
that was pushed forward was that it was
related to this whispersync protocol so
if you've used one of these kindle apps
before as you know it synchronizes your
bookmarks and the the books that you've
purchased obviously annotations and so
on between your various Kindle devices
so it turned out that there was a bug in
this implementation a bug in the
protocol such that when you did not have
access to Wi-Fi the kindle app would use
significantly more energy than if you
did have access to Wi-Fi i thought this
was interesting because it's kind of
counterintuitive i would think that you
would use more energy when you're using
an energy hungry resource like the the
Wi-Fi then when you're not it turned out
to be the opposite and it was a fairly
significant difference so if you just
turned on Wi-Fi when you were using the
kindle app you'd get another 36 minutes
of battery life and this is a statement
that we can say with ninety-five percent
confidence that for a typical buggy user
of the kindle app that's what they will
see
and the the diagnosis tree that we build
looks something like this I'm just
showing a part of this tree to give you
a sense of the sort of information that
we can compute so this is saying the
battery life without kindle versus
running with Kindle one thing that's
interesting is that actually you get
better battery life running Kindle then
when running standard other apps so so
when I say without Kindle that means
running an average of all the other apps
that someone might run so in other words
on the whole kindle is a very
energy-efficient act
we can displaying something on the
screen it's not like you're playing a
game that's using a lot of the CPU it's
not constantly using the network and so
on so it's actually not an energy hog
which is important to recognize so it's
actually a fairly energy-efficient app
in general but it turns out that during
this whispersync process it uses a lot
of energy so you can see this by looking
down the next level on the tree which
describes network connectivity so this
with kindle is 8.4 hours is you know
this sort of agglomeration of the three
distributions below it when the
network's off versus when you have these
two different types of connectivity so
what this is saying is that when the
network's off and it's not trying to do
this synchronization you get pretty good
battery life is we're using basically no
energy but when it's actually doing the
same its energy inefficient and in
particular it's less energy efficient
using 3g than when running Wi-Fi okay
and this is the sort of surprising
aspect of this diagnosis to me at least
so before I move on is our question
about this yeah so the difference
between running with the 3g versus
running with the Wi-Fi so essentially if
you're in the 3g node and you turn on
Wi-Fi then you move over to this Wi-Fi
node and so that's the difference in
energy that you would expect to see
during the synchronization process so
the third the 30 do you but you you at
your 3g radio it's on anyway right
yeah so well some of these resources are
complicated right so like the radio for
instance has tail energy that you need
to consider so if something else uses
the radio then you pay for that tail
energy for some period of time following
the actual use of the resource and so on
so all of these computations that we're
doing are just talking about averages so
it's very hard to say for a specific
device what's gonna what's likely to be
the energy consumption because there are
lots of kind of chaotic things going on
so but as a sort of general statistical
statement users who have Wi-Fi access or
you are getting better battery life
maybe I'm naive to be surprised by this
but anyway okay well well then there we
go okay so turn on Wi-Fi kids I guess is
the moral ok so I'll move on so another
example is the Twitter app on Android so
most of you are probably familiar with
Twitter the this was reported as a bug
on about fifteen percent of the the
clients running it so this is a
relatively large fraction I think if you
look at the the diagnosis tree which we
sometimes call an MCAT for reasons that
I won't go into it implicates the
operating system version so in
particular what it suggested was that
users running ice cream sandwich for
point no point for we're seeing 94 / 94
minutes more battery life than Twitter
users who were running any other version
of the operating system so there was
something going on with Twitter and
interactions with these other versions
of the operating system that was causing
a significant drain on the battery life
so this was a serious problem it also
turns out that Wi-Fi helps maybe this is
this is now becoming like a thing get on
the Wi-Fi network ASAP so this is
interesting so this is another
actionable thing that you can do so in
addition to the recommendations of
killing a nap or restarting a nap
carrick sometimes will tell you upgrade
your operating system and currently that
recommendation at least on the UI side
is only coming about if you see that
users of a particular version of the OS
as a whole are getting much better
battery life but on the back end there
are all sorts of things that we're
computing that would allow us to say you
know you're a Twitter user and so
you're going to use the Twitter a lot
you should definitely upgrade because it
seems to make a big difference so we're
computing all of this on the back end we
just haven't been focusing on the UI
recently such as research I suppose okay
is there a question about this story of
a friend move on okay cool so in
aggregate what sort of effect does
carrot seem to have on battery life so
I'll describe this plot and then there
are a bunch of caveats in trying to talk
about this sort of data that I'll
describe as well so the x-axis here is
the days since the first report that
carrot gives the user so the user uses
it for a while and then at some point
carrot thinks that it has enough
information about that user to report
back and say here are some energy hogs
and some energy bugs on your device so 0
here is the day that Karen has made that
decision and sent out that information
it does not necessarily reflect anything
about when the user first read those
pieces of advice when the user first did
anything about them and so on all that
is very hard for us to measure from our
side of the fence but in aggregate what
it says is that after 10 days the
average carrot user sees about ten
percent more battery life and then after
90 days it's Luke keeps up to about
thirty percent so this is a significant
improvement in battery life there are
obviously a number of caveats with this
so the first is that we have a very
biased sample of the population this is
users who sought out and installed and
then continued to run carrot for 90 days
or more so frequently these users had
pretty bad battery life to begin with so
the baseline may have been quite low the
second thing is that users typically
will not do a nice controlled experiment
for us will they will change no other
behaviors except running carrot which is
you know unfortunate but that's what we
got so there there may have been other
things that these users were doing
during this period of time that caused
their battery life to improve so one of
the things that they could have done
even is like actually replaced the
battery with a new one at some point
during this this process okay so this is
aggregated over hundreds of thousands of
users but there are various confounding
factors that we just don't have the data
to disambiguate all we can say is that
in a general sense it seems like users
of carrot are improving there
battery life by double digits even after
relatively short amounts of time yeah I
don't have those numbers for you
unfortunately we did compute them but I
just don't remember them it's it's a the
95 percentile band so it starts out and
it's pretty thin it gets a little bit
bigger as you go out in the graph there
are fewer users that's sort of why you
see this jaggedness there are fewer
users who have been running it for that
long just because we haven't been around
for that long I mean 90 days is almost
the entire lifetime of the project yeah
so okay so one thing I talked about
before is this convergence of the error
bars the increase in our confidence as
we get more data and so this is an
example of this happening this is an
aggregate of the error bars and the
expected value estimates for several
hundred of these energy hogs so on the
x-axis you have the number of samples
and so again we have like 20 million
samples or something so these are really
small numbers that we're looking at here
in terms of how many samples and on the
y-axis you're looking at the relative
expected value which is so zero would be
a perfect estimate of the true expected
value based on all of the data as
opposed to just this sampling of it and
then the error upper bound and lower
bound are these dashed dotted lines so
this is your would essentially what
you're supposed to see here is that
those to converge very quickly so your
error decreases rapidly as does your the
accuracy of your air estimate of the
expected value okay and this this seems
to be this is very helpful to us I guess
this is the point so this is a property
that we relied on and are glad to see in
practice okay and then the last thing
that I'll mention in terms of the data
here is the prediction accuracy and this
graph requires a little bit of
explanation because actually very hard
to visualize this statement essentially
what's happening is that carrot is
saying if you killed this app you'll get
let's say an hour or more battery life
plus or minus 10 minutes with
ninety-five percent confidence let's say
that a user sees that kind of a
statement so we'd like to do is assess
the accuracy of that
then over a large number of users over a
large number of these recommendations so
if you had perfect prediction what that
would essentially mean is that if a user
so what an hour plus or minus 10 means
if they went from using it all the time
to using it never they should see an
hour plus or minus more battery life but
this is a sort of we treat this as a
linear relationship for the purposes of
of assessing its accuracy and so if they
go from using all the time to just using
it say fifty percent of the time then
they should see half of that benefit
should go 30 minutes plus or minus
instead of an hour plus or minus and so
this is essentially describing this our
number essentially describing the slope
of a line and so we can characterize the
slope that users see in practice when
running these apps so we can say the
user running this particular app when he
goes from Z at sixty percent of the time
to twenty percent of the time how does
his battery life change and so you can
do this over a large number of users for
a large number of these recommendations
so each of the so the green line here
would be perfect prediction that would
be the the slope if we get it exactly
right and each of the other lines is
representing a particular recommendation
for a particular user so one of these
lines might be you know Dan's phone
running Facebook and we told him 45
minutes so it should be that we should
be right on that green line if it were
perfect prediction and The Closer these
lines are to the green line the closer
we were to being correct so the gray
lines here are ones that were within the
ninety-five percent confident bounds for
that particular recommendation so if we
told him an hour plus or minus 10 and it
was an hour and seven minutes then that
would be within the the confidence
interval within the error bounds and so
that would show up as a gray line here
and the orange lines are ones that fell
outside of the confidence interval so we
measured this for a large number of
these recommendations and for the
ninety-five percent confidence bounds
95.4 percent were within those bounds so
this is exactly what one would hope you
know just just under the wire there
right so 95% would have been the lowest
that week ago so so this was a very good
result for us we were pretty excited
about it basically what it means is that
we're able to not only say with pretty
good accuracy what
sort of battery life improvements users
should expect to see but even the error
bounds that we were computing seemed to
encompass the right fraction of the
users yeah it is it's a relative slope
so it is yeah so I mean this I wanted to
get a lot of information into one graph
and so the idea here they are actually
yeah there are slightly different slopes
yeah it's it's a difficult graph to
describe I'm trying to figure out better
ways of visualizing the sort of
information but the high bit here is
that bullet point which is that for a
ninety-five percent confidence bound
more than that or within that that error
so it was good okay so I'm just going to
wrap up here by talking a little bit
about what we're working on now so the
first is we're still working to improve
the air and confidence bounds that we're
giving two people so we'd like to be
able to give more complicated diagnosis
and still be able to attach these sort
of confidence intervals so telling a
particular user that you know when
you're running Twitter you should turn
on Wi-Fi or something like that or
upgrade to Ice Cream Sandwich 404
another thing that we're looking to do
is build an API for developers so if we
for example see that Twitter is running
is exhibiting an energy anomaly on
particular subset of the devices we
don't know for example whether that
subset of devices is just has a
particular setting inside of the app or
if the user is just interacting with
that app in a particular way that's not
something that's visible to carat so one
of the ways that we can get around this
is if the developer of that app adds a
little bit of instrumentation something
that you see in like flurry or similar
sorts of libraries and so that's this
would let us know what are the settings
of that app and so on and give us
additional features that we could mine
for relationships between energy
consumption and those features so this
is part of a sort of vision that we have
for doing this sort of collaborative
debugging just deploy to the crowd and
debug in the cloud there's nothing
Pacific to the analysis about energy so
that semantics is entirely visible to
everything that the backend does so
there's no reason that we can't always
you also use this for performance
debugging for example or really the
consumption of any of these resources
and more broadly we're starting to look
at this as statistics as a service so we
we've been talking about applications of
this to other domains for example one of
my favorites is the bodymetrix so a lot
of people have like sleep monitors and
running monitors and so on and you could
ask similar questions about those sorts
of data so compared with other people
who have running habits like I do how do
my sleeping habits compare why am I not
getting enough sleep how do the people
who are not getting enough sleep what
are the dimensions in which they differ
from the people who who are who do and
so on so anyway it seems like there are
a lot of interesting questions that you
can ask of the kinds of of the form of
the questions that Cara tasks and so
we're starting to build up a sort of
statistical service so you can start to
answer those questions if you want to
download carrot either for iOS or
Android there's a link at that URL the
client code for both platforms is on
github so you know feel free to fix any
bugs that you find and help us improve
the app we certainly are short-handed so
we could appreciate we appreciate the
help but yeah so go download it we're
keen for feedback and we'd love to talk
with some folks here about some more
ideas so oh wait that's all questions
yeah
they are and that's a rapidly so the
question was how do we build those
decision trees what's the algorithm for
exploring the feature space and that's a
rapidly changing question so currently
it's fairly brute force the the feature
space that we're considering is a little
bit constrained so we we're only looking
for example at Wi-Fi access the device
model operating system version and a
couple of other things so doing a brute
force exploration is still tractable in
the longer term we're obviously looking
at ways of prioritizing how to explore
the space etc and so yeah I'd be happy
to talk with you about that actually you
have ideas yeah yeah so the question was
could we get more attention by shaming
people into being aware of various
energy problems and so on and yes we
could so the issue here is that it's
basically three people working on it one
of whom is international and none of
whom as a primary job is trying to like
build a product or garner attention and
so on and so there are lots of cool
things that we have in our heads that we
would love to be doing we just don't
currently have the people to do it so
that's a problem that we're trying to
solve but yeah we have more data than I
ever expected us to have so yeah it's
really exciting actually alright cool
thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>