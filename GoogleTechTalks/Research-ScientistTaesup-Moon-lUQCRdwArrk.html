<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Research Scientist--Taesup Moon | Coder Coacher - Coaching Coders</title><meta content="Research Scientist--Taesup Moon - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Research Scientist--Taesup Moon</b></h2><h5 class="post__date">2008-05-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lUQCRdwArrk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">looks like it's a familiar screen to
everybody suppose you want to correct a
misspelling from a corrupted text like
what Google search engine is doing every
day or imagine that you have scanned
image and you want to remove noise that
are injected on scanner these problems
can be formulated in terms of a more
general framework called discrete
denoising which is where you want to
clean up a discrete data they are
corrupted by discrete noise a main goal
of this problem is to extract clean or
relevant information from luigy
observation and has been one of the
central interest of many statisticians
computer scientists or information
theorists hello everyone my name is
Tessa moon and I'm from Stanford
University today I'm going to introduce
a new take on this discrete de-noising
problem and present a scheme that will
work universally well on any kinds of
this discreetly noticing problem and
which will become even powerful when the
source or the signal is time varying or
a spacefaring and this is a joint work
with my advisor professor ducky Wiseman
at Stanford my talk consists of three
parts background motivation and main
results so discreet de-noising problem
is essentially an estimation problem in
this problem you have a discreet source
which I denote as x1 through xn and it
gets corrupted by discrete noise and
results in the noisy sequence z1 through
ZN there is a discrete danuser which
observes this noisy sequence and it
tries to estimate or reconstruct the
source by x1 head through xn hat by
discreet I mean the valve the values
that X Z and X hats are taking in the
finite alphabet for example English text
in 26 alphabets or binary signal to
office given the setup the goal of this
script in using problem is to choose X 1
hat to xn hat as close as possible to
the source x1 through xn based on the
entire noisy sequence z1 through ZN i'll
specify more precisely what i mean by
this as close
as possible as I go along a binary
version of this discreet denoising
problem is as follows xn is a clean
binary bits trim like this now suppose
there's a noise which flips bit with
some probability point one now observing
this noisy sequence the user tries to
reconstruct xn x x hat n maybe it
corrected some error but since it
doesn't know which bit is noisy and
which bit is clean it may also inject
some errors like this but hopefully it
reduce it reduced the error from the
noise and the performance of the danuser
is measured by in this case bit error
rate which is the number of errors / the
sequence length n so again the goal is
given luigi sequence how will choose
exhale n to minimize this bit error rate
now life is easy if the new user knows
both about the source and the noise for
example in the text correction problem
is the text character knows that the
text is coming from English and he knows
how the tape typo is made then probably
he will soon figure out this K in this
book is a noisy symbol and K replaced
with L is much probable or in English
another example suppose now source is a
binary bit but it's coming from binary
Markov chain with transition probability
P and then suppose it knows that the
noise foot spits independently with some
probability Delta the resulting noisy
sequence in this case is called a hidden
Markov process and there exists a simple
optimum scheme called for backup
recursion which can minimize the bit
error rate for estimating this bit
stream in this case so this setting of
known source model and noise model and
trying to do the optimal thing with
respect to this model is called the
Bajan setting however look in real life
life is not that easy because we often
don't know what the source is for
example in the binary bit kit bit stream
case we can have any binary bits trim
like this but we don't know where it's
coming from then even though we know
what the noise is and no is probably
t delta is it's not clear what scheme we
should apply to minimize a bit error
rate in estimating this bit stream
another example is it is an image
denoising suppose the possible clean
image source can vary from einstein or
some scan text or monkey or a letter
image again even though you know what
the noise model is since this images
have different characteristic for each
from each noise so different denoise ur
will do better for each each each images
so it's not clear what kind of dinner
sir we should employ if we don't know
what the underlying image is so these
kind of scenarios happen often in real
life because the source model that we
take may be wrong or the source may not
have any any probabilistic model for
example images it's not come from in a
probabilistic model it's just an image
or the model that we assume may be
changing over time in fact our everyday
life is also like this the world is full
of noise and we try to extract something
from that noisy world but we don't know
what that something is so the name of
the game throughout this talk is that we
try to learn the source by knowing the
noise we're specifically I assume that
the source is unknown but I assume that
the noise mechanism is known to the
dinners and this is and and I focus the
noise mechanism to be case for the
memory this channel by memories I mean
the noise components are independent
over time now for this memory of this
channel it's completely characterized by
this channel transition matrix pi whose
x z element stands for the conditional
probability of the noisy symbol taking
value z given that the clean symbol was
x the goodness of de-noising is measured
by given loss function lambda and the
simplest example of this loss function
is the hemming loss where it takes value
one if the reconstruction is different
from the source and 0 otherwise so if we
apply this hemming loss to the bit
binary bit case it's just counting the
number of
so given this setup we as a cush we ask
a question can we steal the noise as
well as if we know the source
distribution even though we don't know
anything about the source so with this
setup and this question is this is
called the universal setting now
throughout this talk I'll focus on the
universal discrete denoising problem and
I'll present that we have devised a new
universal discrete in user called the
stud algorithm but before I explain what
studies and how it works I'll first have
to briefly overview the previously
developed scheme call the dude dude
which stands for discrete Universal
danuser is the first universal
description oyster that is device in a
recent paper by Wiseman at all I'll show
that this dude is tailored for
stationary sources and then I'll devise
a new universal discrete in user which
generalizes this dude to a more general
case where the source is non-stationary
so having this rough road map in mind
let's see what do is and how it works ok
so dude is the first universal
discreetly new user and it's pretty
intuitive scheme for each location T
that it wants to Dino is it first fixes
the window size 2k and then at each
location it looks at the left k and
write k of this symbol this left and
right k symbols are called the context
of this noisy symbols ET and once it
identifies what what is the context of
that symbol it tried to search over the
whole noisy data and finds the places
where it has the same context as this
point okay and then it counts what kind
of symbols occur hammock for how many
times at this location for the position
that has the same context and once
getting this count it applies some
simple rule which depends on the channel
matrix pi the knowledge of the channel
and the last function and discount
vector and this noisy symbol at time T
so I won't go into the detail of what
kind of simple rule it's a point but
let's look at an example and see how it
works yes so all occurrences of letters
with the same context I shall see if I
understand the directors meaning ZT of
course is a ziti yeah with those
contacts l12 lk are 15 k yep so now
those are going to be noisy as well yes
yes exactly the context exactly yeah
these are noisy context but it just
tries to search over the whole musi data
where it has the same context and counts
like what kind of symbol occurs at the
middle point and how many times it
occurs up so let's look at an example
suppose we have some noisy list text
like this and let's say it gets
corrupted and dude observes this noisy
text okay and suppose it gets interested
in this letter M here okay it doesn't
know whether this is a noisy symbol or
clean simple but it tries to come up
with the reconstruction value for this
letter M so do with window size too it
looks at the left to and right to around
this M and it tries to search over the
whole document where this H so in this
case is a GRE and search the over the
whole document and see how many times
this pattern of course and then it
counts what symbol occur at this middle
point here it's space it's on oh and
it's I here okay so once you search over
all these stop all this document get
this the Dude cop gets discount vector
that the stores what kind of letter
occurs how many times so I has occur
once em once Oh fourth and space have
five times so very simple scheme may
just go with the majority vote in this
case but dude is doing some something
more basically based on his knowledge on
the channel how noisy it is and based on
how the loss is measured it make a
decision on how much it's going to
believe on this discount vector okay so
one thing I would like to note here is
do if you see this simple rule this rule
is going to be only the
panel on this h er M re these five
characters because PI and lambda is
fixed throughout the whole sequence so
whenever dude sees this five characters
it's going to employ the same decision
for this this letter M here okay so this
current characteristics suggest that
dude is a sliding window generator so by
sliding window dinner sir I mean as
follows a que todo sliding window
danuser is defined by a mapping or
function called sk which maps luigi 2k
plus on top o to the reconstruction
symbol so basically what it's what
sliding window the users are doing is at
each location t it looks at the new easy
to quepos on top o center around this
time t and by observing this noisy
sequence it comes up with reconstruction
by employing this sk rule and it's it's
lies over this newest new easy sequence
like this so if it sees different
different context different symbols then
it comes up with a different
reconstruction and so on but the
important thing is if we see is the same
noisy symbols like this as the red
symbol it comes up with the same
reconstruction value and so on so so the
important thing about the study window
danuser is this rule sk is fixed over
the whole noisy sequence now this class
includes many practical in reserves
including medium filters and
morphological filters and so on and we
have just seen that do is also a member
of this class of sliding window danuser
with window size k we also see see that
do is a linear complexity algorithm
because it just needs to search over the
whole document once and go back and do
the d nuit ok so now let's see how it
works for the real noisy data suppose we
have a noisy noisy text like this which
is new easy version of Don Quixote noble
which had which had 21 errors originally
now if you run this dude algorithm on
this noisy text the error reduces to
seven it works almost correct sixty-six
percent of errors this is pretty good
result because dude does not know
whether these texts are
English it doesn't know anything about
this source it's not a dictionary based
scheme but it it corrects error to seven
errors also let's look at this binary
example suppose now our source is coming
from binary markov chain with transition
probability point 1 and the sequence
length is 10 to the 6 now let's say
noise flips this bits independently with
some probability point 1 the resulting
noisy sequence is again called the
hidden Markov process and we can employ
the for backward recursion which knows
completely about this p and the sequence
length n and this delta to get the
optimal bit error rate now the dude that
does not know what the source is if we
are interested in how it will work for
this noisy sequence if we employ the
dude and if you run the example as you
can see in this pot where the x axis is
the window size that dude is employing
and the y axis is the relative bit error
rate to the channel error rate Delta so
if you see this plot if we increase the
window size of the window size of the
dude the performance is attaining the
Bayes optimal performance if we increase
increase the window size of the tool we
also see that the window size K is some
kind of design parameter for given
sequence length n because we increase
the window size too much the performance
is little bit degrading so dude is a
simple algorithm and it works well in in
these examples but the original paper by
Weisman at all they also show that it
achieves the optimal performance is for
stationary sources the performance of a
danuser is measured by this per symbol
loss he noted as L which is just
normalized some of the losses that this
danuser is incurring at each time T so
given this performance measure the
original wiseman at all papers show that
if the window size of the of the dude
increasing with logarithmically with
sequence length n then what they show is
for any stationary on the line source
process X
the difference between the expected
personable loss of dude and the best
expected personable loss is going to
zero as the sequence length increases so
this performance is the best performance
of the divisor which is the Bayes
optimal performance match it to this
stationary process X but do which does
not know what the source is and does not
know about the source probability loft
they show that it attains the Bayes
optimal performance they show even
stronger result that for all underlying
source sequence X now this is a random
source sequence it's a individual source
sequence they show that without
expectation the per symbol loss of the
dude is attaining the value of DK where
DK is the performance of a genie who
knows both the underlying source and the
noisy sequence and can choose the best
sliding-window danuser for that source
source and noisy sequence pair so this
theorem shows that as sequence length
increases the performance of the dude is
attaining the performance of the best
sliding window danuser for that source
sequence and the noisy sequence pair
regardless of what the source sequence
may be and this is a with probability
one result meaning that for almost every
newsie sequence realization this result
pose yes you're in depth in that result
yes is the is the case selected yes sir
so it's a same order but it could be
conceivably that could be a better k
right so DK is not necessarily the
optimal sliding window decoder or for
that and but for that particular
individual sequence that's that's true
that's true but this what this result is
saying is that for same order same
window size k dude is attaining the best
performance with the same order of the
sliding into things do they have any
results as what the regret is for if you
if you against so we have eternity bad
yeah yeah so to choosing the best k
forgiven n is a hard problem which is
but
still still this result shows that for
for the class of same window size
starting with all the noise our duty is
attaining the performance of the best
guy in this set for a given source and
noisy sequence pair with probability 1
yeah so the K is a window size that do
this employee right and if we increase
the window for Gibbons equals length and
if we set the window size can grow like
lower limit in sequence X n and then we
are looking at the sequence of problems
for each end as any engross we choose
the window size grow growing with n like
this and that sequence of problem shows
the property that this hole and these
the these annoyed agree sighs I'm sorry
Z is the vocabulary size for the symbols
yes sir Z is the alphabet size of the
noisy noisy symbol which is e so does
that mean that there's a little higher
the vocabulary size the better it will
do um not really because of this
actually there I just suppress the
convergence rate of this this limit
because there is also a result of the ER
how fat it goes to 0 and it also depends
on the window the Alpha a size of the
noisy sequence and if alpha size is big
then it's getting slow slow converters
but yeah I mean so sorry I was unclear
so you will need a shorter window for a
larger without resizes yes yes that's
interesting yeah because if the
vocabulary size is large then no the
data the context the type of context is
going to be you know sparse if we
increase the window size too big so
that's the way how there's a trade-off
there in 30 k okay so we have seen dude
is a simple algorithm works well and has
some strong performance guarantees now
is this the end of the story let's look
at one more example suppose again we
a binary sequence like this and now
let's say it's coming from binary Markov
chain but this transition probability
shifts from point 1 to point 2 at the
midpoint of the sequence now this source
is not a stationary source it's a non
stationary or it's a piecewise
stationary source and let's say the the
noise also flip it with point one
independently over each bit again if the
four back or recurrent the optimal
scheme for hidden Markov processes it
knows what the parameters is where it
changes and from what parameter to what
parameter then we can employ the for
becker recursion separately on each
stationary box and get the optimal bit
error rate the question is can do it
still achieve this optimal bit error
rate in this case the answer is no so
even though we increase the window size
the bit error rate of the dude does not
obtain this base optimal performance so
this example shows that dude has a
limitation for time-varying or
spacefaring sources this is due to the
fact that dude isn't always employing
the same rule throughout the whole noisy
sequence for example it only cared how
many times this HTM re occur in this
noisy text but you didn't care wet where
or when it occur so dude is Taylor for
stationary sources however in practice
many sources are time varying or space
very suppose you have a text we're at
some part is written in English and then
translate it to Spanish or German or
suppose you have a voice signal where
some part it's quiet and some part is is
is Islam or let's say you have an image
where some part you have only you have
more text and so far we have some
different texture of images therefore we
ask a key main question of this talk
saying can we do better than dude for
when this source varies more
specifically can we perform as if we
know the source and we know the change
points if the source varies and if you
can do it
can we do it efficiently so these two
are the main question that I'm asking so
we have seen the background and
motivation for the main results so is
there any questions to this point ok now
I'm going to move on to the most
important part of this talk the main
results basically I want to say answer
this the question to previously asked
question and say that the new algorithm
stud shifting Universal danuser can do
better than good for varying sources we
can answer this to question of
relatively carrying for perform as if we
know the source including a change point
yes stud can do it as if in those source
and where it changes and stud is a
linear simple complexity algorithm so we
can do it efficiently the main point
that I will make this section is stud
can still universally attain the optimum
de-noising performance provided that it
is at all attainable I'll again specify
what this means as I go along so to see
how we can devise the new algorithm stud
and how it works let's take a closer
look at the simple binary example again
we have a binary sequence and the bits
are flipped with probability Delta
independently let's suppose dude with
window size 3 has decided as follows
suppose it decided wherever it sees this
noisy bit topple several bit topples
like this throw 100 110 suppose it has
decided the reconstruction value for
this 0 is 0 and let's say it has a pole
it has decided the reconstructive value
for one is one with the same left and
right context so from this decision
basically we can say that this left and
right context has defined a mapping say
what you see mapping for this middle
symbol right so whenever for this left
and right context whenever dudes is 0 it
will come up with 0 and whenever it's
this one it will come up with one just
to saying what this is and since do
these are sliding window danuser it will
always employ the same this single
letter mapping say what you see mapping
for this middle position wherever is see
so the single letter mappings I mean by
it what I mean by single letter mapping
is it's a mapping that takes one Luigi
symbol the single noisy symbol and comes
up with single reconstruction value and
in binary case there are only four
single letter mappings we can see first
one is the saying what you see mapping
we're flipping what you see we're always
saying zero or always saying one I
didn't go into the detail of the simple
rule of the dude but I can say little
bit more for this binary case
essentially what dude is doing is it
looks at this left position where it has
same left and right context and it
counts how many zeros and ones occur at
this little point and what it's doing is
if it sees similar numbers of zeros and
ones it gets confused and it's just
saying it's just employing say what you
see mapping but if he sees much more
zeroes than once it will employ always
say zero mapping for that disposition
and vice versa and the and the threshold
deciding the level of similar number or
much more will be depend depending on
the noise level know is probability
Delta so for for this example probably
dude has seen similar numbers of zeros
and ones for this leveling right context
so they employed same watch see mapping
now let's look even more closer to this
example suppose now zeros and ones at
this location had so the subsequence
points of the noisy sequence were looked
like this so there are similar numbers
of zeros and ones so probably so from
this dude has Duke may employ saying
what you see mapping for this position
but if you see even closer there are
much more zeros here and much more ones
here probably the underlying source
which results in this noisy sequence
have not be almost zero here and the
most one here so if you can split this
and employ is always saying zero mapping
for this position and always saying one
for this position it may be helpful in
reducing the overall loss so generally
if this single letter mappings that we
are employing for each context
can switch or it has some freedom to
shift along the sequence they can attain
smaller laws right but the question is
again how can we decide when to shift to
what mapping because remember this is a
luigi sequence and we don't know exactly
about the underlying source because we
don't see what is the underlying source
sequence exactly so ideally if you can
shift the single error these mappings
every point of time to the correct
mapping that will be the best thing to
do but this is equivalent to knowing the
underlying sequence perfectly exactly
which is import impossible thing to do
therefore we set some limit on the
number of these ships that we allow and
we call that number as M and then we
consider a set or class called SMN which
is a class of all possible the single
letter mappings that allow at most M
shifts for the sequence length n so for
example let's say this is a noisy
sequence of length n you can think of
this as the subsequence points that have
the same context so for this Luigi
sequence let's suppose at this part the
mapping the that we employ is this
saying what you see mapping and let's
say this part is always in zero mapping
here always say one or saying what you
see mapping then this combination have
shifted three times and this so this
combination is a member of s3n now we
realize that the size of this class or
size of this set is growing
exponentially in both NM because there
are total inches m possible cases where
to decide where to shift and for each
point of shifting time it can shift from
whatever mapping to whatever mapping and
from the previous slide the question of
deciding when to shift to what mapping
4m x is equivalent to selecting the best
combination in this set so having this
setup we know device the
e main tool in devising our new scheme
let's consider now the simplest example
now we have supposed only one source
symbol X the single letter setting and
let's say it gets corrupted by channel
the noise and then there's a noisy noisy
symbol z now the noise are observed this
C and suppose it employs a single letter
mapping which takes this seat and comes
out with reconstruction value X head now
lambda of X and SC is a loss that this
dinner is incurring between the clean
source symbol X and the reconstruction
but the problem is this loss is not
observable to the user because it
doesn't know what the underlying source
symbol is however from the knowledge of
this channel the noise matrix pie we can
devise a new function called el which is
a function of noisy symbol and this
mapping such that it has a property that
the expectation of this L is the same as
the expectation of the true loss in
other words we can devise a function L
which is an we can always devise a
function L forgiving noisy channel pi
such that it is an unbiased estimate of
the true loss lambda we can also
interpret this L as another loss
function between ug symbols e and the
mapping that the user is choosing so
this is a new loss function but all of a
sudden now we can observe this value of
the loss because Z is what you're
observing and s is what you're choosing
so given this key to now we can define
our new algorithm the stud algorithm so
what it's doing is for each context see
for example this binary context stud
finds the location that has the same
context as this see just like in the
dude and then it adds up the estimated
losses incurred by this noisy symbol at
those location and the mapping that is
employed for that location then it tries
to minimize the sum over this set where
you only allow these singular mappings
to shape up to n times
it defines the minimizer of this sum as
s hat ideally the best combinate the
best minimizer that we would like to get
would be the one that is minimizing this
true loss because this is the
performance that we are shooting for but
again since we don't know what this true
losses since we don't know what access
we just replace this lambda with this
new loss l once we get this combination
now stud applies those combination
single letter mappings for each of those
location so this definition looks pretty
simple but again the question is how can
we get this this combination of
singulair mappings that minimizes this
over this set efficiently because as we
saw the size of this set is growing
exponentially in sequence length N and
the number of shifts that we allow
different for every different context
yes you can it can be you can have a
freedom to ship for we employ this thing
for every every context in parallel you
saw or would it be better to constrain
the context the ship's essentially
corresponding changes in the source
model C yeah but I mean context you're
letting them do different things yeah
but if if the real sort if the real
source shifts at the you know if the
real source is shifting but then this
change point is going to be occurred in
the similar part because I mean this
this gives more freedom to choose where
to where to ship so we can ship at the
same time but as we you can see in the
in the theoretical result this is good
enough for getting our result so the
bottom line is how can we get this best
combination efficiently and the bottom
line is we can employ this stud
algorithm with a simple to pass
algorithm so to see how we can devise it
let's again look at the binary example
and this a binary noise case again i
restate the pull-up problem how can we
find the best combination with single
error mapping that minimizes this sum of
estimated losses incurred by this noisy
symbols and these mappings
recall that for binary cases there are
only four possible single letter
mappings always say 0 always in one
seeing what you see and philippine what
you see to solve this problem basically
we have to allocate a matrix of
dimension M by four where m is the
number of ships and four is the number
of single letter mappings for each time
point t once we assign this matrix
during the first pass of this algorithm
it scans the noisy symbol what they are
and while scanning it update this matrix
by simple dynamic programming once it
updates all the matrix all the way up to
time and the last sequence now it can
extract the best combination of single
error mappings by doing simple backward
recursion so this is a rough skeleton of
the algorithm and let's see how it works
more in detail so the goal was again to
minimize this sum of estimated losses
from time 1 to N but now what is mt mt
is a matrix that stores stores the
minimum sum of estimated losses up to
time T not to the end by up to that time
T and for example this eighth row of
this matrix and say what you see column
this element is defined to be the
minimum sum of estimated losses from
time 12 t but it has some more
constraint attain the minimum attained
by combination of mappings that allow at
most I ships this I ships until time T
and the last mapping you're employing is
this saying what you see mapping so this
LM each of these elements of matrix is
defined to be this way and once you
define matrix with this now during the
first pass we update this matrix as we
go so let's suppose at time T the
algorithm tries to get the value of this
this element the minimum sum up to time
T by allowing I ships and last mapping
is saying what you see we soon realize
that there are only two possible cases
to get to this minimum value first one
is
that this ice ship the last shift that
we allow has occurred at this time T so
until previous time points only I minus
one shift has occurred and the IT ship
has occurred at time T then for that
case it only need to look at em t minus
one the matrix at the previous time and
look at the I minus one throw and find
the minimum among these once you find it
you add the loss that is incorrect I'm t
by employing this this rule and another
case is this I've ship has occurred
sometime before time T so you're already
occurred and in that case you just need
to add this value plus the loss that is
incurring at time T so basically to get
this element just need to compare this
row and this value at the previous time
point of matrix find the minimum and add
the loss that is incurring the estimated
loss incurring for employing this rule
and we can update each element of the
matrix by the same way now once you
reach the end point of the sequence if
you look at the last matrix and look at
the last row m-flo and you find the
minimum among these value then by
definition that minimum value is the
minimum sum of estimated losses until
time n by allowing and most M ships of
this mappings okay if you find the
minimum argument of this this value then
that mapping is the one that that the
study is employing for time n once it
gets that mapping now it peels off the
optimal path to get this minimum value
by again doing the back row recursion
and guess the optimal path of the
mappings and employ that and do the
denoising for those time points so as we
just have seen the complexity of this
algorithm is linear in both sequence
length n and the number of ships m so
it's a simple practical algorithm to
employ so to summarize our new algorithm
stud for each location t as similarly as
in the dude it first fixes the window
size equal
okay and then it fixed it says the
number of ships and so we have another
parameter it's just the number of ships
that we allow and then again you search
for the left and right context and look
at the old position that have the same
context and for those position employ
this previous algorithm to get this best
combination of mappings that minimizes
this estimated losses some of estimated
losses and once you get that mapping to
simply merely employ in Ghost mapping
for each of these points we can also
show that if we set this m equal to 0
this stud coincides with the original to
Darwin so we have generalized the dual
algorithm and we added one more one full
more dimension in devising the universal
discreetly know either you can also show
the optimal performance guarantee of
this algorithm first again if we
increase the window size now slightly
less than logarithmic clean n and for
any for all piecewise stationary process
X whatever mean by piece Y stationary it
means a process that has some stationary
segments and for all such piecewise
stationary processes again the expected
personable loss of stud is obtaining the
best performance or the Bayes optimal
performance matched for that process
provided that the number of stationary
segments is sub linear in it meaning
that it's not very into fast again for
individual sequence setting for any
underlying individual sequence X now the
personable loss of stud can attain the
performance of dkm which is the
performance of a genie who knows both
source and noisy sequence and you can
pick study window danuser and who have
freedom to ship em x in a way to
minimize the overall loss now this shows
the stud can attain purpose of that best
shifting sliding window the users no
matter what the underlying sequence is
only based on the noisy sequence
provided that the number of ships
again is sub linear in n we also have a
strong converse theorem saying that if
the number of ships is linear in n then
no danuser in the world can attain this
previous theorem so this is a strong
converse meaning that if the ship is
linear in n stud cannot obtain this
previous theorem any kind of dinner ya
know just any kind of diversion so any
kind of no dinner can attain this
previous theorem that that's why it's a
strong converse so this sublunary
condition on the number of ships is
necessary and sufficient condition for
this previous students to hold and
whenever this condition is true stud can
attain the performance guarantee yeah so
you know the statistics you yes is of
course of course but the universal
danuser which does not know about the
source but if the shift rate is linear
in n then no danuser can do it no
Universal dinner can be so I don't have
a time to go into the proof of these
theorems but I can give one simple
reason why stud works basically I assume
that the channel is memoryless and I
design that this estimated loss is an
unbiased estimate of true loss therefore
by some law of large number type of
argument the sum of estimated losses is
going to be similar to the sum of the
true loss and minimizing this is going
to be similar to minimize this ok so
this is the main idea and let's go back
now to this example again again we have
a binary sequence coming from binary
markov chain where the parameters
changes from point 1 to point 2 at the
midpoint of the sequence so the optimal
performance in this case what was
attained by the four back or recurrent
which knows everything about the source
dude was not able to attain this
performance but the question is can stud
achieve this Bayes optimal performance
and the answer is yes as you can see
this blue curve is the error rate of the
stud with with em
setting equal to one so if you give a
freedom to choose this mapping to shift
along the sequence it successfully
captures where it ships ships and it
gets close to the Bayes optimal perforce
therefore we can regard this M as
another parameter in devising a
universal discreet dinner so far we have
seen this tell algorithm which is
tailored for the one-dimensional case
now let's see if we can extend this to
the two dimensional data case but so we
can realize that extension is not
trivial at all because in one
dimensional case just need to segment
the data into this thing intervals where
in each interval you apply the fixed
rule but in two-dimensional case it's
not trivial how we can segment this data
into homogeneous regions that we apply
the same rule in each region so this
extension is not trivial but the
extension of the dude to the
two-dimensional case is straightforward
because there you don't need to shift
just apply the same fixed rule
throughout the host whole sequence so
the extension to the two-dimensional
case is straightforward but it's not the
case for the stud now instead of going
into the general scheme we have a
version of two-dimensional scheme of the
stud so we again look at the combination
of mappings that can shift at most M
times along the region but we limit
those regions to be those that can be
get by this quad tree structured
decomposition so we set the number of
regions equal to M and we give freedom
to these rules to shift along these M
regions and then we define the 2g major
version of the stud in similar way
meaning that minimize the estimated
losses yes you are going to go explore
all possible tree structure with n nodes
yes yes with the number of leaves fix a
leak
so we try to compete with this kind of
shifting yes one question though is if
you there is a related one-dimensional
ok yeah so those blocks that are not
rectangular like the one out there that
all the three red right yes that's a
single block from the point of view of
your coding yes but it is not from the
point of view of the number of leaves
yes oh so you couldn't you get you get
into a sparsely the problem that is you
don't realize that the book that those
three blocks are the same have the same
compare just have the same rule yet so
so I define in the one divided cade also
I I can have them to shift at most M
time so it doesn't have to shift every
point of time so so this can be the same
region it may not shift the region or
all this region yeah so so we define the
two dimensional version of stud in
similar case meaning that minimizing the
estimated losses on this this regions
and we have the similar theorem saying
that if m log M is now sub linear in
sequence in the data size n then again
the personable loss of two-dimensional
stud is attaining this dkm almost surely
with probability one for almost every
new is realization sequence previous one
yes so what is the conversions rate on
those for this orbit yeah also for the
original document image ok so for this
expect expectation result right for the
for this one so for this one it's it's
hard to get the convergence rate because
we we prove this by using this Borel
Cantelli lemma which which shows the
almost sure convergence so it just shows
that it is converging to this this limit
with probability one but it was hard to
get the exact performance but yes of
course course it but as is as we see in
the example it gets pretty close to
optimal for like sequence 10 to the 6
like 10 20 6 so yeah so for for this
almost or with probability 1 convergence
it was hard to get the community's rate
but but for the expectation result it
was slightly was like 1 over square root
of n over this way so instead of going
into the detail of this 2d module scheme
i just want to show an experimental
result for images let's say now the
clean image looks like this which has
different texture along the region's
suppose it gets corrupted by noise this
is binary image and bits are fluid with
probe on 2.1 this is the output of the
stud algorithm two-dimensional still
arrogant and this is the output of the
dude algorithm the original dude which
extend it to the two demons in case we
can see our algorithm is doing much
better job in getting this boundary or
capturing this different texture of the
images so here it gets confused but
installed we get much we do much better
job yes ok and for each case ok so M so
I didn't mention this but this so for
the scheme for this two-dimensional case
the the optimum scheme that we deploy by
minimizing this estimated loss as it's
not as simple as a one-dimensional case
the complexity for m is more expensive
than the ones in the case but we have a
practical scheme which again have the
which again have the linear complexity
in sequence the data size n and this
number of regions and that scheme what
it's doing is actually it builds a full
tree and then you try to trim each
leaves and get get the region's to
employ so this one started with the
depth of four full tree and then it it
you know it trimmed the leaves and get
to get the region so if you see the bit
error rate for this these two schemes
you see the study is always dominating
dude for this kind of varying
spacefaring images
so I have presented a new algorithm
called stud which universally obtain the
optimal denoising performance on any
sufficiently slowly varying sources by
sufficiently very slowly I mean the
number of ship is sub linear in sequence
length N and i also have shown that if
the source is very too fast no dinner so
still can no danuser can do better than
stop that was another point so what we
learn is by observing the noisy sequence
long enough we can figure out what the
source is and how it varies as a future
work we try to find an application in
the internet for example some text
correction or book digitization and we
hope to find some connections to other
areas which involves with the noisy of
the region and non-stationary
probability laws for example in data
compression or machine learning or
information retrieval so thank you very
much yes on the time and space
complexity of solving the dynamic
programming problems you show there
proportional to N and M yes but there's
got to be some other interesting factors
that have to do with the alphabet size
the number of cognac just a number of
mappings and so on so so a number of
number of our context it's the same
because we're employing this parallel
scheme for each context press it divides
down the other day yeah yeah but ok so
it also depends on the number of
alphabets I didn't say here but
essentially that's number of these
columns so for buying your case there
were 44 possible cases so so that the
number of this single letter mapping is
actually Z the oppo a size of the noisy
sequence to the Alpha besides of the
reconstruction symbol so in binder cases
it's 420 too but it but is fixed with
the top-earning non-trivial size
alphabet this this could get
prohibitively expensive
so true for that case but for binary
case it was simple but I mean for
reasonable number for example for it I
think it's fine for for DNA sequence
easily I think it's fine about letter
sequences if you get for the text and so
will make this practical for English
text so I agree for like twenty six off
way it's going to be too much so yeah so
I should have to think more about
getting practical algorithm for this
text yes so if I understand correctly
this is a single pass right they what
happens if you was the fixed point if
you just need the right like youtube
running the algorithm oh you mean though
how'd you do that single path oh so
sorry quince and i apply do tourists are
the word and then the way i do it again
and again I'll gave it so what's the fix
point of that what's a fixed point
that's right so basically this depends
on the property does this channel is
memoryless independent over time right
that was a key property that was using
but if you process this dude to the
output and get the reconstruction symbol
then this like memberlist noise is going
to be destroyed so there was a there was
a attempt that tried to get again like
like empirical memoryless noise between
the luigi sequence and the
reconstruction simple sequence and run
this thing again but uh so but okay from
this theoretical result at least for
know for this way stationary case it is
is already attaining there's our best
performance that you can get from the
base base scheme but doing this
iterative scheme it may converse a
little bit faster this but you will not
you know do not go below this is optimal
so but i'll have to
experiment more on this iterative point
but I think it's all it's really hard to
prove something about it but maybe in
practice can be home yes did you try all
the true that he mentioned a lousy
picture that is not conveniently
perfectly divided as you okay yeah so so
this was a model of for example you're
scanning a book some part is an image
and some part of the text but if we if I
run to the like war or general images
then I could I was able to observe that
for small window size K still still was
doing much better than to do but for
like larger larger k it was a giving the
similar perforce so for that case I but
I will have to you know experiment more
extensively to see how how I can get the
parameters and the things right to see
the overall general performance on this
wheel images but for generally if it's
if it's like very like spacefaring like
images in the book and a text then I
think it will will still improve on dude
other questions
okay thanks okay thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>