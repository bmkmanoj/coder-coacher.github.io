<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: Session 4 | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: Session 4 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AGI 2011: Session 4</b></h2><h5 class="post__date">2011-09-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WYnLlk58EUE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay I welcome everybody to this first
session in this during this afternoon
session for this is a session that is in
particular I think dedicated to the work
that we study in lugano in huge me to
this group we had several talks but
nearly all talks are from lugano so to
speak and we start our first dog by
written by to be as class majas and yoga
Schmidt over the title is optimal direct
policy search or yours thank you I don't
know why they put all our talks together
like that but it's fine with us this
this should have been done by to be us
to be as las cajas who is now with his
family who couldn't come and so i'm
going to try to be a substitute there we
will have several talks in this session
that range from very theoretical stuff
to rather practical stuff for example in
a couple of minutes I hope I will be
able to talk about how to use very
practical methods neural networks for
winning all kinds of traffic sign
competitions and pattern recognition
competitions but now this is a
theoretical thing and and it's about
optimal direct policy policy search to
understand what was going on what is the
topic of this talk it on it's good to
recall what is the difference between
traditional reinforcement learning and
direct policy search both can be used to
maximize fitness or reward functions in
many situations now with reinforcement
learning you really have the the general
approach you can you act you perceive
you act you perceive as you are doing
that you are building a better and
better model of the world and you can
then use this model of the world to
compute action sequences that maximize
future expected reward and any any
problem can be formulated in this
framework important with traditional
well with general reinforcement learning
you can predict things that happen never
before and as an example consider that
we today can predict that at some point
the Sun is going to explode although it
never did before and we can already plan
for
event now diode search is more limited
because they're you assume these
repeatable trials you you perform your
action sequence based on your current
policy maybe a couple of time to gain
statistical reliability and you you then
evaluate your policies based on the
achievements during these repeatable
trials and you try to come up with
better and better policies using some
policy search method which does not
really predict the future while
reinforcement learning of the general
type is more general than dying policy
search almost are the benchmarks that
are being used in reinforcement learning
like for example the helicopter flying
problem or the mountain car problem or
the car racing problem and all these
things they can be solved easily one not
easily but they can be solved by both
types of approaches so most benchmarks
do not reflect that there is a general
type of enforcement language is more
powerful here I'm going to focus on
tasks that can be solved by both and the
optimal direct policy search which we
are going to discuss here is in
principle or rather simple it's inspired
by the universal such techniques that
leonid live in developed in I think 1973
we are going to translate programs into
policies which I essentially
probabilistic ways of acting in the in
the environment probabilistic well it's
a problem essentially that determines
the probability distribution of see
possible actions that you as an actor
can draw your actions from as you are
interacting with the environment and and
like let in search or universal search
we iteratively and exhaustively go
through pro-one space but in a good way
we start with very small programs and
don't run them for too long etc and we
keep growing this procedure the problems
get long and long as the runtimes get
along and longer and essentially what's
happening there
is that each program that we are testing
gets a fraction of the total run time
allocated which is proportional to its
probability where the probability of the
program is essentially to 2 minus n
where n is the length of the program so
that's essentially the probability of
guessing that program if you had a fair
coin now the practical way of doing that
is you have a sequence of phases and in
the end phase you test all programs that
has a property set their size plus the
logarithm of the runtime does not exceed
n for example in the tenth phase you
test all programs of size three for at
least two to the seven for at most two
to the seven-time steps and all programs
of size 54 at least at most two to the
five-time steps etc now what is not
there in the original formulation of
universal search is a good way of
dealing with sarcastic evaluations it
can be shown very easily that universal
search is going to solve any quickly
verifiable problem as quickly as the
fastest algorithm that solves it the
unknown fastest algorithm that solves it
say for a constant factor which is
essentially the probability of the
program that solves it fastest so it's
that's why its asymptotic the optimum
now if you have stochastic evaluations
of your policy you never know exactly
isn't that just an accident that I got a
high reward now in response to that
policy and you have to do it a couple of
times to gain reliability and we are
doing that in a good way and the way to
do that essentially is we we are going
to explore by searching in this space of
programs however as we are exploring
more and more and more we also
increasing the time allocated to
exploitation of the best policy found so
far essentially and the way to do it is
is justified by this little theorem
which essentially
says that if we do it the way it is
described in the paper then we can
guarantee that as we are increasing the
fee number n which is the index of the
programs that we are testing we are
going to end up with a policy with a
global strategy that is going to
converge in it's a reward in take
towards the optimum that you can obtain
and this is true the theorem holds for
certain technical assumptions and these
assumptions essentially are that the
episode length which can vary greatly as
you're testing this policy under several
in under several different circumstances
in your environment that see the episode
length or the mean of the episode
lengths is finite and the variance as
well and the same should hold for the
rewards the mean of the rewards should
be finite and the variance should be
fine and if that is the case then the
third method is going to converge and
here's a little illustrative example we
are not claiming that is a very
practical method in many applications
but this is an illustrative example
where you see lots of little states now
each of these little circles here
represents a state and you start in one
of the two states in the left upper
corner now in each state you can make a
binary observation either black or white
or 0 or 1 and then in in each side you
have two actions action one and action
tool now if you are in the white region
in one of these white stripes then
action one will take your left and
action too will take your right however
in both cases you don't know exactly
where you're going to end because there
are two arrows growing from from each
state to two to the left and also two
arrows going from each state to the
right now if you are not dark stripe
then this is reversed so action one
means than just the opposite of action
to and because it's very very stochastic
in thirty percent of all cases this rule
that justice
i doesn't hold but you would just get
the opposite effect that then what I
just is crying now the goal is to end up
at the goal which is to the to the right
and this is a partially observable
markov decision problem because in every
state you have only very little
information about where you are and you
actually have to take the history of
previous visits of previous dates into
account previous observations into
account to come up with a good strategy
to traverse that as quickly as possible
for every step that you are using you
get a reward of minus 1 which means you
are motivated to maximize external
reward which means that you are
motivated to go as quickly through this
maze as possible and then the instead of
using a universal Turing equivalent
programming language we are just using a
search through the growing string of
previous observations so first
observations of size 1 then of size 2
then of size 3 and so on until we get
sufficient data from from the history to
solve that particular problem using the
scheme which I just described which is
described in detail in the paper so this
is just repeating what I just said and
then you you see at this in the slide
how in the lowest line you you get the
total average fitness which is the
combination of the exploitation and the
exploration which are running in
parallel now the op de the uppermost
line with the within narrow dots is what
what you get during pure at what you
would get during a pure exploration the
dark line that you see in the middle is
the exploitation Fitness that is what
you would get if you always took the
best thing that you found so far and you
see that this thing is quickly
converging to the best possible thing
only in the beginning for the first 25
or so steps in this case we we have a
significant difference and over fitting
because the exploration behaves
differently from the exploitation but
quickly they confer
urgent after a couple of trials the task
is really solved so we have something
that is a policy search app which is
even under stuck a stake conditions of a
very general kind going to end up with
the optimal policy in all of these
environments of this types that
satisfies these conditions and that's
good all right thank you next okay thank
you very much our next talk the title is
planning to be surprised optimization
exploration and dynamic environments
paper was written by ye son faustino
Gomez and juergen schmidhuber and this
title is planning to be surprised it's
basically about optimal patient
inspiration we use some quite simple
stuff here so some preliminary a
background so basically that you can
imagine an agent is sent to exploit by
exploration we mean that there's no
external reward so basically the agent
sort of interact with the environment in
sequential orders we only assume one
dimensional time and it is basically
limited by time or resources so in this
case you can ask essentially a question
well there's an example that if you're
familiar was the Markov decision process
you can basically think this problem as
if you have only 100 actions how do you
actually do in that environment so that
you can learn the transition probability
of the Markov decision process as
quickly as possible then you can sort of
generalize this tool I can arbitrary
environment then you ask this general
question basically it's on how should
the agent choose the actions so that it
learns the environment as effectively as
possible so this is the question we're
trying to answer so there are basically
two terms in this question why is what
does it mean by learning or the other
why is how to measure the efficiency so
we adopt of some rather classical
framework so here the learning is
through probably
stick inference and the effective this
is basically measured via channel
information guy when you use this combo
of complexity here so a more detail the
agent assumed we assume this agent
interacts with the environment in time
cycles T equals 12 and at each time
cycle it basically performs an action
then receive observation that this can
be defined in some general spaces and we
use this repeatedly use this terminology
called history which is basically either
it's an empty string or it's a some
history padded with the action
observation pair here this H prime AO
just means you concatenate the two
string like H prime end of the action
observation pair so we make this key
assumption which enables us to do the
sub to do all the following duration so
we assume there's a random element of
theta that describes the environment and
with known prior p theta which that
means that you know something like
there's a probability distribution and
some measurable space and then we know
that the predictive probability p 0
given H a and theta so that means once
you know the model exactly you can
predict what's going to happen and once
you make these assumptions the learning
problem is kind of straightforward you
can just do the learning through
progressive inference and here the
tricky part is that you see that there's
a PCT given H a that basically means
that you want to update your belief
after you observe a history h and the
additional action a but knowing the
action a without the history without the
consequent observation would not change
your belief it's you can think of this
as throwing throwing a coin but not
knowing the results of the flipping or
not knowing the result will not change
your belief about this prova t of
showing hat so about measuring the
learning progress so consider H and H
prime be two histories and I use this
sign to should say that H is
prefix of H prime so H Prime has some
additional action option action
observation pairs so we measured this
learning progress essentially through
information gain which is defined as the
KL divergence between the posterior
distribution of theta in two time steps
which is given by this formula and it's
the this information guy basically
quantifies that I'm expecting this
caused by this the additional action
observation pair h prime like exclude H
so additional action observation pair
calls you information to change and this
information gain quantifies how much you
gain from this additional information
and then we can define this expected
information game which is of this is
actually the one-step expected
information gay as the expectation over
the information gain provided that you
can only do one actions and then you
predict the observation according to the
current posterior so this expected
information game page basically
quantifies the potential and expecting
the spoon when you perform some action
and the key property here is that the
action with high expected information
gain allows one to learn more in one
step so you choose the action with high
expected information gain that on
average will allow you to probe more
into theta the environment so because
the agent is sort of interact with the
environment in sequential order so we
consider this sequential by exploration
problem so we approach this problem in a
certain sense that assume now you
already experienced some history H which
might be empty then you have a policy
like exploration Posey pie basically map
the history to some kind of probability
over the possible actions and assume
you're going to make another tamil
actions so how do you sort of plan this
normal actions the center definition
here is that what we call curiosity q
value which is basically the multi-step
expected information gained from
performing a the action at the current
step then you following the given policy
for another prime and swans a terminal
mind
one steps and then based on this you can
define the curiosity value just lacking
in reinforcement learning as the
expected information gained from
following the policy for tomo steps so
basically this is a expectation of the
curiosity q value assuming a straw from
the current policy so the properties of
the securityq you value your curiousity
values the policy was the hike us the
cubicle earns more in expectation so on
average if you follow a policy with
higher curiosity Q value your I can
inspect ation getting more information
from the environment and a particular
nice property of the security Q value is
that it has this recursive decomposition
which is pretty nice you can link the q
pi tall with be pi/2 minus 1 or q title
x 1 that means that you can sort of
cascade this Q values or recursively
decomposes Q values and well and this
decomposition also shows that if you
want to have a good policy it has to
balance between the immediate expect the
information gain and then the
information gain you hope to get from
the situation where you end up with so
that requires planning and this
decomposition is actually not so trivial
it's a consequence of using he'll die
virgins so if you use F divergence of
Bragman or general divergence it doesn't
have this property and it's similar to
value function reinforcement earning but
with some subtle differences actually
not because the information gain is
additive only expectation you cannot go
back and adds up those information gain
it doesn't make much sense so once you
have this decomposition you can define
the optimal exploration policy and the
policy which is optimal turns out to be
the greedy policy if you just maximize
the curiosity Q value for any history
and the future look at steps in
particular well I already said this
acting criticism is the optimal policy
so in particular we have a small theorem
here well then you can
recursively defined the curious optimal
qzq value then you choose your actions
greedily according to that security Q
value then you end up with the optimal
policy this basically this theorem c and
the optimal policy is defined
constructively for finance look at steps
and that means you can in principle for
this theorem and do it but is
intractable and but well the good news
is for finance marketing environment
this optimal policy can be well
approximated through dynamic programming
that requires additional proof that you
can find in my home page there's a tech
report with quite long proof but you can
approximately well in certain
environments and here's the illustrative
example that you have a two small word
on the left part of the graph that
connected through a very long corridors
we have two clicks then there's a very
long corridor you have to move back and
forth to get to the interesting part and
then there are four different
exploration strategies considered so the
random didn't get you very far because
the random walk is quite slow along the
corridor and if you use the expected
information gain as the reward and then
took you learning basically getting
nowhere and if you take the greedy
action basically you only maximize the
immediate information gained you get to
the other part of the world but it takes
a long time but if you do this dynamic
programming essentially move back and
forth inside this inside this
environment well in this figure this the
goal part an upper part sort of
corresponding to the the to click and
the party in the widow is the the
corridor and here's the as a result the
accumulated information gained depicted
in this graph shows the difference so
the TP 10-week programming paste the
declaration has very high community
information gained especially in the
early phase and this you can see this
greedy exploration is also going quite
well but it's a slower than the random
is even slower then if you use
q-learning basically stuck in one side
of the world so the accumulated
information again
I'm basically it doesn't increase well
it finally happens okay yeah that's it
thank you very much next talk tom hollie
Oh Papa Tobias glass majas vinson the
Graziano and yoga Schmidt about
coherence progress a measure of
investigate to invest interestingness
based on fixed compressors your anger
image okay so um as we just mentioned
this joint work with two vs glass Maha's
Leo Pepin is Gaussian and you can ship
it to her it's going to be a slightly
less technical talking in the previous
one and it's going to be about what is
interesting pneus and how can we measure
it and so i'll start by like making you
take a step back and think about how
would you define interestingness and how
would you think it's possible to measure
it and it turns out this is a problem
that psychologists have been thinking
about for quite a while like over 100
years and here's a qualitative result of
what they have been saying is basically
if it's basically you can link
interestingness to novelty so if you
have something that is not normal at all
it doesn't have any new information
content it's probably also not going to
be very interesting so that is the left
end of the spectrum on the other hand if
you have something that is to novel that
is say so for example in a book that has
written in the language you don't yet
understand it will be increments of also
there's too much novelty for you to find
it interesting and so there's a sweet
spot just in between for example I could
talk like this one which I hope is novel
enough but then not incomprehensible and
so there's one additional thing to note
that this matter of novelty is a
subjective one so it depends on the
observer you know not everybody has had
the same lifetime experience not
everybody has seen the same like history
of observations
and so this curve will look different
for every agent and in more than that it
will also look different for the same
agent at different points during its
lifetime so once you learn this new
language the curve will shift and
something that was incomprehensibly
before will maybe know very very
interesting all right so now we have a
quantitative idea of what
interestingness could be now let's try
to see how can we make this sorry we
have a qualitatively how can we make
this quantitative how can we get a
measure that we can use in artificial
agent that is a numeric value of
interestingness and so there has been a
lot of interesting work in that
direction one one of the prime examples
being compression progress by juergen
and in that case we have a learning
agent that has some kind of compact
representation of its experience and
we're going to look at how compact this
representation is and we're going to say
okay whenever the agent manages to
compress more so there's compression
progress then it did something
interesting so if it sees something that
enables it to understand patterns in the
world and allows it to contrast it down
to a simple rule that say it discovers
gravity and then all those phenomena
that are based on gravity become
compressible to a simple rule then that
was something very interesting and so
there's two different sources for
interesting events one is basically new
data you observe something new that is
helping you compress and is therefore
interesting the second source is
learning progress it's basically a
thinking stuff where you revisit your
experience and think about it do some
processing and that allows you to
compress it more so there you also get
compression progress so that is also
something interested now in this paper
what we're going to try to do is
disentangle those two sources and try to
see if we can find a matter of
interesting is that is only based on the
first that doesn't assume learning so
we're going to look at the
interestingness of data
data alone and so of course always in
the subjective scenario based on the
previous data the history that we had
and and exactly so no learning being
involved so we don't assume a learning
agent we're just going to try to base it
the matter on the data itself and even
in that case what we arguing this paper
is then there are still some sequences
that are more more interesting than
others and in order to do that what
we're going to use again is the
principle of compression and the result
that I'll present how it works is called
coherent progress so as a preliminary
let's look at compression and
compression similarity so let's assume
you have two items and there's two
possible ways of compressing them one is
to compress them separately each one
without the compression algorithm being
involved when the other one is
compressed or to compress them together
and make use of potential overlap
between the two and this very simple
idea is is a way of it's one possible
way of defining a compression similarity
so it's a measure of similarity between
two things based on compression so it's
basically the compressed length of the
first object and plus the compressing of
the second minus the compressed length
if you compress them at once so this is
our similarity f sub C and now we're
going to introduce this new concept of
coherence and coherence is basically the
expected or average compression
similarity over a whole set so whereas
compression similarity was a measure
between two items now we're going to
have a measure on a whole set and so
this is just the average over all
possible partitioning of the virus at h
and as an interesting matter what we're
going to use is the coherence progress
so how much this coherence increases
after new observations gets added to the
side so the data changes something new
gets added to the set and we want to
determine how much does that increase
the coherence so after that new of
salvation comes in can we compress the
different components of the set together
in a better way so that is that is the
intuition that's that's the coherence
progress is basically just the
derivative on this coherence measure so
that was it already that's that's
already the the whole formulation is
that there are some details in the paper
but let's let's see if our intuitions
the qualitative intuitions match the
formula so what we would like is that an
observation that is blank that doesn't
contain any information content also
doesn't introduce is it's not an
interesting observation so we don't want
it to introduce coherence progress and
indeed that's what we get out of the of
the formula if the observation is random
again as it is by definition than
incompressible it was also
incompressible when compressed together
with items in our SAT so it won't help
it won't increase the coherence similar
for an unrelated observation which is
something let's say you have all items
are in one language and then you add one
that is in a completely different
language where there's no overlap then
that won't increase the coherence of the
set at least not the first item in that
language later that might not work and
what we would like is that if you repeat
something you did before that might be
interesting but there should be some
kind of decay you don't want to get an
agent that tries to enter the
interesting things and always keeps on
repeating the same things which sounds
like something humans do as well and so
we actually get from from this
formulation we get an exponential decay
on repeating items that are in the set
already and then all other cases we have
a strictly positive coherence progress
so this is just a small demonstration
there's a couple of those in the paper
this is a demonstration using Wikipedia
articles of animals so our original set
contains the article of human and just
just the tax
and then we add new observations and new
observations are articles as well and we
are earning them based on which one is
the most interesting and so we're
measuring this coherence progress with
respect to all the potential articles
which are other animal articles or
animal ethics we already know or empty
articles with no information content or
random scrambled text and so what we
would like is that this kind of
algorithm does indeed go for new animals
all the time and that is what happens so
that is good and the those are the
results of the the animals that increase
coherence progress most based on how
they are written in Wikipedia currently
so not so surprisingly the first one is
chimpanzee because there's a lot of
compression similarity between those
that one and human and then the next
most interesting is hope opotamus and
then Jaguar I don't know if that's a
it's hard to evaluate how true truly
interesting those animals are but
they're at least not the most
uninteresting ones under center so all
right so that's already my talk let me
conclude so what we did is we introduced
a new quantitative measure of
interestingness that is based only on
compressed it doesn't require this
assumption of a learning agent so you
can just use it and apply it to data
streams directly and it's a very simple
formulation and it happens to at least
qualitatively match what we expect from
an interesting measure all right thank
you very much
thank you very much next presentation is
entitled on fast deep nuts for hei
vision yoga Schmidt oba Thank curson
early Maya Jonathan Mirsky and Alex
grace I'm going to talk about now about
the second neural network Renaissance
which is currently ongoing and I i'll
try to show you how you can win all
kinds of pattern recognition
competitions using our special breed of
deep or we can't neural networks AGI is
driven by two types of progress at least
on the one hand we have these new
millennium results on optimal universal
problem solvers that are theoretically
unbeatable and mathematically optimal
but then we also have another type of
programs driven by hardware advances
computers are still getting faster by a
factor of 100 or 1000 per decade per
cent and this is what we are going to
use in this talk a long time ago 20
years ago my first student support
either he showed that you cannot really
train deep networks with many nonlinear
layers or deep or recurrent networks
which you can translate into DP for
networks which have as many layers as
there are time steps in the sequence
that you are processing because there's
this vanishing gradient error problem as
you are as you are increasing the number
of layers nonlinear layers to which you
are propagating in these networks every
time there's a new layer a new time step
you decrease the error signal by at most
a particular constant factor which means
that after it vanishes exponentially as
you are adding layers and time steps
which means that you cannot really
propagate far down in traditional
networks like this this prompted one of
the pioneers of neural networks to say
that nobody in his right mind would take
a deep multi-layer perceptron and and
training it by backdrop I won't mention
this pioneer
my name except to say that he is geoff
hinton so nobody in his right mind that
seems like an accurate description of
our neural computer vision team here we
see Dan and Wally and Jonathan and Alex
and so we took a deep multi-layer
perceptron with many layers and trained
it by background although my own lab
showed that you shouldn't really do that
however something has changed because
now computers are million times faster
than they used to be 20 years ago and
suddenly we can propagate arrows down a
little bit further then be used to and
now we can have as many layers in a deep
network as are connecting our retinas
with the visual cortex 789 no one knows
exactly something like that and we
applied that to the most famous
benchmark of machine learning which was
developed in the lab of another paeonia
of neural network of young a car in New
York and there's a long history of
rockland benchmark records in on this
problem and many people use all kinds of
super support vector machines and
whatever however our approach achieves a
new world record just last year and then
we greatly profit from graphics cards
which makes everything 50 times faster
than on traditional CPUs now you can
further improve this result if you take
not only one deep network like that but
you can take seven or ten and then each
of them is slightly differently
initialized and maybe has slightly
different p processing and you get
different experts automatically as a
by-product is a random byproduct of that
and then you do a Democratic Committee
which just takes the average of all
these of all these predictions of the
different networks and the result is
better than any individual network and
this led to a new world record on the
same data set which is now 0 point
thirty one percent and what it's really
what you really should do is take not
plain background but something which is
combining all kinds of ideas that range
back to Fukushima the Neo cognate Ron
and Max pooling
which is a way of down sampling from one
layer to the next and convolution and
weight sharing where you have not as
many free parameters as you would have
in a plain deep network and then the
current world record on that emma's
problem is 0 point twenty-seven percent
you can also do other images with that
for example this is the cypher object
recognition problem of Jeff Hinton's
group in Toronto and then we also just a
while ago broke the record on that one
and the current one which is not even
published yet is now thirteen percent on
this one here there you have to
distinguish frogs from cars and and and
and and and horses and boats I have a
lot of problems with see with the frogs
and just a while ago Willie and Don they
they were instrumental in winning the
Chinese handwriting recognition
competition which where we used exactly
the same approach now that's an
interesting problem because unlike with
endless for example you have not only
ten different patterns but 44,000 and
there we got the first and the second
ranks that was just a two months ago and
if you look then at the details of these
seas feature detectors that are being
learned automatically as a byproduct of
these deep networks you see the stuff
that you know from vision of decades ago
you get automatic automatically you get
Gabor filters and on sentox around
detectors and orientation sensitive by
detectors in the lower layers and as as
you are looking at feature detectors in
higher layers they get more and more or
Park and in transparent ins less and
less clear what's going on there just
like in human brains now we apply the
same thing to the no object recognition
benchmark and also set a new record so
I'm sorry my slides are going to remain
boring like that for way one but then
they will become boring in a different
way
yes the traffic sign competition that
took part in January there was an online
traffic sign competition and and our
toughest competitors usually are those
from young leÃ§ons lab it's interesting
to see that only neural networks are in
the are now playing a part in these
competitions and no support vector
machines no boosting and all these
things it's a competition mainly between
networks now which is kind of unusual or
unexpected because ten years ago neural
networks almost vanished from the scene
because support vector machines were so
much better in many applications and for
a long time it was not possible to
publish anything at the leading neural
network conferences that had the word
neural in the title and and there on the
last day of the competition where we
thought we have a comfortable lead then
suddenly a PS M&amp;amp;E of young leÃ§ons lab
he had an even better result and then
down in early and Jonathan worked late
night ooo and to reestablish the correct
order and then the the the best teams of
that time they were invited just a
couple of days ago to the IJ CNN
conference where the goal was to takes a
new data set with more data and then
participate in the final in the final
competition and down there we see down
with the with the number one traffic
sign that you can see in his hands they
are see the differences were
unexpectedly high so PS M&amp;amp;E this time of
New York University of younger cows lab
achieved one point sixty-nine percent
correct the second best were the humans
humans are able to get one point sixteen
percent correct according to the
conference organizers and in first place
for our team with 0 point fifty-six
percent error now this might be of
interest to everybody who is doing a
self-driving cars and of course we all
remember the this incredible keynote of
unstick man's who is who is not used
in gps for self-driving cars but it's
doing it in the human-like way and in
1995 already used these active vision
systems to drive from munich to denmark
and back in traffic with up to 12
vehicles surrounding the car and
tracking them all using these you how
many people saw this talk and how the
cameras built along there the active
vision camera I think this will also be
the the future of self-driving cars in
addition to GPS and radar and leader but
to do fast driving you just have to ask
and stick months you need these these
these active vision techniques for
getting high resolution input of stuff
that is far ahead as you are driving
fast you will need to look far ahead to
be able to break in time if there's an
obstacle on the street and so leader or
something like that is not sufficient
now once you have identified salient
points then you still have to do the
traffic sign recognition or pedestrian
recognition or whatever you want to
recognize that and there at the moment I
think I can recommend this technique now
of course in the in the long run we
don't just want to recognize patterns
that are single patterns by future or
networks know we have streams of
patterns videos essentially where there
is no segmentation where no nobody
segmented the stream into relevant parts
and and and to do that we we cannot use
feed-forward networks that we really
have to use recurrent networks which are
general computers and one of the
examples of of of applications is
connected handwriting connected
handwriting's there somebody scribble
something and you're supposed to
translate that into plain text which
means you have a rather short label
sequence however you have a long real
valued input sequence of pixels that you
are sequentially observing and and there
you need a recurrent network and
preferably a long short term memory
networks that can really deal with these
long delays between relevant events
and Alex grades our my former PhD
student and then past the post doc was
now going to Jeff infants lab actually
he he used this approach in conjunction
with a technique called connectionist
temporal classification which maximizes
the probability are the label sequences
given these long real valued input
sequences without any pre segmentation
and and then applied that to various
handwriting recognition competitions
connected handwriting and so that's how
he won the the French handwriting
connected handwriting competition and
the Farsi connected handwriting
competition and the Arab connected
handwriting recognition contest without
speaking any word of Arabic you may have
guessed that my that the Swiss AI lab is
in Switzerland and in in in Switzerland
traffic signs are really important if
you don't obey a traffic sign in
Switzerland you go to jail however
across the border there is Italy and
then Italy they also have traffic signs
in the street however only for
decoration well we are currently trying
to write a book about things like that
and I think that's the end of my talk
thank you
thank you very much next talk entitled
sequential constant size compression and
reinforcement learning the authors are
Elena's whistlin whistlin met Lucy F
Vincent Katja no and yoga Schmidt ok so
the title of my talk is sequential
constant size compressor for
reinforcement learning and it's as you
know now part of the work week sales
group in Switzerland so we try here to
do me do maybe more practical small step
against AJ I we've thought about this
idea that the Pollock had in 1990 so
more than 20 years ago which is called
recurrent out associate rememory which
was used in in the context context of
natural language processing and I can in
principle compress agents entire history
in a spatial code but it's not we need
something more than two it's not enough
to remember all the history we also need
some reinforcement learning top of this
to actually do meaningful actions so we
put this on top of this rom network and
also a part of this is to use the
information that we get in a meaningful
way so we don't want to use information
that is has no real value informations
we use some kind of exploration and
exploitation drive so the day for this
is to on this on the bottom this system
we use a router encoder which is
optional we will use this for one of our
experiment but not for other so and
after this out encoder we feed the
visual input into this system this ram
which can now then compress this spatial
and temporal features and essentially
build a code of the entire history and
then we use some trick
call a vector conversation I will go
through what it actually means so those
who haven't seen that before and then on
top of that we use a SATA saw which uses
this vector qantas is representation we
call this system server so first
explanation a little bit how round works
so in principle it's out encoder I don't
know if you all know that but you see to
the left bottom you see the the network
itself it consists of input layer
arbitrary size and then I hit a layer
means h1 there the one stands for at the
time so it's time moon and it can be a
betrayal size 2 and then the output of
the target is supposed to be a
reconstruction of the input and the
input h0 that is actually the previous
time step of the hidden layer so when
let's go through time this hidden layer
will be sent back and be used as input
again and another will compress that and
I can repeat that so for every time step
we build a proper station all the
previous Satan actions and in the end h3
we have all the previous time step in a
compressed manner and in order to see
how this works we need to decode this so
we use take now the upper part of the
system and we feed in h three now and
assist system has learned to reconstruct
the observation and action and also the
previous hidden state we can get out
from h3 time step three we can get out
to the time step 2 plus the observation
and then we can send in that back in the
system and get another observation from
that time step and we repeat this
until we get all observations out the
system and this can be a very long so
containing all the lifetime or less or
agent in principle and on top of this
now use a vector conversation we use is
the we use the output from the hidden
layer and it is is used because we have
a lot of noise on our input so if we
just make it group together the
different state that actually belongs
together and so we do some we do to
experiment the first one we use mace
that looks like the picture to the right
there that's the top view the agents
view is the only thing that the agencies
so it doesn't see the top view at all it
just sees looking forward in this maze
and it you can see a little bit noisy
but you can see the walls there's some
texture on the walls and the end of the
corridor and you're supposed to reach G
which is the goal and it's placed
randomly it has 250 actions to solve its
task otherwise it's taken back out and
put down at a random position again it
has four actions can do can move forward
and left or right turn 18 degrees and
they input this 160 pixels so here's
some technical details i will try this
we use a normal backdrop learner 8 0.01
put it sir salsa we use eligibility
traces learner 80.1 and decay rate of
0.8 and the trace decay 40.9 and we
shift between exploration mostly or
fifty fifty percent exploration and we
go down to total exploitation of the
of the agent so when it launched the
policy would stick to that no matter
what and we can find that the agent can
find almost optimal solution and put it
in these tags because it has to learn
first where it is to actually take the
optimal step and it can find out where
it is by move around in the maize itself
and we compared this to a SNS separately
natural evolution strategy and as a
method can the through new new
revolution builds a recurrent network
directly on pixels or an input itself so
the idea is that the agent get implicit
feedback between the unsupervised
learner and the reinforcement learner by
slowly learning compressed environment
better because when it can compress the
input in a better way it can give us
some more useful information to the
reinforcement learner which can then
take more valuable actions so they were
enforced on the unsupervised learner
compromise its environment better so it
improves iterative and for the second
experiment we also not include a long
temporal dependency for the format was
just needed one or two steps history but
in this case needs up to 12 frames to
learn were with actually in which stated
is in so this is more less a long
corridor without door in the under the
stars at T equals one and it moves it
helps to action either standstill would
move towards the end of the corridor and
when it gets to the door it gets a
signal on the wall it sees a B C or D on
and each letter represent a time we're
just supposed to wait and 4d for example
it needs to wait the 12-time step before
it move through otherwise it
one gives award so it cannot exit too
early or too late and the technical
details here is we use backdrop and also
salsa here again we use around the walk
to collect samples but we only use the
successful episodes and it can this is
our racial ten different agents and we
turns out it can learn up to about
ninety five percent but then it will
make some mistakes because the noise can
cause problem it for it to actually exit
at the right time I will show a short
video how it looks
so this is during the learning episode
so it moves moves down the corridor and
it tries to exit and when it gets a this
red signal its exit at the wrong time so
sometimes it doesn't even know that has
to move forward and that's one you see
it stands still and so it needs to learn
to move forward wait through right time
and then exit and see how it looks when
actually has learned something so it
gets a green signal than done for
example this case D now it has to wait
for twelve time steps and you can if you
actually count quick you can see the
other actually does to correct waiting
time and B means have to wait 48 some
stuff in this case
okay so this is maybe not could be yet
completely practical for mainly two
reasons that one is that it has no
attention it tries to reconstruct
everything it sees I mean that's gross
abeer advantage but in this case it
could be a disadvantage because of the
noise he tries to reconstruct all the
noise and stuff like that we must have a
moving target which is an inherent
problem with this rom architecture
because it tries to reconstruct this
system itself and when it does that this
is myself changes so it can never get to
a peripheral possession of itself
because then it will change already so
but to include we have shown that can
sue says successfully deal with high
dimension observation and also deep
memory requirements we you know it can
in principle code entire history of
agents lifetime in depending how deep or
how big your hidden layer is but of
course as added as the hidden leg goes
larger than Horace to train to and we
also show that the system couples the
problem of encoding the relevant parts
of temporal regularities from learning
how to act on them okay i would add I'm
thank you okay our last talk for this
session direct mana and James regia
systematically grounding language
provision in a deep recurrent neural
network good afternoon everyone how can
people hear me okay enough and so I'm
not for media um
so it's an important point I think I'll
get to a point about halfway through the
talk where I can maybe explain why I was
picked to be up here and what I what
I've been jokingly referring to all
weekend is the the schmidhuber session
but i'm here to talk about a neural
network model we we've come up with to
try to get at some of the keys that we
think are important to general
intelligence so the first is obvious and
leave
you
our sleeping disorder and I'll agree
that this is a good test or a good
requirement for general intelligence but
I think we can mostly agree that it's a
good start because you know it's the
same kind of test that we implicitly
give to each other every day so I won't
talk too much about about that one but I
do want to move on to a second point
which is taken from the cognitive
philosopher John Searle his his Chinese
room thought experiment from decades Oh
was it had the point of trying to tell
us that simple manipulation alone is not
enough to give us general intelligence
so the easiest way to think about that
is to think about looking up a word in
the dictionary and so if you start with
the word you want to know what it means
you look it up in a dictionary and you
get a definition of it in terms of other
words and of course if you don't know
what those mean you have to look those
up and get more words and this can kind
of get out of hand very quickly and you
end up with an infinite regress if you
don't know any words in the language
that we're talking about so the way that
humans solve this is by taking some of
these words and grounding them which is
to say relating them to other things
that we see in the world via other
senses so Searles point and the point
that that we want to drive home about
about grounding and embodiment is that
humans ground language in their other
senses like vision and if intelligent
machines are going to interact with us
then they're going to have to do the
same thing so the third and final key
point that we wanted to address with
this is something that the photo or and
pollution would call systematicity so
this is it's kind of related to both
compression and abstraction which we've
discussed at this conference a lot
already but it's operationalized it's
it's that same concept but its measured
only by performance the idea is that
human understanding of certain
situations is kind of intrinsically tied
to understanding of other similar
situations
so one way to think about that is to
think about trying to interpret you know
classify a big set of objects like this
you might say that you know those three
are large and those three are red and
you know those three are pyramids and
you know what the the systematicity
people would say is that if you you know
if you really understand this in a
systematic way you should be able to
look at an object that you've never seen
before and kind of take take abstract
out the largeness and the redness and
the pure business and be able to
describe this novel object you know with
a phrase that you've never heard before
maybe you've never heard of a large red
pyramid but if you understand these
concepts you should be able to put this
together and so this is not a problem
generally for these these higher level
symbolic models that kind of build
systematicity in the neural networks in
particular I've had trouble with this
over the years so we figure if we can
get all three of those kind of get on
that road we can we can start
potentially building an AGI with a
neural network so we're going to I'm
going to show you a model that we've
made that it starts down that road
here's the kind of the zoomed out
version of it the idea with what we're
trying to do here is take a neural
network and get it to learn to interpret
sentences that adheres in terms of micro
world that it sees so it kind of has two
major sources of input one is visual and
that input comes in the form of a scene
which is just the collection of objects
that are presented temporally each of
these objects has three attributes a
size of color and a shape it also has a
unique identifier in case we need to
distinguish between two similar objects
the second source of input is auditory
and we call this the sentence this
consists of a sequence of phonemes each
phoneme being represented as a bundle of
auditory features so you the network
will see somewhere along the lines of 20
to 40 phonemes come in temporarily as a
sequence and these if you sounded them
out
would produce a sentence in English that
is talking about the scene that the
network has just experienced and then
the job of the network is to produce a
sequence of predicates that's going to
encode the meaning of what was said in
terms of the scene so we'll try to
concretize this a little bit by going
through an example so if we have the
scene that you see at the bottom here
with these these objects you can take a
look at the the visual input on the left
there on top where we have you know a
small red pyramid object number one a
large blue block object number four that
would be the scene and the network is
going to digest those into its working
memory and then the network will hear a
sentence so the example sentence here is
the small blue or sorry the small
pyramids are near the blue block and of
course it doesn't see that as words it
sees it as one long uninterruptedly
quence that is not pre segmented at all
so it has to you know not only figure
out that there is such a thing as a word
in here but it has to pick out the words
segments them out figure out what they
correspond to in terms of the visual
environment and then as far as the
output goes the intention stream on the
right there if the network is going to
produce you know for this sentence we
have to it has to kind of produce all of
the the salient things that were said so
has to know that we're talking about
objects one and two and they're small
and they're pyramids and they are near
object number four which happens to have
the color blue and B Block shaped and I
think it's it's important to note at
this point that it's not really a simple
input output mapping because if we
change the environment and but keep the
same auditory input saying the answer
will be different so here the you know
the picture change the the visual input
changed and we can see that at least
part of the answer the part where the
network is picking out which objects the
speaker is talking about has to change
as well
okay so now that I've given you an
overview of the task that we're trying
to get a neural net to do I can kind of
blow this up and show you the zoomed in
version of the model and here is where
we get into into why I'm part of this
session you can see out there that i'm
using essentially the lstn style of
neural network and you can maybe you
can't read it up there but it says was
developed by by a Schmidt Huber and
colleagues at it Zia and so the idea
here is that we have the objects coming
in on the right side at the bottom and
we kind of accumulate those into a
working memory with one of these self
recurrent layers of lstm memory cell
units and we do something similar
starting on the lower left where the
phonemes come in and get accumulated by
two separate layers now we're using two
layers there because the auditory input
is starting it a much lower level and we
want to induce extra abstraction with
the extra layers once we have working
memory representations of both of the
visual in the auditory inputs we can
combine them in in an integration layer
that's also of lstm units and then we'll
use the final representation on that
layer to produce that sequence of
predicates that we want to get out at
the end it's essentially the semantics
yeah so we could at some point take it
down to you know using a pixel array or
something but we wanted to keep it kind
of high in cognitive we're really
focusing on the auditory aspect of it
well they're they're encoded as you know
feature vectors essentially right so red
is a feature and pyramid is a feature so
but we imagine that that's you know
that's the kind of representation that
you would expect to be formed you know
further along the visual system so you
know here's some results from training
it training in network in different
contexts and I want to draw your
attention to the one on the right which
corresponds to that the strong
systematicity notion that i was talking
about earlier you know after something
like three million you know
sentence scene presentations the network
is able to get you know ninety percent
ninety-seven percent of brand new novel
sentence sentence is correct right and
these are sentences where you know one
of the objects has never been seen
before some of the phrases in the
sentence I've never been seen before so
you know we would like to think that
that this is pretty strong evidence that
the network is able to come up with the
systematic or presentation and really
you know generalize what it's learned in
it in a symbolic and compositional way
kind of one last point is that we found
it interesting to kind of decompose the
task a little bit and look at how we
fare on kind of the two subtasks one of
which is interpreting the language which
is to say you know duo does the network
know you know what it means to be blue
does it mean what it note or doesn't
know what it means to be a pyramid and
so forth essentially can it come up with
the the stuff that's highlighted in red
there in the intention stream output
sequence and that corresponds to the
solid line on this graph so we can see
that you know after only let something
like twenty percent of the training it
reaches ceiling on this part of the task
and at that point as far as identifying
the objects in the environment as far as
doing the actual grounding right in the
vision it's not doing very well it's
something like seventy percent accurate
and it takes the the rest of the
training time and extra like two and a
half million scene sentence
presentations before it gets there so we
think that that's pretty decent evidence
that you know this grounding part of the
task is hard right but at the same time
you know based on the arguments I've
made earlier we think that it's it's an
essential part if you're going to have a
general intelligence so just to
summarize the contributions we have here
we've you know created this neural
network system that is able to interpret
essentially speech stream one step
removed from you know an auditory signal
to produce the predicate based meanings
of an entire sentence and classes of
sentences and brand new sentences and at
the same
time it you know kind of does this this
cross-modal association and is able to
pick out the visual reference of the
sentence so it grounds the words that
has learned in the vision and it's also
learning to generalize in a strongly
systematic way the kind of way that we
expect a human to generalize or that we
expect a symbolic system to generalize
so we hope that you know by kind of
solving these three problems with the
neural network we can potentially you
know come forward in applying neural
networks to AGI in the future so lets me
thanks so that was our last talk in this
session I would ask now the speaker's of
this session to come to the front and
take a seat for the questions this is a
joke question is for Tom well your paper
was really interested it's about
interest in this and the question is
about don't you think that your
definition is a necessary condition but
not sufficient because you're not
talking about the any utility function
because I might found something novel
and ultimately comprehensible but I'm
not interested at all because this is
not useful for me so are not sure if you
just considered an absolute measure
vintage interesting absolutely it's
always relative but just disregarding
this utility thing can you comment on
this I think there's a difference
between interesting and useful some of
the useful things we do are not very
interesting and some of the interesting
things we do are not very useful so I
think those two concepts don't have to I
mean there's no overlap clearly but they
don't have to be the same so the utility
doesn't have to get into the measure of
interestingness but I agree that I mean
this is a very simple this is a very
simple definition in a very simple
measure so it clearly can't capture
everything in capture captures one
aspect of interesting so maybe there's
others and maybe utility can come in a
different way but I don't think it is
necessary
I was a short question thank you first I
have a very short remark regarding John
Searle you cited him as some kind of
suggestion that we should do grounding
and I don't think that's what he did at
all I think he suggested not that we
need to do grounding but he suggested
that we need not to do AI because
computers cannot be intelligent and this
is very wanted to go if you want to
suggest that we hook up computers to an
external environment with visual data to
in order to have representation that are
grounded you can secure rings 1950
something paper he said that in there
already but it's one of the other side
he's on the dark side basically um
that's yeah that's fair enough and
there's a lot of problems with the
arguments that that Cyril makes I think
as well but the yeah the upshot that i
got from from studying him and studying
that problem was you know essentially
that you you couldn't do his Chinese
room argument at all because of the
grounding problem right I don't think
that his his idea that you could you
know contain in a in a book in an Oracle
the ability to have an intelligent
conversation is misguided because it
doesn't it isn't grounded I'm very much
tempted to ask your ghen better it's
true that 1.16 of all people in
Switzerland are in jail because of minor
traffic violations but what I really
want to ask is I really was very
impressed by your results and your work
and the way you did it do you think it's
possible and meaningful to integrate
this with existing cognitive
architectures for instance or is it
something which you would suggest should
scale up and replace cognitive
architectures altogether as a guiding
principle so as it stands now I think
it's a potential upon generally useful
ingredients for all kinds of
architectures I imagine for example a
self-driving car like the ones of and
stick months which has a complex
architecture for finding out salient
points in
the images you are not looking at the
entire visual scene you are just finding
you are pre processing it and then you
find a couple of salient points and
that's why you focus and then you still
have to do with this high resolution
pattern recognition for example it can
be a small part of something like that
on the other hand in the long run I
think this direction directing the
attention etc is also going to be done
by a system which has learned to do it
well using reinforcement learning as you
are figuring out in in the context of
many many different visual scenes was a
good strategy of moving the eyes around
to find the most informative points that
you need to solve the current problem or
maybe just interesting points if you
just want to have fun with you in a
museum for example so in the long run I
think we will have systems that are
coming from the pixel level and scaling
up all the way to the top but for the
moment we should take advantage of the
existing smart work that has been done
and just plug it in where vets use for
my question relates to this last
question in this last answer in the AGI
in 2008 set up a situation where you
know questioning whether you can scale
up if you learn single patterns can you
address multiple patterns because the
multiple patterns quickly become a
combinatorial explosion if you know a
hundred thousand things there's about
five billion to pattern combinations and
so on and so forth and segmenting out
things in a scene it's a little bit of a
chicken and egg problem because you
don't know exactly how to focus your
attention to a region if you don't know
anything about the scene so you have to
know something about the scene so then
you have to deal with that so it's kind
of this back and forth and I just wanted
to a comment how do you do you think the
network's so far will scale up to that
and you know how do you think you might
be able to scale up to dealing with
Simon
things after you learned single patterns
there was a long question them say so
personally I have great hopes that we
will be able to deal with that problem
just using this creative or curiosity
principle where you allow the system the
additional freedom of posing its itself
its own tasks which just are directed at
better understand the world so yes you
are right in the beginning I don't know
anything about this visual scene here
but at some point then as I'm randomly
motor babbling around I see all that
certain patterns I get rewarded for
discovering these patterns which will
lead me to certain behavioral strategies
that I'm going to store and reuse in a
in some sort of mighty modular I
reinforcement learning system and then
going from there you will hopefully see
this open-ended creation of novel
behaviors novel patterns being
recognized plus the strategies the
behaviors that lead to the creation of
these novel patterns such that the
system is continually motivated to learn
those things that are still easy to
learn in addition to what it already
knows always on this thin line where
things are not yet too hard and not yet
trivial and relative to the current
knowledge of the system so I think this
whole theory of curiosity and creativity
business is going to help us along these
lines because it opens this additional
degree of freedom of unsupervised active
learning where the system itself can
measure how easy it finds to learn
something and how and to direct its
attention to those things that are
easily learnable in addition to what I
already knows as opposed to just trying
to solve the external post tasks which
often are so complex that there's no
chance of solving them immediately yeah
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>