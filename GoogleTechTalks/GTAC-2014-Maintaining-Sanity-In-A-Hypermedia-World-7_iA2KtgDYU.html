<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Maintaining Sanity In A Hypermedia World | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Maintaining Sanity In A Hypermedia World - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Maintaining Sanity In A Hypermedia World</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7_iA2KtgDYU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Sonal Shah: So I'm going to start introducing
our next speaker, in the interest of time.
Since it's a lightning talk, I don't want
to shout on your time.
So Amit joins us from Comcast today.
Amit has been at Comcast for over six years
and has helped launch many, many mobile and
Web applications there.
Amit is going to talk about how Comcast balances
test quality and release velocity today.
So, Amit.
&amp;gt;&amp;gt;Amit Easow: Thank you.
It's incredible how Sonal can introduce every
speaker and not use any notes as a reference.
It's amazing.
My name is Amit Easow.
I come from a small cable company from the
East Coast.
Some of you might have heard of it.
Yes?
I am glad.
Ankit's presentation from yesterday, how to
move really fast and not break anything, could
very well be the name of my presentation.
But my presentation is only 15 minutes long,
so I'm going to call it maintaining sanity
in a hypermedia world.
In this talk, because we are at the Google
test automation conference, I'll focus on
the sanity aspect of this topic.
In 2013, the product team approached us and
to describe their vision, which new customers
will experience by the end of the year.
By the end of 2013, we would be launching
the X1 Entertainment operating system.
We were going to make 
movies and TV shows available on set-top boxes,
mobile apps and Web sites via the Cloud.
Your linear tv, recordings and video on demand
would all be made available across all these
platforms.
And this is what I saw.
[ Laughter ]
I saw a spider web of APIs.
Each of those nodes are hypermedia pages.
And each of those lines are links to other
pages.
And this is only a fraction of the API that
we have built so far.
But we were on the right track, because we
were going to use a hypermedia API.
Here this is a quick snapshot of what a hypermedia
API looks like.
External services talk to the middleware.
We have built a hypermedia API layer on top
of it so that the same API is called by every
single one of our clients, namely Android,
iOS, Web, and set-top box.
So the same experience is made available to
all these different devices using the same
API.
Here is another way to look at it.
A client calls the home page, and the home
page in turn calls a bunch of resources via
links.
We'll dig into what it means to maintain sanity.
How do you maintain sanity in a hypermedia
world?
Feedback, feedback, feedback and extensive
testing.
So the first thing we put in place is a code
repo.
And we decided to use GIT.
We made sure that we had extensive code review,
and we used a tool called Gerrit.
I'm sure a lot of you already use this.
We also put in place a continuous testing
infrastructure with a commercially available
tool.
We were successful in putting together hundreds
and thousands of functional and unit test
cases.
And we are proud to say, in about a year and
a half, as of last week, we have over 900
functional tests and close to 9,000 unit tests.
The code coverage is over 90%.
In fact, we have set Gerrit up in such a way
that as soon as the code’s test coverage
goes below 90%, Gerrit immediately fails the
build.
The developer has to add more test cases and
that helps us maintain a high level of test
coverage.
Continuous deployment is made possible by
the same tool.
Continuous deployment happens every weekday.
In order to explain this part of my presentation,
let's make up two names for developers and
one name for an SDET.
For the developers, how about Larry and Sergey?
Totally random.
And a name for an SDET.
How about Eric?
So Larry and Sergey are tasked to build out
a new feature in the mobile app.
Larry thinks he knows how to do it.
He starts writing some code.
He checks his code into Gerrit, and all the
unit and functional test cases start running.
Two of them fail.
So he goes back and fixes the code, adds more
test cases, and checks it into Gerrit again.
Now all the test cases pass.
Code looks good.
Great!
It goes into code review.
And Sergey has something to say.
He starts adding a bunch of comments into
Gerrit to tell Larry that there is a better
way to go about doing this.
Larry goes back to modifying his code and
test cases.
And then checks it into Gerrit.
Verifies that all the test cases pass.
And it goes into the code review stage.
This time Eric asks, what about these few
test cases that I think should be added?
They discuss and Larry says, you're right.
So he goes back to writing more test cases,
running them through GIT and Gerrit, more
code review with a larger group.
And finally, it is merged into master.
So that is how the continuous testing pipeline
works.
It's a little slower than what we used to
do before, but it offers much higher reliability
for writing code and testing.
All the engineers become experts in all areas
of the code.
There are no specialists.
Everybody are generalists.
Here is a daily deployment pipeline.
The developer writes new code on his local
machine.
The test cases are also on his local machine.
He runs them.
He checks his code into GIT and Gerrit.
And then it gets deployed to dev.
The dev environment might be just one node.
Again, feedback, feedback, feedback, just
like in the previous slide.
Here, in the dev environment, the code is
deployed to one node, test cases are run against
one node, everything looks good.
It goes to the QA environment, where there
might be two or three nodes.
Is the code handling it well across the different
nodes?
Great.
And then it goes to QA load.
And here, performance test cases and load
test cases are run.
Is everything working fine?
No.
So this pipeline allows good feedback and
encourages investigation when something goes
wrong.
So more code review happens, more writing
of code and test cases happen.
It is merged.
It passes in the dev environment, passes QA,
QA load.
Great.
It passes here.
And then it goes into staging.
Staging is exactly like production.
It is actually a node in the production environment,
but closed off from customers.
So once it passes in staging, we know that
it is going to pass in production also, and
we deploy to production.
And this happens every day.
And the good thing about doing a daily deployment
pipeline is, unlike in the past, when we used
to have long sprints, which was about three
weeks long, we would crossing our fingers
every three weeks, hoping that all the code
that we checked in over those three weeks
would actually work in production.
But now, the only code that could fail was
the code that was written the day before.
So we could be confident that things work
every time we deployed to production.
How is all this possible?
It was made possible through a system we built
called the Web Application Resource Testing
System.
We enterprise sourced it within the company
and we realized nobody used it, because the
name was kind of unattractive.
[ Laughter ]
So we decided to, as engineers, name it exactly
what it is, Python function test, PyFuncTest.
This library of hypermedia functions allows
us to make sure that all the calls that the
client would make to our API are well tested.
And another thing we also built was the third-party
API mocking framework.
This time, we tried another name, called Hydra,
multiple heads, mocking our different third-party
APIs.
It worked.
People started using it.
Wiremock was mentioned in a previous talk.
It is another option we are actively looking
into, because as we tried to extend Hydra,
we realized that Wiremock does a lot of the
things that we wanted to do anyway.
All of our testing happens in the mocked-up
framework and only a portion of them run against
a live environment, because all the functionality
that we are trying to test might not be available
to us through the live environment.
So we mock it out.
And I believe some of you might be doing this
too in your companies.
For the others, come on, you need to get on
it.
Another way of making sure that feedback,
feedback, feedback is always maintained is,
by putting together a reporting structure
that automatically gets populated as we add
more test cases.
In this slide, you will be able to see on
the top left, there is environment equal to
tests.
So this particular report was generated in
the developer's local machine.
You can imagine dev, QA, QA load, Prod all
coming up there as the same test scripts are
run against those environments.
Anybody can look at the report and say, oh,
there in the accessibility suite of test cases.
Only four test cases are present.
What about this, that, and the other test
case?
Then the developer or the QA person can go
in and start writing more test cases.
That gets populated through feedback.
We have a higher quality of testing.
In summary, we maintain sanity at Comcast
by having a good code repository system and
code review process.
We do continuous testing and deployment.
We use PyFuncTest for API functionality testing.
And we use Hydra for mocking of third-party
APIs.
Like I mentioned, Wiremock is another good
option.
Finally, we are continuing to explore new
ways to improve our testing efforts.
I'm going to end my talk by saying this: In
2008, I had come to GTAC to be introduced
to new testing frameworks.
At Comcast, we were mostly a manual testing
shop, and I came here for inspiration and
ideas.
And I was introduced to Selenium Grid.
I went back and implemented Selenium Grid
for my project.
I worked on it for almost a year.
And now, six years later, we have improved
our automated testing efforts tenfold.
I encourage you in the audience to also take
back all the things you have learned here,
explore it, see what works for you, and come
back to speak at a GTAC conference in the
future.
With that, thank you.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you, Amit.
That's a great success story.
Thank you very much.
Any questions for Amit?
&amp;gt;&amp;gt;Amit Easow: No questions about your Comcast
bills, please.
[ Laughter ]
That's outside this conference.
&amp;gt;&amp;gt;Sonal Shah: All right.
So I guess that's it.
Thank you so much.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>