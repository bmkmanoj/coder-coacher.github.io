<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical Aspects of Data Mining (Stats 202) Day 12 | Coder Coacher - Coaching Coders</title><meta content="Statistical Aspects of Data Mining (Stats 202) Day 12 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical Aspects of Data Mining (Stats 202) Day 12</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fmZYH3rmqDQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so sorry about that so today is
lecture 12 Friday will be the last
lecture because the final exam for
Stanford is next week there's a fifth
homework post so that's the final
homework today we're just going to get
through more of chapter five and then
finish it up next time and also start
chapter eight so as I mentioned there's
another homework posted for the students
at Stanford that will be new next week
their midterm is next Thursday so the
plan for next week would for them would
just be to review on Tuesday and get the
final the last day so again this Friday
will be our last day so I want to get
through more of chapter five today it's
called alternative techniques last time
I went through the ROC curves I'm not
going to go through that again I just
want to make a note that if you looked
at this slide from last time the true
positive rate and false positive rate
were written down incorrectly they had
the correct numerator but the
denominator is so total or true positive
rate is the true positives the
denominator on the true positive rates
are basically the total number of
positives that you have in your data so
everything that's a positive is either
classified as such in which case it's a
true positive or classified as a
negative in which case it's a false
negative right and then same thing with
the false positive rate when you're
making the ROC curve the numerator is
the number of false positives obviously
but the denominator is the total number
of negatives in your data so negatives
are either incorrectly classified in
which case they're false positives or
they're correctly classified in which
case they're true negatives so this
denominator is the total number of
positives in your data this denominator
is the total number of negatives in your
data so those never change as you're
making your ROC curve and you're going
from left to right this always is the
number of positives in your data this is
always the number of negatives in your
data those never change the only thing
that changes is you're making the ROC
curve is the numerator as you increase
the number of positives that you
classify this goes up and this goes up
the whole point is you want this one to
go up quickly and this one to go up
slowly so that when you make your ROC
curve and you plot the true positive
rate on the y-axis and the false
positive rate on the x axis you want to
go up before you go over right
eventually you're going to end at the
point 1 1 but on the way there you want
to stay above the diagonal and go
faster than you go over at the beginning
okay so we talked about that last time
and we went through an example again the
diagonal represents random guessing
right if I just say okay flip a biased
coin right 40% of everything is called a
positive well then this thing is going
to be 40% and this thing is going to be
40% I'm gonna be right there on the
diagonal at point four point four and
there's nothing special about 40% you
could do it for any constant value if
these two things come out the same
that's just random guessing a good
classifier we want to be in the upper
left of the pot of course the perfect
classifier would should go straight up
and then shoot straight over these are
useful for comparing two classifiers
again for a range of values you want to
classify the lives on top and the area
under the curve if you have to pick one
single metric from the ROC curve
sometimes people literally look at the
area under the curve of course a perfect
classifier would have every a point of
1.0 random guessing would have an area
of of 0.5 oh we went through this
example last time now the thing I'll
point out in this example the
classifiers actually give out
probability estimates so we order them
based on the probability estimates from
akemi ROC curve so this first
observation this if this classifier m1
had to pick one observation to be
classified as the positive class it
would pick this first one because 0.73
is the highest now you don't actually
need probability estimates to do the ROC
curve you just need sort of any any
ordering of these in terms of confidence
right so these could be sort of any
ordering I'll talk about like in
boosting it could be that the F measure
for support vector machines it could be
the distance from the margin could any
any sort of ordering in terms of
confidence you could use to make your
ROC curve they don't have to necessarily
be probability estimates you just have
to order your estimates in terms of your
confidence and then we went through this
exercise last time so I just wanted to
review that today what we're going to
talk about are some of these additional
methods for doing classification talked
about in chapter 5 so again chapter 4
was all about decision trees and I said
that it was just one example not a
particularly modern or effective
technique but sort of a starting point
the techniques we're going to talk about
in this chapter
nearest neighbor just because it's a
very simple algorithm and you often like
to consider that as a baseline for
comparison we'll talk about support
vector machines today and then tomorrow
I'll start talking or sorry Friday I'll
start talking about bagging around
Horsham boosting which our book
classifies under this general category
of ensemble techniques and that is sort
of a correct characterization although
arguably boosting differs a lot from
random for sandbagging so we'll talk
about those next time but today I want
to get through nearest-neighbor and
support vector machines so the nearest
neighbor classifier is sort of an
obvious thing to try the main thing you
need to have for nearest neighbor is you
need to have some notion of distances
between your attributes
now if the attributes are all numeric
and they're all sort of on the same
scale obviously you could use Euclidean
distance now if they're not numeric if
you have categorical attributes or even
ordinal attributes you know you have to
sort of be a little bit more creative
about how you're gonna define distances
and actually there's section 2.4 in the
text has a very long section where talks
about how to measure distances between
attributes the example we're gonna look
at of course is the sonar data set where
everything is numeric all 6dv attributes
are numeric and we can just use
Euclidean distance but in general you
know a lot of effort goes into sort of
how am I going to define distances but
once you have a notion of distance you
can use nearest neighbor classifiers the
K nearest neighbor classifier basically
you classify a point according to the
majority of the K closest training
points right so suppose I have this new
point X right this is a point for which
I want a classification well I look in
my training data and I find the closest
point to it closest point to it is this
point which is negative so I call the
point negative that would be the one
nearest neighbor just for every point
that you want to classify find the
closest point in your training data it's
sort of the most obvious thing you could
do to nearest neighbors you look at the
two closest points take the majority
vote well it's 50/50 so actually it
doesn't matter you might as well flip a
coin on that one and that's actually
what the software will do when you get a
even K and it's an even number of votes
in either direction and you're using a
point five threshold it actually does
randomization which is kind of annoying
because then your classification is
non-deterministic and you learn it the
next time and get a different answer so
anyway that's sort of the default is to
do a coin flip there and then three
nearest neighbors well these are the
three closest points so you say well two
thirds of them are plus so this point
would get classified as plus now you
could say well you know shouldn't I wait
this one closer or this one more because
it's closer some algorithms do but by
default k-nearest
just takes the you know K closest points
and does a majority vote on those other
other you know nearest neighbor methods
you'll see will actually instead of
taking a fixed number of neighbors take
a fixed radius or something like that
but the most simple thing here is to
just look at the one two three or in
general K nearest neighbors and take a
majority vote now some things to say
well here's a picture actually made this
picture in R this of course is a picture
of the one nearest neighbor classifier
and it's sort of interesting to see like
these two points here right so I'm
classifying everything that's closest to
a yellow point as light blue and
everything that's closest to a black
point is purple so all the points on
this line right there equidistant from
these two points then as I turn the
corner here you know this line is
equidistant from these two points then
here you know I start to become you know
closer to this point or close to this
point so you sort of see you know this
is the neighborhood that this yellow
point gets it gets cut off there of
course because the black point then
starts to occupy some space and you can
sort of trace these guys all around and
I made the resolution pretty high on
this so it looks like you know nice
smooth boundaries so that's the nearest
neighbor one classifier now in a two
dimensional problem you know no problem
it's you know pretty easy to use a
classifier like this however these
things really break down when the
dimensionality is large they really
suffer from the curse of dimensionality
so sort of every in high dimensions
right every point is far away so you
often have to do some sort of variable
selection first if you want this
algorithm to work or you sort of have to
hope that you filled up your space but
you to fill up a high dimensional space
you know the number of points you need
you need a lot okay so these scales are
important okay by that you know in this
picture
it's perfect right x 1 lives on 0 1 x 2
lives on 0 1
the space is sort of evenly filled up
however you can imagine if X 1 lives on
0 1 but X 2 lives on 0.001 then the X 1
is going to dominate the distance metric
so what people generally do in that case
is to scale all the numeric attributes
to have equal variance because you don't
want to just have one variable
dominating the distance metric just
because it has a lot of larger variation
so a common thing to do is to scale all
these so they have equal variance in the
sonar data which we're going to look at
it's not a big problem because they're
already scaled to live on zero one so it
works well k n n K nearest neighbors the
our function it's in the library class
and it does K nearest neighbor using
Euclidean distance by default which
means of course if you start to give it
a tributo char not numeric then it's
going to do something probably pretty
silly with them but by default it's just
going to use euclidean distance for
numeric attributes so we can look at it
for the sonar data again when i have the
test and training split which i'll just
use that and here's how I would do this
right so again install dot packages
class will install the class package
library class then I read in the
training data again I have to make sure
I'm trying to predict the sixty-first
column right if you remember the sonar
data
it's the 61st column that is the class
label so I'm trying to predict that
using the other 60 column so let's see
here's the training data for sonar so
you have all these 16 numeric attributes
and then you get to the 60 first column
and that's your class variable what
you're trying to predict so I make that
sure that one is a factor so our knows
that it's a categorical variable and not
numeric the training data is the other
60 columns and now here this is sort of
silly what I'm doing I'm just trying to
compute the error on the training data
so the function KN n takes three
arguments it takes your training data
your test data and then the Y labels for
the training data so if I make the
training data and the test data are the
same both you know both the training
data and the test data I give it the
training data it's just going to give me
the fit for the training data and
actually because the default for this
thing is one nearest neighbor and
because all values in the sonar X data
are unique but I know this thing is
going to come out to be zero
I know nearest neighbor one is going to
give me a perfect fit on the Tang data
why because every point is its own
closest neighbor and so just so you can
see here what R is doing with the KNN
function let me just show you so help
cannon oops I need to read in the
library okay try that so by default
again it's doing one nearest neighbor
here you can see I give it arguments
training data this is the expert X
values for train data
test out of this is the X values for
training data I tell it the labels right
CL is the factor of true classes for the
training data so those are the Y's for
the training data k equal 1 is the
default and then you see some of the
other other things there so that's what
it's doing and so if I make both of
these the training data then it's going
to give me the predictions for the
training data which by default one
nearest neighbor it's gonna do a perfect
job again because every point is its own
closest neighbor the only way this
wouldn't give me zero misclassification
are on the training data would be if I
had duplicate X values with different Y
labels right because then it would have
to pick and it would pick some of them
incorrectly but in this case every point
is its own closest neighbor every point
is unique so of course this thing is
going to give me 0 in this
classification error on the training
data right which is no big deal right
it's easy to fit the training data
perfectly in this case the sort of a
trivial way to do it just say every
point gets its own label right so this
way I have zero Mis classification on
training data of course that's not at
all what I care about what I really want
to know is how well it does on the test
data the data that it hasn't seen before
so now I read in the test data now I do
KN X is the training data X test is the
data that I want the predictions for and
of course Y is still the training labels
and so now I'm gonna look at where does
Y test equal the fit what's the fit the
fit is this thing Y test of course is
the labels from the test data and so
where are these equal take the sum of
that divided by the total number
observations 1 minus that that of course
is the error on the test data now if you
remember the last time we use the trees
from our part and we got about a 30% in
this classification error actually we
got down to like 27 or 28 this guy oh
I'll just ask you anyone know off the
top your head nearest neighbor wanna and
sonar data no ok just thought I'd ask
it's about 20% there you go 28.5 let's
call it 21% so for sonar data nearest
neighbor 1 gives you about you know 79%
accuracy or 21% misclassification error
and some people generally use this as
you know some sort of a benchmark
because it's such a simple procedure if
you have all numeric attributes you know
I would be curious to know what those
nearest neighbor 1 do
for your data set and here it's about
21% you know so if you write a paper and
you say I have a straight classification
technique it does really well on this
data set you know at 79% accuracy or
whatever well you know let me compare it
to nearest neighbor one which is maybe
the simplest thing I could do and see
how well that does so that is nearest
neighbor one on the sonar data set and
let's see yeah I did all that so I think
that's all I wanted to say about the
only reason I really wanted to mention
this nearest neighbor classifier is
because it is a simple technique and
it's often something I want to consider
as a competitor to sort of evaluate
other techniques against and it's pretty
flexible you know you can have nearest
neighbor one two three you have a lot of
choices and how you define distances you
can do variable selection it also gives
you probability estimates of course not
nearest neighbor one but nearest
neighbor three if you go back to this
example here with nearest neighbor three
you know the probability estimate for
this point would be 0.667 right because
two-thirds of the three nearest
neighbors are plus so you also get some
sort of probability estimates out you
could use those to make an ROC curve if
you wanted to compare it for a range of
values so it's sort of a very nice
simple classifier that you ought to
think about when you're comparing
classifiers and you ought to have it in
your sort of toolbox okay so we did that
we did that yep that's all I wanted to
say about the nearest neighbor
classifier any comments about universe
in your classifier questions okay so
it's a pretty simple classifier it's
something you can always try on the data
the next thing to talk about is a more
complex classifier this is the support
vector machine this is talked about in
section 5.5 which starts on page 256 and
motivation for these are the following
right if you think about your two
classes being linearly separable okay so
why do I care about linear boundaries
well a lot of procedures look for linear
boundaries so for example logistic
regression yeah you would think of a
linear boundary even linear discriminant
analysis would use a linear boundary now
with logistic regression on this data
okay any line that separates the points
would sort of be perfect right if you
think about misclassification error
right if I'm trying to separate these
points with a line any line I put
through them gives me zero
classification right that line gives me
zero Mis classification error that line
give me a zero misclassification error
the question is how do we choose the
best line when there's infinitely many
lines that all give me a perfect fit
right they're all perfect what's the
best one you know what's the most
perfect of all the perfect ones right
well you sort of think that like
intuitively this diagonal line looks
like it's doing a lot better than this
line even though they're both doing a
perfect job of classifying all the
points here I sort of have a couple
close calls right whereas here you know
the call is not as close I guess I'll
say so in that sense that looks like a
diagonal line is doing better and to
formalize that what the support vector
machine is looking at or more
specifically the maximum margin
separating hyperplane is looking for the
hyperplane or in two dimensions it's
just a line right two dimensions line
three dimensions plane higher dimensions
hyperplane it's looking for the line
that gives you the largest margin where
the margin is defined is the distance
between the two parallel lines that you
can draw on either side of the line and
still have perfect classification right
so from the diagonal line I can go this
far in this direction in the same
distance in the other direction and
that's my margin right and so actually
that's two times my margin the margin is
defined as this distance which equals
this distance that is your margin
whereas this other line here the dotted
line you see the margin is just not very
big it does a perfect job in terms of
misclassification error but in terms of
margin it does not do a perfect job so
you can sort of think of margin in this
case as being a metric like when we were
growing trees right then this
classification er may not improve but
the Gini index might improve same thing
here both of these give you zero
misclassification error but one of them
has a larger margin so if we search for
classifiers that have a larger margin
it'll sort of point us in the right
direction why does it matter if they're
both doing a perfect job it matters
because when we get new data the one
with the larger margin is going to
generalize better okay it's okay you
know I do a perfect job on the train
data that's no big deal but the large
margin hopefully will do a better job on
new data ok so there's some notation
there book use as I won't get into this
too much but this is pretty standard
right so you sort of think about you
know a line in the equation ax plus B Y
plus C equals zero that
all this right w 1 times X 1 plus W 2
times X 2 plus dot dot plus the
intercept equals 0 that's the equation
of your line or in higher dimensions
your hyperplane okay
the two hyperplanes are the two lines in
either side well this is w X plus B
equal negative 1 and this is w dot X
plus B equal positive 1
right you could replace this by any
constant but let's just scale it to be 1
and negative 1 okay in that case then
the margin becomes 2 over the norm of W
squared right that's going to be this
distance here if you go through and do
the geometry in the math and then you
also have to get every point right so it
turns out that if you're on this side
right because this is the plus 1 line
all these points sort of have higher X
values in that sense so they all become
greater than or equal to 1 and all these
points have lower X values in the sense
of the dot product with W so they all
become less than or equal to negative 1
so on this line I'm equal to 1 if I move
off in that direction
I become strictly greater than 1 on this
line I'm negative 1 if I move off in
that direction I become less than
negative 1 ok so this is sort of the
notation we'll use for the problem and
now formulating the maximum margin
hyperplane becomes simply a convex
optimization problem right because you
want to maximize your margin which is
you know to well you want to maximize
right so sorry the margin was this thing
right you want to maximize this so
that's 2 over the norm of W squared so
that's equivalent to minimizing the
reciprocal and then you have the
constraints the constraints are
basically if you're on that side your
dot product plus B has to be bigger than
equal 1 if you're on the other side your
dot product plus B has to be less than
or equal a negative 1 so we have a
quadratic objective function with linear
constraints so we're doing convex
optimization and we can use the Grange
multipliers and you can sort of turn the
crank and you can solve this problem
right quadratic criterion with linear
constraints so no problem for
optimization now the question comes up
and you're trying to apply this thing in
practice what if the problem is not
really linearly separable right in this
case I have one two three green guys
that are on the wrong side the margin
and one two three red circle guys that
are on the wrong side of the margin I
there's no line I can draw through this
data that will
separate it perfectly so you can sort of
allow for that and that's allowed for by
introducing some slack so each one of
these guys excuse me has a slack value
and so you add those guys up and you
give yourself a penalty every time you
have one of those guys okay and that C
is a parameter that you can tune the
software has a default value but you can
change and I'll show you how to do that
and so now you're trying to minimize
this thing subject to your old
constraints but now I don't have to be
strictly greater than equal to one I'm
greater than equal one minus some sort
of penalty there and same thing here
unless income on minus whatever whatever
the extra value I had to add on right so
for this guy I have to sort of add on a
value to make him actually on the right
side so that exactly is my penalty term
there so you just introduced these slack
variables and you can still do the
optimization you just give yourself a
penalty for every one of these guys and
winds up being on the wrong side and
your penalty is proportional to how far
he is on the wrong side times some cost
value okay so that allows you to sort of
deal with problems that aren't limited
sufferable there's another trick that
allows you to deal with problems which
is which are not linear separable and
that is that you don't have to use just
x1 and x2 right what if you say you know
even if I drew a line through this data
and allowed slack a linear boundary
isn't really the right thing in this
space right really I want some sort of
nonlinear boundaries so this is the same
trick you use in regression problems
right if you're doing simple linear
regression there's no reason you can't
do simple linear regression on x1 and x1
squared so it's not really linear in
terms of x1 and x2 but it's linear in
terms of whatever transformations you
define so you can just sort of define
any transformations if x1 and x2 don't
do the trick
let me try x1 squared and x2 squared
right it's still linear in the space of
x1 x2 x1 squared and x2 squared so you
can sort of make it linear in this
higher dimensional space by just making
a higher dimensional space by taking
transformations right so here they use
x1 it looks like X 1 plus X 2 to the
fourth and actually in that space you
can actually separate it perfectly so
you can always map up it in some higher
dimensional space in order to get these
things separable in fact there's
actually this kernel trick that you can
use where you can actually map up into
an infinite dimensional space how does
that work well the whole soar
you know all the optimization is in
terms of dot products right and so dot
products well those are just sums and
some consumption converge even if there
have an infinite number of terms in them
so there's some spaces you can map into
that are infinite and when you talked
about like radial basis function things
like that you're actually mapping into
infinite dimensional spaces now it's
hard to draw those sort of see but if
you look at the math you realize you're
just taking dot products and so there's
no reason you need to finite number of
dimensions so everyone seems to like
radial basis functions but you can you
know people write papers about this I
will tell you that the default in the
package I'm going to use is not the
linear basis function it is to take this
radial transformation no you you you
work in this warp space right and so
there's this reproducing kernel Hilbert
space that you work in and sort of map
everything into that space right right
right yes so I mean the sort of the
overfitting problem and the criticism
dimensionality is something people argue
about with these things and there's a
lot of theory that goes into
generalization in terms of the margin
and there's a lot you can read about
back in the 90s these things were sort
of very very hot topic they're still
well respected as a technique that does
effective classification I'm not the
best person to speak about them they do
come more from a Cs perspective than a
statistics perspective so that's another
reason I'm not the best person to talk
about them but I will say sort of this
and that they are a very effective
technique and I'll sort of illustrate
that for you okay other comments about
what I've said so far about support
vector machines
okay so let's yeah
yeah it is really so I mean statistics
you know we had logistic regression and
we have linear discriminant analysis but
support vector machines were not
invented by statisticians
so I mean the one thing I'll say is they
tend to do really well for problems that
are deterministic right and and
statisticians tend to be a little bit
more attracted to problems that aren't
deterministic now arguably though you
know both you know techniques can work
well it's just it's a matter of where
they come from right so I'm not saying
that they're not effective I'm just
saying that sort of their background is
from CS and so if you talk to an average
statistician they might sort of have
some opinion about that me talk to the
average person from doing machine
learning they might say something
different about them so I don't want to
say too much because I'll show my hand
or something like that but anyway
they're coming from they're coming more
from a CS background no statistical
background but in terms of solving
problems I mean they're a competitor
right their competitor allege it's
regression their competitor to anything
else you can think of okay so okay so
here is in R so I know there's other
packages this is the one that I was
familiar with the e 1071 the sort of
maybe an older one than some of the ones
that might be around now I know there's
this SVM light that people like to use
but anyway this e 1071 what does it take
here basically you give it I don't have
the call up here but oh yeah I do so you
give it X and y by default the kernel is
the radial basis functions you can
override that by specifying the linear
kernel and then you can see there's a
lot of other values here the cost by
default is set at 1 which is actually a
pretty low value of cost if you play
with this on some toy examples you'll
see that it won't even separate them
when it easily could separate them by
line so you often might want to crank up
the cost but again that's sort of a
tuning parameter and there's some other
tuning parameters if you look at this
thing on something like the sonar so if
you remember with you know the tree we
had like 20 oh sorry with the tree we
had like 30 percent accuracy with
nearest neighbor wall
sorry 30 percent of classification error
with nearest neighbor 1 we had 21
percent misclassification error with
this thing it turns out I can actually
do a lot better so install the package I
read in the train data and make Y a
factor and then the call here I'm just
going to do the default SVM X comma Y
okay once I do that it gives me a
separating hyperplane
okay
so I have this fit object that gives me
a separating hyperplane and then I can
predict using the fit object so I can
predict the fit for X so this would just
be in this classification around the
training data which I don't care about
too much but just so we can compute it I
will do that and you'll see that on the
training data okay it actually gave me
1.5 percent so I actually know that you
know nearest neighbor one gave me zero
percent so one point five percent well
that means that it's actually not
separating all the classes perfectly
because of that you know it has that
slack variable in there and the default
value of cost is not forcing it to
separate perfectly it's getting I guess
what is this this is probably point zero
one five times one hundred and thirty
points it's getting all the two of them
I guess it's separating all but two of
them perfectly by default okay but again
misclassification error on training data
is not what I care about I care about
the test data so to do that I'm going to
yeah use the same object but when I do
predict I'm just going to predict for
the X test predict for the X test as
opposed to the X so the same object that
I gave X before I give X test and when I
do this I can see how well performs on
the test data which it hasn't seen
before so fit did not see the test data
it just saw the new data and you see
12.8% for SVM 12.8% for SVM this you
know if you know much about the sonar
data if you ever read a paper where
they're comparing different
classification algorithms that's about
as good as you're going to do 12.8%
after this you know any improvements
arguably are sort of in the noise and
depends on you know how I split that
into training and test sometimes to get
more accurate measures of this people
will do repeated partitioning Xin to
training and tasks but you're gonna be
hard-pressed to be you know anything
around 12.8% with any classification
technique you use some of the other
techniques I still have to show you will
also be competitive to this but if you
just sort of think about it you know we
started off with like 30% you know
everything we did with those decision
trees all we really got out with some
it was getting like 30% miss
classification error which is really
pretty bad compared to 12.8%
nearest-neighbor technique you know I
was able to do 20% you can do a little
bit better if you change I think to
nearest neighbor two or three do a
little bit better but you don't get
close to the 12.8% that you guys with
SVM so whether or not you sort of like
the the theory and and the development
of the SPMS they are very competitive in
terms of classification which is really
in the end of the day what you're after
you know classification accuracy on new
data SVM's do quite well this one did
pretty well you know without me even
tuning any of the values so I think
that's all I want to say any comments on
this example let's try it so let's see
what I have to do so the question is how
well would it do with a linear kernel
instead of the default one which is
radial basis so let's see if I just
changed this thing and I say comma
kernel kernel kernel equal linear okay
and so let's see on the training data I
have the same error that's I did change
it didn't I
kernel equal linear yeah okay and on the
test data how well do I do on the test
data look bad
37% but you know I didn't tune it right
and so presumably those defaults are set
to be good defaults for you know the
radial basis function which is the other
default so arguably changing one of the
defaults without changing the others is
I'm half tuning it so doesn't really
work that well okay I will show you an
example with the linear basis function
in a second any other comments questions
on this this example so so far this is
the best thing we've seen on the sonar
data okay let me show you sort of a
little two-dimensional Twiggy example I
actually gave this as one of the
homework problems just so you can
visualize it so here's just
two-dimensional data right X 1 and X 2
and I made the first five negative ones
in the next five positive ones
and I just said let's fit the SVM with
kernel equal linear so we can actually
see the operate optimal separating
hyperplane in the original space and I
cranked up the cost value if you don't
set that really high it actually doesn't
give you the optimal separating
hyperplane okay so we're gonna finish
this two-dimensional data and I want to
give a plot of the result and
classification rule so you can see this
sort of a little bit of a hack but it's
sort of a cute trick I did to make that
plot so the first thing I haven't showed
you this syntax before so I can read
acts in I just read it in as a matrix I
just go zero point one point eight point
nine like that and then I say by Rho
equal true number of columns equal to so
this will actually read in the X data
and then the next thing I give it Y and
make sure it's a factor so let me just
search can then see that this is reading
in the X data let's see
okay so there's X right that's X that's
the matrix two columns two dimensional
problem then Y is just a factor five
negative ones and five positive ones so
do that and I have Y okay now I just
want to sort of look at the data so I'm
going to make this plot here and I think
this plot isn't too mysterious what it's
doing I used a really big character just
so we can see it x1 and x2 I've showed
you that before how to put the
subscripts on so basically this is the
this is the training data I don't know
is there anything I didn't tell you here
so a couple tricks I used here is that
cex equal to that makes the really big
points cex is the size of the plotting
symbol color equal to x as numeric y
right so color we usually say red or
blue but you can give it numeric
arguments I think zero is black one is
red I think two is blue something like
that so as numeric Y this will default
in numeric and so but it's by
multiplying it I think that way I get
what were the colors here to anyway
doesn't matter too much but red and blue
so I guess two times those numeric Y
gives me twos and fours so I think yeah
so that's why I get the red and blue
those correspond to colors two and four
and then see X equal to I showed you
that okay so that just gives us our plot
the next thing I'm going to do of course
is 250s
- X&amp;amp;Y with colonel e collinear and i'm
going to crank up the cost so this will
fit the optimal separating hyperplane in
the original space the cost will be
really big so i'll make sure that it
goes through all the points and gives me
a really large margin and doesn't sort
of allow for any slack because if you
look at the data right i ought to be
able to sort of do a good job through
this data you know if I come somewhere
through here I ought to be able to use
you know these are sort of the four
points that I need to avoid I actually
didn't talk too much about that one of
the cute tricks as you're fitting the
SVM is that it turns out there's a lot
of data points that don't really matter
at all and so in this case you know you
start to see these four points are going
to be really you know maybe this one two
but certainly no reason to worry about
those points okay so and that's where
this term support vector comes from
anyway so I ought to be able to do a
really good job by fitting a hyperplane
or a line right through there so that's
what I'm going to do here fit stores SVM
kernel XY kernel linear cost a hundred
thousand so I do that and by the way if
you look at the object fit you can sort
of see that there are three support
vectors and it did use the cost I gave
it and things like that okay then what
else do I want to do I want to sort of
see I mean so now okay I've fitted SVM I
can use that to predict but I just I
don't want to predict just for one or
two points I want to see what this what
this decision boundary looks like so I'm
just gonna create a whole bunch of
random axis right random uniform two
hundred thousand this will give me two
hundred thousand random uniform X's
random uniform defaults to zero one
which is where this data lives two
columns and just do by row that doesn't
really matter so I'm just getting to two
columns of a hundred thousand just a
whole bunch of X's I'm just trying to
fill up the space so then I can color it
it how am I gonna color it in I'm gonna
call points on the big x color I haven't
showed you this one before color can be
RGB so if you don't like any of the
colors that are gives you it gives you
like you know 300 of them but you can
just specify your own color using RGB
red point five I did blue point five but
then the I sorry red point five green
point five but then the blue I'm gonna
do it as numeric predict fit big ax
right so predict fit big ax this is
going to be the predictions from the
support vector machine
for this huge X matrix I just made and
when that equals equals 1 this is going
to be 1 and so my blue is going to be
0.2 plus 0.6 0.8 when this is false this
is going to be 0 and so my blue is going
to be 0.2 so this will give me two
different colors depending on the
prediction and then the plotting
character is 19 then the other the final
problem is that once I do this it covers
up my original data so points always
puts on top the last thing that you made
so that I just recall the original
points command to put my training data
back up there so let me do this and I'll
show you what happens with that much of
it might take a second because there's a
hundred thousand points but you can see
oh I should have had that window on top
of me first well there you go
you see it just basically colored in
everything blue that was classified as
blue and everything with this greenish
color that was classified as red and
I've covered up my original training
data but I can put it back if I just
call points again of course there's more
smooth ways of doing this plot but this
is sort of an easy way to do it and
points there there you see so you know
the the hundred thousand points just
spilled in the space and I color them
and then you can see here is your linear
boundary given to you by SVM and it
turns out if you actually look at these
two points and this is what I gave the
students on the homework if you actually
look at these two points and compute the
distance to this line it turns out to be
the same and of course that's the
largest margin you can get any other
line you do through that data there
would be some point that was closer to
it and then these two distances are so
of course the SVM here you can see is
doing what I said it would do of course
to get it to do that you do have to
trick it a little bit and say kernel
equal linear and make the cost really
big but that's what it's doing and so in
two dimensions I can see what it's doing
in sixty dimensions you know for the
sonar data it's hard for me to see what
it's doing but what really matters is
that I get you know 12.8% miss
classification error which is very very
very competitive okay so I think all I
wanted to say about that here's the
picture up on the slide the same as the
one I just drew that's your optimal
separating hyperplane
as computed by the SVM in our okay so I
think that's all I wanted to say about
that example and all I wanted to say
about SVM's and general question you
don't okay so it's interesting with a
small cost it goes pretty close to one
of those points so the question is what
do you get with a small cost so let's
see so let me I'll just do the cost of
one the default and show you so cost of
one and then do everything again here
cost of one sides on top I should just
make this window different size up so it
looks the same right but it's not
because well where's my points it's not
done yet
oh I didn't hit return so right see this
is really really close that's not the
optimal margin right you can do better
than that
and so it's separated them but it didn't
you know it didn't get that one really
really close and also if you're curious
I guess the other question to anticipate
is what if I don't give it a linear
kernel right what if I just do the
default and of course then you get sort
of this curve boundary and look at this
one yeah let me be smart and let you see
it color in so here's the default the
default is to use a radial basis
function which winds up being a curve
boundary in the original space okay
the other thing you can play with which
is kind of cute I won't do it now I'll
probably mess it up if you move one of
these points so that it's still linearly
separable and you ask for the linear
kernel with the cost of one you will
actually get solutions where it could
have found a perfect separation and
doesn't which is kind of curious so
that's why I'm saying that that cost
default value seems pretty low because
often it can find a perfect separation
in moment okay I mean that's just sort
of you know if there exists a perfect
separation it's you would think it would
find it but I default it won't if the
cost is one okay other questions
comments about
spams so again I'm not the perfect
person to tell you about these things
they come from machine learning which
isn't really my background but I've
encountered them you know as competitors
to sort of other classification
techniques I've studied and the bottom
line is they do pretty well yeahthere's
if you look in the the help there's um
sort of I think it has to do with where
do they go is it it's either this new or
it's the there's another tolerance I
think it's probably the tolerance right
either the tolerance or the epsilon or
some combination of both so you can sort
of specify how much you want well so
you're saying that so the question is it
so it seems like a weakness that's
ignoring some of the data points but
some people argue this is a strength so
it's true whether it's a weakness for
the strength
right yeah yeah I mean your intuition it
sounds logical I mean people will argue
this both cases but people that are
proponents of SVM say this is a great
feature that it doesn't have to use all
the data because only some points matter
so right so that's yeah so so I mean
yeah so there's a lot to read about
these things
there's a lot of literature it's still
in active research area actually I say
that they come from machine learning but
the group at Stanford is actually pretty
active in studying these things and
they've done a lot in terms of this
regularization parameter where they
actually let you vary it and find sort
of the entire path as opposed to just
you know let me put in one cost and do
it let me put in another cost and do it
they actually have a solution where you
can vary the cost and trace the whole
path along and there's a lot of
interesting research in this area it's
not really my area but you should know
it's a very effective classification
technique okay any other comments
questions on svms all right so the last
thing in chapter 5 is we did this I did
this we did this did this last thing in
chapter 5 is to talk about ensemble
methods this is section five point six
now ensemble methods is a pretty big
umbrella but in general these methods
your books describes as improving
classification accuracy by aggregating
the predictions from multiple
classifiers so the most obvious way to
do this is simply averaging a large
collection of classifiers and if these
things make errors somewhat
independently of each other and we take
the majority vote we can do quite well
so for example let me just motivate this
and oh this is the same motivation your
book uses so imagine that you have five
classifiers and each one is correct
about 70% of the time okay also imagine
that these classifiers are completely
independent of each other now of course
when we're doing these ensemble methods
we're not going to get complete
independence but you should sort of
think about what happens if you were
able to get complete independence and
then you take the majority vote how
often is the majority vote gonna be
correct so this is sort of the wisdom of
crowds thing right if people function in
pendant Li and they're all functioning
independently and they all have some
accuracy rate the overall actress
Scituate of the majority vote is going
to be much higher and specifically in
this case if each one is 70% and are
looking for the majority vote out of
five well of course if all five are
correct one two three four five that's
just going to be you know 0.7 to the
fifth okay so that would count also if
just four of the five were correct okay
so you could do this and you could
actually do this you know of course
binomial distribution here you could do
this five different ways right
correct correct not correct correct
crack plus probably correct
C correct not correct correct correct
correct plus the last one not correct
correct correct correct correct okay and
that's so that's going to be there's
five of them and they're all 0.72 the
fourth times point three to the one
power right okay and then finally if I'm
doing majority vote I need five out of
five four out of five or three out of
five would do it right things like this
it's not correct not correct correct
correct correct plus all possible
reorderings of that and you can actually
figure out this is going to be five
choices times four choices divided by
two should be ten choices so ten ten
ways of writing that times 0.7 to the
third times 0.3 squared right so this is
simply the binomial distribution you're
probably used to seeing this probably x
equals x equals n factorial over X
factorial and minus X factorial that's
the number of ways of arranging X
objects in a sequence of N Things times
the probability which is just going to
be whatever the probability is for X
successes times 1 minus P for n minus X
now of course the whole point to this is
that we're assuming I said perfect
independence right they're completely
independent of each other
but if I figure out if I figure out this
number this number and this number and
add them together that's going to be my
probability using binomial distribution
which of course is assuming complete
independence so let me just do that so
let's see here so we have the chance
that all 5 are correct which the
majority vote would be correct or one is
wrong
and four are correct which can happen
five different ways and then I could
have two of them wrong as long as the
other seven were correct
which can happen ten different ways so
it's a high probability so turns wait
where's point seven so there we go
Oh point eight three six nine two is the
answer here at point eight three six
nine two compared to point eight three
six nine two compared to the original
probability which was 70% right so if
you have five people that are 70% in
there they're correct 70 percent of time
they vote independently of each other
their majority vote is going to be
correct 83 or 84 percent of the time so
this is sort of the whole you know
motivation behind some of these ensemble
methods is it I can get a whole bunch of
different independent classifiers get
them to vote take the majority vote I
can do a lot better right so for example
you know you might say let me fit you
know a whole bunch of different SVM's
and take the majority voter let me put a
whole bunch of different trees and take
majority vote now those trees aren't
going to be completely independent but
if they're somewhat independent if
they're somewhat uncorrelated you might
be able to improve your accuracy so
that's the motivation here now of course
this value of 0.8 three six nine two I
showed you doing it sort of this way
there's of course a built-in binomial
function in R and you give it well I'll
just show you the help on it the
arguments you give it are the five there
is the total number the point seven of
course is the probability but question P
by gnome it's not it okay so by default
it's giving you so here's the binomial
distribution right this is this end
shoes X thing here
PTX one might see the N minus X by
default of course is going to give you
the less than or equal to probabilities
so that's why I did the one - right
because I want the majority vote correct
so if the probability practiced 0.7 then
1 minus 2 or less 1 minus 2 1 or 0 will
give me 3 4 5 which is the problem I'm
after here
okay so you can do that and the very
next slide says well it is instead of
five you have a hundred and one now I
picked an odd number just because you
know what do you do if you have a tie so
it's just one hundred and one so that
way if I get fifty one or more of them
correct and each one is 70% I should do
pretty well and so that will give me the
answer there and just to compute this
thing you see here control-c if I had a
hundred and one of them I would be
correct Oh point nine nine nine nine
eight seven like a hundred percent
chance almost of being correct by taking
the majority vote from fifty sorry from
one hundred and one independent
classifiers that is each correct 70
percent of the time so again this is the
motivation behind ensemble methods if I
could grow 101 trees and they only had
you know 70% accuracy which is what our
trees had but there was a hundred one of
them that were completely independent
I'd do almost a perfect job every time
now the complete independence of course
is the catch because you're not going to
get that but the whole motivation behind
ensemble methods is to try and get as
much independence as you can with regard
to you know bagging and random forests
that's sort of the motivation for those
boosting is a little bit different right
so it's not obvious that we should
combine that into these sort of
collection of ensemble methods but this
is sort of the organization in your book
uses so we'll talk about these next time
just to give you sort of an overview
what bagging is doing I mean the whole
question is right how am I going to get
these five or how am I gonna get these
101 independent or somewhat independent
classifiers well with bagging what you
do is you take your classification
algorithm but you train it on different
samples from the same data so you build
different classifiers by training on
repeated samples with replacement from
the same data so if I have a hundred
observations let me sample a hundred of
them in dÃ©placement sample another 100
with replacement another hundred three I
spent training my classifier and each
one of those sub samples and an average
over them of course those aren't going
to be independent because they're
trained on a lot of the same data but if
they're somewhat independent I can
improve accuracy okay random forests
those are so bagging works you know any
classification technique you can bag any
classification technique because you
just you just take samples from the
training data random forests you know
the name Forest sounds like a tree and
in particular random forests are dealing
with trees they basically average many
trees how do they grow the trees well
they don't sample the data so much as
they sample the attributes so as they're
drawing the tree at each node right when
we agree the trees last time in each
node we had to select which attributes
how much we were going to split what
random forest does and it does different
randomization techniques inside there
but one of the things that does is at
each node instead of taking the best
variable to split on it takes a random
collection of the variables and chooses
the best among those or sometimes just
choosing as chooses a random variable
and takes the best split on a random
variable and so then you get a whole
bunch of different trees you average
those you take majority vote that's
random force it's a very very effective
technique it's rivaling SVM and other
techniques this is the O'Briens
invention if you had a chance to meet
leo breiman before he passed away a
couple years ago up at Berkley he's a
really really smart guy and he invented
these things and to this day there's
still a competitor to a lot of the other
techniques that you know have a lot more
theory and things that go in I mean he
developed some theory about these he
gave he gives bounds based on the
correlation and the accuracy but in some
way there's sort of an ad hoc method
that just tend to work really well and
it's because they understood why trees
weren't working well he understood that
trees had too high of a variance and
that you needed to sort of stabilize
them and the way to do that was to
average them and introduce the
randomization so these are a great
technique all in should all run through
those next time and then the final thing
to talk about is sort of one of the
areas that I like which is boosting
which it's still combining trees but
it's not doing it so much by random as
it's up waiting data which you got
classified incorrectly right so you grow
a tree you get some observations right
you get some observations wrong you go
back to the ones that you got wrong you
up weight them the ones that you got
right you down weighed them you grow
another tree you combine those two trees
together and it's a very aggressive
procedure because it keeps up waiting
the data that it got wrong but you can
start with very small trees and by
outputting the data that you got wrong
you can actually from very small trees
get a collection of small trees which
can can learn sort of any complex
decision rule you want okay again that's
just the training data everyone can do
well in the Train data it turns out
though that these things actually just
like SVM just like random forests
perform really really well with regard
to new
data and these are a little bit
mysterious people tend to disagree on
why boosting works so well but you know
we won't get into too much of that
except to show you that it does work and
to show you what it's doing so I'll talk
about these three procedures next time
and we'll also talk about a little bit
about clustering next time in particular
I'll go through the k-means algorithm so
that's all I had today any comments
before we take off
okay so again Friday will be the last
day and I'll see you Friday</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>