<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Automated Reconstruction of 3D Models | Coder Coacher - Coaching Coders</title><meta content="Automated Reconstruction of 3D Models - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Automated Reconstruction of 3D Models</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/JtWD9Fh43qA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'd like to welcome professor a bday
Sakura from UC Berkeley she's been a
professor there since 1988 and actually
started working on the project she's
going to show us today since about the
year 2000 and automatic 3d
reconstruction of City models and this
is something obviously we're very
interested in and this tape is this talk
is going to be broadcast externally so
if you have any questions of a
confidential nature say though still
after the talk okay thank you thank you
very much can everybody hear me okay
great so I thought what we would do is
just start with a little demo what I'm
going to talk about today is the method
that we've been developing at UC
Berkeley over the last six years or so
for automatically building
three-dimensional models of cities and
we've inserted some of those models
inside Google Earth which I know so most
of you are probably familiar with so I'm
asking Steve why don't you navigate in
and out a little bit so people can see
the the model within the bigger context
zoom out they can see the model within
the bigger context of Google Earth so if
you if you look you see that the model
is sticking out right here there's a 3d
thing and the rest of it is just your
the satellite imagery from google earth
and now we can zoom in to go to to
berkeley this is downtown berkeley this
is a shadow street that's the bart
station entrance to the bart station
this is the power bar building and just
it just to make sure that you get a
sense of the 3d why don't we turn the
downtown berkeley model off and this is
what you would have gotten with Google
Earth and now turn it on again and you
can see that there's a real sense of
three dimensionality that gets added on
and off so one to navigate a little bit
more around campus and maybe now turn on
the the campus data so this model for
downtown Berkeley as you see in a minute
was collect was generated by use
both ground-based data and airborne now
we've also generated campus data which
is Joni uses airborne data and that's
down here okay so we'll talk about that
a little bit and once you turn that on
and off just real quickly okay so i
think i'll stop now with the google
earth and get on with the main part of
the talk when we exit out of that and
i'm happy to at the end of my talk i'll
send you pointers you can download these
and plaid it to your own google earth
and play with it I don't you know TT
ok
okay why don't you fire up my powerpoint
now okay great so what I'm going to talk
about for the next maybe half hour or 30
minutes or so is a quick overview of how
we generated some of these models this
is all done at my lab at Berkeley which
is called the VIP lab video and image
processing is this
I'll go Valon which button do I push
okay so just before going into the
details of that I wanted to quickly give
an overview of other projects that has
been going on in my lab over the last
few years being and Google I can't not
talk about video similarity search this
is a thesis that was done by Sampson
trying in 2004 where we recognize in a
very very large database of videos that
i dint nearly identical videos there's a
quite a bit of work on multimedia
networking and streaming specially over
wireless some work on compression of
VLSI data and finally 3d modeling of
urban environments which is something
I'm going to talk about right now so a
bit of acknowledgments this port was
sponsored by aro Army Research Office
under a university research
investigation program Murie from
2002-2006 Google generally started
support in 2006 and we're very grateful
for that we've also received what's
called the doer equipment grant from air
force in 2006 and a whole host of
postdocs graduate students undergrads
and research staff have been working on
this and three of my students are here
and you're welcome to talk to them after
the talk if you're interested so the
goal is to generate three dimensional
City models that are useful for both
virtual walkthroughs drive-throughs and
fly throughs and we want it to be asked
for a realistic as possible and there's
a whole host of applications and I'm
sure for this crowd I don't really have
to motivate the problem too much and our
objective has been to do it in a
automated way fast scalable and
photorealistic now just as a way of
background there's been a lot of other
work and City modeling for example set
Taylor at MIT developed this system
where you would park your apparatus at
the particular location in front of a
building in a city scam for 30 minutes
or 20 minutes the move to the next
building scan it etc what differentiates
this work from existing work in the in
the literature is that we actually
acquire our data in a very
fast nonstop and go fashion and so that
enables us to generate models and fairly
quickly you'll see that in a little bit
more detail so the approach that we've
taken for full blown modeling is really
come up with two models a model that's
been generated by a acquisition vehicle
that drives on the ground like such a
truck and that results in what we call
ground-based modeling and that really
models the facades of the buildings and
then we do what's called airborne
modeling where we have helicopters as
well as our planes collecting laser data
as well as imagery from the top in order
to build a 3d model of the rooftops and
we convert we merge those two things to
come up with a 3d City model and so a
big chunk of our effort has to do with
registering this that's different
sources of data and fusing it together
so they all line up and get something
nice and good so I'm going to talk very
briefly about the the ground based
modeling this is our acquisition vehicle
parked in front of cory hall at UC
berkeley it consists of this board here
that has three things it has two zig
laser scanners one of them is vertical
the other one is horizontal and it has a
camera it's nony camera and the data
from all of these things is connected
through these wires to a PC that's
sitting in the back of the truck getting
powered by the battery on the truck
itself and as I said we call this drive
by scanning we collect the data as we
were driving under normal traffic
conditions in the roads we know we don't
stop and go so here is a basic system
you you have the vertical laser scanner
that takes a swipe like this vertically
as the truck is driving and the idea is
to stack up this vertical laser scans
next to each other in order to build the
3d profile of the facade of the building
okay but in order to know how far apart
you stack these vertical scans you need
to know how much the truck move you
essentially have to localize your truck
and that's not a terribly easy problem
and the way to solve this localization
problem is is to use a horizontal laser
scanner and successively match horizonal
laser scans in order to deduce the
movement of the truck so that we can
then stack these vertical scans at
appropriate little distance from each
other and synchronize with all of these
things is a camera that acquires texture
together with the laser scans as we move
along so this shows the the process of
pose estimation in other words
localization of the truck using
horizontal laser scans so this is a
visualization of horizontal lasers can
add listed time T naught this is the
next time step T 1 and what you want to
do is you want to come up with the
translation and rotation parameters that
makes these two scans match with each
other and to do that you solve an
optimization problem which I won't go
into the details of it but once you do
that then you can list you can recover
these little vectors Delta u Delta V and
Delta P concatenate these in order to
reconstruct a path and this shows that
we construct a path by successive
matching of the horizontal laser scan
and if you and if you then use this
reconstruct the path to superimpose the
horizontal scans you get this blue line
and ideally you'd like to have this blue
line to be as thin as possible so that
the successive horizonal there's laser
scans match each other if they weren't
matching this blue line would be very
very thick so this method is okay but
it's not great if you start at location
1 and drive all the way to location 2
and this is the reconstruct the path
that you get the red and what's
underneath this is the digital surface
map that we've obtained areally in other
words a height field of the area
underneath then you can see these are
the streets in a Manhattan structure and
as you can see the red path doesn't goes
all over the buildings it doesn't
li i follow the streets very well so
something has gone wrong we need to do
what's gone wrong is that especially at
turns and other situations these the
method doesn't quite work and arrow is
also accumulate and to prevent errors
from accumulating you have to do what we
call global correction and that's
exactly what we did next and the way you
do that is you say okay I'm going to
start within either an aerial picture or
an aerial DSM digital surface map this
is just a height field using laser scans
we're going to use this later on for our
rooftops anyway so we might as well just
utilize it for localizing the location
of the truck so if you use these two
things detect the edges and then match
these edges with the horizontal laser
scan then you can have a shot at
globally correcting it and making it
work so that's exactly what we did is we
applied what's called Monte Carlo
localization techniques using particle
filtering this is explained in this
paper that that's that's listed here and
we match the horizontal laser scans
essentially with airborne edge maps okay
in order to localize the vehicle I won't
go into the details of Monaco
delocalization but it essentially
consists of a motion phase where we
increase uncertainty and a perception
phase will decrease uncertainty and this
video here shows you the result of
particle filtering so these these what
we see here is the dsm the airborne
depth map and the the bla here is the
probability density of the location of
the vehicle based on this technique the
red in the middle means that the thing
has the highest probability of being
there and the one the yellow means the
probability decreases but the good thing
is that the block kind of moves on
nicely and smoothly way in the middle of
the street even though we change lanes
and I believe if this video is the one I
have in mind it even when we make a
right turn the BLA stays within the
streets these are the edges of those we
not switch back to the DSM again so
these are so
so there's a there's points at which
this thing disintegrates a little bit
but composes itself back again in the
middle and we used for those of you who
phone were familiar with this particle
filtering we use 10,000 particles in
order to accomplish that there's
absolutely no GPS that's a very good
point we do old about this problem quite
a bit and we decided not to go against
it because GPS doesn't work in places
are these high buildings or or indoors
that's correct if you had GPS you didn't
have to do any of these tricks yeah
trains when you try to align but it
really they do have some smooth
humor
I think that's what the global edge
thing from the top will help you to
correct those situations even just one
frame change the location
yeah you imagined on the fact that your
your your laser scanner runs at 75 hurts
and compared to the speed of your truck
that's pretty fast so you're right there
will be occlusions there will be some
changes the matching is not perfect but
in almost all the experiments we've done
with you know tens of minutes or hours
of driving together the combination of
airborne and the horizontal match has
resulted in very good localization yep
yeah listen to some interesting
architectures that we're considering not
to have two lasers Canada 45 degrees
because that would also deal with the
occlusion problem that's correct so so
in that Monte Carlo localization
situation not only do we compute can
calculate the orientation but also Z
which is the the height of the truck if
you're going up the hill for example in
Berkeley or the slope of where you're
going so it gives you all the parameters
kind of in one shot and whereas what we
got before with localization that looked
like this now what we get is with this
kind of localization which is perfectly
aligned with the streets and just to
give you an idea this was the 78th
minute drive resulting from to data
acquisitions 24.3 kilometers 85 million
scam horns 19,000 camera images for this
drive with in Berkeley
and not at you localize your truck you
can then stack these vertical scans at
the appropriate distances from each
other and and and get a point cloud
that's known here for those of you who
are familiar with Berkeley this is the
entrance to the bart station and as you
go down and so the next thing that you
do is question you are all time with it
it is one second so the next thing you
want to do is triangulate or tessellate
this and generally if you have a point
cloud triangle it in the most general
way it's a very difficult problem but we
have the fortune of the fact that these
these vertical scans are given to us in
order so that makes this triangulation
problem extremely easy and after you
triangulate the point cloud that like
this you get something like that and at
the first look it looks pretty
disappointing that these holes are there
because the infrared laser goes right
through the glasses of the windows
there's there's cars here there's trees
that block the window but after some
processing you can do foreground removal
which I'll talk about in just a second
as was brought up we fill out some of
these homes and we get something like
that and I'll talk about this step from
here to here in just a second and then
with texture map it and we get something
that looks like this so so if you just
this this is going to reinforce it what
I just said the second I go if you just
triangulate from the you get something
like this which books semi-okay from the
front view but from the side view
there's all these garbage kind of
flowing so how do we clean it up so we
do we apply standard image processing
techniques okay we transform each path
segment into what's called a depth image
so this is a tree in front of the
building cars in front of the trees etc
and what we do then using the histogram
analysis over the vertical scans we try
to do what's called foreground
background separations the idea is that
the trees and cars are at a distance
from the trip from the buildings and
therefore by looking at
histogram we can remove those and this
works fairly well in downtown areas
where there's a fair distance between
the trees and buildings for residential
regions as else I'll talk about in a few
minutes it doesn't necessarily apply so
you start with with this kind of a image
is separated into foreground and
background as I was just talking about
so the background ends up being the
buildings and the foreground is trees
and the cars and various other things
and once and you apply a bunch of
techniques that I won't go over into
details but with the to fill out the
holes in the background which is the
building's you apply some interpolation
techniques and at the end you end up
with a clean background which consists
of your facades of your buildings this
video here shows the whole filling
process and as I said in in real life we
actually do the foreground removal
before the whole feeling but and once
you exit out of this
and then that and the next one shows
foregone removal it's pretty much
impending and actually filling holes in
the 3d data is a lot easier than texture
filling I'll talk quite a bit about
texture feeling okay and while we're
added one once you show the texture
mapping one as well I'll explain how the
texture mapping works in a little bit
more details in a second but this is
essentially University Avenue McDonald
Taiwan restore the futon thing etc
I
comment you go to a little bit more to
tell about how you interpolate over the
renewals I didn't make any slides on
those basically you look at the hole
from the left from the right from top
and bottom and you do interpolation but
you try to do interpolation while you're
preserving the edges kind of a clan so
you're not just fitting a plane to the
hole I think we do a little bit of that
we after we decide what's going on
around that you try to feel kind of
something that fits the neighbors but
also as smooth as possible in the moment
in the middle if it can be a plane it
can then that would right right we're
not I could tell you this is not
something that was terribly difficult
with him the texture hold filling was a
lot more problem I mean that required
the whole master's thesis let's put it
whereas the 3d hole filling was I would
say a month worth of work it wasn't
that's right and the answer to that is
that we're making up data anyway at both
texture and for 3d and at the end what
the best method is is subject to a lot
of debate you're right because that's a
lot of your goal is it your focus to
make a picture like this you do one
thing if your goal is to buy petrol
model into scheduled practice you
actually and actually if your goal is to
satisfy the military guys they don't
want you to handle the data in any way
because I mean my goal was to generate
something aesthetically beautiful and
let's say Hollywood wanstead the game
companies want that architecture firms
want it but the military wants accuracy
then I want you to mess around with the
data and make it up let's go Megan
underlying assumption
the distribution of the sky model you
could make that assumption what most of
our holes actually were a lot of them
were the windows because it went right
through the glass and so extending the
surrounding area was was kind of good
enough so this shows without processing
and with and there's a few other
examples that I haven't included here
and next we move on to what I just
talked about texture mapping these these
images I showed the video already ahead
of time but but here's the basic idea
we'll remove foreground objects like
trees from in front of the buildings so
in the 3d model there's no there's no 3d
model for the tree anymore that caused
the hole in the building and we filled
up the whole but now we want to do
texture mapping and if you if you really
want to do texture mapping you should
make sure the texture corresponding to
the to the tree doesn't get mapped onto
the building because there is no more
tree and so the question is how do we
identify the pixels corresponding to the
foreground objects that we just removed
from our 3d data in our images and it's
not you know your first thought might be
oh the laser scan and the and the camera
are synchronized and they are therefore
we came back project into the images and
figure out the locations in the images
where we removed the foreground objects
and you're quite right we can do all of
that but the resolution of the laser
scan is quite different from the
resolution of the of the image so what
this method works a little bit it needs
to be refined so for example if you
apply this method what you can remove
most of the tree but a little bit of the
tree residual still remains in your
images and you want to get rid of that
so the technique that we use and this
was the master thesis of seedorf Jane
who the PhD student at Berkeley was to
use optical flow and region growing
techniques in order to more finely
define the foreground objects like the
trees like the cars
and that worked pretty well and so you
can see that these dotted regions are
the foreground objects as detected and
we've removed it from in front of this
power bar building this is ross and
we've detected these white tree points
here and we've removed the tree entirely
from in front of this building so what
are the steps for texture mapping so you
start with these after you remove the
pixels corresponding to foreground
objects you have a series of images like
that they overlap quite a bit with each
other you make a mosaic out of them and
now because you've removed foreground
objects once again you have to invent
data for them because because now we're
trying to figure out what's in the
background and and fill it out and again
for downtown regions this program back
on separation works very well because
the objects are well separated for
residential areas and others might not
be very appropriate to do so but anyway
so we net next applied texture
synthesizing techniques which is
essentially in painting kind of
techniques and I'll explain that so to
go from here to here explain that in
just one second one second it's the
example that that on the next slide so
what we do is is a copy and paste method
but for some regions that are easy you
can do in painting and interpolation
simple things for other regions like
this you can do copy and paste method so
you're missing here you get a window you
go around this and compare that with
other parts of the image and see these
bricks are kind of a clue as to finding
out what ideally you should put here so
you do a search around this region do
copy and paste and you gradually start
building in those those missing parts no
it's not supervised there's copy and
paste and there's interpolation and
depending upon how big the hole is we
choose between one or the other that's
automatic yeah
Lafayette success advice i'ma show you
something
this would be kind of a good example
yeah we actually a whole lot of our
images could not be used because the
camera was pointing right at the Sun
those images got thrown away right at
right off the bat and then i would say
that there's probably thirty percent
overlap between successive images the
camera the intensity camera we were
running it at five Hertz and the
probably the speed of the truck it was
about 25 miles an hour that's the speed
limit at berkeley and i doubt if if my
students exceeded speed limit but unlike
their adviser they actually stick to it
so and and by the way that 25 miles an
hour results in these vertical scans to
be approximately five centimeters apart
if you're I don't know ten meters away
from the building or something so so
your resolution of these facades is very
high and you couldn't argue it's too
high because in it's actually the reason
we had to bring a PC to show some of our
models and not a laptop because the
model is so rich and and and that's the
reason that we slow down on Google Earth
so much when we put our 3d models in it
because it's a lot of data and we have
to do a lot of simplification as I'll
talk about it in just a second so so
this shows the copy and paste process we
do it in a to speed things up we use a
pyramid in order to do the search for
for these regions first I would do the
search over smaller regions then a
little bit bigger and then and the final
image and this is before and after so
for these regions for this picture the
holes were large enough that we had to
apply a copy and paste method and this
is another example by the way the yeah
I already showed the texture mapping
video and in the interest of not
crashing PowerPoint again we'll just
skip this okay so so now comes to yeah
question yeah
dered something turn the back
not that I know explicitly right now but
we've tried this on a this whole thing
on a four by four block of downtown
Berkeley so it's possible that that
would generate it could they could
happen the copy and paste method is
quite compute intensive actually and if
you skip that part and just interpolate
then things go would go a lot faster but
of course you wouldn't be able to for
example recover that arch in that
rolling ok so the rendering of these
things that actually the very first time
we generated these models we were like
four days away from our government
review in Washington and I kept telling
my students okay where is it put it on
the laptop I want to see and they said
you just don't understand it doesn't fit
it there's no you r ml browser that can
enable us to see this model it is kind
of like inventing something that you
don't even you can't even observe it
like a phantom of your imagination or
something anyway so the ground based
models the towpath segments they have
roughly 250 70,000 triangles or 20 to 20
megabytes of texture so many million
triangles for for downtown blocks but
four million and 400 megabytes of
textures so these are all difficult so
you have to build what's called levels
of details and generate scene graphs to
do that and sure enough we apply
techniques like Q slim to to go from
something that was more high resolution
like this something that was low
resolution here the geometry is ten
percent of this one and texture is
twenty-five percent of the original and
I'm one of the areas that I would like
to work on in the future is push Q slim
further and further in order to generate
even simpler increased it decrease this
number to maybe even one percent while
preserving the approximate shape of the
buildings this is something actually
Ming who is sitting here worked on a
little bit as part of his class project
at Berkeley last semester
so you can build these scene graphs with
different levels of detail by making
these cuts along the segment that's
shown here texture for the parts with
electric fluid like the top of that this
part I mean no the outside of the bottom
here you mean not all the balance of the
circle is actually faster look at it go
to the sea oh you mean behind this thing
nothing I didn't add input and actually
another good point that I didn't mention
is that this top part is blank because
our camera wasn't pointing high enough
and I can get into all the details of
why the camera wasn't pointing high
enough and all of that but basically the
next revision that we will do to this
system we're going to have multiple
cameras if the camera was pointing to
high the Sun with a block did ninety
percent of the time you would have
useless pictures so you have to come up
with a scheme where you get useful
pictures at the same time as covering
the top of the buildings correct you'll
see that in just a second and and
unfortunately the area has lower
resolutions and so you married to the
high resolution ground-based there's the
visible lines that shows the difference
I'll show it in just one second the
resolution mismatches horrendous that's
put it mildly I think okay so so then
after all it said unknown you have this
interactive rendering with a web-based
vrml browser which which Steve will show
in just a second so so this is the 12
block for sod models of downtown
Berkeley this is one facade this is the
street behind it etc etc and we put it
all together it looks something like
this which is looks a little bit like a
ghost town but if you put the rooftops
it will look better but so they just
move on to that just to give you an idea
the acquisition time for this was 25
minutes and the processing time was four
hours on 45 minutes on some penny of
machine and and and it's fully automated
we didn't hand-tuned and any part of it
by the way this number it does not
include the copy and paste part here we
we reduce that require we just did
interpolation all across so that's an
important point to emphasize so arable
modeling hire the company from Southern
California to fly their airplanes over
Berkeley to collect laser data they have
the laser equipment right on the plane
and they set everything up and then we
hired the helicopter pilot together with
one of the students to take pictures at
a separate time at the different
location etc and these pictures are
oblique airborne images that would
hopefully paint the the top of the
buildings the upper part of the facades
were the image the camera did not the
ground-based camera did not capture for
this one for you yes and Ming who is
sitting here is we finding the scheme
that I'm going to talk about in just one
minute that that is a hard problem
so this is the flow diagram of our urban
processing you start with the airborne
scans you do the sm generation post dsm
post-processing triangulation and then
image registration selection texture map
i'll go over each of these in just a
second so the airborne laser scans look
something like this we have to re
greeted on a direct on a square grid in
order to to get it into a format that
you can work with and figuring out what
the optimum grid parameter is not that
difficult and after you read we did some
grid points have multiple data and some
of them have none so you have to do some
interpolation again to get a height
field and after you do it you get
something like this which is called the
DSM a digital surface model and if you
just connect everything up and
triangulated it looks something very
noisy and Duke useless implication look
something very noisy and the reason is
because the move talks would work
specifically if you zoom in you see that
the rooftop super bumpy the edges are
very jittery all that so you go through
this set of processing steps which I
unfortunately don't have too much time
to go over it but the highlight of it is
this ransack polygon ization technique
that will divide up the rooftops into
segmented into planar regions planes can
be either horizontal or that an angle
like this but we're not fitting
quadratic surfaces or anything like that
and at the end you end up with this kind
of a model and now you're DSM after
triangulation looks neat and clean like
this
the next texture mapping from aerial so
this is the helicopter and we did the
20-minute ride 5 megapixel digital
camera 17 images and it's from from both
rooftops and facades so now that the
question that this gentleman asked comes
up how do we determine the pose of these
images for the airborne images so the
approach we took before I even get to
the approach me too you can just use
lowes algorithm I have a human being
click on seven points in the image seven
points on the model those are the
correspondences hit the Go button and in
like what 30 seconds or one minute or
something like that you get the batch so
that is very easy but the problem is it
doesn't scale if you have thousands are
going to sit doesn't scale so for the
models that have shown you what we've
done is we've actually done the human
processing part however after we compute
the answer using a human correspondence
type approach we perturb that by five
degrees tending in this direction 10
degrees there so many meters in the and
the Z direction and ask yourself if we
know approximately the pose if we had
IMS on the helicopter or something like
that could we have arrived at those same
poses that the human operator could get
in other which we perturb the true
answer we try to arrive at it again and
this is the technique we used and and
you'll see in just a second this method
assuming certain amount of uncertainty
in your gps and I NS etc results in a
processing step that's 24 hours per
image which is very large and that's
exactly why I'm ink roller sitting here
is working on the next generation scheme
which I'll talk about very very briefly
in order to speed this process up so
what's what's this step that takes 24
hours well you you compute 2d lines in
your images match try to match them with
3d lines
you're in your model and you you form
what's called this cost function here
and essentially you do an exhaustive
search over your six dimensional post
space at verifying increments in order
to find a particular pose that matches
that results in the best possible match
within these two de lines and 3d lines
and so here's an example where where the
match is excellent we have a good match
and here's an example where it's not
where the green line doesn't add up line
up with the edges of the building so
this this technique is ok but this seven
dimensional search space is extremely
non-smooth in our if your step sizes in
your exhaustive search is a little bit
too high you could easily miss the peak
in this optimization problem so
essentially steepest descent is inactive
inapplicable and that's why you have to
do exhaustive search to do that and this
slide shows you what I said earlier
which is if you have absolutely no idea
of where your helicopter is in this
column here at 360 degrees you are 180
degrees page thousand meter uncertainty
in your gps this is the number of poses
you have to go through and it will take
you 3.4 million years if you have
low-cost gps it takes to 25 hours which
is the number right through around a
minute ago now if you have differential
GPS with a little bit more expensive i
ns etc etc you could get it on in 40
seconds and what Ming is working on I'll
talk about it in just a second is a
method of using vanishing points and
other techniques in order to achieve
something like this 40 seconds or a
minute or two but using low-cost
equipment and I'll talk a little bit
about that sure
bottles
exactly edge extraction have
because he was doing it an image to the
match not image to 3d
he mean he had a 3d model he had images
of it and he was matching that he had
original person using a reg extraction
matches that the edges of the water
I have to look at the details of that
but you actually be very interested
so so now I get into fusing texture from
multiple images so each of our triangles
in our airborne model has been has been
imaged by more than one picture and so
the question is which texture do we use
in this series of images to to paint
that and we use a series of heuristics
that's that's listed here you want to
pick an image that has the highest
resolution for painting that triangle
you want to use visibility
considerations normal vector
considerations the triangle would look a
lot better if the picture was taken hand
on them for example from the side and
also enabled with consistency ideally
like triangles that are next to each
other to receive their texture from the
same images so that you don't have so
much jitter across the triangles and so
this picture here shows in your downtown
3d airborne model this shows that these
are the regions that were painted by
this red image these are the regions
using those criterias blue blue and gray
image and you and finally the last thing
you want to do is because there's a lot
of overlap between these images and only
you because you only use a small piece
of each image you build a what we call
an atlas image which physically doesn't
have any meaning but it it has texture
composited from all the different images
that's actually being used so this is
what the graphic cards IBM will use in
order to do the rendering of your
airborne model instead of having to
store 225 megabyte texture you want to
have 272 megabytes
so here's an airborne only model with
all the different things going on
everyone run the model from a different
view and finally model fusion and I'll
go over this very faster so that we can
look at some more videos so what you do
is if you look at the airborne only
models the down at the bottom these
models don't look very good because the
resolution of the images is low and
because we basically it's a rectilinear
model we had the roof talk segmentation
I would just brought it down so the
details of the 3d model at that street
level is not very good so you divide
that your model into segments remove the
part of the airborne model that's
touching the street which is shown in
this gray area here insert the ground
base model which is shown here and this
part of the ground base model doesn't
have texture but it is still
ground-based you can see that and as you
can see there is some holes between the
ground-based and the roof top model and
what you do is we apply what's called a
blend mesh to connect the two thing
together and finally you slapping the
airborne picture in order to paste or
paint that the upper part of the
building which didn't have and so you
can see that is quite a bit of
resolution difference and color
adjustment that needs to be done yeah
exactly that's very true especially
because buildings are highly repetitive
that's a very good point so so this is
it and this shows other examples of you
know both positive and negative the
airborne image is very nicely aligned
for the ground-based image showing that
the truck localization worked very well
and every the registration problem has
been solved nicely but the downside is
that these this resolution is max and
those can be solved by either collecting
the better data at the ground level or
just as its gentleman said just
extrapolate this data to make up this
data same things here same thing here
same thing here that's downtown Berkeley
another fly through model so I think at
this one I'm going to have Steve before
i get into future work just show us a
demo long from of a vrml demo of this
downtown model just hit the nice thing
so this is the entire fused model for
the four book four by four block of
Berkeley
good did you eat the triumph button yeah
once you do that you have to quit okay
why don't you do that
so this is the walkthrough and this was
the model that we inserted inside Google
Earth at the very beginning this model
has one JT fun ok so this model has
three levels of detail whereas what we
inserted inside Google Earth just had
the medium level and one of the reasons
was that if be if you're trying to put
the high resolution version on the model
inside Google Earth it's too much
texture it would just not be smooth on
any of the machines we have four
interactive or rendering of it
ok
thank you so few words on future work
it's actually 50 more slides on future
work but i won't i won't talk so long
it's very hard to compete with lunch so
try to wrap up everything in eight
minutes so scaling to very large regions
is a big goal of ours and i'll talk
about some of our thoughts on that
dealing with trees and vegetation this
program background stuff extended to
endure modeling integrating the 3d
models with sensor networks that's an
area that the government is funding us
and streaming this 3d texture models to
maybe handheld devices and also model
updates if you've already collected a
bunch of data and now some of the
buildings are gone so new ones have
appeared how do you fuse these two
models without having to start
everything from scratch so scaling to
large regions so far we've used laser
data and camera imagery both from
airborne and ground and I can't start
wondering whether we could have
simplified our life if we just didn't
have ground-based laser scan for example
or how would our models look like if we
just have airborne laser and camera and
that would scale very nicely because
when you go up in the air take one
picture and covers a huge area or if you
can take even a video camera up on the
helicopter and covers a huge area said
so if your goal is just have fly
throughs and you don't really want to
band in the ground and see what's going
on at the street levels then the
airborne brains urban only models can
look very nicely however for urban only
models as was pointed out during this
talk the pose estimation for helicopter
images is still a big problem and that's
one of the things that that was a
24-hour thing that was talking about
your choices are 3.4 million years 24
hours or 40 seconds depending upon what
kind of equipment you have on the
helicopter so what Ming is working on is
is methods of developing low complexity
pose estimation techniques from from
from for airborne images and so the
lines of thinking's that we have is to
use a electronic compass or hooked up to
a camera take pictures apply vanishing
point kind of techniques to recover some
of the cameras then do 3d feature
matching both in the and they from 3d
model today to the images and this is
all work in progress so there's no
results but merely outlines the kinds of
things that we think we're going to be
doing in the next five to six months and
apply a ransack type algorithm to do
this correspondence and in order to
derive the model the parameters of the
camera and we're hoping that this
approach of using vanishing points will
be less time-consuming than the
exhaustive search that took 24 hours per
image the other problem is is trees what
do you do with trees in residential
areas what you do with trees in in in
downtown type areas if you'd only move
the trees sometimes your model could
become very cluttered on the other hand
if you remove it you have to make up for
the texture behind the trees and that's
always that's not always easy John
Secord was a student who worked on it
got his master's thesis in 2006 and he
came up with a tree detection algorithm
using registered aerial leader or add
imagery based on segmentation and
classification we used region going for
segmentation with together with training
and classification we use support vector
machines these are the features that we
use for for segmentation height
variation normal vectors hue saturation
value etc and this is the weights of the
different features as it turns out the
height variation is just about the most
you apply normalized cut technique and
you find out that height variation is
just about the most important feature in
doing that segmentation process then you
do it support vector machine
classification and you start with the
leader and airborne data texture mapped
that was for residential region and
that's for campus this is
result of your segmentation and this is
a final result of three detection so the
green shows trees that our algorithm
actually detected as trees the blue dark
blue ones right here are there were not
trees and they were incorrectly
classified as trees and the purple they
were incorrectly classified as non tree
but they were really trees so you can
see the algorithm mostly works this work
is going to get published in I Tripoli
Geoscience letters in about few months
this is the same thing for campus data
the green is the trees and we pretty
much detected most of them so ideally
you like that you like to have very
little purple or blue and detect all the
greens properly
probably how could it detected it as non
free this is our you know saying why is
it that it does it you saying those
things are trees the green is the trees
that we detected as trees correctly that
ice cream yes it's an incorrect nostrils
correct correctly a tree that's correct
and you're right why is it then maybe
these two need to be flipped yeah you it
depends what the meaning of this thing I
put down here yeah yes so you can
actually protect our
pressure
they're directly distance education
you haven't reflectance for we laser
scans I'd say if you have wanted we're
in a picture to find the periodic fire
as equity 0
translate thank you and I go back to
this one yeah Oh actually that's a good
point I was never brought up even though
this talk has been given before so I'll
look into them and see why why the
purple is on top of the building this is
the positive false positive rate versus
true positive rate and the the purple
line is this segmentation classification
technique and the green is kind of the
same approach if you just skip the
segmentation and do it on a point wise
way and this is for residential data for
campus data so you can see that actually
segmentation does help you to accomplish
this task portable modeling possibly
endure you can there's nothing to stop
you from applying a lot of these things
indoors because we didn't use GPS
outdoors we're now uniquely qualified I
guess to to apply the same kind of
techniques for indoors I don't think we
want to carry a horizontal and a
vertical lasers kinda that's too heavy I
think Steve would know the numbers but
it's about 10 pounds each or isn't it
yeah so 20 pounds i think is is
excessive on anybody's back but you
still need to recover the six
dimensional vof but together with some
ims and camera imagery you can you can
recover those things there's a lot of
nice things about indoors you're walking
a lot slower than a truck was that when
you're not doing 25 miles-an-hour as you
enter rooms you can click a button as
you leave it you can clear the book
click button therefore you can closed
loops so one nice tricks you can play
you can have additional sensors like
gravity and altitude etc and update is
easier indoors than it is outdoors so
you can use the same kind of sort of
sort of equipment maybe just one
vertical laser scanner scum cameras and
some local scientist and i am u units
just a quick cost calculation would be
something between seven and 14 k to
build the equipment the power
requirements is 31.2 watts and pretty
much if you have these
pack of 12 volt batteries that you can
put that like a built around yourself
you can make it portable unit and the
weight of the whole thing total would be
24 pounds with just one laser scanner
again this is something that's in
progress we don't have any results to
show us I'm going to skip the sensor
placement stuff but you can see that if
you play sensors around the city then
you can visualize these things nicely
and can catch the bad guys as needed I'm
also going to skip dynamics in modeling
you can extend some of these techniques
so that if there's an object moving with
the hands and arms and you have scanning
equipment and you can generate a
three-dimensional model as a function of
time 30 30 frames at 30 times a second
for example and this this system we have
actually built in it and we've published
about it there's a rotating mirror
there's a laser hitting it and this is
this causes a laser line on your object
and as the mirror rotates this
horizontal line goes up and down and you
project also with the halogen lamp and
ir filter uprichard vertical lines and
we can compute the depth along these
lines and build the depth as a function
of time right it's a screen it's a
screen with a bunch of vertical strips
on it punk I've never seen the term
roast before applied it's probably a
foreign student using that term but it's
a screen with a bunch of strips going up
to project the vertical actually it's
right here
you paste a bunch of stripes and that
that's what projects the vertical lines
onto the object okay and then here we
have cameras that are have IR filter is
and also visible camera the IR filter
one will capped this signal coming from
the IR halogen lamp and the control pc
and the sync generator and all the rest
of it that's the polygonal mirror with
the horizontal laser and so at the end
you end up having a depth sparks that
representation of the object as it's
moving you can interpret that to get
then step and then you can let's see if
this video works probably not no it does
random so then you can reconstruct the
3d depth of this person as you go around
the person to the right and to the left
and we only have one of these stations
if you duplicate multiple stations
around it you can get the back signal
that merge those two things and I get a
true sense of a three-dimensional depth
of it so real quickly other areas
streaming and and and also model update
these are areas that we haven't done any
working but quite interesting we hope
that in the coming year will be able to
to get to it I'm going to stop right
here and this is the various places you
can download papers and demos and models
and various other things and shall we
end with the we give all the demos it is
the more really okay you mean the vrml
model okay so I'm just going to stop and
and show you the now it's a vrml model
that has it has airborne only you got
flight for dr by dr whoa it would look
terrible you did right we can do it just
so you can see it but no it wouldn't
look good because it's all airborne only
the only thing else thinking why is the
problem may be stupid but why why why am
I to remove trees from arts with us if
you want to have a truly immersive
experience I think you really need
speaking publicly the problem is that
the day at the ground level when you
scan the trees you don't do a good job
of making a model out of it because the
resolution of the laser isn't very good
but you're quite correct for residential
am personally believe in more and more
that we shouldn't be removing trees
because it's so difficult to replace
them with I mean the idea is let's
remove the trees and then replace it
with an artificial tree so that we can
have a better model of the tree that's
kind of it thinking but that that
process as time goes on I'm beginning to
believe is more and more difficult not
to good idea and probably less immersive
as well that's true that's true i think
that's that's just about yeah question
yeah i forgot the way you're right i
guess threw up for carnivores fret and
then what else once you're left with you
do an ionization but this entire 12
possible state is our small fillers and
you will have very careful those
teachers and cuz we remove yeah
to get a very good geometry for those
features done and that will add
unnecessary clutter to the model in my
opinion thus to make sense to sebago are
split on
why would you actually the modern and
one way to do that revisional maybe a
procedure thing is that so
yeah it's possible to do it there are
people who do for surgery model fitting
for example all Rick Newman at USC does
something along those lines however his
methods are semi interactive so there's
a user that comes in and clicks on
things and says you know for the
parabolic surface here and for the plane
here for the amount of plan here etc but
I think your point is quite well taken
and philosophically that you're also
bring you a very good point why are we
move things and then make up data why
not try to fix what you have and do the
best of what you got without extracting
and of because the model at the end also
looks a little bit less for the
realistic
fiction that there's another talk that I
said let's discuss this offline</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>