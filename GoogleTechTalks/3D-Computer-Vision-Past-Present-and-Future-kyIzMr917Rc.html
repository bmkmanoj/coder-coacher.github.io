<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>3D Computer Vision:  Past, Present, and Future | Coder Coacher - Coaching Coders</title><meta content="3D Computer Vision:  Past, Present, and Future - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>3D Computer Vision:  Past, Present, and Future</b></h2><h5 class="post__date">2012-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kyIzMr917Rc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I somewhat foolishly agreed to give a
talk on the history of 3d computer
vision for a workshop in Italy a week
ago not realizing how much time this
would take so basically you know I'm
very familiar with a lot of computer
vision and 3d computer vision in
particular but really trying to give a
history of this problem you know goes
back decades and so I had to consult a
bunch of people to really get this right
so I immediately sent email to a number
of the pioneers you'll record many of
you recognized on this list like Shree
and I are and bertolt Horan and andrew
zisserman and so folk so forth basically
you know all the people you think of
about doing pioneering work and computer
vision to try to get their input and
what work are the breakthroughs in
computer vision over the years and in
the 3d domain when it closed the door
yeah and so of course everyone send you
different lists and you know all the
structure motion guys sent me structure
motion references and all the you know
photometric guys like a horn and Woodham
sent me you know shape from shading
references and so everyone had a
different interpretation of what is a
landmark or a breakthrough so I had to
kind of call from this some subset of
things and I think every one of these
people would be horrified by the talk
I'm about to give because it only covers
a small subset of the references that
they gave me for what they think are the
pioneering your references so if you
want to cover all 3d vision it's
actually a very broad area on so I had
to select only a small number of things
from each subfield and so because I'm
not going to cover all of this stuff you
know many of these emails I got from
these folks were just terrific they you
know really detailed histories over the
last several decades so I'm also going
to post online their responses so was
brittle horn think were the canonical
references in
3d vision and so forth okay so in
addition to these individuals who gave
me some information I also there's lots
of good information online and in papers
and so forth and here's here are a few
links if you're interested to know more
about the history of photogrammetry or
bundle adjustment and so forth there's
some good materials online
alright so disclaimer I've already said
this a little bit but this list is very
incomplete for sure it's also somewhat
biased because it's my own
interpretation of the highlights in 3d
computer vision over the years although
it's informed by a lot of by experts by
readings and just you know my my
experience over the years I tended to
select for high impact results results
like the things which I think had the
biggest impact either in practice or on
the research community and so there's
there's a fewer kind of theoretical
results although there's a few in this
list and of course I'm omitting a lot of
really important results just before did
you fit this into an hour okay so with
that let me jump into it
so first of all fit of so-called
prehistory so so here are a few really
important people over the years the
centuries up to now so Leonardo da Vinci
is not really the final means the first
one to know about perspective but he
articulated especially well so I love
this quote perspective is nothing else
than the seeing of an object behind a
sheet of glass smooth and quite
transparent on the surface of which all
things may be marked that are behind as
this class so he had the concept of a
flat image playing all things
transmitted their images to the eye by
pyramidal lines so rays and these
pyramidal lines are cut by said glass
the New York of the eye
these are intersected the smaller the
image of their cause will appear so you
know really beautiful description of
perspective and have you know all the
key idea
at that time okay so another really
important figure in 3d is Lambert and
you probably know him best through
Lambert's law which is which is a really
model how how light gets reflected off
of it after it hits a surface and gets
scattered in all directions for matte
surfaces and and and really you know
this has been the bane of computer
vision algorithms because almost
everything assumes lambertian
reflectance and therefore breaks down
for anything that's shiny or specular or
non-membership so we really have Lambert
to blame for for all these troubles in
the field but also it turns out he he
was the first person who who proposed
inverse projection so basically figuring
out where the camera was from from one
or more images and that's also known as
camera resection about geometry
literature next on this list is Gauss
and Gauss is you know hugely important
for all sorts of things but one the
element among those is least squares and
of course least squares is is really
fundamental to almost all of our
algorithm of these days both linear
algorithms as well iterative non linear
algorithms and so it's it's it been
central to photogrammetry as well
now Wheatstone is maybe a bit more
controversial to sort of put in the same
line as these these other three but he
was the first to really articulate the
principles of stereo and basically the
idea that stereo consists of horizontal
parallax between two images between
retinal projections and it's kind of
surprising that this wasn't you know
known and in 1807 in you know maybe it
was I mean there's some evidence that
Leonardo knew about stereo although he
didn't explain it as as parallax but but
what's interesting about wheatstone's
work is he was able to basically prove
it by creating images synthetic
putting them in the stereoscope invented
the stereoscope which then you could
look at and see fuse a 3d image so
basically that's a proof of this
parallax concept and of course stereo
has been central to a lot of major
advances in 3d vision okay
so in parallel with computer vision and
actually predating a lot of computer
vision is photogrammetry so
photogrammetry is basically the the
science of measurement from from optics
from some optical observation I'm sort
of science stepping the word image
because for the gram tree in some sense
predated photography so the mathematical
foundations go back to the early 1800s
so Ponce lays invention of projective
geometry terms you know things like the
her Opteron multi D relations and so
forth and I'll cover some of these later
in this talk in the context of other
computer vision of breakthroughs but the
mathematics goes goes back a long ways
practical use people interested in
photogrammetry that's part of map making
and geodesy and basically the idea is
that if you're an imperialist country
you want to make sure that you can map
the boundaries of nations that you're
conquering and so forth so people have
long been interested in map making and
one of the one of the great maybe the
biggest breakthrough in photogrammetry
is is the this study called the great
arc of India that the the British so
when the British were surveying India
this this turned out to be a huge
undertaking so the idea was basically to
create a 3d map of India and so they
took these huge theodolites that weighed
a ton literally a ton through the
jungles of India it started to take her
measurements in different places
and they're able to show that the that
mountain Everest was actually the
highest mountain on earth so it was
thought before then that the Antonino
mountain in the Andes was higher but so
this was one of one of the things that
photogrammetry has to claim as
a big advance anyway so the basic the
way this was done was taking
measurements from these different
theodolites triangulations and kind of
putting all these triangulations into a
big manual bundle adjustment so they had
all these measurements between pairs of
you know sensors if you will and they
had to kind of bundle them all together
by writing out the equations and solving
for the relative positions of objects on
paper so but this was an example bundle
adjustment in the 1800s of course this
would this was advanced with the
invention of photography and around the
same time and they started putting these
things together in the late 18-hundreds
okay so now that the net the next big
advance you know I'm skipping a lot of
big events here actually in
photogrammetry but when the computers
came about computers that were we're big
enough to do large least-squares
computations photogrammetry immediately
saw the potential to use this to for
photogrammetry applications so so if you
think you're someone named doing braun
pioneered use of computers for bundled
husband and you know basically
triangulation efforts in the 1950s and
so you can really date back
computer-based bundle adjustment to the
1950s although at the time they weren't
doing any image analysis they were just
using the computers to solve the
equations not to extract features or
doing any measurements on the images
themselves okay so I called that all
prehistory because there was no computer
vision there's no analysis of the images
using image using computers so let me
talk about what were the first uses of
computer vision for 3d measurement and
so as far as I can tell the first
example of 3d computer vision is stereo
and this is work that by this crazy
inventor named Gilbert hope RAL and
basically he
he was at the time the way
photogrammetry worked mapmaking worked
as you had operators who would look at a
pair of images and find corresponding
points by hands and then specify the
displacements and then feed those into a
computer to solve for the positions of
these points and so homo observed well
wouldn't it be great if you could
automate this manual correspondence task
because that was really the bottleneck
and so he invented this machine to do so
and so this is really an analog stereo
machine analog hardware and it's pretty
it's pretty incredible so I wasn't able
to find original paper on this but the
patents are online so here's an image
from the patent so what you're seeing
here is basically a table with two
images slides on them transparent images
and there is a CRT down below which is
scanning basically a spot across both
images in parallel and so it's
illuminating a particular pixel so to
speak in in each image and the intensity
of that pixels being picked up by a
sensor which is measuring intensity in
both images of the spot and then there's
a correlator which compares those
intensities and decides whether it's the
same or not if they're the same then it
knows a disparity based on the relative
positions of the spot and say and so
there's a match it gives you the
disparity in the depth if they're not
based on the difference in disparity it
will shift the beam in one of the images
okay so and it would do this over and
over again so analog implementation of
stereo
its automated and he had a series of
innovations on top of this and basically
invented lucas-kanade e stereo on a
pyramid you know back in the 50s and for
those of you know a lot about computer
vision you'll know what I'm talking
about but yeah the there's this is a
rediscovered and computer vision they
you know three decades later but pretty
pretty amazing stuff all right so I'd
say that was not very well known you
know when I almost no one that talked to
computer vision has ever heard of the
previous slide this one is very well
known
so Larry Roberts his PhD thesis in 1963
at MIT he invented what's known as block
swirl so this phase of computer vision
where where you basically take an image
of a couple blocks and then based on
extracting where the edges are which he
did using a grading operator on a
computer which was the first
implementations of doing a gradient on
computers he's able to extract edges and
lines and then based on the relative
orientations of these lines is able to
figure out their configuration 3db using
a line labeling technique and came up
with a way to render the the thing from
a new viewpoint as you're seeing in the
far right
using presumably some kind of vector
graphics anyway so this is this is
actually an incredible achievement in
1963 to do all this stuff to implement
the whole thing and have it work even if
it only works for two blocks but so
Larry Roberts is often considered the
founder the father of computer vision
you know this title often thrown around
so I went to his Wikipedia page to sort
of see what it said about him and
actually his Wikipedia page doesn't even
mention computer vision so it turns out
after his thesis he stopped in computer
vision altogether maybe is disgusted by
it and it's dead he started working on
networks and he was one of the you know
one of the he used the founder of the
ARPANET as considered one of the four
fathers of the Internet so that's what
his Wikipedia page talked about computer
vision but anyway so this was a pretty
pretty impressive breakthrough and you
know one of the first results in 3d
computer vision for sure
so by the way people have questions feel
free to to interrupt you know there may
be some people in the audience who who
are aware of things I'm missing and you
know any feedback is great okay so this
really fueled Larry Roberts work really
fueled essentially a generation or at
least a decade of work on blocks world
modeling and and the culmination of this
or one of the best-known examples was in
the late sixties early seventies there's
this demo on this system at MIT where
they they try to introduce a robot which
would basically take pictures of in real
time of blocks and then based on that
construct a plan for building a
structure that it just took a picture of
from another set of blocks so basically
you have a set of blocks stacked in a
particular way the robot tries to
reproduce that exact stacking
configuration from another set of blocks
by figuring out the structure of the
blocks as well as planning and making a
plan to navigate the robot and this is
incredibly ambitious because it
basically involves solving the computer
vision problem solving the planning
problem solving the AI problem in some
sense at least the manipulation problem
and it turned out that most of this was
doable the weakest link was actually
low-level edge finding yeah in images so
kind of a disappointing weak link and
this really inspired people to work on
edge detection which they did for for
many years after that so this really
inspired kind of a dive into low-level
computer vision and you know all the
kind of canny
and similar results came out of this
alright so fast board actually this is
around the same time actually so Myrtle
horn vision pioneer so he did his thesis
at MIT and in 1970 he published a CSS
and this was on shape from shading and
this is one of the first messages that
said showed how from a single image you
can infer the shape of an object and
basically the idea as you can somewhat
see in this slide if you're depending on
the projector which is not so good on
our end but there are these curves
superimposed on the image and these
these are so-called characteristic
curves or characteristic strips where if
you know the depth of the scene along
one point on this trip you can figure
out the depth along any other point on
this trip so you can't figure out the
absolute depth of this of this trip but
if you know one point that you can
integrate out the death long every other
point and he also showed that if you
have a network of these strips and you
have certain boundary conditions then
you can actually integrate out the
surface of the scene so you can really
recover the shape in other words the
surface orientation and the mesh of the
face from from a single image
assuming making lots of assumptions
about constant reflectance and so forth
so this is this is really a key result
it turns out the astrophysicists were
also doing similar things around the
same time around slightly earlier around
mid 1960s there's a field called known
as photo Kannamma tree and they were
there in their interest was they wanted
to turn the shape of planets from an
image of that planet and so there's
similar work it doesn't it's typically
for profiles so 1d instead of for full
images Einhorn is really credited with
one of the first to do kind of really 2d
shape from shading now for each of these
landmarks I'm talking about you can sort
of trace on the origins back many years
before so Ernst Mach actually formulated
the some of the basic equations behind
this image irradiance equation almost a
hundred years before but he actually
concluded that inverting this equation
recovering the shape was impossible so
it's kind of cool at least that Horan
was able to prove him wrong wrong
although I guess it took a hundred years
to do it well but I did wasn't he just
saying that the problem was ill posed
and horned regularized on top of it well
so in some sense yes but but he also
thought that it was too unstable to even
to actually haven't looked at the
original quotes of Mach so you know I'm
not sure but the implication was that
Mach didn't think it was feasible
because there there there you wouldn't
be able to get boundary conditions or
whatever it's a good question this may
be too strong to include those but he
had a quote saying it's basically
impossible all right so another really
amazing result from the 70s so this is
Bruce bomgaarts PhD thesis from 1974 I'm
he was the first to do shape from
silhouettes so the idea is that you have
some scene and you have a set of photos
and you you're somehow able to extract
the silhouette of the object the
boundary of the object separate the
foreground for the background in this
case you see this is an image of a doll
they're pretty low quality these are
scan from his thesis and so from this
doll here's the silhouette of the doll
here's the sole of the doll from
different viewpoint you can see it's
actually missing the head and this
points to the difficulty of actually
getting good silhouettes and in this
image it's also missing the head and so
the basic idea is that if you know the
camera positions for all these different
views then you can basically back
project the images so this is another
great slide from his thesis and so the
idea is that you know where the camera
is you know the image it's image plane
sitting in front of the camera if you
trace a ray from the center of the
camera through every point on the
silhouette that gives you a conical
volume which the scene must lie in 3d
and so from each view you get a
different volume which constrains the
scene if you intersect all those volumes
that gives you an approximation of the
sea
and so this is an idea this is a figure
which impressively shows that kind of
intersection where you see the
silhouettes at the end and the back
projected intersection so here's the
results that he got they look a little
bit crude but they're actually pretty
pretty nice so here's some here's the
back side of the doll you can see the
head is missing again because it was
missing in the silhouette and that's
this shows what the limitations of shape
from silhouette methods if you for if
you miss a part of the object in any of
you it disappears in all of them all of
these so this figure so each row is a
different view and the two columns
correspond to a stereo pair so if you're
able to fuse these images students ones
people can do stereoscopic fusion you
might be able to better see the 3d
structure from the image on the far
right but it's kind of cool actually
this thesis that he came up with the
idea of doing a stereo reproduction like
this so in order to do this you just
think of how much he would have had to
do in a 1974 computer in order to do
this you have to do the edge detection
he had to do the boundary follow
following he had to do the back
projection in 3d and he had to fit these
polyhedral models to the back projection
just that last part could have been a
PhD I think in the 1970s and he had to
invent all these edge data structures
some of which are still in use today to
do the geometrical model so very
impressive piece of work actually his
thesis is online so if you go to this
link at the bottom I've tried to include
links as much as I could to information
online if you're interested in digging
up more these blue links on the slides
and I'll post these flights as well ok
so here's another example I think which
looks a little bit better images of a
horse and you can see the horse model
which looks a bit more like a horse
alright so I'm not going to go into
details on these just because I'm
probably not gonna have time but there's
a lot of follow-on work on shape from
silhouettes and if you're interested
here's more where you can read what the
vision community has done which builds
on this basic stuff
all right so I talked about shape from
shading horns work but to this date
actually the really the community has
not been able to successfully make get
good results out of shape from shading
maybe as mock predicted it's very
unstable and it's hard to get accurate
results you have to make very strong
assumptions to make it work and really
the only reason for including horns work
as a landmark is in my mind is that it
led to photometric stereo which is this
slide so the key idea is that if instead
you just take one instead of taking just
one image if you take say three images
of the same scene from the same
viewpoint then their different
illuminations those three images give
you enough hues that you can reliably
extract shade and this works really well
so this is Woodhams PhD thesis Woodham
was a bob wood and was a student of
Berthold horns and so and this has an
interesting history as well so the so
wouldn't describe this photometric
stereo approach in this thesis but
actually he wasn't the one who
implemented the first implementation was
by masters student William silver in
1980 also at MIT and this you can show
see by the result that you know so this
is a reconstruction of like basically an
egg it's not actually a real egg it's a
wooden egg but but it if you compare
this model to all the other models I've
shown up to this point the details are
spectacular and if on the right is a
profile of the egg in black and there's
a dotted line which you probably can't
see very well but the dotted line is the
ground truth the correct answer and
they're basically identical so
incredibly accurate models so one of the
people in computer vision you know about
photometric stereo and Bob Wooden's
worked in particular but I think a
misconception is that people think it
only works for Lamberson
and in fact they showed the very first
work in his thesis and in Silver's
implementation show that you can use
this for any kind of scene any kind of
brdf as long as it's not translucent has
to be an opaque surface but it can be
shiny or can be made out of whatever you
want as long as you have as you know the
form of that reflectance function okay
so and in fact in Silver's
implementation what he did was he
measured the reflectance function of
wood basically by having a reference
object whose shape was known and taking
measurements under different luma
nations and use this to reconstruct
other wooden shapes okay so as long as
you know the reflectance function you
can use this method it doesn't require
membership all right so one metric
stereo I mean works so well it basically
it's an inspired lot of subsequent work
in the community and here here are some
of the highlights I mean basically each
one of these landmarks has like a series
of arguably landmarks after them you
know so people who work on photometric
stereo would say here are the landmarks
of photometric stereo so I tried to do
that for a couple of these subfields but
I'm not gonna go into detail just
because they don't have time all right
so okay so the next major landmark is
something called the essential matrix
and this was basically one of the first
algorithms for recovering a scene from
two projections under under perspective
and basically the the observation was
this is work by lunga higgins in nature
it's actually cool that there's a nature
paper on entitled a computer algorithm
for dot dot dot so basically he came up
with the the observation that if you
have points in correspondence between
two images they're related by a three by
three matrix and in particular there's a
three by three matrix or if you take a
point represented in homogeneous
coordinates XYZ sorry XY 1 from the
first image and you multiply it by this
3 by 3 matrix you get the corresponding
Pepe polar line in the other image in
other words the line in which this
correspondence must occur in the other
image and this mapping from points to
lines can be represented by this Rank 2
through by 3 matrix and you can also
compute camera matrices from this matrix
and so forth and so he came up with a
very linear algorithm for recovering
these this matrix from multiple
projections points sorry two images
multiple points and this is very very
influential paper and inspired like a
whole area of computer vision now known
as multi-view geometry with books by
Harvey and this ermine faux Jarrah and
started this explosion of interest in
this area in the 1990s now it turns out
that the theory all the math here can be
traced back to mathematicians notably
Chaz 'el from the 1860s
so the back in the 1860s they actually
figured out that there is a three by
three rank two matrix which Maps points
in one image two lines in the other
image okay so this was known they didn't
have an algorithm that you know
recovered these points because you know
there mathematicians don't work in
algorithms but interesting to note that
a lot of lot of fundamentals were work
or a known long time ago all right so
and then there's been a lot of such good
work since then stuff like the
fundamental matrix the essential matrix
wasn't grand enough you had to have
another matrix which is even more grand
the fundamental matrix is basically the
uncalibrated version of this the
trifocal tensor is the three view case
and so forth all right so next major
breakthrough and this is an interesting
one I think it's probably not too
controversial for this audience but
might be controversial for some of the
others you know is this really computing
is this really computer vision so this
is a algorithm called marching cubes by
Lawrence and Klein
published in SIGGRAPH 1987 basically
this was an algorithm to go from a
volume to a surface very simple
algorithm on the surf you know
at some level but actually involves a
fair amount of sophistication to get it
right so the way it works is you assume
you have a volume which implicitly
represents the surface so typically
these are signed distance functions so
the appositive my mean outside negative
insight into the zeros are the surface
so if you start out a voxel and this
volume which contains the surface and
that basically tells you how to March
in order to stay on the surface so he
started a seed point on the surface that
he started marching over the surface and
then every time he marched to another
voxel which is on the surface you create
a polygon okay and you have to figure
out which polygon to create based on
where the sub box will I so function
would intersect that voxel okay so you
do this at sub box of precision and to
create you know different sorts of
polygons and there's basically a table
which tells you for every possible
configuration of previous voxel next
voxel the type of polygon to create
intersecting which sides of the of the
voxel and so you have to get this table
right in fact they didn't get the table
quite right in the original publication
so if you implement this make sure you
look at the the updates to this table to
get it right but so and there's some
earlier work there's you know 1970s
paper and and so forth where they try to
do similar things but I think this is a
big landmark I mean something which is
still in widespread use today so it's
pretty uncommon in our field as you all
know to find an algorithm which is still
used you know more than 20 you more than
20 years on 25 years later and this is
an example so important landmark okay so
when I started so we're actually
catching up to word I actually started
working on computer vision in the 1990s
and and really one of the things was
really inspired me was this work by
Tomas and Kanade
on structure for motion and the basic
idea is you give a set of points over an
image sequence and if you know the
correspondence of all these points you
can just put them the point coordinates
in rows of a matrix let's call it W so
the first row of the matrix would be the
point 1 position
the x-coordinate of the point in of the
first point in all the images the second
row might be the x-coordinate of the
second point in all the images and then
you'd have a series of y the bottom half
the matrix b the y-coordinates so it
turns out that if you have this matrix W
you can express that matrix as a product
of the motion of the camera and the
shape of the scene and so the the
entries of the motion matrix are
literally the camera parameters under
orthographic projection and the entries
of the shape matrix are the 3d positions
of those points ok this is basically a
one-line solution to this structure
motion problem being able to recover 3d
from a sequence of images so extremely
elegant very very cool highly highly
influential it turns out to be optimal
under under the right assumptions and
alpha and projection model which is a
version of orthographic and inspired all
sorts of follow-on work these are just a
few extensions to multiple bodies
optical flow that knows about 3d
structure non rigid shapes and so forth
now turns out that this hasn't really
been picked up in modern structure for
motion bundle adjustment 3d
reconstruction problems because it only
works for a line projection and turns
out not to work so well with outliers
and there's all sorts of caveats but but
it's very cool that you can do this and
and I think you've got a lot of people
interested in computer vision alright so
next landmark on my list interative
closest points because this was an idea
which was simultaneously invented by
three different authors so I think this
is evidence
the idea was kind of in the air that so
many people came up with at the same
time but the basic idea is that suppose
you have overlapping range scans of an
object and you want to piece them
together into a 3d model you first have
to align the range scans and ICP is a
way of doing that and the basic idea of
how it works you have this green range
scan and this red range scan over here I
guess you can't see the green at all
must be the green pixels are knocked out
of the projector or something like that
but you have correspondence between so
the key thing is that if you knew the
correspondence of points on the green
curve and the red curve then you could
find the alignment between them there's
a linear way to recover the the
alignment of the two curves but you
don't know the correspondence so the
trick is for every point on the green
curve you find it and you have an
estimate of its current rotation
relative and translation relative to the
red curve you find the closest point to
that on the red curve and then you
assume that that's the right
correspondence then you solve for the
alignments and then you get a better
align pair occurs and then it's not
perfect because the correspondence was
wrong but now it's closer so you then
Rhys Rhys off for the putative
correspondences and then iterate this
procedure and over and over again this
actually works quite well and again this
is one of those examples that you know
20 years later is in widespread use
alright so another really important
result 1995 this work by a Sri Neiers
group on depth from T focus so here
you're seeing a real-time range scan
system one of the first of its type of
its kind and what you're seeing is on
the left there's a cop pouring milk into
another cup and on the right is a is a
depth map computed in real time and
again this is you know this is 15 years
old or more and and basically what it's
doing is it's taking two images with
different focus blur so the two images
are have different focus one is out of
focus
a different way than the other one
because they're using different aperture
sizes and it's a different no I think
it's actually focus settings and based
on the relative blur you can figure out
the depth of the depth of the scene okay
so this is they weren't so nice group
wasn't the very first to invent this
idea of D focus but they're one but this
was the first I think really compound
system that showed how it can be
practical and another innovation here
was typically when you take images with
different focus settings and you can try
this with your own camera when you
change the focus the image actually the
magnification of the image changes a
little bit and so they came up with a
way to make this really work reliable
reliably they had to come up with the
way to avoid this so they used a notion
called telecentric optics which allowed
you to have two different camera pals
with two different focus settings which
produce images with the exact same
magnification and so nigh are also
contributed this idea to the computer
vision community and it made a big
difference so this implementation it ran
at 30 Hertz accurate to you know 0.3
percent error
very impressive again from 1990's all
right so 1996 there's this work by Paul
Bewick and collaborators at Berkeley
this is this famous video that Paul made
there he is running and he's holding a
model of the Campanile II was actually
sound on this of like bells ringing
Kevin Orie bells ringing
so there's the model of not using the
rendering of his reconstruction and the
berkeley campus model in 3d
you can think of this as kind of a
predecessor to Google Earth and similar
products you know that try to do really
high accuracy in 3d modeling and I think
it's inspired a lot of a lot of efforts
in this area so going back and forth
from the real footage to the 3d model
a little dizzy watching this but it's so
cool
so I think this this video really showed
I think people what was possible in
terms of realistic really highly
realistic 3d modeling and photos so
definitely a breakthrough landmark
result appeal so the next images you'll
show yes you can see the real on the
synthetic wood right next to one another
actually the model in that case so here
the input photos of the tower and some
of the photos of the environment of
Berkeley campus and here's the model
that they recovered and you can see that
the model is actually not very
complicated
for example the buildings on the bottom
or just these boxes but when you text
your map it it looks really good alright
so this paper is important for a number
of reasons
so really coin well well introduced it
didn't coin the term image based
modeling I think this came from the
Macmillan's work but it introduced the
concept of image based modeling in the
graphics community and there is you know
some people argue that that this work
didn't actually have a lot of novelty it
was just an impressive system but
actually there there were some really
key innovations so for example if you
dependent texture mapping this is really
the first use of it which which is now
used routinely in Street View for
example and was really the foundation
for the Autopia work here the model
based stereo the idea of recovering
stereo of recovering geometric crew
geometric models but recovering height
fields on top of them these guys came up
with that and published that for the
first time and of course it inspired a
lot of interesting products that can
have come out of it alright so same year
range scan merging also known as V rip
this is this is like it's Martha Kennedy
like elegant results really cool method
for so basically the idea is that you
somehow managed to align your range
scans that now you want to fuse in
a 3d model so how do you create the 3d
model from the align range scans you
want to create one consistent watertight
mesh from all these partial scans and
the key idea here of this work is that
there's basically a one-line solution to
it again
so if you represent not a surface but if
you represent the surface at the level
side of a 3d volume where the surface is
zero and and it basically every point of
volume is a distance to that scan and
you represent every scan as this this
signed distance function in the volume
that if you simply add up all the signed
distance functions one from each scan
and take the level set of that that
gives you the optimal surface and a
least squared sense okay so very elegant
result and and this this basically was
the method of choice it still is the
method of choice for a lot of people
more recently competing methods like
Plus on surface reconstruction have come
out and they're people are starting to
switch to those but for more than ten
years this has been really the standard
our range scan merging method okay
so of course I had to include my own PhD
work here that that's why I have to give
this talk as opposed to someone else so
I can check my own thesis work so 1997
was not just me but foes Roswell came up
with multi-view stereo and he's actually
arguably multi-view stereo dates back
further photogrammetry had some similar
methods for doing multi-view stereo in a
decade previously but in computer vision
these were kind of the main methods
which came down the late 90s for
reconstructing high-quality 3d models
from a set of multiple photos not just a
pair of images and some of the key ideas
in this work both space carving work
that I participated in with Kuro screws
lock us in the level set work of carbon
and and Fodor ah is basically to
reconstruct a 3d shape directly instead
of matching images I'm doing it in three
3d space also visibility modeling it
turns out that you know figuring out
which point is visible in which images
is difficult but if you do it in scene
space the right way you can solve this
problem and and these were the methods
that basically first described how to
solve the visibility problem in a
principled way with provable convergence
properties and so forth because these
are important results in 3d in computer
vision so of course you're all familiar
with graph cuts or at least vision folks
and they're really the first paper that
introduced graph cuts to computer vision
in stereo particular was boy coughs
paper cvpr paper from 1998 and this
basically this is a really cool paper
not as I mean the algorithms are cool
but but more importantly I think that
some lessons learned out of these papers
is really cool so on the left you see
window based matching these are def maps
which correspond to depth of points to
the image so basically on the left is
more left the state-of-the-art algorithm
at the time from 98 although it's not
quite fair there's better algorithms
they could have used but this is a sort
of kind of a simple method from and on
the right is is the graph cut result
which is much higher quality and up
until this point people had thought that
stereo was a really hard problem to get
the right correspondence between a pair
of images in order to really solve this
you had to get better and better ways of
describing of comparing images
describing the region of pixels around
the point of interest that you're trying
to recover correspondence for so by
coming up with better ways of describing
pixels using edges and primitives and
other things like that you can get
better matches now graph cuts work
basically said forget about all that
take the simplest possible way of
comparing images basically taking the
difference of pixel the left image the
pixel in the right image brain-dead but
use a really good optimization algorithm
you couldn't beat all this other stuff
just knock it out of the water okay so
this this work really showed the power
of optimization and ushered in so-called
graph cuts or computer vision
allegations all sorts of different
problems but in particular it has been a
really big development for stereo now it
also had its roots in the 1980s this
work by Baker and Binford who I think
people my generation are all durable
Wilson will say it was a really
important paper in stereo but these days
almost you know I think it's kind of a
foot another board than anything else
but they proposed they were basically
the first to propose global optimization
were stereo but they did it a line at a
time using dynamic programming but for
you know basically 25 years or whatever
yeah 15 years no one was able to figure
out how to extend it this one D
optimization to 2d because the dynamic
programming solution didn't extend graph
cuts was basically at the extension to
to team Michelle mentioned there's some
other work around the same time notably
rowing Cox who came up with different
formulations of graph cuts to do stereo
problems all right
so 1998 mark Pollock fees and his
colleagues published paper to ICCB on
doing 3d reconstruction from basically a
video camera from uncalibrated 3d
information so the idea is you you kind
of fly around in this case or you walk
around the scene with a video camera and
you get out of 3d reconstruction you
know have to know anything at all about
the video camera none of the parameters
no calibration nothing so this is this
is very inspirational work it was really
the you know the the same way that Tomas
and Kennedy work was influential I think
this was very also very influential it's
also the culmination of many research
advances the paper itself was on
actually a relatively narrow topic of
self-calibration finding intrinsic
camera parameters from a sequence of
images it didn't it all talk about the
stereo the texture mapping or anything
but this is one of the key pieces that
enable them to put this whole system
together and also the community envision
communion as a whole have been working
through the 90s on solving this
self-calibration and Bunnell adjustment
problem this is really one of the
combinations of
or that showed you can put it all
together in a really compelling system
so this is a big milestone for the field
alright so 1999 blends it better at
SIGGRAPH published this amazing paper
where they showed from a single photo
you could create a 3d reconstruction
that looks like this so basically the
laser scan quality 3d model of the
person from a single photo so parking
back to the shape from shading results
you know there's no way that Schaefer
J's gonna be able to do this so that
this is pretty incredible now what it's
doing is not using just image
information it's also using a database
of 3d models so what they did was they
they scanned 200 faces 200 people and
they have this basically 200 scan vector
space then through linear combinations
of these scans they can produce any
other person who looks like anything
like any of these 200 people so that was
basically their model so instead of
having to reconstruct the whole face all
they have to do is find 200 coefficients
that are the best combination of these
input people to this output person and
in fact they are able to compress it a
little bit more using principal
components and I think they only
recovered around 100 coefficients anyway
so there's similar work around the same
time active appearance models but it but
I think these are these results are
really unrivaled even now I don't think
there's better results in 3d
reconstruction from faces now if you
look closely you'll notice that you know
the details may not exactly line up
maybe Tom Hanks's knows it's not quite
this long and it sort depends on your
input space and how well the the new
photo lies in the space of input photos
but it gives a very compelling argument
for using so-called model-based
approaches for 3d reconstruction which
you strong priors on the type of
geometry you're trying to reconstruct
so 1999 was also a big year in other
ways the there's digital Michelangelo
project so marked Lavoie group at
Stanford and many other collaborators
and basically they set up to scan a
bunch of statues and other things in
Italy and and the the centerpiece was
Michelangelo's David and this was a huge
scanning effort probably the biggest
scanning effort ever undertaken in terms
of amount of resources and time and
planning it basically took a team of a
dozen I think they're twenty people
involved total working scanning all
night long so they first of all they
they had they had access to the statue
only at night because it's displayed in
the museum I think it's a PC and the
museum is closed at night so they are
allowed to scan all night long and had
to stop in the morning and so that's
basically what they did they scanned all
night long for a month to get this model
and with you know roughly 20 people
involved so huge huge amount of work but
the result is beautiful they have
they've captured the sculpture basically
almost every single nook and cranny at a
quarter millimeter precision and so you
can zoom in on the right you're seeing
zooming into the eye of the statue and
you can zoom in further to this like
Ridge and see the details of the mesh
and so there's also other scanning work
like the pH app project around the same
time but this has really shows you know
what you can do with 3d laser scanning
technology and also how much work it is
to use it okay same year so camera
tracking I was looking for the canonical
reference on bundle adjustment it was
actually really hard to find find a
canonical reference on the
computer-based bundle adjustment and so
I went with this this is this is really
the first time practical bundle
adjustment system
were used in in an application so that
for all these basically for almost every
special effects scene these days there's
a combination of real footage and
computer graphics and doing that right
involves figuring out where the camera
is precisely in the shot so you can
overlay the computer graphics in the
same place and so so this process is
called match move and it requires bundle
adjusting or reconstructing the
positions in all the cameras and all the
images and so the first truly automated
methods were developed in the late 90s
for doing this for actually tracking
tracking the images and doing the 3d
analysis hi and so there's a pioneering
commercial systems a particular
matchmover came out of India arras group
led by luke robear and buzu grew out of
oxford Fitzgibbon and zisserman and so
forth
there's also Concord where I could put a
Graham tree around the same time let's
see so I'm running a little bit low I'm
just going to skip over some of these
there are some important benchmarks for
stereo that came about in 2001 some
really elegant worked for doing stereo
with non-membership surfaces in 2002
2003 is the year I'd like to say stereo
equals laser scan so this is the first
stereo computer vision passive computer
vision method no laser scan that could
produce 3d geometry that looked as good
as a laser scanning and arguably is
almost as good and this was Carlos
Hernandez's work and Carlos as you know
at Google now so very very cool work
which inspired a lot of subsequent work
which I'm just going to skip over
including Josue's work and and others in
the community 2006 first work on
reconstructing 3d information from
photos ohmmeter nexus or photo tourism
work which led to microsoft's Photosynth
and really the key ingredients to make
this work were sift these reliable
feature points and the photograph to the
progress in photogrammetry
over the years and there's similar work
by other authors 2006 this important the
this there's a really cool effort of
right creating 3d models of a city by
just driving a car around and so it's
kind of like the street view lidar but
but just using raw computer vision
measurements instead of lidar this 2008
there's a great paper by is evident at
all bishops group and there they showed
basically an automated version of palta
Bewick's facade so from aerial imagery
it creates this really really impressive
3d model of the city completely
automatically 2009 Samir did his Roman a
day work and this is sort of city scale
structure promotion from image
collections and sorry I don't have more
time to talk about these and of course
2011 is connect so Bonnie clothes from
single depth image 200 frames per second
fastest selling electronics devices
history so they had in the first three
months I think they had double the sales
of the iPad when it came out so not bad
for computer vision system of course it
has other things aside from clear vision
but we like to take credit for it
alright so in 2013 there is this
fantastic work on basically digital
Michelangelo from a few photos so
instead of having to scan all night long
this group was able to just take a few
photos and create a the model of digital
Michelangelo that had the quality of
this Lavoie group that took a month on
2015 there is this wonderful project on
creating a public repository in the
world of geometry or basically they scan
everything people's places and scenes
things and put it all in and publicly
hosted web site 2015 this work on
I'm sure you all remember of how do we
construct yourself from your photo
collections so it took all of your
photos of your poses expression body
shapes over time and create the 3d model
perfect 3d model of you and all the ways
that you move 2020 it took a lot longer
to solve this inverse CAD problem so the
idea is to create when architect would
call a 3d model from not just a point
cloud but accurate 3d models captures
the salient features of the scene walls
floors small number of polygons from raw
point clouds fills in the gaps easy to
edit and modify 2020 we finally saw the
visual Turing test so this is creating a
3d computer vision system when he
creates models which are truly
indistinguishable from reality where
you're able to move however you want
through the scene and it still looks
perfect so it looks just like being
there I'm finally in 2030 the computer
beat human that's it thank you guys
thank you very much for praying together</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>