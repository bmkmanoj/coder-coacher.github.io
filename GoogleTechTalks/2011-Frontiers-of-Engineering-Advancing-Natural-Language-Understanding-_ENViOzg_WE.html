<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2011 Frontiers of Engineering: Advancing Natural Language Understanding | Coder Coacher - Coaching Coders</title><meta content="2011 Frontiers of Engineering: Advancing Natural Language Understanding - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2011 Frontiers of Engineering: Advancing Natural Language Understanding</b></h2><h5 class="post__date">2011-09-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_ENViOzg_WE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">up next we have F Jenny Gabrielle ovitch
from yahoo research he's a senior
research scientist there and he's going
to be talking about using
collaboratively generated content oh gee
thank you for the introduction and I
would also like to start with a
thinking's a symposium organizers for
inviting me to present this dog here a i
would like to devote this talk to
describe some steps in making computers
understand human language we also call
it natural language distinguished from
unnatural languages like computer
programming languages hey I would start
with the realization that we humans as a
huge amount of world knowledge which
allows us to understand text or
understand each other in conversation
computers usually lack such kind of
knowledge and I'll talk about where we
can take this knowledge from and how to
let computers do this kind of knowledge
I'll also talk about a paradigm shift
that happened about ten years ago up
until about ten years ago the main way
to teach computers about the world was
to use small scale carefully crafted
with professionals data sets one example
would be the word net electronic
dictionary which was crafted by team of
lexicographers at Princeton for about 20
years or so and then about around two
thousand one something happened tools
have been developed such as a wiki
approach which allowed essentially
millions or tens of millions of people
around the globe to put their knowledge
online and the most prominent example
would of course be Wikipedia so we'll
just oppose the amount of information
that was available until recently that
is available today and then we'll see
how to distill knowledge of those
resources constructed collaboratively by
multiple people in order to make our
bureaus smarter or make them understand
text I'll also briefly mention some
future research directions whereas we
don't only look at the currents a
snapshot of Wikipedia but we also look
at the behavioral information of all the
editors humans have contributed to
Wikipedia look of the revisions history
every single minute change of
information in Wikipedia and we'll see
how we can get semantic loose from the
way
people right information of it from the
actual document also in process so let's
see what really the fridge
differentiates us from computers now i
understand i am the last speaker for the
day I separate you from dinner so let's
take dinner for example because anyway
everybody thinks about it so when a
computer things about dinner it quality
doesn't really think it says six letters
di double n er that's about it let's see
what humans think about dinner when they
are faced with it and tell us actually
are my kids so their six-month-old
doesn't think much for him dinner is
just milk right there is a single
supporting concept the four-year-old
thinks much more so for him dinner
immediately triggers notions of washing
his hands whereas dishes he can have for
dinner a possibly apple juice and
getting to sleep after that he should
actually also thinking about reading a
book before sleeping a all those
concentrated sugar by the mention of
dinner it is those concept that help us
humans understand each other it is
exactly this kind of world knowledge
that computers do not possess so let's
see how we can teach them to use this
kind of knowledge in order to motivate
the rest of my talk I will take one
particular resource of knowledge one
particular task and i'll show how this
external knowledge becomes industry
indispensable in solving this task so my
example me for the resource would be the
open directory project you can access it
at dmoz org it is arguably the largest
catalogue of URLs or web sites on the
net it has about 1,000,000 categories
and five million urls catalogued in
those categories by about 100,000 of
volunteers around the globe these are
the volunteers the way the director was
constructed was editors in charge of
categories would take a website they
believed to be prominent enough to be
catalogued in the directory and they
would associate it manually with some
nodes of the directory in this case I
took a web site of a mining company and
the editor in charge associated this
site with science technology mining and
business management drilling categories
and is he open city directories
hierarchy call so there is a notion of
science underneath those technology and
money now let's take a particular task
in text processing and my
task will be text categorization our
input is a piece of text and the output
is a set of categories or labels you
would like to assign to this text so
given a new stream say from Reuters you
might want to distinguish between legal
and business and metal particles and
within a medical profession we might
want to distinguish between dental
medicine and a say of technology or we
can do it at final resolution now I am I
told this particular expert excerpt from
a real document from which comes from a
real and often use collection for text
categorization research Reuters to Anna
while 578 which is a number of documents
in this collection every document coming
out of reuters has been catalogued by
professional editors in Reuters that's
how we get gold gold standard data we
can train computer algorithms to predict
those labels in this case we have a
document talking about mergers and
acquisitions between a group of
companies and they're establishing a
joint venture in holland valley and the
reuters editor who is very knowledgeable
about this domain tab this document with
the category copper now as we learn from
the previous talk only showed that the
documents often represented is a vector
space and the dimensions of the space
are usually as actual words so the
paradigm city describes documents in
this way is called the bag of words now
the trouble with the notion of copper is
never mentioned in the document there is
no word saying copper so any classifier
that only looks at the level of the back
of port would fail to recognize the
category of opera correctly however what
we can also do before actually solving
the text categorization problem would
map the document on to the relevant
nodes of the open directory and then the
magic would happen so the mention of
those companies would trigger the
association of the document with the
relevant nodes of the open directory and
those nodes will become additional
features or attributes of properties of
this document the way it works is this
document has been written about in about
1980s come in context have been separate
companies back then they have merged
into a single chemical company but there
is a prominent website describing this
new merged company the mentions of those
words in the text help us associate this
text with external knowledge which is
extremely relevant so the notion of
mining and drilling will become
properties of the document but the same
way will note that highland valley is
actually prominent copper mine in Canada
now when we add all this knowledge into
the bag of course we're with a bag of
course plus concept it is in wrist
feature space we are actually able to
easily understand that the document is
about copper so let's talk about the
sources of knowledge where we can take
this knowledge from again we'll have
before and after and when we think about
external knowledge arguably the first
thing that comes to mind is encyclopedia
in which case the first thing that comes
to mind is encyclopedia britannica it
has been in print from over 200 years
and it has a reputable a respectable
number of 65,000 articles the problem is
it ceases to be a good respectable
number when we look at wikipedia which
only existed for about 10 years but it
has 3.5 million articles in English
salon and about eighteen point four
million articles in over 200 languages
the world and electronic dictionary
which already mentioned has been
developed by a group of lexicographers
in Princeton it has been developed since
1985 has 150,000 entries very nice
except it's dwarfed by the collaborative
regenerated wiktionary dictionary which
has 2.5 million entries again in English
alone algebra the largest contender here
would be site the brainchild of Dublin
odd and his colleagues said sigh Corp a
back in nineteen eighty-four they tried
to manually catalog all human knowledge
and today they have about 4.6 million
assertions catalogued looks nice except
yahoo answers by now has 1,000,000,000
question answer pairs flickr has over
four billion images many of them tagged
with human generated concepts or notions
or attributes this kind of knowledge has
not been available before I would argue
that it gives us a qualitative change
regarding the access of information that
computers can have to we just need to
differentiate between computer readable
information we call all of this is
obviously computer readable it's not
sufficient we need to make this
knowledge computer usable we need to
make this knowledge to let computers
represent the meaning of language and
then reason on top of it
I'll use several sample applications for
the duration of this talk this is my
first application we want to quantify
the relatedness between individual
worlds of pairs of text so simple
problems here would be we have a pair of
words cat and mouse and we can ask how
related they are or you can use longer
phrases we can ask all related Augean
stables and go to tables of the hess
prides an idea how those might be
related no one hmmm there's actually
happen to be labors of Hercules and you
can also ask it in multiple languages
for example in Chinese we can ask how
Mouton Hugh and simet I are related
which happened to be no which happened
to be prominent segments of the Great
Wall of China in all of those cases
there is exactly zero overlap between
the words on the left and words on the
right so the beg of 4th approach will
not help us because there is no overlap
in the bag of words not a single word is
shirt when it access to some more
general information to reason about
those about those words those decks if
we have this technology it would be
tremendously beneficial in a number of
applications so for example information
retrieval the most common example of
witches web search you have a query of
documents you need to judge relatedness
or relevance of those documents to your
query and worse sense disambiguation is
about having polysemous words was having
multiple senses a word like this appears
in the text you need to judge which sent
this world appears in this context we
can also do error correction this way Oh
into my recent hand injury have been
using the patient software a lot and it
frequently misunderstands me with
homophones worth it have completely
different minutes are written
differently but sound similarly I have
to correct those manually but if we use
semantic relatedness technology we can
actually judge the site is much more
relevant to web than this kind of side
so let's see how to use Wikipedia for
judging semantic relatedness of course
we'll start with the words that use it
structure of OT period that is
identities of articles and the plurality
of links between those articles and then
you'll talk about using the entire
content of Wikipedia the entire text of
those 3.5 million articles there are
also some words to try to marriage
structure
and content but as it happens the major
improvement comes from using the content
of Wikipedia heading structure on top of
it adds very very little if at all so
will primarily focus on the structure
alone and then the content alone the
first approach in using Wikipedia for
judging semantic relatedness was
introduced by Steuben pancetta back in
two thousand six again our task is with
two words and we want to judge their
relatedness step number one is to make
those words to relevant wikipedia
articles which is done as a level of
comparison between the work and the
titles of those articles heavens the two
articles we now judge relatedness of we
reduce the problem of computing
relatedness between words to computing
the relatedness between articles when we
are about to associate words to articles
well a single word would correspond to a
single article up to more than one if
each work responds to multiple articles
we can do some very simple
disambiguation or join disambiguation if
there are a pair of articles which have
say common designation such as chairs
will choose those otherwise we'll just
choose the first article or first cents
for each word and then we'll compute the
relatedness now we have two articles we
need to compute the relatedness of
similarity we can either do so at the
level of the words in the two articles
now some words might be shared some
might not be shared so what the authors
proposed is to use the system of
categories in Wikipedia to have some
generalization structure Wikipedia
actually has a very rich system of
categories and each article is supposed
to belong to at least one category many
articles belong to me anymore so we can
use this hierarchical structure to just
relatedness of articles and actually
many approaches have been developed to
do so one approach would be simple age
counting on the shortest path another
approach would judge the information
content of the lowest common ancestor of
the two knows there are multiple a
approaches to do this using the
hierarchy the main a conclusion here is
it using the hierarchies through cliff
helps as opposed to merely using the two
bags of world in the previous work we
only used category links and
subsequently million written in 2008
proposed an approach that uses the
entire richness of links in Wikipedia
now we have articles we have many links
between them even conventional and
secure
p destroy cancer crappy dia britannica
have many links between article
wikipedia takes its much farther because
it's much easier to create article in
HTML like environment and if we have
articles it incoming and outgoing of the
articles we can have two matrix one
based on incoming articles a Sorak
incoming links one based on outgoing
links if we use a knowledge analogy with
a bibliographical citation domain in
terminix correspond to bibliographic
coupling we can say the two articles are
related if they're caught cited by
another article outgoing links
correspond to what they call co-citation
to articles might be related if they
cause I'd another article now this type
this thing needs to be done with care
because it is the identity of the co
cited article that can reveal a lot of
information or a little information as
we shall see in this example so suppose
I have two articles one extremely
general like the article about science
and one extremely specific such as Road
atmospheric thermodynamics now the 52
articles cause I the article about
science gives us very little information
because it's extremely general notion
however if we know the two articles co
site an extremely specific notion such
as that of atmosphere of thermodynamics
we can infer that those are probably
much more tightly related so let's now
see how to use the actual content of
Wikipedia and not only the links it has
been a long-standing dream of artificial
intelligence to use encyclopedia to
empower computers in different kinds of
tasks not necessarily text understanding
hey there is one problem with this
approach in order to understand
encyclopedia we need natural language
understanding now natural language
understanding is hard so we need
encyclopedic knowledge to actually
understand the language in which
encyclopedias written and whether
vicious cycle now here's how we break
this cycle so we want to get rid of
understanding at least at this level
what we propose instead we'll use a
structure and content wikipedia to
develop a new kind of semantics a new
way to represent the meaning of text and
we'll use this representation directly
to understand subsequent texts and to
apply the entire knowledge in Wikipedia
for solving new problems there is one
card
here because we circumvent the actual
language understanding will not be truly
understanding language will be doing
what it is called statistical language
processing will be counting words will
count in concepts and Counting
categories will be doing remarkably well
in a variety of tasks such as judging
semantic relatedness think of web search
but again we are not there yet we cannot
truly understand the full power of
natural language so here is how we
develop a representation of language
meaning using Wikipedia we start with
the realization that every Wikipedia
article represents the concepts own
article about Leo parts represent the
concept of little birds wikipedia could
then be viewed as a huge ontology a huge
collection of concept and those will be
dimensions of meaning for representing
the meaning of text again it has about
3.5 million concepts in English alone
the semantics of a single word would be
a vector of associations of this world
with multiple Wikipedia concepts there
will be some concepts to which this word
is irrelevant and the strength of
association to those is exactly zero but
there will be concepts to which its
strength of association is much more
prominent again we represent the meaning
of a word in the space of Wikipedia
concept those concepts have been
explicitly defined and manipulated by
humans in the first talk tomorrow Sammy
bender will talk about a different
approach to talk about how to actually
teach computers to learn this
representation it will not be based on
concepts that humans defined you'll be
based on a statistical approach to learn
the concept so in an approach we called
explicit semantic analysis explicit
comes from the use of explicitly defined
concepts basically defined by humans we
represented millions of texts in the
space of Wikipedia concepts however the
first step would be for each article we
want to define the concept of this
article as a vector of words and the way
we do it we identify all the non-stop
all the content words is ass article we
compute some weight or some prominence
of the word in this article and each
article is represented as a sequence of
words and weight and now we can do it
for all the concepts in Wikipedia now we
take the dough of view instead of
representing concepts as vectors of
words we represent words as vectors of
concert
so for example the WorldCat appeal here
here and here so all these concepts
contribute to the representation of the
word yet in this case if we have a
representation of a word is a vector of
concepts if you want to judge
relatedness of towards now for two words
we have concepts for the first word
concept for the second world where
vectors in this high dimensional space
again remember the previous talk with a
vector space in order to judge how
similar to vectors are in this space we
can use cosine obviously we'll only when
computing cosine we'll only use the
identical dimensions and we will
essentially ignore all the non identical
dimensions okay so let's see some
experimental results previous approaches
that use the word annette electronic
dictionary or zero justice cesaro's
achieved the performance of about 0.3 to
0.5 and i have to explain you how this
performance is quantified so the main
way to quantify the relatedness of words
would be to take long list of pairs of
words and as human judges to judge how
related each pair is say on a scale 0 to
10 obviously if you want to achieve some
consistency we need to ask more than one
person so we usually say ask dozen
people and then average their scores so
we have a call a powerful list of pairs
of wars we have human judgments with
computer produced judgments and we can
compute correlation between them so
approaches that are based on small scale
resources such as worn it or Rogers the
towers achieved 0.3 to 0.5 course a
correlation with human judgments as we
start using Wikipedia we get higher and
higher numbers so usually categories
alone allows us to go to zero point 49
using all the links in Wikipedia allows
us to go up 2-0 69 if you use the entire
content of Wikipedia the content of the
actual articles and not the links alone
we go to 75 and in the immediately
following slides I'll show you how to go
even further by using temporal
information about how concepts evolved
over time so again we have so far used
the content of Wikipedia in the sense of
looking at what is written in Wikipedia
now let's see how this content concepts
change over say human history
so yeah let's take 150 years worth of
New York Times articles and for example
let's take two notions the work the
notion of peace and war and let's see
how they're correlated over time as you
can see the actual patterns are
obviously different but there is a huge
correlation between those notions so we
can infer from this very graphic the
notions of war and peace are probably
related and just to show you what
several spikes here mean an idea with
this spike between 18 50 and 80 70 s
about this American Civil War this one
whatever one this one world war two and
this is the Vietnam all right any idea
what this peak of blue pic of peace
around 1905 that's actually the Treaty
of Portsmouth it ended the Russian
Japanese word in 1905 so if you look at
those graphs you infer that in the
temporal dimensions those notions are
correlated let's all meant the static
representation with the current content
of Wikipedia with this temporal
dimension so what we have done so far
given a word we represent it as a vector
of Wikipedia concepts now we augment it
with the temporal dimension we compute
how this each concept is changed over a
150 years worth of New York Times
articles so each concept now comes with
the time series given towards whose
relatedness you want to compute with to
search your presentations and we need to
define some measure of distance now we
started with a bag of words and we
looked for identical words is there no
shared words to bed we generalize to
concepts or categories to get some
high-level notion of what similar or
identical might mean now we go another
level up even though the two concepts on
the two sides might be different that
time series might be pretty similar and
it is a correlation between or alignment
between those time series would let us
know how related those concepts are
actually and we can use a plurality of X
approaches such as say a dynamic town
time work in dtw which is common in
speech processing to actually quantify
this similarity so far we talked about
primarily relatedness of individual
words a very short text in which case
representing them in the space of
concepts was just
fine let's talk about processing longer
text I think about web search we have
fairly long documents it actually
doesn't make much sense to get rid of
all those words in the documents because
words are extremely informative in text
categorization which already mentioned
we have a text and we need to classify
it with respect to some labels
subcategories again does make sense to
get rid of all the words so instead of
representing the text is a space of
concepts along we will augment a bag of
words who have a bag of words plus
concepts and let me show you a brief
example so suppose here is my text
Walmart supply chain goes real time I
won't specify the task for now here is
my text let's fetch some external
knowledge to enrich our understanding of
this text so we can obviously tokenize
or break this text in the features and
we'll get the Battle Force essentially
all the words it is text but before we
continue any processing we consult the
future generator we call it official
generator because it generates
additional features of properties of the
text it is powered by Wikipedia but it
could also be powered by any other
language or knowledge resource and it
constructs a collection of relevant
concepts in this case concepts from
Wikipedia so here are some concepts it
fetches a we suddenly knows that cell
wall turn is the founder of wikipedia we
learn that Sierra's target and
albertsons are relevant because there
was a prominent competitors of Walmart
hypermarket is the more general notion
of what Walmart is actually like
rfid-based here because it's clearly
them a very relevant notion out of ideas
a tracking technology that Walmart
pioneer to track it's a supply we won't
know that RFID is related to this
paragraph which clearly talks about
almost supply chain and unless we
consult a huge external repository of
knowledge now we have a bag of world
Plus concept in this augmented
representation we can learn better
classification functions we can learn
how to better retrieve documents because
we have better ways to judge relatedness
a in about two thousand eight a podcast
atau and independently Jorgensen Jana
invented a cross-lingual extension to
explicit semantic analysis so if we know
how to represent the text in the space
of Wikipedia categories he also noted
that wiki
has a lot of links not only with within
the same language wikipedia but also
between different versions of wikipedia
there are multiple lanes between
multiple articles in one language to the
same notion in other languages so if we
train this model and we represent the
text in the space of English concepts we
can use the cross language links to
easily represent the same text in the
Russian language a high-concept some
some links don't exist yet some articles
don't exist yet but I emphasize yet here
because Wikipedia gross as we speak and
if the article doesn't exist today it
might as well be there tomorrow or the
day after so let me briefly summarize my
talk I started with the realization that
exogenous knowledge or external
knowledge it is not explicitly present
in the text is critical for computers to
understand language because we humans
possess kind of language is this kind of
knowledge through our world experience I
then argued that the event of
collaboratively direct knowledge
qualitatively changed the amount of
information we can have access to info
computers this knowledge wasn't in the
first place designed to be used by
computers wikipedia was launched to
allow many people around the globe to
have access to knowledge it just
happened incidentally so that we can use
this information to make computer
smarter in quotes and make them solve
tasks they couldn't solve before this
concept based approach allows us to
address at least partially the two main
problems in natural language processing
nearly sunana me much the ability of the
same notion being expressed by multiple
words if we have two texted talk about
the same notion but use drastically
different words we suddenly are able to
realize that those concepts are related
and we can also address polysemy the
property of words having more than one
sense if we have text in which a policy
melz world appears the word that has
multiple senses where you can use this
technology to figure out which exactly
sense it appears in this political world
so in this talk I only mentioned
Wikipedians open directory there are
many more resources it simply a way to
detect and the future work would be not
solely working with the current snapshot
of those suppositories but also looking
at the entire history of changes so
Wikipedia tracks every single change to
every article however small big it is
even at the level of
single character changed across all week
in media projects which narrow
Wikipedians alike there are over 1
billion changes trip we can look at how
people change information we can
actually gain semantic loose from the
behavioral information of people and the
brief idea here would be if I know that
the particular world has been introduced
early in the document life and it
survives for longer this what is
probably prominent in the document for
the concept described in this document I
can do simple counting in the last
version of this document I can look at
the entire history of things hey I think
I'll stop at this moment L thank you
very much for your attention I'll be
happy to take any questions you might
have question thank you brilliance with
point so in earlier in your presentation
and also in the previous presentation
there were human based correlation
values and scores are given how much of
that work is done in foreign languages
as well and and how much is English
really shaping all the natural language
processing work that's going on the
future because of that hey so with
advent of face the world wide web it's
probably the problem was alleviated to a
certain degree because more content was
available to researchers in multiple
languages definitely a right and there
is a huge amount of work done in English
by the church of availability of
material it's probably much easier to
process language languages such as
Chinese of French or Russian which are
spoken by many people as opposed to say
a much less privileged languages which
are spoken by only small group of people
and some of it can be solved by
reliability information online but
probably not everything maybe it's a
quick follow-up so just with in English
we're going to have certain associations
between words and whatnot but those I
wonder how much we're losing by by
focusing on that just those associations
become of that language as I said can
you please explain hey I wonder what
other associations we might be missing
if we by not using correlation scores
that were generated by non english
speaking people set that make sense and
I'm afraid I'm still missions a point of
your question are talking about
and quantified related in languages
other than English and how much we miss
by not being privileged to have so many
resources yeah yeah I'm afraid it cannot
cite many relevant worse i know a esa
secretion semantic analysis have been
redone for german and they got
comparative numbers of performance
similar to English Germans pretty
commonly spoken language with lot of
material I would Express the numbers to
be lower i just cannot sail by how much
hi I'm John sample from the Naval
Research Lab I have a question about how
the meanings of words change over time
do you have enough training data to do
that kind of analysis to look at words
over like a hundred years or 200 years
to see how they're changing and how you
know the meanings are changing yeah I'm
sure that meanings I changed a I am
afraid they haven't worked pacifically
on this topic I think the corpus of book
that google has and google books which
dates back i think at least 500 years
would be a tremendous asset in judging
those things and there have been some
works is that try to see how word usage
is changed so there was an article
published in Nature a few years ago
which studied how irregular verbs verbs
in English become regular and they try
to correlate between this phenomenon and
the frequency of the words that are more
frequent the verb is the more likely it
is to become regular over time it's
probably the only relevant research
journal to date Hot Lips and cornell
here I have a question about you know
all the information you showed it has to
do with that taking text out of the web
what about audio spoken text video is
probably just as much information in
those channels is that also being mined
in the same way is that as reliable as
written text hey I would assume it has a
lot of information it's probably even
more difficult to mind those because
probably the approach to do so would be
to first transcribe audio to text and
then do text mining but then you suffer
some imperfect pneus because of the
noise inherent in the audio and probably
in the video if you try to transcribe
all the captions of video and I'm not
certain about the current work that
tries to use those through central
repository of the spoken or a video
material to learn about the world I
would assume it's possible I assume it's
interesting to do it on the video
because you can have clues from the
image and correlate them with what is
spoken and again I'm not aware of the
work that try to learn about the world
from those sources said Sir Ian RN and
Colorado State University my question is
on on how you handle bad data because
wikipedia it doesn't have the same level
of rigorous peer evaluation as most
other content does so how do you check
for veracity hey so it's a good question
i would probably dress it in two ways
first able to beat is not as bad as one
good thing there was a study in nature
in nature i think in 1995 comparing the
quality of wikipedia funded it's to be
on par with that of britannica actually
wikipedia has a lot of editors who
oversee the development of individual
articles and then there are many people
who simply monitor the stream of changes
to wikipedia and just see if those
changes make sense or if say vandalism
or an attempt to promote some commercial
entity so the quality is not as bad one
thing that we alleviate this problem in
this setting is it it we take a text we
generate relevant concepts from
Wikipedia those concepts are not
user-facing we do not show them directly
to the user they are used as an
intermediate representation of the
meaning of text to achieve some
additional goal so what we usually do
with first generate relevant concepts
and then we decimate them actually
generating concepts automatical is very
easy we can easily generate millions if
not more relevant concepts it is the
subsequent selection that is in
professional language called feature
selection we decimate those concepts to
only leave the truly relevant ones and
it is this step that allows us to get
rid of a lot of noise only retain
informative concepts and those concepts
to
improve their final pick yourself
whatever task it is a routes arpeggio
MIT so a 12 year old child has much less
knowledge than Wikipedia for sure and
i'd be willing to bet has better
semantic understanding than anything a
computer could do and that's because
knowledge is not understanding and the
reason i think a 12 year old child can
do that is it's got feedback loops
between all four parts of its brain that
are processing images emotions verbal
knowledge world structure all at the
same time and like you put up you know
with the kid in the beginning of your
talk having the idea of dinner it's
those feedback loops between all those
things that are creating all these
associations that give it understanding
even though it has very little knowledge
which is why I'm stein said imaginations
more important than knowledge so I think
you have to simultaneously model image
processing emotional processing verbal
understanding and world structure to get
true meaning and it seems to me like all
this work is all left brained you're
completely ignoring the right analog
feeling emotional imaging graphical
imaginative part of our brain which
actually gives meaning to tying the left
and the right brain together which is
why a kid of 12 years old can do what a
computer khong so why now I'm completely
ignorant of this whole field so maybe
I'm missing something but it seems to me
like this is to brute force there has to
be more insight into how the brain
actually works and processes language to
get understanding not correlations I
think you're right I think the main
reason we are not there yet is because
what is truly difficult and fifty plus
research in natural language processing
didn't afford that the ability to do
what you described I think
humans differ from computers in a very
key property humans can learn from
single examples he'll show a child
single example of what the kid is you
can reliably understand what kids off
from this point computers don't work
this way computers need to be taught
multiple times that's why we need
multiple examples multiple labeled
examples for each task hey I believe the
multiple modalities you mentioned are
obviously very important is very
important to join audio signal and
metallic signal and language signal
we're not we're just not there yet it's
not because the approach I advocate is
the right one it's what we know how to
do today we would love to be where you
described we're not there yet any other
questions hi Madhu iyengar IBM are there
any parallels between your semantics
recognition methods and those spoken
today with other fields like pattern
recognition or feature recognition in
physical parts and and that sort of
thing so there is a definitely one
parallelism is that most of the tasks
you mentioned probably would benefit
from injection of external knowledge
this knowledge can come from bodies of
text or they can come from additional
sources such as observing lots of audio
samples or lots of video samples because
this level I'm sure there is definitely
a lot of commonality multiple techniques
which I mentioned today with the idea of
generating features a we used generation
of features for text there is also work
on generating features for images or
generating features multiple other
domains the idea of post selecting the
automatically generated features feature
selection is also used in multiple
domains especially in machine learning
when you need to retain only the most
informative features with respect to
some labels so there are definitely
commonalities here as well
hi Andy whiner from Purdue so since the
effectiveness of these semantic
classifiers are judged against human
judgments so I'm wondering what
attention is given to how you select the
population of humans for example in the
previous talk we heard that the Booker
Prize Committee may see things
differently than other groups or some
reference to sort of people at the
graduate student level perhaps judging
things different than other population
groups so you know in doing this type of
work what kind of thought is given in to
where the human sample comes from right
so it's definitely a very important
question so when people publish papers
like this they usually say we recruited
such-and-such number of people who
happen to be representatives of general
population or happen to be graduate
students of a particular university or
happen to be people we found in a
library definitely it affect the results
so we need to let the reader know who
the people are one common if not
dominant way to get judgments to date
with Amazon Mechanical Turk amazon has
this nice platform of outsourcing
various human level tasks like you have
a sequence of images and you want to
label them with respect to what is shown
in the image there I don't know exact
numbers but definitely thousands and
probably tens of thousands of people
around the globe many of them in the
u.s.a you can ask those to do the
judgments for you yes there will be
noisy yet it will be suboptimal and yet
though those will be very cheap so it's
definitely a way to give them one way to
get some control of the qualities to
write it to run so called pre
qualification tests you ask those people
some questions the answers to which you
know ahead of time if they party test
with some score you said ahead of time
you let them do such a sequin test this
way you can hope to get a reasonably
representative sample of the population
okay let's thank the speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>