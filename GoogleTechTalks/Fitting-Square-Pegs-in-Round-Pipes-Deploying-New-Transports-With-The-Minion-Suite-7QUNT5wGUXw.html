<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fitting Square Pegs in Round Pipes: Deploying New Transports With The Minion Suite | Coder Coacher - Coaching Coders</title><meta content="Fitting Square Pegs in Round Pipes: Deploying New Transports With The Minion Suite - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fitting Square Pegs in Round Pipes: Deploying New Transports With The Minion Suite</b></h2><h5 class="post__date">2012-11-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7QUNT5wGUXw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you everybody for coming out my
name is Jenna I Inga and I am going to
talk today about how to fit square pegs
in round pipes and i do not have square
pegs with me and I don't have round
pipes but hopefully I get it on how to
do those things today well so this talk
is really about a project called minion
and this is joint work with brine
forward at year and a number of other
folks at yale and franklin Marshall
College which by the way is a small
liberal arts college in Lancaster
Pennsylvania and I hope you can I am
half Amish if you couldn't tell I'm not
even not ha the heart that loves
technology so so this is joint work and
and we start the talk by by going into
what happened a long time ago and and
some of you are actually around when
this happened so a long long time ago
TCB used to be and when some mysterious
was built at least as the internet
workhorse this was a reliable oriented a
reliable ordered connection-oriented
byte stream and and it also had flow
control right was brand new and UDP was
also built as a transport no op
basically it largely was the transport
know of it did d max I'll give you that
but that's what it did applications were
largely happy as well at this time PCB
generally sufficed there were stellar
those FTP and smtp and and you could
basically do with with tcp and UDP for
these things you DB was built for simple
messaging and it served wonderfully well
for simple messaging and there was all
fantastic so that was fine early on but
over the next several moons tcp
continued to mature as you would have
expected it was the internet workhorse
and it continued to mature with new
things getting added condition control
came in hicieron am given and now we
have MP tcp or multiple network
interfaces so all of these things that
keep coming in
a TCP and UDP remained a knob here you
can't do very much with or know of it's
a no op right so haha even with this
change is coming in DCP modern apps
found these services insufficient we
have we heard them audio and video
communication which many of you are
familiar with the problems that they
have a tcp and this multimedia streaming
there was the web which had trouble with
tcp early in the day as well and so we
had all of these these these issues and
there we build new transports in
response the the networking community
went out and said you know we need to do
next generation things for these next
generation applications that are coming
out and we like to build these and so we
started building them and we did we
build a CPP RSC 4960 came out about 2960
came out in two thousand the year 2000
and it gave multi streaming message
boundaries multihoming so he could do
multipath gave partial reliability so he
could do fun things with audio and video
value could cancel transmissions and so
on and it died conditioner told baton in
quite a neat way and then we went out in
build dccb which was basically UDP with
congestion control that was the goal
right and this was supposed to be for
audio video strings for media streams
and so it did all these things and then
we also had some research proposals for
the transport SST partial order POC and
then be anybody here remember or no beep
yeah well I am I am classifying it as a
transport and if you want to argue with
me on that I will but we built be we did
all of these different transport and and
we try to deploy them but the internet
remained loyal they completely loyal
only TCP and UDP go through metal boxes
and we couldn't really do very much with
this right some that is only DCB go
through metal boxes and often only to
put 80 or 443 right these are the only
ones which remains sort of open so to
speak a new transports rarely ever go
through and even now this means the case
that new transpose
get through I see DB and DC CPR still
not supported mostly by mailboxes and
it's almost impossible to build and
deploy a brand new transport on drive on
top of IV and this remains the case for
us right now so how deep does this
loyalty run what are we talking about we
talk about metal boxes so the first
metal box that we talk about is
basically a knot right let me see if I
have this thing with me this is a gnat
and that's I mean the reason I'm doing
that is more than just for comic effect
is to is to drive home the point that
these are incredibly pervasive right
you're talking about middleboxes has
something somewhere in the middle it's
not really it's all over the place it's
ubiquitous box so we have gnats all over
the place and we know what they do we
all are familiar with firewalls these
are essentially boxes that have the
purpose of protecting of security but
also that of enforcing network policy so
if you want to disallow something you
want this allow UDP for example from
getting into your network then you do
that at a firewall so that's what
firewalls are used for now there's
traffic shape is always a familiar with
these as well that control how much
bandwidth is given to what kind of
application and so on and so forth and
then there's the less understood perhaps
box the performance enhancing proxy
which does a whole slew of different
things and thus them so transparently
until it breaks of course when it's no
longer transparent but these boxes are
all over the network all of these boxes
are distributed all through this network
so we have these boxes and then several
more that maybe even I'm not familiar
with and there are a number of these
things that are in then in the network
that are now glued and so to speak are
tied into TCP and UDP and other
transports don't really make the cut so
to speak so what are the applications
doing in the meanwhile right well we
have this sort of net we already said
that are these transports and then there
were there was a need but then when
build new transport those didn't get
deployment because metal boxes didn't
were loyal to TCP so to speak so what
our application is doing in the
meanwhile well no prizes for guessing
this one they basically build their own
abstractions on top of UDP and TCP we do
this on a routine basis we do the saw
routinely that we even ask the question
why would you do anything else if I
would he go out and build a new
transport but this is what we do we
build everything on top of this we use
multiple TCP connections from work what
really ought to have been multi
streaming when you use multiple
connections between your browser and the
server what you are looking for is
parallelism not multiple connections we
do that using multiple connections
because there is the only mechanism we
have that's the only tool we have that
works and we do other stuff on top of
you d be in cases where ECB is too much
of a constraint so that is what we do
this has of course its own issues right
are starting on UDP eventually leads to
something that looks a little bit more
and more everyday like tcp on top of you
d be it's really fascinating we see
these iterations of applications that
sort of a new DB going obviously tcp is
not my answer i'm going to build
everything on UDP and then they start
building in this one of the first things
that they realize is i do want to try
and do retransmissions if i can they
won't try and get as much of liability
as i can not perfect but at least some
and then slowly does the flow control
comes in because the receivers running
out of buffer so that you need push back
okay you're dining the clock back 30
years right these are questions that are
answered and then eventually some sort
of rate control sets right in because
they you don't want the sender to send
much more than the network can manage
simply because you can do more
fine-tuning at the sender if you know
what the rate allowable raid on the
network is and so some sort of
congestion control creeps in so you
basically start getting closer and
closer to DCP and that happens but
unfortunately because of these middle
boxes in the network still see this at a
UDP there's poor interaction between
those two things and one example is
there is no session state yet
middleboxes so an ad box for instance on
top of which Iran through which you're
running a UDP
through which you're running UDP traffic
doesn't understand an upper-level
session right so it's gonna drop NAT
state at some point that's going to be
much sooner than it would drop a TCP
connection state because the TCP is
looking for the fin or a reset or
something to drop that state so so
basically UDP at second over UDP can
interact or UDP service model and when I
say you to be service model it's not
just at the endpoints it's also in the
network yeah also in little boxes in the
network so outside on top of TCP has its
effect then we know these ones quite
well right as i added buffering and a
latency and this can be poor
interactions between an application
tuning its raid and TCPS congestion
control and all of that fun stuff right
so again we put interactions here as
well so these are necessarily really
good substrates these were not built to
be very good substrates and certainly
the network didn't see them as
substrates it just looks at them as
transport so what have we done so far
what are we as a networking community
done to address this issue because we
have this problem we have networks that
are not moving and they have
applications that have needs so what
have you done so far well we started off
by saying that's our email there was a
first response right he said gnats are
evil we're not going to care about and
then we said it'll all change with ipv6
so we know how that's gone but I want I
won't say that I wouldn't say we know
how that's gone the answer to this
question isn't necessarily ipv6 is what
i want to say so and then he said don't
design a non metal boxes it'll only
encourage them so we won't we will sort
of let them be there but you're going to
design with them in mind and then he
said made me accept them but will
specify how they ought to behave because
we want to be able to control because we
have to interact with these things
there's no choice left here then finally
we gave up the why build a new transport
some of the work we'll just build
everything on top of TCP and UDP right
so I start off with denial we moved on
to anger we went down to bargaining and
then we ended up in depression and if
you
I'm going with this the final stage is
acceptance so I think it's about time
that we came to terms with the fact that
middle boxes are in fact here to stay
right and we should stop grieving it's
time to accept this and move along and
see what we can do with this so here are
a couple of here's one big new design
assumption for end-to-end services that
we need to make which is that new
end-to-end services cannot require
changes to middleboxes to get deployed
so if I am building a new end-to-end
service I can't wait for metal boxes to
do something before I see deployment
that's not really going to happen a
consequence of this is that new
end-to-end services must appear as
legacy protocols on the wire so there
anything that we built that's new should
get through the network as it is that's
a consequence of this assumption and I'm
going to run with this because this is
what I think we need to counter terms
with so this is I mean it's not a it's
not an accident that people had
difficulty or people still have met
folks in the network media still have
trouble coming to terms with metal boxes
it's because we didn't put them in there
in the first place we have a network we
built a network that was supposed to be
open and there are these boxes that are
sort of come into play in the network
and are starting to control in some ways
what I would call the architecture of
the Internet these are what I am calling
now accidental control points right mean
these these mailboxes have started to
control the architecture but they're
doing so accidentally so to give you a
sense for what the networking community
came out the network community came at
this it's instructive to go back to this
RFC if you have not seen this before
it's the nat RFC RC 1641 from 1994 and i
will read this loud because i think this
is worth it the two most compelling
problems facing the IP internet or IP
address depletion and scaling and
routing under the long-term solutions
are ready an easy way to hold down the
demand for IP addresses is through
address reuse and this was the the time
the
crashing issue was at this d be expected
running out of addresses and nag was a
mechanism to to allow us to extend that
time frame right the fun part of this
comes in the conclusion of this document
NAT has several negative characteristics
that may make it inappropriate as a long
term solution and get this may make it
inappropriate even as a short term
solution they knew this they knew that
the source of problem and we all knew
that the sauce and the sunning thing is
that when you look at the document in
the conclusions a list of things that
can go too bad with Max that can go
wrong when you have Nats in the network
and the last item there is things will
break if T or something like that ftps
and NP coma you name it it basically say
is that in the RFC of or not right i
mean so we knew this and get we went
with this because it was a pressing
issue and sometimes it's not just the
issue that drives deployment it's a
number of other organic factors that we
can't control anymore and it's something
that we need to sort of come to terms
with so our idea is to say well this is
the state of affairs as it is we've
tried we've had Nats we put in Nats in
90 early in the mid 90s as a stopgap
solution knowing that maybe it wasn't
good even for that but we still have
this is just max right this is just nuts
number of boxes the network didn't even
come up by design they just came up
organically so we came up with this idea
and I'm going to describe it now this is
the minions feed which is basically as
we call it a packet packhorse for
deploying new transports and i will
split this I'll break this on the moment
what we basically try to do in minion is
we use legacy protocols like TCP TLS but
we use them as a substrate use them as a
basis on which we can build new services
so that we can build new services their
applications want so we are trying to do
the same thing or not the same thing
you're trying to use existing transport
protocols as a basis on which we can
build a
new services applications want when I
say protocols what I am talking about
here is the wire format and when I say
the wire format what I mean is not just
the headers but also the state machine
because there are machines they are
devices in the network which track the
state machine of a TCP connection for
example in for instance there are
intrusion detection modules or boxes are
actually track the state machine and
look to see whether retransmission is
the same as a transmission and so on and
so forth so that's pretty crazy but it's
there and so we when I say they use
legacy protocol we constrain ourselves
so using the wire format and the state
machine but we try to do change
everything else so that's our overall
minion sweet thing and so main goals
here are to see how far can we stretch
the TCB wire format right so the
constraint here is that we have devices
in the network which are stuck to tied
down to the wire format of TCP protocol
they expect to see tcp on the wire so
say well let's give them DCP on the wire
and let's see what else we can do at the
ends with the tcby format right if you
can open it up at the ends we can make
at least we want try and see how far we
can open it up at the end points so
that's of that is our main goal and our
some goal also say well DCP is becoming
increasingly a substrate it is something
that's being user substance just make it
a good substrate right and the big one
here was to eliminate trying to
eliminate latency in the in the protocol
itself so I'm the protocol but in common
protocol implementation so at least the
way people see the TCP service model
that's what we want to go after so a big
constraint for us was to ensure
incremental deployability we wanted to
try and keep incremental deployability
as a goal because again we are trying to
get over the problem of not being able
to deploy new transport services it
wouldn't help if we came up with a
solution that also had trouble getting
deployed significant trouble anyways so
so our constraint was this was to try
and get incremented to probability and
and so do to to explain mini in which I
will in a moment in
a snapshot we had a paper at nst I this
here this at the NSD I conference and
one of the anonymous reviewers said a
couple of things which fit quite nicely
so I have them here it said it's a
reasonably performing solution to what
is sadly a real practical problem and
they also said that minion accept the
new narrow waist as TCB and shows that
with enough this Isle of devious thought
one can manage to implement
functionality at cross-purposes the tcp
on top of it so we sort of tried to do
that so I mean I what I want to say is
that we try to do this well here's
here's something that's interesting this
was on of this was on the review this
was a strength that was a weakness I let
you figure out where they should fall
may I buy that I understand why that's a
weakness but that is that actually
pretty precisely what we do do so here
is the rest of the talk I am going to
start with an overview of minion I'm
going to talk about you DCP and you cobs
which basically give us unordered
delivery in tcp and make a Datagram
service look like a TCP string on the
wild that's a goal if you can achieve
that you can build stuff on top of the
Datagram service and then we do the same
thing but we do it this time with ssl so
that you actually get data grams that
are completely indistinguishable from
the way HTTPS right Emily good an order
Datagram service so you can build stuff
on top of it so and then talk about the
our implementation and impact on on on
real applications real applications in
quotes because we well we built some of
them but he also basically emulated
these applications usually a preso some
real data though i'll talk about the
connected there yes yeah so that's
something that's commonly done right
that's very commonly done the problem is
that middleboxes do not necessarily
understand what it is that a tunneling
through UDP so we have a metal box and
tunneling a session through UDP you
still have to build in code to make sure
that when that middleboxes do not say
drop state
leon or they do not assume that this is
a UD be random mean this there's
interactions between an application that
is trying to maintain session state and
a middle box that doesn't understand
that there is such a thing for UDP
that's the one thing the second answer
is that is there are boxes in the
network like performance-enhancing
proxies are actually do things with TCP
congestion control to make it better
over network links and they can do
nothing when you are tunneling
everything through UDP they can't even
participate so beach I mean we don't I
don't want to come off wrong in this I
do not want to treat metal boxes as an
adversary I think they are doing
something in the network for operators
which is valuable for operators and
that's why they are going to be there
our goal is to try and work with them
right so we're not trying to tunnel
through middleboxes we are trying to
work with water it is that they do so I
I want to clarify that as well sometimes
it difficult or more difficult right to
find the middle boxes which would but
let me let me let me get to the dog it
because this is actually quite
interesting the incremental
deployability thing I'll we can talk
about later as well I think it's really
it's all the cool things about minion is
that he get incremental deployability
here which is which I which I like and I
think the incentives are aligned well
and if I will try to get to that later
as well yes
I agree so that's actually a very good
reason why you want to retain a lot of
TCP so that is precisely right i think
this item if everybody can hear what you
said but should i try to well i will
repeat what you said which is that a lot
of engineering that goes into tweaking
and making tcp stacks work well and
having have used reusing that code
reusing those tax is valuable instead of
rebuilding your own repeatedly I'm and
I'm obviously paraphrasing here but that
was the gist of it so I think that's a
very valuable point as well and we try
to do that as much as we can in menu it
is one of our goals what's that do so
what is in the minion sweet the minion
suite has a few components and I'll
throw them all up together in one shot I
will go through this in pieces so the
first part just looked below the red
line and if I wasn't half Amish this
would be animated see there's my
evidence is it ok to take potshots like
that so under the red line there's
there's the colonel that's a kernel
space right and we have PCP in here
Europe implementations and other
Transfer Protocol and limitations which
live in the kernel typically we extend
TCP here and we extend it to be
something called you tcp I'll get to
that in a moment but that's an optional
mini next extension to DCP above the
OSAP I above the socket CPI we have a
user space library that sort of sits
right on top of this stack and does a
bunch of bit managing between what comes
out of here and what the application
expects to see yes no no you should not
read your diagram like that it with this
whole box you TLS in you cops will work
with either of these yes right and if
you want UDP I mean we haven't really so
we can be we can throw in shims here
that allow you to work natively with
protocols that you want to use the point
here being that an application does not
care what protocol you like you use
here below if anybody is familiar with
beep those are all things I loved about
beep was that you don't have to choose
the application does not have to say I
want I CTB I want TCB that seems like
generally a bad idea because an
application does not especially in this
world but not everything may get through
the network we need a part we need a
piece here that basically says this is
going to work use this you want these
services i will try to give them to you
right as best as we can try and match
those things but the goal here is for
the minyan protocols we to try and use
one of these transports where the
application gets a service right there
we have an unordered a diagram delivery
service that appears right here on top
of which you can build things like multi
streaming and other things that I will
talk about later in the talk so that's
what we want to be able to do okay so
I'll start here with you d cv i am going
to talk mostly in this talk about you
cobs and you tcp i will briefly skim
over you TLS and i'll point you to
papers which have detailed descriptions
of these pieces but i am going to talk
about you curves in new DCP for the
resource talk so solid you DCP which
basically stands for unordered DCP and
give a surprise that nobody had taken
this you DCP acronym i guess nobody
thought about are not a DCP before
that's why entirely true it's not
actually others half as well but you DCP
was not taken so we took it so what is
your DC be well we introduce two new
socket options in Linux and that should
tell you what you TCP does the first one
is SO unordered receive right so what it
does is basically the colonel delivers
all incoming data excuse me immediately
up the socket so if there's data coming
in it's out of order doesn't matter
everything just goes up into user space
okay up through the socket and it also
does a DCP sequence number with data
will get into this well show you
something that describes this in more
detail and then we have I saw an ordered
send which basically accepts a priority
with every application message when
application sends on a message to the
socket also specifies a priority and the
kernel code UDC be in the curl uses to
put that in a priority queue okay nor in
a fee for socket buffer in a priority
queue at the socket door
so we should be thinking what sucks
cream doing with this or in order to
receive s on order socks 3-1 ordered
don't really fit but that's what we try
to do here so let's look at how to
receive I talk about that here and we I
won't talk about it unordered send but
i'll show you results with it so what's
on all ready to see well here's what
happens in normal tcp you have data
coming in in order let's say that's 101
and that's an order it's right past the
queue Mac point so the PS get skewed and
the applications blocked on a read so
application gets that data immediately
delivered because it's in order data
appears that's not in order there is a
gap data got dropped perhaps in the
network and the application is propped
on the read this data comes in but get
skewed waiting for the application
waiting for the retransmission of this
day other of the missing data to come
through which eventually does and then
everything is given up given sent up to
the application there's a period in
which obviously this data that's blocked
in the colonel so with you DCP things
proceed as normal when you have data
coming in in order with one modification
we also send up the sequence number okay
and you need that to make sense of this
in the library above so when data comes
out of order it goes into the out of
order queue but also gets delivered
right up to the application okay just
gets delivered right up and then when
data fills the hole that gets delivered
as well so the big difference here was
the 301 right was with the out of our
data that just gets delivered
immediately but we are also delivering
the sequence number so again going back
to this question of what a sock screen
doing it this one ordered well it's
right so that's actually its should get
delivered in this case it does it does
allow implementation it does get
delivered and there is no queue 00 it
gets delivered here in order so you do
not need to queue it you're right we for
purposes of not changing the code too
much
leave it there and we deal with
duplicates up in user space so that can
be changed though I mean that's a
question of whether you can make it work
or not so what you need to do your
duplicates anyways in user space doesn't
matter where they're going to deliver
this or not so ehsan ordered and so
extreme what are we going to do with
unordered bytes unordered bytes doesn't
make sense right how do you know what's
order in what's unordered well as it
turns out we can't we have basically
these mmm this is odd okay there's no
inherent structure in a byte string I
mean the only structures that it's a
layer byte stream it's a stream of bytes
right I mean that's the one structure
that is in it there is in it little
boxes can be sigma t CP segments or TCP
segments doesn't hold structure per se
it doesn't hold meaning across the
network and so you need a message
framing mechanism which allows us to
detect messages in arbitrary bite
fragments because the receiver is
basically going to get these random pipe
fragments are not random but arbitrary
in sequence means it could be after a
big gap and there's a bunch of bytes
that have appeared for the receiver to
actually make sense of this data you
need to have some structure in there
some way of framing some framing that
you can detect that there's a message in
there and that's something that we need
to make sense of this out of order of
this soo unordered in sauk stream and we
do that with self delimited framing
basically what we are looking for here
is a framing that identifies itself a
framing mechanism that can say all you
need to see is this frame and you know
that's a complete frame you don't need
any bites before it no bites after it
and Cobbs which is an encoding mechanism
it's called consistent it sounds for
consistent overhead byte stuffing which
was built for a completely different
context it becomes very useful here for
us so we use cobs for this and I talked
about this briefly basically what we do
is we take an application message at the
send side we encode up with cops and
before be encoded with cops we take the
application message which slab two zeros
at the two ends
okay two zeros because what we wanted
the receiver is when the receiver
receives bite fry up a fragment of a
byte stream it finds 10 finds 120 and
goes huh that is my message so now it
doesn't depend on any other bites but
there's a problem here right now you
need to escape all the zeros inside the
data and there is a problem with this
which is that you can do several
different end coatings we choose to go
with cops because it's really efficient
so Cobb's allows us to eliminate zeros
and thus original data and in and it
also allows us to have a guaranteed
maximum bit overhead of point four
percent which is about six bytes for an
ethernet frame it's pretty good which is
really good in fact it's something that
we can actually we can actually leave
space for when you're taking application
messages in right you need this six
bytes of space and you can encode and
add increase up to expand up to six
fights and you can send this off so you
can use other free other encoding
mechanism here Cobbs happens to be a
good one that we use it remains the same
it doesn't change that's a that's a
maximum bit overhead and you need to
understand the mechanism is actually
fairly straightforward no it's a maximum
guarantee so that's it is precisely all
respects me what's it yes so I meant to
mention that this is actually Stuart
Cheshire's PhD thesis from Stanford from
98 I think 97 or 98 some here like that
it was built for different context but
it's a fantastic idea that we are able
to use here in this context so that is
what we do for for this and I can go
into cobs later at the end of the talk I
think it's a pretty cool mechanism it
can be extended in interesting ways to
we can get to that later so so now what
we have is the ability to pick up frames
from arbitrary bite chunks at the
receiver as long as I send the cops and
codes and sends them in right and so
this is how we use many an application
that's using minion basically a UA you
cop sender takes a message cops and
codes with slabs zeros and
sends it down with a priority that
application specifies okay and sends it
using your DC be at the receiver these
white blobs come out and a receiver the
user space or you Cubs receiver can
decode these frames or application
messages and deliver them even if
there's a loss so what we have done here
is effectively sitting on top of TCP we
basically deliver everything we can
right and we can deliver frames or you
know if you want to think of otherwise
datagrams that's fine too except that
they are still reliable in this case and
we can talk about that later I'd love to
talk about a liability that's the
question I am looking into now so that's
what we do at the center at the receiver
we can do very similar thing with you
TLS the cool thing here is that UTS has
its own framing mechanism we don't even
need to use you cops for this or
anything else you TLS uses the ssl
framing mechanism and we use that we can
actually work with that so you do is
protect center and signaling and data
and it looks like SSL or TLS on the wire
right but we are able to provide Otto
Graham out of order Datagram service
with this so how do you do out of order
decoding I'll talk about that in just a
moment so this basically allows us to
make it so that your stream is
completely indistinguishable and again
this is not an adversarial position when
I say indistinguishable the idea is that
they don't need to necessarily do
anything for this to get through right
so the only the encrypted content gets
affected so the main challenge is here
as the recession was built for order
delivery using TCP it assumes things
about sequential delivery at the
receiver for decoding and decrypting so
these records these TLS records are not
encoded for outer for a decoding so the
receiver you have to overcome a couple
of a few changes three in well so there
are these these particular challenges
where you have to basically these
surface streets are chained across the
encryption say it is chained across
these these TLS records and you have a
the simplicity cod counter that are
that's in the mac inside of these frames
and when you don't have order if you
receive an arbitrary frame at the
receiver you have to do something to
figure out that Mac is and basically
what we do is we guess we guess max and
we scrub of you and we can figure the
certain so very quickly what we do is we
try to decrypt the frame using a
particular Mac we are using a particular
sequence number and if it doesn't we can
check to see if the hash works out at
the end and if it doesn't work out we
just increase the record number and you
the sequence number in the integral
decrypt again so you can do this with
not very much in our experiments at
least the the cost of doing this
iterating through these sequence numbers
is not that high so look for the frame
we look for the frame header if it's a
false positive again the hash shouldn't
work out right there's a cryptographic
guarantee there so that's what you get
for you dls and I can get into this as
much in more detail about exactly what
we do here later I want to get through a
few more slides first so i won't talk
about the implementation next we're
gonna stop and quickly check the pulse
of the room there's any questions
anything that you want to get there
early in the evaluation that's
definitely something we are going to do
now ok so the implementation just to get
your sense well well I will give you
some numbers so in the linux we worked
with the Linux to 6 30 do colonel and we
are these soccer options we end up
modifying 565 lines of code in the linux
kernel to do this out of order delivery
basically what we did was that I don't
know whether data structures now but the
on the receive side when data went to
the order for a cube we also sent it up
to the receiver that's what we did right
so that it was easier for us to not have
to nod NQ it and have to deal with
whatever complications that caused us
later but of course in a clean
implementation you might want to do that
no way did not even touch that so this
is purely in the upper space of TCP if
you want to divide DCP into two parts no
it was it's a strict priority queue so
we were basically VI the inserted data
we came across a very interesting
problem and so normal actually if you
want I can talk about the implementation
we see how much time do I have left 20
minutes and I should probably save time
for ya ok i can let me talk about this
now this is fun stuff so ah the colonel
implementation right so on the send side
what we simply do is we take an
application right and we normally what
linux does is it tries to pack it into
the escape of the sk buffs are basically
NTU sighs I'll exercised it tries to
pack it with him nsk buff and the sk bus
basically become packets at leave the
wire what we did do what what what we
did was we took the message and we said
this message is an entire Escobar and we
going to delimit the message by saying
this is where in a skip of a boundary
will appear and we added some metadata
to the SK buff so that it could tell us
what the priority priority was for each
Escobar so when he came in game when a
new right came down and became in a ski
buff we would insert it in the right
place in the escape of cue the real
essence one of the problems is that the
Linux does congestion control in sk
buffs this is fun stuff so when you have
small messages coming through suddenly
your tcp throughput is down on the floor
right it was like totally because you
have because the next congestion control
machine assumes that one escape of maps
21 m SS and when you have small
application message rights that are
coming down that's not the case in our
implement now modification so I think
the right solution to the head rested
change the Linux implementation to bite
condition control and
anybody who is going to watch this video
later or is now listening to it and
wants to do this go for it is a good
idea yes that's correct but then but
then they don't all the problem is that
you so let's let's think about like this
right when you have a serial when you
have a FIFO queue and data is coming
down from the application even if it's
small rice they all get coalesced
back-to-back-to-back-to-back but if i
send out five package with five
different priorities they're not going
to eat koala sitting because one of them
comes up let's say let's say the
application says he has a message
priorities five where five is low
priority and one is high right so far he
comes into the queue then down comes for
goes ahead of this and get stuck up here
and then three get stuck in front of it
so yes you can try and call us within
priority so if I have a number of
application rights that are coming down
that are the same priority you can call
us them and we do that but when there
are different priorities you cannot call
us them
oh ya know exactly that's right it
happens way up and did stuff gets queued
in the send buffer yes it will be lovely
if the coalescing happened later yeah so
so yeah so we have to i mean that's
that's something that can be fixed it's
not a fundamental problem it can
definitely be fixed it's just something
we encountered so i wanted to share that
because you're asking about the
implementation so that's what we we
modify this here and the user space
library has it turns out is basically
San hundred thirty two lines of code for
you curbs and that can be reduced but or
at least it can be made tighter and then
we modified 586 lines of code in ut LS
in openssl to implement you TLS there is
no a lot of you know you have to see
there is not a lot of code and here is
what I want you to compare this to
compare this to a brand new transport
protocol okay and that's the comparison
you ought to be making because you sort
of going there yes yeah I I'll check the
numbers in the paper again but I'm
pretty sure the kernel code the DCP so
sorry yes yes so I'm right yeah so it's
right i believe so am i i'm not going to
so i have to go back to the paper to see
what exactly but yeah it has to be part
of the because we compared against other
transfer protocols ultimately so it
doesn't necessarily make sense at a gate
a kernel code but only that easy p code
that yes I should know that okay so i
want to show you a few results excuse me
one of them is just to show you that
minion actually or UTC be does what it
does what I say it should know that the
implementation does what I said it
should do and what would we expect to
see is something that you should think
about let me see where we want data do
not be held back when there is a loss
right and
that is what we see so on the y-axis
here using app message sequence number
at the receiver we have these app
messages which are basically tlv encoded
frames just for this experiment and we
have just sort of time on the x-axis
here okay these values you do not matter
this is a functionality graph so they
are seeing here with the green is TCP
behavior when there is a loss there's
this long pause and then this is bursty
delivery that happens with the receiver
as you can see here for example right
here is a pause and all of these X's
throw you tell you when data packet
delivery happens or message delivery
happens up to the application and the do
TCP you can see this really nice sort of
the slope is maintained and then data
gets retransmitted and is delivered and
then we continue on with data that we
hadn't received before right you will
see these dips with UTC be in sequence
number because older data just came
through now with PCB you will never see
that you'll never see a downward dippin
sequence number right because it's
always in order delivery so so it seems
to be working and this is to show you
what the sense I'd can achieve so this
is basically with application message
priorities at the sense ID where we are
using a priority queue in kernel and the
experiment here is the completely
artificial experiments just to show
functionality but it's very instructive
but the application here sends every
hundredth message with a higher priority
than the rest of the 99 messages so 99
messages with low priority there's just
two priorities in this queue and then
100 message is high priority okay and
the network here has the 16 millisecond
count of time and has point five percent
loss which is which is what we used here
for this experiment and here is what i
want you to see in this messy sort of
crowded graph we try to do the same
thing low priority and high priority the
application this is an application that
is using either tcp or UDP it does not
matter which one right it's Cobbs
encoding and underneath it can be TCP or
UDP we compared the two basically with
TCP what you would expect to see is that
high and low
priority does not mean anything because
TCB does not respect those things right
it does not mean anything for tcp / say
everything is free for you send out a
message it gets to the back of the queue
and that's all you can expect so the TCP
both low and high priority traffic which
in this case are the other red and green
curves basically run with each other and
with the UDC p case you would expect the
low priority traffic which is the bulk
of the reader to sort of run with them
as well which it does but what you get
with you d CBS is really nice low
latency curve for the high priority
traffic and this is partially expected
but mostly took me by surprise it took
me by surprise it was so low it was much
lower than the rest of it and one of the
reasons why it actually ends up being so
low is that this works beautifully with
PCP and explain this what I am trying to
say here is that PCP is an engine that
works well when there is a lot of data
in it right so the low priority traffic
is sort of keeping this engine running
when high priority traffic comes in it
hits the head of the send queue and
there's probably an ACK just coming in
and immediately leaves and reaches the
receiver not blogged anywhere at the
receiver gets delivered up immediately
so you get this really nice effect and
if it's lost in the network this is the
best part if it gets dropped in the
network the subsequent low priority data
will allow TCP to recover that with a
fast retransmission so this basically
allows us to send higher priority data
not only in the way that it bypasses
buffers that also bypasses the sense I'd
buffer but also in the way that the low
priority traffic actually helps to get
across the other side and I think that's
that was something that I hadn't thought
about but that came through yes
MDM one packet in this case yes it is
the case i think but we have some
numbers in the paper that try four
different message sizes the results I
mean these these numbers do not matter i
think what matters here is the principle
or at least what mechanisms kick in to
help what and if I it's actually
difficult to run a TCP connection with
sparse data right I mean it's a
difficult thing because if data gets
lost in the network gets dropped a
minute of that TCP connection single TCP
connection by itself will have
difficulty recovering from that loss
quickly but by combining it with a high
load a TCP connection but allowing that
priority to be maintained gives us this
nice low curve yeah so so that's so
again I mean I should have said this
earlier that lower is better in this
case we are measuring latency here at
measure end-to-end latency at the
application not in the stack not on the
network at the application yes there so
that's an interesting question it could
be either well no I would see in this
case in general I expect that it would
be a loss event of a high priority
packet that's what I would expect it to
be not that of a low priority because
even if a low priority in this network
your windows not going to be too small
the condition window for the sender's
not gonna be too small which means
there's only all you need is for the
congestion window to have space for one
new data packet to go out in this
experiment right here yeah it's it's
definitely order well it could be a
timeout I don't want to speculate at
this point I actually have to look at
the data teams yeah Matt what's a
yes yeah so I mean you probably will see
more timeouts here than you that's what
I'm saying yes you can probably got time
off from that you could have gotten
timeouts from that yes the blue line you
mean yeah yeah yeah ah ok I see so there
are two answers to that I don't know
that that's so this does not affect
that's playing in this experiment which
is the condition control effect that I
talked about when these messages are
going in that this varies overall
throughput here was a lower we fixed
that later but then this was from an
earlier iteration of the implementation
these are different experiments so I
don't know that and this is just a you
know a time slice from two different
experiments with no statistical
importance at this point X that we see
these trends so it's a little hard for
me to talk about it that way all right
just a couple more results but before we
get into the results you said why
somebody said what applications do care
about this right so the first thing I'd
say is that you get some instant kind of
a building mini the instant Karma's I
can work immediately with interactive
streaming with videoconferencing with a
tabbed browsing you can get parallel
HTTP requests i'll talk about that in a
moment i'll show you some results from
there and this is a really cool one
which is that you can use minion tunnels
instead of ssl standard tcp ssl tunnels
because it closer replicates what the
network looks like then a tcp channel
does we have again results in the paper
i won't talk about that here but these
are all instant applications that can
use a million right away because they
are well tuned for that medium-term
Karma's Arminian service has become
available at design time so forbidding
any application you should
think about this you can actually use
minion services to build new
applications from the ground up instead
of trying to port existing applications
to use minion and it does always of
course does the reincarnate of karma
which you know happens if you believe in
it where you get this next-generation
transport abstraction on top of which
new transpose get built right so
reliable good
right so my understanding is similar
that video in particular is a expect is
increasingly is needs reliability audie
on the other hand can still deal with
losses that's my understanding because
they aren't really doing a lot more
compression and there yet those are
trying to get that little bit of data
across right right that's I think that's
yeah yes okay so so I'll show you a
couple of real applications one of them
is voice over IP so we basically know
that these are a delay sensitive
applications were long rounder time
delays for recovery of losses is
actually perceptible can be perceptible
for long RTD networks and it can
frustrate users quite a bit when that
happens what could extend to be highly
sensitive to burst losses and and
generally can't interpolate when many
packets are lost but they tend to be
okay with singular losses one or two
losses that they can interpolate over so
we built this into applications into a
VoIP application we used real voice file
to and and and we figured out we made it
run over TCP as well as UDP and over
minion and as I set up what you are
seeing here is basically burst length
because that's what matters in many of
these cases burst length from the
applications point of view of losses and
the fraction of frames that were lost on
all and UDP is very uptight to the left
so left is good right we are smaller
your burst length smaller than every
loss incident if it's smaller than burst
size the better the codec works so more
to the left is better more to the left
hand top is better so you will be it
does the best in terms of burst length
because you don't get typically applied
in this case at least you don't get a
burst losses huge burst losses peezy B
does quite worried and the reason it
does police because
not because these many births packets
are lost in network it's because those
they don't get delivered to the
application by play out time they're all
queued in the receive buffer waiting for
a loss recovery to happen and when they
get delivered they're all wasted or they
are mostly all useless and million gets
over that hurdle right Minnie and
basically delivers everything up so
codec can interpolate over those
individual losses and can use the rest
of the data that otherwise tcp would
have held in the kernel the question our
half year is yes you will get out why
not be able to do anything right so in a
case where you need reliable order
delivery minion is not the right answer
I mean TCB still your best answer at
that point when the case where you can
actually use on order data and do stuff
with it like in this case we did it's
useful that's what I would say I mean
women is not the answer to all questions
right that's what saying it's the big
easing deliberately lossing that's sort
of right yes yes I mean me that the
point is that here even of the Rita the
retransmission can be received within
the playout buffer there is still useful
that's where actually minion wins over
UDP is that we get a hundred percent
delivery some of it may not be used
because the delivery still same as TCPS
right yes sure it's not yes
so I want to show you one more result
I'm running out of time and I don't want
to keep this result from you because I
think this is a valuable one as well the
other application we looked at was the
web and this is something that we were
the first ones to discuss folks here
have done this work done basically
trying to do to build parallelism here
right we have independent objects on
their pages PCP have this throughput
versus parallelism trade-off where when
you put everything down the pipe which
is what T which is what you try to do
with pipelining is that you are you send
multiple objects on the pipe but there's
head of line blocking because TCB
imposes this ordering constraint SCDP
got over this by giving you multi
streaming which is basically multiple
logical channels within one whole
connection the reason you didn't want to
break up the TCP connection to across
objects is because you have smaller
objects and the overall performance
would not be as good TCB does not do
very well with smaller objects with
small payloads so you have a trade-off
between parallelism and throughput and
we were able to build multi streaming
with minion million gives you an ordered
delivery we were able to build partial
order on top of it which is what multi
streaming is its partial order it is
ordering within a stream but order
independent across streams within a
whole convection the whole connection
ensures that you get congestion control
loss recovery everything is aggregated
across all the data so you get the good
effects of TCP you take out the bad
ordering constraint and you can get
multi streaming where we have a center
that breaks their data into chunks and
adds a stream header and throws
everything into you cobs and sends it
over you TCP and so our actual blocking
at the receiver across these streams we
don't really use center set priority in
this one at all but it's purely head of
line blocking avoidance and they tried
this with the real workload we got a
bunch of traces and we drove them over a
network and we had a sender send back
responses and we measured different
times and what we're going to see here
are a number of grafts but I will
describe them to you the top three
graphs here so these are just divide
total paid sighs okay these are
different plate sizes on log scales what
you have here is I'm sorry these are on
the x-axis you have a total paid size
and there is a total number of requests
per page so total number of objects in
the page and you would expect to see
more benefit as with many and multi
streaming as parallelism on a page
increases right so you want to see
better benefits later this way and up
there those three graphs simply show
that minion does not outperform what
does an underperform DCP in terms of
throughput because it is basically TCP
in terms of throughput but here we
measure the average time to first byte
it was a metric we thought might capture
the idea when does a user what is user
perceived latency average time the first
bite of each object on the page and you
see that overall mini and generally
tends to have lower user perceived
latency at any rate right because we are
able to basically multiplex across these
different streams and this is something
that's again not new this is something
that's been known but the fact that now
you can build it and make it look like
tcp on the wire we think is new so right
so without pipelining when you basically
you're basically saying it's in this
case once input object there was any you
don't really need pipelining in the
traditional sense it's not back to back
so let me step that from that for a
moment what I'm trying to say here is
that we could get the problem with HTTP
1.0 on the big globs of the HTTP 1.0 was
that you had one connection per object
and they were doing that because that
was is perhaps the simplest thing to do
but it also gave maximum parallelism we
are able to achieve that kind of
parallelism here you know it's more
streams within the one connection so we
have we are able to create multiple
logical channels within a TCP stream
within one tcp connection such that no
stream blocks any other stream so they
are independent logically parallel
within one tcp connection
browser 634 or multiple reproducing
browsable people that people retort you
try to understand what your edit bottle
is oh well so no it's sending them all
at the same time no it's it's it's 1pc
began so the capacity near the green is
HTTP 1.1 which is pipelining so the
requests are sent back to back but the
response has come also back to back on
one connection one tcp connection that's
right that's right and the HTTP and this
is basically all the requests also send
in a similar pipeline fashion with one
connection but the responses are sent at
the same time they're multiplexed I know
that yeah yeah and in that this was my
so I entirely make perhaps make the
state stay this case very clearly i am
trying to stop that from happening and i
shouldn't say it like that but what I'm
trying to say is that the six parallel
connections is to try and get
parallelism because one large object on
tcp connection between block a whole
bunch of small ones behind them and if I
can give that parallelism whether one
tcp connection you need just one sure
sure that's right until I start opening
18 connections mean where does it stop
no no I know I know it's happening the
problem is that there is no magic number
right there is no real magic number
besides one that's right in this case
there is no reason to say that force the
right answer maybe you know twenties
perhaps better the point is that it
depends on the fact that others are not
doing the same thing or it's an it's an
arms race basically of number of
connections if you think about in terms
of throughput my argument would be that
keeping it all within one connection
allows you to use as much throughput
well you can change the condition
control on that connection if you want
to do other things
but really the connection is sufficient
we should not have to open more than one
connection if you are doing it to
increase throughput that's okay i am not
going to argue that you shouldn't right
now at least a different forum i would
argue that with you it'sit's i do think
that you shouldn't have to open more
than one connection you have another
right yes it is i actually in the paper
we call this multi when we call
multi-stream tcp and i used to call it
poor man's a CD so i have i have a
scream header which is very heavily
inspired by a ctp but it's basically i
mean so like i said i used to call this
poor mans sctp the reason it's a poor
man's sctp is because i can't get
everything that a CD be gives me right
now but at least i can try to get I
can't get a CDP but I get something that
that looks and smells like it yes oh the
SST buggy so this so Brian Ford was did
the SST work and it's all in there
together I mean I worked on a CD be for
a while and I think these ideas are
generally they're definitely there yeah
yes
that's right many Polacks ended your
dynamic streams you could actually open
streams on the fly you could and you
could prioritize across them and it had
something really cool which was received
side flow control / stream which we can
try and build on top of these things so
yeah very good so in conclusion TCP and
TLS work on the internet and they have
been these workhorses of the internet
that we've used right Andy and
increasingly becoming user substrates
and either this point instead of trying
to fight this battle we want to try and
optimize and make these substrates good
solid substrates and we want to
eliminate and kick-out latency that's
hidden in there and there is this very
well known if you not read it you should
go read it article from Stuart Cheshire
from a long time ago that I think still
applies and a lesson here is that we can
fit square pegs these packets or
datagrams into these round pipes and we
can eliminate delivery delays and we can
make mods deployable most of these
modifications deployables with
applications and turn these workhorses
in two pack horses and I will close with
that thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>