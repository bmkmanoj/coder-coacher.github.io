<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability: GIGA+: Scalable Directori | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability: GIGA+: Scalable Directori - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability: GIGA+: Scalable Directori</b></h2><h5 class="post__date">2008-06-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2N36SE2T48Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi my name is Swapnil potter
I'll be talking about our ongoing
research on building scalable
directories for shared file systems this
is joint work with God Gibson at CMU so
today the demand for scalable storage
systems is increasing rapidly as more
and more applications begin to harness
the parallelism of big computers like
massive compute clusters and cloud
computing infrastructure large file
systems do a great job at scaling the
data path what I mean is file systems
specifically cluster file systems use
techniques like striping and sharing
resources across multiple servers an
object-based storage to scale the
performance of operations performed and
file data in in addition to that these
file systems enable highly concurrent
access to the file data using
sophisticated locking mechanisms and
distributed cache consistency schemes
however large file systems haven't
focused a lot in trying to scale the
metadata service for file systems so in
this talk I'm going to focus on one kind
of metadata which is file system
directories I'm going to show you how we
can build directories that can store
billions to trillions of files in every
directory and be able to handle a high
degree of concurrency in terms of
inserts and lookups our goal here is to
build file system directories and when I
say file system directory is what I'm
trying to say is that you want to
maintain the UNIX file system semantics
and we want to support the VFS interface
so there are two trends that motivate
the need for scalable directories the
first trend is that file system vendors
and users are seeing an increasing
number of applications that use the file
system as a fast and a lightweight
database typically this happens when you
have lots of clients trying to insert
small files at high speeds in a single
directory these kind of applications are
common in scientific experimentation and
high-performance community as well as
data analysis for post-processing one
such example is poor process
checkpointing of large applications so
imagine an application that runs on a
big cluster of thousands of machines
and is executing with thousands of
parallel threads when this application
decides to take a checkpoint what
happens is it dumps all its state from
the memory on to small files and all
these thousands of execution threads
insert these file in a single directory
as a result you what you want is the
directory subsystem to be able to handle
these high degree of concurrent inserts
in that directory so there are a few
other applications in there where you
can see a steady rate of creation of
inside a large directory these
applications are common in Internet
services are long-running scientific
experiments for instance you can imagine
small kilobyte size record associated
with every URL which you use later to do
some post processing the second trend
that motivates the need for large
directories is the increasing
application level parallelism so
applications today are beginning to run
on large compute clusters today's
cluster consists typically if thousands
of nodes slowly this number is going to
tens of thousands of nodes and with the
increasing use of multi-core machines
you can imagine that applications in
future would end up running on hundreds
of thousands of execution threats at
this high level of parallelism it's
important that the underlying file
system and the most specifically the
metadata services be able to sustain the
desired scalability so what we decided
was okay let's go ahead and try to come
up with a solution by which we can
support really large directories and the
central tenet of our research is extreme
scalability through high parallelism so
we build a distributed indexing
technique which we call Giga plus and
what this indexing technique does is it
allows highly unsynchronized and
parallel growth of the directory over a
scalable number of servers and this
growth happens in a completely
decentralized manner without a
coordinator in addition we also support
classic requirements like incremental
growth and load balancing across the
number of servers that you are using the
second key property of our system is
that we tolerate the stale mapping
information at the clients this allows
the clients to at use the system at high
speeds and the servers can update the
on demand in lazy manner finally we use
a unique self-describing structure which
uses a bitmap which allows us to encode
the entire index in a highly compact
representation so before I get into the
details of our technique let's take a
look at the current state of the art in
directory indexing and how the large
file systems today implement directories
so when we first think about of core
indexing structure has two things come
in mind hash tables and B trees file
systems have used both of them SGI is
XFS users be trees for its directory
implementation Linux ext2 ext3 uses a
modified version of a hash table to
implement directories so what are the
desired properties from any indexing
structure if you want to build a
scalable system so the first property is
load balance key distribution what we
want is if you're using number of
servers you want to distribute the load
in terms of capacity and performance
across all these servers so by simply
hashing the key by if you use hash
tables hashing the key gives you this
property assuming that the hash function
is fairly random and distributes the
keys uniformly the second key property
is incremental growth of the directory
this is particularly important for
filesystem directories because most
directories in any file system are going
to be small some of them grow to
extremely large sizes hence we don't
want to penalize the performance of
small directories however classic hash
tables the you know traditional data
structures 101 hash tables don't support
incremental growth the problem is that
in a hash table you divide the key space
into a fixed number of partitions and
once a single partition gets filled up
you have to go and rehash all the keys
into a bigger hash table clearly that's
an infeasible approach if you have
billions to trillions of keys in a
single directory there has been decades
of work and trying to make hash tables
grow in a dynamic manner and one of the
classic work in that is extensible
hashing so the key idea of extensible
hashing is to grow the hash table one
partition at a time without actually
rehashing all the keys let's take a look
at how extensible hashing works in
practice so this figure here shows how
extensible hashing world
where what happens is inextensible
hashing you add a layer of indirection
which we call the header table
essentially this header table holds
pointers to the different partitions
where the keys are eventually stored
each entry in the header table points to
a single partition but different
partitions could be pointed to by
different entries in the header table as
I'll show soon so once you have lots of
entries in this heritable you need to
use a suffix of the hash to decide which
entry to point to we call this our with
suffix as a radix and for instance in
this case since the header table just
has two partitions we use one bit radix
and this one bit helps us decide whether
you want to go to the zero bucket or the
first bucket what happens when the
partition gets filled up is the header
table doubles in size so what you have
is increased size of the header table
where initially the new entry is point
to the old partitions so when this
happens what you want is you want to
increase the number of bits that are you
going to use from the hash table to
decide which of these entries which of
these entries in the header table is the
correct entry so we increase the radix
by one use two bits to decide which of
these four entries should insert the
should have should hold this particular
key and in this case for example this
happens to it happens to go to the
fourth entry and the fourth entry
currently points to a partition if that
partition gets filled up you split the
partition into to create a new partition
move half the keys from the old
partition on to the new partition once
this happens you update the pointers of
the header table to point to the new
partition so the key idea here is that
this extra bit of radix helps you decide
what key is moved from the old partition
onto the new partition the keeper of the
key drawback of this mechanism is that
it was designed to be a single server
implementation what it means is that
everything the header table as well as
the partitions are stored on a single
server as a result all operations on
this hash table are going to be
serialized and the scalability is
limited by a single server
so let us look at how current file
systems implement directories most file
systems implement directories on a
single server or a single machine the
common way to get access to multiple
directory stored on different servers is
by using NFS where you can just mount
different directory stored on different
servers on the client namespace and get
access to those directories but the
problem is that the directories
themselves are still stored on a single
metadata server so that's still a
potential source of hot spots in a big
system a clustered filesystem spect take
a step further they have two kinds of
servers an i/o server and a metadata
server the i/o server is used to stripe
the file data across multiple IO servers
and the metadata server stores all the
metadata one such file system is the
lustre file system which is now and by
sun microsystems what lustre does is it
uses a single metadata server to store
the entire directory tree and all the
metadata associated with the files as a
result all operations all metadata
operations get serialized and you have a
potential source of bottleneck in a
large system p v FS is another open
source parallel file system used in a
lot of real world hpc deployments what
PDF is does is it distributes the
directory hierarchy across multiple
metadata servers but the problem is that
every single directory is still stored
on just one server so you still have
this issue where if a directory goes but
becomes big you're still going to hammer
on one server the state of the art and
distributed directories is IBM's gpfs
what gpfs does is in addition to
striping all the file data on the
cluster nodes it also stripes all the
directories so if a directory grows big
it will stripe it on multiple servers so
the way it grows directory is it is
using the classic extensible hashing
scheme and it also enables concurrent
access to the directories using
sophisticated distributed locking and
cash consistency mechanism let's look at
both the issues the first issue we
should look at is how GPFS enables
concurrent inserts in a single directory
so GPFS is a symmetric file system so
you have a node which can act as a
client and a server so if in particular
node wants to part once
operate on a partition of a directory it
gets a write lock gets a write lock on
that particular partition caches it and
does all its operations so here's an
example where you have two nodes trying
to insert concurrently in a single
partition initially in the first step
node one has the right lock on the
partition and node 2 comes along and it
says I want to access this partition the
node 2 sends a request to the lock
server for a write lock the lock server
response saying that oh I don't have the
right the right lock is already being
taken by node to go and contact node 1
so known to sends a message to node 1
saying that I need a write lock on the
partition that you hold node 1 says fine
I'm not working on it it flushes the
cart can contents of its cash on the
disk which is in a shared disk storage
and sends a response back to node 2
saying that here have the right log and
do what you want to do no 2 unruh
sieving the right lock reads the fresh
copy from the disk and cashes it and
performs all the operations so what's
happening here is that on every insert
there is there are two sources of
bottlings first you have to go and talk
to the lock server every time that
happens that's a potential source of
contention at the lock server and second
is the disk i/o bottleneck so for every
insert a client is going to the holder
of a write lock is going to flush its
copy to the disk and the reader the new
holder of the right block is going to
read it back from the day so you have to
disguise at least you described is
happening in every insert so the second
property the second source of
synchronization in gpfs is the way you
ensure this mapping of a distributed
directory so if a directory is striped
on three servers let's assume this is a
directory which currently has three
partitions p1 p2 p3 how do you find out
what server stores what partitions so
the way gpfs maintains this mapping is
it that it stores it in the inode of
that directory so the direct indirect
and w indirect pointers hold addresses
to these particular partitions so as the
directory grows new partitions are added
and I node is updated but what happens
is this copy of an inode GPFS uses a
strong consistency on this copy of the
inode so if there are thousands of
clients working on a single directory
everybody will have
this copy in cash so if the directory
grows and even a single pointer is added
the splitter or the node that created
the new partition has to go and
invalidate the cash for all the rest of
the clients update this inode write it
to disk and then notify all the clients
to go and fetch it back from disk and
that's a possible source of a big
bottleneck in terms of workloads that
have a high degree of concurrent inserts
so the key benefit of GPFS GPFS is
really great at work loads that have
look up intensive operations so because
there's heavy caching so you can benefit
a lot from cashing you cash all the
small directories that you have and
everybody holds a read lock on that and
performs all the lookup operations
that's great that gives you extremely
high performance but as I described
earlier the concurrent inserts are
expensive because of two sources of bar
tonight so we spoke with the GPFS
developers and they acknowledge the
these issues with the concurrent insert
workloads and the current versions of
GPFS are trying to overcome these issues
in concurrent inserts however we still
have this residual issue of trying to
have a synchronous copy of the mapping
information in the system and I'll focus
a little bit more than that so let's try
to think of ways about how we can scale
beyond gpfs gpfs does a great job at
scaling now let's see how how can we
overcome the problems with gpfs itself
the first way is ok we know that the
lookup mechanism is a problem so let's
not use any partition to server mapping
what we can use is a well-known mapping
scheme that is known in advance that is
you don't have a symmetric system but
what you have is certain nodes that are
going to act as servers and all
directory operations are go to those
particular servers as a result you don't
have a lock contention that happens in
GPFS and i believe the GPFS developers
are looking into such a scheme for the
next version the second problem is we
could this mapping issue and we could
just use stale mapping information at
the clients we don't need to keep a
consistent updated I knowed every time
what we can do is since we know that
some some nodes are going to act as
directory servers clients continue to
use that stale information and send the
request to a server if that happens to
be an
incorrect server the server can update
the client in an on-demand manner just
as a side note there is this indexing
scheme called LH star which enables some
of these properties discussed above but
the problem with LH star is that it
imposes of strict order in the way
partitions are going to be split and
because of this strict ordering you
cannot have any parallelism in that
indexing scheme at any given point there
is only a single partition that will
split and the rest of the system has to
wait for that split to complete so
having discussed the issues in current
state of the art what's new in our Giga
plus directories so we have three new
features first we eliminate all the
serialization in the system the way we
do that is by allowing all the servers
to grow in an independent manner in
parallel without using any central
coordinator the second the second key
feature is that we don't have any
synchronization or consistency
bottlenecks that's because every server
only keeps track of a local view of the
directory the directory is partitioned
across multiple servers and each
partition just each server keeps track
of its own partitions they do not talk
to each other when they want to grow and
the third feature is that we use weak
consistency for the update messages so
what I mean by weak consistency is that
whenever an update happens it does not
need to be propagated to the rest of the
system however applications that use the
directory are always going to see a
consistent image of that directory so by
using this mechanism we can tolerate
stale information at the clients and the
server's we can lazily an on-demand
manner update the clients now let's look
I'll give you a quick example of how all
these three features work together in
practice before getting into some of the
indexing details let's take a look at
this example you have a directory foo
which is divided into divided across
three servers currently this directory
has three partitions p1 p2 and p3 and
each partition holds a particular range
in the key space for instance p1 who is
the first half of the range ptoo ptoo
holds this next quarter and p3 holds the
remaining quarter of the range and since
you have the directory striped on
multiple servers you need a way for
clients to look up a partition on a
given server
and for the sake of simplicity let's
assume clients use a partition to server
mapping scheme so client be comes along
and wants to insert a particular file in
this directory it hashes the file name
and this hash value determines that this
key is going to be inserted in partition
P 3 and it sends a request to server are
the server are on receiving the request
can either just insert it in the
partition respond back to the client or
it might split because the partition
gets filled up so when this split
happens server are will contact another
server to create a new partition and
move half the keys from the old
partition onto the new partition thus
the key space is divided into half
across these two partitions the key
thing that happens is that every server
keeps track of how its partitions are
growing so when this split happens
server are keeps a split history
associated with p3 so what this split
history the card indicates is that ok p3
split into a new partition called p4 and
that was stored on server y and some
additional information like time stamps
or other bookkeeping information that
you might want to keep after this
operation is completed the server
responds to the client and the client
updates its mapping information now note
at this point client a and client we
have diverging copies of the mapping
information and that's what i mean by
weak consistency whenever an update
happens not all the clients know about
it or they don't need to know about it
so the client continues to use its old
mapping information and decides to look
up the same file f1 and since it has an
old mapping information it sends the
request to server are so when a server r
receives this request it realizes that
it no longer holds this particular file
and that's because a split happened in
the past and moomoo it moved half the
keys onto the new server when this
happens the server are uses its split
history that we recorded and sends an
update message back to the client so
what is happening here is the cost of
tolerating the stale information at
clients is that you have a few extra
hops few extra forwarding messages
before a particular client requests hits
the correct server and as I will talk
about it later depending on how much
information you send when you're trying
to update the client this forwarding
cost is extremely low plus you can also
use tech
sophisticated techniques like gossip
protocols to make sure that servers
periodically exchange messages so that
everybody has a consistent image but we
haven't looked into that because we are
still pondering over the complexities of
those and we really don't need it
inherent for our basic scheme so there
are two main questions let's get into
some details now there are two main
questions here first is how do you
servers keep track of all the partitions
they store and second is how does this
how do these partitions grow on any
given server so we use a unique bitmap
representation that allows us to store
the information about the presence and
the absence of a partition on any given
server so in this bitmap a bit value of
one indicates that a partition is
present and a bit value of zero
indicates that a partition is not yet
present so servers use this bitmap to
keep track of all its partitions and by
partition it mean when I say servers
keep track of only its partitions what I
mean is the server's don't talk to
whenever a server decides to create a
new partition it does not talk to the
rest of the servers in the system or
update the rest of the servers these
bitmaps are also used by the servers to
provide the clients with look up hints
when clients receive these bitmaps from
different servers what they do is they
simply merge all these bitmaps which is
a simple or operation and what they have
is a complete map of all the partitions
in the index and they use this bitmap as
a way to look up the mechanism so in the
previous animation that i showed the
partition to server mapping we don't
need to keep that mapping it's simply
this bitmap that allows the clients to
determine is still look
deterministically look up a partition
and the server that stores that
partition let's take a quick example to
see how bitmaps actually work in
practice let's assume this is a bitmap
one indicates a partition is present and
zero indicates a partition is absent
you're trying to look up a particular
file you hash the file name and you see
that the hash tells you that it's
indexed on the partition which is
pointed to by the I 8-bit location in
this bitmap you see that the bit
location is zero which means that the
partition is not yet existing so what
you do is you divide this location I by
2 and then look at that particular
bitmap once you look at that index you
again see that it's 0 and it does not
exist and you repeat the process
and you continue until you hit the first
location which indicates that a bit is
one that means the partition exists and
you go and send the request to that
particular server that holds the
partition so using this bitmap
representation has a couple of benefits
first it's a highly compact
representation for instance a directory
with billion entries can be stored in
less than 16 kilobytes so even if you
have lots of huge directories in the
system you can easily always keep this
bitmap in memory at most nights and
since it's small it's very low overhead
to send it across the network the second
benefit is that the entire state of the
server is encoded in this bitmap so when
index decides to grow what it does is
the server was just simply updates it
bitmap about the new partitions that are
created it does not need to talk to any
other servers or exchange the bitmaps
explicitly with any other servers and
the server's also use these bitmaps to
update the clients so when a client
receives an update message it
essentially receives the bitmap from the
server and since this bitmap has
information about all the partitions on
the server you are sending a lot more
information to the client than at what
then then what it needs so since the
client gets so much information the
possibility of it being stale for a long
time goes down significantly now let's
look at the second issue which is how do
you partition the directory across
servers how does the directory grow
house then a index grow let's like take
a look at this quick example imagine a
directory directory start small most of
them are small initially there is a
partition which holds the entire key
space this partition receives an insert
if this partition gets filled up it
splits into two so what's happening here
is that directory the key space is split
into two and you have a new partition
created on another server and similarly
you get another insert which happens to
go in the second partition and it splits
into two so what's happening here is
every partition is splitting
independently in the traditional
extendable hashing scheme whenever a
growth happens you have a you the tree
grows symmetrically that means whenever
you add a new level new depth to the
tree all the nodes that have all the
nodes are added
in this case that is not happening
because every server makes an
independent decision of splitting a
partition and this goes on further so
what we achieve is highly concurrent
growth without any synchronization in
the system and in addition to this
servers as i said earlier also keep a
split history associated with its
partition so in terms of the tree
representation the nodes the pointers to
the children node essentially capture
the split history of that particular
partition so what happens is you use
those split histories to connect the
clients that have stale mappings so the
one way to look at this split history is
that they give you a complete history of
the growth of the directory as a
transitive closure so when I mean by
what I mean by that is if you go to any
partition and follow Traverse it's split
history is what you are going to get is
the entire partition all the partitions
in that particular directory and we can
simply encode them in bitmap as well as
keep them as an attribute of a partition
the cool thing about these split
histories is that it allows us to
reconstruct the bitmaps so if a server
the server goes down and comes up if
there's a crash and reboot you can
simply just reverse these split
histories of the partitions and recreate
that bitmap without actually contacting
any other server and we use them to
update the stale information at the
plants as I said earlier so the key
elements of our gig applause design are
completely decentralized and parallel
growth and the fact that we tolerate the
use of stale information at the clients
n now let's look at let's look at the
evaluation of some of the core indexing
properties there are three goals of this
evaluation first is we want to check the
overall scalability of these indexing
structure we want to see the cost and
the benefit of using week consistency
and we want to see the trade-offs and
choosing the right partition size for
the directory we model clusters up to
tens of thousands of nodes our server
model is simple we use a local file
system with a 64mb directory entry cash
and two disks on every server the disks
our model using a seagate barracuda
model with 5 millisecond seek latency
and 300 megabits per second x4 transfer
time the network is modeled using a
Gigabit Ethernet we associate the cost
of one
second to insert a directory in a
partition and clearly it seems like so
what it means is a disk can handle
thousand I operations per second it's
faster than what most is today can do
but it does not affect our scalability
issues here so what this graph shows is
basically that yeah our indexing scheme
achieves linear scaling what we try to
do here is that we have thousand clients
trying to create 10 million files in a
single directory striped on varying
number of servers and with the partition
size of 10,000 entries are in every
partition as you can see that whenever
you double the number of sir double the
number of servers you see a 2x
improvement in the sustained throughput
of the system sustained insert rate of
the system the second experiment we ran
was to compare week consistency with the
strong consistency semantics what we
compared with is an improved version of
gpfs as I mentioned earlier gpfs ER is
trying to fix the synchronization issues
with concurrent inserts so we use an
advanced version of GPS gpfs that will
come out soon that only has a problem of
consistency when the bitmap changes or
when the mapping information changes we
compare it with our Giga plus model and
what we see is that the amount of data
moved by diga plus is much lesser than
what a strong consistency protocol will
do we can see an at least an order of
magnitude difference in terms of the
bitmap the mapping information that is
sent across the network the third and
the final aspect of our evaluation was
let's see how read their performance
it's an important operation where you
want to go and look at all the entries
in a particular directory and read it
depends on the size of the partition so
we varied the different number of we
played with different partition sizes
and try to understand what's the
overhead of using small partitions or
big partitions so using small partitions
the problem with using small partitions
is that you are going to you're going to
pay a heavy read their latency and the
reason that happens is read there goes
to every server and reads this small
partition so effectively what you are
doing is you're going to the disk
reading a small amount of data and your
bandwidth defective utilization is
dominated by disk seeks that's it as a
result your parallel read there is go
to take a lot of time to complete and
clearly since the partition size is
small partitions are going to split more
frequently as a result clients are going
to get a out of date faster and you're
going to send the bitmap across the
network a lot more than you should so
you will see almost a three to four
orders of magnitude more bitmap traffic
in term for a small partition compared
to an extremely large partition that
does not mean that we could just use
large partitions that's because if a
partition is large it's going to take us
take time to get filled up and therefore
it will be slower to split so you won't
be using all the available parallelism
in terms of the number of servers very
quickly so you will take it will take
time to insert a whole number of entries
in that particular directory so in
conclusion if this talk was one hour
long what could have what I could have
told you is basically that there are two
prototype implementation that we are
working on first is in the PV FS 2
parallel file system which is an open
source cluster file system we're working
with the PFS developers currently the
code is still in a graduate student
version so if the code becomes it
becomes stable there are chances that we
might merge it with the main p v FS line
we are also implementing a user level
implementation infuse the goal of this
implementation is to create a vehicle to
play around with ideas with scalable
metadata and directories and large
directories we are also working on
trying to come up with schemes to handle
the operational realities of any large
system like handling server failures we
have a novel replication scheme that
tries to improve on the classic chaine
des clustering model what our
replication scheme does is it gives you
highly parallel and fast operations both
in terms of failure and data recovery
plus we are working on things like what
happens when you add and remove servers
from the system and other issues like
handling overloaded servers and the hds
I'll be happy to discuss these issues
offline so finally the key takeaway
message from our work is that today's
file systems don't do a good job in
scaling for large directories so what we
came up with this is a POSIX compliant
file system directory idea which has
three key properties first we achieve
highly concurrent growth with
minimal synchronization we don't have
any serialization in the system and
allow the server's to grow their state
independently because each server only
keeps track of local view of the system
and there is no global shared state and
finally using weak consistency semantics
for the update messages we can tolerate
the use of stale mapping information in
the system thank you and I'll take
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>