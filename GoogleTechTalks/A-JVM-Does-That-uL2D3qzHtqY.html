<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A JVM Does That? | Coder Coacher - Coaching Coders</title><meta content="A JVM Does That? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A JVM Does That?</b></h2><h5 class="post__date">2011-04-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uL2D3qzHtqY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our speaker today is awesome how awesome
is he he's he's so awesome that if you
were a car he'd be a helicopter he's so
awesome that you were a plane he'd be a
rocketship he's so awesome that if you
were a software engineer very much if we
were a software engineer he'd be the
chief JVM architect at his all systems
ladies and gentlemen Clif click thank
you um well that was a little more of a
intro than I usually get at least have
somebody fold all right so this is me
rambling about what goes on inside the
gate iam having worked on them for
almost 15 years a little on time and I'm
still amazed at what goes on inside with
um and there's a huge pile of services
and people been adding services David
that the last time I'm sure everyone's
right here many of these services are
sort of volunteered to the JVM
engineering teams by naive changes in
specs which turned into like horrible
long disasters of engineering efforts to
make them do something useful so some of
the services involve a high quality GC
and this is like one that's on
everyone's mind because there's so much
engineering effort to put into try and
get on GC pauses to be reasonable on
these ever larger and larger heaps but
really the GC that's in there now is
pretty amazing compared to what was
available 10 and 20 years ago the actual
total allocation costs are quite
reasonable compared to the alternatives
and the GCS are parallel and concurrent
incremental and yada yada yada they have
all kinds of cool features about them
there's a high quality machine code
generation going on in there
hotspot has two different jets um most
of the Veeam VM is out there have
several different jets in motion at once
there's management of the legit code
there's profiling and the combination of
these things turns into gives you a
bytecode cost model and that's sort of a
key bit of transparency that's not one
that people often think about a bytecode
cost model tells you something like a
get filled instruction is typically the
cost of an x86 load right and that
actually only works because the JIT puts
its head around what it means to do a
get filled removes all the weird corner
cases optimizes away the null checks on
the range checks
everything else on the planet and in the
end the bike codes that you get actually
have a cost model that's somewhat
related to the underlying hardware
processor cost model and then there's
uniform threading and memory model and
this is basically tells you the meaning
of multi-threaded programs what it means
to have two threads where they read and
write the same shared memory and whether
that's correct or not or what how the
data flows in win and what and how and
why it also gives you type safety and
that's you know crucial for large
programs your C programs do not get that
and suffer from weird bugs the same you
know that kind of a way um you get to do
things like load code and when I say
load code I don't mean just loading new
code and running event loading it
running it using all the same services
that are already there meaning it has
the same bytecode cost model despite the
fact the code was not available in the
JVM started you have a long running
server you download them to go to over
the web it's just as fast as if you had
it locally on the spot meth because you
optimize and reach it on the fly high
quality time access we'll talk about
that because it led to one of the more
horrible things inside of a DBMS there's
a lot of introspection services you know
JVM tid ipi agents reflection yada yada
huge access to a huge library o access
to stuff in the OS thread scheduling
priorities things like that those are
all brought through the JVM and
available to your Java program so where
did all this come from so most of these
services were incremental e added over
time the hot spot you know 1.0 or java
virtual machines wanted o didn't have
all this stuff but the language the JVM
and the underlying hardware have all Co
evolved over the last decade right so
the incremental addition it's like
finalizes the thing that worked there
originally were added by people who
wanted it same with the Java memory
model where we started seeing lots and
lots of multi-core systems that we had
to know what it meant to have two
different threads on this running
concurrently to communicate 64 bits was
added when it became clear that we would
have lots of machines with 64 bit
capabilities support for high core count
and that means different underlying
algorithms in use in the JVM itself for
concurrency for scalable concurrency if
you're wondering on a garbage collection
on 100 gigabyte heap you're not going to
run it on a single thread so and take
too long so now you need a garbage
collection that scales will all the
cores that you have available right and
now the real question is is why the
services came
why do they come they came because the
services provide an illusion a powerful
abstraction it changes it puts a
dividing line between where the jvm
engineer is innovate and where the
writers of Java code innovate right so
people who are writing Java focus on
adding value above the JVM layer and
people who are implementing JVMs provide
services below the JVM layer and that
separation is similar to the separation
between x86 and the programmers who
write software on an x86 it's a great
place to draw the dividing line between
who's doing which piece of work right
now if I look at the set of services
though I see that they've been very
ad-hoc they've been grown over time as
people requested or needed some of the
services are unique to Java or niek to
the Java Virtual Machine but a lot of
services sort of overlap with prior
existing services but there was some
stupid requirement that wasn't quite
right and so the actual service you know
like priorities within Java and
priorities as they exist in the OS are
not quite the same thing and they don't
do the same thing and they have slightly
different jobs and you can't end up
using the same priority mechanisms and
I'll have some more discussion on that
but there's a lot of these services that
following that camp
they look like they're similar to what's
been done before there's some stupid
thing that's different and we had to you
know replicate that service the hard way
ok so that was my intro uhm let's talk a
bit about the illusions that we have the
abstractions that we have the big one is
obviously garbage which is just to call
and we'll fix it it's the illusion of an
infinite heap right just allocate memory
via new and poof you got some memory
right don't bother to figure out when
you're done with it track its lifetime
whatever GC we'll figure out what's live
and what's dead and do the right thing
recycle the dead memory um it's just so
much easier program than malloc and free
that it lets you write code in an
entirely different way you can remove a
huge class of bugs from your mental
thinking process and that means you can
write code quicker to get the same job
done quicker time to market usually but
you know the other hand you might take
quicker time to market to turn into more
time to add more features or more time
to add more performance work or more
time to do what
ever um it was just a lot easier to use
GC than without it and then it turns out
that it enables certain kinds of
concurrent algorithms that you simply
can't do without it it's just too hard
to track wideness on a bunch of these
interesting concurrent algorithms and in
particular of the JDK five locks the
java.util concurrent locking mechanisms
do allocation in a way that requires a
lifetime management that's not really
feasible without using GC they're
fundamentally those locks require GC
deformation okay more on GC this is this
huge strife who made in the last decade
and we went from having a GC that was
good enough to use in production in a
lot of scenarios fairly reliable but
didn't scale will hugely well and still
have all kind of weird corner cases and
still had issues with very large heaps
or with pause times whatever and people
have been adding more and more more
features to the GCS over time so you
know it's robust it's parallel
concurrent but it's still too many G C
tuning flags at least I claim there are
on this is a major point of
differentiation among different people
who are working with different juicy
algorithms it's a source of active
development throughput on the different
GC algorithms might vary by as much as
30% um where the very concurrent very
low pause time GCS have higher overheads
and then the sort of top the world ones
so here's six orders of magnitude on
pause x azul cell you machine they'll
take hundreds of gigabytes of heap with
a ten millisecond max pause IBM will
sell you machine that has you know
hundreds of megabytes not gigabytes at
heap but with ten microsecond pauses
this is good enough to do hard real-time
stuff and I in fact know that they are
doing hard real-time stuff like avionics
control systems and the like and the
stock TC starts having serious pause
issues when you head for the tens of
gigs of GC of the of heat and then you
know CMS and g1 fit in there and there's
other GC algorithms floating around so
there's that illusion so let's move on
here so byte codes are fast it's a nice
illusion my codes in fact suck as a way
to describe programs they're rather
horrible on but we're sort of stuck with
them now the main wind of byte code so
is it does hide the CPU details it's a
way to describe a program without
knowing what the actual annoying
hardware is ten years ago this is
interesting as a differentiation between
x86 and spark or
day6 and power or x86 versus something
else these days it's the low-power side
it's x86 versus ARM chips or MIPS or
whatever goes into a I pad iPhone I got
a yachtie thing right so really what
we're doing with the byte codes is that
we're expecting them to be fast but if I
do sort of the naive interpretation of
full semantics of a get filled or get
static they're actually pretty slow this
is a lot of work that goes on there and
ginning brings back the expected
expected cost model lets me have byte
codes and kind of believe oh that'll
turn into something that's reasonably
efficient and it actually only works
because the JIT does all kinds of magic
tricks um so eventually the byte codes
have to be cheated right and you know
the Jets themselves are these days are
very high quality optimizing compilers
in their own way they're giant complex
things 20 years ago an optimizing
compiler oh that's now inside of hot
spot we'd be consider most complicated
programs on the planet these days it's
just pieces part of a JVM
right and the other kind of thing that's
interesting here from a historical
perspective is it brings sort of GCC -
OH - that level of optimizations the
masses every time you guys run hotspot -
server they're running an optimizing
compiler running behind so it probably
is the most executed optimizing compiler
on the planet now or at least is gonna
be really close to you know GCC but I
can't just pick up GCC directly why not
just you know why right write your own
optimizing compiler well this is one of
these services it didn't work to use the
existing services are out there there's
lots of compilers 20 years ago 10 years
ago lots of optimizing compilers out
there why would any of them work because
they don't track pointers for garbage
collection writing they didn't do the
right thing there they didn't do the
right thing for the Java memory model
which has strong requirements on how you
do we were ordering of loads and stores
and fences I'll talk more about that in
a minute and then there's just some new
code patterns that needed optimising
things that weren't happening in C and
Fortran programs we're happening in Java
programs needed optimizations things
like class hierarchy analysis or no
pointer check eliminations
um ginning requires profiling at least a
minimal amount of profiling because you
don't want to get everything right when
profiling allows you to focus your code
generation efforts on the small pieces
of hot
which happen to be the case of all
modern programs right but profiling
allows you to do better cogeneration
then sort of static compilation because
you know something about the code you're
looking at you know what methods are hot
so you know what to inline where to do
your heavyweight optimizations throw
that throw the work the big effort from
the compiler gets put down in the right
places and then just you know general
profiling turns out to be let you do
better things with code layout for
branch prediction or spill code
generation or instruction scheduling
things like that is a bunch of stuff
here and basically you know everyone
gets to use profile covered if I look
out sort of static optimizing compilers
for the old spec unit spec FP style
stuff profiling those machines those
programs was worth fifteen percent or
more sort of raw speed-up over a non
profile but fully optimized code so
profiling has a real interesting real
benefit to the performance of your comb
another illusion we have is that virtual
calls are fast um
C++ has virtual calls kind of introduce
the notion of it but they avoid it by
default because you have to ask for it
and they avoid because it's slow
requires multiple levels of indirection
including in the end a jump in direction
off some table Java sort of embraces
them by default everything is virtual
and Java unless you say otherwise is you
add the final keyword to say then again
it's not virtual right well that that
makes them common and if make them
common you have to make them fast or
they're unusable so mostly they do get
fast the big-ticket item here is class
hierarchy analysis which tells you that
in fact while multiple targets might be
possible in practice right now there's
only one if I know what the one is I can
make it a static call it on me to a
virtual call and it says it's a static
call and I can inline it to all other
optimizations I want to do but that
might change over time so if new classes
get loaded I might have to rerun my CH a
and maybe reach up my code and you know
that all works at all is in there and
does the right thing and then finally if
you can't use CH a turns out in practice
most call sites that are labeled as
being virtual by the programmer are in
fact static in a dynamic sense only one
class of object ever hits that call site
for this pointer that's the same class
every time you use a simple inline clash
it's a classic key value pair in
in the code test the key against the
class is the expected class you've
already done to look up for the targeted
method and you encode it as a call
instruction at like the hardware x86
machine code level and that makes a
virtual call that hits in an inline
cache basically one clock cycle longer
than a static call it's extremely cheap
but when that they all should go back to
being slow turns out in practice this is
almost never an issue that the number of
times you have to do the load load load
jump register to get the virtual call
table lookup is so vanishingly small
that it makes no difference on on any
real program partial programs are fast
right this is the whole I didn't have
the whole program at startup time this
is the open world models for the closed
world model right you can say class or
name and get new code loaded in and then
that runs as fast as code that was there
all along because you optimize it and
you may have to re you know D optimize
old code and repro file read yet to get
there but that all is in there that all
works you can add code on the fly and
it's just as fast as if it had been
there from the start a consistent memory
model so this is a this is an
interesting one at these days although
ten years ago it was highly unknown
people are not thought about was it
important in people's minds all machines
out there have different memory models
and the rules vary on visibility vary
widely from machine machine even with
insane within generations of the same
kind of machine so you might think an
x86 which has a very conservative memory
model has the same sort of memory model
from x86 within across generations turns
out within the same generation there are
some variations turns out it varies
across motherboards as to the timing of
wind loads and stores cross back and
forth so the rules here are in fact
quite variant it's hard to say what they
really are
um powered nips have a have a more
aggressive memory model and Azul when I
taenia my actually a very aggressive
allowed wide out of orderings in the
hardware level now we map this very out
of order widely varying loads and store
timing issues back to the Java memory
model by doing something ok lets me next
slide ok so really we have to we have to
match the Java memory model despite the
fact that we have all these very
different memory models from unknowing
Hardware if you
execute loads and stores without
thinking about the order in which they
happen you'll get programs whose very
meaning depends on the hardware you run
on if I ran it on this x86 today and
that MIPS chip you know tomorrow the
meaning of the program would be
different because the underlying memory
model is different so given different
memory models and actually none of the
match the Java memory model they all
have to be you know the gap has to
bridge by the JVM but has to keep that
gap bridge while keeping the bytecode
cost model normal loads and stores have
to remain fast and we do it with some
combination of code scheduling fences
very careful placement of your locks and
your compare and swap operators requires
close cooperation from the jet requires
a JIT engineer to have detailed hardware
knowledge to do the right thing you know
when you can insert an x86 infants or s
fence or a lock to add to the top of the
stock whatever the hell it is today or
tomorrow the next day on the right way
to do this on x86 or spark or you know
arm or whatever okay another one
consistent threading models there's a
lots of very different OS is that hot
spa or jaebeum's and general run on
linux solaris and x sort of fall in the
camp of like pthreads works and you kind
of port pthreads and it's all good
but also you run on other things cell
phones and iPads or maybe on some
thousand CPU you know mega no server
somewhere and these guys have very
different alessa's but Java just makes
you know new thread do the right thing
and immediately synchronized wait notify
join all just work your multi-threaded
program does the right thing across all
these different forms and that requires
a lot of work under the hood okay here's
an illusion blocks are fast okay so
obviously not all locks are fast you
have a contended lock and obviously
something has to block go to sleep in
u.s. so that's fine but then we might
expect fairness from the OS so if I have
a thousand runnable threads I'll try and
take the same lock I might hope for well
if I'm running on you know for anyway
machine most of my threads are asleep
they're getting woken I'm taking turns
I'd like to expect fairness from the LSN
in fact that's not the case
most OS is a might as they say like all
the ones I know about do not hand you
fair lives so you get thread starvation
it's very easily testable on write a
Java program once you into Runnels and
you can watch threat to probation
especially if you have one thread whose
job it is is to shut all the other ones
down after he goes to sleep he'll never
get another cycle and go there or shut
the program down so we fake fair locks
and JVM itself and that requires a huge
amount of engineering work to get the
right kind of things going on fair lines
now if you move away from contented
locks and go to the uncontained locks um
they're very cheap these days and in
fact they're getting cheaper if biased
locking gets turned on um we can expect
a uncontained locks to be in the handful
of flock cycle range um so this is sort
of amazingly fast compared to you know
hot spot one night Oh ten years ago
where locks pretty avoided at all costs
because we're so expensive these days
and if they're not contented they're
pretty damn cheap and why did they get
optimized you know here's the different
questions that if they are optimized why
because they're so common people don't
know how to write programs how to write
concurrent programs just in general so
the industry as a whole
settled in on this just add locks until
works kind of mentality so the least
common denominator I don't know why it's
busted I got some weird eight erase I'm
going to throw another walk in throw
another walk I thought oh the bugs might
have done right and so what's become
common uncontested locks become common
because they are defensively programmed
in right and so the JVM is optimized
them we get this interesting concurrent
programming style of atom zillion locks
until it works and as an industry we
have learned a lot about concurrent
programming but I don't claim what I
claim we have learned is that this sucks
as a programming style and we're looking
for a battle model we don't know what it
is yet
but if the model we have right now okay
change topics quick time access system
darker time Milly's the reason it's in
those slides is because it turned into
them one of those grossest hacks
floating around inside hot spot and in
general and I've come to discover today
that Google has replicated the hack over
again uh-huh
so under some particular interesting
benchmark not to mention jvv or anything
yeah a winning score will be called you
know it'll be called billions of times a
second so it's not millions it's
billions of times a second but it is
actually fairly common in all walks
Valve apps people timestamp
a lot of things a lot of the time they
like to throw their timestamps out and
they do that with my calling card time
Milly's but and here's this
the light right here what this little
piece of stupid Mouse says is if one
thread thinks his current time noise is
less than another thread by even one
millisecond which if they're just
comparing millisecond counts they might
actually differ by no more than a
nanosecond time but they just happen to
get the edge case where the the
milliseconds flip then this thread
thinks all his reads and writes are
completely available to that thread okay
so what that really means is is if one
guy goes and populate some data
structure and reads the current time
Milly's and writes it to memory and a
second guy reads it up and says oh it's
off by one millisecond even though there
might be one nasa contain difference the
millisecond count value is one-off that
he believes that all the writes that
this guy did are visible on that guy and
he merely can rely on them being there
when in fact they haven't had time to
cross the chip and the caches yet you
know that your light only goes so fast
and so he pulls a bunch in an
uninitialized data and real large you
know three-letter company acronym java
app servers will crash if you don't
provide this guarantee it's just
something you have to do it is
definitely not in this back okay so then
you know this situation is kind of
stinks and Intel said oh we can fix it
we'll add this TSC register this nice
thing that will count ticks on a running
core and will give you you can
converting the time and the obviously we
convert a time you just load the thing
add some base and scale it and poof it's
it's going to give you milliseconds
right but it turns out the values not
coherent across all CPUs now new on Hale
emits coherent within a socket but
across sockets is still not coherent so
it varies you know it goes faster and
slower and faster and slow and some CPUs
we get read receive use it's doing this
right it's all over the map and that
means that if you just read the TSC
shift scale ad you get issues where this
doesn't work so it's not sufficient for
running large java programs that way um
right now a hotspot does current time
Milly's by doing basically an inline
version of the fastest flavor of Linux
time of day call which is mostly some
sort of user mode atomic structure read
you read some big user mode struct that
the kernels fixing behind your back and
a read-only page for the user side and
he reads the user reason wants and then
read the sec
time to confirm it hasn't changed
between reads and if they get an atomic
read of all the fields then they use
that to shift scale add and come up with
your current time Millie's it gives you
a nice quality time and involves a
number of memory fences and reads of
memory that are long enough that loads
and stores from different CPUs settle
out and so you get the atomicity
guarantee that you need across machines
if I read current time Miller's on one
thread and read it on another they will
agree if they know they differ by a
millisecond they will agree that all the
loads of stories from one are visible on
the other okay but a better way to do
this are faster waiting how is to do
some sort of plain load instruction or
the kernel updates the current time
Milius in some user mode page that's
just actually the milliseconds I just
have to read it and that means a
thousand times a second I want to tick
that one word up in the kernel well
that's pretty damn cheap actually that's
worth 10% on jvb and if you turn on - X
x1 + aggressive optimizations on hotspot
it does this but the negative crashes
all those app servers but it's been for
so much ATP so pre I gots a pretty gross
situation and then hypervisors came
along and said oh we can fix TSC we'll
make it make it idealized we'll make it
monotonically uniformly taking across
the CPUs by intercepting the reads of
the TSC register and doing the right
thing under the hood in hypervisor and
handed you back the right answer but
that means access the TSE make sure is
like 100x slower it's just as fast as
doing this so you know it's like now
they didn't actually fix the problem
it's not this fast it's that fast but
it's secretly that fast cuz it looks
like it's just read some hardware
register but it's not okay uh moving on
so here's some illusions we think we
have but don't actually or I would like
to have so we'll start out with
something like two halves here infinite
stack everybody who likes these
functional languages like closure or
whatever would love to have tail calls
tail call optimizations and that gives
you the illusion of an infant stack
running Cody's data that's closures
that's coming actually a project lamdaur
coin or some combination of the two it's
going to hand you closures which is
essentially CODIS data um capital I
integers this jeepers lowercase integer
and that's useful for again some of
these functional languages which likes
you to take primitive in some let you
throw virtual calls on them on to do I
don't know
what I don't know why but but that turns
into autoboxing Java see if you use cap
if you use a cap y integer to get
started somewhere and then you do
lowercase int like things like for I
equals 0 I lesson in I plus plus well
the I plus plus turns it into autoboxing
that is it becomes a capital I adjourn
and then you have to allocate an object
for the capital integer and you have to
get the value out of it and add one and
make a new cap Elena German yadda yadda
it's like ten times slower if you head
down this path and it's hidden from you
that it happens that way because Java C
does it makes the capital a metres
automatic and now people like the JIT to
undo that under the hood um so that's
some piece of work has been done by son
before Oracle took them over I don't
think it's turned on yet and I know that
it doesn't work for anyone it is turned
on because I've looked at it big
integers is cheap as little in that's
you know that's a silent overflow to
infinite precision integers that to do
that efficiently you you want to do
something called tagging and your math
which requires close cooperation from
the Jets
and the underlying runtime system but I
think the number of consumers of this
are very small although some of the new
languages want to go down that route
they're going to make all the users of
that language pay some heavy penalty or
I think the number people that need
infinite precision math are very small
and I know who they are so they can just
go ask for it so I think this is barking
up the wrong tree but I'm not a language
designers well quit now yes it does very
interesting
ok other allusions would like to have
atomic multi address update which is
another funny name for software
transactional memory a bunch of people
are playing games here the JVM and the
JIT could conspire do interesting things
there if this becomes the programming
current program style of choice I don't
believe it will because it has some
fundamental flaws but I don't know if it
turns out that people like software
transactional memory above all other
options then there's things that jet
could do here JVM could do and then fast
alternate call bindings you know this is
invoke dynamic this is like um you know
JRuby has different call semantics than
Java does and invoke virtual doesn't do
the right thing so you want to do some
sort of different thing there but you
still won't have to be as efficient as
an invoke virtual which in the end
turns into like an inline cache kind of
thing and that's what invoke dynamics
about so it's not here yet but it's
probably coming so here's some illusions
who think we have right that's the big
one massive code is maintainable
hotspots approaching fifteen years old
and large chunks of the code are very
fragile or aren't a very fluffy per line
of code there's a lot of code per unit
of work done right and so you get a very
slow new feature rate of change things
don't get added hotspot very often like
invoke dynamics the biggest newest thing
get added in a long long long long time
what we've been busy rewriting lots of
it that was the fun part of going away
from a big corporation to a startup and
I in fact have rewritten lots of
subsystems to become the old faster
simpler lighter and it's just sort of a
you know the takeaway from this is it is
possible to a fixed hotspot to give it
new paint jobs and get rid of lot of the
really crafty stuff it just hasn't been
happening at sign or who knows what or
whole dunno okay more illusion so I
think we have thread priorities people
think we have thread pyatters because
obviously the OSS have thread priorities
and they work right well it turns out
they don't Linux has for instance thread
priorities but you can't raise your
priority without being route and
everyone starts at the default of max
which means that you can't lift your
priority without being rude well that
means if you want to have a concurrent
GC which wants to provide sort of hard
real-time guarantees on your GC
performance that needs cycles and if you
have a lot of running mutator threads
they're competing with those GC threads
because everyone's at the same priority
unless you lift the priority of the GC
threads above the mutator threads but
that means you have to run as root and
that's an ops guys nightmare every JVM
runs its route in order to get
priorities for okay so bad news
you get priority inversion you get a
bunch of this use one of the other one
here is a next slide talk about this one
you get priorities that are relative to
the entire machine but you need
priorities that are relative within a
JVM as well as across machine it's a
different notion of priority and then
obviously you know we think we might
have run one write once run anywhere but
scale matters stuff you do for small
scale low power low battery machines
it's very different from what you might
expect if you're running on a you know
quad socket eight core
the halo thing you need you know one
matters for memory size footprint and
power consumption and one says I got to
have tons and tons of threads got a
giant keep I got to have giant i/o
things I have different programming
styles okay finalized errs um people
think finalizes are useful somebody does
because I see them all time but they're
not but they think but you but you're
led to thinking that they are and you
know what the issue is of course is that
it's a way to reclaim OS resources but
there's no timeliness guarantees given
the code will eventually run but
eventually could be you know heat death
of the universe kind of thing right and
the the obvious example here is some
Tomcat which turns to file handles as
part of accessing web requests and you
can trigger an out of file handle
situation which requires a full GC cycle
run the finalizes to reclaim the
alessa's then the OS resources of the
file handles and in the olden days and
heaps for small you ran full GC cycles
fairly often and your file handles got
returned in a timely fashion and the
newer machines with more memory you
return through file handles faster than
you run GC cycles and so you right out
of file handles and so there is a back
door hook in the out of us file handle
resource thing the towel GC to go around
an emergency GC cycle reclaiming
finalizes as best you can and hopes at
this then you can go reax LS can you
give me a file handle now and maybe you
can you know right but do we really want
to program our systems this way every
time you add a new OS resource you have
to add a back door to the GC is stay go
run a finalizer now and just in case
something comes back out of it um it's
sort of the wrong way to manage
resources soft phantom roughs weak refs
and maybe we grow so useful and this is
basically another case of using GC to
manage it use a level resource in this
case usually user mode caches and the
idea of course is you have a cache that
was on a lot of stuff that you might
need but sort of optimistically in the
cache and then if you're low on memory
can you throw stuff out of the cache and
then you know reclaim a bunch of memory
right and so what happens and I've seen
this happen in number of occasions you
get some sort of low memory situation
which is going to run a GC cycle
well the GC cycle flushes your soft
roughs the GC has no clue what a
software F points to at the user land
it's just an opaque software thing so he
passed to choose some he picked some ran
sort of soft refs to clear out and that
causes your cash to flush well then you
miss in your cash because the userupp
was using the stuff in the cash it was
doing what a cash does and saving you
from work so you have to do all the work
that you would have said had you hid in
the cash and now that work causes you to
do some more allocation and since you're
low on memory you do another GC cycle
which flushes your cash again and so
what happens is you have some sort of
server which is behaving nicely and as
you add load the load ramps up the
server's throughput matches and ramps up
with it until you hit some threshold of
out of memory you're getting close to
memory edge and then of course the load
never comes to in some uniform easily
scaling thing the loads bobbing up and
down as it ramps up and you get a little
bobble at the wrong time I run a GC
cycle and suddenly you have you know low
memory situation and you're trying to
hit your cache the same time but your
cache flushes and you hit this cycle
where your cache flushes you don't want
your work now at the GC again and you
get in this endless GC cycle where the
servers doing GCS constantly and every
time he does a GC all the load has do
all the work the hard way because the
cache isn't functioning anymore it's
been stripped clean and since it does
all the work the hard way his throughput
sucks and he fills his memory up with
all the work so if you fill the cache
but then the cache gets wiped clean
again and the throughput on the server
crashes and remains low until you take a
load off long enough for GC to catch up
quit flushing the cache and let you
start applying a load again gradually
and of course at this point people just
think the server has crashed and they
rebooted which is sort of another way to
get the same thing done so I clean that
the failure mode in this scenario is
you're asking GC make a decision about
your application level performance
application levels behavior without that
GC having a slightest clue what the
application level behavior is he doesn't
understand things like this is an LRU
cache and I want to do some sort of
least recently used notion on the things
the software so I'm going to wipe out
and on he just has a pile of soft rest
he has to wipe out sort of blindly and
hope okay so let's go sort through all
the different things I've got here
here's a summary with services GC
ginning Java memory model thread
management some fast time jetting
includes you know hiding CPU details the
hardware memory model from people um
services that are provided below the JVM
now everything like threads and context
switching from LS priorities IO
filesystem stuff like that
then there's a set of services above the
JVM application level stuff thread pools
and work lists transactions or crypto
models or your user mode caches Yuma
JDBC driver caching here's what models
of concurrent programming right and then
you know some of these new languages
alternative languages might have
different rules on dispatching or what
it means to have an integer or
concurrency so I claim that I would like
to see a fast quality time moving to the
OS um because you know the TSC registrar
get something very fast assuming the
hypervisor or some kicked in on me but
it's not quality enough and I can use
call good time of day but it turns out
that that's not as fast as I might like
or could get if I move the thing under
the LS all I have need to do is have the
OS tikka memory where 2,000 times a
second and some user mode read-only page
and and this would give me something
that's coherent on a clock cycle clock
cycle basis all the CPUs at cache it in
their l1 cache should be cheap be
coherent whatever be easy to do thread
priorities they always gives me thread
priorities at the process level um but I
want to have priorities where if I have
a high priority JVM that's doing some
sort of critical performance latency
performance thing like a web service
I want that BL starve out some other JVM
might have which are running some sort
of batch oriented background thing you
know somebody's doing statistics
gathering across a large DB data mining
that guy needs to be lower priority than
the guy that's trying to make money by
having you click on a webpage right but
I can't specify that without doing it
sort of at the entire process level but
then I get thread parties within a
process and so I can't distinguish
between GC threads which need to have
cycles ahead of the mutaters or else the
concurrent GC won't actually be
concurrent it'll get starved out right
and the mutator it ends up blocking for
GC cycle the same story goes with
getting threads on the jet needs to get
cycles eventually or you will always run
interpreted and this was a actually
failure mode in that rule eval on a mark
where a thousand run will threads would
compete at identical priorities to the
JIT thread and the jet thread would
starve getting one one thousandth of a
CPU and the entire benchmark could run
interpreted unless the compiler if the
JIT thread has given a higher priority
than the runner bowls to go ahead and
you know and get his job done
right now social systems is faking
threat priorities with duty cycle style
locks and blocks we're getting the right
set of thread priority games done but
we're thinking what the OS should be
doing for us right and we need it
because we're trying to sell a low pause
concurrent collector we need the right
kind of thread parties games but I claim
this should just belong the OS OS is
already doing thread priority games they
already does process priorities ideas
context switches so I can do thread
parties without requiring everyone to be
rude all right and then you're the other
failure mode with everyone being rude is
if I take all the mutaters and
voluntarily lower their priority then
they don't compete well against a am i
squeal it's also running on the same box
they don't get cycles against the non
Java processes that aren't playing the
same game but aren't being polite
ok alternative concurrency models these
are things that on people are looking
for other ways to write large concurrent
programs other than using blocks um the
JVM provides you thread management locks
but some of these new concurrency ideas
like actors and message passing this is
skaila and Erlang are software
transactional memory out of closure or
fork/join your Doug Lee's fork/join
model um these are things for which the
JVM itself just sort of too big and too
slow to move fast here
I think these experiments should be
above the JVM and they are now and they
should stay there until we get some
consensus on the right way to do
concurrency and then maybe there's some
interesting pieces that should go into
JVM at that point like some sort of
specific kind of STM behavior or some
sort of magic fast Parkington parkour
cyclic barriers are on or what it would
be but this is something where I want to
see people keep doing stuff above a JVM
so fix nums fix gnomes are one of these
funny things all rail on it one more
time and quit um
they sort of belong in the JVM
implementation not in the high-level
language implementation because the JVM
can do good things there that the
problem is that the obvious translation
to infinite precision math at the Java
levels fairly inefficient
we'd rather have some kind of tag
integer math and then the JIT can do
magic things at tagging at your mouth to
make it really cheap um but then I'll
throw my two bits out you know do a Bill
Gates here and same 64 bits ought to be
enough for anybody you do that level
program know if you need more than 64
bits I don't make everyone else pay for
it but I'm not a language designer it's
all quit railing on the language
designers I gave this talk first to the
Java gave iam Lang summit people which
are all
Java language engineers or JVM power
users new language people things to keep
in the JVM so GC chitting and the Java
memory model all sort of Co intertwine
in an intricate way the JIT by itself
really needs to be below the app unless
you want to be in the you know the
business of doing x86 code generation um
but it definitely belongs above the LS
and has been about the OS for the past
50 years so you know it works well there
as a standalone separate process like
GCC so keep it you know in between that
map puts it in the JVM GC also requires
deep hooks into the ginning process
because the JIT just really has to track
pointers and know what they mean where
they go in order to do the right things
with the right GC invariants so that
makes sense to keep that below the
application as well the Java memory
model requires deep hooks in the JIT
because it has to do with all kinds of
games involving loads and stores and
code selection code scheduling so again
make sense below the app on these things
kind of all tied together an intricate
way and that sort of defines or the JVM
O's there's some of the alternative
current concurrency models being exposed
by the new programming models people
looking at might enable faster cheaper
hardware with weaker memory models but
in order to do that you're still going
to need close JIT cooperation which
makes it still in the you know within
the JVM um OS resource lifetime
management I claim that finalizes are
not the right way to do that move this
outside the JVM make the application do
some sort of life time management with
its reference counting or arenas or some
sort of lifetime manage but there's lots
of different techniques here but don't
burn the GC with the knowledge that more
of resource X can be had if only
finalizes will run right now right and
same thing for sort of weeks off phantom
refs there are some interesting very
rare use cases for some of these but
most of them are you know III don't
think GC should be changing application
level semantics if if I have a
concurrent GC and it's running at a high
cycle rate on you um and you make a
softer phantom ref cycle by cycle it
could go to zero you could make it one
cycle the next cycle can be null doubt
and is that a useful programming model
and I do have programs that will crash
when that happens and I can it happens
fairly easily actually with a good
concurrent GC
so summary move some of these things
that are in a JVM now into OS read
priorities fast time access resource
management for the you know should get
moved out fixed em stale called closures
moved in
Azul systems is doing some different
stuff so this is now been me Azul system
specifics my one two slides of that we
do very aggressive virtual memory to
physical memory wrappings more or less
we are in user mode controlling the
virtual to physical mapping and that
requires terabytes second remapping
rates which we can't get out of map and
memo map and friends arm so we need oh
it acts to do that right on this is
still going to be safe across processes
you don't break process safety but you
know if you screwed up within a process
then the jaebeum's got a bug and crashes
JVM cell bodies in the garage what
happens so this is something we have to
fix um but if we do this we get this
really nice GC which you know handles
sort of memory of any size without with
those sort of low max pause hardware
performance counters Intel sort of
screwed it up here and that they didn't
make it cheap to get at the hardware
performance counters and have to go to a
kernel which you have to do a kernel
context switch in and out but the JVM is
a natural consumer of hardware
performance counters I wish the hell was
cheaper to get at them he can take perf
counter data remap it back to byte codes
in from there and to like Java lines of
code or whatever and that's all up to
the JVM to do that mapping if I can only
get that damn code out of the kernel in
a ficient way so you know Xul's doing
thread priorities by hand we'd like to
see that going to OS fast time but we'd
like to see hardware perf counters come
out in the user land because I want to
screw with them look at them and same
with virtual physical memory mappings so
yeah summary there's there's work to do
and that is it and I have a few minutes
any questions
you mentioned a lot of code
optimizations this is my gun the area of
vaporization Lucas detail please write
write worry okay so um there's a lot of
code optimizations are there a lot of
data optimizations floating around I'm
hot spot right now doesn't do a lot of
data optimizations so sort of like value
objects there has been a push for a long
time to bring up the notion of value
objects where for instance if you had an
array of points you didn't have to have
a Java object header for every point for
the billion points you wanted to draw
your 3d rendering scene right um I don't
have a good answer for that I have my
stock bad answer which you know turning
array of structs them to instructive
arrays
it kind of gets rid of the overhead that
way but makes it obviously less
convenient to manipulate use those
things those systems went to a one word
object header awhile ago that was a
little bit of data layout change but
that sort of been it we're looking at
changing the layout of strings well
we've changed it once already what might
change it again to be a more dense
format that's sort of it though no whole
scale interesting data layout
optimizations going on and escape
analysis is not enabled that's a
different that's not a data layout game
they're removing the whole object itself
they're turning in registered object and
it's not turned on because it has I
assume it has bugs bug issues so let me
go this guy menu yeah good Oh God okay
can I talk more length about software
transaction memory
yes I can should I probably not in this
context I think it's the wrong way to do
programming I have a large set of slide
I have a full you know 45-minute talk on
why it doesn't work that you know I'm
not going to do right now because I have
a hard deadline and so let's just say
that I think it's the wrong way of you
if you google me and software
transactional memory or look at my
website you'll eventually find you know
me blogging about the evils of STM now
not Josh one behind
multicast is still useful for power
users I will agree to that
why am i comfortable enabling users
um okay so that has two different
answers
one is I like enabling users because
it's how I get my you know my my good
feelings about myself in life I'm
helping other people with fix what they
claim is their problem and the other one
is as all systems we fell over everyone
else's broken concurrent programs
because our memory model was much more
out of order than in x86 and our
programs that would run on the next 86
but had data races that would not appear
on an x86 or would appear rarely on the
next 86 those data races would appear
frequently and quickly on the Zul box so
something that would crash once a week
in production on x86 for crashing ten
minutes under the Zul box and and at
first people claimed our hardware was
broken and eventually I figured out that
no in fact their programs were broken
and they were happy after a while to go
fix their programs and happy that we
discovered their bugs because they had
been eating their lunch in production
for you know years on end but it was
very painful path to go down to go tell
people that your program is busted and
I'm going to make it obviously by making
my new product dysfunctional on your
application so I think that is not a
good business model although maybe it
would be a wonderful you know
theoretical I love the world model um at
the time the java bytecodes were out
there were other formats lots of other
formats being explored for how to pass
around program semantics in particular
right after Java came sort of semi
popular IBM Microsoft put out a bunch of
research work on a different alternative
format that was clearly superior in a
number of ways since that time people
have done a lot of work on Java class
files sort of semi kind of backwards
compatibility on this on that makes
format a lot denser for instance but
still the act of turning byte codes into
jetted code has a lot of Hoops have to
run through that with a little
forethought in designing you know if
somebody who knew how to write a
compiler had talked to a designer of the
bike codes who's not here today um
things have been different a lot easier
I can name you a bunch of tricks here's
one if you have a full
to format into the stack format for your
byte codes you can pre-register allocate
it so that has a correct rich allocation
for a7 CPU and then I double that a 16
CPU 14 CPU and a 30 CPU 30 register
machine ace 8 16 and 32 register machine
your subtract 1 for those and then you
could the ginning process could have
been as cheap as take the byte codes
unpack them unfold the register
allocation of flavor of choice and you
wouldn't have to do anything more than
straight-up stupid cogeneration to get
mushy
it gets you an uber fast jet for the
first-tier jet as it stands now you know
jetting that first tier of java
bytecodes is substantially slower than
some of the alternatives that are out
there for ok no real good reason
it sounds like an ad for Zul systems we
changed GC algorithms to one that is
independent of the size of your heat so
we have you know people in production
with 300 gigabyte keeps with no max
pause times on the order of no 10 mil
easy and then we have our x86 stuff
which we have seen people do I think 100
gigabytes with max pauses or you know
handful Milly's um we changed GC
algorithms we have a better GC story on
that sort of you know half a business
model so it is a solvable problem ah
doesn't help you much you buy our gear
but you know you can talk to a soul
people
because it's too big and complicated in
the crashes and if a JVM crashes but
leaves your box up that's more pleasant
situation than your box hardcore crashes
in Africa power cycling it's true but
how often how often does your operating
system crash here I how it really how
often does your operating system go down
versus a java virtual machine go down so
ok so if you're in the embedded handheld
device the OS and the JVM are sort of
you might imagine them being basically
one setting and then yeah then the
whether or not you're within the kernel
outside the kernel sort of moot as all
systems we did a microkernel OS more or
less microkernel style but we kept it as
a kernel to get process protection
across JVMs in case the JVM crashed and
after a while the chrono became very
stable and machines would essentially be
up until you know they got power cycled
but the JVM were undergoing rapid you
know evolvement and they had bugs and
they would crash and you would be nice
to not bring the whole box down you know
if you've got a torrent CP box and it's
got 20 JVM is running on it if one of
them's got a bug you don't want to bring
everyone else down
um the optimization is the act of taking
an optimized running a piece of Java
code so Java code plus all's in lines
code instruction selection rest
reallocation in lining loop unrolling
everything everything on the planet mmm
and then discovering that deepen your
grander loop you had a virtual call that
you made stack and inline and unrolled
and replicated in yadda yadda yadda and
somebody loaded a subclass that now made
that virtual call truly virtual and you
have to JIT different code furthermore
right after they made that they loaded
that class they made a new object and
sorting the global published it which
your other thread immediately picked up
and the next round through that entirely
and heavily optimized unrolled Luke had
better pick up the new virtual call when
it picks up the new object and the more
make that all work correctly is sort of
you know it's semi hat-trick there I can
discuss in more length although probably
out of time to do justice uh basically
something I figured out while I was a
grad student at Rice and put in the
first version of the server compiler
hotspot I'm team million years ago and
the idea sort of spread around but
really you you need to track the java
virtual machine abstract state in the
compiler and be able to rebuild it at
some point during some point in that
loop we're going to acknowledge and
recognize the loading of another class
and get that all to work right and then
once you acknowledge it you have to take
your stack frame which is this heavily
optimized thing and unpack it rebuild it
as for instance interpreter frames which
then might get later we optimized I
don't our but that whole job is very
machine specific because you're
rebuilding actual frames on the stack
it's very jittered code specific it's
very it's a bunch of things it's it's a
lot of things are all tied up together
in a tight little water
well I am going to call it quits because
I have a hard deadline and I want to be
out the door shortly armed so thank you
very much for your time and attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>