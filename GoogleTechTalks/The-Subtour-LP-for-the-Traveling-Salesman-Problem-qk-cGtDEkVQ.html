<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Subtour LP for the Traveling Salesman Problem | Coder Coacher - Coaching Coders</title><meta content="The Subtour LP for the Traveling Salesman Problem - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Subtour LP for the Traveling Salesman Problem</b></h2><h5 class="post__date">2018-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qk-cGtDEkVQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'll give you a brief duction to the
traveling salesman problem although I'm
sure most of you are familiar with it
we'll talk about this sub 12 P and some
experimental analysis that's been done
with it and I'll introduce this
outstanding open question that still
remains outstanding we didn't answer it
I'm insane we answered a related
question that was a conjecture made by
Sylvia void and and Bob Carr um and then
we'll end up the talk with more
conjectures and some more experiments so
hopefully a direction forward for trying
to resolve this outstanding question so
the Traveling Salesman problem it is
probably the most famous problem in all
of the discrete optimization you're
given n cities and you're given the cost
of traveling between any pair of cities
and what you'd like to do is find a
minimum cost tour that visits all the
cities exactly once and goes back to
where it started from
throughout this talk I'm going to assume
that these costs are symmetric so it
doesn't cost any more to go from I to J
than from J I and that these caused to
be something called the triangle
inequality so it doesn't help you
doesn't make anything's any shorter to
stop at an intermediate city K when
going between rmj so on the right side
hand side of my screen here I have a
hundred and twenty dots that represents
cities from a certain country and you
can view the cost of traveling between
them as just the Euclidean distance
between them on these hundred and twenty
cities are from what was then West
Germany where it's very easy to pick out
the city of Berlin right there and
here's the cost here's the optimal tour
for those 120 cities so at the time that
this was solved optimality this was the
largest instance of this sort of the
Traveling Salesman problem for which the
optimal solution have been provably
found this is a an instance do to Martin
virtual from 1977 since that time a
number of researchers have worked very
hard on coming up with optimal solutions
and spread problems and there are now
much bigger instances of this sort that
have been solved optimality so David
Applegate I think you're joining in I
saw your face anyway here's one of the
things that David felt together with Bob
Bixby
project bought the Linda Kuk so now
here's 15,000 city instance with salt
optimality by that team of the now
unified Germany so there's the instance
um there's the optimal solution for the
missus and even larger 25-thousand City
instance from Sweden solved by the same
team plus one more researcher there's
the instance there's the optimal tour um
there's a researcher in theoretical
computer science by the name of Alice
Benson and he's from Sweden and he
points out that the Swedish population
is only ten million so these cities are
kind of a misnomer sort of the average
size would be about four hundred people
the way this research got started
looking you know take cities from one
particular country and think about a
tour through them was started in a very
famous paper by dancing Wilkinson and
Johnson in 1954 where they were looking
at 42 cities from the continental United
States and they were able to solve that
instanced optimality and the method that
they use is still the one that gets used
today for finding these optimal tours
and has been applied broadly to problems
in combinatorial optimization ever since
so what I want to do for you now is I
want to sort of explain what their
methodology was and how it applied to in
particular to the Traveling Salesman
problem that they tried to solve so just
to simplify our notation some lets us
think about having a complete graph so
we have all possible edges of the graph
the cost of an edge is just the cost of
traveling between its two end points and
what we're going to do is we're going to
set up a linear program where we're
going to introduce a variable standing
for whether or not I'll use that edge in
my tour because it's a linear in program
I can only indicate that the variable
has to be between 0 or 1 but what we'd
really like is that it is either 0 or 1
indicating whether the edge is in the
tour or not so the methodology that they
use is that they set up a linear program
that models the Traveling Salesman
problem if they happen to get a solution
that gives an integer tour then they can
stop and know that they're done and so I
have found actually the optimal solution
otherwise they introduced something
called the cutting plane so they
introduced a new constraint that's valid
for all possible tours but not valid for
the current solution so it's satisfied
by all tours but not satisfied by the
current fractional solution
linear further I'm so here's the linear
program that they started out with so
the goal is trying to minimize the cost
of all the edges that are in the tour
subject to that these variables have to
be between 0 &amp;amp; 1 and also something that
constraint that if you look at any city
and you look at all the healthy
variables for the edges that are
incident on that that should sum up to
two and that's because the tour will
have to come in to that city and then
leave it now if you go ahead and solve
that linear program and get what's
called a basic solution or an extreme
point solution um you'll get things that
look like this and we'll call them
fractional to matchings so in these
functional to matchings we're going to
have cycles of this where all these
healthy variables are set to 1 and they
go are on three or more cities or they
look like this or I have these healthy
variables and online' cycles where all
the opening variables are half and then
they're joined by paths where all the LP
variables are set to 1 if I look at
integer solutions of this type of course
now I only get this type of this type of
thing so cycles on three or more cities
sometimes these things are called
subtours because if you think about it
this is just talk taking a tour of just
those five cities and ignoring
everything else so when Dan psycho
President Johnson were solving the
linear program for the 42 city instance
I'm one of the things that they found is
that some things they would get these
sub tours and so they introduced a
cutting plane an additional constraint
that satisfied by all tours but not
satisfied by those subtitles so they
call these loop conditions but the name
does use narrow as subtour elimination
constraints so the constraint looks like
this let's take a set of cities s like
so these correspond to the vertices are
in the sub tour and we look at all the
edges that have exactly one endpoint
inside that set in one endpoint outside
we'll call those the edges in the cut
for that Ted Oh cities and what we
guarantee you is that the sum of the LP
variables for the edges in the cut has
to sum up to be at least two and that's
exactly because we want it to be the
case we know that any tour is going to
have to
into this sort of cities isn't some and
they come back out again
now the tour could go back and forth
across it a couple times but we know it
has to at least come in and go back out
again so that sum has to be at least
here so here's the linear program that's
come to be called the the sub tour LP so
the same constraints I showed you before
minimizing the cost of all the edges
every vertex has degree exactly two
across every cut I have at least two
edges crossing and the LP variables are
between 0 &amp;amp; 1 / what I talked about
later it's also going to be useful to
note that there's an equivalent
formulation for this problem which is
rather than having those subtour
elimination constraints I can instead
take a set of vertices and just say that
I can't have any cycles in that set
alright so if I take a strict subset of
the set of cities and I look at it it
should be a case of the number of edges
that have both have endpoints inside
that set can't be more than the number
of cardinality of that size minus 1 and
that'll guarantee that there are any of
these of these sub tours in this
solution so an equivalent formulation of
the subtour LP is keeping the degree
constraints and replacing the subtour
elimination constraints by these
constraints that said there's no cycles
for any given given subset of vertices
so how good is this sub tour I'll be
bound
well experimental work has shown that
it's extremely good so the the late
David Johnson who spent a long time in
Bell Labs and then AT&amp;amp;T Labs did lots of
experimental work with the Traveling
Salesman problem and they reported some
of the papers that he wrote with me
humid Rothberg and then later with me
you show that this sub tour LP is in
fact very close to the optimal solution
so here's a table from one of those
papers so on the left hand side here we
have a set of random instances so points
drawn uniformly from the plane and some
other types of things and you'll see
that for these instances that gap
between the LP bound and the costs of an
optimal tour is more or less less than
1% ik with that exception where it's one
point zero one and then if you look over
here for a set of structured instances
that were drawn from what's called the
kia steel if it's a library of standard
traveling salesman problem instances
the gap is always under 2% so
experimentally speaking this down is
very very close to the cost of an
optimal solution and that makes it
extremely useful when it comes time to
doing computing optimal solutions the
gap is very small and in techniques that
get used for solving these things
branch-and-bound methods and branch and
cut methods having a very strong bound
is extremely useful in getting optimal
solutions quickly so that's the practice
what about the theory how good is this
bound theoretically speaking in the
worst case how bad can it possibly be
and for that we measure something that
we call the integrality gap of the
subtour LP which I'm going to denote by
by gamma and what gamma is supposed to
be is the ratio of the worst case
possible case between the costs of an
optimal solution and the cost of the
subtlety so given a particular instance
where costs are symmetric and obey the
triangle inequality what's the worst
case ratio possible between the cost of
the optimal tour and the value that the
subdural P has the previous site exists
don't you need to say something about
like how many of these exponential extra
constraints you need to in general is
it's just not too bad I mean you end up
you have to solve a bunch of flow
problems repeatedly to actually find
that button yeah I understand generating
them on the fly you generate them we're
not calling practical you would need
another side very say like on practice
we only need to it like I know like I
think it practice is not very much David
Applegate if you wanted to speak up and
tell us how many unique although I think
it's actually at some point they stopped
generating those and start either
branching or start looking for other
fancier types of violated constraints um
that can the integrality gap be well we
actually do know that the the gap isn't
really close to one that it it's always
in fact at least four over three and
here's the instance that shows you
that's in case so it's an instance of
the type that's called graft ESP in
graft ESP I give you an unweighted graph
as input and then I tell you the cost of
travelling between those City I and City
J is just the number of edges in the
shortest path between those those do n
words um so here's the graph I
essentially just have three long paths
let's call the length k then the the
linear programming relaxation looks
something like this where essentially
put weight one on all of the paths and
then a little bit of weight on the
triangles at the end and so the cost of
the LP solution is something like three
K whereas if I look at the cost of an
optimal torque at some point I need to
introduce one long edge that jumps all
the way back and so the cost of an
optimal solution is something like four
over four K so the integrality gap in
this is something like poor K over 3 K
I'm finding a little bit of budget but
if K goes off to infinity it becomes
something like horribly okay so the
integrality gap is it at most is at
least four over three um now I want to
show you an upper bound on the the
integrality gap and in order to do that
I have to either introduce or review
depending on what you already know an
algorithm for finding for finding an
approximate tour call Chris does do that
Nico's Chris de P DS and he showed in
polynomial time how to compute the tour
that had costs at most three halves
times the cost of the novels so what a
crista fini say he said first let's
compute a min cost spanning tree on the
set of vertices in my graph and then let
me look at all the vertices that have
odd degree in in that spanning tree and
let me compute something called the min
cost perfect matching on those habibi
vertices so I've drawn the matching
edges in dashed lines here um what the
matching is it just a case where I get
to a single edge that's incident to each
one of the auditory vertices I'm
matching up those of you versus then
once I've added those edges to the graph
I have what's called an alluring graph
that's connected and all vertices have
even degree so I can do something that's
called taking an Euler and traversal
of the edges I can visit every edge
exactly once and come back to where I
started
now what of course we lost is something
that visits every vertex exactly once
and comes back to where we started so we
can use the technique dis called
shortcutting whenever I come if when
following this traversal whenever I come
to a vertex that I've visited before I'm
just going to skip over it and go to the
next possible vertex and because the
EDUCAUSE evade the triangle inequality
the cost of the resulting tour isn't
going to be more than the cost of these
edges that were in this alluring about
to start so here's an example I'm going
to start here I'm going to skip over
this vertex that I visited before again
skip over something I visited before and
in the end I have a book tour and the
cost of this tour isn't more than the
cost of those areas now the proof that
this is at most three halves times
optimal the first thing to notice of the
cost of the min cost spanning tree isn't
more than the cost of the optimal tour
and this is pretty easy to heat see take
a tour pull out an edge you have a
spanning tree
so the min-cost Fanning tree isn't more
than that and then it's not too much
harder actually to prove that the cost
of the matching is at most half the cost
of the optimal tour so those edges
together give you something that's no
more than three halves times the cost of
instances crista PD's Alban um but what
I claimed I was trying to do was trying
to give you an upper bound on the cost
of this significant on this integrality
gap so how do we get that well in 1980
Lawrence Woolsey and then a decade later
David Boies myself show that in that
proof you can replace those places where
I said within a factor of octaves the
optimum factor be optimal to work with
within some factor of the subtlety so we
can show that the cost of the main cost
spanning trees at most the cost of the
subtour LP and the cost of this perfect
match you know an odd degree vertices is
also at most 1/2 times the subs ROP so
the Christa Feeny's give some tour
that's within cost three halves of the
subtlety so what we end up showing is
that Christa biddies produces some tour
which was in three halves of the cost of
the sub tree okay so certainly the
optimal tour is every within three
houses at Sub Zero and that proves that
the grounding gap is always going to be
at most 3 over 2 so what I want to do
now is I want to sort of walk you
quickly through
this proof even though it's an old
results some of the techniques that I'm
using in this group are the same ones
I'm going to use later when I talk about
some of the newer results so it's it's
mercy
so let's first prove that the cost of a
matching on the odd degree vertices is
at most half the subtour LD and for that
we need to pull out a result from the
60s due to Jack Edmunds that says the
minimum cost perfect matching problem
can be expressed as a solution to the
following linear program so I introduce
a variable ze for each one of my hedges
and now I want it to be the case that if
I sum up all the healthy variables
incident on any given vertex that sum
should be 1 because I want to have one
edge incident on that vertex and also it
should be the case that I might look at
any odd set of vertices then I'm going
to need to have at least one edge on the
inside of that said match with something
on the outside outside of that side not
just because it's odd right I can have a
so even the number of pairs in here is
going to be even the one X at least one
extra vertex needs to be matched with
something that's like so accuse an easy
way to see that the cost of the mean
cost perfect matching is at most half
the value of the subtour LP I'll just
take a solution to the subtour LP and
show that what half of it is going to be
feasible for this matching limit if I
have a feasible solution that has cost
that's one half of this and certainly
because I'm minimizing the optimal
solution is going to be at most one hat
any claim this is pretty easy to see the
fact that some of the open variables
incidentally a vertex is going to be
equal to one follows because it's half
of the sum of the LP variables since the
non named vertex is going to be equal to
two and the fact that some of the LP
variables across any odd cut is grating
equal to 1 follows from the fact that
the sum of the healthy variables over
here across any cut is greater than or
equal to 2 and so if I take this
solution take half of it it's feasible
for this so the cost of the matching
here is going to be at most half the
value of the subs work now I need one
more idea which is just to note that the
the subtour LP is non increasing as I
remove vertices from that set so
if I think about the subdural beyond
just the odd degree vertices from the
spanning tree the min-cost perfect
metric on that is that most cost of the
hats half the cost of the original
subject okay so this is one ingredient
in the proof we've shown that the cost
of matching that the audio converter
sees that most have the subtlety what
about the spanning tree part well once
again we pull out of a hat result that's
known for a while to Edmunds that says
the min cost fanning tree can be bound
as a solution to the following LP where
I have a variable again for every edge I
have exactly n minus 1 edges in my
solution and it should be the case that
if I look at any set and I look at all
the edges that have both endpoints in
that set I have been most cardinality
that set - one of them expressing that
there are no cycles in my Spanish and
that's enough so once again we play
exactly the same trick which is to take
a solution to the subtour LP and then
show just a minor modification of it is
feasible over here in this case the
minor modification is that we scale down
by a factor n minus 1 over n then it's
easy to see that because all of these
constraints were met in this alternate
expression of the subtlety all of these
constraints will be satisfied - um the
only trick is just showing that this is
satisfied because all of these are
satisfied well the flash the algebra
there it's not too hard just a couple
lines but that's all you need to do so
the costs of a because we come up with a
feasible solution for the min cost
spanning tree problem that's equal to n
minus 1 of r and x sub 2 LP solution
than the cost of min cost spanning tree
is at most and so the cost of the main
cost spanning tree is at most the cost
of the sub terapia now we're done I've
shown that the crista Fiji's will
produce the solution whose cost is at
most 3 halves times the value of the
sub-zero um this has been a really
really interesting time to be around for
thinking about TSP research because
there's been lots of very interesting
progress that's been made on lots of
different questions related to the TSP
particularly for these integrality gaps
of LPS and for
Meishan algorithms so if you think about
this case I mentioned early of graft ESP
where you get this unweighted graph and
cost they're just the length of the
shortest path there was a sequence of
papers that made progress on this
question so the first real breakthrough
was this paper dude that Shia no vice
Quran I mean Sumerian Mohan Singh
ensured that they could you showed the
in growly gap was slightly better than
three over two which was followed up
soon thereafter by a paper by monk ins
Benson which is really beautiful it's a
really nice result they really bounded
things away from three halves and then
there was a sequence of papers ending so
far in 2012 I'm showing that the
integrality gap in this case is at most
one point for another set of recent
progress has been for the asymmetric
Traveling Salesman problem now the cost
of going from city or in the city jay is
not necessarily equal to the cost of
coins Jada I if you think about that
case where the triangle inequality does
hold and we look at a related linear
program well it was known for some time
that the antigravity gap was no more
than a factor of log log n then 30 years
later so just in the last decade Assad
pour it all showed that you could do
slightly better than that you could show
the gap was at most log n over log log N
and that was exciting
mostly because in the previous 30 years
people hadn't done anything other than
digit with the constants over in front
of the law but then more substantial
progress was made after that Cheyenne
had a student an art showed
existentially that the gap was going to
be a polynomial in log log N and then
just this past fall Lewis Venson his
student jacob sarnavskiy and lassie they
showed in fact that the gap is a
constant now the constant is bad about
5500 and you can ask how large is n have
to be in order for it to be better than
that result but you know it's progress
now that we have a constant gap
hopefully we can start driving that
constantly in terms of what we know on
the and as far as lower bounds on the
integrality gap does this paper by
Charikar Goins in car law that says the
gap has to be at least two
so we don't expect it to be better than
two if I had to bet I would say the gap
is too but we don't really know let's go
back to the question we were originally
asking them what about this case of
symmetric TSP where costs of a the
triangle
well I claim I tell you that every I
just told you everything is not the gap
is at least four over three it's a
mostly over tube and we don't know
anything more than that now there's a
long-standing conjecture it was made
explicitly in a paper by Gilman's but
they've been floating around for a while
that in fact the gap should be at its
lower bound but we don't know how to
prove just to convince you how ignorant
we are we didn't until recently even
though the equivalent of worst case
ratio between the costs of a two
matching right these cycles of size at
least three and the value of the
subtlety so Boyden car showed that this
gap is no more than four over three in
at least 10 over 9 and they conjectured
also that this gap should be at its
lower balance right now at or is a kind
of two matching but of course there
other types of things that aren't
towards there are two matches I'm
thinking juncture this ratio is there's
10 over 9 and the thing that I'm going
to sketch in the next 10-15 minutes is
why does the Poincare conjecture is true
so we can prove that in fact that gap
has to be at a slower bounced one back
gap is at worst tenable so what I'm
going to do is I'm going to show you two
different things quickly one is a sort
of a combinatorial proof that the gap is
at most 4 over 3 of a certain condition
is met and then how did we end up
proving that the gap is in most 10 over
9 and then we'll get to the section that
gives us some more conjectures and some
experimental directions to try so before
I get started on on showing you the gap
is at most 4 over 3 let me just
introduce some terminology we're gonna
be playing around with these fractional
to matchings again remember that these
are the things where we have these LP
variables in odd cycles that have value
have joined by these paths where the LP
values or what
so naturally enough I'm going to call
these
path edges setting to call these dotted
lines where the old p-value is half of
how those psychologists and if I have
something like this
I'm we're removing an edge would break a
component into two I'll call those cut
edges those are my fractional tea
launchers so here's the strategy of the
proof I'm gonna start with an optimal
fractional to matching and I think this
gives me a lower bound on the value of
the subtlety remember that we get a
fractional two matching just by taking
out all the sub for elimination
so if I take out some constraints the
value is a LP solution can only go down
so we'll start with this optimum optimum
fractional to match um what I want to do
is I want to add a low cost set of edges
to create something I'm gonna call the
graphical to matching so in a graphical
to match me I'll get components that
look like this the size has to be at
least three on every vertex will have
degree they're two or four of zero one
or two copies of every edge and why is
the graphical to matching going to be
interesting because I can play the same
short cutting trick that I did before to
get myself down to the two to match
right so if I just think about taking a
tour on these oil area and components
and then skipping over vertices that I
visited before I get little some
tourists on on those sets of vertices so
the case that I've consider is when I
have a fractional to matching where
there aren't any cut edges and what I
want to show you is I can add a low cost
set of edges that produce a graphical to
matching without increasing the cost by
more than portables so I'll show you
that I can get a graphical to matching
whose cost is at most 4/3 times of
fractional to matching we know that if
we shortcut the graphical to imagine we
get a two matching of no greater cost we
know the sub fractional to matching is a
lower bound on the subtlety so this will
prove that we have the two matching is
the most 4/3 times subtour and that'll
prove that this ratio view is it most
flawlessly okay so
we gonna prove this well let's start out
with some component from the fractional
- magic and I said I had these passages
and I have these cycle edges and what
I'm going to do is I'm going to create a
new graph from the component where I
take every path of these edges and
replace it by a single edge that has
cost that's equal to the sum of those
costs and I'll take every cycle edge and
I'll insert it and I'll have the
negation of the costs that it had and I
claims that the new graph has got to be
cubic because the vertices that matter
are the ones that are on the cycles
where they had to psychologist incident
in one pathogen so every vertex is going
to have degree exactly three it's also
going to be two edge connected precisely
because we didn't have any of these
cuttings so what are we going to do well
take this new graph that we're going to
compute a min cause perfect matching so
let's draw an orange say up in cost
perfect matching and then we're going to
use information from those match that
matching to help us produce a graphical
to matching from our original fractional
T so in particular if I have let's put
the matching down here I have a passage
that was in the matching up here I'm
just going to double all the edges that
were in that path right so here this
path wasn't in the matching this path
was in the matching so I don't double
close and for any cycle edge I'm going
to keep it if it wasn't in the matching
and I'm going to remove it if it wasn't
imagine so this these edges where the
matching so they disappear these cycle
edges aren't in the matching so he so I
claim that the overall cost here is got
to be the cost of all the path edges
plus the cost of all the cycle edges
plus the cost of the edges of the
matching because I precisely doubled all
the ones where the path edge was in the
matching and I removed all the ones that
I had the negative of the cost
whenever the cycling so the first
I need to convince you of is in fact
that this gives me a graphical to magic
so every node should have degree 2 or 4
and the component size is going to be at
least 3 because it was before so why is
that the case well if you just look at a
given node on a cycle right what could
happen in terms of what it gets matched
with well it could get matched along its
path a jerk as a matching edge is one of
the cycle edges make it's matched along
the path Ajay keep both of the cycle
edges and I duplicate the passage of the
vertex has to be 4 if it gets a match
with on for one of the cycle edges then
I remove that cycle engine keep this one
and keep the path that has to be 2 and
then same thing along the paths right if
if I have a node all along a path the
path that it gets matched this I double
all the edges it has before if I didn't
space
so I produce a graphical to managing and
let's end up bounding the cost of the
solution so I'm going to let capital B
be the total cost of all the passages
and capital C veted total cost of all
the cycle edges so I claim that the cost
of a fractional 2 matching is the cost
of the passages plus half of the cost of
the cycle edges because the LP value was
half on all this the edges in the cycle
I'm going to claim and I'll come back
and argue later that there's a perfect
matching in that new graph that costed
most one-third the cost of all of its
edges so it's going to be most one-third
times the cost of all the passage and
the negation of all the psychology so at
one most one-third p- so let's suppose
that that's true um we said the
graphical to matching costs the passages
plus the cycle edges plus the cost of
matching so past the cycle plus I said
the matching said most one-third P minus
C and just do the algebra and you get as
equal to four-thirds the quantity the
passages plus half the cyclists so we're
done well we showed us that I introduced
a graphical to matching this at most the
fact 4/3 the fractional to matching the
to mention is that most the cost of the
graphical one and the fractional T might
change the lower violet subtly so we get
this ratios in those floors
okay I had this one extra step which
said that the cost of that matching was
at most 1/3 times that costs weight of
all the edges that were in that graph
and I mean follows from a little lemma
that student and Essen totally blank so
they say that any cubic two edge
connected weighted graph has a perfect
matching of a cost that most one-third
the sum of the edge weights and the
proof is really slick and it just uses
this idea that we had before which is
take a look at the LP that expresses the
cost of a perfect matching and what they
say is just put one set all the
variables equal to one third for all of
the edges and then they claim well that
has to be a feasible solution if the
graph is cubic and two inactive um the
fact that the sum of all the healthy
variables incident on any edge is equal
to one call is easily right if the graph
is cubic it has exactly three edges they
all have way two thirds so they're going
to someone the only tricky part is how
do we know that these constraints are
satisfied so let's take a look at some
set of Audrey vertices and let's think
about counting all the edges that are
incident on each one of these vertices
right I have three edges incident on all
those vertices tests so I have three
times s I count every edge that has both
endpoints inside to seven twice and I
count every edge that has one endpoint
inside the set that are in the cut my
council's once so if the cardinality of
this set has is odd I have something
that's odd on this side this is even so
this has to be hard the number of edges
crossing anything that has to be an odd
number and because the graph is two edge
connected I know there can't be just one
edge across that cut so if it's on and
it's more than one
it's got to be three which means that if
I sum all sum of 3 LP variables that are
also to a third it'll be something
that's great it's pretty nice ok so now
I've completed the argument that the
this when when there are no cut edges in
the rational to matching the cost is
that most Four Thirds types of value
field how do we end up proving a
stronger result that the ratio is at
most n over nine
um what we do in that case is we
actually give the lp that whose extreme
points give us graphical to metrics so
let me sketch a little bit about why
that's how we do that we'll take every
vertex in our graph and we're going to
make two copies of it i prime and our
double prime will say that my prime is
required it's going to have to have
degree 2 in our solutions and I double
prime is optional it may have to be
either 2 0 or T so when you think about
the combined vortexes that have either
degree or 2 or 4 which exactly what you
want for every edge in the graph I'm
going to create three copies of it one
between the two required copies of those
endpoints and one between the required
and the optional required will copy of
one endpoints the optional cop with the
other and I precisely don't put an edge
between the two optional copies because
I want to force every component to have
size at least three right if I'm trying
to get a solution where I only have two
vertices in that one um I can think
about joining i prime to Jake Ron and I
require a copy of I to the optional copy
of J um if I do that then I know this
this is supposed to have degree either 0
or 2 that has one inch incident although
it'll have to have another you can't
join us up back to the optional copy of
I so it'll connect itself to a required
copy of some other vertex and that'll
force the component size to be at least
so here's the formulation that we come
up with for every required copy of a
vertex we want the sum of the only
variables incident on to be exactly 2
for every optional copy of a vertex we
want those some of the healthy variables
incident ought to be at most 2 we want
the very most between 0 and 1 and we
have this complicated set of constraints
it just says across every cut I want
there to be an even number of edges and
you can show that the extreme points of
this LP are graphical too much so now
how do I get from there to proving that
the upper bound is 10 over 9 um well I
play exactly the same trick that we did
previously when we were talking about
proving Christina's algorithm I'm just
going to take a solution from the Septor
LP and construct from that a feasible
solution here that I've cost in most 10
ninths of what what this solution costs
and we do it in the following way so
suppose I'm given a solution X let's
take a look at one particular edge IJ
I'm going to set the Y variable between
the two required copies of I and J to be
eight nine times that one between the
optional copy of I and requiring copy of
gave me one knights requiring a copy and
optional copy of J people in line so the
sum of this is going to be 10 ninths
times the sum of that variable the cost
of solution over here will be 10 9
solution and then you just have to argue
that all of these constraints are
satisfied I mean these constraints is
very easy to see the bounds on the the
variables on the decree constraints are
also equal to see easy to see because
the sum of the LP variables here
incident on many emerges was equal to 2
so if you look at the required copies of
these edges it sums up exactly to the
edges coming from this LP so if you look
at all the edges incident on the
required vertex set also up to be
exactly 2 and on an optional copy it'll
solve to be something less than positive
all the work is in showing that these
constraints are satisfied
given that the subcircuit string sir
sighs so that's how we prove that the
bound is at most 10 mm okay now we get
into the conjectural side of things and
conjectures about the TSP go back aways
so in particular in one of the papers
that Edmonds rode into the 60s shortly
after he had defined polynomial time he
said that algorithm was good if the
number of steps could be bounded in a
and some polynomial in the input size so
he called those good algorithms we call
them so he made the following conjecture
he said I can check sure that there is
no good algorithm for the Traveling
Salesman problem remember this is before
him P completeness or anything right I
conjectured there is no good algorithm
for the Traveling Salesman problem my
reasons are the same as for any
mathematical conjectured one it is a
legitimate mathematical possibility into
I do know so emboldened by Edmonds
we make we make our own conjecture which
is also legitimate mathematical
possibility too
no clue we conjecture that the worst
case for the sub Toria LP in a grounding
gap occurs exactly for the solutions to
the sub Terrell feed that are fractional
to mentions so it's exactly the case
where the cut constraints don't really
do subtour elimination constraints don't
really play any role that the worst case
is going to have now one of the
interesting thing about this conjecture
is not only we have no idea whether it's
true or not
we don't even know what the consequences
are if it's true right if you told me
that my if somebody comes an angel
visited me and sold told me your
conjecture is true I still wouldn't know
anything more than I did before well we
know something but I wouldn't know any
consequences on the integrality gap I
don't know how to come up with a tighter
bound on cases in which the solutions
are rational two measures attended
previous um one of the threads that came
out of some of this recent work on the
Traveling Salesman problem is a
conjectured algorithm that might have
better performance than festivities so
it might give us a way to resolve
something about this conjecture and come
up with a better bound on the
underground gap for the subtlety so
here's an algorithm mystical come to be
called best cementing crostinis the
ideas for it were in this paper by a
vise drawn at all and then made really
explicit in this paper by han-klein
murgatroyd's I'll say more about that in
just a second so one thing you say is go
ahead and solve the sub through LP and
get your solution X scale it down by a
factor of n minus one over N we said
that that's feasible for the min
spanning tree LP in particular what that
means is that is a geometrically
speaking is a point inside the feasible
region of what's called the spanning
tree polygon and that means that you can
express the point as a convex
combination of spanning so here's the
idea if it's a convex combination that
sort of gives you probabilities that it
is one of any particular one of a set of
spanning trees so just sample according
to that probability that gives you a
spanning tree ground christianities on
that rather than computing the min
spanning tree run it on this sampled
solution
so they came up with this algorithm for
a particularly reason that I'll describe
in just a second but we could at least
ask the question experimentally does
this have any hope of being better than
Christina's and this is the question
that I worked with the undergrad kind of
Kyle Jen over the answer and what we
found was the following we said there is
some reason to think that this spherical
e speaking anyway this is better so over
on the left-hand side here I have a
column mark standard and this is just
Krista Feeny's algorithm run across a
couple different classes of instances so
these are instances from the Traveling
Salesman problem library these are VLSI
instances and these are instances of
graphed TSP so this is consistent with
what the experiments that David Johnson
did earlier we're finding that Krista
VG's is something like 9% away from the
cost of an optimal tour for a TSP it
seems to be it worse
you can compute something like 12% away
from opt we tried a number at different
ways of expressing the convex
combination of spanning trees and then
doing sampling so be used max entropy
sampling and took the best solution that
we found then what we would get is
something that was like between say
0.31% of optimal and five-and-a-half
percent another type of of sampling that
we did involve something called
splitting off and there we could find
that solutions where a 0.8% away from
off all up to 6.6 percent so why does
this work I mean why it is why discuss
the many crostinis do better than what
Chris de Fiji's is doing and sort of
heuristic Lee speaking I think it the
answer is this which is that this sample
spanning tree that we get ends up having
lots and lots and lots of vertices of
degree 2 which means that if you look at
the look for odd degree vertices they're
kind of few and far between what this
means is that the cost of the matching
edges actually are going to be more any
particular matching edges gonna cost
more than it costs over here because the
other be vertices are far away from each
other but they're going to be a lot of
fewer and so the the cost of the
spanning tree of course only goes up
this is the main cost spanning tree but
the cost of the matching goes down
and that's enough to offset the increase
in the cost of Hispanic David yes sir
kind of a knife quit now you listen so
even even possible or even feasible
reasonable to think if the whole step of
sampling in expressing their getting is
up to LP and expressing in terms of the
convex combination could all those all
of these things be accessible as solving
an LP and some cans were rounding and
some careful rounding um yeah if you
know which spanning tree to round to
right I guess you could think about
doing that but I guess you have to be
careful about costs to the other thing
that the comments common example in from
the convex combination does for you
this is let you say the expected cost is
going to be about the cost of the subs
around alright so you both want certain
problem property you want to if you
didn't rounding you'd have to make sure
that your cost and so on
and that you were getting whatever
properties as a tree David yeah some of
these experiments when you do best of
money I guess in the splitting off you
can actually then take all of them but
in mice entropy has how many matter how
many how many yeah we we just fixed the
number and did it so we did something
like a thousand samples until the best
one and the genuine posing yeah I mean
knowing how many we didn't do I mean
this is where we fell down in our
experimental analysis we probably should
have very big about to see how much
better we got one more sizes or was here
I don't think it was too bad um just to
give you some sense here's a here's a
class of instances I think these are the
VLSI ones but I'm not on admission sure
um if you look at the degrees of the
vertices that we got with standard
Chrissa biddies forgiving the macaws
spanning tree so I'm only about sixty
percent of them have degree 2 or we have
something like twenty percent or maybe a
bit less than that have degree 1 and 3
but when we do max enter pretty sampling
you know we get something like 90% of
the vertices have degree to about 5% of
the vertices that we want
so what ends up happening is that the
cost of the treaty goes up right so
these are expressed in percentages of
the cost of an optimal tort for standard
Crysta fees the cost of a tree the Minho
spanning tree is between 80 and 90
percent of the solution for best of many
because those value of the subtlety is
really close to optimal it's essentially
the cost of an optimal time but the cost
of the mansion really falls down so here
in standard crostinis it's between 30
and 40 percent of the cost of an optimal
solution for these sampled methods that
falls down to be something like between
4 and 13 percent so this cost goes up
the matching cost goes down but you win
trade-off so how could this result an
analysis that shows something that's
better than 3 over 2 well the expected
cost of the tree is going to be the most
the subtlety again just by construction
if I sample from the convex combination
was a feasible solution in this sector
LP scaled down by a little bit the
expected cost of the tree is going to be
the most which is exactly the same thing
we have in the analysis of Krista
feeties on what we really have to show
safe the expected cost of the matching
somehow is bounded away from 1/2 times
the value this subtree and that's the
part we just don't know how to do be
nice there is some hope that this should
work and the reason that we should hope
that it does work is that it didn't work
so for another variant of the Traveling
Salesman problem called stts path TSP
faceted work yes so that when you then
showed this path of many groups seemed
to work better than you your cost was
for the best solution say goodnight like
your your I mean that the table was for
the for the least costly solution
there's thousands NM
now you're claiming something for all of
the thousand so I sort out this part
yeah it was just going to be true just
by the way they don't know that the
second one so you would have to do
something for the expected yeah but
expected we beginning and sometimes you
could verify this one as well as that
empirically speaking out for service
each of the matching is
less closely for the for the best luck
over those like like this one also
ultimately that so the reason we have
some hopes that best so many Krista fees
might work for regular tsps it worked in
the special case so SP TSP path is this
I give you a thick start vertex and a
fixed end vertex s and team and then you
find the middle cost Hamiltonian path
that visits every starts at us ends at e
visits every other vertex exactly once
so if you do the analog of the Krista
Feeney's algorithm it's been known for a
while that you get a 5/3 approximation
now it's also known that if you look at
the corresponding linear program the
lower bound on is integrality gap is at
least 3 over 2 by an example similar to
the one that I showed you before um so
that's so many crostinis I said showed
up in this paper by on Feinberg richboys
whoever they were read about this
problem and were able to show that you
could be Krista Feeny's by using the
best of any recipes particularly
something that was golden ratio as
opposed to one point six six six and
since then there's been a sequence of
papers improving on that results so and
improved analysis of this algorithm
bases the gap one point six and by
thinking hard about what decom
properties you want out of a convex
combination of trees people have been
able to drive that down to the point
where now the integral gap is known to
be three halves plus one over thirty
four so we're really close to that lower
bound not quite there yet but but really
really close okay I think this brings me
to the end of my talk I have this is a
sign from a radio station outside in
Anyang Mayan island so Yamaha now
Howland is owned by the Norwegians it's
about I think it's 400 miles to the
north east of Iceland so it's in the
Arctic Circle and it's cold ones I don't
speak Norwegian but I haven't I'm good
at the 40 that the translation of the
sign is as follows theory is when we
understand everything but nothing works
practice is when everything works but we
don't understand why at this station
theory and practice are united so that
nothing works and no one on
this is sort of the pessimal case about
the interaction of theory and practice I
mean the optimal case is when when
everything works and we understand why
and the case where the subtlety is that
it really works right and practices
extremely good and we hope that we can
unite theory practice so that in the end
the understand so that's the end of my
talk and thank you yeah the fact that
the budget ballin seems fine this for
finding optimal solutions yeah it works
really well I mean I guess one thing I
should have mentioned along the way is
that in terms of what you want to run if
you just want to empirically good
solution for the tourists I'm some
variation about heuristic off little
increments
it's very fast and sure yeah I mean
branch-and-bound is there are you can
you would imagine that if P is not equal
then P you should be able to find such
instances and I think there are so Sarah
is that it would rather lose the
solutions I mean random distributions
are easy in the sense that if say you
draw points uniformly from the unit
square when we understand the behavior
of the TSP on such as this is really
really well it's known that there's some
constant times the value that or some
constant transparent so that's not the
best settings to grab high dimensional
maybe mean again that result that I just
mentioned for the unit square has some
expression in terms of two-dimensional
space as well other questions listen we
have not necessarily I mean so the I
mentioned for asymmetric TSP II just a
few years ago there's this result by
inari and vice Quran and that was not
algorithmic I mean I tend to think of
course when I think about such things
I'm thinking in the other direction I'm
thinking about an algorithm that starts
with a LP solution so it's always going
to constructively it'll
constructed little prove the integrality
gap by giving an algorithm but one could
imagine a cruise that doesn't use it
great thank you thank you so much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>