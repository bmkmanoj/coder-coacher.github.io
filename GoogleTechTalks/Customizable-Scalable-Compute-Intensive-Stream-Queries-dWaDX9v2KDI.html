<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Customizable Scalable Compute Intensive Stream Queries | Coder Coacher - Coaching Coders</title><meta content="Customizable Scalable Compute Intensive Stream Queries - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Customizable Scalable Compute Intensive Stream Queries</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dWaDX9v2KDI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Jonas Coulson I work here
google haha
and I'm very happy to introduce my
professor to a rich here from Sweden he
has somehow somewhat a Silicon Valley
history he used to be h HP Labs in the
database lab where it's very interesting
work on kind of object relational
database system and main memory database
systems and he's also been problem in
number of startups in AI and knowledge
management and now he's professor in
Uppsala in suite database lab formerly
it was in lynn chirping and he learned
me a lot about databases especially
database implementations so really
interested in math you can have a really
good discussions but afterwards wherever
so this is a top which is recorded for
the open audience of google videos so
please be careful both questions you ask
so that was going to take confidential
we have a ability to ask those questions
after talk is finished if you have
anything so you must be careful but what
you mentioned and the bad side that's to
rehab the floor here okay thank you very
much for the introduction yes I'm going
to talk about project actually two
projects that we have been working on
recently the type is customizable
scalable computing tensive stream
queries this talk is mainly based on the
first publication here with me and
milianna even over it was in vldb
conference in Trondheim which was
sponsored by Google I think and that's
the main part now milianna have moved
over to walk to hold onto CWI and we are
continuing with a follow-up project with
your exciting i will mention
little bit about what you are doing that
but this is more ongoing work second one
okay so this has been recognized a lot
recently that we have new very
data-intensive applications and where
data is not stored on disk as they are
in regular relational database but they
come in large streams and what we have
in particular interesting like satellite
data or space data scientific paper you
have you have things like different kind
of measurement the map of devices that
that manipulate is data coming down
patient monitoring stock data process
industry and traffic controller other
examples but we are mostly interested in
the scientific porch so at the first
party and the interesting thing with
these new kind of applications is that
data coming streams rather than being
stored on disk and this thing can be big
so then have been lot of interesting in
doing something called a data stream
management system you know we talk about
database management system a data stream
management system is something which is
actually rather similar to database
management system but it is also
different and this picture is actually a
modification of the major textbook on
databases in fact but it's now instead
of the database management architecture
data stream management or structure so
on the top it is the same and we have
users and programmers and they post
queries to the system that's not SQL
maybe something else and then you have
this big piece or system software called
the data stream management system or the
dsms and it has some software to process
queries and if it had been a database
management system there will be a lot of
software process disk but here is
instead software to access streets and
exactly as a database management system
you input some metadata schema and the
input basically what it does it accesses
streams and the result of a query in the
relation database is a table here result
of aqueous three so three mins remote so
it's really rather similar but actually
now you want to see what kind of
applications have been looking and not
in you know in Sweden for violin and
I've been looking for a real killer I
mean real challenging applications and
he lay we got it in Uppsala where they
have a lot of physicists so this is what
they're doing in Harlan they are
building the first in the world if all
software radio telescope no moving parts
instead of having these big steel sinks
you know what they are doing is they
putting out a lot of transmitters in
this in this pattern here every dot here
is a transmitter station and there's a
better or some there's some software and
hardware over there and it receives data
in all directions from space collects it
into fast ethernet cables and it were
collected into a central processing unit
in groaning and northern Poland and and
so now the stream rate here is pretty
enormous it is 20 terabits per second or
together and they really needed a good
computer so they bought one of those
blue jeans IBM blue beans and that's
what you're running on actually these
days and actually it is it not even a
standard a blue jean it is a specially
configured IBM blue jean for handling
large stream voting because the blue
team is really for number crunching only
here we want to deal with streams and
what we have been working on it now and
now you can actually see what society
sort of astronomers do they're looking
for phenomena you know things happening
up there and what they basically are
doing is that they are putting filters
on the streams I want to see when there
is this kind of solar storm or something
yeah and so this can be seen as a stream
query but the way you express the queers
isn't rather different from from SQL
because you have this rather advanced
interpolations the strongest knows how
to do that has to be possible to plug
them in and it is also very challenging
because there are many computers
involved there are several chapters
involved and an enormous amount of data
and it's much more fun than
just business data from our point of
view and here the reason came into it
because there is a associate project
called the lowest outrigger in Sweden so
the physicist in Uppsala they have been
setting up they're making their
specializing in antennas they have
special kind of these are software what
software antennas where you can program
the direction where there are sensitive
by the way this telescope you know so
you program the lens in software and
then you can actually program the
direction because if you want to look in
a certain direction there's going to be
a difference in the time it was going to
interference between so you can program
direction so you can actually say I want
to look in that direction and then you
can look more carefully there and
similarly in fact these antennas you can
program in different directions they all
software directly so anyway so they it
is connected to the it is incall
collaboration with the middle of a
project and they are actually have set
up with some time this is the real photo
in fact and this data is being fed out
into the internet in a UDP stream and we
are actually in creating this the
immunity so let's sum so order in that
will do that ok so now the first thing
to do is to look at requirements well
first the real challenge here is the
scalability because the data volume now
suddenly something I've never seen
before in the database working on in
fact the insert rate is pretty high in
relational database you know it's for
business yeah you want to insert quite a
lot of Records and you want to access
some pretty fast but it is not at all
this rate we're talking about
little one is much bigger and one
effector this is now you can talk about
even extreme databases why don't we just
store the steam on disk but you know in
this case no it is too much they either
have to be reduced before you can be
stored on disk and another thing is you
cannot religious 241 steam database
management system that you cannot create
do join in the regular way instead you
have to look at pieces of the stream at
the time that this was called movie
windows so you always look at windows at
slides on the streams and that's why you
make the queries that's how it works and
then all these streams 11 apply
different kind of filtering in the field
rings or when there are regular field
rings like like Henry legislature basis
but there are also special kind of feel
things that are made by the astronomers
you know to do you see in the process in
the different kind of silly reduction
algorithm and you want to combine very
important here is actually joined
because what I said you know when you
aim their the telescope you look at
difference in time of the signal of this
lead from from different different
elections and also at the most the
introduction combination has to use
user-defined functions it is not enough
to just have except language like in its
period and it was it has to be scaled we
will respect to data volume but the
problem is that these algorithms they
have four combination filtering they're
all expensive to touch the scale when
you have experienced the computations to
if a relational database the
computations you're doing or very small
it is comparing signal of different
kinds an indexing is amazing but here we
also have rather expensive computations
being done in real time on the streets
arrived so this is requirements or
dependency
so what we did down there and this is
the first this is the main part of the
talk and that's the vldb paper something
called agreed stream data manager it's
basically is a stream data management
system that is specialized for very high
volume scientific data and as other
stream database management system to
define continued squares over them
filter conditions and you allow user
defined functions very important because
you have to be able to plug in your own
see call now to get the performance this
is a parallel or distributed system you
can't you can it is not running on one
know that is running on many nodes orbit
or even number of nodes and they can
communicate with each other each other
and the system can dynamically start new
nodes or even kill gnus know it's
completely dynamic city dynamic moshe
texture and what I mean what the paper
was mainly about was how do you define
the computation in such a distributed
and parallel environment and what we
have it's a little it's a way called
data flow distribution templates it's a
way of forgiving algorithm you know
foreign functions have been used in
databases for long times you can easily
define your own your own code at the NC
and the new sneakers but the problem is
you want to paralyze it you also want to
tell the system how to paralyze not
really and this depends very often on
the algorithm and I'm talking a bit more
about that so there has to be a way to
customize the parallelization as well
and that's what data flow distribution
templates are about and of course in
order to get the performance we need
their the end in itself has to run in
main memory then it's not time to save
on this you know so here is nori how use
the system so you're a scientist to the
right here and the scientists talk to
some kind of a child program and will
get the result back of the query
normally sav silylation program you
don't want to get test numbers that you
have some kind of graphics for that
and then we have we have a coordinator
that is the important ater and then we
have access to some cluster or can be
more than one in fact but right now it's
only 11 cluster and here is a radio
signal now and this is very high volume
no so what happens then is at the client
specifies the continuous query and sent
it to the coordinator and I coming to
the details how this is done later but
basically use these templates to define
it the coordinator when you say compile
the coordinator generates a logical
distribution grab this is a data flow
distribution graph describing how to do
the computation and the system knows and
forgive and queer how to parallelize it
so here it is using this template to
generate this load with you draft now
you want to run this you say Rob and the
first thing that happens is wrong is
that the coordinator download the code i
execute that graph in through the
working mode here it starts for working
out in this case it downloads different
pieces to the different nodes but it
starts them first and then it downloads
now download it no and here you can see
well you can get an idea what is
happening is doing partitioning here we
have to partition this institution zero
action once the partition and then we do
parallel computation here in this case
is fast Fourier transform it's not the
normal one it's a 53 because of this
antenna which are three dimensions and
then what what you want to do is to
merge together and this is how you get
speed out of a 50 because 50 it paralyze
it you can get good speed out
ok so we install it but you can do that
in different ways and it runs and then
it starts to flow so data flows through
here and get the insulation value but
everything is completely dynamic ok and
this is how an old log star away so no
one has a engine the query X if you turn
it around the loop to receives with
logical windows I mean schedules there
there are some incoming there's need to
face call the stream consumers and here
i need the face to what they call it
beam beam it's a strong numbers word for
radio signals for me yeah you look at
the bead so there are two beans coming
in but it can be another D Estienne mode
and the result can be an application you
know it's looked on the previous page
here this one no so that's what you see
application here oil can be another DSD
emoji
and they need to face yes that being the
merge data from a separate independent
or is that each separate
actually if you look at how it is really
done in practice we're in the Swedish
project it is one of them yes in a
little touch I think they have a station
but where they first do some combination
before they sell it to the central
computer but this is how I mean the
problem here is also that you can
configure these things very differently
and this is part of the optimization
problem where you want to run walked and
we're going to collect work what I
wanted to show these pictures that we
have something like an engine that can
you can have interfaces to impose
three-minute the face its output streams
and you can combine them and then the
problem documentation problem is how
many reason is supposed to start where
and when yeah and by the net plug is
very important because you have to be
able to plug in your same code your
applications in order ok so now the data
for distribution templates is basically
high level primitives and we provide
where you can define your own execution
strategies you can for example defined
as i will show how to title IX st in
three in two different ways and now it
turns out the following which is very
interesting a data flow distribution
template can be independent on what you
are going to compute and and for us
that's something which is built into the
system it's what we can look at it like
a customization to you know the generic
one and we have defined one which is
paul window distribute and this is
actually what people are doing with
distributed databases you we do
round-robin you know first three first
window did there than that than that
than that so it ranked by robert e
distribution window and that's the fft
parallel in each window well the problem
with that is to do windows not smaller
so 50 will still be slow so it turns out
that if you have something advanced like
even such a simple thing as FFT that
everybody knows you wanted like to do
the partitioning customized and so
window split allow you to do your own
customization here and now it is to
split the big window it's smaller and
this depends on the algorithms be
running and for fft there's a special
way of doing that for other algorithms
it
that's Windows key and then the
interesting thing is combined with all
into something called the PCC partition
compute combine it is very very common
that's what you want to do is first to
partition then a parallel computation
and then a combination as a role and it
turns out to be this one is defined in
terms of the others and this is what you
normally use so we provide PCC but
actually PCC can then choose you you
specify what what variant of PCC you
have I come to that so stream carry
function is where the computation is
being done here I just have on a very
high level in fact definition of such a
screen print function seem very question
basically take the call total of one or
several logical windows combine them and
produces a stream of logical windows and
what we want to do in this case if we
take the current winter one input stream
in fact and then we apply f50 here you
see x y and c you remember with
three-dimensional autonomous we're going
to do it in XY and z coordinates so we
pick up that component of the window we
put the time stamp on it and we compute
a new radio window but the main
computation being down here is a single
air 50 now it's a regular 5073 it's
really regular FFTs in our case yeah so
we have it that's why i call it 83 very
simple so this is a very small program
effects of course somebody has to
implement a 50 but as well known okay so
the idea here is to define customizable
distribution temples and the continuous
queries are defined as compositions of
simple functions and what the system
does is to assign logical sides patterns
like this you know and basically what
ADF DT is it's a constructor it is a
constructor of this structure data
structure we want to say we want to do
something here and then this connects to
that so you basically just construct
this graph so it's actually very simple
program and PCC is that it's very very
it's a parameter eyes constructor
constructors take parameters and it can
in general construct these kind of
and you want to scale here the number of
parallel computations of course then you
pay pricier and the price here and this
you also would like to parameterize if
you wonder what is 256 is that's the
size of the window if you do this round
robbing thing it turns out what you're
doing is to send the same window size is
256 in it's going to be 250 650 and it
will be for size 256 out yeah okay and
this is how you use it so it was also on
the double sliding first you say I want
to do PA CPCC I want to use four nodes I
want to do this three in this commute
and it takes a parameter round-robin
partitioning yeah that's a function yeah
all our functions you take that stupid
function it takes our partition as a
parameter and then we want to apply in
the middle at fifty three and then at
the end we will weigh as much and as
much actually takes in timeout parameter
Oh point one see the parameter can be
functions or it can be numbers and then
we define the stream and that's roll it
straight for you want to read this to
some input stream it's called rated it
so kind read radio signal it's on that
port or host address and it's a UDP yeah
because that's what they deliver the
lowest product and then there is one one
two three five we have the visualized so
that basically say it is listening
according to some kind of association
protocol which we implemented i know and
i'll compile it compile this this query
we didn't put on a powerful any water
running on this cluster which is our
cluster computer
so that i know that's yeah you can we
determine how many knows you
you have to aesthetically something she
said for know they're even in a pain PCC
you do do that at the end i will have
imagined that we had done yet another
one which actually typically this is
also determined by how many resources
at least that's the way to know
somebody's we have to reserve certain
amount of notes so you have to know in
any case how much resources yet so in
any case you have to set the number
needed along your sources here change
during live
no on our that could happen that could
happen
we have to be a living sources and oh
sorry for the moment yeah repeated
questions professional ah the question
is can i dynamically change down on the
most during long time not in our current
very implementation and furthermore the
way you use the cluster in Uppsala we
have to use the cluster we have to pre
reserve how many nodes we can use we
have we have a certain location of nodes
so am typically what you would in any
case this a maximum
like
open it please you have to transmit them
some copy down you using these pigeons
yes so have four pockets of four
machines on this cost of the waiver
argument on the river god no no
absolutely not that's why there's a loop
you know okay there's a required that's
not able to conditionally like three
first of all if you want to do a
parallel implementation of fast fourier
transform it better return the same
result or at least approximately the
same result yeah and furthermore
indifference to relational database is
not an important thing with streams is
have an order they come in an order we
are not going we don't want it to
reverse order of a logical windows
necessarily and for for it to be good in
any case you know it has to do some kind
of load balancing you know if one note
is doing all the work we haven't gained
anything obviously and so examples and
window distributing window split have
this property okay so let's do the first
simple one that everybody knows about
round-robin window distribute this is a
generic method it can be applied used in
many needs to be the databases very very
standard and it is sqft independent
independent on what you're going to do
and the only thing we are doing is to
distribute the logical windows to
different nodes depending on arrangement
round-robin fashion and then the
computation is being done in parallel on
those and then it's collected and
basically we have a unique frankness
distribute and we here is a partitioning
function and these computation and then
emerged in the time out
so basically it consists of three
reports distribution phase a computation
phase in merge phase and this call will
generate this graph and once again you
know or or pork is a user-defined
function on the next slide that says
this is 0 to 1 out of 2 so every second
goes there every second second goes
there the size of the window is going to
be the same or so this is how defying
its rebuild it patiently says it's a
sliding window that's very important to
by the way and you're sliding with a
certain step to take a certain amount of
data and you get an array back and then
we select else the piano number all out
of that array sliding window basically
collects the incoming email and on three
two arrays of a given side and then
jumps forward but that's it you know
select piano out of a toot no he's
writing of course sliding windows built
in okay so we will not split this
interesting now it is dependent because
now we want to customize this we want to
do this but you want to do it so we can
we can use use it for the FFT now FFT
you cannot you can use window distribute
doesn't but it will not speed up as much
as if you can make the window smaller
and it turns out to be something called
the f F F T radix algorithm that allows
you to do this and now we do the
partition and coordinator 50 ratings and
basically it is to say but the only
difference is PCC but we use something
called operator split and operator join
and here the partitioning function is
called fs3 partitioning and this is
finest qualities we combine and these
are much more complex than than just
round robin because round robin just
selected so here basically what happens
and it will do fft petition 0 to 1 out
of 2 and then do a 53 in same a 53 and
then the combined here is using the
combined function as an organized an
interesting thing you know if the
incoming size or the windows 256 here
they will only besides 128 and since you
know the complexity of fft is n log n
this is going to be faster yeah but the
problem is that it's not it's not at all
application tent anymore and here is how
I 53 part actually is implemented or
looks basically it takes a window and it
put it picks up the x y and c coordinate
and then it applies a user-defined
computation here to pick up actually
picks up sly elements in a certain order
and then it forms a new radio and oh so
this is only on the high level how it is
done the real work is done by this
affinity port on one but because I said
it's XYZ is 350 but you get the idea you
know you define your own partitioning
function and the interesting thing now
is that to really make these powerful is
wherever you put a function can put the
template so suppose you want to make
this thing this is actually PCC s
distribute with round-robin partitioning
and we are calling TCC in the middle and
it gets this argument we are doing a to
hear distribute partition so it's twice
recursively doing distribution into so
now you can see what you can do where
you can create enormous patent search
very easily but I think you can call it
recursively now if this is faster or not
is a different story but there are cases
where this is faster now it turns out in
our case here it wasn't that necessarily
but could be and then I think then we
get rather small windows in the middle
layer what you can gain here is because
we speed up the distribution phase
compared to the other one if there's a
high cost on distribution and the same
for merge get more balanced on the other
hand you use more notes but you get the
point you know by having this recursive
ich an extremely complicated pattern
compare this to relational databases
where it more chaotic you know you try
all kinds of orders here we have a
certain pattern generator and it's only
constructors yeah
let's go on so we did some experimental
result and they are we did window split
we know this will actually central by
the way it's another template it's it's
really the distribution is null and the
joint is null yeah so they all expressed
as template so centrally execution we
get us very three builds the special
case and we did some scalability we were
we actually on the mention in the paper
was only a maximum throughput of here
also we can change you know depends on
the metrics if you want to do delay can
do that as well which has been done in
fact and what the strummers want to do
they don't want to use to only 256 size
because remember what they do and I have
a raw signal and then we do Fourier
transform they want the frequency
spectrum and then you want to tune in
the frequencies you're going to look at
and and the bigger window you have the
more you can choose so you a bigger
window you have the least the most
morning for me the more information have
retained you lose less information so we
would like to do is to scale the window
size and we scale it quite a lot to 16 k
there and then then this is heavy duty
and we measured in type of communication
and computation and another thing is the
speed of the algorithm does matter and
we did this royal simple we took FFT and
we introduce some artificial delays in
it so it became slower to see what
happens when it gets slower than and
this is the first it's just a to you
know split what I had on the picture and
not you can see central is lower
apparently and we in the distribute is
slower than window split you don't see
and it it depends on actually how much
we need it slow down the algorithm or
enough eventually she has slowed it down
or the difference between these two
would have been waiting
okay so now we did degree before instead
we split it in four and here we also
tested the tree the one the recursive on
and it is still the slow computation and
it turns out that if you look at the
error when you have small windows the
window split wins and you know this is
exactly what to do in distributed
databases you have small records window
split is a good which is a standard
round robin its its standard distributed
database splitting it's good but once
you start to get into complex
computation once the computation starts
to matter then the window the window
split wins window distribute wins for
small records windowed window split for
big yeah no big windows and one can also
see it here you know that window
distribute is slower here on the other
hand the problem is that the
partitioning combine algorithms are more
complex in this case now because it was
very simple partitioning and very simple
combination so the thing is the more the
more you work you have to do them the
better it will be to use windows blitz
okay we we did also with a with a fast
I'll optimized implementation of a 53
and here it starts to show that actually
the tree is passed
on the other hand then it's not so heavy
computations as before and you also see
that for small Valley if if you think
it's the same picture it is just shifted
and we can just deal with much bigger
windows now that's the main difference
so for small windows you still have the
distribute it's better but for big
windows and the bigger windows the
batteries is to do split so apparently
there is a trade-off between one or the
other and I can also see it here on this
graph set the bigger the size the more
overhead you have on there 15 and this
is the cause of the complex is over 50
and and then the split Alvarez the split
the partitioning starts to pay off okay
that here here we have a lot on it where
will you see how much is the speed up
compared to Central case and you see
there's a window of opportunity there is
a wind of opportunity for the window
distribute here but then it goes
launcher it is bigger by the way you see
this point over here that's three
distribution the problem with where the
three distribution use much more notes
and a and the thing is you would like to
do the splits strategy when there are
some limitations on how many nodes you
have if you have some computation
limitation and so you have to have a
trade-off here you know if you are doing
too complex or two big patterns you're
using too many nodes and then it might
not pay off complimentary notes you have
yeah now if you want to do get related
work to this first of all there's a
whole bunch of work done on on data
stream management systems recently and
our roller is probably the most famous
one well I shouldn't say that is one of
the most famous ones it's a stone
breaker and company on the on the east
coast and there is a stream project at
Stanford on the west coast
and there's a gig a scope at AT&amp;amp;T and
there's a telegraph CQ on the west coast
by stonebraker former guy and basically
the first generation data stream
management systems they were all central
it was like a central database but they
deal with streams so the main difference
here is to have a distribution and that
they can do advanced numerical
computations we are not just dealing
with simple things and you can have
large seen windows interesting is
recently people a bit starting to look
at peril and stream database systems and
it's a flux at Berkeley on the East
Coast and West Coast and the borealis is
the successor of Aurora on the East
Coast and basically what they mostly
been looking at this load balancing and
fault tolerance and the main difference
is that we are looking at these
expensive computations and how to use
customizing distribution templates and
then of course in a distributed database
team they have been dealing with
partitioning for a very long time and
they basically have only generic ways of
doing it and they always assumed small
windows or small data records run
robbing hashing range in our case we
provide that to you know that's just
another customization conclusion so we
have been looking at how to do a
continuous queries where you have
extensive user defined functions and
it's not enough to just implement them
in C or something you also have to
specify how to distribute distribution
patterns and this is the answer this
language or constructor set of
constructor templates we have where you
construct the distribution templates PCC
is the is one Guinea research one which
is a very common pattern when you first
distribute and then combine and it is
yet another one only and then it is d
Phi then you use that in combination
with either window split which you can
use for any function you want to compute
or we have done a special one no window
split this is computation dependent and
we have implemented in 550 radix we
measured on that
window distribute is a generic one that
would be in a distributed database as
well and this in application independent
the split is dependent the split is pair
is good when you have expensive
functions like their 50 which is lower
than heavy goodness whereas a distribute
is pretty good for small operations it
turns out and we're actually the size of
the window doesn't influence the speed
of the algorithm that's about that so
you mentioned something else here we
have now this has also been done in fact
you can define another template could
opt and here opt bhalu teasing me I
wanted to optimize PCC we will respect
to FFT three and what does is to well it
looks up what are the templates defined
453 and then it varies the parameters
and it runs a profiling program it
stores the profiling data in the
database in the same database in the
coordinator and then it uses the
cheapest one and okay you know this
takes ten minutes but then you run it
yeah why not so that's how you so you
can do specialized optimizers this way
and it's this is not complex it's about
200 lines of code or something so I want
to try to talk a little bit about
another ready the continuation of is
very exciting thing called siskiyou
supercomputer stream query processor and
as i said earlier in the loofah project
this is mainly together we loop for the
other one was primarily in collaboration
with luis but these products are
together and they have then this IBM
blue gene which is a very different kind
of computer and having used to before
not very much but somewhat different and
we want to do the same thing you know we
want to do scalable search and
processing for high volume data and this
is scientific data certainly scientific
data astronomers such the same
application area this is how it looks so
it is really standing there and how it
looks is running in fact
what is special with this new computer
is at it first of all there are twelve
thousand processors in Holland but
Lawrence Livermore you know they have
hundred and twenty-five hundred twenty
eight thousand lawyers little more
that's the biggest one but this is the
biggest in Europe the twelve thousand
processors now one problem with so many
processes is a energy generates a lot of
energy so what they have done is to
deliberately make them slower it's only
740 or something megawatts so they are
rather slow but that also means that
every processor only uses 10 watts you
have to compare that to PC just hundred
words so all it slow however it turns
out that it's a fantastic communication
between them it's a gigabit network
between them so it's very very fast
communication between these nodes and
you have about half a gigabit gigabyte a
main memory and each node currently but
that can be increased yeah another thing
to make this lightweight this system
they actually do not have full UNIX at
all on this they have a subset of UNIX
but it is really a subset which is very
nice so if you don't do certain things
you can run your standard units program
yes and so but you cannot use threads no
no you there are different kinds of
nodes in fact some notes are
specializing in communication with other
nodes and some notes or communicate
specializing communication with outside
world and some processors are doing
computation so you can you can do
different things on different nodes and
there are different kinds of
communication channel so you can choose
different way so community or
communicating between these so they say
there's lots of tools Hiram's hammer
lots of hammers but you have to know how
to use them and the way some people do
this is they do an assembly program you
know to what to make very very fast
implementation on a blue Dean or
whatever they want to do like some kind
of signal processing pull over that has
been done you know
and also it is said by IBM I heard that
said this is not designed for mainly in
dbms because it's it's much better when
you have a synchronous processing and it
turns out that this is what we have
stream processing a synchronous i should
say something more the the particular
blue team they have in all and
especially especially configured for
stream management what that means is
they have a lot of in incoming
connectors are critical the gateways but
Reuters you know so they are locked
incoming cables basically into the
computer and also out much more than 11
words what i heard is if you try to do
the same with it with hundred and three
hundred and twenty thousand processors
heaven and live a more it will be very
hard test and also there's a trade off
your hand and you notes you can have
depending on how many input connection
you want to have so and this is how it
really works which even makes it even
more interesting you see on the top
layer atop that we have the input
streams from the antennas now
distributed overhaul and they go through
this fast internet into a back-end
cluster and the backend cluster is just
a regular Linux cluster regular IDM
Venus cluster and then there is a fast
connection between the backend
chattering the blue jean and in the blue
team this data reduction is being done
and then reduced data goes to the front
cluster and then the user will see it
and now in our case you know we are
basically submitting stream queries and
it is going to start off nodes all the
way not in the not in input stream but
from the back end on the bloating and on
the front casting all three of them and
these are going to communicate with each
other yeah I'm both the front on the
back end or Linux regularly image and
here you can see how what are the
components protocol preparador Zyzz what
this pre-filtering made on in the back
end denotes
and then we start a bunch of SP here's
stream from the stream processors oh
that's the same as working notes in
GSD&amp;amp;M and there it's a coordinator
computer coordinator which starts this
and one of and this is stream the big
arrow and there is a query mosta which
collects the stream and sends them over
to the front stream processor and then
order to the clyde and here typically do
these realization here you do data
reduction he also designed data direct
reduction one particular problem with
blue jean is that you have to start
exactly the same software on all nodes
no it's the same these are identical
executables and but this fits well with
the deal with our model because what you
do is then if we start the same
executable and then we install code that
is being interpreted on each node so so
even though they are they have been
thinking of it like heavy running the
same software on each node when every
node we are able to run almost anything
by the way they using MPI here native
MPI for communication which is pretty
interesting fiction and there are lots
of hook ship so research is you're here
or you have to include hardware
specification in optimization like how
how to use the communication in a good
way how to use the different mood
specializations in a different way you
know that you do different thing in the
back end in the blue jean and in the
front end for sure and then you have
these limitations on the OS and what are
the opportunities you know there are no
threads but instead of having many
threats one can use many process
solutions instead yeah so it's very
cheap to have more processors instead
and you have lots of main memory and you
will talk to match the streams across
several different clusters and you want
to do customization exactly as we did in
bsdm here as well actually so that's it
that is not an issue but Yuri stop it
yeah how we live Russia's dash dash no
you know apparently google crashes in a
disciplinary the most
Oh
yep to cancel
it is tradition yeah do a second yeah
the google split split I see
maybe
is this what you need Atticus napkin
which are forever yeah so this is
template I mean this is my drug us using
your NetID because you know you can
staged that he's a coconut respect I
think that you do yes where you did
 my chauffeurs do is to it
haha well I rather than said what is the
difference between speak this is maybe
that's the problem in this case is going
to do a split there when it is Pippa
what is the difference I don't
understand the question of what is the
difference between click on this device
is bigger wave have a clear idea this
religious can do regular 30 but each
around around coach
Tekkit overall it takes a term which in
function
this man has the knowledge
and when the core we customize
and then
I mean I know you have some people that
you're using so the river and of course
the firm for my quantity that is another
kind of distribution pattern action
more questions
so thank you very much for attending and
you can watch it again on the movie
thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>