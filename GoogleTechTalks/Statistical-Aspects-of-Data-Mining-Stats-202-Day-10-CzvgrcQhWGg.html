<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical Aspects of Data Mining (Stats 202) Day 10 | Coder Coacher - Coaching Coders</title><meta content="Statistical Aspects of Data Mining (Stats 202) Day 10 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical Aspects of Data Mining (Stats 202) Day 10</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CzvgrcQhWGg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome to lecture 10 so we didn't
have a class Friday so today we're gonna
start something new which is the chapter
on classification IQ chapters four and
five are both on classification I'm
going to do some of both of them chapter
four deals with basic classification
defines the problem talks about decision
trees and then Chapter five gets into
some of the more modern techniques so
we'll start with chapter four initially
there is a new homework if you're
interested in playing along with the
homework that's posted on the webpage
but I'm just going to start into a
chapter four the main main concepts here
are talking about what the
classification problem is how you how
you fit decision trees and how you
validate models then in Chapter five
we'll talk about some more modern
approaches the what do you want it's on
okay so if you think about the
classification game generally you have a
training data set in a test data set and
the test status that is unlabeled now
what I should say though is that often
this this distinction is artificial
because what you do is you always want
to know how well you're going to do on
your data you don't have new unlabeled
data or if you did you want to know how
well you're doing on it so what you
often do is just segment your data into
a training set and a test data set and
pretend the glass label on the custard
is unknown and we'll talk about that in
a second but for now let's just say I
have a training data set in the test
data set and in this data set you see
there's three attributes attribute one
two three and the one that I'm really
interested in predicting is that the
attribute here called class okay and so
I have some sort of a learning algorithm
and then I learn a model and what I'm
doing is I'm trying to use these three
attributes to predict the fourth one the
class as accurately as possible but not
as accurately as possible on the
training data of course I want to do is
as best as possible on new data so this
is sort of a pictorial representation of
what's going on I learned a model on the
training data okay then for each one of
these new observations I can fill in the
class based on the model I learned on
the training data and see how accurately
I can I can perform so that's sort of
the pictorial representation to make it
more precise classification problem is
the following given a collection of
record just which is my training data
each record contains a set of attributes
X okay so X's are your attribute
and of this course this could be
high-dimensional could have a lot of
different attributes and then there's
one particular attribute in particular
that I'm interested in which is the
class y okay and I'll talk about binary
classification so Y will take two values
you can extend most binary algorithms to
handle more than more than two classes
but it is important that Y is
categorical and not numeric if Y is
numeric then we course get into sort of
a regression setting we're trying to
predict a numeric response so Y is a
binary class variable and then we can
extend the binary case into multiple
classes so we want to find a model to
predict the class as a function of the
values of the other attributes okay so
it's a prediction problem we haven't
done predictive modeling until now we've
always been doing descriptive statistics
this is a prediction problem and the
goal is that previously unseen record
should be assigned as actually as
possible and so how do we know how well
we're gonna do on previously unseen
records well again we take some of our
data we sometimes you call it a holdout
data set or a test data set you leave it
out and then you see how well you can
predict on that so that's sort of a fair
game because the model hasn't seen that
data before so you leave it out when you
fit the model but then to assess how
well the model is doing you fit to that
see how well you do on the test data set
and that tends to be a good metric for
predicting how well you're going to do
on new data so it says you're usually
the given data says divided into
training and cus that's with the
training set used to build the model or
learn the model or fit the model and
then the test set used to validate or
measure how well you're doing so as an
example credit card transactions right
you might have some data on previous
transactions that you subsequently found
out or this was fraudulent or this was
legitimate so you look at all your X's
all the covariates and then when you see
a new example like that come along that
your model predicts to be likely to be
fraud then you flag it as fraud right so
when your credit card company calls you
and says we've noticed some unusual
activity unusual is in their opinion
unusual it's some classification
algorithm that's identified that these
covariates are predictive of a
fraudulent transaction so it gets
flagged by the algorithm some example
from chemistry classifying secondary
structures of protein as one of these
cleats as one of these on one two three
one two three classes again you know I'm
going to talk about the to class problem
but you can extend those to three class
and general K class problems
categorizing news stories as finance
weather entertainment
sports so you can imagine applications
of that and then there's Arnold
Schwarzenegger from Kindergarten Cop
right remember he has the tumor and he
says it's not a tumor but either it's
been what I can't say that either benign
or malignant benign or malignant okay so
there's many algorithms for carrying out
classification this chapter somehow we
deal with decision tree is the question
of why do we start with decision trees
it tends to sort of be I think more
historically accurate that's kind of the
first algorithm that people started
using for handling classification it's
certainly not the favorite algorithm
anymore but it is a sort of simple
algorithm it kind of tends to be
computationally efficient in some
aspects and has some nice features so
we'll start with decision trees but then
in Chapter five we'll start to study
other techniques that are more modern
and effective and by saying that it
implies that decision trees are neither
modern nor effective and that's true to
a large extent I don't think that there
are sort of the favored algorithm of
choice anymore but in terms of going and
chronologically and started with
something simple we'll start with
decision trees and then we'll extend in
Chapter five to some more modern and
effective techniques so what does a
decision tree look like well obviously
it looks like a tree and so here's some
data you might imagine where you're
trying to predict whether or not
someone's going to cheat on their taxes
and you have three attributes so X has
three components it has whether or not
they applied for a refund whether or not
there's single married or divorce and
then their taxable income and then
finally the attribute that I'm trying to
predict which is whether or not they're
going to cheat so how do I do this while
I take data from previous years fit a
decision tree to it and then I can use
that decision tree from previous years
to predict data for future years and if
someone comes up as being predicted that
they're going to cheat then I can audit
them right so how do you how do you fit
new data well it looks like this right
so a new observation comes along this
person did not want a refund they're
married and they had 80,000 taxable
income so I want to know whether they
cheat so what I do you know going back
right I fit a decision tree on the data
where I have the labels I learned from
the labeled examples and when a new
example comes along I just sort of
chased him down the tree right so this
person let's see start at the beginning
at the beginning you say okay did he get
a did he request a refund yes or no and
so this person did not request a refund
so he's gonna go down this ranch okay
and so and then I asked about his
marital status right is he married or is
he single and divorced and this person
happens to be married so I follow him
out here and then the last thing to say
is well that's it right because he did
not request a refund and he was married
so I conclude that he's not going to
cheat I didn't even have to look at his
taxable income in this case because
knowing that he didn't request a refund
and he was married is enough for this
tree to predict no he's not going to
cheat okay so that's you just sort of
chase them down the tree until you get
to a terminal node then the terminal
node gives you the prediction one of
your two classes in this case either
cheating or not cheating now one thing
you'll notice about the decision trees
is that in this case right I never use
taxable income and it's possible that
you could have a lot of predictors that
may never get used so it does sort of
some implicit variable selection for you
which can be very useful when you have a
lot of redundant features and you don't
know which one of them you want to which
ones of those features you want to use
in your model okay so if we're using our
the function are part of course there's
some other functions that fit decision
trees but our part in our tends to be
the favorite these days you should be
careful when using this function though
because it also does regression trees
right so a regression tree would when
you would be when you have a numeric
response and that can be problematic
because you might think oh I think I'm
fitting you know two zero one I think I
have a two class problem and all of a
sudden that gives you a prediction of
one point two and you realize that our
part is thinking that your zero one is
really numeric and just happens to take
two values I think I think it's actually
even smart enough to give you a warning
like warning I'm assuming that this is
numeric but I noticed it only has two
values I think sometimes they'll even
give you a warning but to make sure that
you're doing it right of course you want
to make sure that your Y is a factor you
know so as dot factor and I quoted a
little bit of note a note here from the
the help it says if Y is a factor then
it will assume method equal class so of
course you can force it to say method
equal class but by default it's going to
look at what type Y is and if Y is a
factor it assumes they're trying to fit
a decision tree which does
classification as opposed to regression
tree which fits to a numeric response
and you can imagine things are slightly
different you have a slightly different
loss function you know with the decision
tree you get you get predictions that
are either 0 1 whereas with the
regression tree you get
actions which could be any any value
okay so just be careful that you're
telling are that you have a factor so
that it knows you're doing
classification not regression when you
use our part because our part does do
both of them okay so what does our part
look like well here is the text output
from finding a decision tree on some
data that I didn't show you and you can
go through I'm not going to go through
all three of these that you can chase it
down and figure out what the prediction
is of course there's a function that
does the prediction for you I'm sorry I
don't have black if anyone has black
I'll use it but I don't see any black so
I'm gonna use blue first person age is
middle number equal five start equal ten
and we're trying to predict the presence
or absence of this I think it's a rare
childhood disease so what you do you
sort of chased this person down so you
start for a this is a really bad marker
you start at node one okay the root node
and they're all 81 people are there and
then the first thing and asked about the
indentation is meaningful here right
either the start is either greater than
equal to 0.5 where the start is less
than 8.5 and this person start is 10
right so he happens to be up here you
know start greater than equal to 0.5 so
from 1 you go to 2 and then the next
question asks is it 4 or 5 it says age
old or young or is his age middle okay
and it turns out this person is middle
so then I stay here at 5 then after that
what do I have to ask
oh again it asks about start again so
it's allowed to reuse the variable and
it says his start is either bigger than
equal to 0.5 or less and start with 10
so that's gonna be less on a note 11 and
that is a terminal node and I know it's
a terminal node because it has a star by
it right nodal line 11 represents the
terminal node which has a star by it and
in there there's four people and 75% of
them are present and 25% are absent so
he gets predicted to be present so my
prediction for this guy is that he is
present present okay so that's how you
read the text output from our I'll also
show you what what the graphs look like
but as you do this it's sort of logical
how you follow it down you should also
note that it does you know some
variables aren't included
some variables are used frequently
variables are allowed to be used more
than once it tells you at each stage you
know a terminal nodes are with the star
but at any stage it tells you how many
observations there are what the majority
class is because that's what you would
predict and then how many are in each
class and then these two numbers are the
probabilities and this function works
even if you have multi class right it
would give you all if you had three
different classes that would give you
all three probabilities in this case was
just two classes these two probabilities
always sum to one but the function is
flexible enough if you had a factor with
multiple levels it can do multiple
classes so that's what the R part
function looks like in our I'll talk a
little bit about how it works but that's
how you read the text output any
question about reading the text output I
think I'll just skip B and C it's pretty
intuitive okay so let's fit it to some
real data
Oh real sort of in quotations so you're
all probably familiar with the sonar
data this is one of those classic
machine learning benchmark datasets it's
a pretty small data set I think that's
like two hundred and maybe two hundred
and eight rows and sixty predictors and
then the 60 first column is the response
and where this data set comes from
there's a two objects right a rock and a
metal cylinder
and you bounce sonar signals off these
two things and obviously a human can
label it as a rock Homer doll cylinder
but they the challenge is to use the
sonar signals to learn an algorithm that
can predict accurately new objects as
being the rock or the metal cylinder
those are the two classes so I'm going
to look at I split it into a training
data set and a test data set I think I
put a hundred 30 observations in the
training data set let's take a look at
it so here's the training data set and
yeah I put 130 observations in the
training data set and then you can see
there's 60 columns which isn't really
obvious in Excel since it puts letters
but then the 60 first column is negative
one or one rocker metal cylinder I
forget which is which and so basically
we're trying to use these 60 columns to
predict the 60 first column as
accurately as possible but not on the
data that we're going to fit on of
course on the new data okay so that is
the classification problem here and so
let's see here how do we do this in R
well the first thing to say again our
part which I'm going to use to put my
decision tree
is not in the base package right so you
have to do on Windows this works well
install that packages are part on Linux
machines sometimes you have to sort of
download it locally and install it from
a local directory but this works pretty
well on my Windows machine I haven't
installed packages on Linux in a while
so maybe easier now
anyway install that packages are part
you only have to do this once right I
mean if you reinstall our you have to
reinstall the packages again but you
only have to do this once but every time
you want to use our part you have to
read in the library ok then this is a
read dot CSV on the training data of
course there's no hetero size 800 equal
false otherwise it's going to use the
first row as the column names
then why the important part here is the
S dot factor so I'm going to read in the
61st column as Y which is what I'm
trying to predict but as not factor Y so
it's going to know that it has two
classes and in fact the classes in that
data were coded as negative one and one
so let's see here where is our there is
file change directory - I think that's
on my desktop see there there there
there there
oops I don't need to install it ok there
we go and if you look at Y you see it's
negative one in one and the key thing
there is I know it's a factor because it
tells me it has two levels negative one
in one doesn't matter what numbers you
use if it's a factor it'll adjust to
those being the two levels okay they
chose negative one and one
some people like zero and one depends on
who you are what you're trying to do
then X is the rest of the data right
exit columns one through 60 and then
here's the here's the key right the
syntax is our part Y which is of course
your response your factor your to class
vector this is the tilde and then dot
right dot just means everything right
you don't have to write x one trick so
if you just dot and then comma the
second argument is your data excluding
your response okay so that's the syntax
that will fit a classification tree or
decision tree whichever you like to call
it as predicting Y as a function of X
based on all the default values right so
just leave the default values here we'll
talk about what it's doing in turn
in a bit but for now let's just take the
default values presumably someone has
tuned those to try and do a good job and
then the last line here actually I asked
you to compute the misclassification
error this would actually be one minus
that this would be the accuracy and you
can see let's see where did I I'll just
grab all this stuff again so this last
part should be intuitive to you but I'll
just mention so on the training data
it's predicting with 88% accuracy right
so the inside argument here actually let
me point on the slide it's bigger so
this would just be why right that's the
training data why and then this is the
predict function right the predict
function takes the model object the data
that you want to predict which if you
wanted to predict for new data you can
put new data here but I'm just trying to
get the prediction from the training
data type equal class even though fit is
you know fitting with respect to a
binary class label if you don't say type
people class you get probability outputs
and so I just want the class label
outputs so I say type equal class and
then I just divide by the length of Y
which is 130 and I'll just show you that
what that predict function does predict
is a very general function and you can
use it to predict from a lot of model
objects and it works in particular with
our part here so when I say predict fit
X type equal class those are the
predictions from 130 points in the
training data if I have left off class
it would actually give me the
probabilities right the probabilities
are the two classes for each observation
so by saying that I get the class labels
and then in here you see the inside of
this sum was simply a logical vector
giving me true or false whether or not
the prediction observed with the actual
value in the data and then when I sum
this thing it sums up how many truths
there are and then I just divide by 130
to see that I'm getting 88% accuracy on
the training data okay so in other words
the misclassification error on the
training data is like 11.5 percent okay
so that is the MIS classification error
on the training data I want to make a
note of it because we're going to
compare some models so this is number 33
this is the default to call to our part
with this data and I got what I say
11.5% miss classification error so this
is number 33 ah
I said 11.5% was my miss classification
error 0.5% that's on the training data
though okay which I don't really care
about because the training data I can
just fit a really big tree and I can
always drive this down to zero assuming
that I don't have duplicate x-values in
there so I don't really care what value
this is what I really want to know is
how well this thing does on new data so
I read in the test data and do the same
thing but now I use white s and X test
and I look how often Y test matches the
predictions for X test as being the new
data okay so this is really the number
that I care about to see how well it
does on this new data which I just sort
of you know held out from the fitting
procedure so the model hasn't seen this
data before and not surprisingly I do
worse right I have 69 percent accuracy
or another way of saying that is the
misclassification error rate on the test
data is 31 percent so let's call that
miss the other one
okay so call it in this classification
error on the test data is 31 percent so
this is the default call to our part
function if you know anything about the
sonar data set it's commonly used as a
benchmark data set thirty-one percent
isn't that good right you can you can
beat that with some pretty simple
algorithms but by default that's you
know that's what our part is giving us
now this whole split between the
training the tests is somewhat
artificial and of course you all know
that there's other ways of doing that
some people do cross validation some
people use leave out one things like
that but just to make it simple I split
it into 130 observations for training
and the other I think 78 for tests and
that's one way of sort of determining
how well we think the data will how well
the model will do on new data assuming
that new data differs in the same way as
the training and test differ okay so I
think that's sort of the basic exercise
that we go through in terms of fitting a
predictive model for solving the
classification problem and assessing how
well it does and so we can say the
default call to our part for the Stata
is 31% miss classification error on test
data ok any questions about that
okay so let's experiment with the model
a little bit what if we were to say
instead of using oh you know I wanted to
show you actually sorry let me go back a
little bit I do want to show you just
what this tree looks like so one thing
you could do is say what does it look
like to say fit and you can see it has 1
2 3 4 5 6 terminal nodes okay which
predictors is it using what's not using
all 60 columns it's using V 11 V 27 254
v8 and b-52 so it's not using all the
columns so it has variable selection
built in with 8 terminal note sorry with
1 2 3 6 terminal nodes and remember it
was getting about 11.5%
misclassification error on the training
test so by default our didn't grow the
tree as big as it can it stopped for
some reason and we'll talk about how it
knows when to stop but this is what the
tree looks like another way to see it is
to say plot of fit and then that gives
you a plot and then you can add text to
it which you have to actually play a
little bit with the default here and the
text to make sure so the labels don't
get cut off but I'm not going to do it
anyway you can see there's there's other
interfaces for plotting it better but
you can see how the tree looks and it
has 1 2 3 4 5 6 terminal nodes just like
I said and you can see here the
variables that it's splitting on ok so
that's the default tree so now suppose
we wanted to try some other trees from
our parts so the first thing we might
say is well let's try a really really
small tree and let's try the smallest
tree we can think of which is a max
depth of 1 so that's just gonna split
the terminal node into two parts our
part only makes binary splits when it
does each attribute so we're going to do
a max depth of 1 which means it's going
to restrict 2 trees of depth 1 which
will only have two terminal nodes and
see how well this thing does so this is
about the simplest tree you could think
of and here's the syntax for fitting
that you see that there is this our part
dot control so I say control equal our
part
actually maybe I'll leave this up here a
little bit more I say the usual call to
our part but then I say comma control
equals our part not control and if you
do a help on this thing you can see
there's a lot of different defaults you
can set one of them is max depth and I'm
just going to set next up to one so this
is sort of the best default tree of
depth at most one right so do that
measure the accuracy on the training of
the test data and not surprisingly right
the first number we see 77% accuracy or
78 percent accuracy on the training data
which not surprisingly you know is worse
than this one so if we compare in terms
of this classification error this is
number 34 right there we go number 34 if
I look at the MIS classification error
for this guy on the training data I see
he's gonna be 1 minus 78 so let's call
him about 22% right so he is only
getting ah we have a better monitor
there is a black one okay so what is
that 22% of the training data is
misclassified yeah
22% okay so not surprisingly the small
tree is doing worse on the training data
but what really counts is how well it
does on the test data and turns out this
small tree actually does slightly better
right 28% so that's my miss
classification error for test on this
one is 28% so again the fact that it
does worse on the training data is not
surprising so it's a much smaller tree a
much simpler model but the fact that it
does sort of a little bit better on the
test data might suggest to me that this
model might be preferable now I know I
mean these numbers are close I haven't
done any sensitivity analysis I just
took one you know hold out samples so we
could argue about this but at least
they're comparable and in fact I might
favor the smaller model because of that
the you know there
comparable and generally parsimony
suggest I should favor smaller models if
you want to see the fit well it's not
very exciting right it's just a single
split on D 11 right 79 observations go
here and 51 observations go here and
that's it
right that's your your fit so you can
see the picture of it again not exciting
at all just single split right to
terminal nodes that's all you're doing
but it's interesting that that simple
model where I just found one we're able
to split on actually does better on the
test data than the default tree you know
slightly but arguably better okay so
that's so you so it shows you a little
bit how you can override some of the
defaults now what's interesting though
is how these defaults sort of affect you
in some way suppose I said okay so this
was a depth one tree here I'll just
I'm not going to store it as anything
let me just fit a depth one tree and see
so there's the depth one tree then I can
go to depth two and get that and here
let me plot these so that's easier to
see but at some point it's going to
refuse to make them any bigger even if I
tell it max depth people a larger number
so there okay that's step two one two I
can ask for depth three I have it one
two three I can ask for depth before did
I get it one two three four but I think
at some point it's gonna refuse one two
three four yeah
it's not gonna grow it bigger than four
so I have to actually change more
defaults if I want to grow a bigger tree
and so that's what I did here on this
last part of this example I went in and
got I think these are almost all of the
values that you can set so there's a min
split criteria criteria and there's a
min bucket criterion I know min bucket
is like the the smallest number of
observations that will allow it to be in
any terminal node this is some sort of
cross validation criterion that I that
should be positive so I said to be
negative there's a max compete all these
things I sort of zeroed them out and
then tell it next step sixth and this
will actually get it to fit a tree of
depth six and if you don't believe me
here's the syntax and again you can do a
help on all these
different parameters in our part not
control but there is the tree of depth
six if you don't believe it's a tree
abduch depth - six I can plot it for you
and text it and you can see well that's
a mess right let me just plot it and not
text it so you can see this has depth
six because if you start counting these
splits and you go deep as deep as you
can one two three four five six right so
this is a tree of depth six it's not
surprising that it has a hundred percent
accuracy on the training data not
surprising because it's a very complex
tree and so that's why our was sort of
resisting me before when I was trying to
grow the tree bigger and bigger because
I was pretty much you know fitting
everything perfectly and Here I am
fitting exactly perfect 100 percent
accuracy on the training data which of
course I don't really care about but
it's also interesting that this one also
has 73 percent so you know a little bit
better actually than the other two trees
I looked at even though it's bigger
actually gives me you know twenty-six
twenty-seven percent misclassification
error rate which is actually a little
bit better than both of these although
you know arguably it's comparable and I
would be hesitant about fitting a tree
this large but you know if you if you
wanted to this would be how you would do
it and whether or not it's really doing
better is suspicious of course the fact
that this is a hundred percent tells you
nothing but twenty seven percent it's
not that bad I haven't if I am
overfitting I certainly haven't done it
that much that's severely and it's not
that much worse than the default so that
shows you a little bit about our part
I'll I'll give you some insight into how
the algorithm is working but I just
wanted to show you the syntax and how
you call it any questions and anything
I've said so far okay so let's talk a
little bit about what's going on with
these algorithms so I don't think you
have to be sort of an algorithm person
to realize this it's not searching over
the space of all possible trees right
you wouldn't want to do that there's a
lot of trees that you could grow if you
think about the sonar data set with
sixty predictors and splitting each one
of these on any value well it's too many
trees to consider right so it's doing
sort of probably the first thing that
you
think of a sort of a top-down approach a
divide-and-conquer approach and this
gets called Hunt's algorithm a lot in
the literature and you can read on page
152 about it basically what you do you
start at any node with the set of
records that you have if all those
belong to the same class you stop it's a
terminal node if they have records that
belong to more than one class then you
split on it according to some criteria
and we'll talk about what criteria were
used but basically this is the top-down
approach you might say well that's the
obvious thing to do but you know you
could think of other approaches right
you could think of searching over the
space of all possible trees to maximize
some criteria you could also think about
starting with the single observations
and sort of joining them together until
you get you know so there's different
things you can do but this probably is
what we come to your mind first this is
the top-down approach this is known as
Hunt's algorithm so let's sort of think
about how you do this one here so this
is this example of cheating on your
taxes and if you think about these guys
1 2 3 4 5 6 7 8 9 10 and they go no no
known for knows and then let's see 5 is
a yes and then 8 and 10 are yeses ok no
no no ok so then the first thing you do
is by default right if you just if you
had no tree at all right a tree of depth
0 you would just say they don't cheat
right assuming you have equal loss then
you just say I'm gonna go with the
majority class and say people don't
cheat ok then after that you know it's
not homogeneous because it's not true
right there are people that seed so you
split on that ok the first thing that we
split on here and we'll talk about how
we chose what to split on and later but
suppose we first cheat on refund ok so
if they ask for a refund yes
that's 1 4 &amp;amp; 7 none of those guys
cheated everyone who asked for a refund
1 4 &amp;amp; 7 did not cheat so 1 4 &amp;amp; 7 go in
this I asked for a refund category so
this with the black square becomes a
terminal node and then the other 7 guys
stay in the don't cheat category we're
not done with them yet we'd still can
consider
splitting them but the don't see people
were done with okay so then we go sorry
the refund people were done with now we
go to the no refund people okay so these
guys were done with they don't yes
refund are done with the no refund
people we're gonna split them and we're
gonna split them suppose on marital
status okay so we can take the single
and divorce people over here now the
married people
okay who's left here to is married he
didn't sheet three is single here okay
so five is not okay eight eight is not
okay so nine is married he didn't cheat
so it looks like everyone who's says no
refund and is married didn't cheat right
no refund is married didn't cheat is to
also it's not five it's not eight
it's is nine right
all those guys right they said no refund
and they're married and none of them
cheated okay so that again is another
terminal node because it's homogeneous
so now we're just left with these one
two three four people and we need some
way to split them because it's not
homogeneous so we consider splitting it
and we need some way to separate this
guy from the other guys so it turns out
we can use their taxable income right
because number three he made seventy
thousand whereas five made ninety five
eight made eighty five and ten made
90,000 so if we just take people that
made here it says less than eighty k
right less than eighty k that guy's the
only one who made less than eighty k so
he goes over here into the don't cheat
and the other three guys going to a
terminal no that's homogeneous but with
respect to cheating all three of those
guys cheated so in this way you sort of
divide and conquer until you get to
terminal nodes that are homogeneous now
of course the actual trees we don't grow
down that far but that's the direction
we're going and we're pushing towards
homogeneity and all the terminal nodes
and once you get there of course you
have to stop so that's sort of the
top-down approach that's Hunt's
algorithm it's done again in this greedy
fashion where at each stage you do well
it's usually done in a greedy fashion at
each stage you do what's optimal that
means that you don't really look ahead
so
this may not be optimal in the end even
for the same criterion that you're
trying to minimize and there's an
example on the homework where it turns
out if you start with a suboptimal split
you actually wind up with a tree that
turns out to be better than if you do
the optimal thing in each stage right
because the optimal thing in each stage
doesn't lead you to the global optimum
and so this is not in turn teed to find
the best tree in the whole universe
but it's sort of efficient because you
just do the greedy thing you just do the
optimal thing that you stage and that's
what I said last down here it's it's
computationally efficient so it tends to
be popular now once you say I'm gonna
use Hunt's algorithm going to use this
greedy approach there's still some
questions you need to answer so in
particular what type of test conditions
should you consider right if it's a
continuous attribute how should you
split it if it's ordinal attribute how
should you split it what criteria to use
to select the best split right how can I
say this split is better from this but
how should I know if I should split on
this one versus this one and then when
to stop okay so for number one that's
sort of easy I'm gonna match what our
does which is to only use binary splits
for both numeric and categorical
predictors so categorical dickerers I'll
just put some classes on one side other
classes on the other side for numeric
predictors I'll just choose a split
point we usually choose like a midpoint
between two numeric values and use that
as a split point every one less than it
goes one side you're going greater than
it goes the other side
number two common things to consider to
measure how pure the nodes are our
classification error the Gini index
which is popular in statistics and
entropy which is more popular in machine
learning then for number three we're
gonna say that for next time because
that's sort of tricky that really gets
into the idea of model selection how
deep should you grow the tree how big
should the tree be you don't want to
over fit you don't want to under fit
it's sort of a tricky question and it
actually turns out that what people do
and we'll talk about it next time is
they actually grow large trees and then
cut them back and that tends to work a
little bit better than just sort of
choosing the right point to stop just
grow a larger one and then figure out
where you should have stopped sort of in
retrospect okay so with regard to
question number one how should these
splits look again are just those binary
splits the book talks about multi way
splits you know you know it just starts
sports luxury and family but R is always
just going to do binary splits so we
have to put some
the classes on one side and then you
just put the other classes on the other
side so the counter type could be sports
luxury or a family now ordinal arguably
you might not want to split and break
the order right so small medium and
large could be legal or medium and large
and then small could be legal but it's
questionable whether you really would
want to do small large and medium
however excuse me for paging through
slides here but if you look back at one
of the examples we had right it did old
young and middle right arguably you
wouldn't want to do that but our does it
I'm almost positive there's no way to
tell our part not to do it without sort
of doing something pretty creative so
all you know we'll allow it but you know
maybe you want to feel a little bit
guilty about it because arguably you
don't want to do that yeah I mean it's
ordinal right so you you're not taking
quadratic transformations of your
numeric thing so arguably you wouldn't
want to allow your ordinal things to
sort of break like that but there's
really you know you can argue either way
you know whether you should allow this
or not or allows that some people don't
like to allow it we're not transforming
the numeric things so that's sort of
when I'm yeah you can get reasonable
people could disagree on this I think
and the other thing you could say is
well sure if you don't allow it now you
can always split on it later and break
that one out later so it could happen
anyway right so so yeah you know that's
not too big a deal but you can think
about whether or not you want to allow
that and then for numeric things the
real question comes up is where should
the split point be right if you have one
observation at ninety and the next lower
one is at seventy usually people use
midpoints but sometimes people do some
more careful interpolation and of course
it doesn't matter on the training data
but it's going to matter on the new data
which is really what you care about
so for numeric data you know you choose
a single split point and you either less
than it or you're not less than it in
this case started greater than it or
you're not greater than it and that's
going to determine how you split the
numeric attributes so these are the
types of splits that are part allows and
so will sort of go along with this but
different people disagree on this some
people allow multi way split some people
some things and other other things some
people allow a split for every value of
the categorical attribute so anyway
that's we'll just sort of match what our
does here then the next question is how
do you measure how well your splits are
doing right how do you know if I have to
choose between splitting one way or
splitting another way how do I know
which way is better well the obvious
thing you could look at is you know what
is the MIS classification error you know
compare the two and transmits
classification error and so for example
you know miss classification error here
this is sort of a general form of it but
for two classes right if the majority
class was 70% then 1 minus 70% you'd
have 30% miss classification error so
that's the obvious thing to do but it
turns out people don't like to do that
other popular approaches are the Gini
index which is to take each probability
for each class square it some of them up
and take one minus that or the entropy
which is P times log base 2 of P summed
up with a negative sign so these are
common approaches all I think I'll go
into detail on these next time but
basically Gini index is what our part is
using an entropy tends to be what things
like c-45 are using you can correct me
if I'm wrong but I'm pretty sure on that
one so let's talk about miss
classification error for example so this
is generally our final metric right in
the end of the day we judge the model
based on how is the miss classification
error on the test data so it makes sense
we might use this as our criterion I'll
talk about why it also makes sense we
might not but you know arguably you
could say let's just use miss
classification error it's simply the
fraction of total cases which classified
and also we used in accuracy
interchangeably 1 - miss classification
error is the accuracy so let's sort of
take a simple example here before I do
this one let me let me before I get into
this example let me frame the problem a
little bit just to contrast these three
so I wanted to just mention to you a
case where they differ see if I do I
have it here oh maybe I don't see it
okay I'll talk about it next time I'm
gonna give you an example with a
different question
right so when you like a each layer okay
I kind of movie stack know the best
place it's not terminal yet so there's
no classification right right right so
we look at basically the reduction in
the miss classification error as if you
were to sort of stop there right so
we're just being greedy we're just being
greedy so you know you say should I
split here well how much is that going
to reduce the miss classification error
for now but you're right it's not the
terminal node and so does that maybe
it's not helping maybe well I go through
some examples and see if this comes up
right right so in with this thing the
same question comes up with with genie
and entropy right you calculate the jr
the entry within a single node and then
you take the weighted average across the
terminal nodes to calculate the overall
right right right right right right
right right so you go with the majority
class in there and then that becomes
okay so we'll do some examples and if
it's not clear after that let me know
okay I mean okay should be clear so
let's let's start with this example
right so we'll start with miss
classification error because that's what
you asked about anyway so this case here
we have three attributes ABC now once
you pick the attribute there's no choice
about how to split right because each
one is either true or false
so you know normally I would have to
pick the attribute and we're just what
the attribute but here I just say okay I
could split on attribute a true/false B
could be true or false - C can be
shortfalls okay and then you see the
number of instances for each one the two
classes here are plus a negative you can
think of a plus one and negative one in
case of sonar and so this is actually a
problem that I just got right out of the
book and it says according to the
classification error rate which
attribute would be chosen as the first
splitting attribute so what's really
going on here right you have there's
actually 25 100 hundred things right so
you start with a hundred things in your
terminal node and you can either go and
do a equal positive sorry they equal
true or a equal false that could be your
first split okay you could do B equal
true or B equal false that could be your
first flip or
you could do C equal true or C equal
false that could be your first flip and
so the question is how do you decide
which one you want to do well you could
look at which one reduces the
misclassification error the most at that
point you could look at which one
reduces the Gini index the most or which
one decreases the entropy the most and
so this problem specifically says use
according to classification error right
so we'll see which one decreases the
classification error rate the most so
the first thing to note is that
initially this classification error is
at 50% right because if I had to pick
the majority class here well 25:20
there's 50 here and there's 50 here so
it doesn't matter either way I do it I'm
gonna get 50 percent error initially so
initially I'm at 50 percent miss
classification error rate right in the
in the initial node so that's the
misclassification error initially before
I start splitting at all 50 percent so
then we just want to see how each one of
these does well if I look at a right so
let's see a can either be true or false
oops
there's that one true or false and this
is a equal true a equal false it looks
like there let's see for a equal true I
have 525 looks like 25 here and for
equal true zero zero negatives and for a
equal false I have 25 there and 50 on
the other side equal false 25 50 yeah so
there's my hundred okay so if that's if
that were my split what I would do is
every time a equals true right I would
call this the plus class because it's
actually homogeneous right this node is
actually perfect I would stop right
there
that one's 25 to zero that when I would
call the plus class this one I would
call the negative class right says 50
negatives and 25 positives so I would
call this one the negative class and I
would be getting these 25 wrong these 25
wrong so this one would give me on this
classification error rate of 25% so my
Delta if you
think about in terms of delta is 50 i
went from 50% down to 25% semi delta for
a is 25% so splitting on a gives me a
you know reduction from 50% down to 25%
if I'm using this classification error
as my criterion we could also talk about
using Gini and entropy but for now we're
talking about using this classification
error rate okay you could also do the
same thing with B right B goes plus
minus C there true false I split on B I
think for B for B true there are 530
here and I think the true there's 20 20
negatives and then for B false there are
20 positives and for be false
there are should be 30 right 5 yeah 30
30 negatives so if I were to split on B
then I would have to say well this would
be B equal true would be a positive
class coz 30 versus 20s always picked
majority this would be a positive class
and this would be the negative class but
getting this 20 as well as that 20 wrong
okay so I'll be getting 20 wrong or you
get 40% wrong so I'm going from so so
then this classification for splitting
on B would go to 40 percent so if you
want to use Delta your Delta is 10
percent right you went from 50% down to
40% if you split on B so I'd much rather
split on a over B if miss classification
error rate is my criteria okay and then
finally that leave of C so C plus minus
either true or false and so see this is
easy cuz they're in order I get 25
positive when it's true and 25 negative
when it's true
so that's 25 and 25 and then when it's
false I get 25 positive and 25 negative
so C is absolutely useless right I went
from you know it doesn't matter which
one you call this or which one you call
this there's no majority either one
either way you're gonna get you know 25
wrong in each terminal node so you're
gonna go from 50 percent to 50 percent
so C gives us no information
misclassification rate equals 50% and so
your Delta is you've gone down zero I
think this is the one in the book I have
to check where it actually turns out
though this is where you get in trouble
for being greedy because if you split on
C eventually that on the next split you
can do perfectly so it's sort of a cute
example for that reason but if you're
being greedy you don't want to split on
C initially because it buys you nothing
at the time but in the long run and
actually turns out that C works better
but these these algorithms are always
greedy so that's the story when you
split on this classification error right
now it turns out that you can get
different answers whether when you split
on some of the other the other criteria
being whether you split on genie or
entropy if you think about these things
are using extra information about how
pure the node is other than just how
many are classified as the majority
class and arguably these things give you
a smoother criterion and use more
information so again actually people
prefer these statistics we often use the
genie index entropy we often use in
machine learning I think c-45 was famous
for using entropy there they're very
similar and we'll talk about the
comparisons but you can get different
answers depending on which one of these
you use and we'll see sort of how they
differ but both of them are trying to
sort of balance this trade-off between I
mean well ideally what you want is you
want a split that gives you sort of two
you know two large classes right so that
both you want both nodes to have a lot
of things in them and you want both
nodes to be homogeneous given that you
can't have a perfect 50/50 split and
both of them be homogeneous you have to
sort of trade-off the fact that like
well can I break off a small fraction to
have that be homogeneous versus break
off a large fraction but not have it be
quite as homogeneous so you have to
trade off sort of the homogeneity with
the fraction of observations or the
balance of observations between the two
terminal nodes and so these different
metrics do it differently I went through
an example with miss classification
error today but on Thursday I'll go
through an example with Gini and entropy
also on Thursday I'll finish up that
we'll talk about sort of the class and
balance problem we'll talk about the
area under the curve we'll talk about
that issue number three which I didn't
get to today which is how do you know
when to stop it
and I'll tell you a little bit about
what what our part does that'll sort of
wrap up chapter four and then next week
which i think is probably going to be
our last week I'll get into some other
algorithms for solving the
classification problem such as you know
nearest neighbor support vector machines
bagging boosting all these sort of
popular things that are much more
effective but once you sort of have the
background and you sort of understand
one algorithm well you can appreciate
how the others work and how they relate
to it and well so we'll do those next
week so any questions for takeoff yes
one step ahead I'm not familiar with a
specific one but it'd be simple to think
of you know looking at one step ahead or
two steps ahead again what what they
generally do though the most common
implementations I've seen are just take
a greedy approach and then prune it back
so in that way you have looked ahead at
least along one path and then you prune
it back some other people sort of will
do like random initial starts and sort
of do put some randomization in there
there's different algorithms I think
other people here may be more familiar
with some different algorithms than I am
but the greedy approach tends to be sort
of the approach that dominates the fact
that it's suboptimal is sort of not that
worrisome because you're not really
trying to do the optimization on your
training data anyway so if you really
wanted to do some intelligent
optimization you might actually want to
do something with holdout data because
optimizing the training data really
isn't the end goal anyway and if you you
optimize it to well you might actually
be overfitting
so people haven't sort of thought too
much about how can i optimize this lost
function on this training data so much
as how can I fit a good classification
model that will validate well on
training data on test data and sometimes
the things that work well on test data
actually tend to be things that are a
little bit suboptimal because that makes
them more sensitive okay all right any
other questions you're free to take off
okay oh one last one
well I mean so yeah okay so I'll write
them as a function of P for the two
class problem and you can see that the
Gini becomes simply the binomial
variance it becomes like P times 1 minus
P and then the entropy there's sort of a
strong motivation that you can actually
come out from gambling in a lot of other
ways I won't say too much about it I
mean they wrote they're both measures of
homogeneity and sort of one of them is
steeper near zero and one than the other
so I won't say too much about it but
I'll discuss a little bit next time
about you know why they differ and why
some people prefer one or the other but
I'm not I'm not really an expert on that
but I I'm aware that there's differences
and people give strong arguments for
either one ok under that request since
your take off ok see you guys Thursday
or Friday sorry</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>