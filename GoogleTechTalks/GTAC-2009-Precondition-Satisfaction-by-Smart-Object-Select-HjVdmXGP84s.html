<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2009 - Precondition Satisfaction by Smart Object Select | Coder Coacher - Coaching Coders</title><meta content="GTAC 2009 - Precondition Satisfaction by Smart Object Select - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>GTAC 2009 - Precondition Satisfaction by Smart Object Select</b></h2><h5 class="post__date">2009-11-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/HjVdmXGP84s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's my pleasure now to introduce the
first regular talk of this conference
it's going to be preset be presented by
ye and search just like Professor via
they come from all the way across the
city from ETH Zurich and they will talk
in a subject that bridges the divide
between specification and testing good
morning everybody sometimes you are
sitting your office you think you're
working but you're not the same thing
also happened in your automated unit
testing tool you think it is testing
yourself room but it is not it may get
stuck in the face of generating valid
test cases this talk will address the
problem address the issue of valid test
cases to be fully automated but unit
testing two must be first of all can
must be able to automate it xq test
cases but it also must be able to
automate automatically generating valid
test cases first and verify the results
and the end as nicholas already said
assertions can help to increase the
quality of the death of the software and
then at the same time it can be helpful
to automated testing because normally a
routine doesn't work with arbitrary
input data and is not supposed to
generate arbitrary outputs it needs
preconditions to specify the properties
to specify the properties over the
inputs in order to make the function
call to make a routine function properly
and it needs post conditions to specify
the constraints over the generated
results
as I said before as I said before with
preconditions it may cause a problem for
the automated testing tool because
you're in a case that not arbitrary data
is okay the tool have to come up with
valid inputs in general pre and post
conditions are called contracts let's
reserve a look at a specific example of
contracts here is a put routine from
irate list a person element into a
specified location of the list the
precondition space right here says that
the list must have at least one space
slot one empty slot for the element and
the specified location should be within
the length of the array and the first
condition listed here does after the
insertion of the item first of all first
of all the length of the list is
increased by one and then the third item
should be available from this vest vitac
position in automated unit testing
preconditions can be used as input
filter because it tells you which input
data are valid and postconditions can be
used as Oracle to verify if the result
is correct although knowing that a
precondition doesn't help us to come up
with a valid input data they're
satisfying those preconditions it
enables a very easy implementation of a
unit testing tool random based testing
we don't know how to generate valid test
belen inputs but we generate arbitrary
data and then use a very cheap way to
validate if this input is okay for the
rent a on a test so the general
architecture of a random based testing
tool will be first select the output
selective routine on a test and then
randomly select some data use the
precondition to verify if these are
input a validator and then devote a
routine as an example we can continually
generate data and then change test cases
as a test going on more and more data
are generated more and more routines are
tested and the two should also create an
object po collecting all the objects
that are encountered during the testing
process so far so it can be reused again
but sorry although this strategy seems
very naive but it has been applied to
find thousands of volts in production
software as set before the precondition
can cause some problems in generating
valid test cases because the two may
stuck in generating the stuck in picking
out the correct input values so there
are two consequences here one some
routines or some part of the program is
never tested to even though part of the
program is tested most of the test cases
generated are invalid in a sense that
the test case valid in the test case
both finds the preconditions of the
routine
which means a lot of time are wasted in
those invalid test cases let me show you
the problem as I mentioned before what I
have here is items video it is an ide
for the iphone language there is a tool
called auto test which is which
implements the contrabass random testing
strategy or the our strategy we mesh um
we're talking about later the pit is
developed in the sheriff software
engineering in ETH Zurich I'll show due
to two
in here I specify the class to test is
to the linked list but you've seen
before and I put linked list cursor to
be tested together wizard the cursor in
a linked list encapsulate the idea of
the location and this cursor is used as
a um iterator for the list these two
classes are strongly connected
experience sure that when strongly
connected classes are tested together
the chance of fighting mortals are
higher so we put it together i specify
the testing to be one minute and now I
start testing
you can see from here is the testing
output every line represents a test case
there are three possible outcomes from
poor test case past failed and invalid
past cases ends with Ann's normally
which we're not very happy about because
the single rule of testing is to make
this a fail and fail the third test case
and with an exception indicating a Fault
in the software that the third state is
too invalid test cases you see here and
they're quite common they represent as
cases that does not that do not satisfy
the precondition of the routine on the
test they're fairly common end for some
routines all the generated test cases
are invalid hence we have the problem
before the testing too may continuously
try but fail in jail even a single valid
test case okay let's have a look at what
are those preconditions which are very
very difficult to satisfy by the testing
tool here is one is a remove right
cursor from a replaced it takes an
cursor and removes the item to write up
the cursor location it has five
precondition predicates some trivial one
stating this list should not be empty
the crew season is not void and there
are some long Qi the ones specifying the
cursor must be in such a position where
the item to write a bit exists in fact
the testing tool in fact the testing to
failed
to Jarrett even a single test cases in
the test about 30 right 30 hours it's a
very long but when we actually look at
the testing process we found that
actually India object pool where we've
seen before which collects all the
objects that are encountered to in the
testing process so far there are
actually a few object combinations or a
few list cursor combinations best does
that do satisfy this precondition for
example in a randomly chosen test run at
the beginning of the 50th minute fine
out of almost 70,000 combinations that
satisfy this particular company
particular precondition but since the
probability of choosing the right one is
very load testing two failed in doing so
another common pattern these
preconditions containing linear linear
constraints this is the case well it
will prune and elements attend after it
specified location so the location must
be within a certain range and this kind
of linear constraints happens quite
often especially in data structure or
code container classes but the testing
tool or the all our strategy has great
difficulty in generating well test cases
for those preconditions the general
observation is that during purely random
testing actually some objects that
satisfy certain preconditions exist but
it is not selected
effectively as a solution we implement
an prick guided object selection
strategy or preconceived faction will
use PS strategy later to keep track of
those precondition satisfying objects as
the testing go its goes on and when
objects are needed in when objects are
needed to generate test cases those
preconditions as find objects are
selected in higher probability for the
linearly constraint preconditions we'll
use a linear constraint solver as a
comparison the workflow of the PS
strategy or a new strategy is displayed
on the right the different steps are
highlighted in red foxes the first
difference we start in object selection
instead of random selection where you
will select objects that we already know
freakin freakin satisfying so the
probability of generating a valid test
case is higher than before of course in
order to keep track this information we
need a data structure we call it
predicate evaluation pool after every
test case execution we need to do
something to populate the pool test that
is the second difference the third
difference is that is for optimization
sake in order to make the two effective
will introduce some heuristics to only
turn the precondition satisfaction on
from time to time not for ways I explain
this later let's first have a look at
the object the predicate evaluation pool
and how to select or generate in general
valid test cases from it the object
evaluation pool is structured in a table
style all the preconditions in the test
in a classes on a test as an entry in
this table and some some of the objects
that satisfy this particular predicate
is associate with juice is associated
with the predicate in a pool in the
bottom this part of the object pool we
see there are some lists some curses c1
c2 c3 and some other random objects and
in this end in the center of the screen
the five precondition pred case of doom
remove right cursor routine are listed
and some objects that's in the pool in
the object pool the step slide those
predicates are attached to this to these
predicates to see in order to select
valid input the PS strategy will go oh
go through the predicates the desired
predicates in a pool and try to
construct the combination that satisfies
all the required precondition predicate
for example the two may first consider
the combination l 1 and C 1 but
unfortunately it is not a valid input or
remove right cursor because it only
satisfies four out of five of the
required preconditions another
alternative is to combination l2 and
Stitch you they together satisfy all the
precondition thus they are
they can be used to generate a valid
test case for remove write cursive just
like them Oh our strategy needs to build
to build up the object cool by
collecting all the objects that
encountered so far the PS strategy needs
to populate the predicate evaluation
pool as test goes on that two kinds of
death there are two kinds of updates one
up to every passing test case because
the test case is passing we know that a
system is for right we evaluate part of
the predicates as long as the signature
of the predicates fits the objects that
are used in the last person test cases
so this way we're trying to grow the
predicate evaluation pool as much as
possible so we can have more choices
during the guided object selection phase
of course because because the objects
are reused to generate new test cases as
test goes on the states of the object
may change as a result there may be
inconsistency in the predicate
evaluation pool to totally remove this
problem we are required to re-evaluate
the whole pool after every test case
execution this is very expensive so we
do it in a lazy case in a lazy fashion
only when we see some objects violates
the particular predicate we'd go to the
particular position in the pool and
remove this precondition violating
object combination
for example here is another routine
replace at cursor it will replace in
value and it Atticus a location and in
health three precondition assertions
here is part of the predicate evaluation
pool and estás ghost goes on more and
more preconditions less flying objects
are added into the vehicle for example
after the test case new cursor a cursor
is created right for the list so Valerie
curson evaluates to true on this object
combination of course because this
cursor is them because a curse is
returned is not void which means person
of the predicate curzon avoid it
evaluates to true unwritten code as goes
on
the object see satisfies the freakin
satisfies the predicate not all at this
end I'll see as the object combination
actually can be used as invalid test
inputs for replace accuracy because they
together says by all the precondition
for the preconditions of the routine but
unfortunately the testing strategy
decided to go wipeout wipeout does two
things one remove all the elements from
the list to prove all the curses
attached to the list of the list which
means this C does not set by not on
anymore but because L is the only object
that are used in the last test case only
predicates associated with L gets
evaluated which means after this step
see this verdict predicate not office
not evaluate on sea leaving some
inconsistency in the predicate pool of
course if the next text on this case is
for replace a cursor it will fail
because I'll see as a combination
doesn't testify all three predicates
anymore in this case we observed well
that LC is precondition violating for
this particular predicate
the reason this happens is that the veep
will only contain snapshot of the
properties that holding in the certain
time at a certain time point in the
object pool it doesn't say anything
after that to correct this problem
lazily we only do something after there
is a precondition violation indicating
an invalid test case in this case we
know that lnc are not satisfying this
particular predicate in the pool so we
go to the week before and remove it
prevent it from being selected wrongly
again in future test cases the question
will be if even the case strategy can
generate invalid test cases what's the
six desperate of it according to our
experiment is the overall success rate
is above sixty percent compared with the
old strategy which is below ten percent
so it's a great improve improvement for
linear constraint solving we use the LP
solver which is widely used linear
programming cover for particular
constraint it can give a minimum and a
maximum solution the ph stretch you use
this solution to define the possible
solution space and one particular
integer from this from this range is
selected for as a input candidate of
course by only knowing to extreme values
does mean that every value in between a
valid the pair strategy well actually we
decided to assume so and according to
our experiment
of all the cases it is it is this the
assumption is correct and we give
slightly higher probability to extreme
values the border value because I'm
studies in Vemuri testing showed that
border values has higher probability of
rebuilding fault because them because
the linear solving is low compared with
searching in a predicate evaluation pool
we cash every result so the next time
when a similar problem comes up we
consult Cashman's optimization even
though searching in the V pool is much
much faster they're searching the object
pool because the people only contain in
precondition satisfying objects this
still involves a lot of overhead if we
enable the preconditions s-line search
every time the overhead will be fifty to
seventy percent and mostly due to linear
solving this in this way in this way
although we can generate more valid test
cases but over the overall effectiveness
of the testing process goes down one no
more routines are tested too we found
much less fault because your number
thoughts are the number one criteria for
us we can say no no no for this one
that's why we need to have this
optimization we use some heuristics to
only chandel precondition satisfaction
search from time to time basically the
heuristic says if this routine has
already is tested quite recently we
don't turn it off we don't turn the
freakin intersection on
now if this routine has not been tested
for a long time we turn it on we this
heuristics keeps a good balance of
generating enough many test cases while
preserving the original speed of
traditional random testing okay now i
have explained the algorithm of the
guided object selection strategy are
handed over to sell who will present the
result in the evaluation Thank You jay-z
so the theory might the strategy might
look good in theory and on paper but to
really know if it is better we had to
run the large-scale evaluation so we
will be comparing the new strategy the
PS Vita g2 the old one the original
Awards so basically we want to
concentrate the evaluation on for the
questions the primary one being how many
more routines can we actually test so
jason said the or strategy mrs. quite a
few routines because of unsatisfied
preconditions we wanted to know how many
more routines can we test second is how
often can we generate very test cases
how often can you test these routines
third one being how many more faults can
find in the end this is what's
interesting right about a testing
strategy we want to find fault and the
last point is how fast is a strategy we
don't want it to be to be a lot lot
slower than the original okay the
experiment was set up in the following
way we had 92 classes from two libraries
the Eiffel base and gobo these are too
widespread libraries they use in
production software so the faults found
I really production I have a really
false found in production production
software and it covers a wide Verity of
data structures
so Aries lists trees stacks and this gap
is even a lexer based on regular
expression we arranged these night two
classes into 57 groups based on on how
closely related they are so some classes
have to be tested together makes much
more sense of testable together for
example there's dependency between the
classes and this results in higher
diversity in the object pool thereby
hoping to find models I rerun these on
30 test runs of one hour each and for
both strategies so the old one and the
new strategy and this results in nearly
three and a half thousand hours worth so
back to the first question the primary
goal how many more routines can we test
with the PS strategy compared to the
over strategy um for this let me first
introduce a concept because we did not
evaluate this on all the routines but
only on the so-called hard routines add
routines are routines for which the
original strategy failed to generate the
test case in ninety percent of the cases
the other the other routines the easier
ones are not interesting because they
already covered by over so we're going
to concentrate on these how to eat this
is the coverage graph between the two
strategies on the left is the bow our
strategy the original one on the right
is and p.s strategy we see that o our
covers roughly sixty percent of the
haifa teens and the pier strategy bit
more than eighty percent and what's
interesting is that the pier strategy
manages to cover fifty-six percent of
those which were missed by the your
strategy so it does test more more
routines but it also failed to test one
percent of routines
so in general it's a it's a good
improvement but we did not find we
couldn't have tested all routines
decimal to the second question how often
can you test these routines well this is
a graphic generated on the x axis we
have the the hard routines so it's about
thousand ha greetings in turning these
92 classes on the y axis we have the
number of valid test cases number of
generated valid test cases the blue area
is the aura strategy and the red area is
the PS strategy and now what might be a
little bit difficult to see is that each
line is actually one feature that's our
11 happy team so it looks like an area
but actually it's made of a single line
so for each routine we can see how many
test cases that were generated by the
war and how many would any way to the PS
what's interesting in this graph is that
for some of the routines we can generate
a lot more test cases so with the new
strategy some routines become more
easier to test but for some of them we
cannot succeed in testing one in
generating more valid test cases so we
actually on the par would be but overall
we managed to to generate over three and
a half times test cases as some the
primary or the most important things
about about testing how many faults
managed to find this is a graph showing
on the x-axis the different class groups
as i said this 57 class groups and on
the y-axis the potential increase or
decrease and the number of on fault and
what we're targeting for is of course
with a positive increase which is shown
on the left this is really what we would
like all the test classes to be in there
but unfortunately
so this is when the PR strategy finds
more folds this is when you find equally
as many pots and this is going to find
less fault there's 28 groups out of the
57 will find more faults there's a 19
later equally among pots and there's 10
or we find best spots but what's
surprising is that in three class groups
we find over thirty percent more ports
which is quite quite a lot now the
faults might be discovered by different
strategies not like I said before the
pier strategy does not manage to find
all the thoughts so this this pie chart
shows the totality of the faults
separated in into which strategy found
the pots so the blue area means that it
was found that both strategies eighty
eighty percent what's what's nice is the
fifteen percent found by the PS strategy
what's not so nice is the six percent
found only by the US war strategy so
this shows that even though the pier
strategy is better in many areas and
does not find all the faults that the
ultra 2 g's in total there's ten percent
increase balls so overall strategy with
bombs better this is the same data but
divided into class groups so each bar
represents one class group and on the y
axis we have the number of handfuls of
distant pods what's interesting in this
graph is to see that it does not perform
equally well across all the class groups
so some class groups perform extremely
well the ones on the Left should clearly
show that where they are already a lot
default on below are the PS manages to
find even more a lot more then some are
very nice when is no
along with a new of a lot of red in the
bar which means the PS found a lot false
and then there are some which are not so
nice yes now on to the detection
probability the detection probability is
how probable is it for yes yeah what is
the probability of a strategy to detect
a given fault in one single run because
the motivation is the higher the
probability the less fault the less runs
I'm going to need to find this part so
we want the probability to be to be
closer one need to be short and papa so
this is the definition of the
probability it's basically the number of
test runs where the fault is found over
the number of test runs adulthood we had
this 30 test run for strategy so this is
always for one strategy and when
comparing the behavior of this of this
probability of the two the two
strategies we get this graph and rappin
the upper one is the u.s. strategy the
lower one is a PS refugee and what we
say right away is that both of them are
very very similar when they look the
same do exactly but what this graph does
not show is that each fault is actually
equally probable across both strategies
so you might have a fault which is very
very probable to be found by o R which
is on the file right in the or o our
graph but which is on the far left and
the PS graph we want all the faults of
being you new one so when comparing
these two strategies on the on the
detection probability in more detail we
get this graph where we have the
defaults on the on the x-axis so in
total we found over 2,000
and on the y axis we have the difference
in probability between PS and again it
looks like an area but it's made out of
individual bars what we're targeting for
is the ones in positive side and the
ones in an upper side these are all the
faults which are more probable to be
found at PS means PS needs needs less
runs to find them in the middle all the
faults which are equally probable from
the PS and 0 are for these false it
doesn't make a difference if we use bar
or PS going to need an average the same
number of runs to detect the Fox and
this one is not so nice part is when the
or our strategy is more probable to take
the fault about the coloring the yellow
bars a false found in both strategies
the green ones only found by PS and red
ones found on ebay goulart there's
thirty-seven percent of the faults which
are more probable to be fine with PS
which is nice f30 thirty-nine percent I
equally probable between ps4 so for them
it doesn't make a difference but there's
twenty four percent when 0 r is more
probable to kind of all this again in
general we can say that the PS rata g
the new strategy is much more probable
it's it it doesn't much better job to
find the fall somatically on the Left we
see the faults they nearly go up to 1 so
each single test run will would it's
very probable to find this fault so its
wares on the bow our strategy it goes
down to around thirty percent so the PS
strategy really for the fault it finds
finds them much more systematic as i
said the third most important part is
the speed all of these four important
points
because even though we get nice results
we don't want it to be a lot slower and
the original strategy and so this is raf
each well on the x-axis is is the time
so we tested for one hour and on the
y-axis is the relative overhead so
everything closes here is equally fast
between bovar and PS what's positive
means it's fast in PS what's negative is
passing for each of these lines near the
center represents one class group sick
one in the middle is a median our class
groups the blue one is the fastest one
the red is cisco's so on average we have
only as 0.03 percent overhead which is
legible nothing the faster john might be
surprising why the fastest one is so
much faster it's because the pier
strategy manage the way it was the pier
strategy was used very often and every
time it was used it managed to generate
a valid test case so in the end there
were a lot more valid test cases whereas
the slowest one was also very used very
often p.s strategy precondition
satisfaction but then the suggested
solution actually did not work so a lot
of time was wasted wasted resource
wasted time yes there still some
routines untested at the pier strategy
as i showed you there's more than half
of the untested ones where then covered
by PS but it means the other half was
not covered by the PS refugee we
identified two groups basically why this
where this happened one is not related
to the strategy so we really cannot do
anything about this the accounts for
about half of the other routines it's
for example preconditions which I have
coded as being unsatisfiable it's an
artifact of class inheritance should
actually not happen
it happens in those libraries so really
we cannot do anything about this second
one is that some routines need a
different environment for example some
can only be executed on but net whereas
we tested on limits so these also we
cannot we cannot test them it's not them
that the strategy could have the second
one is strategy related it means this
could really could potentially be
improved by the strategy it's also about
half of the anti-semitism it's out of
bad luck it might happen that we never
create a satisfying combination if a
routine needs particular combination and
random algorithm just chooses one which
does not satisfy this and just out of
bed looking so this is part of random
testing just happens the second one is
as Jason said before might happen that
even though in the pool the pool says
this combination satisfies the
precondition it might happen that the
objects in this combination get damaged
before they used so yeah there's
different strategies we could different
different algorithms we could use for
this we tried some of them but this one
become better and the third one would be
than the test runs are not long enough
so if we would have tested longer maybe
that regime dentist there are few
limitations to generalization you cannot
assume that these results are
representative for for everything so in
particular the classes which shows a
mostly data structures so of course if
we apply this to classes which are not
data structures to more calculation
classes or something like this will not
get different so this is really based on
data structures and the other limitation
is that we used one hour test runs so in
many of the of the test classes we did
not reach a plateau and the number of
faults we show you just to grass just to
see where
on the Left we have one which keeps
increasing all the time and even after
the one how our test run it's not it's
not done yet might see group was the one
on the right clearly reaches of little
it's not very probable that they will
find more fault so maybe who would have
run longer test runs would have slightly
different now to the conclusion but the
comparing the pier strategy with the or
strategy I'm going to come back to the
four important questions I mentioned
earlier one is how many more routines
can we test compared to be our strategy
well other seven can can tell us fifty
six percent of those contests so half of
the routines which really could not be
tested can now be tested how often are
these routines tested it as I said over
three and a half times as often so this
is really means three and a half times
as many then test cases are generated
and the same amount of time how many
more faults are detected as I said this
is a ultimate criterion for the random
testing while it finds ten percent more
votes overall and how fast is it as I
said vegetable move ahead it's your
party of three percent but also as I
said a few times in in the evaluation
it's not a total replacement not yet
might need some future work it does not
find all the faults but the or strategy
found it did not it missed one percent
of the routines test it before so it's
not yet a total placement but we hope to
make it we hope to really replace the
way random testing was done and naive
way by this pious wreckage village
any questions does it work this okay
great on one particular craft it was the
fault coverage based on strategy where
you had the different class groups I
noticed the first two class groups
seemed to have a distinct number of
faults that were only found by each
strategy and i were there any particular
characteristics to those class groups
I'm just probably good to go back to me
okay that one yeah it was the first yeah
they had quite a few that were found by
PS and that were found by Oh are were
there particular characteristics to the
class groups that you think led that to
happen or has that been analyzed was one
of those the lecture person for these
two closets actually compare strategy
can test more because it's a city in
testing more routines and those who
routines in turn to in Gentiles to some
fault rebuilding states object States
and on the green part we also see that
there are a long green part which is not
nice this is because some of the fort's
here are causing their info cause in
variables the same class in very
important BTW tected within different
calling situations and they are much
less different fault because they are
called from different contexts and
depend depending on the randomness it
can be different false revealing the
same class inference can be detected
that's why we see this difference in a
long leg thank you
thank you you're trying to replace the
ostrich d by the ps1 and why not trying
to mix both strategies to take I feel
family our van you take the best of both
of them as looks like another
illustration as there is no silver
bullet there won't be one that we eat
anyone anyone else so when I'm trying to
mix them actually I'm an described that
Jason showed it shows that they are
actually mixed we have this heuristic a
PR decision function which uses some
heuristics to decide if we're going to
stay on the o our strategy to adjourn in
the green box or if we're going to move
on the PS strategy so basically we do
mix both of them it would be too
expensive to only use a PS fit
especially for routines which do not
have strong preconditions but don't have
preconditions at all for these we don't
want to to call the PS strategy and
anchor very big overhead by doing so so
this will give in to the aura strategy
so really 11 the pier strategy to
replace the o.r strategy the naive one
but actually the pier strategy is kind
of a library well it is designed to be
mixed at first but we don't know the
optimal mixing criteria for example if
in the first five minutes we use only
for and later we use MPs and to turn it
on and off frequently we don't know
which is the optimal configuration that
we are trying to do that it is a good
question and it's frequently as one
house I have a short question requiring
the wave pool as I can see that the way
people are highly you know related to
the result what you have shown and I can
imagine you have a cold start phase for
the Whipple and how do you initialize
this way pool and have you tried to
chastity from a different say strategy
to initializing the we put actually i'm
going to start and maybe actually we
don't set it up it's set up as testing
proceeds because it's closely linked to
the test cases so actually it's it's
populated after each test case so as
testing goes on the vehicle just grows
so there's no real setting up and
there's no real overtime in there so you
start from emptying expression that's a
finger words thank you but for me it's
quite surprising that sometimes their
strategy is better than the PS strategy
i think it's also surprising for you too
did you try to understand why it's is uh
pants yes um we don't we don't get a
full picture so it's just some guesses
from the data that we analyze or more on
one hand the PS strategy is a city in
testing in generating more test cases
for the hot routine in the same period
of time which means it spends less time
in testing the easy routine would so
well in in other words less test cases
for easy one so it may be possible that
pure Isaac fewer test cases covers or
meet some part of the object a space
that can be explored by the or if there
if there are more head cases this may be
also not possible because they PA
strategy some kind of record
three this coin is starting with one
precondition then if it's okay it goes
farther than five apart and beats a kind
of of precondition tree and in general
we found out that to find one if there
is one in the pool the combinations that
is my condition go very fast so there is
not there are huge amounts of possible
combinations but satisfying combinations
are quite a few yeah once we focus in
the v po there's only the satisfying
ones so once we focus on finding one
even though there's this tree and we go
back and if it doesn't it doesn't match
we yeah we backtrack even with all this
it's very reference but can you can it
not be possible that by building these
these three of satisfying combination
there is some some some kind of dark
pattern which the old strategy finds
because it has no no intelligent logic
and the ps3 fails because if there's
some intelligent logic it might but
since the o.r is purely random base if
it's really just one particular case
which PS would miss and that oil finds
or to find this one case very very
improbable so actually it could not be
systematic we're still trying to figure
out exactly what is the reason but this
I don't think because I think it's a
problem there may be dark corners but we
don't know but one thing is that the PS
in the o.r up in towanda together there
so there are the in the experiment of
the to evaluate appear strategy part of
the ORS deer running its ears to hear
you you mix them and then you get
worried the best so theoretically if ur
fault can be found by Omar it should be
found by PS awesome although it is done
yeah that's the point the reticle but
you see the graph and edge say something
else so that's why I was up ok thank you
I have a question about the type of
faults it's finding that's quite
surprised you said this is a production
like we're testing yet it seems them as
to being five 600 volts at least we'll
sort of faults are these because it
seems that's not all kinds of different
property really as I said this is
production software some faults were
already discovered before in these
libraries but they were not yet fixed i
mean the library were used is in the
state as it's now in the superheater
it's these are open source libraries so
yeah all kinds of different different
pot but it is it is surprising that
there's so many faults in production guy
buries it considering there's simple
effective strategy no but that it's
surprising that there's so many which
are quite easy to find</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>