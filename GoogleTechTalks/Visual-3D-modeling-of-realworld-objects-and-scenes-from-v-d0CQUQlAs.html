<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Visual 3D modeling of real-world objects and scenes from... | Coder Coacher - Coaching Coders</title><meta content="Visual 3D modeling of real-world objects and scenes from... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Visual 3D modeling of real-world objects and scenes from...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/v-d0CQUQlAs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so
talk about modeling 3d real-world
objects and scenes and people etc mostly
I'll be looking at at seeing how we can
move things outside of the lab to to
real-world conditions and look at some
of the challenges that that you would
occur there and and basically beyond
using the imagery to extract information
from from the scene that we were
observing also try to see how much we
can use the the imagery to also acquire
the necessary information from the
camera system from the camera setup when
that's possible okay so here a few of
the a real world images that we've been
taking and using for our constructions
so a lot of things are much harder once
you go in and control settings you can
control lighting anymore there's there's
a wide range of lighting in the scene
there's the problem of obtaining
calibration you don't always have access
to the site you might just be driving
through quickly or you might have
cameras or you might get videos
afterwards but not really have access
anymore to to the place also even if you
can obtain calibration initially might
be hard to maintain it during the time
of recording or so and very odd other
environments or real-world environments
can be quite complicated large cluttered
also it can be hard if you're looking at
dynamic events to isolate the event of
interest they can be many things
interacting and so on so I'll talk about
two things first i'll talk about static
scenes and objects that you would
navigate through and observe and capture
as a static scene and then also talk a
little bit about capturing dynamic
events out in the real world ok so this
is basically work with it we started
almost ten years ago or we started a
little more than 10 years ago starting
from images going to to
three-dimensional models so basically
the first step is is up here we start
from a collection of images at that time
we were using images pretty close to
each other feature matching wasn't
backwards today so we basically the
sequence of consecutive inches we'd
relate those images by finding
corresponding features and computing
robustly epipolar geometry and the
relation between neighboring images
assembled all of that information at the
next stage here to structure for motion
recovery at the same time not only
recover so we'd recover basically one
hand the location the treaty location of
the features we had observed and tracked
at the same time we cover the motion of
the camera using actually the
information we're covered in a previous
step to a seminal of that and we'd also
recover actually the calibration of the
of the cameras at the same point
calibration including focal length radio
distortion all those parameters would be
recovered at that point and so we do
basically with leverage results from
photogrammetry bundle adjustments and
things like that to get the best
possible result at this stage next once
we had recovered all of that the key
thing that we needed actually at this
point was the location of the cameras
and the calibration parameters and also
rough idea for the scene was located the
range of depth where the scene was
existing and then we'd revisit the image
and instead of just have it where
initially we had focused on the on
points that were easy to match between
images in the next stage we actually we
go back to the images and now we try for
every pixel in the image that we see we
try to compute the depth from from the
camera to the to the scene so basically
we try to obtain as you see here for
most of the observable scene try to
obtain a dense surface representation at
least from that viewpoint and then
finally we put it all together in dense
3d models so here an example this wooded
roughly in 2000-2001 so we start from a
handheld video sequence just a camcorder
bal resolution camcorder it's roughly so
it is 507 on the resolution receipt here
what you see here is for a number of key
frames so views that are too close to
each other are not very informative so
we we kind of subsampled that based on
the data compute the motion for a number
of key frames and at the same time
for the features the future location do
bundle adjustments and all of that get
the best possible results at that level
then revisit the image new stereo
between each consecutive pair of images
here and then assemble that link that
all together to get higher accuracy by
linking from one view to the next and in
the end therefore end up with a depth
map or a surface representation or you
can see that even things like eyelids
and so on are in the geometry as are
present in the geometry so we have
estimated that actually both the
structural motion as well as the the
depth accuracy that we obtain is roughly
of 1 in 500 between the the size of the
object and then the detail that we can
extract out of this ok actually quickly
quickly show you that model also
so here
this gives you an idea of what to obtain
there of course that that was all using
a lot of computations typically this
sequence would have taken about an hour
so this was five years ago we've done
lots of more work in digitizing we've
looked at digitizing small objects when
we can combine actually both you know
let's say you have a small object on the
table you can actually also nicely
segment it this is this is in the lab
and a lots of things on trying to
combine different types of constraints
for example if you can also delineate
the object if the object has a finite
extent then combining the silhouette
information which can be very precise
with then more information in each
optimized which is the photo consistency
how are consistent something looks from
one image to the next if you have the
right surface it should look consistent
from all views but that's hard to
optimize for so so combining those type
of information doing that efficiently
and so on so we're we have ongoing work
in that area but I won't extend on that
at this point I prefer to go to the real
challenge which is to model the whole
world basically and so you guys do a
great job at that at least from the air
at this point but so really what we want
to do is is scrum based being able to
model not only provide imagery but
really try to model in geometry
everything and there's different
alternatives there and so there's one
part is to actually use laser range
scanners and scan as you go through
cities the route we were exploring is
trying to just use video data to see how
much we can extract just from video data
you need video data anyways for the
texture and so if you would be able to
recover everything efficiently from this
raw video data then that that has a lot
of potential and it can in terms of
acquisition it can make acquisition
potentially very cheap there's here in
this particular case it's it's DARPA
footing the bill they'd like to have
they will have eventually cameras in
every vehicle and so therefore if
vehicles are patrolling a city pretty
quickly after a few days they could have
the whole city modeled in 3d whereas if
they need an expensive laser
them combined also with an expensive GPS
NS system suddenly becomes a lot more a
lot more expensive and not and also
cumbersome and not all vehicles can be
equipped with it so in practice for the
system we have currently we have four
cameras on each side each of them
captures 2024 x 768 30 Hertz and and
basically it's the type of imagery that
you see there that that we capture
roughly the multiple cameras are mostly
for field of view we could have we could
use other setups 360 degree cameras or
things like that mostly we like to have
a good amount of resolution to get
tagged you know texture quality that
that you can look at it on the models
that that makes sense at this point we
capture roughly a terabyte per hour of
video so the the reason for that is that
we didn't use any compression in the
first stages of the development we
didn't want to deal with compression
artifacts or everything meanwhile we
verified how much compression would hurt
us turns out that if we do for example
mpeg-4 compression by a factor of 100
there's no effect on the accuracy that
we obtain and so we have grown to a
run-through models to actually validate
those statements okay this is a rough
overview of our processing pipeline at
this point so also I should mention
mention that this so this work is
actually work that is in common between
UNC Chapel Hill where elite division
group and then with David mr. at the
University of Kentucky who in the
meanwhile we've done to Microsoft but
but so but basically this was a common
project and a whole bunch of people
working on this so sorry I basically
what we start from is video data and at
this point we also use GPS inez or
future plans are to also be able to do
without any GPA sinus input but
currently the goal was to focus on the
geometry or construction first and so
that's what we've done roughly for a
single video stream so basically 1024 x
768 we don't process everything at full
res but
we can roughly process it at 25 Hertz on
a single CPU GPU by leveraging mostly
the horsepower of the of the graphics
processor to do most of the of the image
processing computer vision low-level
algorithms what we also do is exploit
the video structure of urban scenes
there's a lot of facades and things like
that but so we try to explore that but
while remaining generally in terms of
modeling so we won't try to enforce very
rigid higher order more high level
models that are just a few planes and
things like that what you really do is a
generic reconstruction of the scene but
where we try to take advantage of the
fact that most of the time we're going
to look at planes and things like that
also important of course in the real
world is to be able to very gain and
things like that so that you can adjust
your you know only eight bits of dynamic
range or soft of the camera that you can
adjust those to follow along the range
of other brightnesses and so in the end
will generate a texture 2d mesh so
basically we we start here from reading
the data in and then the 2d tracking 3d
tracking so first we track features in
images we combine that with the GPS inez
information into 3d location then we
perform a sparse analysis which from the
treaty tracks is going to extract the
dominant orientations in the scene so
that we can direct or Sarah algorithm to
leverage that information so then next
stage is multi new stereo where we
extract now for every image the depths
from that viewpoint then we so we have a
lot of redundancy at this stage a lot of
overlap between views then we fuse all
of that information together at the at
the next stage and we make it as
consistent as possible this two-stage
approach is a lot more effective than
more expensive optimization algorithms
by doing a fast job here and then just
quickly kind of seeing what's most
consistent in the data this is quite
effective in terms of computation power
and then in the end we assemble it all
together in 3d models okay so first
stage gpo implementation here
of the KLT tracker the standard feature
tracker actually it's it's open source
on the web so people can play with it
the we've done further work to extend
the KLT to which is not yet integrated
but to extend the KLT to actually deal
with game changes automatically in case
we would have cameras where we we can't
extract it directly from the camera and
we can do that quite efficiently mostly
the future tracking is still the same
amount of work is the 2 by 2 matrix you
invert to compute how the feature moves
along but at the same time we actually
also compute the exposure changes and
using a hood compliment we can actually
do that quite efficiently at basically
no extra cost we can basically estimate
both the featured tracks one at the same
time estimate the global change of
brightness in the imagery now of course
if you come as a program about you don't
need to do this but so it works a lot
better than the equivalents of the
standard solution in the KLT tracker to
deal with with Bryce's changes okay next
step is to compute the inas GPS I mean
how to compute the the geolocation so
while we have already we use basically
because this the project we work in with
DARPA has also a separate track and some
of you might know urban scan who is
doing the other approach using laser
scanners so they already had this
expensive system in there in the vehicle
to for capture and so we could just pick
it back on that however still it turns
out that there's still some cases where
it's useful to to combine that with
vision using our Comment filter because
here for example what we had we were
driving we stopped here for minute or so
and then drove on meanwhile we had
drifted about 10 centimeters up so the
car had lifted up somehow according to
Ina's GPS and of course the the video
that was just looking at the facade he
could see that nothing had changed at
all and so could actually completely
correct that that problem so so it is
still important even if you have very
high end a very high end system it's
still very important to actually use the
video imagery also in that loop to
figure out especially when you
when you stand still somewhere and go on
your models could actually really a
drift off and i noticed this is this is
relatively this is about a hundred fifty
thousand dollar system from a plan X and
it's post-processed so it's a lot worse
if you would do it in real time on the
fly online so once we've recovered how
we're moving the next step is then to go
towards trying to recover the
information about the geometry of the
scene the algorithm we use for that is a
very simple stereo algorithm initially
Bob Collins then at CMU proposed it it's
basically just when you have multiple
views instead of trying to rectify
images and do things like that for
cereal what you actually do is just for
one of the views you pick one view as a
reference view in the first from you you
hypothesize number of planes at
different depths and then what you do is
for all the other images you will just
project all the images onto that plane
and back into the reference view and see
how consistent that projection is
ideally it should all be exactly on top
of each other and so this is for plane
that's actually far too close and so
basically everything is blurred if you
would just blurred images here you get
actually the the rim here of the of the
teapot is roughly in focus as we go on
here in the back we have the the canvas
that gets in focus and then in the end
everything is out of focus again now of
course we're not doing that from the
focus what we actually do is immediately
look at the sum of absolute differences
between the views so that's what you see
at the bottom there this is when we get
the teapot in focus or part of it this
when we get the background in focus and
then everything is out of focus again
notice of course that there's lots of
small little points which you know where
something is actually consistent but it
just random to avoid problems with that
of course in stereo you will always
integrate over certain correlation
window basically do a low-pass filter on
this at some sense okay so
so so basically the way it works is that
we we have we have our stream of video
so we don't have a stereo rig however
what we have is basically temporal a
temporal baseline as we drive by the
buildings and so the way we'll use that
is we typically take let's say 11 images
so we have a reference image in the
middle and we have a group of five
images before and after and we will use
those groups separately to deal with
most to try to deal with most occlusion
problems so at least reduce a lot
problems of occlusion notice in the
slide before if in one of the images
there would have been an object in front
so I'm trying to reconstruct the wall in
the back there but in one of the images
in this view I see that the wall in the
back but in this view interview next to
me here I would see something else that
would create a big error and so if you
just naively sum up all the errors here
you will actually have a big term in
there and that will degrade your
croatian result so in practice what we
do is use both left and right that's
something proposed by by singling Kang
so basically it's quite effective on
real-time algorithms we do this on the
left and here we have an occluding
object out there so some of the race
will actually see something green here
instead of seeing the right thing that
we were seeing here which means that in
terms of for the consistency here the
sum of absolute difference will be quite
big and we won't find that as the best
correlation however from the other
direction we won't have that problem so
typically if you see something in one
view there's at least or on the left on
the right that you would see it except
in very cluttered environments but
that's not much you can do there often
so then I also notice here the better
term that's basically just in that
computation we take into account the
change of gain in the cameras we
actually do that every time over and
over again when we draw things on top
just because it's actually cheaper to
move 8-bit data around and then do the
and then do a small multiplication on
the basically it's saturated by memory
transfer so you don't want to move it up
to 16 bits of image data to move around
you can do as many computations as you
want on on the graphics processor that's
very cheap and so it's easier to keep it
in 8 bits and then multiply it at higher
precision in
every time over and over again while
doing the computation so much you don't
have any performance hit basically and
of course you sum over some window and
so basically what it's just an
illustration of what happens when you
don't compensate for gain and there's
actually gain change going on it just
completely gets random okay so that's
standard algorithm now of course the
problem with the standard still
algorithm is that it will tend to prefer
frontal parallel surfaces as you would
see here two slides ago oops sorry
really what your hypothesized
hypothesizing is this frontal parallel
planes and as long as everything is in a
frontal plane like here you get a great
result but if you have slanted surfaces
like up here then basically while the
center point is actually at the correct
that on the left and on the right of it
points because of the slant you would
hypothesize this depth and therefore
neighboring points would be hypothesized
at those steps however that's that's
incorrect if there's high-frequency
detail on the surface this might look
quite different from that and so you get
a bad correlation again so it's much
better if you could align the surf your
hypothesized surfaces of which you'll
integrate your coalition windows it's
much better if you can actually align
that with the facade you expecting to
see and so obviously typical urban
scenes have a lot of those very dominant
orientations and so ideally you would
want to take that into account and so
here you see that the tree orientation
so what we'll do is instead of doing a
single front panel sweep will actually
do three different sweeps they don't
have to be orthogonal typically our
ground plane actually is computed not on
terminal to the two to the two vertical
facades we will assume that the two
vertical facades are orthogonal but if
we only see one facade then that's not a
problem we have the other direction just
being orthogonal to it the way we
compute that is basically we've at this
point we've already done our sparse
feature tracking and reconstructed the
3d location of those parts features in
our common filter and so so we already
have quite some information about the
scene we're observing the first thing is
if we have a9 SGPS first thing is to
recover the direction of motion which we
get from structural motion or
from the minus GPS system and also the
vertical which or is gravity from the I
nests or or you just get it as the
vertical vanishing point which is
typically very stable to extract from
from urban scenes okay then the the
other assumption is for the ground plane
our heuristics is that the direction of
motion is going to give us the main
direction so we that's going to give us
the pitch of the vehicle and though
we'll assume that there's no role so
typically and that's typically the case
even for steep streets typically it's it
I mean there's no role so there's pitch
but but no role so so that works quite
well as an assumption and again it's
just an assumption it is not satisfied
perfectly that's fine or algorithm is
generic it remains generic it just has a
preference for those directions but
nothing beyond that and then the last
orientation which is the orientation of
the facades so we have the vertical at
this point and so we just have one
degree of freedom of how our facades are
aligned the way we compute that is we
have for our point distribution we know
the vertical so we project everything
down we eliminate the vertical component
and what we want to find is basically
the orientation aligned so the simplest
way we found for that is basically just
looking at projecting down in two
orthogonal directions and so if you have
the wrong orientation here you get
pretty much a random histogram here of
where the features occur if you actually
choose the right orientation then your
histogram is going to be very peaked
basically minimal entropy and so we go
for minimizing the entropy here and so
you should see okay yes you see here the
entropy going down and then going up
again and so that's basically dia that
gives us the right orientation very
reliably very simple notice we do that
for every single frame along the way so
if buildings are not aligned and so on
that's not a problem it will look for
the dominant orientation in every at
every point in time
ok
okay so basically going back to the
stairwell greynium what we do now is
again same thing sum of absolute
differences left right and gain
compensated we can also now include
priors because we have now looked at the
structure of the scene if we look at
this structure here these histograms
that basically gives us a good prior
assuming that there's a correlation
between where we found feature points
and the actual surfaces which is very
likely then basically the most likely
positions for for for surface points is
going to be here and here and basically
in the other direction of the sweep is
going to be here and maybe a little bit
there and there so basically and of
course the ground plane is also going to
come out very strongly there and so we
can actually include that very
efficiently in in the in the
optimization here just as because this
prior here get some on top of it the
effect of that is if there's not much
there's a big ambiguous region because
for example you know like a lot around
here these white walls work with no
nothing to correlate on well in that
case in within the military region we
were going to prefer the dominant
surface the dominant facade or surface
so if part of the facade is blank and
has no texture at all if other parts
were texture than we we have the general
position of the facade we'll just
default to that as long as that's a
possibility that that's that part is low
cost so that works quite well to deal
with those also of course you can
consider optimization at that point
knowing that most of the facades are
there you might actually not do an
exhaustive sweep of all possible depths
but focus on the most likely depth as
what you helped in there oops so here's
an example here seen in the beginning of
11 the beginners and what you see here
is the computer depth so we did three
different plane sweeps here obviously we
can well these are actually the
orientations we had here what you see
here is the basically for every pixel
the only thing we do is for every pixel
take the best the lowest cost including
a prior lowest cost porn
and so here you see the depth that we
obtain coded by light lighter is further
and then here what you see here is
basically the label of surface
orientation so remember we have three
different surface orientation so we
sweep this way we sweep that way and we
sweep that way in this case and so
basically you can see that the colors
here make a lot of sense you know using
our higher order understanding of that
scene all main regions are labeled
correctly and of course around here
there's some small surfaces with
different orientations and that it seems
to indeed kind of wrote the algorithm a
little bit at those transition places I
notice also that this heap here of stuff
basically then we'll typically default
to the front of parallel case but it's
were constructed also very nice nicely
there so it's not only finding you know
two three planes it's actually finding
out google construction so this is from
just 11 video frames so we didn't kind
of use the whole video sequence there so
that's just a single depth map with five
years before and after to kind of curl
it from and so this basically takes
about a second or so to compute for this
example okay so so those were wrap how
to compute route dead maps for every
frame in a video sequence the next stage
as I said we're trying to do something
very efficiently and we end and
therefore word you try to do is mostly
use the redundancy of data to quickly
compute something and then in the data
look at consistent consistent things and
pick up the most consistent signal in a
second processing step so the first step
was computing those multi-view
stereotype maps and then the second
stage is to basically fuse those dead
maps by by looking at visibility
constraints at getting the most
consistent thing in terms of visibility
visibility constraints are explained
over here basically we have a reference
view for which we try to compute this
accurate depth map and then we have a
number of other views that also have a
depth map associated with them and so we
have hypothesized from the reference
view the depths a B and C here and then
from view I we try to see if the
measurements from you I are consistent
or not with that so clearly point B is
bmv prime are consistent in terms of
measurement there's a problem with a
prime here or a na prime because
basically from this view we are able to
see a prime somehow that's in conflict
with having a point a here we should
have seen a instead of a prime as a
surface notice of course that the other
way around if a would have been behind
here somewhere that we have been
perfectly fine because of course they
can be that the complexity of a scene
doesn't have to be one of course there
can be multiple depths correct depth
along array but from a certain viewpoint
you only see the first one so it's only
when you have a conflict that you don't
see the first one that there's something
wrong but if there are more depth behind
it that's perfectly fine and so there's
like the the corresponding conflict in
the other direction is basically here we
hypothesize see and however view I would
actually put C prime in front of it
therefore that's also a conflict in this
direction here and so the two algorithms
that we have to use that information one
is basically for the reference view we
count how many views how many times
something is projected in front of it
for certain depth hypothesis so that
basically you know this is only the
third thing along the way and not just
the first thing so that's a those are
two conflicts here and vice versa we
also count how many times this thing
itself is in front of other stuff in
other views so and so basically we try
to balance that out and we take the
thing that's in the middle of that that
is stable in terms of having the same
number of conflicts in front of it as
conflicts behind it the that algorithm
actually is squared in the number of
views we try to fuse and so slightly
faster algorithm is over here where what
we first do is we pick the most likely
hypothesis based on our stable actually
also go through the confidence and so
based on confidence we pick the most
likely solution at first we look for
consistent data within a small epsilon
of that depth and actually then also in
the meanwhile fuse that information to
refine on that measurement and then and
then we look for conflicts of both types
here that seem to indicate that this is
not a correct depth and as long as the
combined result is a positive is we're
still confident about a result will keep
it otherwise we throw it away and try to
find another way to define the depth
hypothesis for that the key in getting
all of this fast is that those are
basically all rendering operations back
and forth of that meshes and also quite
efficient on a GPU so we don't do this
one pixel at time obviously we do full
renderings in from 12 to the other
finally once we have all those dead maps
that are as consistent as possible will
generate of course strangler meshes on
top of it I mean we could do other
things but do point based rendering or
so but we generate meshes
multiresolution meshes then of course we
still have some we still have about two
factor tree overlap typically in or in
our depth maps mostly so that behind
here we're missing of course the part
that is behind the pillar here and so by
having two or three depth maps that
still see about the same region of space
in the next view or the previous you
will actually see we'll be able to fill
in that gap basically but so of course
we'll have then typically most of the
surfaces will be seen two or three times
and so we try to remove that also in a
step also where we render and we see
which is consistent also things like sky
that is attached because of the stereo
correlation window and so on we try to
eliminate all of that and and also gain
normalized textures and so on okay so
here some results this is a building
that DARPA surveyed for us or asked the
company to survey so this has been
surveyed within six millimeters this was
using fixed stations of laser scanners
and then tell the lights to line up
everything and so on so it's very high
accuracy so that was used as grant
through to compare our model too and so
this is basically or this is our model
and this is our model color-coded based
on the difference between our model and
the ground truth and so what we see here
is this is the histogram of errors in
centimeters actually most of the points
are you know well below 10 centimeters
and if you look here at the statistics
the median error so half the points
actually better than three centimeters
on the surface and you see the color
coding mostly degrades in regions where
there's not much texture and actually if
you notice we didn't use the prior thing
here so not all the results are in sync
with
latest developments we didn't use the
prior and so here the whole homogeneous
region which is basically just plain
white we completely lost out that region
and so that's not counted against us in
this case so have in terms of
completeness we were probably only at
sixty seventy percent of the or
something like that of the total facade
okay here we also of course used well
envision now most people work actually
on small objects instead of big scenes
and so to compare algorithms to we were
trying to we also have to look at how we
would do on the standard data sets which
is a small object about this big that is
a turn table with the robot you know
turning a camera around it so just what
we wanted to show is that we would
perform reasonably well on that but of
course much faster and so does the
results that we get actually quite
reasonable they're selling on the worst
of the of the group of results but
what's important to notice is that so
this about 47 images around a circle d
and so reading in processing generating
the model and outputting the model takes
takes us basically 20 you know less than
30 seconds basically the at the point
where we submitted this the other
fastest algorithm was 30 minutes or more
so it's about two orders of magnitude
sweet yes most of the stuff is going to
GPU so it's about five seconds of stereo
like overall the depth maps in stone
total it's about five seconds of stereo
on the GPU and about ten seconds or so
of off of the surface fusion at this
next step that making a consistent and
yes it's mostly that's if that was on
the CPU it would it would be one order
of magnitude slower but but actually the
30 minute thing is also on the GPU as
far as I remember so it all depends of
how you do it we didn't try to get a
perfectly closed surface and stuff like
that so these are algorithms are doing
volumetric things which are quite
expensive here we just generate depth
maps because there's no hope to get a
closed surface when you're more than a
city to get one single nice you know
manifold surface that ma does the whole
city that doesn't make any sense but of
course for closed small close objects
you could
that and some a lot of people focus on
getting the proper topology and so on
okay so here's another example so this
this is model that we models from
hundred seventy thousand frames so
that's four cameras for about 20 minutes
driving around here you know all around
this region here and and basically what
we are it's only actually half the model
that you see there is only one side of
the street we have both sides we don't
yet have a good way to render this and
so it was kind of painful to make just
this image here but but I can show you
one example here of a small part
so of course so those models are
computed only from the from the video
data so no lasers anything like that
used they're certainly not perfect and
also there's a lot of small things we
could do to improve them so it's the
wrong result sort of that comes out of
our processing we haven't done anything
to kind of clean up or dead maps or to
fill in small gaps or to do that number
thing so it's the raw processing data
and when I talked about sky removal that
wasn't used here in in this case I
should go to
but it certainly allows you to get a
good idea of the place and notice that
those unstructured scenes like treason
so actually you can kind of get a good
idea of the shape of the tree and get a
feeling for for how the place looks of
course what we don't see in the in in a
viewpoint we haven't filled in gaps of
places we weren't able to see in the
cameras
but it turns out that the the lighter
based models also aren't perfect on
those type of scenes the difference was
a lot smaller also in terms of accuracy
the few centimeters accuracy we had
turned out to be very competitive with
the alternative lighter based approach
as long as the liner is actually also
captured while you drive by at high
speed or a reason I mean at a reasonable
speed and you have to process everything
also in real time it's not always that
easy to get much better results even if
you use liner here's some other models
from Chapel Hill so the processing
varies between us 83 hers and 25 Hertz
depending on what settings you would use
so one thing I really want to bring in
is basically that straight lines should
be straight it's something silly but you
know models the raw output of the Dead
maps are will not preserve straight
lines for example and that that
immediately due to a viewer immediately
pops up the journal is very challenging
because notice there's trees in front
and of course we don't see the whole
facade behind it but you can see that a
lot of the facade is actually filled in
behind the trees not everything when the
trees are too close to the facade it
doesn't work but when a trees are a
little further we do actually get
reasonable fill in look for example here
behind because we use all those
different viewpoints and so if we didn't
see it in one viewpoint we sit in the
other
of course windows and things like that
are kind of a challenge and so of course
this doesn't use any kind of higher than
knowledge about architecture of models
like that beyond the fact that we prefer
those those few sweeping directions it's
our goal is to up to go to a much
simpler low end system ideally really
just a camera so the challenge is really
too when you build a long model or let's
say you want to model a whole city is
really to avoid drift and video would
drift very quickly because there's no
absolute reference so the key thing is
really every time you get an
intersection here you really want to be
able to kind of find that back and
stitch that together that way it's
reasonable maybe with you know a few GPS
locations or a few reference location a
few geo-located images that you could
attach your construction to would be
sufficient to do a fully video based
system but so the first thing we're
we've worked on very recently on in this
area is is to try beyond the typical
sift features that a lot of people are
using it's actually something very
similar to see features except that
because we all we assume not that we
have like it for the tourism and so on
that you just have a bunch of images of
course in photo tourism you assume that
you have a bunch of images but they all
kind of close to each other I mean you
assume that you have a reasonable
density of those images and so you don't
need to match immediately from one
viewpoint to a viewpoint 90 degrees
apart or so while of course here with
when you have videos you do one video
stream you drive down this street and
then you pass through orthogonal e the
few points can be quite different and
with no other images in between but of
course if it's video every single video
stream already allows you to reconstruct
the whole scene from that single video
stream and so our goal here is slightly
different than what you do it sift is
basically we have one 3d model and
another 3d model if it's just computed
from video with no absolute reference
they're basically determined up to a
global scale and up to an absolute
location in space so basically up to a
3d similarity transformation that's the
unknown transformation we have to do
there and so basically our approach
consists of computing the local
structure of motion for each of those
video segments we generate ortho
textures by that we mean that if if I
was taking this scene from this
viewpoint once I've reconstructed the
scene i can actually regenerate a
viewpoint for every surface patch or
terminally to it so architecture really
means that we have an orthogonal
orthography view straight a straight
view at every part of the facade or of
every part of the building or of the
scene that we're looking at and then
within that within that you basically we
do something very similar to sift to
extracting sift features but on these
rectified use the advantage of that is
that now if if one viewpoint is from
various you know from this direction and
another viewpoint is from somewhere 90
degrees away from that or any other
angle then basically by right we
rectified all to the same viewpoint that
is defined by the local surface normal
and of course there's some practical
tricks and so on but basically what we
do is basically in that surface extract
first the difference of Gaussian
extremist which gives us both scale as
well as to the location in the auto
texture and of course that thing is on
the 3d surface so that's a treaty
location and then we extract the normal
while we we already used it before so we
have the normal also as well as in this
in the texture on the texture we look
for the dominant gradient which is the
same that sifters for finding it to the
orientation so basically we we have now
a to the orientation on the surface
together with the normal that gives us a
3d orientation basically so we basically
have all the degrees of freedom we were
looking for from a single feature if we
basically of a single feature we got
completely invariant to all of the
variants we had in we are expecting in
terms of geometry that's quite nice and
then basically on the texture we compute
the sift descriptor for doing matching
then and then we do a robust article
matching where we start the nice thing
to notice is that both scale and
orientation are actually for all the for
all the correct matches will be exactly
the same so there's the same relative
scale for the whole model it's one
consistent scale it's also one
consistent change of trot
one consistent rotation that will align
the model so here it's very easy to
compute matches very efficiently and
once we've through runs a kind of got a
robust estimate for this then we can
already get a better solution there so
quite accurate rotation and then use
that to then verify that all the
transfer the translations for all the
future matches are correct and so
basically what you see here is a partial
model capture driving this way another
one captured going that way and here all
the matches between them and then here
the same thing with only very limited
overlap between two partial models and
and also there it was able to compute
the correct matches so basically those
are this is the texture the original
textures here and then this is once
they've been rectified obviously trying
to match this with this it's a lot
harder than once you've rectified it and
it's very much closer so the nice thing
is also actually for those examples we
also try sift of course the standard 2d
saved and and that just failed we
couldn't get anything out of that oh and
actually I think he's gonna work over
the summer in santa monica leigh here so
i can ask you more questions about it so
same thing we can actually do for aerial
imagery here we have a helicopter video
we reconstruct a long strip model of
that sequence here just from video and
then we go to the USGS server or we
could have gone to Google Earth but we
also from USGS we got both the digital
elevation map as well as the texture and
then we aligned those to robustly using
those that 3d registration
a few more things that I won't have the
time to talk about is as important as
the geometric calibration for in many
cases also the radiometric calibration
so we have some automatic procedures to
with a handful camera with a nonlinear
response function to estimate the
nonlinear response function the exposure
changes white balance changes all of
those radiometric changes or properties
of the camera as well as actually
finishing which is corrected on the left
and not not on the right here extract
all of that from just a moving camera so
you don't need to do specific motions
just a randomly moving camera would
actually we can extract that from um
same thing and that's actually work with
Sri Rama tutela who works for you guys
in the meanwhile basically methods to
very simple nice linear elegant linear
methods to calibrate this kind of very
nonlinearly distorted sensors in a
nonparametric fashion a few more things
that I want really spent too much time
on this is something that goes beyond
the typical ransack so ransack is is
nice very robust algorithm its random
sampling consensus its algorithm that's
robust too many many different things
even sometimes programming errors or
fixing that if if only half a few
hypotheses are generated correctly
ransack will actually still be able to
pick those up and it just try it keeps
trying until it find something that's
consistent if half of the things you try
are actually incorrect while the other
half would allow you to still find the
correct solution one thing it is not
robust 02 though is that if you have
even your data set you have a subset of
the data set that is kind of self
consistent as a subset then ransack will
sometimes be confused by that so if you
have basically so the typical case is if
a lot of the points you're looking at
are on a single plane and the solution
is unique as long as points I spread
over 3d but if all the points are on a
plane which if you are looking in urban
scene or so could happen more than you
would think then of course there's many
solutions that are consistent with that
plane why
there's only one that's fully consistent
with the whole 3d shape and if only very
few points like up here are of the plane
most of the points are actually in the
single plane then as soon as ransack
finds the point in the plane it finds a
lot of consistent points that vote for
the same solution and run SEC is just
happy at that point and says you know I
have a great solution i have so many
points that support it you know this is
it i'm done and basic you end up with
the wrong solution so we we've worked
out this for anything that has linear
system of equations and so this is fully
generic for any fuel or so as long as
you have linear system of equations it
will basically looking at your set of
equations at your data matrix it will
look for in some sense the robust rank
of the data matrix so it will go for it
and try to kick out a few a number of
outliers and see what what's the
remaining rank so basically we look at
how many in liars it can fit within a
more constraining model so what you see
here the typical this is for fundamental
matrix a typical you have you need eight
points or a rank eight so data matrix
but if all the points on a plane you
will only have a rank six and what you
see here is basically that going from a
to rank 6 only reduces the amount of in
lines by a very small fraction if you
try to all squeeze it into around five
data matrix suddenly there is only a
very small fraction of the points you
can still squeeze into that ranked 5
matrix really squeezing it in rank 5
means that it's the code i mentioned
that counts is that you try to increase
the null space by kicking out only a few
points and so that's that's what you see
there and of course once you've found
that the true rank is six of that data
matrix the robust rank you can look then
for the few additional points
specifically go searching for them that
would support then that would be able to
fill in the remaining two degrees of
freedom in this case this is quite quite
nice algorithm and could really be used
in on any kind of problem that's that
where this could occur well the problem
is extracting six degrees of freedom
from a camera system even if there's
multiple cameras if there's no overlap
it's actually quite hard you get five
degrees of freedom so you get the
relative translation but the absolute
scale is really hard to get and so we
did some work on kind of
trying to get that it's really hard I'll
quickly show something here which is
what we try to do is have a different
actually me stop this and go back here
small parking
okay we try to do tracking the camera
location a camera as it moves around but
for doing that typically the typical
approach is to have an a way to explain
the scene and to be able to model the
scene let's say you can model the
position of a few 3d points and then you
track that and so you can compute your
relative motion the problem is if the
scene is really too weird and too
complicated that you can't model it then
how do you still do optical tracking
well one way we try to do that is we
call it manifold surfing it's basically
we consider the images of a scene so we
have a rigid scene and so moving a
camera with six degrees of freedom of
motion through that scene good in
general if you'd go to every location
you will basically span a six
dimensional manifold within well
basically all the possible images if
let's say you have a camera that the
styles and by thousand images then you
have a million points million pixels and
you can see that image image as any
possible image as a point in a 1 million
dimensional space so every possible
image is going to be a point in that one
dimensionals 1 million dimensional space
and then all the particular images of
this scene that we're considering is
going to be some 60 manifold within that
million dimensional space and of course
in general is very hard to model that
manifold like if it's a simple scene
than actually having a 3d model allows
you to generate that manifold generate
all the images all the forms on a
manifold but in general let's see Jeff
Nolan version scenes curves mirrors
semi-transparent see very complicated
things we just don't know how to model
that yet so the idea is just to use a
sample based approach we're beyond
actually taking a reference view that
the camera itself we also try to get we
also record the number of we have a
number of additional cameras just next
to our central camera that immediately
record how the image would look like if
we would slightly move to the left and
you know and slightly move up and
slightly moved on and so on and so as
long as that's really the cameras are
close enough that we can have a linear
approximation in that of in that many of
that manifold we can actually we can
actually all married directly so to get
it close enough it means we do typical
what people do in optical flow and so on
we do a multiresolution approach we
blurred images and work on our lower
dimensional images basically we get
something like this here we so we we
have the image for the reference and
then the change as we move left right
etc the rotations actually you can you
don't need a camera to critic the
rotation that's just a home ography
transformation so so we don't need for
cameras total we have all those samples
so that's basically the amount of change
you would have for any type of motion
and then basically of course as we move
our system around we will observe some
type of change for the center camera and
then basically we need to explain that
change by just a linear combination of
those canonical changes and basically
you solve that by linear system of
equation and that's basically the motion
for a computer generate scene so you see
if you synthetic scenes first so you
have a scene here with lots of textures
and stuff and then a cursed mirror so
this is the image to see from the
reference camera and so you see strange
effects because of the curved mirror so
those are the estimates and so this
actually works quite well the next next
thing you'll see is the same scene but
now with some semi-transparent see also
folded in so you partially see the
cursed mirror and partially see through
it and the algorithm is actually totally
insensitive to to all those complicated
visual effect
and then what you see here is a real
scene the calibration target and then
the few points that I've been clicked
here are just for verification for
seeing her arcuate we are and so if we
accurate those points should not drift
and so you can see them drift a little
bit but not too much here this scene is
actually a lot more interesting if you
look carefully we're actually not
looking anymore in the same room we're
actually looking out the window from
that room however who I was doing that
at four o'clock in the morning so that
it would be only a little bit of
sunlight out there and most of it would
be reflection from the room and so what
you see is basically you see the street
lights there you see the chimneys up
there here it does add the chimneys of
the hotel across the street here you see
some trees etc but what you mostly see
the reflection from the scene and so
it'd be pretty hard to kind of use your
stomach tracking algorithm on this of
course well it could still probably
track some of this here but so this
algorithm is completely insensitive to
that and that's actually quite quite
good job of tracking that so so it's
just a completely different ticket at
tracking things okay now I'll really
quickly go through free dynamic scenes
so basically the first thing we looked
at this can we recover how much can we
recover from a single video stream and
so what you see here is basically from
just tracking features we automatically
segment the motion there's complications
because the motions this is an
articulated motion and so the way it's
modeled is actually you would get
intersecting linear subspaces and
because in this section the segmentation
gets a lot harder and then in the end
once we are able to segment and we can
build a kinematic chain up and so that's
what you see and then you see the
computed articulations and actually we
also have the 3d shape of often you here
and all of that can can be recovered of
course it's much simpler if you have
multiple cameras so here you see a
setting with four cameras this was
actually recorded MIT we just got four
minutes of this video from the for video
different video cameras from that we
were able to recover both the camera
locations and calibration
well as actually the synchronization
notice that this one is out of sync with
this so so all of that can be computed
just from the video data that's very
important not for this setup of course
where they could calibrate it very
precisely but imagine is sitting
outdoors where you have people kind of
just random positions with cameras maybe
it's after the facts you get all those
videos in from different viewpoints
nobody can still go in there and
calibrate where the people were standing
it's just not there anymore so this type
of technique actually makes it possible
to use that type of data set I'll
quickly go through some work on
reconstructing events based on
silhouettes but in a pro ballistic
fashion there's so much time to explain
it so I'll go quickly but basically you
have you take first a reference image
without people in and then as people
come in by making a difference between
the two you get evidence of where where
they must have been an object somewhere
and by combining the information from
many different views so basically
intersecting the silhouette cones you
will get the likelihood of where the
person must have stood mostly it's all
using a patient formulation here which
makes it robust to mistakes in one or
the other image it's basically invert so
you can easily do the direct process
given a grid how would the silhouettes
look like and then basically use Bayes
rule to invert that and do inference of
where the scene was I'm going to be
located let's skip that for sake of time
but just show you the illustration here
one view and as you integrate multiple
cues from multiple views coming in you
get more and more evidence at the center
here and then if you trash hold it you
basically get an isosurface of certain
likelihood of having an object there
that was work actually by my post doc
johnson hongkou before he joined the
group what I was interested in is
pushing that out towards you know going
in will scenes were these occlusions and
other effects going on and so for
example here the problem is now of
course here you don't see him in this
view but he's there but he's hidden so
you only partially see him and so we
wanted to be able to take that into
account so what we wanted to be able to
do is not only recover
John sylveste I'm walking around but at
the same time also recover the geometry
of the scene that was interacting with
the dynamic object and generating
occlusions and basically use the
occlusion events as cues of where the
actual that the geometry of the you know
of the scene that's standing there is so
both to be able to get that geometry but
also to be able to use it to get a
better estimate that now properly takes
into account the occlusion relationships
and so we know that from this viewpoint
if John Sebastian is here once we've
recovered the scene here we know that
we're not going to see him here and so
we not we don't have to penalize the
reconstruction for that so skip this
here but i'll just show you the video
yes
ok
okay so here you see as we go along and
we accumulate over time from a signal of
you you don't get much information of
course but as you accumulate over time
both free space constraints and
occlusion events you get a lot of of
evidence where there's actually an
intruder let me try to Susan work she
tried to skip through some of this
here's another one with the statue we're
working around as you get multiple
people it actually gets a lot more
complicated and our initial assumption
was actually there was only one object
that and so that all voxels could be
computed independently as you see with
multiple objects it starts to degrade
there's a lot of stuff happening in
between the two objects here last
example a chair and so it's also a sense
walking around the chair sitting on the
chair doing things and after a while
basically we have the whole detailed
geometry of the chair recovered without
ever having had a direct measurement
from the chair it's just indirectly just
by walking around it and so on that we
get the chair geometry okay and then it
seems I have to wrap up so this is this
last thing as I said we have problems
basically when multiple people are
interacting and so of course the obvious
thing to do is then instead of having
one generic for grant model is to
basically have a few have built up
separate appearance models for different
people that would be interacting and so
you see here this this model triggers on
one of them and and this model triggers
on on the other student and so we
basically instead will now have
multi-label multiple labels also we
still have an generate label which will
be able to pick up new people coming in
the scene and then on this blob that is
unmod 'old will train a model in and get
that person out and so so just to wrap
up here
just short video here we're basically
here what you have is five different
people interacting very closely and we
have about about ten cameras like with
the other examples and so if you don't
do it properly you basically so this is
the five different people extracted and
on top of them this is summarization of
their appearance color appearance model
it's a very simple appearance model but
already quite effective as you can see
here those are the camera location so
it's not a large number of cameras so
this what you see so it's not not yet
perfect but you actually get quite
concede and we were able to disentangle
all the different people if you just do
a standard visual help technique you get
one big blob and it all looks like one a
big big cluster and so here from a
texture on top of its own okay so really
our goal is to be able to to to capture
in outdoor environments so both capture
the environments efficiently with very
little means very flexible but also
capture dynamic events that would take
place you know ideally you know maybe
we'll ask people to submit their videos
somewhere and and you know a fun event
that they would have recorded and then
combine all of that and try to estimate
a full four dimensional representation
of that including preferably also a full
representation of the surroundings so
that you really have an immersive
presentation they're really nice to be
able to assist to festivals like this
really like from within the crowd and
and and see all of that this also much
more serious applications to this of
course also let's say as this to a
difficult surgical procedure as a
student be able to assist you any kind
of complicated procedure that would ever
have happened that would be you know in
some sense much more useful than this
but of course this you know but both did
so applications are in many different
areas so so basically well let's give
this and I just thank all the people
that
but that help with this and and didn't
sit so i'll just stop here and i think
there's only maybe down for what okay so
stop here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>