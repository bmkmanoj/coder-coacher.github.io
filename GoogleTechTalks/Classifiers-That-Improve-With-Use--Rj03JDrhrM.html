<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Classifiers That Improve With Use | Coder Coacher - Coaching Coders</title><meta content="Classifiers That Improve With Use - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Classifiers That Improve With Use</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-Rj03JDrhrM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">georgia has been a legend in the OCR and
document her standing and machine
learning community for many years I'm
sure many of you have heard of him today
is going to talk about classifiers that
improve with use he's going to talk
about a variety of topics around the OCR
and machine learning we thank you very
much thank you for letting me come and
talk here and I'm an old guy so i'm
going to show you all ideas this is the
thesis there is never enough training
samples so what do you do when there are
not enough training samples I'm going to
let you read some of the slides because
I think you can read faster than I can
talk there is nobody literate here I
assume this is maybe what I cover or
maybe maybe not stop by anytime you all
know this but this is the representation
i will use here there are two classes
circles and xs and they are they have a
probability distribution for two
features we can only show two features
in two dimensions and we have some
equity probability contours are of
course more more than one equi
probability contour for each each class
these appear to be Gaussian as far as we
can tell and there is some kind of a
decision boundary that separates the
excess from the also i'll use this
representation which i think is familiar
to all of you these are the cases that i
think are interesting and this may be
the if there is one slide to remember
this may
slide to remember the simple case is
when we have representative samples that
means that the distribution of the tests
that is similar to the training set
there is no problem we create some kind
of a decision boundary here I'm showing
a linear decision boundary but it could
be any decision boundary at all its
linear is is simplest in this case we
have plenty of training examples but the
but the test data comes in groups of
homogeneous samples we call them eyes or
journals because they are from one
source so they could be a book in a
particular type face or it could be a
business forum in a one person's
handwriting or could be a billboard with
one display font and in that case if
they have a training set between the
classifier all the all the data this
will be the decision boundary but if we
can somehow use the information from the
test data we can get a more accurate a
more accurate boundary these two cases
are similar in that we have again we
have one of many many many many styles
or typefaces or handwriting's different
kinds in this case we can assume that
the means of the individual distribution
distributions are distributed as
gaussians and in that case it's possible
to develop a classifier for any single
source document that is optimal in this
case what we would really like to do is
something like font recognition but font
recognition has some problems with it
and you can do better than that but
these cases are sort of similar to this
then we have
situation here which is the most
difficult situation in which we really
is difficult we're the only relationship
between the training data and the test
data is in a sense of topological
relationship that the the feature so for
the blue are bigger than the features
for the red in both cases it's some kind
of a topological relation but there is
no overlap between the distributions and
if we know what the relationship then we
can use some heuristic but if we don't
it's a difficult problem so i'm going to
show you any question about this this
slide the what the difference i just
want to say that the difference between
here and here as you will see one
possibility is that all the patterns in
a single source data from a single class
or similar and then it's possible to use
adaptation we can somehow estimate the
new means the parameters of the new
distribution or nonparametric
distribution but it is possible that
there are only a few samples for example
in a zip code zip code recognition there
is not enough samples to estimate the
probabilities of zeros ones for this the
particular zip zip code and in that case
the best you can do is infer from the
characteristics of the tools what the
ones look like and that's different from
adapting on the same class
this is the conventional classifier that
we have seen since the 1950s there is a
training set we estimate some parameters
there is some hidden input that nobody
talks about meta parameters a little
fiddling there and then we have test
data we recognize them we either
recognize the class or we reject them
either way we may want to do some
proofreading and find them a few
mistakes in the labels all the rejects
are manually entered and we get a
transcript conventional system so this
is our straw man that we should do
better than the classifiers themselves
and there may be others here in any case
the rejects are normally key them but
very few of the engines for OCR connect
the reject entry which may even be done
somewhere else with the classifier they
don't have provisions but they should
for routinely using every correction to
reactivate the classifier parameters
they should have they should be done
routinely nothing should ever be
corrected oscillates it's a it's an
engineering problem not a scientific
problem the only difference here is that
if the classifier is already pretty good
then maybe you can do a best girl you
can use the classifier labels instead of
the reject and relay Buhl's to retrain
the classifier and this is an old idea
and I'll show you some some results this
is a picture from 1966 I was working at
IBM this is the results of this of using
the classifier results for retraining on
a set of 22,000 printed patterns of
various type bonds and you can see that
there was a significant improvement at
that time this time of experimentation
was difficult because computers were so
slow that goes the feature extraction
and the feature comparisons the
classifier itself was performed in
hardware twas a complicated mixture of
of IBM 1800s 1401 computer and a
gigantic shift register of a thousand
bits which we shifted the patterns and
extracted the features but the idea was
there and this looks like magic because
all it says its bootstrapping
so I showed that what this is how it
works some of you have seen this picture
before here is a multi font training set
and this is the best linear classifier
that we can do but notice now look
carefully at the fonts and are colored
them to make it easy this goes with this
this goes with that and this goes with
this in a particular document okay these
are just different typefaces and the
reason for that is that any typeface
designer always designs the font so that
within that phone all the characters are
distinguishable do they try the same for
handwriting your handwriting one
character may not be differentiable from
another person's different class but you
can ought to be able to read your own
handwriting so your it's different so
when you only have examples of the blue
blue font then this classifier that you
three make some mistakes and this is a
mistake because it classifies this blue
say this is obviously blue but it
classifies is it red and it classifies
this red as glue however if you don't
take nothing more see more complicated
and take the centroid of all the things
that were classified as blue so the
centroid is not here but up here because
of those patterns and the centroid here
is moved down then then use the bisector
of the classifier then you have a new
classifier that's essentially error-free
so this is the you know it looks like
you're getting something for nothing and
you are so I showed this 30 years later
to Henry Baird and he was skeptical as
he ought to be and and I said try it and
so he tried it on millions of his pseudo
random Singh characters now the other
are not error rates here because the
error rates on the different this is a
hundred fonts and on some phones the
error rate is two percent on other font
is point
% so we are only talking about how much
the error rate is reduced not about what
it actually is the averages are
essentially meaningless they are
dominated by the most error-prone
classifier and you can see that almost
all the fonts improved in all the
different sizes and the ones that got
worse only got a little worse so if I
finally his conclusion was that there is
a large potential for green and low
downside downside risk this was at the
time when he was wearing a manager head
so this is not so technical parlance
there this all these things were just
estimating the mean but if you estimate
the variance the same way you get
further improvements and Harsha geometry
nene was my student tried that and it
worked as expected so so you may well
ask if if a estimate unsupervised
quote-unquote estimation of the mean and
the variance is good what about the
covariances well it turns out that it
requires these are real things with 50
features there are 1200 covariances you
need for every class tens of thousands
of patterns so you really need very
large documents possibly books but
nothing that we are available covariance
is need for unsupervised estimation for
each source need much larger and data
set a different different idea
completely sometimes you want to use a
human interaction and the usual thing is
that there is a fair amount of human
interaction when you acquire the data
and then the computer runs and runs and
as the other end there is some human
correcting mistakes and the question is
can you use interaction more
effectively and this particular project
is on flower recognition one of the very
nice things about flowers is that the
field work is so pleasant we collected
these flowers in many expeditions to the
New England flower garden I don't know
if you know that saw us half an hour
wildflower garden it's about half a mile
marlborough half an hour from Boston you
may well as the philosophical we can't
use ordinary flowers because ordinary
flowers don't have class distinctions
day they're hybrids you can make
anything anywhere you know you can cross
them but wildflowers by definition have
distinct species the question is can you
really have a wildflower garden they
have been growing the same wildflowers
there for 70 years and they know exactly
when they blew so this is an interface
on a PC and the computer does its best
to do the to estimate the boundary we
try to fit it with a roll curve which is
a six parameter mathematical curve
embedded 200 years years ago and then if
it isn't right the human can can change
it if it is a one of the top three these
are the top three candidate occur the
pacifier ranks the flowers doesn't
classify them by rank them only it shows
the top three candidates if you think
one of them is right you click on it if
if you don't you can either correct the
boundary by dragging on the handles or
you can scroll to other species or you
can look at other samples of the same
species we have three four five samples
of every every class but if you don't
see it among these then you can scroll
or again you can correct and so there is
an interplay between the Machine and the
and most of you can see that very likely
the middle k is ranked second it's
this is a case where the training set is
very small so here is the general flow
the Machine extracts features and
displays the tops tree and we can either
the human modifies the boundary or
classifies it by clicking or looks for
more classes but the important thing
which connects it to the previous slide
is this red arrow is the fact that after
you have classified it most likely it's
right you saw a human make mistakes most
likely it's right so everything gets
reactivated again before you look at the
next flower so it's also adaptive now
the adaptation here is complicated the
system improves but you have to be
careful because the human improves to
you get better at classifying flowers
surprisingly the engineering students
eventually were able to classify flowers
so the point is that this kind of an
interactive system is much much more
accurate than a machine alone a machine
alone the top candidate is really very
poor here very small training set and on
the other hand it's much faster than the
human alone human could of course look
through the hundred flowers and
classified but a hundred classes there
are honors classes the only six classes
so it's much faster than human alone and
much more accurate than machine alone
you could argue about the fact that the
I we ought to have a better classifier
if we have a better classifier then the
margin of the interactive classifier
will be even greater because you don't
have to very often interact so yeah the
use the best classifier you can have of
course so here is the same kind of
seeing in with a personal pocket
computer a master's project little later
and this is inserted these slides
because this is what I would like to do
but we haven't
then these are these are ugly as sin
there are these atlases have about 2,000
kinds of skin diseases each there are
about 2,000 conditions that are
dermatologist nose and they are
illustrated here and they are not nearly
as nice as the flowers to look at but
people have talked about of course the
reason why we are looking at skin
disease is rather than heart disease is
because the skin disease is you can take
a picture with through the cell phone
and people have developed color shape
and text is features one big thing that
goes for you is that almost a skin
disease is a skin condition is never so
bad that your whole skin is is a change
so you can take a compare you have a
baseline because a person's skin is
relatively uniform if the booboo is on
your left hand take a picture of your
right hand whatever and you can also
with just put a filter in front of the
light and take various pictures and you
can also with a cell phone take a
picture from far away and get a picture
of the distribution of spots for example
measles and you can take a close-up or
you can see the details of one spot so
it seems to me that what there are a lot
of possible applications these
and I'm sure that you can think of
others the the big thing is that well
occasionally there is the dermatologist
for one reason or other or you don't
want to consult a dermatologist
sometimes you want to keep it private
but a big thing about the cell phone is
that it can also ask you whether you
were out in the bushes yesterday whether
you are running a temperature does it
itch do the spot change from day to day
all kinds of things like that and it can
also keep a copy look again compare it
with tomorrow's picture yesterday's
picture and so on and possibly if it
turns if you give it permission it can
forward everything to a specialist or to
the National Disease Center so you can
find if there is an epidemic of be bites
or whatever that you have so there are
lots of advantages in using a cell phone
and the pictures now are of course I
think good enough unfortunately the
atlases on the web are all studio
pictures with usually with film cameras
that were digitized so there is and and
and i have to say that it's a problem
for a university engineering department
to take pictures of people's skin it
needs a lot of permission so it's
something that really has to be done
collaboration with medical people ok
this is a project for years ago i'll go
through this rather quickly the it's
it's a recognition scheme where we are
looking at several several at words but
we don't have samples of the words so
there is a reference set of words maybe
a thousand words but we are trying to
recognize words for which we don't have
examples of and the way we try to do it
is by matching pieces of the word we are
trying to recognize with species of all
1000 words
but peace is longer than one character
because one character is barely
recognizable in handwriting there isn't
much there so that's what this thing
does at nannys teaching in Jordan here
are some examples of features and I mean
I'm sure that you can't read this but
this axion w Poppa Poppa Pump is a
transcription of of these features and
in red is this is just some some of the
features so there are many features and
the occur sequentially in time or in X
and for example here these two segments
of these two different words match by
string matching some kind of a string
matching algorithm and suppose that the
unknown and word is is really founding
and the reference word is amendment and
there are different there are pen up and
pen down here and that's why there are
different colors here but all we are
showing here is one word we are trying
to recognize any one example of a
reference word one of the hundred
reference words or thousand reference
words now suppose that we don't know
what founding is but we hypothesizes as
contract and we know the transcript of
the reference world we know that these
two words share the diagram nt4 the
reference words we know the transcript
so if this where contract we would
expect that something in the middle
somewhere around here would Mac
something at the end but it doesn't so
that's a little piece of evidence that
maybe the word is not contract on the
other hand if B hypothesis
hypothesize it as founding which is
actually the case then we would expect
that there would be a match in the
middle and a match here and there is
actually a match so so this contributes
evidence now this is only on one of the
thousand reference words but each
contributes a little bit and there is
some kind of a big Viterbi tell us this
is a usual two-dimensional
representation of string search and we
localize it depending on where we off
Lee where we expect too much the match
to happen so we only pay attention to
mattress in some region of this space
here and this is basically the sum of it
can be pre computed because we know the
locations of the reference words and we
know we know the the description of the
words we know the transcript of the
words we are trying to recognize we just
don't have samples of the meaning we
don't have analog samples we know how
they are spelled otherwise it's in the
dictionary but we don't have samples of
them so it's not the conventional word
recognition and here are some some
writers and this is a database we got
and we we do about the best we do well
known on some of on the for writers we
like which are of course the writers on
whom we do well we do as well as the
external system and the external system
between us is the IBM research system
also their data and it's a group of 15
people who have worked in over 15 years
so I was pretty happy that we were in
the ballpark with that now here also yep
great
we are so for we have to run a viterbi
for every candle every candidate
transcription but we abort early we may
not go through the full Viterbi search
and we only go through a partial Viterbi
in the restricted space for examples I
think we have the maximum was a thousand
categories unknown categories yes that's
correct we only had a thousand words so
we had to shift in ten riders it's
already yeah that that's right and we
have to run the Vitter before everyone
except that it's only a partial return
and it's aborted beyond and it's not
that fast yeah we are this is not not
yet this thing where you write in your
honor think that implementation is in
its matlab pops up okay but here also as
soon as something is recognized we added
to the reference list and so so again
even if we just use now this is I
believe on a hundred hundred reference
words and 100 unknown words which are
different different words and we
recognize them but we don't touch it we
just add the recognized sinks to the
reference list and we rerun the same
ones without ever having said what and
each iteration it gets better so our
idea was that this is for if if if you
have a personal computer and you are
writing eventually it will get used to
your writing and you don't have to
identify the worst this doesn't involve
correcting the words you just feed it
back in and you can see that the
improvement is quite significant and so
eventually it essentially adds the
hunger eventually there are nearly 200
reference words but it is good as if
there were 500 which are not represented
of those so it's another example of
adaptation I'm going to skip this this
is a complicated scheme yep that's right
that's the downside risk and and
everybody has reservations about
misclassifying I mentioned to you that
there is only one one one person I'll
get to you let me answer this one is one
guy called morrow she probably know who
has actually written about this kind of
adaptation in a commercial in a
commercial situation and even the
general idea is that you fight this by
only adding things that you recognize
confidently that's the natural thing to
do and that occur to me in 1966 and I
tried it and it never worked and barely
insisted it must work and he spent two
months barely made it to the conference
doing it it didn't work it doesn't work
because when you add only confidently
recognize words you are adding worse
near the middle of the distribution is
the boundary that you must correct you
are tweaking the boundary so yes it's
hard but it doesn't hurt us if we
haven't it usually doesn't hurt enough
to come offset the good but yeah let me
come around because I've all will never
hear
what you recognize who
to recognize
the question is once we add the word in
the new data to the new data set is it
possible as we add data further to
change the label the experiment I showed
you we reiterated the whole data set and
so in that case it's quite possible that
on the next iteration it would have a
different label yes and in fact I've
learned the case it wouldn't keep
getting better because you would have
the same label and you would have the
same training so yes anything else yes I
say this is a very complicated scheme
let me talk about this here on we have a
training set where groups or the style
or the typeface is not given but you are
given groups which are told are in the
same style or handwriting or or typeface
but not which just that this is one
group and if you don't have that you can
do clustering and get the same thing but
you don't know which and the test set
also comes in groups but there is no
correspondence between the training set
and the test set and so in other words
so you don't know that this is
bookman size 12 and what we classify
instead of single characters is fields
which may be an entire document minimum
of two characters so this is not
classifying anymore a character at a
time and the idea is this many of you
has seen this so the question is whether
these this is a one or a seven how many
of you think it's a one huh nobody the
one on the right the one on the right
well some of you are wrong now instead
of looking at one character we look at
all four of them do you change your mind
so that was the one on the right but
really there was no evidence because we
really try to make them look the same so
this is the idea and style
classification this is highly simplified
because yeah this is highly simplified
but this is the idea so here if we have
this is this would be what you would get
for a singlet classifier and again if
you are classifying if you happen to
figure out which kind it is by looking
at all the patterns in the field you can
change the boundary and do better and
you can do that and this is again
simplified because you classify
everything at once and I'm only showing
the boundary for a single pair of
classes and I can't show it on to the
boundaries for all the classes at the
same time this
is a tricky slide and those of you have
dune classification will understand that
it's a one feature classifier not two
features it's one feature but is showing
this there's only one feature in each
character these are ace and these are
bees there are two kinds of ace and two
kinds of bees and the picture shows the
feature for two patterns not one you see
that it's so so this distribution of
patterns is an a1 and the day one this
is an a1 and a2 this one is an A one and
A B 1 and then a two and the b2 I don't
have here any a2 b1 it would be here
there are no mixed Styles here only pure
styles and I expect only pure styles if
I had if there were no constraint on
which patterns which pair of patterns of
course at once that each of these would
be a for distributions and the optimal
boundary between the four classes there
are four classes a a BBA BB the optimal
boundary would be this that is the sing
but the singlet classifier does since
there is no constraint between we don't
know any more about the second patterns
when we have seen the first or the other
way we might as well classify each
individually but if we know that they
always occurring same style pairs the
optimal boundary is this and this is
done by simply plotting these gaussians
and numerically finding there's no
analytical expression for that it's a
mixture of gaussians and there's no
analytical expression but we can
numerically determine the boundary is
this this picture here I think this is
somewhat difficult to picture so this is
what style classification
this is one aspect of style
classification of classifying two
parents instead of one now if if we were
doing the fields classification in the
compound you might think it's like were
specification so if you have to classify
sequences of four digits Ukraine or all
possible sequences of four digits on
every possible word that's worth
classification but if you are doing
digits are too many however with a
Gaussian assumption the style
classification you can train on only
every pair that's still more than
training on every single it but from
training on pairs you now find out all
possible relations and you know the
relation between this and this and this
and this and this and this and you can
get an optimal classifier and this shows
this classifier on a really long
Gaussian data the national institute of
standards and technology as these three
and as the training set and one of them
is the more difficult data set than the
other and you can see that you can four
fields of two you got some gain four
fields of five you you get much more
gain of course the reason why the error
rate is higher for fields of five is
because we are counting feel there is
not cingulate error so obviously there's
more chance of making an error in 15
characters than one of one but it's much
better with fields of 5 and again this
so style classification is kind of a
good thing this is with support vector
machines and it's again a difficult
slide suppose that you have a triple x 1
x 2 x 3 there are two styles a1 and a2
b1 and b2 c1 and c3 1 and c 2 i'm sorry
and
we classify we have three linear class
if a pair wise linear classifiers now in
this region because of the fact that
this first kind of see and the second
kind of a are so near each other they
are easily confused so the probability
of this being this x1 being a see is
slightly higher than it being an A and
it could well be that it's on the wrong
side so we have suppose we have three
patterns or temptation would be to call
this a c.c be that's what a singlet
classifier would do but that's
inconsistent because the second one is
classified as a sea of the second kind
and so we really think it must be a
mistake and B a B or something is that
the one I'm not sure and you saw you can
what you do is calculate for all styles
the probability of all of them being in
the same style and pick the maximum and
so this is a way to use support mac
vector machines you can also have
training on each style separately and
then you have many more regions and the
classification is much more complicated
because you have to each classifier
science as zero and the one to an
unknown pattern and if you have a two
patterns it's a big table and you can
still find you still get a big gain this
is with singlets and all right all this
shows is that again with with increasing
field side you go faster now what
happens if nothing works this is my
concept of of a multi-dimensional space
where there are classes these have to be
equidistant because if they were in the
alphabet the representation would have
changed over hundreds of years because
with the same
Matthew can make the space between
classes bigger but only by making the
class is more complicated take you
longer to write bigger to print so if so
naturally everything has evolved and
this is experimentally verified to to be
at the vertices of a multi-dimensional
tetrahedron 26 dimensional tetrahedron
regular tetrahedron in the case of the
lowercase alphabet because again if two
were closer then you could decrease the
one that make the two those two farther
and the expense of the two that were
farther and then in addition to that
there are styles in each which are
governed by different distribution see
there are four styles for each classes
so it gets complicated to to think about
the optimal Gaussian classifier but okay
now here is a question what do you think
is the this is a weekly constrained
situation what do you think is that
class over there what's your best gas
which
it's a four yeah it's probably a floor
and probably is a four yeah it's some
transformation so but this we haven't
solved yet and but but I'm sure that
something interesting can be done but
what do people usually do is go to
linguistic concept and I will go to a
very old sing here is a situation where
you don't know the classes but you you
cluster them and all the ends are put
into cluster 2 because they look
different in some feature space so we
give a name to each cluster in the
arbitrary number and then we have
something that I call the ciphertext
it's an encrypted version of the data so
the output of that unknown sentence is
this and then we have a language model
which decodes it either an Engram
frequencies or lexicon and we have done
this a number of times here is one that
was with Steen ho so here is the unknown
texts and can you read it so the experts
in this is a bizarre typeface it's
actually the typeface is called spits
clips but many of you don't know it I
see maybe it's too small here is the
same thing in 22 point type still hard
so we apply the the algorithm I just
showed you this is the actual ground
truth this is what the algorithm gets is
not so bad it makes mistakes for various
reason that we we understand but it's
not so bad so so here is an example
where a language context is every single
shape is nothing except that we are
assuming that every pattern has
different shape every class has
different treatment and independent
noise but other than that there is no
assumption of a no pre-defined concept
of
of the shape of the alphabetic
characters like I'm assuming you're sure
sure yeah yeah yeah the good important
assumption important assumption yeah and
then all both the dictionaries and the
Ngram frequencies are hideously
different and English is nice big for
Italian you need much bigger vocabulary
ok I think that's okay what I want to
share this is no I think this is the
only equation that I have shown what I
want to say that in style context we are
assuming that there is statistical
dependence between the features the
shape features of the pattern in
language context we are only assuming
that there is statistical dependence
between the class labels of the pattern
so the formulation is really very
different and the solution is different
ok that's the mathematical difference
all right here is the conclusion
I do like you know what Thank You
Illinois thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>