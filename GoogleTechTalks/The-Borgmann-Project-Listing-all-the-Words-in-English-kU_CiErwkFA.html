<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Borgmann Project: Listing all the Words in English | Coder Coacher - Coaching Coders</title><meta content="The Borgmann Project: Listing all the Words in English - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Borgmann Project: Listing all the Words in English</b></h2><h5 class="post__date">2008-08-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kU_CiErwkFA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">talk i'm giving today is the boardman
project and describing it which is a
project to list all the words in the
English language the reason we're it's
called the Boardman project is in honor
of Dimitri Alfred borgmann who has been
called the father of low gala gleau
ecology being the study of wordplay
particular sometimes we call it
recreational linguistics as you know of
course linguistics goes back for a
couple hundred years but wordplay goes
back a lot further Aristophanes coined a
word for hash which when transliterated
to english comes out to 183 letters so
it's been around since the beginning of
time what the contribution that borgman
made in a couple of seminal books and
later in a journal was to try to
systematize the study try to come up
with all words of a certain character
and so on and of course foundational to
that is what's a word how many words are
there in English so in his book beyond
language borgman gave an estimate
between two and three million words in
english now that estimate is quite a bit
larger than any existing list of words
so the question is where to get that
estimate where do you come up with it so
here's a place to start this is the
preface to the mirin Webster's third the
new international third edition and in
which go the editor at the time notes
that there's about four hundred and
fifty thousand words in the book in the
dictionary but that it would have been
possible easily to him to expand the
lexicon by many times here's a similar
quote from the Oxford English Dictionary
in which it's noted that the lexicon
again is much larger than can fit in the
dictionary and in fact the the last part
of the preface is interesting where he
says that the question of how many were
are there in English language cannot be
answered by recourse to a dictionary so
the question is the purpose of the
project is to try to answer that
question and to come up with a list so
the first question you have to ask is
what is a word okay so for the purposes
of this discussion I don't want to try
to define a word I'm just going to say a
word is a the smallest unit of meaning
it's analogous to a letter which is the
smallest unit of spelling or a phoneme
the smallest unit of pronunciation and
if anyone has a question about what i
mean by that i'm not going to get into
some cycle of trying to define things
that that's as much as I'm going to say
about what a word is okay so current
unabridged dictionaries if you combine
together all of their entries you get
about 500 thousand words so if we accept
goes estimate that it would have been
possible to have many times that many in
the dictionary and if many times means
four to six then we do get a number
somewhere around Boardman's estimate of
two to three million it's important to
point out by the way we're not just
saying it's some big number I'm not
saying it's four million and we're not
saying it's 1,000,000 borgmann said two
to three million and and that I think
turns out to be a pretty good estimate
and part of the content today will be to
back that estimate out numerically so
question is how do we find those words
well let me go through some of the
problems in fine I isolating this list
the first is the problem of names a name
is a word it designates either an
individual or a class of individuals and
there seems to be essentially an
unlimited number of names so how can we
say that there's two to three million
words in English it essentially names
can be generated at will we also have a
problem of prefixes and suffixes we can
take a word like countermeasures and add
counter counter measures and counter
counter counter measures and so on ad
infinitum and generate infinitely many
words all of which have distinct
meanings again so how do we say there's
a finite number of words another problem
is English is very loose with closing
open compounds there's a example I pick
up his air vent and air hyphen bent and
airspace vent so in this room there's an
air vent it's not a word that is rare or
unusual it's just a word that sometimes
spelled open and sometimes Bell closed
and there's an awful lot of examples
like that then there's the problem of
derived forms we have the verb Shanghai
and we have the participle of Shanghai
which can be converted to a noun
Shanghai and then pluralize Shanghai is
the reason that's interesting to do that
is to Alec ologist is because that word
has exactly two copies of every letter
that's in it so we want to know whether
that is a legitimate word we also have
the problem of rare words specifically
we have a lot of different dialects we
have obsolete words where do we draw cut
off we have jargon slang technical terms
loan words from other languages I mean
what is the English language it's a very
fuzzy demarcation so I'm going to give
you an example of a rare word that we
want to include in the language and the
word is a Mitchell er the reason we want
that word included is because it's
constantly coined it's a word that needs
to exist it wants to exist it's
incorrectly coined in the sense that
it's not correct Latin in the way in
which is created but that doesn't really
matter words get created all kinds of
ways in this case it's a parallel word
to the it's sort of semantic cousin of
Uncle ur so that word shows up in many
reference works as I said it's been
coined independently many times but I
just want you to remember that word
we're going to come back to it that's an
example of a word that we want to be
able to say yes that words in English
so how do we solve all these problems
well the first question is I said I
wasn't going to talk too much about what
a word is but I am going to talk about a
thing called the paradox of the heat
paradox of the heaps of very old paradox
dates back to the sofas and Beyond and
the paradox goes like this everyone
agrees that one grain of wheat is not a
heap and everyone agrees that if you
have something that is not a heap and
you add one grain to it also that's not
a heat so logically no matter how many
grains of wheat you have in a pile you
don't have a heat so there's something
wrong with that argument clearly that
argument is incorrect because we all
know what a heap is and we're we freely
identify heaps of wheat and other things
all the time so the problem is with a
concept called vague terms which is now
a very much studied problem in
philosophy of how do we identify what is
true when it went can you apply a term
to us to an object in one can't you and
the secret to solving the problem of
vague vagueness is probability in other
words we no longer think of the
application of a word to an object as
being yes or no we think about it as
being a likeliness a likelihood the
probability of being applied and as you
might be not shocked to learn an awful
lot of words are vague in fact there's a
school of linguists and philosophers who
would say all words are vague it's just
a question of to what degree are they
vague so it has something to do with the
shape of the distribution but we don't
need to believe that for the purposes of
this talk we just want to argue that at
least the word word is vague that what
is a word is a vague concept and in
particular we can we have some help here
from the philosophers again specifically
vitage tyne and said wait a minute a
word is not just something that anybody
makes up and says okay that's a word I
I call this a blog that doesn't make
blorg a word a word has to be part of a
language it has to be recognized by
other people there's no private
languages so to be understood or to be a
word something has to be understood by
speakers of that language and so the the
probabilistic solution to the problem of
what is a word is the probability that a
speaker of that language will recognize
it now probability has its own set of
problems there's a whole argument about
what do I mean by probability so again I
don't I'm not taking a position on that
subject if you were a frequentist what
you might mean by the probability of a
word is that the number of people in the
language who recognize it divided by the
total number of speakers of the language
or if you're a bayesian you might mean
that it's a the probability that an
ideal speaker of the language or a model
of the the speaker language would
recognize it so I'm not again I'm not
taking sides or attempting to enforce
any particular definition of probability
how does that probability few solve the
problem the way it solves it is in the
following ways first of all as far as
names are concerned the vast majority of
names are not recognized by very many
speakers of the language the few names
that are our words for example Christmas
that's the name and it's a word prefaces
and suffix is as you stack them up they
make the word very hard to understand so
again likelihood falls off compounds now
there's where there's an exception for
example i pointed out air vent everybody
knows what an air vent is so that's a
word that's a word that's not in any
dictionary no dictionary has the word
air vent in it and I'll explain why in a
minute but it's a word perfectly
legitimate English language word again
rare words are are not understood by
definition that's what you mean by a
rare word not understood by very many
speakers and derived forms again same
problem is with prefixes and suffixes
you create words through exchange of
derivation where the likelihood of
understanding falls off at each
application
of the generative principle so how would
we assuming that probability is the
right view to take on this how are we
going to estimate the probability of a
word that a string is a word well one
obvious idea is which most people would
use as a common sense solution to the
problem is you use a dictionary if it's
in a dictionary a large dictionary it's
very like you know it's not as likely as
if in a say a college-level dictionary
or high school dictionary but there's
some there's some argument however
there's problems with that solution it's
not a crazy solution but there are some
problems the first problem is
historically illustrated by historical
fact that early dictionaries did not
include common words it was not viewed
as being important to define common
words and it was expensive to print
dictionaries so the word the this what's
the point of wasting page space on
defining common words so they would just
weren't too fine another example is air
vantage fri mentioned which is not occur
in any dictionary the problem is that
because dictionaries cost money to print
there has to be a reason to put a word
in the dictionary and in fact
essentially a dictionary is an economic
act in cute including a word in a
dictionary is an economic act it there's
a cost-benefit analysis and so the
people who create dictionaries create
them and put entries in them if they
think it's likely that that entry will
be a value to their the purchaser of the
dictionary now that's not the same as
saying that it's likely that they will
be understood you see so there's a gap
there so it actually is an incorrect
assumption that words that are easily
understood will be in dictionaries
they're in fact the vast majority of
words are not in dictionaries so okay
another idea let's use large corpora
there are
many large corpora available just to
illustrate kind of a jump that's
occurred historically the usenet corpora
was corpus was available relatively soon
after usenet ran this was in the early
late 80s in the early 90s and it had it
had about 1 million distinct strings in
about 1 billion instances so that's all
the Usenet traffic that was run through
and captured and turned into that corpus
Google hat in 2006 early two thousand
six took a snapshot of the web and
released to the linguistic data
consortium a corpus-based on one
trillion word instances it contains over
10 million distinct strings now this but
there's an interesting detail which I'll
get back to later but what what the
reason for this is and the detail was
that unlike the Usenet corpus which
simply had every word that that had been
sent to through using it for whatever
reason the Google corpus only occurred
included words that occurred at least
200 times on the web and of course
Google did a lot of work there by the
way in first of all they wanted to
isolate English language web pages and
then they also they it's non-trivial
just to decide what is a word because
there's a lot of stuff going on on a web
page like graphics and so on so both of
these corpuses corporate required a lot
of work to get them down into some kind
of a sensible list however there's
already some problems just with these
with any of these corpuses the corpora
one is the word counter counter measure
just as an example that word is in a
college-level dictionary it's in the
11th collegiate but it's not in the
Google corpus and the reason it's not in
the Google corpus is because the it had
less than 200 hits now if you go to
Google right now and look up counter
counter
sure you'll discover it has about a
thousand hits but if you go and peer
into those hits a little more carefully
there's several issues that will happen
one thing that will happen is the actual
number of hits will shrink down to less
than 200 but so there are actually
several other problems which I'll get to
so but let's take a look just we have a
list of words from dictionaries let's
take a look at how the Google corpus
does against those that just that list
what we'll discover is that there are a
lot of college-level words that are not
in the corpus about ten percent nine
percent or so of the words that are in a
college-level dictionary don't show up
in the google corpus i give some
examples there obviously i'm not going
to list all 10,000 examples this really
scary part though is the next fact which
is over forty percent of words that show
up in an unabridged dictionary aren't in
the google corpus missing an awful lot
of words in that corpus and the problem
is the corp is cut off of 200 why did
the team here at Google cut the corpus
off at 200 well here's why boy it's hard
to read I guess those are okay you can
read those last two lines the blue ones
okay i'm giving you hear a count of how
many words there are by frequency in
this corpus okay so the frequency is the
first column the number of words is in
the next column and the ratio of one
call one row to the previous row is the
last column the last two rows are
extrapolations in other words I don't
actually know how many words exist in on
the web in January 2006 so I'm just
using this ratio of five times
so in other words for every factor of 10
that we cut down the frequency we see a
factor of five in the number of words
but it doesn't matter whether that
number is exactly right or not the point
is that as soon as you get down to all
the words defined in the entire web
strings defined on the entire web you
get somewhere between two and three
hundred million strings different
strings now let's go back to our belief
that there are two to three million
words in English so you've got two to
three million words that you're trying
to find in 200 to 300 million strings a
one-percent signal-to-noise ratio
extremely hard to extract that
information and so what the team at
Google did was very sensible he just
said okay we'll just cut it off at 200
now what happens roughly at that at that
cut off well let's start with sampling
this can be i'm showing here data just
for a particular sample which is the the
2747 strings that start with the three
letters a ir but i could this this same
pattern shows up pretty much everywhere
no matter what string you pick so I've
given samples of non-words which is all
the different ways for example that
people can misspelled aircraft and an
awful lot of that has to do with the
layout of keys on the keyboard and I
also give samples of words that are not
in the corpus I mean sorry that are in
the corpus but not in the dictionary
you'll see the first column if you look
under college-level to see a nice little
peaked distribution where it starts off
with there's three words that have let's
see is that 10 million hits and then it
goes up to 30 ish and then goes back
down again so that's it that's a pretty
nice distribution that says okay cut the
college level dictionary did pretty much
what you thought it would do it has all
of really common
words and a lot of sort of middle common
words and then it falls off then there's
not much left over which is what you
would expect and if I put a high school
level dictionary up here you would have
found the same thing a peaked
distribution but the peak would have
been at the higher hit rate the
unabridged is where the problem occurs
is that the unabridged starts off and
then peaks but doesn't go back down
again because the cutoff cuts it off
before it gets a chance to go back down
and in fact the number of words just
keeps going up so the cutoff is
essentially cutting off half way through
the distribution of words and that's why
there's more than forty percent of words
are not in the corpus okay so a corpus
by itself is not a very good solution to
the problem of estimating the
probability it has several problems what
are the problems well I didn't you can
sort of see from when I showed you
earlier the problems of misspelling
obviously the problem of names the
problems of spam and there's a subtle
problem that I didn't mention which is
the problem of use versus mentioned in
other words many words that occur in a
corpus are not actually being used in
the corpus they're just being mentioned
in the corpus as we all know probably
google knows better than anybody there's
an enormous sort of arms race going on
with trying to get people to go to your
website and one of the ways to do that
is to try to overcome Bayesian filtering
and one of the ways to do that is to try
to include words that are uncommon well
and what does that mean well it means
words that are not really being used on
the site they're just being mentioned on
the site so there's an enormous kind of
distortion by the way there's a similar
distortion that's occurred through the
creation of the use of spell checkers in
the early days of email and in the very
early days of the web before a lot of
use of spell checkers
integrated into word processors there
were many words that were misspelled
more more often than they were correctly
spelled an example is the word
desiccated that word in the old days was
more was misspelled more often than it
was correctly spelled but because of
spell checker is now being essentially
ubiquitous you'll find that now desig
the correct spelling of desiccated does
in fact dominate over the incorrect
spelling so at any rate the point is
that that the corporate by themselves
are not going to be enough to solve the
problem of estimating the probability of
a word that the string is a word so what
we've decided to do in the project is to
try to combine two approaches
essentially under the general theory of
modeling human understanding and the two
approaches are a Bayesian model of word
understanding which we bewitch
neuroscience gives us some reason to
believe is how the brain works that's a
whole long discussion which I'm not
going to bore you with but there's just
results from neuroscience that makes us
fairly confident that the Bayesian
inference is how the brain works and
there are also other theoretical reasons
to believe them and the second is a
generative model of word formation which
comes from linguistics in other words
words are not formed randomly words are
formed for reasons historical reasons
for example we talked about the word of
mitchell er which I'll get to in a sec
so let's take an example of a Bayesian
model of word understanding we look on
the decision about whether of computing
the probability of the word Shanghainese
that the string Shanghainese being a
word as simply an example of Bayesian
inference and in each step of the
inference has a probability the
probabilities come from observed ratios
of occurrences of related cases or
similar cases which of course is the
tricky part what we invest similar case
but nonetheless it's a fairly
straightforward application of Bayesian
inference
and does rely on the corpus to give us
some of that information the generative
model of word formation comes from
essentially from linguistics from the
information that's largely from
information it's already in dictionaries
together with information that linguists
know very well so things like how
parallel ISM works how of course
etymology how sounds change how spelling
change and so on over time and as I gave
an example earlier we can start with the
parallelism between avuncular and amita
and get to the words of on killer and a
modular we can predict that that's where
the language will land and in fact we
have some reason to believe that so by
combining these two together we create
an iterative approach in other words
you're working out word from the
dictionary using the linguistic
information and you're working in word
from the corpus using Bayesian inference
entire amount from the height so I'd
like to believe that this is going to
work well there's some underlying reason
why this program will work and the
reason is because there's an underlying
process that's that's common to the two
things and namely there's a thing called
zips law which linguists identified a
long time ago for the distribution of
words in a in word usage and pretty much
any sample and it follows a power law
and that but we now know that these
power laws come out or result from
scale-free underlying systems similarly
we now know something about how the
brain works and that the brain has the
the the neuronal connections in the
brain are scale-free it creates a
scale-free Network of axons so we have
reasons to believe that these two
linguistic laws together with the beige
in laws are actually going to work
together to create a stable solution to
the problem and we also have the
evidence that language exists which
makes us feel pretty confident that
there is something that we can fall back
on so it
in conclusion a goal of my programmer of
the program that the people who are
working on this project is have adopted
is that what we're going to produce is
not really a list it's not a list of
words what is it's a process which has
various assumptions in it parameters
such as probabilities and rules and from
that from a given set of rules and a
given set of parameters you can assess
the likelihood that any given string is
a word and then you're free to make your
own choice and choose choose to change
one of the parameters or delete or add
one of the rules and out of that you'll
get a course a slightly different
probability assignment but ultimately
the idea would be to go in and say okay
look given these set of plausible
assumptions give me a list of words sort
of in decreasing probability and the
idea would be the words high up on the
list would be obvious words common words
in the words later on the list would be
more and more obscure words and at some
point you would say let's say if you're
the International scrabbled Association
you would say okay we're going to cut
off at this point right here because the
way they work right now is they cut off
based on the inclusion in various
dictionaries and there's not a lot of
words that aren't as I've already said
that aren't included in dictionaries
that should be legal for Scrabble play
okay so that's the end any questions yes
tours yeah you we you almost oh it the
quick let me repeat the question the
question was it our if you have
different forms of word like cat and
cats the plural do you consider that one
word or two words and for the purposes
of this project we're considering that
two words and the reason for that is
because there's some ways to generate
inflected forms and other forms of words
like transitioning from as i mentioned
from jaron to a noun that aren't it
aren't a hundred percent going to be
accepted and so it's just a simpler
process to assume that all move
movements from one form to another have
some likelihood but less than and moved
from a noun like cat to a noun like a
plural like cats is obviously going to
be a very high likelihood okay any other
questions yes
okay so the question was amongst the
problems that I mentioned at the
beginning I didn't list misspellings as
a problem why didn't I and the answer to
your question is what I was trying to
say in that earlier list was things that
are specific are not specific to
dictionaries or corpuses or so on but
you're right the problem in misspellings
is a big problem for corpus linguistics
and in fact if you go back to that slide
where I was showing all the misspellings
of the word aircraft it's useful it
would it would be useful to go back to
the original source material and fix
those misspellings because those
misspellings are telling you know
there's actually information and all not
in so much in the misspellings but in
the in the text surrounding the
misspelling where the person
accidentally typed the wrong word and if
you fix that you might be able to make
get some meaning extracted from that
that you wouldn't otherwise be able to
get
right okay so the question is aren't
them I'm going to rephrase your question
aren't that lexicographers really right
in keeping air vent out of a dictionary
because isn't air vent really a
misspelling of a either hyphenated term
or an open compound and my answer to
that is it depends upon again everything
depends upon everything else so it
depends on how often it's used is air
vent used in the closed of way
frequently enough to generate 22 to
decide that well you know that's moved
over the hurdle to being a word because
there are an awful lot of binomial terms
just two-word phrases that don't use the
don't appear closed up at all or very
infrequently and so there is some sort
of a and that's an example of where i
said earlier what were conclusion is
we're not trying to come up with a list
or can I come up with a process that
will create a probability estimate and
one of the questions that we would have
is how do we decide when is a binomial
term turned into a closed compound and
when and however I want to say that
since I know a lot of lexicographers I
don't think the average lexicographer
would say that air vent is not a word I
think he would say Aaron is not in the
dictionary because its meaning is
obvious that's not the same thing as
saying it's not a word
I don't know why that particular word
okay the question was why did the
Microsoft spellchecker decide about 10
years ago or so that the word millennium
could be correctly spelled with one and
and and I it may be because there was
some Japanese auto manufacturer that
came out with a car with the word named
millennium with only one N in millennium
and they decided oh well we throw in the
towel at that point you know but there's
a lot of so I don't know whether they
had any rationality behind that or not
I've similarly would as i mentioned that
were desiccated and when I point out to
people that the word desiccated for a
long time was spelled more incorrectly
more often that was spelled correctly
some peoples natural instinct is to say
well wait a minute in that case the
incorrect spelling is the correct
spelling because if something spelled
more but there's reasons I would want to
argue that that by pointing out that it
that yet that's putting too much
emphasis on only one aspect of what it
takes to form a word there are a lot of
other reasons why desiccated is spelled
the way it is that have to do with its
etymology and it comes from a from a
Latin word which happens to be spelled
the way it is and that's why that word
is spelled the way it is and if you just
rely on Corpus linguistics for deciding
what is the correct spelling of
something you'll get an awful lot of
weird results however and I now know I
don't have to make that argument anymore
because now it's spelled more often
correctly than incorrectly so yes you
know well then allow
Mitchell er that's a tie Mitchell isn't
it I picked because it's a particularly
thorny case the the argument that it
should be a word is somewhat subtle but
I don't think we and I and I like the
subtlety of this argument because i
don't like arguments that are sort of
you know too broad and 22
sledge-hammering um it here's why as
I've already said why it's subtle it's
subtle because it's actually if you say
well it's valid from the etymological
point of view it's actually not valid
from an etymological point of view it's
not the correct inflection of the Latin
term so there's something about the
etymology but there's also something
about the parallelism with another word
there's also something about its filling
a hole in the language which is why it
keeps getting invented over and over
again so what I would like to argue is
and if you don't have a system that
takes into account all of those factors
and actually comes out at the end and
says darn it a Mitchell ER that's a word
then you haven't really captured the
process that goes on and you have to
keep working on it until you until you
do have that system now however i just
did point out another somewhat subtle
when one of the most frequently
misspelled words is familiar and it's
miss it's one of the top ten misspelled
words because it's spelled in parallel
with simet similar this is for the fit
so what happens is they want the ending
of those two words people who are
misspelling I want the ending to be the
same so the either misspelled similar or
they misspell familiar and you could
argue a wait a minute on the one hand
you're saying parallel ism matters and
on the other hand you're saying it
doesn't matter and I'm saying you know
what that's true I'm being I'm saying
it's complicated and it's not easy to
figure out in what cases it matters in
what cases it doesn't matter so I I
guess I'm saying as the Greek said
moderation in all things we have we have
to have a balanced approach to the
problem it has to combine these various
influences
and in the end of the day we don't want
to be trying to legislate against and
this is the problem with many sort of
artificial language and spelling reform
movement spelling reform movements
people try to legislate against what's
actually going on out there which really
isn't what our job is our job is to
figure out what's going on and figure
out how to encode that in a series of
its science after all it's basically
science yes hey frankly I don't know the
question was how does the word in other
languages compared to the number of
words in English I think English has a
much larger lexicon and other languages
but I you know I have not studied other
languages and I don't know and in
particular I it may be that Chinese for
example has has an enormous word horde
that I'm just unaware of so I just don't
know the answer to that yes
well the question was do people who are
working on this project get into very
big arguments about what's a word on
what isn't a word and the answer would
be yes if what we were trying to do is
is have it like the definitive list that
says this is what the word is this is a
word or this is not a word and and so
very early on in the project we decided
that the only rational approach to it
was to say look let's just get a system
set up where you can change the
assumptions if you don't like that
assumption you can change the assumption
but the thing that still makes it more
than just making up your own list is is
that if you if we agree that this is the
basic process that's going then
sometimes you can't have your cake and
eat it too you if you if you think this
is the rule here and it gives you a
result over there that differs from what
your instinct is it's that's very
elucidating you learn something from the
process of doing it so there's a
consistency that compressing it down to
a set of rules forces on you that
otherwise you wouldn't really know so
it's simply it again it's like science
you try to simplify down to this to a
set of rules and processes that will
generate the language that most people
would agree now of course isn't that no
two speakers of English will agree
completely and whether everything is or
isn't a word so it's actually a feature
of the system you want it to be to be
able to incorporate those distinctions
you want to be able to say yeah here's
why some people think this is a word and
other people don't more questions yes
okay well we're we're go got the number
was from his knowledge of the way in
which Miriam Webster created the new
international dictionary in other words
he he had access to the process of
deciding whether something gets in the
dictionary doesn't get in the dictionary
and so he was able to say look we've
done a reading and marketing program
where we've gone and looked at you know
well one thing you learned from this one
takeaway from this is it's not enough
just to go to a very large corpus
because you won't you won't get the
words you have to do something more than
that and so lexicographers have known
that forever and what lexicographers do
is they do a program called reading and
marking were they you know sciences with
design defined to be the process of
torturing nature for her secrets and so
reading and marking is a little bit like
that you purposely go out and try to
read stuff that you know is going to
have different words in it unusual words
especially journals in certain languages
and certain areas and so on so you it's
very much like the process of creating
an encyclopedia or anything you have to
consciously go out and search out these
words so as but gova sort of sitting at
the top of that food chain so he sees
these words as they come through day
after day after day for months and years
and so he gets in his mind he basically
knows I know how many words I could put
in versus how many words I did put in
and so when he says us you know many
times whether that means four or six or
some number like that I take him to me
know what he's talking about but that's
really the the correct ratio between the
real number of words in English and the
words that are in the dictionary yes
so the question is could we apply this
work to the Google corpus and do better
than just cutting off at two hundred to
do sort of you know here's the good 10
million words or the good two million
we're selling an answer obviously is yes
and the reason I'm here talking to you
guys today is because I would like to
work with the people on my on the team
would like to work with the people at
Google to try to improve with the the
Google corpus and who knows as I already
mentioned one idea which is the spelling
you know improving his been around for a
long time spell checkers and they're
very in the one that is used in
Microsoft because I know where that one
came from there's a small number of
rules like I think 20 or so rules that
it applies and they have to do with the
keyboard layout and so on stuff like
that could be done very efficiently
computationally and could greatly
improve this signal to noise ratio in
the in the Google corpus so yes is an
awful lot of things that can be done
we're just at the beginning of learning
of this stuff and I think we the people
on the team would love to work with
Google to try to improve that looked at
work more questions okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>