<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Technology Behind Debian's Testing Release | Coder Coacher - Coaching Coders</title><meta content="The Technology Behind Debian's Testing Release - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Technology Behind Debian's Testing Release</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3aoDjW1bjxo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today we've got a new towns current
dimming particular creator the testing
distribution finding he was talked about
who
okay so that's obviously who I am I
guess all of you are familiar with what
debian is I'm sorry I am like to do
these talks by actually getting audience
participation to like asking questions
and getting you to stick up your hands
and as I understand it you're really
into unit tests and stuff here so if we
could just run a unit test to see if you
can stick up your hands now okay we've
got one failure over there but it's okay
I guess and as an optimization thing can
we just see if we can do that as well
because it's probably easier than doing
all this stuff okay optimization of this
leaves and burger here okay so Deb Deb
Ian's a volunteer linux distribution
that's in a sense all it is so we don't
have we don't have major overarching
goals like support the most
distributions like net bsd mightor be
that as secure as possible like openbsd
mi or get lots of market share or get
get microsoft off the standard install
like a bunt he does or any of those
things we just get a linux distribution
out and whatever anyone else wants to do
is is just whatever they bring to the
project that's fine they can add it in
but nobody else will necessarily work on
it so that means that things like
releasing and stuff depends on people
who actually want to do the work so
you've got to have your release managers
might be focusing on getting the bugs
fix and getting stuff done while other
people might be just happy plugging away
at the latest numeric release or
something like that and not really care
if it gets released officially and just
keep using unstable and so forth okay
does everyone know what debian stable
and unstable is yes stick your hand up
if you know hmm excellent ok so what who
does anyone did anyone here use debian
before 98 cool so way back then we only
had stable and unstable and to do a new
stable release would freeze unstable
nobody had upload to unstable that all
upload to froze
unstable so you wouldn't get any new
development till the freeze ended and
fries would then go on for years because
there were bugs and whatever else and we
kind of found that or at least the kind
of conceptual email was one of B towers
quite a while ago which took stable
stable uses as kind of wanting something
that kind of just sticks there on a work
and that they don't have to worry about
too much while unstable people wanted
all the new software and could put with
whatever crap bugs like in it not
working or bash not working or D package
not working or not having apt in the
first place back then I guess but they
were also kind of a whole lot of people
presumably like everyone who uses good
one too who wants new software but
doesn't want it to break all the time
here and so that's kind of the third
group that Debian wasn't addressing it
all at the time okay so cool so for for
for stable Debian can sort of say well
look it's meant to be stable so it
doesn't matter if we delay the release a
little bit more as long as we just keep
getting stuff right people might
complain a bit but I mean the overall
goal for stables obviously just make it
work make it upgrade cleanly make the
packages work have it be supportable and
if it's delayed well we're a volunteer
group people are used to Debian being
delayed whatever and for the unstable
side it's completely opposite you can
say as long as we get the packages in
doesn't matter if they're a bit buggy
like all the Debian on stable users
elite acts at hackers so they can fix
the problems themselves are expected to
fix the problems themselves and they
know that they're going to be problems
so this isn't a surprise for anyone so
you just want to get the stuff in as
soon as possible and once it's actually
in there you update it again when there
are bugs fixed so that kind of works too
but for a kind of middle ground do you
have a problem where if you update it
too often you start getting too many
bugs and that annoys people and if you
don't update it often enough you stuck
with really old software and people
start complaining about that even if
it's basically just a month old or
something
so where you can kind of let things
slide in one direction for stable
releases and let things slide in the
other direction for unstable releases a
bit you can't really do there's no
simple middle ground for that for
testing either okay now I've got some
numbers in here and I've talking to
Google i'm not sure that they're all
that impressive because they're in like
the thousands and hundreds of thousands
rather than millions and billions but
we'll go with them anyway so debian has
lots of packages and they basically just
keep increasing and so forth and we try
to support all of them essentially
equally because yep larger so how many
developers is climbing in our question
or a difficult question to answer
properly we've got about a thousand
official developers at the moment but
for the last what three four maybe five
years we've had sponsorship which means
that people who aren't developers are
still maintaining packages and doing a
lot of the work so it's not exactly
clear how you count those 1000 is one
guest saying that like a bunch of the
developers don't actually do any work at
the moment so just assume that the two
numbers balance out and a thousands
right I've seen 1,500 guests as well but
yeah a thousands been the nominal figure
for quite a while now so a thousand yeah
our last estimate for users with
something like 50 million if you count
all the distributor's all the
derivatives and stuff as well but yeah
no idea is really the best answer so
actually dealing with all these packages
I mean there's quite a bit of I don't
know there are quite a lot of nodes in
the graph and quite a lot of connections
between them and it gets kind of
complicated how's that show up ok so way
back when in about 2,000 when we started
implementing testing we were getting
500 Meg may be a gigabyte of changes a
day at the moment we're well back here
before the last three months we were
averaging about two gig as the release
gets closer we're kind of leaning
towards four gig of changes a day which
is also kind of unpleasant when you've
got to push that through to people and
and whatever else so can you see the
labels on that okay from the back so
each of the different colors or
different architectures that have
uploaded packages that day so at the
bottom you've got the blue graph which
is the source megabytes per day above
that you've got the architectural
packages so the ones that just get
installed on any architecture no binary
is there anything like that and
alphabetically each of the architectures
Debian supports in unstable above that
so on the days when there's a lot of
source uploads then although auto
builders will give you a lot of um
binary uploads that day too and yeah as
you have I think weekends you can tend
to see a little bit of spike on and as
the release get serious you can kind of
see a bit of a trend onto okay and the
other problem with this is that when you
actually do when you actually do updates
for testing people actually run it so
they'll go up get upgrade and not be the
elite hackers who use unstable but be
someone who's actually going to have a
problem if stuff breaks likewise you
have all the mirrors so that if you
update a bunch of stuff in testing and
thus lose all the old versions the
mirrors are going to delete that from
there from their disk which means that
if you then wanted to revert something
that's not going to work because you're
then going to have to pulse it all out
again so you've kind of got a lot of
constraints there because this is all so
users are just trying to use it they're
not volunteering their time to test it
so much they just want to have a working
system so breaking it for them is kind
of rude having
mirrors get screwed either by having
well mostly by having lots of bandwidth
synonym day is also kind of rude because
we already do four gigs a day which at
least okay maybe not in Silicon Valley
but in other places is quite a lot of
quite a lot of transfer we've got about
200 gigs on the archive and having it
increased too much is also bad which is
a problem if unstable and testing start
diverging so if you have the same
version of the package in unstable and
testing that's fine because you've only
got one copy on the disk but if you have
20,000 on packages or 200,000 packages
with different versions in four
architectures in for sweet sorrow so
woody Sarge etch and said and possibly
experimental as well then that's an
extra an extra copy as well that you
don't really have space for on many
mirrors and the other constraint is that
when you do an apt-get upgrade it'll
tend not to be willing to downgrade for
you so if you release a buddy package
and say oh crap i should have just
really better with the old version if
you just put the old version back on the
mirrors that won't actually get upgraded
it'll just sit there on the mirror new
installs will get the old version with
people who already upgraded will say now
I've got a newer version I'm not
upgrading to that so and just sit there
and be broken so you've got a fairly
complicated bunch of stuff you've got a
lot of people who complain if you get it
wrong and you don't really have much
chance of fixing it if you get it wrong
okay and the other thing i seed
downstairs was one of the original
Google racks or something so four cpus
or something per level and dozen of them
or so and yeah that's probably a few did
that today with whatever the current
current model gigahertz machines are
that would that rack would contain more
processing power than debian org heads
so we don't actually have that mini
machine so the the ftp master which runs
the testing scripts hosts the archive
also hosts the wiki also hosts the bug
database and is admin by about three
people so we don't really we can't
really say that ah well if something
goes wrong then someone will just wake
up and get a page and fix it and if if
it gets kind of the wrong answer well
it'll be someone awake the next morning
to review it and whatever else you have
to get it you have to get all those
constraints satisfied essentially
automatically every day because the game
if testing doesn't get updated on there
on the everyday people say well I wanted
that package it was going to get through
today why didn't it get through and
again you have the you have their not
being update enough to get the new bugs
figs and whatever else okay okay so if
you're going to automatically do release
management the question is what are you
going to be able to look out
automatically because you can't just do
it install get a feel for it oh yeah
that's pretty much as as smooth as I'd
hoped it to be I'm into some bugs but
yeah they're within the expected sort of
tolerance which you could do if you were
doing the release management management
with an actual person so the way that
the testing stuff works is it looks at
the criteria that it can automatically
test so one of the things that you can
automatically test is the consistency of
the distribution by looking at the
dependencies and conflicts and seeing if
the packages can actually be installed
together so it's no good having an
updated version of a library if one of
the applications that depend on depends
on it hasn't also been updated to the
new version of the library and likewise
if you update a library that suddenly or
if you update one application that
starts conflicting with another one
because it's changed its API then that
other application needs to be updated to
so that you can still use them all
together and have a consistent system
that doesn't actually cover everything
though because there are some sorts of
things that uses it
back so you expect to be able to install
all the KDE packages or all the genome
packages at once but sometimes some of
them will depend on one set of libraries
others will depend on other nor the set
of libraries that will conflict and why
you can install that one program fine
and this one program fine you can't
actually install them both together at
the same time and you'll see a few thick
well i'm not sure if you've seen if
you'll see that these days break but at
least in the past few years you will see
a break because it can't be checked
automatically easily okay so we have all
these leet hackers using unstable who
then go ahead and file bugs and file
patches and staff and we've got a bug
tracking system that will track all
those bugs and actually make them and
you can query that and see well which of
these new versions actually have
introduced new bugs so we can track that
automatically fairly easily too and we
can also check whether all the building
demons which create those pretty graphs
have actually successfully built the
source packages and if they haven't
there's probably a bug like no sorry a
byte order sort of sorry there's
probably a platform-specific bug that
will only show up on some platforms
which will often actually be wrong on
other ones but just not show a
compile-time error which basically gives
you another set of automated bug bug
testing and I mean for packages that
have test Suites that will also be
covered and all those logs are available
automatically and will come up as to
whether the packages up is up to date on
each architecture and unstable okay
so those are basically the three key
things that you can check automatically
I'm going to skip ahead for a bit that
one okay so what what the automated
testing stuff will actually will
actually produce is a list of excuses
why packages can't be considered so good
new cash yesterday is two days old so
it's been uploaded a couple of days ago
but was obviously high urgency so is
able to get in fairly quickly because it
fixes some released critical bugs it
didn't build successfully on m68k but we
don't caring about m68k so the testing
scripts are willing to ignore that and
we've also found that the new cache has
introduced new bugs since the last
version that God got accepted in testing
and that bugs still present so okay did
everyone understand that explanation
then yep can you tell me what you didn't
understand why do I excellent one person
understood it that's not so excellent
that one greater than zero and
automatically numbers I can't
decide more bugs than
the 0 is the number of bugs in the
version in testing or the number of
reallys critical bugs there are probably
some other bugs but not ones that were
bothered caring about that much the one
is the number of bugs currently in the
bug tracking system against unstable so
if it had three bugs but the version in
testing had four it would have said
three less than or equal to 4 and said
not worrying about the bugs because it's
better off this way we introduced
version tracking in the bug in the bug
tracking system last year which in
theory should allow us to do those
numbers a little bit more accurately at
the moment they're approximated somewhat
so for example that new bug that's been
found since the last version got into
testing could actually apply to the
version in testing as well these scripts
don't check that yet because they don't
have the appropriate support for it okay
so that gives you an idea of the first
round of checks that it can do before
it's checked actual dependencies
okay for actually checking dependencies
that gets to be a bit of a harder
problem so you have lots of packages but
each one of those has multiple
dependencies and some of them alternate
alternate dependencies and many and
there are also conflicts thrown in as
well and essentially we want every
package in testing to be installable at
once so you don't just have to test
those million odd equations but you have
to test it for each particular package
being able to be set to be install well
as well so you've got a million
equations in two thousand variables and
you want to find this you want to make
sure that there's a solution where every
single one of those variables is that is
set to true independently so you first
want to check the D packages installable
then you want to check that genome is
installable then you want to check that
fe w m is installable and check that you
can satisfy each of those dependencies
and conflicts all at once while having
one of these packages packages set to be
installed so when I was writing these
slides I tried to do the proof further
for the proper entropy completeness but
yeah got confused and it didn't fit on a
slide so whatever
so everyone here knows what NP
completeness is right who thinks it
stands for non polynomial excellent okay
so you have essentially an np-complete
problem that you need to check for every
package every single time you try and
add a package so if you've got a hundred
updates to see if you can put into
testing each day you need to check each
of those million dependencies for each
of those hundred thousand packages for
each one of those hundred 100 packages
you're going to update and technically
you ought to be doing it for every 2 to
the hundred combinations of those
packages that you could be putting in so
for the NP complete thing you need a
backtracking algorithm or else it's not
going to be correct you don't really
want to approximate because well you've
got a bunch of people hacking and
funnily enough they actually turn up
some of the edge cases for the sorts of
algorithms you come up with when I was
originally trying to write the algorithm
and see if I could get it correct I
would come up with odd cases where no
clearly this package is installable and
go back to get and I'd say no I won't
install it for you and I'd go no that
seems fine and that seems fine and that
seems fine I mean you could go down four
or five levels and come up with
something that's just not quite right
because someone edited the dependence
line by hand and got it slightly wrong
so you do actually have the depends on
this or this and this or this depends on
that and that depends on this which
conflicts with BS and this other thing
depends depends on that and because they
conflict you can't actually install this
thing here but you can install every
other thing that any of them depends on
okay so you can't approximate because
you just don't want to have people get
the result that no I can't install this
that's the entire point of this you want
to be able to sit at your computer and
go back get install whatever and have it
work and have it come up with a package
it's not terribly broken that is the
same across all architectures so that
you're running the same system on spark
or AMD 64 or I 386 or m68k or whatever
okay the normal case are obviously for
an np-complete problem you don't want to
just do exponential stuff all the time
so you want to find the normal case and
have that be fairly quick so the normal
case is that generally things aren't
broken so in hundred thousand packages
you've got 800 released critical bugs at
the moment vast majority work fine
generally while you do have options in
in dependencies generally either option
will work so you can just choose the
first one or the highest priority one
and generally conflicts don't actually
matter they usually refer to old
versions of programs that you're not
actually using at the moment and you
know okay so the algorithm we use are
basically tries iterating through
iterating is there any sort of thing I
can point with okay I guess a hand
what's that excuse me
excellent now frankly okay so this is
for Gaza which is some random package
that had a problem on September the 16th
or something and it couldn't esting
couldn't work out whether it was um
whether it was installable over about a
hundred million iterations or something
because there are too many alternatives
and it had to backtrack too much in
whatever else so these back here are the
first level dependencies of go sir so
those are the things it depends on
directly and the next lot of things so
these are the ones that something it
depends on depends on directly and
there's no alternative so there's no
point backtracking if they're not
install right so if smart smarty get
text doesn't exist you know you're not
going to be installed a below install
goes off so you just stop if somebody
get tex depends on lib c6 then if flip
c6 does is an installable you're not
going to be able to install go sir
either so you know you can just stop
there so we basically expanded tree
backwards in a list on here until we get
to something with an alternative so
whatever package it is that depends on
PHP 5 could also be satisfied by a
different package so php5 WWLP HP six or
PHP four or something else yep
together that would be nice but it's not
always the case so if you if you've
picked one package that requires you to
use Exim so you've got some XM log
analysis tools that are obviously going
to specifically depend on XM and you
have another package that will work with
any MTA and then you've got something
else that depends on both then while the
MTA package could be satisfied by
anything in this case it won't because
you'll need to have XM installed to the
other thing that depends on ideally you
would like to be able to do that and in
theory debian has a policy that any
optional package or rather all the
optional packages can all be installed
together it's been a while since we've
actually made that true though okay
so the PHP 5 thing will have some
alternatives and will probably depend on
some of these PHP fire packages here
maybe so you need to delay these those
are there those alternatives past all
the other stuff you can't just say well
if I'm depending on PHP 5 then I'm
obviously going to depend on all the
stuff php5 depends on because you don't
actually know you depend on PHP 5 oops
so the trick we use there is basically
to have a cut off as well as all the
actual dependencies in the list so that
if I've decided I'm going to backtrack
here then my cutoff is going to tell me
where all the dependencies I've had for
this package are so if I'm going to
switch from looking at php5 looking at
PHP for is my solution to the dependency
then I can cut off all the dependencies
after that cut off and add PHP falls in
there from there and I know at this
point that once I move on past php5 to
lipsy 6.1 and whatever else I've got to
look at I'll only need to add their
dependencies after the cut off so that
wasn't the best explanation but so the
idea is that you're putting the ones
that you know you need to consider first
in the list so you can skip over them as
quickly as possible and you're delaying
the ones with alternatives till the end
so that they're the ones that you'll
have to retry if you get into conflicts
so if you definitely fail early that's
good because you then know you failed
you definitely can't do anything if
lipsy 6 doesn't install and you don't
need to worry about the about the
multiple options which is where you get
into the exponential sort of cases and
sorry yeah
okay but when you do get to the options
you have a very easy way of backtracking
because you can just cut off all the
crap that you've added to it afterwards
and just step back through the list for
each thing okay this huge dependency
tree also included solution of the
cabbage
breaking off the packages in this hole
into small pieces is what tends to get
the huge dependency grace
because then you have one large package
like behind that depends on all the
things that take up all the different
sorts attackers make up below if you
notice about the direct appeals tree
good known as a pseudo package that then
depends on all the specific packages
that the known uses some of them in
alternatives because you won't be able
to use firefox or epiphany or evolution
or thunderbird and it's those
alternatives that then start getting
your complicated cases and each of those
packages have to be in store for the
good and sedo package to be installable
which then gives you all their libraries
that you have to worry about which tend
to include sound packages which have
alternatives for also OST palestine OSS
and often once those or fail or maybe
both of them will fail and then you've
got gstreamer and all sorts of other
things and yes genome regularly gets
into the into the exponential cases any
other questions on that oh Christ okay
so first version of the algorithm to do
that was written entirely in Perl
because pearls kind of easy to write
with and I didn't know python at the
time but it was horribly slow like it
would take 10 minutes just to see if one
package wasn't available as opposed to
all 100,000 and then do it again and
again and again so rewritten in sieve
relatively quickly quickly and obviously
then I wanted to be able to call the sea
from the pearl that I'd written that's
pearl excess can anyone understand it
has anyone written / like this before is
it horrible that's the Python version
sorry whose yes there is I think this
was before sweet was really popular I
was writing this and then I didn't want
to touch it again and then I found
Python have an actual same C syntax oh
so yeah so there's my a Python thing for
all the Googlers whatever
okay so one of the things that we
actually come up with a still a problem
this transition so if you update
postgres from version 7 to 8 or 8 to 8.1
or 8.1 great point to you change the
library and you also want to update all
all the programs that have books for
postgresql or basically at the same time
so that they all keep working together
and that then becomes a problem if one
of those random programs that you want
to update at the same time is broken for
completely unrelated reasons or also
needs to be updated because of a cutie
transition and QT is up is broken
because of something else so as well as
all that stuff we also need to be able
to say to the testing scripts that look
these combination of packages are the
ones that are most likely to work
together and that we need to find more
information about and so the release
managers and really steamed and release
assistants have a way of hinting
Brittany as to what the most important
sort of things look at ah ok so for
example the hint command there says that
the new steps in a transition at the
moment and you'll need to update all
three of these packages at once and
probably a whole bunch of others for the
install ability to be satisfied because
if you just update if you just update
the new step base that will probably
break the new step GUI and a whole bunch
of things that it depends on if you
don't are going to step back to whoops I
didn't format that very well and
likewise if you update the new step back
without updating the new step base it's
not going to work because its
dependencies aren't satisfied so it
won't it will be unavailable to okay the
other ones aren't really that exciting
the force hints are just to say that
okay so it's got bugs we still want it
to go in straight away for sent is just
to say that okay we don't care if it
breaks other stuff either this is too
important and we wanted to go in and
we've already checked and the breakage
isn't too severe so
so it's through the hints that the human
kind of part of release management comes
in so the hints are basically updated in
a text file whenever the release
managers feel like it while Brittany
runs straight after straight up to the
mirror pulse every day and if it doesn't
get hints it'll just do the easy stuff
basically um the other ones are urgent
which lets it go in sooner unblock which
gets rid of the freeze requirements for
a package and that's about it yes about
coming are there is easy humans to
figure out raids by it sorry how many's
how many hits a barge
the system running on a day to day basis
and the other question on a on a
day-to-day basis kids are required it's
just when there are transitions that
come in there's probably a couple of
transitions a week but it tends to take
a while for the brawler related packages
to be in an appropriate state to
actually be able to go through youngest
and the other question is is needed for
a change
it's hard to figure out what to do or is
it immediately I'll tell ya walk-ins is
this pack
okay so skipping on kind of one of the
things that the excuses page also has
which I didn't show there was it also
tells you when one package can only be
updated when another package is updated
as well it's got a version dependency on
the new one that requires the new one
before it'll be satisfied with the old
one and this separate site basically
counts the number of packages waiting on
some other package and will give you the
top top 10 list sort of thing so you can
see that a bunch of package is a wedding
on the new step base can you step gooing
going to step back at the moment and
since they're all going to step ones
they're probably all related when you
give it when you give the testing
scripts a hint it will try it try all
the other packages that can also be
included at the same time after those
dependencies are satisfied and then give
you a list of which ones failed so when
you see the ones that failed you then
need to look back and see all their foul
because no one's even bothered updating
them or they've been updated but only a
couple of days ago so they haven't
actually they've still been excused and
not considered or they've got released
critical bugs or they're linked in with
some other transition or whatever other
reason so there's a kind of
straightforward process for it I
wouldn't call it easy per se
the traffic problems have an arpeggiator
accident
we've both been looking from airport it
is a bootstrapping sort of thing yet
here in theory it wouldn't be needed if
Britney could be bothered actually
trying all the 2 202 to 100 combinations
of the hundred packages that are going
to be considered but that just takes too
long it's different for new
architectures because when you've just
got a new architecture you need to get
all the essential packages in and and so
forth too so it's much easier just to
say we're going to completely ignore the
dependencies for the time being and just
try and get the packages in sync and let
users actually figure out at what point
it's kind of consistent enough to start
really enforcing things and that's what
we did with NB 64 earlier this year and
the other new architectures way back
when ok so when when the testing scripts
try try to add the C C++ reference
source package and all its binary is to
testing it'll give you this sort of
output which is to say that so first of
all we know that it accepted the package
so this was one of the ones that was
good enough we originally had 73
packages on installable in testing with
an extra 395 in the RN architectures
we're not considering which were happens
to be m68k right at the end so these are
a 386 which is the only one we actually
care about the install ability of
architecture all packages on and the
rest are in alpha order so alpha amd64
arm HTTP a ia64 mips mipsel etc so as
the scripts run you basically get a
running total of how it affects the
uninstall ability if see if this one had
failed we would have seen maybe it
failed on alpha before it foul on
anything else would see that go up to
six or seven or 100 and then would get a
list of all the packages that it may
done installable underneath which the
release managers would then need to see
oh okay well this is
obvious problem Arlene need to go and
talk to these people to get these people
to get it fixed okay that's fondly known
as update output text doesn't have a
better name okay and who finds those two
pages easy to understand so yeah shortly
after well a little while after testing
was actually running in people using it
I got pestered for explanations which
got put up on the website and once their
explanations up on the website of what
all the stuff meant so you didn't have
to actually look through the code to
understand it we started getting
different sites like this one's a
particularly good one born beyond hack c
/ debian which analyze and make the
information more palatable so packages
QA that debian that all with your little
bit of information about whether
packages are making into testing and as
will the package tracking system and so
forth and yeah that's my that's my
little throwaway for web two point O
which is the grabbing information from
other people on the web and rearranging
yourself from putting it back up and all
that sort of stuff okay so proposed
changes for the future for the scripts
themselves library transitions are
currently a problem because we don't
keep the old versions of libraries
around generally so if you've got a
package depends on lib 20 when lib food
one comes along that'll be broken
because we don't want to distribute live
through 0 anymore and we don't want to
go to the effort of maintaining it but
that makes the transitions awkward
because all the lib food packages have
to be updated at once so fairly shortly
now I believe as part of the Summer of
Code what the Summer of Code student
that worked on this will be um will have
it so that the testing scripts
automatically keep around the old
library version even though the source
no longer builds it which should make a
bunch of
transitions easier likewise the better
dependency analysis basically being able
to come up with hints more automatically
or rather as we know what the process is
for a hint want to write it down in code
once it's written down in code you don't
need manual intervention and you can
just do it every day and the other thing
we're probably going to work on is
making it so that you don't have to just
let the script granted a look at the
result run once a day look at the
results have got change the hints and
then let it run the next day but rather
be able to interact with it directly to
say I think this combination of packages
kind of makes sense to try what happens
when you do try it oh okay what happens
if you add this packaging as well okay
then this is the hint that i want to add
for the next actual run okay
alright so that's the testing scripts
does anyone want to ask any more
questions about that before I go into a
slightly broader topic that you get fish
okay so stepping back from the scripts
is the kind of question of what what we
might want to actually do with releases
more generally so one sort of thing that
we might want to do is extend the Debian
installer BD releases to having the
entire testing suite be a beta release
every noun that and I mean if we do that
and we do security support for it then
that's kind of a short-term support
release in the opposite sense of ubuntu
as long-term support releases that we
might be able to do six monthly or three
monthly or whatever just by snapshotting
testing and putting it up on our site
where it stays for three or six months
so another thing that people often want
to do is partial upgrades of particular
bits of software like you want a new
version of X or a new version of the
gnome but you really just want
everything else to say the same because
you don't care about that much you just
want it to work gen 2 has some
interesting ways of dealing with that by
allowing you to snapshot particular
areas of code that people maintain at a
particular version like you can point
your gen to distribution at the latest
version of everything except an old
version of of genome because the odd
version was what you wanted and they
removed all the options you liked in a
new version because that no that would
never happen sorry bad example and in
theory you ought to be able to do a
fairly similar thing in debian by just
saying use unstable for all of it except
used its best bits from stable or you
can do it the opposite way as you stable
for everything but grab some of these
things from backwards and who you use or
knows of how Debian amd64 worked for
Sarge for the last stable release
yes no so we didn't actually release
amd64 is an official part of the last
release because we had problems getting
space on the mirrors and basically just
froze the addition of architectures for
way too long what happened instead was
that a bunch of people who wanted to see
Debian on amd64 because they have amd64
machines and like debian is that they
set up a separate arc archive called
Debian amd64.deb internet or amd64.deb
Indian that got net or whatever and just
pull the sources from debian updated a
couple that they needed to and built
them all for amd64 and that was
essentially the sarge release of AMD 64
for debian that then got security
support added as well via the official
via the official security arm team which
meant that they do a security update for
everything so md 64 and all the regular
architectures and all the regular
architectures would then go to the
regular archive site well amd64 had to
go somewhere else and the question is
and we don't need to do that for amd64
anymore because it's actually official
now but we're instead dropping m68k and
maybe we'll be introducing some other
architectures or maybe we actually want
to do something with heard one of these
days and so another question is whether
we want to have some sort of not
officially stable but something similar
sort of support for other architectures
like m68k ok
you've got any questions or want to
raise other topics or anything oh my
gosh we're out of time so we can talk
about our swizzle
this was a 64 there was a discussion
about anything with yard support so you
can have certain 2064 feet by oles on
the same story this is an official plan
for this so there's two sorts of multi
arch there's the really easy sort which
is just get the ia32 libs package and
then you can basically just how you have
the libraries on your system you can
install the i386 programs in opt and run
them and that works for VMware and stuff
like that with the real multi arch thing
that people have wanted for ages and
particular the ia64 people back in the
day when I 64 was a going concern wanted
to basically be able to just get
openoffice.org and all sorts of other
packages just straight from the i386
builds and install them natively the
problem with that is that they then also
need the i386 libraries and all the
other stuff which might also exists for
the amd64 stuffs they've got the i386
can own libraries in the amd64 or a 64
genome libraries and you want them all
install at the same time there's been
kind of plans for that for ages but
they're all hideously complicated the
latest i've heard is that we're going to
have been in theory working on it but
that's with the same people who've been
working on it for ages so i'm not sure
it's actually getting getting much
further or not but that's being like a
contract with a bunt do so in theory it
should come up with some result i don't
know how useful it'll be i personally
tend to think the easiest way is just to
do the chroot sort of option and have a
full I 386 install in at route and just
go straight into that
ok there's no more questions
Hector</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>