<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Divide-and-Conquer ... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Divide-and-Conquer ... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Divide-and-Conquer ...</b></h2><h5 class="post__date">2012-02-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_bE-V2xzCgg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is a divided divided character
matrix factorization that dreck work the
lesser jordan so to motivate our work
i'm going to be talking about the
problem matrix completion which ensure
you guys aren't really the idea here is
you're giving a subset of the entries of
a matrix anyone recover missing entries
this problem well studied especially
recently important to get that Blake's
plan is an excitement around that it
often times too soggy sorts problems due
to low rank assumptions they did there
is that super super small number of
factors or where needed to determine
user's preferences so for instance
smaller factors determine one who these
people so we're interested in is trying
to solve these problems efficiently for
very long extensions so when subset of
the items that you open setting and
using recently or those based on choice
norm and these algorithms soft convex
optimization problem we minimize the
trace norm as sort of the scale so let
us start these algorithms have it has
the reticle properties you can provably
show that you have to learn matrix with
high probability they seem to work well
the practice as well for small problems
but they don't scale very well I
required us a truncated SPD degeneration
and you really just can't use them for
very much problems so the goal is to
come up with the way to speed up these
algorithms keep the nice empirical
performance and also retain the
theoretical analysis so we came up with
a net algorithm called the
vector combined it's a parallel divide
conquer friendly and it involves the use
of sub-base matrix completion algorithm
and involves three steps it's very
simple this involves taking the input
dividing it into a bunch of sub matrices
then you saw the matrix condition
problem I need to the sub matrices in
parallel using any basement batrix
completion algorithm and finally you use
matrix approximation techniques to
combine the estimate to be able to each
of the factoring factor in some
interesting and the stretch below shows
two different ways we go about dividing
and combining which I'll go over in more
detail the next few slides but I just
want to know here that in this talk in
this work we focus mostly on algorithms
based on Trace norm that being said this
algorithm works or should work for any
sort of any base algorithm that
performance matrix completion and in
fact we have tried others other than
restart and these empirically we get
very similar results okay so the first
algorithm what you do here is to take
the full matrix and your sample subset
of the rows of the subset becomes and
the factor set in parallel you run the
matrix completion algorithm together to
complete the row single drinks in the
comments
matrix I'm finally the combined step we
use the generalized nitromethane which
is a standard technique to ascend a
standard a sample based technique to
approximate over exactly recover alone n
matrix using the same thing mixed
methods and when matrix review it was
exactly the rank it is often okay to
just sample one subset of the rails and
one subset of the columns however you
deal with a with a with a nature except
that is for rank or noisy by rank matrix
with Rosie by rank matrix it's often
very helpful to the employer and in
several variant of the Sun so we give
you or simply is pretty much perform the
same amount of multiple times you can
partition the columns and rows matrix
get many of our exhibitions and
interesting the average in a practice we
see what this reveals the alternative
method is based on the projection method
we do here is we start the matrix and we
partition the columns into T different
partitions and then the fact upset you
simply again in parallel drum majors
completion of each of these
self-interest sees to get completed sub
matrices and then we use the column
projection method to combine all these
matrices and when you see the formula
there but essentially what's going on is
we are projecting the completed sub
matrices onto the subspace the Kuppa
subspace of the column subspace of one
of those matrices in this case were
projecting onto the column space of the
first sentence and again in practice and
often is very helpful to instead of
doing this one stevenson and solve the
way we're where we project them to each
of the sub matrices in an average and i
should also note that both in the
previous method based on extra method
also this projection method the rake the
time consuming step is still the factory
stuff and even here in the Makah
projection the the comm projector
requires you to multiply let's say this
matrix is n by M very large it requires
an answer a ton of these to the
multiplication however since each of our
each of our sub matrices are factored a
return in factored form by the factor
step end of the rank it's actually much
much faster than without this factor
presentation okay so personally with
that some submissions to see how this
works and you see here on the Left
bigger is the performance as a function
of number of the pension sink so as an
expect of interest completion problems
harder as you see less than I centuries
or alternities easier as you see more
more injuries so you see all these
different curve corresponds different
variants about your binding factor DFC
algorithm comparisons based on going
home using all the algorithms do better
is for simple no more columns but what
they must note here is that extra sample
two or three percent of the columns here
the best of our TFC algorithms which is
the projection and samba method pretty
much performs as as well as the masonic
so Jen's performance you know there is a
trade-off when when you're simple very
very few numbers of entries you can be
better if the base I'll giving them with
this divide and conquer method but just
sampling it here but you know there's
only a very good
window which you see that issue and in
terms of speed up pc really nice so the
black man on the right so this is this
is tyla negra y axis and a function of
matrix size again into square matrices
and we see right life is the base are we
producing that's it's a base see a son
and all of the other variants are up at
the same time it gives again computation
stuff we're doing things in parallel
factory factorization stuff is what's
really expensive when we see roughly so
when you have a 10,000 metals matrix
instead of taking an hour you take less
than 10 minutes okay so then we also
have some results for collaborative
filtering there's a fair bit to say in
this table but just to keep things short
the base algorithm we're working with is
a peachy and so to square up there we
get an our mercy of 8.8 433 at a time
2600 seconds again using settling ten
percent of the columns of our matrix i
use the projection method and using an
insolvent the asamblea variant of it
greatly at the same time i see and a
roughly 10 foot speed up in time which
okay and then as i mentioned we had two
girls where we wanted to get the same he
wanted to achieve roughly the same
performance as the base algorithms but
we also wanted to preserve the fury of
these miss items and I don't have time
to go into
it's too much of it to see you an idea
of what we have to do to make this work
there are too many steps the first
involved showing that performing reaches
completion to be subproblems and succeed
with high probability and that involve a
city permit that the sub samples of
Vienna have the same property baba said
assumptions that were made original
problem and then second you have to get
to study the matrix Prussia the matrix
approximation techniques to shove a
basic city of the high probability under
the same assumptions use biometrics
completion algorithms and what the
results yet where we results for major
innovations in the presence of notice
but just assuming the nurse for a second
using insurance to regularize algorithms
assuming that you're in separate matrix
we show exactly covered with high
probability of course you know nothing's
free we do have a lower success
probability as those probability of
failure each of those two steps that I
mentioned above and also you know you
want idea that you'd like to be able
sample small number of every sub-samples
to be as small as possible and we show
that the sub sample size that is
allowable depend on aging size rank and
the number of entries have you seen in
the worst case can be linear in the best
case it could be a logarithmic in the
size and Jesus summer contain in between
practice I doubt it time to show this
video so I'm skip it but just in summary
so we be developed a student framework
for a matrix factorization when working
with crystal regularized algorithms
where of the yep recover at the same
performance rectly and similarly reticle
guarantees and also the work we actually
dealing as I said earlier we deal with
noisy settings a matrix completion and
we
another interesting problem I call
robust factorization which other people
talk co robisz pca where you're trying
to propose a matrix into a sparse so
yeah we are posters right there</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>