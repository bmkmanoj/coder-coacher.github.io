<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Web-based Inference Detection | Coder Coacher - Coaching Coders</title><meta content="Web-based Inference Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Web-based Inference Detection</b></h2><h5 class="post__date">2009-06-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-MFs3byASqE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh this is joint work with Richard Chow
and go away so just a brief outline
first I want to explain what we need my
infant detection because of course
inference is ambiguous term I'll talk
about our technique and then describe
how it can be used to detect bias and
then if there's time I'll talk a little
bit about how you might use in first
detection to do better access control
okay so I want to start with just a
couple of examples so this is a document
you can find on the web today it's an
FBI document that's been Declassified
and in the process of declassifying it
if certain information has been redacted
so if you look at it quickly you can't
tell immediately who it's about right
it's the reductive family but there's a
lot of other information in there we can
extract some keywords on Saudi magnet
half-brother and if we enter these into
pool at least at the time we took this
screenshot the first pages i'll have in
life so so you might tell you that it's
likely that someone who sees that
document is going to infer that it's
actually about the Vinland family even
though those words don't appear anywhere
one more example that we refer to later
this is a speech pathology to read the
cell it's a scan from Valerie Plame's
bulk so Valerie Plame was the covert CIA
agent whose identity was leaked sometime
during the Bush administration so then
she wrote a book and the CIA redacted
chunks in it so this is a chapter where
she's talking about her first tour of
duty with the CIA and you can't wear
that tour dude he was because they
forgot it and they say redacted tour but
again there's a lot of stuff that she
does say and it hasn't been redacted
shocks thought of being in Europe she
complains about the heat the traffic in
all sorts of things that seemingly are
pretty non-identifying but again
listening to the screenshot Athens
Greece to top two hits and that is where
her first tour of duty was so again it
looks like an inference could be made
based on the content
remaining there there's lots more
examples like this the the US government
had a short-lived website that they they
created for the purpose of demonstrating
the presence of nuclear weapons in Iraq
but it was quickly removed when it was
realized that it could be used to infer
how to make such weapons and there are
all sorts of bloggers who have tried to
maintain anonymity and their blogs but
they've revealed enough to be identified
one kind of well known example is this
blog that was called The Washingtonian
and the the blogger revealed that she
was working for a Midwestern senators
your builder all modern through other
things it was very easy to UM to
identify her based on that so so
slightly more formally the problem that
we're looking at is that when you share
a document with someone they don't just
have the document right they have some
other reference knowledge information
that they know they have the web they
have been all sorts of resources and
that can lead to unintended inferences
so so the challenge here are one of the
challenges here is to figure out how to
model that reference colleges as well as
you can so you can anticipate these
inferences and so maybe you'll protect
the content better okay um so so to put
this in sort of the more general dlp
Senate setting data loss prevention
setting what we do is we take as input a
privacy policy which is probably pretty
course information it's probably just
done of sensitive topics you want to
protect like like bin Laden in that
first example or grease and that other
example so you've got some course
information about sensitive topics we
also required knowledge corpus which was
web the web in this past examples that
it could be an internal corbis we'll
talk about some examples later where
that's more appropriate and what the
algorithm does is it uses association
mining to output keywords that are
closely associated with those sensitive
topics and that could then allow you to
program your content modera that
content on the network and trying to
flag sensitive that and topic to a
better job so I just want to briefly
mention some related work i can i
mentioned association mining well
there's a lot of work has been done an
association reminding particularly in
the structured data settings of
databases they're looking for rules of
the form a implies B where a and B are
usually goods in a grocery store for
example or in a products that type of
thing and they focus on high support
rules so meaning rules that are
satisfied by many records in the
database and for privacy that that may
not make a whole lot of sense right
privacy is kind of a needle in the
haystack type of problem so even if a
rule doesn't have support still maybe a
privacy problem another thing other
point to make is if you are using a
large corpus like the web it's really
kind of hard to baseline you know what
means high support the the other thing I
want to point out is that the way we use
the web here is very much inspired by
what the NLP community is been doing to
use web to model human knowledge and
disambiguate text okay so so what's our
approach is actually very simple so
first we need to get a candidate pool of
keywords that may or may not be Layton's
access it topically may or may not be
related to pin lon so it's a one way to
get that initial pool is just you know
enter the sensitive topic into Google
take the top X hits and scrape those for
for candidate keywords then again you
know this is going to give you a lot of
false positives so the work is figuring
out which of those keywords really do
have something to do with your sensitive
topic so for for that step what were
what we're really interested in is the
likelihood that someone who sees those
keywords is going to think of the
sensitive topic right on so you can you
can approximate that on the web by
looking at the number of web pages that
have those keywords and also have a
sensitive topic so that's that's really
that conditional probability up there
and that we can estimate with to search
engine queries so one for the keywords
that you're interested in and just note
the number of pages that you get in
return the number of hits don't actually
look at the content of them just note
the number pages and then a second query
for those those keywords with the
sensitive topic and again note the
number of pages and then that ratio the
ratio of those two hit counts is what
we're using to gauge the strength of the
difference okay so in that valerie plame
example we got over 59,000 pages for
those sequence of terms and for that
same sequences of terms with breezy
gusts over 42,000 so we get a confidence
of point seven two okay so very simple
so of course you can't stop there right
how do you know if these are any good
heading up your confidence measures is
any good so so it does some experiments
in a couple of different domains trying
to get at how accurate these results are
so I wanted to talk about those quickly
so when we start this work but we not i
did a number of interviews talking to
people who deal with their data in
different domains and this was a couple
years ago down actually Dirk was on some
of these interviews so maybe it was
three it was a while ago so and in the
healthcare space folks were really
struggling with HIPAA compliance HIPAA
and other privacy legislation and what
they had interpreted that legislation to
mean is that they're basically three
categories of information that need to
be protected one is HIV or AIDS and I
think about the patient's HIV status for
example drug or alcohol use and mental
health conditions so so the folks that
we talked to knew that they need to go
beyond looking for the obvious keywords
like looking for HIV are looking for
alcoholism and medical record so they
were actually maintaining long list of
medications commonly prescribed those
diseases psychiatrist names that
pertains to mental health symptoms even
and then they would you know redact
things in the medical records when they
had to produce a medical worker they
would first redacted against this list
so of course very time-consuming and
potentially error-prone so I wanted to
understand how our technique might help
in this setting so so here's here's a
smaller using a sensitive topic HIV so
just disappoint you know one of the
things that they have to redact for so
this this is the the output is no 73 or
sometimes the top seven three or so
terms that we've got again we followed
the algorithm I just described we can
first issued a web query for HIV got
around 2,300 terms and then rank them by
confidence using that process has been
made to queries in taking the ratio so a
lot of these look reasonable at least at
least the first tool is bezel right to
two strains of HIV or at the top
confidence one if you go down a little
bit you've got some drug names
interviewing and food or tide so you
want to get a sense of how good these
results really are so we actually had a
medical doctor who has a lot of HIV
patients they should make positive
patients look at these these results he
gave us a precision of a little over
seventy five percent then one
interesting thing is that well we think
this may be an underestimate because a
lot of things that he thought weren't
correct inferences I think if we had a
chance to explain to me I think it would
have made sense to him but there were
things he wouldn't see in a medical
record things like websites about AIDS
or the UN aves effort they were just
things that were really kind of out of
context for him so so this also sorta
indicates how hard it is to measure
precision I mean inference is sort of in
the eye of the beholder okay I'm what
about recall so for the same reason
recalls pretty hard to ask me right
there there isn't a comprehensive list
of HIV inferences to to work off of the
tickets like the ground truth so so one
thing that we've looked at to try and
get some sense of recall is stability so
meaning you know we've got this initial
full with keywords if there's an
inference that not it's not in that pool
of keywords we're just obviously not
going to find it right so so how
sensitive is that pool of keywords to
you know the cutoff that we give the
number of documents that we take as a
result of our google courier or whatever
so the interesting thing here is we can
see in this curve is it for various
confidence cut-offs there does seem to
be a diminished return after around ten
source documents I were just not finding
a lot more inferences so so at least
within the constraints of this approach
it looks like for a fairly small number
of source documents or a fairly small
training set we're kind of getting the
most of the algorithm okay one of their
experiment I want to talk about this one
is interesting because it demonstrates
when you might want to not rely
exclusively on the web so so a lot of
folks are concerned about intellectual
property leaking out of the organization
right so so again when we talk to people
who are dealing with this process they
were pretty much doing it all manually
again trying to list all the companies
they were negotiating with or new
products under development oh and all
that type of thing and again you know
manually constructing such a list and
using that to flag state emails leading
the organization so so again we wanted
to try and simulate setting and
understand how our technique could
potentially help so so our simulation
uses the the Enron email data set which
is around 500,000 emails sent from Enron
during a four-year period and they
sensitive topic that we cooked up
against is very artificial but once
that's a topic that we looked at is
warming so so imagine you're in Iran and
for some reason you really don't want to
reveal that you have this relationship
with the Wharton School of Business for
some reason so so what do you what do
you use to to flag emails going out how
do you know how do you detect if an
email is about Wharton or not well you
can certainly look for the keyword
Wharton that will I will catch some
things maybe you know the importance at
the University of Pennsylvania so you
could also even look for that but pretty
quickly right you run out of ideas for
what to look for so so we did basically
the same you know the same thing as
before this time we trained on half of
the Enron corpus so we use that half of
the corpus to get our initial pool of
keywords and then for evaluating the
confidence we use the other half of the
corpus and the web as well so here's
some results for the top 40 results in
confidence order of course this is not
very helpful so let's let's sort them
this is you manually sorted so you get a
lot of things that look reasonable
professors contact info for people at
Wharton other business schools okay
that's a little bit of a weak
association and there are a bunch of
things that we think are probably just
errors false positives we can't explain
them but just to hopefully make this use
case more clear here's an example of an
email that you would be able to flag
with these keywords but you might not
otherwise because it doesn't actually
mention Wharton anywhere so it's from
this person Francis do golden and run it
does talk about Philadelphia there's a
upenn address and using the android
corpus these things all have very high
association for and it does turn out
that this person is a professor at
borden so okay so let's talk about
precision and recall here so there there
are a few different sets that are
important for think about precision and
recall here so certainly the set of
emails that
that actually mention in wharton at
least in this data set is a pretty
strong indicator being about the Wharton
Business School in there there are other
wardens in the world but for this data
set Wharton generally means the word
business school but of course there are
other emails as we just saw there are
other emails that are about warden but
don't have that keyword and then finally
there the emails that we would find
using our are set of keywords so so we
get some things right and then there's
this yellow region which I'll label a
that's did some errors okay so to what
in terms of these sets what is precision
it was recalled so so precision is the
number of emails that we find that we
should have found right so we find a B
and C that's the region that we find and
we should have found B&amp;amp;C and similarly
recall is of all the emails out there
that that we should have found you know
what fraction did we actually find so
that's a plus C over B don't know a plus
B equals C but we don't know B plus C we
don't know see so so the best we could
do with precision is actually an under
estimate you can't calculate because be
erased but we can calculate be over a
plus B plus C for recall also we weren't
you know we couldn't have aa they make
this directly so so one sort of
approximation that we did is we
restricted it to the set B so of all
those emails and that's actually already
800 emails that contain the keyword wart
how many of those we've found without
relying on the keyword work so that was
that's what we used to get some sense of
recall so here here's some results the
the star in the upper right corner just
indicating and where you want to be a
great perfect precision perfect recall
the red square but in the the left is
sort of a baseline so that's if if the
only key words you can think of or
Wharton in universe
Pennsylvania how do you do will you do
pretty well with precision like 75
percent but only around 60 percent
recall so the nice thing about this is
it at least we can move closer to that
star particularly using a mix of the web
and email to to evaluate confidence get
closer okay so so now what so so again
the the natural for a natural question
is you know you figured out how to
detect this sensitive content what do
you do next how do you protect it and
I'll I think I'll get back to that at
the end of the talk assuming there's
time another question serve a confession
here cm the valerie plame example was a
little bit of a cheat it does have a
high confidence association those
keywords to have a high confidence
association with grease but they also
have a high confidence association with
france and one thing that we're not
doing now is really modeling how a human
would would deal with these exam acuity
is right i mean a human might say okay
well it looks like it's either france or
grease from this information but later
on she complains about how difficult the
languages but that suggests it's grease
or I don't know Poland or something and
then the human might say well the most
likely answer for this is grease and I
thought they were not doing right now
okay okay so let's um let's shift gears
down and I was going to talk a little
bit about how you might use this to
detect bias okay okay I'm so again
starting with an example so this is a
security book actually very good
security book by Ross Anderson you can
buy on amazon today this is a screenshot
it's got 27 customer reviews you can see
up there at the at the top it's very
highly rated right now almost four and a
half stars okay so so the question is um
well let me show you so reviews first
this is actually the top 10 reviews or
at least the headline to the top 10
reduce so the question we we
we're curious about is beyond reading
the content of these reviews is there
anything quick that you can do to get
some sense of how to calibrate these
reviews so so if you just from this
information so no content here I'm just
the name of the reviewer and then name
of the author the title and such what
can you say well so do many things look
interesting okay well so it looks like
maybe maybe Ross reviews own book that's
pretty common right a lot of authors
review their own books I think fair game
right but probably you didn't write a
negative review right so maybe okay so
may we set that one aside but what about
the other reviewers well this person has
work that's described in this book and
they've served on program committees
with the author this person has
interviewed the author again this
person's work is talked about in this
book and the profs Anderson has lectured
about this person's work and this person
actually helped edit the book so so it's
not to say that these reviews aren't
truthful and aren't accurate but maybe
they're just a more predictable these
external relationships maybe maybe you'd
want to calibrate these reviews
differently so so we're wondering can
you can you quickly detect this type of
thing yeah could you easily find these
relationships and sort of give you a way
to sort through these various reviews so
the idea is just to use a technique
goodness talking about if if a reviewer
and author have a strong relationship an
external relationship that may be
represented on the web right certainly
if it's a professional relationship if
they've written books together they
serve on program committees together
that's that's generally on the web we
might be able to detect it so so the
algorithm is really just the same thing
as before now we've got a reviewer on an
author name and we're going to issue
three queries three search engine
queries on one for the reviewers day 14
there's not even one for the two
together and again just note the hit
counts don't look at the content just
the hit counts and we'll take two ratios
one to measure the strength of the
Association reviewer implies author and
the other to measure the strength of
association and author applies reviewer
and then the maximum of those two will
see the the confidence that we associate
with that pair okay serve a measure of
the an estimate of the strength or
presence even of the early okay so going
back to those those reviewers in that
first example so the numbers look pretty
low right point 02 so the ones with
foxes are the ones who have had a
relationship with the author I would the
exception of frost right he's a strong
association himself but then everybody
else is pretty low but still you're
viewing the data and a little different
way of course it's a very small down set
but it looks like maybe maybe there's
some difference going on so we wanted to
I should say this is this is very much
ongoing work but you wanted to get some
initial sense of how accurate this could
be in practice so we looked at the top
300 photography books from amazon and we
chose that area because we know a little
bit about the community and it does seem
to have a very substantial web presence
of every conferences on delivery favors
yeah just like the lecturer mr. grapes
to NHSN one book they come to their
books all the proceedings are oh that's
right so then we scraped authors and
reviewers from books that had it most 20
reviews and that the thinking there with
it if we make a look has more than 20
reviews that it's pretty unlikely that
those reviews are dominated by friends
with author so probably our technique is
not so useful okay so so then that got
us down to 264 books
here's some statistics on that small
said about one and a half authors for
book about three and a half reviewers
for book and all told we need to make a
little over six hundred queries so we
needed a ground truth to write so we
reviewed all those pairs to figure out
which ones really did have an external
relationship relationship we ended up
with 20 and I just wanted to talk a
little bit about the range of
relationships here some like people who
had written a book together and that's
pretty strong evidence if they know each
other similarly if someone acknowledges
someone fairly reasonable evidence
adviser advisee as well but then down at
the bottom there's some that I think are
pretty weak evidence of a relationship
like people who have spoken at the same
conference you know it's a big
conference that's no indication of
whether or not they know each other but
that said these are the 20 that we
decided where we're true relationships
these are the 20 that we wanted to be
able to find right so our precision and
recall is measured against this so
here's here's how it turned out if you
um you take the confidence really low
point zero zero one you can get up to
around 90 percent recall again on this
small data set we couldn't ever get all
the way to one because we were requiring
a minimum web support of 10 because we
went below that basically the hips were
dominated by the amazon.com is
interested persons review so really
weren't evidence of anything so so that
yet so there's a limitation right there
has to do this this web presence and
then at the other end of the spectrum
going up to about ninety percent
precision with around thirty-five
percent recall so we had a very small
data set but it looks like that it could
work in at least some communities and I
think I think that's actually an
important point if not it's not only
that the relationships have to have a
presents it's also important that the
relationship is a substantial portion of
one of the parties web presence I'm so
for example sarah palin right and
clearly had a web presence before she
ran once she became John McCain's
running mate at her web presence grew
enormously right so there's a very
strong association there whereas another
example is Madonna and Guy Ritchie
married for many years you would think
okay there's a relationship but it's
hard to pick up on the web because she's
this huge web presence independent of
Guy Ritchie okay so so yeah so we're
definitely not going to pick up
everything this way the other point
which I think I mean before is it of
course these reviews you know they may
be very accurate right I think more what
what we're saying is that they're more
predictable right and there does seem to
be some evidence of this people have
looked at the average number of stars
for Amazon reviews they get around four
stars and in our small set of true
relationships we're getting
significantly higher almost 4.8 so these
are just more likely to be positive
reviews and so you might want to one
maybe wait them less because of that
okay and then just you find a couple
other comments again the community needs
have strong web presence so it's not
clear how broadly you can use this
another way to get more value out of it
might be by just making a little more
sophisticated right if I'm on a message
board commenting about company a maybe
there is no association between me and
company a but maybe there is a strong
association between me &amp;amp; Company B which
is a competitor of company a and that
also right could be very useful for bias
and then a final comments that we were
just looking for the associations
broadly on the web doing know sort of
conditioning to get a better signal but
you can you can very easily get a better
signal we had just sort of conditioned
on cryptography than that ups the
association between this review Robbie
movement and Ross Anderson almost eight
times right it just gives you a way to
sort of them make sure that your the web
hits you're picking up or for the right
Ross
the right Bobby ribbon ok ok so now I'm
going to maybe shift gears again there
were any questions are you okay so so
back to the question of them how can you
use this or can you use these second
ideas to better protect content so if
just been talking about detecting
sensitive content Billy I said anything
about how you play might want to
actually protect it so so as a sort of
motivating example imagine that there's
a intelligence analyst who's doing
research on say that that plane week
that we're talking about earlier so you
want to be able to make sure that they
can access anything that's about the
plane leak but not just read arbitrary
valerie plame documents not just you
know snoop arbitrarily so how can you
distinguish between documents that are
about the leak and once they're just
about valerie plame well this is where
the first detection might help in
protection can potentially take that
sensitive topic and quickly give you key
words that are closely associated with
it and some of those documents will be
in a valerie plame article certainly in
her name's dia might be in there but
there are other terms that probably
aren't going to be in this example it
turns out to be this these people know
back and oddly i think but anyway the
high-level idea is that you're getting
this set of keywords closely associated
with your topic and you might just
require that you know there have to be
at least eighty percent of these key
words in the document before you can
access it because that this is a course
form of topic detection it gives you
some confidence that that document is
really about the topic that you want the
analyst access okay so so we we've
developed any encryption protocol that
does this it actually is more general
than what I've described it can support
any boolean formula on the keywords I
think the threshold case is kind of the
most intuitive and the one to keep in
mind for
those have heard of attribute based
encryption this is a form of that where
we're particularly interested in
attributes that are keywords so what I
thought I'd do is um talk about the
protocol at a very high level so so the
slides that I have in the main deck
won't give um you wouldn't be able to go
away and you know write down the the
actual protocol they just kind of try
and motivate it at a high level but if
that doesn't make things clearer if
you're curious I do have a concrete
example later on okay so because how
would we do is how will we control
access based on topic so one thought is
we could associate an encryption key
with each possible user access right so
I apologize it's not showing up very
well but um so so maybe one access right
is this person can see any content as
long as it's not about playing or Rove
so okay so we create a key for that and
I'm we associated with that access right
another access right could be any
content about playing as long as it's
not about rope you know etc right so for
all the access rights we create keys and
we associate those keys with them so
this is good for the user right the user
has some particular access right now
they only have to store one key but when
we go to encrypt the content a
particular piece of content may satisfy
many different access right so it could
mean a lot of them overhead on that side
okay so so why don't we try again and
maybe start with the content so instead
let's look at a particular document may
be the first three paragraphs happen to
be about play man drove but not Wilson
so again we'll create a key to associate
with that content and will encrypt the
paragraphs accordingly and then these
other paragraphs they're about playing
and Wilson but not row so they need a
new key and wake ripped again okay so
this is looking better on the document
side less overhead unless blessed keys
to add on but for a user now
a users access right may require them to
store many different keys so it would
just sort of shifted the problem to the
other end so as you might expect what we
do is kind of in between the two we we
create a key for each tag where a tag
may be a keyboard or the negation of a
keyword so for example there's the plan
key there's the not slinky that road cue
the not real cute cetera and then to to
encrypt the text will just generate a
random symmetric key whatever you wear
your cipher is a yes for example and so
when Krypton to that AES key so here's a
paragraph that's about say play man
drove great we generated a asking it
can't read this it so now it's encrypted
great but we don't we all stop there we
also will append the keys associated
with those keywords so this was again
about flay Monroe to encrypt it and we
append those keys the plane in the road
key ok so the ciphertext is that triple
it's the actual encrypted content and
and those tag associated keys ok so what
happens at the users and so maybe the
way to see that is with a particular
access right so say I'm the user who is
allowed to see anything as long as it's
both about claim and about Rove so what
does this user store well the user is
going to store this AES key that they
need to get the content but they can
store it in encrypted form so again this
is a high-level description but the idea
is that the ASX is going to get broken
into shares to pieces in this case
because there are two key words in their
access right and they really need user
needs both of these pieces to recover
the AES key and again this is where the
sort of high-level description kind of
breaks down because just having one
piece tells you nothing about the AES
key and this diagram doesn't really
capture that bit ok so they have these
pieces and they're encrypted under the
tag
in their access right so they're Krypton
of the plane he wants it returned on the
plane key and one is encrypted of the
rope key okay so so now this is what
these are stores just those us to a
psycho types of the sort two components
I'm sorry to key with two components so
what does the user do when the user gets
that cat ciphertext up there well they
can pull out that plane key in that row
of key decrypt their their cases of ASD
combine them and apply them to the
content and recover it so so you might
say well this looks like a one-time
scheme though right because once i see a
paragraph is about a plane I've got the
plane key and I'm good forever and
that's again that's where the high-level
description sort of fails you right
because what we do is is a little more
complicated than this we actually do
randomized the whole process you're not
actually getting the plane key you're
getting a randomized version of the plan
key that will only work on that
particular program so it gives you no
advantage of decrypting another
paragraph about playing so let me be the
main takeaway here is that the
encryption overhead now grows with the
number of tags not with the number of
access rights make of that so again the
the keys that we're adding on here are
corresponding to the number of tags in
the region and similarly the user
storage grows with the complexity of
their access right again what not with
the number of accessories so I think it
very much depends on the particular
setting which are trying to use this and
what the access rights are going to look
like but this could do a fair amount
better than either of those naive
schemes okay
so I'm just going to wrap up at this
point so you have talked about how to
how to use the web or another large
corpus to find associations between
keywords and between keywords and
sensitive topics and talk about how that
might be used for bias detection and
then also how it might support access
control so I'll just stop there but I
can't let me know if you questions or
anybody want to see a concrete example
that encryption if you apply your
methodology to one of the earlier
examples how does it fare well if you
use it to do say for attacking how much
better do you do it oh it's the old way
right good good question so first what
is it sort of manageable does it get
this is it weird are too much yeah
example there's anything left right
right that's right so yeah so I think it
probably looks the best if you're
thinking about something like medical
records where you know they're feels
like one field might list all the
medication it's a person on yeah so we
can take out less but you're right when
it's when it's unstructured text trying
to think to meet with any experiments
that can speak to that we actually have
some work going on right now where we
are trying to get exactly that type of
get it get at that type of question
where we we've used these types of
techniques to help someone sanitize a
document it's just a little different in
that they're not just redacted but they
sometimes are revising the text and
that's different and that it is
different it is different yes yes that
may be that may give you good benefits
yes you read it may be something left
yes yes but the interesting thing there
is it even has is pretty darn hard so
then what we do is we take the sanitized
and have people try and guess who it's
about and in a lot of cases they can do
that you know Jesse go yeah well they
allow them to use the web yeah they are
probably doing yet and so it's just a
really it's a hard problem yeah I'm
encouraged about error propagation so
some of the inferences are based on hit
counts from searches and that's an
inexact science because of many reasons
for duplicates in things that sort so if
you have an error bar of a certain size
in the hit count coming back how does
that propagate to the end results of the
sims it magnifies the train what's the
there isn't even predictable yeah it's a
real problem Google just way too smart
for us basically I mean Google is trying
to understand what this query is about
right we just wanted we wanted in that
least 10 how many documents match video
do things like look for synonyms and all
sorts of clever things that can make our
results go in unpredictable directions
so yeah I don't know if there's a simple
answer to what it does to the results
and I think sometimes it's giving us hit
counselor too high sometimes it's giving
us hit counts it could even be too low
and in the amount of error fairies so I
mean it's just it's um it's something
we're struggling with of how to how to
deal with the fact that Google is really
addressing a different problem than we
are we're going after it's a good
question it's our big question so you're
kind of using google or more generally
web search as a black box that gives you
these two counts in general what would
you ideally then I'll replace that
function with if you could have your own
best function for that right we just
want to know how many made this stage
what we think we just want to know is
how many documents have those keywords
now we don't we don't want the search
engine to be trying to anticipate what
we're really looking for but you do
something I guess wouldn't you do sort
of some fancy stuff like stemming and
that's true that's true no you're right
I mobilization that's right that's right
I am over simplifying I am over since I
yeah I mean if it if there's if it's a
plural form of something we want this
singular as well but but synonyms and
things like that it at least now I mean
we I think there probably is a way to
take advantage of that but we don't know
how to take advantage of it so right now
it's just confusing for that algorithm
you just need to like 10 documents to
make it work right consensus to get the
terms so it's good I mean it occurs in
my mind that maybe there is a way to use
collaborative filtering so it could just
look at the AL database that you have of
other errors and get some sample maybe
not using a very clever algorithm but
ask a human to put a number of documents
that are relating the number of
governments that are not related and
then infer from that yeah probably are
extensions like that and I think it's a
good point that that there we really are
benefiting from google smarts right the
fact that those ten documents are giving
us really all we need I mean that's
that's showing the ranking algorithm
working right those irrelevant documents
yeah you're right maybe collaborative
filtering is a good because Tendo penis
is not that hard like the person's you
feel look at for us to see a lawsuit and
people are trying to find documents that
mansion documents that do not match um
there is going to be a human looking
through it and if they can do it by
putting 10 top 10 documents in the ass
bucket and tell documents in no bucket
that's a huge right work where you can
never tantrum well because she mentioned
that if you get ten good documents
experiment it looks like being you know
take myself yeah we were ready to
measure turns out it's the size of a
problem that a human can handle it
that's right which means when we could
deal with huge intimate up to the eyes
we just looking for just a further
magical terms right and then it might be
less Eric roundin than using a search
engine yes
but if humans were to do it no no they
just collect their life after you've
connected to the idea I'd better just
the trained over one more question I can
imagine that 11 demand for such a system
an inference system is ok I have a
redacted data set and I want to make
sure that these medical records for
research purposes really don't have any
personally identifying information but I
don't actually have the names of the
people i just want to say hey is there
any personally identifying information
in here but i don't actually know it
comes from Bob and Joe and right you
know you're sensitive topic it's sort of
what you're saying I don't know what to
so if I understood correctly this
research does not address that
particular need or do you have any
sample for that sort of problem right um
well we we've done some separate work on
say looking at the US census data and
trying to figure out how identifying
attributes are there like he probably
heard these statistics about eighty-five
percent or some surprisingly large
number of people are identifiable just
based on their date of birth their
gender and their zip code so we've done
some experiments along those lines but
that's more just you know it's a
structure database use mine it look how
many records you're getting it it's it
doesn't seem for some of that
information it doesn't seem so useful to
use the web am I getting itch I'm not
sure if I'm getting it here sure you're
saying that the fern t doing sort of a
general inference engine is just do the
do statistics and maybe the general
inference stuff is not not she's so
hopeful yeah well so you're you remind
me of other stuff so right now we're
also looking at preference data some of
you've seen there a lot of them there
just seems to be this movement towards
doing authentication based on preference
does you know when you log into a
website and you get asked to set up
challenge questions and folks are
conjecturing that maybe if you use
preferences that's something that people
find more easy to remember later on you
do like Italian food oh yeah I like
you know that it's for constant over
time you might be more likely to
remember then whenever you put down is
your best friend in high school or so so
the one question there is you know how
how unique are those preferences to you
and that's a place where techniques
closer to this I think could be useful
where you know mining the web can tell
you you know how often do people talk
about Italian food and green food or you
guys something so yeah something that's
that's certainly not a medical record
scenario but an extension but I missed
something about your Enron Horton
example what was the actual scenario
there so Enron would write would like
not to I'm just right yeah you probably
miss it because it sounds very
artificial and it is so so the scenario
was that um that that Enron has all
these emails and for some reason they've
decided they don't want to reveal that
they have negotiations with Wharton and
so they're wondering what keywords they
should be looking for to decide if an
email is about it's about Wharton and
that makes sensor so I guess my question
really is probably orthogonal to the
technical aspects of this world just out
of curiosity would they be able to then
say in response to discovery or whatever
legal action is will say well we'd
rather not disclose these emails is that
even it's an interesting question so
apparently there is a bit of a
negotiation you know as part of the
discovery process as to what keywords to
look for so right so that everybody's
got a huge number of documents and they
decided these certain topics are within
scope and they have to be produced how
do they find those documents and both
parties will agree on okay these are the
relevant keywords so so maybe we could
help support that ok so the subpoena
such as for all the email
apparently in some cases they do
negotiate keywords to look for because
it's a lot of work even on on the
recipients end right i mean they get
this stuff you know it's sort of a
denial of service attack right between
these parties right I'm going to slow
down the other side because they have to
go through all these documents so there
is some motivation I think on both sides
to call a list both to hide things and
to making but documents are available i
think the emails i mean data basically
only misused it oh yes yes sorry we're
talking more generally in litigation you
know how much do you produce it's a
subpoenas for all your email it's pretty
easy tennis is no problem it sometimes
will come back to life that's too much
discrimination lawsuit you don't you
don't get to see you know my emails
about the AFL draft or what you know you
know you're not going to get to see
everything and the judge'll sake has to
broader requesting then another question
about scenarios so in the very last part
the topic stuff is the idea there that
you would have let's say in a classified
setting with different levels of
analysts are different you know you'd
have the open-source analyst and you'd
have both oil is that the scenario so
that's that's what that saying that or
that sort of thing and is there also
kind of a commercial world low aspect
absolutely well so it did it did come
from actually a xerox client that the
motivation for constructing a thing
because there was a mortgage company
that was that was saying well we've got
these loan documents and this is of
course in fact when people are getting
more it is too much of it they're like
you know thousand pages or something
apparently in these applications and
they had separation of duty and their
organization so that not everybody could
see they did wonder they see everything
but they didn't want to maintain
multiple copies of these these loan
documents redacted differently great
person so then i think i'm like
encryption and okay well what's like our
speaker
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>