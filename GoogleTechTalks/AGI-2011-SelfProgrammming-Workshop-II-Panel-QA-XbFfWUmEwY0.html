<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: Self-Programmming Workshop II - Panel Q&amp;A | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: Self-Programmming Workshop II - Panel Q&amp;A - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AGI 2011: Self-Programmming Workshop II - Panel Q&amp;A</b></h2><h5 class="post__date">2011-09-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/XbFfWUmEwY0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so of course we did run a little bit
tight this morning so I apologize we had
to cut out the QA bits from several
several talks and I know some of you had
some so I think maybe we'll start well
it's got to leave it open but certainly
now would be a good time if you want to
ask questions you know of particular
presenters related to their talks this
would be a good time otherwise we'll
sort of just have it be an open
discussion I will sort of mention if you
haven't had a chance to glance at the
website for the workshop and see the
sort of the basic ideas that we were
that we were shooting for here of course
it was self-programming and as Ben said
in his talk this morning you had a
pretty wide range of things that can
mean from sort of straightforward
learning on one extreme to these sort of
grand vision systems that rewrite their
own internal circuitry and from the from
the talks we had here you had several
different points along that continuum so
I think maybe we have some interesting
places to start the discussion thing I
think with that I'll just open it up and
you guys start with with any questions
here and the presenters if you weren't
here there's a couple of microphones
here that you can pass around if you
need to so I'm think I'll get everything
started and sort of take a seat so
everybody want to open things up with
yes there we go
okay so there was a question this was
specific to the Becca cognitive
architectures first question was do I
have a simulation environment and the
short answer to that is because the
interface is so simple just a passing
vector back and forth I can interface
with a number of simulation environments
right now I'm just doing extremely
simple simulations in MATLAB I'm looking
part of being able to interface with
ross's there are nodes for the player
simulation environment which is quite
capable as well but others could be
developed the second question was how do
i program in the reward so in in a
reinforcement learning problem an
important part of it is getting a reward
and in tasks that I do the reward is
part of the task we were talking about
this over the break a couple of us
that's something that is hard coded and
hardwired in when you define the task
part of that definition is the reward
when you define the reward then you
define what what behavior constitutes
success at the task okay so the question
was is tinkering with the reward and
intelligence critical feature and if so
is that is that self-programming well I
think first of all that would depend on
on how badly you tinkered with it
obviously I'm in a dramatic change to
your reward function could affect your
intelligence a lot like if i change my
reward function so that all i ever
wanted to do was eat eminem's that would
prep might make me less intelligent
depending on how easy it is to get
eminem's right if i change my reward
function so all i want to do was die
then that would decrease my intelligence
a lot I've become dead but of course a
small change the reward function
wouldn't
it wouldn't need to I mean I think as I
said in my talk yesterday whether an AI
system can explicitly alter its reward
function or not is a design decision and
you could have a lot of self-programming
certainly with that without doing that I
don't think that's necessary for
self-programming consider the effect on
your intelligence of getting tenure
where I to the a approaches that this
thing between fitness and reward and
like oh there is evolving reward
function and it seems that not so much
the outcome is dependent on the reward
functions also its elder or sooner or
later would come to its end provided
that fitness function is well defined so
so if you make this distinction then its
independent if otherwise I don't know
this should be dependent very much but
yes who's that question they dressed you
the question is how would you create a
new reward I am currency my system by
inference again but yeah for example you
want to reach a goal a right according
to the system the knowledge well P
implies a then p become a derived ago
okay that dancing is learned because is
not building because this knowledge
between a and be learned by the system
from his experience
historically yes but in reality yes yes
the problem is all the knowledge which
derived the nuvo are the system's
summary of his experience is past
experience it very likely to be wrong so
badly when this inference chain is long
enough there is some such a signal in
psychology called functional autonomy
okay so historic Adi call a is derived
from be but after that they are handled
independently in the system so after a
while there are several scenes several
possibility why is a still there but B
becomes stronger he becomes the dominant
call for a certain moment even though is
derived from a which wich happen very
often in human life for example why
those this these people want to do a TI
very early maybe is derived goal from
some other scenes some people want to
save the human beings some people want
to make more money some people just for
fun ok after a while this goal become a
goal all by itself so in this sense
people will say that's my goal that's my
choice but if you check your full
history it's not it's derived typically
is derived from something else but after
that the link is broken by relevant
specialty if this is more than once then
ok I also exact stream situation that
derived go make a critical bike to be
against the original goal that's also
very open so as far as go back to my
basic assumption as far as you assume
your system does not have sufficient
knowledge and resource to know all the
consequences of your decision to know
all the future possibility and
this kind of thing become irritable and
then it's a certain moment your goal at
the one that you're developed from your
history by your logic even though it
cannot be reduced to any of those things
become your own so bravely you really
they say that's my goal that's the goal
I created by myself but it's not coming
from nowhere at least that's the
situation in my sis system yeah I think
it's necessarily point out that a huge
amount of human behavior is not goal
driven from the top but is essentially
an imitation of roles from role models
and attachment figures and the goals are
essentially things that you found your
attachment figure trying to do and so
they show up at a level below the level
of high-level imitation of roles and so
to a large extent I think that's where
goals come from and then of course
there's sub goals and so forth
underneath that yeah I was going to say
something similar and large in a bit i
think that the model of intelligence
systems as reward driven and gold driven
it is a useful model but is not a
completely accurate model I mean human
beings are not that rigorously
goal-driven whether you look at it
evolutionarily or psychologically and I
think in reality and a GI system
operating with limited resources if it
has a top-down go hierarchy programmed
into it it's going to be a partially
goal driven system with a lot of
behaviors that either due to entrance
errors or other spontaneous internal
dynamics the dynamics are not
specifically oriented toward what the
top level goals are so given that if you
have built a system that that is able to
install itself with new goals it is
quite likely to at some point install
goals that are not rigorously drive from
the previous ones either either by a
mistake of reasoning or by some
spontaneous dynamics
especially goal-driven and of course you
see this in human life all the time
where people will come up with new goals
that seem unrelated to the goals they
start out with and that someone at age
50 may be pursuing goals unrelated to
what their 30 year old self want to do
where any of their friends or family
want them to do and I think that that
basic phenomenon may happen less in AI
systems and then people but logically
there's no reason it can't happen
because we're not talking about like
mathematically rigorous 100% go
fulfilling systems in reality fulfilling
goals and rewards is just a model of
what these systems do Stefan that's
right yes i agree i also wanted to
comment on that i think when it comes to
rewards it's important to not mistake
the the map for the territory so a
simulation of a reward or a goal it's
not the same as having a an internal
reward or a goal if we have a simulation
we want the program to operate in a
certain way or towards a specific end
that we imposed on it that means that is
if it feels it's basically our failure
and not the theater of the program or
the system if you want a reward to be
part of the program itself that means it
doesn't need any external reward
function it needs its own dynamics to be
geared towards a specific end so I think
the goal if you want if you wanted an
artificial system with an imminent goal
then you shouldn't expect a reward
function to be sufficient you should
build a system in such a way that its
dynamics are geared toward that goal my
comment on it the Sailor idea yeah I
believe we use a little bit different
approach and we try to design a
universal without function which is
intrinsic reward and all those would be
just dependent on that reward which is
purely information theoretic approach
and we want just to say
that what is rewarding is the same as
what is where you gain information when
you get information you get a reworked
and it's not the only approach it like
shoe cover is doing compression progress
approach for intrinsic rewards so so far
but these approaches where does a like a
attempt to make a intrinsic without a
universal one that is built in and all
the goals that you can imagine are just
only cases of this information hungry
preceding sources or so that's that our
approach so in this conversation it's
useful to make a distinction that I
think often goes unmentioned in this
community which is there's 22 goals that
can be orthogonal one is creating
artifacts that do something really cool
and one is creating artifacts that that
behave in the same way using the same
mechanisms as the human brain so
modeling the brain or performing on
tasks and I think most of us hope to be
able to do both but there are many
instances where you can do one without
doing the other and so in the instance
of rewards there's two questions there's
one is how does the human brain do it
and how do animal brains do it and the
other is using rewards can I get
artifacts to do what I want them to do
one way or another and I think that's
what I'm hearing is two sides of that in
these comments what's right
I didn't know the other approach before
this conference so of all the speeches
this was closest to what we are doing
but to compare I I think we should make
more comparisons I'm not sure if you are
now doing it in real environment in real
time so that may be a difference but in
general reinforcement learning is one of
the approaches that what is quite common
but but how is self-programming it's
quite different in all these approaches
lipids yeah this is also my first
exposure to age yeah now so it would
probably require some more talking to
answer that question in detail I mean in
general they're they're both cognitive
architectures and there's a number of
approaches that fit within this same
broad category yesterday we heard a
great description of the light
architecture which also you know has a
number of similarities and in general
anytime you have an agent interacting
with the world a lot of things are
likely to look the same so there
probably are some similarities i think
the small differences in the details all
of us who are working in this area and
developing in this area we don't know
yet what's going to work the best so
we're all kind of pursuing different
answers and cherry picking from each
other trying to pick what works best and
come out with something really neat they
wrote a question here and long one on
the back guys have a good one more
comment on that I think one possible way
to parse the different kinds of
approaches to AGI is to say that at the
very very highest level everybody is
trying to imitate a human intelligence
in a very abstract sense and what you
want to look at is how far down they go
before they cut over to an engineering
approach and and I think you can
actually get a spectrum simply saying
okay we want to get the just the the
notion of intelligence or the notion of
intelligence with certain human-like
characteristics or we're going to try
and reengineering zuv v1 v2 the
particular parts of the
brain architectures if we know them all
the way down to we're going to simulate
individual neurons and there's just a
whole host of different cut offs in
between but I think that a lot of the
different approaches you can you can
look at in terms of just saying okay how
far down do we go before we cut over to
the engineering yeah this early agree
because I think in that that captures
one aspect of what happens in a GI but
there are also respects in which many of
us are explicitly not trying to do what
the human brain does or what the human
mind does I mean I think we're there's
also a strain of trying to go beyond
what the human brain is capable of even
in the architecture and this business of
goals and rewards ties into that because
I think a system like open cog tries to
be more rigorously goal-directed then
then I think the human brain is and I
see that as a feature rather than a bugs
i would like powerful AG AI systems to
have more of a tendency to direct their
actions toward their initial goals then
people have so it's not all about trying
to emulate the human mind brain unless
you can do that very very very broadly
well I think actually what you've done
is to find an abstraction above the
actual human brain sort of like an
angelic or something that that is it
hopefully yeah exactly but it's sort of
an ideal human which none of us actually
is and and that's what you're and so I
don't get a give it I don't think of it
it's trying to make an ideal human I
just think of it it's trying to engineer
a different sort of mode well an ideal
intelligence that's AI excited that you
need it for the computer what can you
say anybody on this topic I operates
Josh I sinkers also no matter what you
do we have a lot of difference right but
still we sink even been seeing in some
sense it's a mind also that in science
is similar to the human mind it just say
that below that level I don't care it's
just like Josh side where you cut
two men mentioned that in a TI 08 okay I
have a paper just on this topic is
called what do you mean by a I you know
we're in that paper I t-55 different
level okay to try to capture a different
perspective about which level are a I
should be similar to to the human mind
and below that every whistle and say
Lord I oh I don't care okay our
difference is like that so if I would
take creativity as my approach at least
in this presentation the general idea of
creativity is that you ignore certain
differences on a higher level so that
things become alike you're able to
transfer from one system to another
system so transfer the knowledge of one
system to another system so that's
technique that's common in recognition
for instance where we ignore some
differences between objects in order to
say that they're like order say say that
they're of the same class and in terms
of creativity you're doing at on a
higher order so Einstein was able to
ignore certain differences making that
you know his theories he was able to
ignore certain differently so that his
theories of physics could become
geometrical for instance if you think
about art and I'm talking about the
eminent sciences right now because those
are the best examples that we have
ignoring certain physical differences
would allow you to go beyond just
picturing them realistically but and
then picture so that you can picture
them imaginatively so I think
in debt approach there's definitely an
element of going to a higher order level
and then seeing well the short answer is
you copy off of the ones who get rich
powerful and lots of women and you you
don't copy off of the ones who get
laughed at at least that seems to be the
human history yeah but yeah we act we
have a bunch of built-in heuristics that
form some of our basic motivations that
stand for that in the in the in human
mind and there's clearly a version of
that hierarchy an invitation thing that
is the aversive side of this you you you
match for bad outcomes and and avoid
doing whatever it is that that your
theory of causality tells you led to it
and you know sometimes that works and
sometimes not but yeah I mean it didn't
have time enough to give the whole spiel
there but that that that's clearly
something that the system has to take
account of
well the the it was an attempt to define
the extent to which a given active
learning is an act of self-programming
so there were two components one how
much is learned about X the other is how
intelligence critical is that thing x
that was learned about right yeah to
tell how intelligence critical a given
our active learning is first of all you
need a measure of the intelligence of
the system and then you need to somehow
estimate like how what was the causal
impact of that given active learning on
the intelligence of the system and in
the case where you could create an
ensemble of systems so you say you have
an AI system your computer and it
learned something at midnight you could
kind of freeze it at eleven fifty-nine
and look at a bunch of systems that
didn't learn that thing and a bunch that
did learn that thing you could calculate
the intelligence of each system in the
ensemble and then you can get some
quantitative sense of how critical that
particular act active learning was for
the intelligence of the system but if
you're dealing with it with a singular
system like in person there's something
where you can't make an ensemble and do
that experiment then you have to use
reasoning of some kind to indirectly
infer the intelligence criticality which
is a hard problem I'm
qualitatively we seem to be able to do
it in everyday examples all right so I
mean look if you learn the meaning of
some new words and in the server
Croatian language versus you learn some
new strategies for doing scientific
experiments that you then kind of can
generalize to other things it may be we
have a sense of the lather it's probably
more intelligence critical than the
former which is based on kind of
intuitive reasoning rather than rather
than rigorous reason and to do that kind
of reasoning rigorously of course it's
hard but I guess in in educational
theory they try to do that right because
one of the goals there would be to
improve the performance on standardized
tests of students and they try to teach
various things and that they do have
some idea of which which things you can
teach kids to improve the problem
solving the building and intelligence
and there there's been some idea that
progressive education trying teaching
children to problem-solve works better
than just rote memory for example they
decide that problem-solving skills and
more intelligence critical than
memorizing lists of facts so there there
seems to be some tradition in making
this kind of judgment qualitatively but
it's not very rigorous yeah on the same
topic because I would like to we come to
the question of what we call
self-programming because for me a
programming is is like that there must
be something like chewing complete or
not necessarily chewing complete machine
and the code that is executed because
otherwise that is that's learning some
type of learning or fitting and so when
we use these terms of programming do we
mean execution of some programs or
program like structures or doing
something more like like I think Ben
wants to to introduce here it's a
question on naming convention just well
according to the definition I suggested
pretty much any active learning would be
soft programming to some minuscule
degree and
then it then becomes fuzzy so a really
strong example of self-programming of
course would be a some system that
actually revised the source code
underlying its core algorithms and that
would have a very high fuzzy degree of
self-programming by the definition of I
proposed whereas learning some random
fact about the world since that fact has
very low intelligence criticality would
have a very low degree of
self-programming and whether something
involves learning a program or not also
that seems a difficult thing to define
because if you have a system like a
brain I mean we can model it as a
programmer as learning programs in
various ways but then whether something
is a programmer is program learning a
lot it's largely a matter of how you
choose to to model that system rather
than the system itself I would say that
that one way of thinking about what it
means to program is simply what it takes
to go from saying what to do to saying
how to do it yeah I kind of have the
same reaction as him about the Penn
Sebastian I think Ben's idea is
interesting is but i will say is is more
proper to be called something as
significance of learning for for certain
learning result it can be important can
be trivial well you want to measure how
important it is it's not really limited
to self programming and exam food it can
be fully applied to declarative
knowledge learning but usually they say
to Gerard Eve knowledge learning is both
of course it's also related to self over
I me but it's a kind of like more
remotely or into early okay so I have a
question for everyone on the panel then
what do each of you mean by soft rug
early I already mentioned naughty man
talk well can you summarize and like
once important actions together into a
bigger truck in black else is this
chunking is so programming the work of
self-programming instead of chunking
well that's what we mean by programming
at least to me okay what programming
means way everyday way to
programming but that's what we do is to
me what every concept is fuzzy okay but
either you make it to fugly it become
you know if we cover everything you
cover number every example of chunking
see every example of choking and not so
going and talking I will just say
programming well in the intuitive sense
then all programming itself program no
no but if an enemy this computer I can
program it it doesn't program yourself
in that comment everyday signs of
programming okay but what is their
learning which is not so programming
yeah learning declarative knowledge it's
not directly related to South
programming but it is indirectly see you
assume a rigorous disinterring
declarative and procedural last so what
if she had an AI system that doesn't
have that kind of distinction built into
it at a certain level of description
avail as far as you stay with a certain
level of virtual machine that's what I
mentioned you are example is when you
move from one virtual machine to another
program and one I will become data or at
another level of you also that you have
a system which is say some kind of big
self-organizing neural net which seems
to have no distinction between
declarative and procedural nodes what
you would say is you want to model at a
level that distinguish is declarative
and procedural knowledge yes and then
only the learning of procedural
knowledge itself programs yes but yes I
want it will be a to be merry mouth so
that we can talk about that clunk really
don't you think sometimes it can be very
hard to model the system in natural in
there just like you can say every
concept is funny if you want to push
that far then the every concept can be
can be exchanged with each other they
become the world i'm lucky that is very
interesting but it's a matter of for how
to use a word is anyone does anyone have
a different concept of self-programming
it hasn't already said I believe I I
said a little yeah we're building up
quite the queue of questions here so how
about jargon I promise promise in one
well good
I can take out that maybe we will put
that on hold for one second because he's
dying say something that they were just
talking about so then welcome now I
think that emotion in a our systems is a
pretty big topic which is kind of a
digression from self-programming like
why I think I can say something short
about it that relates to my to my talk
so I think emotion is uh as a lot to do
with the energetic constraints that I
mentioned in my talk so emotion is one
way of diverting from a from a path that
has been defined by a program and it's
not a programmatic way of diverting from
a path because it has to do with the
energy that's available in the system
and attention that arises from the
required energy in the energy days
available so 00 stick with that
you mean the unpredictability of the
result of that self modification like if
I if I change my reasoning algorithm
it's harder for me to predict what's
going to become of it then if i change
my understanding by the parsed
serbo-croatian verbs or something right
yeah why you say the more useless I mean
it seems like the more the more volatile
the outcome that doesn't necessarily
mean more useless right well well but
just empirically in terms of human
common sense it seems not true right
like learn learning how to solve
problems they're learning how the reason
using Bayesian inference better it may
be unpredictable what the outcome is
because it could lead you in a lot of
directions you didn't expect before but
it doesn't seem to be so unpredictable
but it makes you go jump off a building
or run off into the into the woods and
become a primitive or do completely
crazy things right so it seems like an
inhuman life it's not true we can learn
intelligence critical things and it's
not that volatile as you say though it
is more unpredictable than learning new
words or something so about I think I
think you're you're probably right that
there is there is some result there that
the more intelligence critical and
active learning is the harder it is for
the system to predict exactly what will
come of it but maybe not as extreme is
what you're you're implying
um well I'm not sure I understand what
the question is here I did yeah i think
the the i mean the point was that that a
plastic representation one where you can
force a match in a way that causes you
to see what it was you thought you were
looking at in a different light and that
you can translate that different light
unto the action in such a way that you
get a different action is is key it's
just you you can't do the you can't do
the operation without it or else you'll
you your your ability to imitate would
only be purely in exactly literal
imitation of what you exceed ok yeah
well the examples I talked about in the
paper are essentially being able to
knock out on salient attributes of
things that you're operating with in
favor of salient ones for example if
you're trying to put up if you see
someone putting a small red block on a
big green block and you're supposed to
imitate that but all you have is a small
red block and a big green block then you
have to ignore the color and and use the
size and if ya in yeah at the at the
level where you're you're you're parsing
things into objects with with with
features like that but that's exactly
what that's exactly what it means at
another level for example if you're
looking at you're imitating somebody and
you have a different kind of armed than
they have okay you have to be able to
ignore the the fact that
that you know they're using a human hand
and use your your two finger gripper
instead and have to be able to map what
causes the desired effect namely
grasping and lifting a block in terms of
your own equipment and so that there's a
lots of different kinds of mappings that
are that are necessary and the stuff I'm
doing which is which is really very
difficult and hard to describe is is is
trying to force things like numeric
vector representations to capture
salient features so that you can say you
can do quadrature with with vectors and
and I'm not even sure that when line is
pretty easy you just erase the most
recent memories well I think on them on
a mechanical level that that's easy
because the system saves its state
periodically to disk so the question is
more in an autonomous self motivated
system after it changes its motivational
system maybe it doesn't want to undo and
go back to what it was before which it
which is the really subtle question i
don't i don't think there's really a
workaround for that and i think that's
that's that's a feature rather than a
bug there's no mechanism for that in my
work either there's a in I mean
you and I there are lots of exploratory
things that we can do that can never be
undone as well so one might just argue
that's part of interacting with the
world yeah my system you can you can
partially undo something Oh see at the
beginning you think it's better than B
and you try it turn out to be the
otherwise then you'll prefer be and
after a while you change your mind bike
say you're seeing a PS better than a and
let the other way around but you never
exactly go back to a previous or state
you always have accumulated evidence so
so but partially you can you have h you
or your preference I wonder if I
understand your question because it's
everything it flexible and once it verse
exploratory action and it is not
rewarding after shorter or longer time
then it's discarded or at least the this
route of state action is discarded so so
there's no it might be only just for for
for some time may may lose some
resources the system may be some
resources of doing some action that is
not optimal but this is the question of
exploration to do something that is done
optimal to to possibly maybe find not
local Maxima but global Maxie mind it's
also so there's there's no problem with
discarding this exploratory action right
well in relation to my talk I think it
was mostly about that problem so what do
you do when exploration has sort of
feels and whether you're able to
transform a search space and and and if
you know when you should start doing
that and how do you do that but I could
add to that is that exploration is is
always changing the system in some ways
in that it it takes time and energy to
explore so the system is never in
exactly the same state so just by doing
explorations you change the system in
some some way although it might be
minuscule if you transform the system
itself it's a large
change is just small exploratory steps
it's a minor change well the first time
I have a chance to speak in this panel
but these specific questions my system
is a little different yeah is there is a
direct relationship between knowledge
and results so a reward the concert okay
first I should say some of the concept
that has have been developed during the
engineering of intelligent systems do
not mark quite directly so I'm doing a
little bit of guessing but I believe
that the notion of reward belongs in the
input to the system that is the
knowledge so it would be an act of
learning and when this act of learning
occurs then there would be an automatic
response autonomous response from the
system by creating those structures that
have been speaking about in those
countries are they want to have an
intelligent critical value value value
useful for intelligence that that would
be basically the other comment that is
also somewhat related is that I would
like to see some more emphasis on the
cell of self-programming I am plenty
satisfied with the programming part
we've been talking about programming a
lot but somehow I see that there is
under some underlying feeling that we're
the ones who are going to do it not the
autonomous system so we talk about the
autonomous system we should emphasize a
self part a lot more is not as doing it
is the system doing it so how does the
system do it well I does they don't so
I'm going to abuse my moderator
privileges and follow up there
sort of a follow-up so obviously there's
a there's a pretty big discrepancy on
what we classify a self-programming
versus learning but if we if we pick
something that is sort of far enough
along the scale that everyone would
agree to call it self-programming then
we seem to naturally think of that as
being sort of at the top of your pyramid
right it's you get that after your
system is being essentially reached a
level in which it's capable of
introspection and it's it's not just
making arbitrary changes it has some
theory i'm going to change this about
myself because of something that it
knows do we think that there's any any
value or any help in sort of decoupling
those and having the ability to modify
the system obviously you need in the in
the absence in the absence of a theory
of modeling which this sort of
underlying theory of why I'm making this
change versus making the change itself
so in other words if I took something
like genetic programming it's it's
rewriting source code for some it's not
self-programming because it's generating
something but but it's making these
changes with no underlying idea of why
it's doing it is there some value in
sort of building up to a self in with a
system that includes the ability to
modify its own architecture even if it's
doing so essentially at random and then
getting feedback as to what what worked
other words rather than putting it at
the top of the framework can we put it
somewhere near the bottom can you build
a toddler with yeah with the system that
has self-programming even though it
doesn't have enough knowledge about
itself to to know beforehand why that
change would be good so you're asking
the extent to which fairly strong like
reflection oriented self-programming
could be useful even in the early stages
of the p.o jet and hierarchy yeah in a
manner of speaking I mean without the
without the sort of the introspection
part in other words it's making these
these change
that are extremely significant but it
doesn't necessarily know beforehand that
it need c4 summary it seems that in
human development many many profound
changes occur during the first few years
but a lot of these seem to be like
genetically triggered rather than
through the system's own own thinking I
guess I kind of think that a a baby or a
young human child or an early stage a GI
system lacks the intelligence and the
breadth of knowledge to think about it
its own thinking very significantly and
figure out how to how to modify itself
sorry it's not my gut feeling is that
kind of deep profound self modification
and self-programming should my mind is
something that probably can only be
profitably undertaken by it by a fairly
mature system i could think about my
kids when they were little when they're
like three years old they were learning
a lot about the world but they weren't
doing a lot of reflecting on their own
cognitive processes and how to improve
their own cognitive processes in some
kind of deliberative way they were
learning and adapting right and then
once you get older like it in the high
school or college maybe you start to
think like going on I'm not I'm better
proving this kind of theorem than that
one how can I adapt my theorem proving
process and I'm learning process better
but to me I feel like that comes fairly
long in the hierarchy but well maybe
that's over fitting to human development
it's like the Holy humans do that yeah
maybe there's a whole different way of
doing cognitive jumpman AI systems much
less similar to how humans do it that
would kind of use soft programming at
the beginning to bootstrap itself i
think steve o majandra was working on
that some years ago China do you have
any comments on that yeah
so you were thinking about or working on
a system that would kind of bootstrap by
self learning and self-modifying is that
right yeah it's a matter of what's
sufficiently powerful is because I would
say a two-year-old kid doesn't want to
be a self-modifying system explicitly
because they don't know what the hell
that means but nor would a two-year-old
careful about it I have a major
objection this whole line of talk I I
don't think there's a qualitative
difference between learning about
self-awareness and and and meta learning
and so forth then just plain learning
and I think it's essentially what
happens is that as you learn you you
fill in levels of your ontology
hierarchy and you can't fill in the
higher ones until you fill in the lower
ones for them to have concepts to to
latch to and and and refer to so that
that as you grow mentally you're always
learning new things and you're always
programming yourself by adding new rules
to your codebase essentially and once
you get to the point where you're
explicitly reasoning about yourself
you're just doing the same kind of thing
you've done a lot of before add up at a
higher level and what's happened is that
that these rules allow you to operate
what's essentially a world model and at
some point in the phn hierarchy you
start including a model of yourself in
the in the world model but I don't think
it's a qualitatively lift different
thing than what goes on in the whole
rest of the
here here yeah when that broad says
you're definitely right but then the
line between so programming and learning
gets blurry I thought he was
specifically asking about the mark on so
this phenomena is rationally guided this
phenomenon has actually been observed in
the sum of the simulations that I run as
it goes to achieve its goal spends a lot
of time around the goal and as a result
the features associated with that goal
get populated much more richly or you
know that there is a very strong
movement now in in the field of
neuroscience that refers to the
unconscious they called it the other one
in me and what they basically are saying
is that there is most of what we do most
of our actions and decisions are
entirely unknown to us so we don't know
who is making our decisions or how they
are made there is a big part a
substantial important part of our brain
that is working automatically
in and in complete disconnection from
our cognition that part is what I claim
is exactly precisely the emergent
inference that part is a wonder does the
learning that does the processing and
then create and generate the
intelligence structures that Denny
delivers the cognition for Jules in
probably a reward system people can
control that you can program your
sub-conscience ask questions from it
give information to it at him it will
tell you for example in the P LJ and
hierarchy as the learning process it
will tell you the where you can not go
further and modify some visa a lower
limit of your architecture where you
cannot modify modify down under and it's
depend on where is this line if it is
slamming of self-programming for example
the algorithms for learning or all
actions are fix it and yarmulke
parameter or those or you can burn the
parameters of those well you can use
this as learning of self program if you
can change or or change the preference
for these options so maybe it looks more
like self program but in one sense there
are one level in any system that you
cannot modify the number and one Dionne
asking if this limit could be the
evaluation of the action of this
this is this is Hofstadter's argument to
that he says that there is some level
you can go below but the same is in
nature physics you can even change the
speed of light by a tiny portion because
you lose everything and what we are
trying to do is to build on top of this
but we cannot change a system that is a
close to chewing complete machine and if
you have a chewing complete machine that
is fully flexible then you can looked
right into everything and a pleasant
approach that's a solution so well let
us know this limit of what happens below
monster on get as many more questions
here well in my own perspective as I
presented my talk there wasn't a
threshold and I was just looking at
self-programming as learning of
intelligence critical system features so
in that approach you don't need to make
a choice of what's cognitive content
into what's infrastructure or what to
declarative or what's procedural so in
that sense yet an active learning system
could conduct intelligence critical
learning just by putting itself on the
difference in a different environment
yeah well no because depending on your
AGI system
a system could use different sorts of
processes for more intelligence critical
learning that than for other kinds of
learning which might be rational if what
he says is correct if learning
intelligence critical features has more
volatile results and is more likely to
screw you up you might want to be more
careful about it so that you could have
different processes for that and I don't
actually think that's necessary based on
what I understand so far but it might be
we're not nearly there yet in terms of
playing with it with our systems getting
back to little children I mean you can
see that there's a lot of reasons why
little children are small and weak and
can't cause a lot of damage right
because they're in a way that they're
their intelligence critical functions
are changing all the time and in quite
volatile ways and because they're small
and protected by theirs they can get
away with a lot of volatility and what
we're supposed to be more stable our
intelligence critical features are not
supposed to be changing so much so we're
expecting to behave in a more a more
reliable way right so all right so it
won there and then here
well ontogeny recapitulates phylogeny
and so if you have a theory of of the
brain that that's based on an
evolutionary story you're almost
willy-nilly going to talk about the
development of the individual
intelligence along a similar pathway and
that's the way mine works out and of
course that's not the only way it can be
done and I'll let anybody else that is
doing it differently I think you make a
really good point that for most of us
either explicitly or implicitly we rely
heavily on introspection our ideas about
how we think and our ideas about how we
acquire that intelligence if you take
that away a lot of us are left without a
basis for what we do and it's something
that bears careful inspection for anyone
working in a cognitive architecture all
right here
so we obviously they system that you
learn anything and so create an
arbitrary program now obviously back to
be really stupid algorithms that may
need to be changed and music literally
as you come to just constraints when did
this your constraints out if I make a
sell them and use faster more efficient
and might be unable to do certain things
whereas if I make any slower solving
your puzzle so I'm going to go back to
his question briefly I think the human
human cognitive development is certainly
not the only path to AGI it's just the
only path that I understand well which
is why I'm using it as as a guide and in
in terms of smarter and smarter systems
as they self program necessarily
becoming more and more rational I think
that that's that's highly uncertain
because the theory of rash now the under
limited computing resources is just not
not that clear so that that's one thing
that could happen but the the
circumstances under which it would
happen or not that clear to me and have
a lot to do with the situation's they JS
&amp;amp; 2 i'll leave it to someone else to
answer his question so the question
there was basically myself programming
it's fun well for me i can i can try to
answer that question so the question has
been raised a couple of times before
whether there's a difference between
learning and self-programming in general
I think self-programming is a special
kind of learning and that's interesting
because it requires two things it
requires itself and its requires program
and I think as you already
Shin rightfully focus has been a lot on
the program and not so much on the self
what I personally find interesting in
the concept of self-programming is that
it forces you to think outside of
programs so as I try to demonstrate in
my talk you can't have a program
programming a program because that's
just a program so you need something
else that makes programs what could that
thing be well it has to be itself then
what defines itself I think the best
definition of itself is something that
has a goal of its own so not something
that has been given a goal through by
means of a reward function or something
but something's that's then dynamically
structured so that it tries to achieve a
certain goal those are the questions
that I see coming out of
self-programming and those are the
questions that interest me personally as
Ralph Waldo Emerson would have put it Co
hood thyself all right so i think we're
think we're nudging up against the lunge
here that was out that was an excellent
way to end i think so thank the panel
everyone</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>