<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Using Statistics to Search and Annotate Pictures | Coder Coacher - Coaching Coders</title><meta content="Using Statistics to Search and Annotate Pictures - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Using Statistics to Search and Annotate Pictures</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l4Qb3mMlY6U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks for coming it's a pleasure to be
here
so I visit about a year and a half ago
and then I at that time I gave some you
have a talk on the work that we were
doing at the time so this talk is an
update of the work mostly the things
that we've done since then but for the
benefit of those of you that might have
been around I know that this is a
company that's hiring a lot of people
I'll just start with a brief review of
the things you know historical review
all the things that we've done in the
past and then I'll I'll go more in
detail of the most most recent ideas
okay so my work are a significant part
of the work that we do is in this area a
general area of image retrieval and the
basic idea is what's usually called
content-based image retrieval so we want
to build systems that can search for
images and learnt well it images and we
want to do that by analyzing the visual
contents of the images so the idea is to
see what can be done without relying on
metadata there of course many search
engines already available that perform
image search by the analysis of metadata
a bit in a web pages or little
timestamps that the cameras at these
images but the focus of this area is
more on trying to understand what can be
done by by you know looking at the
images so I'll start with by giving a
brief description of you know the
evolution of the field this is a field
that's relatively new so it's trial
around the late 90s or mid 90s and the
the first paradigm that people came up
with to try to solve this problem it's
what's called query by sketch the basic
idea being that you have some system
which I cannot see that has a little
canvas where users can basically sketch
or introduce a sketch of the types of
images that they're looking for and so
that was that slide that we had up was
about it's kind of a limited interface
because lots of times people don't
really know out the sketch or they don't
wanna it's too much work to provide the
sketch of what they're in stood in all
right and so towards the late nineties
there was a move so this is like what I
was talking about there's a little
canvas where you can sketch the things
that you're incident but it's it's not
very powerful so towards the late 90s a
lot of work done on the area of query by
example and then since the turn of the
century there's been a lot of work on
what's called semantic retrieval let me
briefly go over this too in a little bit
more detail so a query by example system
is we see somewhere that supports
similarity or image search by strict
visual matching the basic idea is shown
here the user provides the retrieval
system with some example image the
system extracts a collection of visual
features it could be about you know
texture color shape or whatever visual
features might be deemed relevant then
that makes a little signature signature
which is extracted from the query image
it's compared against the signatures of
all the images that are in the database
and the top matches are returned and so
like I said in the late night in the
during the night is there was a lot of
work in this domain and you can actually
quite ask questions such as you know
what is the optimal retrieval system
under this framework and in fact we've
done a lot of work where we tried to
come up with you know a framework for
doing this operations inside of an
optimal form
most of our work ended up in this area
of the minimum probability of error
retrieval and this is where we formulate
this matching operation as a
classification operation and then we can
ask what is the optimal decision
function for image ranking in order to
minimize the probability of the matching
error of course a retrieval system is
not only about image similarity so
that's only one of the parts a virtual
system there's a whole other sets of
things that needs to be done and ideally
this formulation should be such that it
also is helpful in the solution of those
questions those questions like relevance
feedback you know indexing how do we
design this classifiers so that they're
scalable and they can be applied to
large databases and so forth and this
minimum probability of error formulation
turned out to be a nice in a global
solution for all these questions and one
of the lessons that we learned is that
when one is trying to solve all these
problems it one cannot really rely on
you know very complicated matching
functions and so all the retrieval
systems in this area turn out to rely on
image similarity operations that were
you know relatively simple so here's a
depiction of the car of our system so
basically there's we have images images
are the compare decomposed into bags of
local features or local feature vectors
there is a huge literature on machine
vision for it with respect you know
local features or local descriptors for
images and pretty much any of those can
be used and then we basically formulate
the problem as one of statistical
classification so we estimate the
density associated with each of these
images so we fit a Gaussian mixture to
the cloud of points that's extracted
from the image and then
we look at these models as different
classes there are available in the
database and the similarity function or
the decision function is just basing
decision rule so whenever we get a new
query we do this feature composition and
we compute the posterior probability of
the query and there all these models and
that gives us a ranking for the images
in the database so this is the you know
basic retrieval query by a visual
example type of operation at around 2000
this was the state of the art or you
know Stevens like these were the state
of the art in the field so here's an you
know an example evaluation in terms of
precision recall but the details of
these are not that important what's
important is that you know it's with a
system like this it is possible to make
image retrieval work but only for
certain types of queries and those are
the queries where similarity similarity
literally means visually simple
similarity so let me give you some
examples of this here we have some
results queries once again is acquired
by visual example paradigm these are
like four queries each row presents the
top matches to each of the queries in
the database the database is about 2,000
images they're very diverse as you can
see you know throughout these queries
and once again so the results are good
when there's like a strong degree of you
know visual similarity between the query
image and the images in the database and
and notice that the system is kind of
relatively good at finding what are the
dimensions of this feature space along
which similarity is strong so in this
case it does it's mostly the fact that
these images have similar colors for
this one you see that the colors are
actually different but it locks on the
fact that all these flowers have kind of
petals that have the same sort of shape
and you know so forth so the basic idea
is that you know this can actually work
fairly well if there's a you know a
direct match between you know the query
image that the appearance of a query
image and the appearance of the image is
that one is trying to find however in
practice visual similarity there's not
always correlate well with semantic
similarity so here's an example of a
query where the system fails so this is
an example of a query image that has
airplanes on a airfield and these are
the top four matches that result from
this query by visual example operation
and if you if you look at this images if
in terms of strict visual similarity
they're actually reasonable matches so
all these images have some amount of sky
in the back like the query they have a
sort of like geometric structure that
looks like a plane and the perspective
in the foreground once again like the
query image but the fact is that for
people these don't really look like very
similar to that image and in fact they
look a lot similar than these images
here and notice that what makes this
image is similar to that one is not the
fact that they have sky or there's a
ground plane but as the fact that
there's these little things here called
airplanes that you know we say okay this
is an image that airplanes it's these
are images that I have airplanes and you
know that happens even though these
airplanes are just you know a little
fraction of the image they don't really
look like much like those ones these
ones are yellow those ones are white the
wings are different they have different
shapes here there are three over there
we only have one so you know it's a very
different type of similarity which does
not is not you know correlated
necessarily with the fact that the
images look the same from a visual
streak visual standpoint and so that led
to this idea of semantic
Tribble into which the field as evolved
and the basic idea is that first you
learn to annotate the images so given an
image like this you would annotate it
with the keywords such as you know this
this image contains airplanes on a hair
field and then you would do the match
directly at the annotation level so you
would look for other images that
contains airplanes on airfields and
that's the basic formulation so we want
to search images are completions of
somatic visual concepts and we want to
search for similarities in terms of
those visual concepts the starting point
for a semantic image retrieval system is
some image database which is annotated
with captions that are drawn from some
finite vocabulary and when it were each
of this captions it consists of a number
of key words and each key word
corresponds to a visual concept so
typically you will have an image like
this and a caption that's something like
people beach sand palm trees huts and
vacation now this is not a very standard
machine learning problem in the sense
that this labeling is pretty weak so
it's still a supervised learning problem
in the sense that you know for each
image we get a caption but we never
explicitly told told what is the
correspondence between captions and
capital captions and pixels in the image
so I know that there's people in this
image but I don't really know that you
know these are the people pixels I only
know that there are some pixels
somewhere in there that are related to
this concept of people and furthermore
the captions are not necessarily
exhaustive it's very it's fairly hard to
come up with a to predict what out where
are all the words that could possibly be
relevant to label a given image for
example here you know one could say that
this is this image contains bathing
suits or something like that but you
know somehow the person that I was
annotating are levelling the image
didn't think that was relevant
so the point is that we have to be able
to kind of deal with that all that the
amount of noise the goals
well the main goal is to learn the
mapping between these visual features
and keywords and we want to do that in
order to be able to solve two problems
the first one is what's called semantic
annotation so if I know if I if I know
this mapping between words and features
well given a new image I can extract
features from it and then I can find the
best caption for that image so I'd like
the system when given an image like this
to say something like people beach then
both trees and rainbow or something like
that
and then the second problem is that of
semantic retrieval
okay well given that we now annotate all
these images then we can support
retrieval by just matching this cue
words if I want to collect you know look
at a bunch of images like this I can
just say show me images I have people on
a beach and then hopefully the system
will be able to recognize - to find the
images that contain those visual
concepts now from you know mathematical
point of view both of these problems can
be formulated as problems in statistical
decision theory we have some data
representation in this case images are
just collections of feature vectors
sampled from some stochastic process X
and the key words are just and the
captions are just collections of labels
sampled from a random variable W that's
defined on our vocabulary and we also
know what I you know the optimal
decision functions in this minimum
probability of error sense both for
annotation and retrieval so if we want
to annotate an image so we're given
basically a bag of feature vectors
extracted for the image and all we have
to do is to find the concept that has
the largest posterior probability given
at those features
the other hand if we're given a concept
on a retrieval operation we want to find
what is the image that best satisfies
that the query for that concept well all
we have to find is the bag of feature
vectors or image which has the highest
posterior probability given the concept
so mathematically this is not very
different from you know the visual
similarity type of problems that we were
looking at before but the problems of
course is in learning this relationship
between words and visual features so we
need to estimate these conditional
probabilities from W to X and that's
problematically in particular because we
do not have Quinn training Veda so it
would be relatively easy I mean nothing
is ever really easy in computer vision
may be relatively easy if you had a
training set or like all the for example
if you're doing face detection usually
you've got a training set that consists
only or very nicely crop pictures of
faces but here our annotations are only
causal so once again I know that these
images of people and it contains some
feature vectors that are related to
people but it also contains a lot of
feature vectors that are unrelated so so
in essence this is a problem of learning
with lots and lots of noise so basically
it's a problem even our positive
examples are kind of quite noisy and you
know some people have looked at this in
machine learning and the framework that
has been advanced to deal with these
sorts of problems is what's called
multiple instance learning the best way
of thinking about multiple ways of
learning to me is through an example so
we can think of each image as a box of
chips like you would have in a casino so
each of these little feature vectors you
can think of it as a chip that's drawn
from a certain concept and concepts are
really just cells on this probability on
this feature space in which these
feature vectors are defined so basically
a concept the
find some region in the future space and
then this feature vectors are sample
from that cell so let's assume for
example that all we're trying to do is
to learn about image colors so an
extremely simplistic simplistic problem
so the concepts are pitch image colors
and the chips are the image pixels so
here we don't have any features to
simplify the matter but only that we're
only really trying to look trying to
learn pixel pixel colors like green or
red or so forth now in this case the
concepts are just you know the the cells
associated with certain colors in color
space so you know the cell of green
colors the cell of red colors and so
forth and let's consider for simplicity
that each image only has two colors so
here this image has a lot of pixels that
are at so it has a big stack of red
chips and it has a smaller stack of
green chips and suppose that we want to
learn you know what is the probability
distribution for green from from the
disruption station obviously from you
know one image is very hard because most
of the stuff is not actually that color
there's a lot of stuff that it's
relevant this big stuff this big stack
of red chips but the idea behind
multiple instance learning is that if we
collect an you know a large number of
images and the background stuff in this
case the the red background is actually
random meaning that in different images
it's sampled from different processes so
by assembling a very large collection of
images we can actually estimate the
probability mass for the concept of
interest and you can see that so if we
have a collection of images where these
little squares are always green and then
there's like you know background that is
drawn from some other color even though
in each image we only have a little
stack of green chips as we put together
this you know big ensemble of chips the
green pile is at some point going to
dominate and the background on the
background we're going to have basically
a uniform distribution over all the
other colors and so that's the basic
idea the basic idea is now for visual
concepts if we have a large collection
of images that have a certain visual
concept and then you know randomly drawn
backgrounds then if we collect you know
a big set of images a large set of
images from that particular concept then
and we feed them a probability density
estimate or a probability model to the
whole and sample of images then at some
point it will converge to the
probability distribution of the concept
of interest plus some background
distribution which because it's
uniformly spread throughout the space
typically has a small amplitude and it
becomes and and the concept the
probability of the concept of interest
becomes dominant so that's quite
interesting and it's a it's in thing in
part because it's very compatible with
the sorts of things that we were doing
before
for visual matching so it turns out that
if these are hold then all that we have
to do for example to learn probability
distribution for the concept of a
mountain or at least to get together a
large collection of images that contain
mountains on them and then you know
computer features dump all the features
into a big training set and fit a
probability distribution to this concept
of mountain and so since we already were
computing individual mixture models for
the different images it turns out that
this can it can be done very efficiently
so there are methods for Arachne
estimating the density
all over the entire grouping from the
individual densities so we don't even
have to go back to the feature vectors
that extracted from the images and so
this process is so efficient that in
fact this is what we were doing before
for image indexing basically given these
images we were just grouping them
randomly and we are creating sort of a
tree structure of probability densities
that then we could search very
efficiently all that we have to change
here is to change this grouping so that
they're you know satisfy these labels
they are compliant with these labels
that are associated with these images
and so once again this is kind of a
simple method but it turns out to
actually work you know fairly well in
fact you know better than like all the
things that have been proposed in this
literature so far so over the past two
years we've actually done a very
extensive evaluation of this which will
come out in a pammi paper soon and
across multiple sets so one of the sets
that's commonly used in this literature
is a set of images from chorale which
has been annotated so that's about five
thousand images they have been annotated
with a total of 374 concepts
each image as one two five labels and
here is a plot of some of the measures
that we use so there's of course
multiple measures that I use both for
image annotation and for image retrieval
I'll not go in detail over all the you
know accidental evaluation here but I'll
just give you I just wanted to give you
a sense for the evolution that has been
happening in this area over the last you
know five years or so so these are here
is a performance so these are annotation
results
there's 260 words in the test set and
this is a plot of the number of words
that have recall greater than zero and
that means that the word appears as one
of the top
five labels given to the image and so
you know you can see that the earlier
systems of farmers was pretty bad today
we have more acceptable results so like
I said out of the 260 words we can do
about 137 which is of course not perfect
but you know there's certainly progress
going on in this area as the plot shows
and 5,000 images is kind of small
there's also a data set that has 60,000
images that was put together at Penn
State by James Wong this one is actually
Zenon are challenging it has 442 concept
the images are annotated in a noisy way
meaning that the fact that the image is
annotated with a label does not
necessarily mean that it has that label
and it turns out that as long as that
noise is not that bad so you know you
don't have more than 1 out of 5 noisy
labels then the performance you know
does not change that much so these
systems are you know relatively robust
to this sort of thing ok but so we
certainly made some progress in semantic
retrieval you know in some sense this is
like the stuff that I talked about last
year so I want to you know do a lot and
on this again ever since then we've been
trying to think a little bit about what
are the implications of these for image
retrieval and in particular the question
of you know did we make any progress
that this did we do we have any benefits
of moving from a query by visual example
system to one of these semantic image
retrieval systems and one can say yeah
you know symmetry was a lot better
because we are now dealing with concepts
and people want concepts but the truth
is there that there are advantages and
disadvantages
the largest advantage of semantic
retrieval is that it works at a higher
level of abstraction for example because
if I want to learn the concept sky
because a semantic image retrieval
system you know learns from a collection
of sky images it can learn the fact that
sky is sometimes blue and sometimes it's
orange if you have matching strictly by
visual similarity and you give a system
an image like this the system is very
little you know we which to work with
and it's very hard for the system to
realize that oh yes this is sky and by
the way some sky is also sometimes blue
and and make that sort of generalization
so there's certainly an advantage for
semantic retrieval in terms of working
at this higher level of abstraction on
the other hand there are problems like
multiple semantic interpretations
whenever we try to label images it's
kind of a tricky business if you show an
image like this to a number of people to
like five people you're probably gonna
get five answers some will say it's a
people-people Overlake others might say
it's a picture of a boat with some
people in it as myself is a picture of
fishing and others might just say it's a
picture of people on vacation having fun
or whatever so it's it certainly
increases the complexity of the problem
and it's actually the issue of the
limited vocabulary okay what if I want
to find images of fishing but the system
does not know about that keyword what do
I do
quite by visual example is unrestricted
by vocabulary it doesn't have any so if
I want to find images of fishing I I can
just provide the system with an image of
some people fishing and you know if
there are other images that have boats
and people standing on them and large
masses of ocean it's probably going to
find those images fairly well and so you
know it generalizes better in that sense
and also of course there's no problem
with these multiple interpretations
because it does not really try to
interpret the images on the other hand
we've already know seen that you know
visual similarity is kind of weak
we correlated with the notions that
people have when they you know evaluate
similarity and so that's that's
problematic so that kind of thinking
into can we somehow draw from the visual
matching experience in order to try to
improve the performance of this semantic
retrieval system and the main problem if
you think about all these problems of
semantic retrieval are due to the fact
that we specified queries in this very
limited form way where we we just want
to type you know even give me an image
that has water well that's the same as
specifying a vector of probabilities for
all these concepts that the system knows
about where water gets probability one
and then everything else gets probably
zero and that ignores this like rich
description that internally the system
has for the images and in fact that is
the sort of the problems for example an
image like this it's probably labeled
with the system by the system as having
some probability of being about water
some probability of being about a lake
about people about voting and so forth
and so all because all these
probabilities have to add up to one all
these possible competitive competing
explanations kind of reduce the
probability of water and so that makes
the image a poor match to that query
which does not seem to make any sense
and also you know by specifying question
this way we're basically just restricted
to the words that the system knows about
or of course you know you can use text
processing tricks and kind of try to
expand a little bit but at the end of
the day you're kind of always specifying
the question as the presence or absence
of some concepts and problems like I
mean a lot of vocabulary generalization
and things like that are difficult to
deal with so it seems that you know the
problem is that the system is ignoring
this rich description of which is the
Cystic image that the system contains
and so it seems thing to look at some of
these probabilistic vectors or these
vectors of crowd
ability vectors of probabilities which
with the system annotate image so so
this is what's shown in this slide for
an example image
so basically image we extract the
features from that from it and then we
compute the posterior probabilities
under each of our models things like sky
mountains clouds and so forth and then
this is basically a multinomial
distribution and so we refer to these
there's semantic multinomial for short
and so here's the semantic multinomial
associated with this image and as you
can see for example for this image it
kind of assigned it doesn't really say
specifically what image this is what
type of image this is but it excites a
probability of even having trees and
having houses and gardens and then you
know some other things that might not
even make sense like mountains or grass
which in this case is okay but typically
you get like some probabilities which
make sense and which tend to dominate
and then you get like a you know a
distribution of probabilities over you
know the the concepts that the system
knows about sometimes those
probabilities are just noise like
mountain here which is unrelated to this
but sometimes there can very important
information because you know if you have
houses then many times you have grass
nearby and that might be like a good
clue with which to match house pictures
very briefly another example so this is
year just for mountain and park as you
can see sometimes you know it makes
errors like it thinks that this thing
there's this big blue patch here is due
to that it labels the the image with
some probability of being about oceans
so these probabilities are certainly
noisy it's not like we can ever you know
we will ever be able to come up with a
perfect labeling system that can you
know give us a probability distribution
which makes absolute sense but
altogether there is a significant amount
of information which is useful and just
trying to restrict
that to a single label or a couple of
labels it's usually not a good idea so
this is just an alternative you know
form of retrieval once again a simple
idea which is what we call query by
semantic example and here instead of
words as is usually done in the routier
paradigm the user we go back to the
having the user provide an example image
that image is then labeled with this
semantic multinomial so with the
posterior probabilities of all the
concepts that the system knows about and
then these semantic multinomial czar
then matched this manual phenomena is
then matched to the ones that were
previously extracted from the database
and so if you if your information
retrieval guy this is similar to query
expansion but it's kind of a funny sort
of type of query expansion where we're
basically using an image and then the
image gets expanded as a vector of
probability along all the semantic
concepts that the system knows about so
what are the advantages of doing this
well as an extension of standard
retrieval now instead of just giving a
couple of keywords we give a
specification as a complete probability
distribution over the set of concepts
that the system knows about and this
basically eliminates the two main
problems that we were facing first of
all if this image is as multiple
interpretations well the system will
assign those multiple interpretations to
it internally you will represent this
image it's some probability of being
lake water people sky boats and whatever
so we don't really we're not really
nailing down what the query is we kind
of just giving the system an example and
then we're letting the system figure out
in its own internal representation what
are the possible interpretations that
this images and also it allows us to
deal with the semantic space you know
outside of the semantic space issues or
out of
February works so I know I'm no longer
saying that I want the image of fishing
I'm just showing the system and image of
fishing and I'm letting once again the
system translate that to whatever
internal description it has and so
because of that you know we have much
better ability to deal with these two
problems when compared to the query by
visual example that you know we were
doing initially well it's the same
paradigm so it has all the advantages of
query by example but on the other hand
we're now dealing with a semantic
feature space so it's like we were doing
the matching operation but now at a
space that has a higher level of
abstraction and for example where for
example you know sky can be both blue
and orange and so that has typically
better generalization so it's actually
interesting to look at this semantic
space so basically what we're doing is
the space is the simplex of posterior
complex probabilities and each image is
basically represented as a point on the
simplex and because we know how to
measure distances on probabilities
simplex is even operations like
measuring similarity and so forth are
kind of trivial and there's you know
many well-known solutions for this so we
tend to use the kullbackleibler
divergence as our measure of similarity
in the simplex but you know whatever are
the measure of distance between
probability distributions could be used
as well okay so the main question then
is generalization so I'll do this
systems generalize out are they able to
deal with these types of problems that
I've mentioned and there are two types
of generalization problems or there are
two types of classes of queries on one
hand we have what we call class queries
that are inside of the semantic space
and the other hand we have queries that
are outside of semantic space so what
are these so if I have a system that you
know it's been trained to know about
mountains cars and so forth if the query
image is an image that contains
mountains then that's basically inside
the semantics
is the concepts of interest are concepts
of the system notes if the image is an
image of fishing and I never train the
system to recognize fishing images then
you know it's outside of the semantics
place and so in terms of the
generalization we already saw that we're
by visual example doesn't care about
this it has no notion of spaces semantic
retrieval only works inside this
semantic space and the question is how
well does that this query by semantic
example do both inside and outside and
so that's something that we've been
looking at over the past six months and
here are some results in terms of
performance so let me show me first
performance inside of the semantic space
so once again we're using this set the
corral 50 set so that of course a
training set and the test set but the
images on from from both our images of
the same concepts and so for that reason
we say that this is inside of the
semantic space so the queries are inside
of the semantic space and here's the
curve of the precision recall for the
system and as you can see so in blue we
have the performance of query by
semantic example in green the
performance of query by visual example
and as you can see there's you know a
significant improvement of precision at
all levels of recall so this was more or
less expected and this just reflects you
know the fact that this semantic
representation has higher level
abstraction you know the fact that it
knows that sky can be blue or orange so
that's that was expected more
interesting is the performance outside
of the semantic space and here we once
again training the system on this corral
50 set but both the retrieval database
that the database that search and the
query database the derivative queries
are images that have concepts which are
not in this training set
and we've obtained this datasets in one
case by you know downloading images from
Flickr and in another case by you know
looking up some further classes of Corel
which were not used to train the
original system and so here's the
performance of the first experiments
that we did in this area and the results
were a little bit disappointing so as
you can see here see the performance
does not change that much from one
training set to the other but basically
the conclusion is that outside of the
semantic space there's no advantage for
doing for using the semantic
representation and so this is kind of
not very exciting because you know okay
we know that there's some advantage
we've seen there's some advantage inside
of the space but most of the queries are
probably going to be outside of the
space anyway so this was a bit
problematic so that led us to look more
closely at these labelings that the
system produces and and here is a
typical example from a class outside of
the space so this is a class of
commercial construction that the system
does not know about and it's it's fairly
typical in terms of what happens so what
happened is that this probability
estimates are are noisy like I said if
you look across the set of these images
the correct labels are the best labels
for these images they kind of conceal it
consistently appear among the top 10 or
15 labels which image so in this case
you have things like people building
streets and even tables you know it's
kind of a these guys tend to be sitting
on things that look like tables so it's
kind of a decent label for this image
however when you look at each image
individually there's the correct labels
do not always appear at the top and if
you look at any particular image there's
the probability of a
random label just creeping in is not
irrelevant so you get things like water
boats three restaurants sky and things
like that that creep in but those creep
in more or less randomly okay and so one
can think of this again as a problem of
multiple instance learning so what you
have now is a collection of semantic
multinomial distributions where part of
the distribution front of the
probability is the probability of the
concept of interest in this little
diagram the green and blue bars but then
there's a bunch of probabilities that
are just appear and they're basically
just appear randomly and so you know
from this a principle of multi pollicis
learning it should be the case that if
we collected a large number of this and
we computed the overall probability over
the whole in sample this thing should
converge once again to the probability
distribution of the concepts of interest
plus you know some uniform distribution
that's just roughly equal for all other
concepts now it turns out that for
multinomial this sort of operation is
extremely simple to do all you have to
do is average them so it's literally
just like you know a nice cleaning type
of operation you just have to compute
the mean semantic multinomial and so for
example in this image construction
example if you do this for all their
four or five images that I showed you
you see that like people becomes like
very predominant as the main label for
this image then there's buildings street
there's kind of this bogus label with
statue then tables and then there's a
bunch of other things that appear they
have much smaller probability so you
know so there's a total of 371 concepts
so you know this this is really just the
top ten annotation
well that's that's that's another
interesting okay so the question is why
is there restaurant there and that's
another okay let's save that question
for the end because that's something
that I want to talk about actually
because it has to do with the fact that
this a lot of time thus you know
contextual sort of Association so here
for this image and you know if you look
at the labeled table is actually not
that bad of a label because these people
are you know standing on you know they
may not be called tables but they look
like tables and and to see what the
system has learned is that when you have
an image that has a lot of tables then
there's this you know a significant
probability that it's about restaurants
as well okay and that sometimes that can
actually be very beneficial and
sometimes that's actually what allows us
to do these operations but I'll talk a
little bit more about that at the okay
so the basic idea then is let because
this is easy to do it's just averaging
these semantic multinomial that's just
instead of using one single image let's
use you know a number a query composed
of multiple images so we collect these
images we compute their semantic
multinomial we compute the query
semantic multinomial which is just the
average of this and then we use that to
search the local database okay and so it
turns out that when we do this we have a
very significant gain so here are
numbers of the mean average precision as
a function of the number of great images
and once again so here in blue is square
by somatic example and in rarely square
by visual example you can also do query
averaging for images and visual matching
so you know it's you can try it in that
domain as well it turns out that as you
can see for this semantic representation
there's an increase up to about seven or
eight images and then the performance is
more or less stable well for a visual
example we don't really see any
improvement
and in fact the performance tends to
decrease as one increases the number of
examples if you look in more detail one
can see that this improvement it happens
at all levels of precision or the
improvement in precision happens at all
level of levels of recall and typically
two or three images in the query are
enough to make up most of the gain and
furthermore these observations are quite
consistent across data sets so here are
results so these are here are plots of
precision recall inside of the semantic
space and on the two data sets outside
of the semantic space and as you can see
so the comparison to query by visual
example is shown here in green acquired
by semantic example in blue and again is
significant these are tables of mean
average precision and you can see that
the increase goes from 30% to a hundred
percent meaning that the mean average
precision actually doubles yes so this
we did like the best performance that we
could get I think you like up to five
images so this is the best precision
recall curve in both cases using up to
five images
now see an example you don't see your
example is this okay you don't get any
there's no difference between inside and
outside of this it's the outside of the
semantic space you don't get any
advantage okay so this is like just mean
average precision and these are this is
the same but you're showing a precision
recall curve for the you know the top
performance now this is inside so this
is inside here and this is outside okay
so the question was if these plots are
this kind of a if these plots are inside
or outside of the space this one this
one this one I'm actually not sure I
think it's probably outside but I'm not
sure I have to look it up but but you
know the basic idea is that it doesn't
make we don't see any change it any
differences that's that's the you know
the one of the most interesting
conclusions as you can see I mean these
plots are pretty similar you know across
data set this is Flickr this is Corel
this is Corel inside of the semantic
space and and and they're very similar
okay so you can ask the question is it
really the semantic structure that's you
know leading to this improvement and and
one of the nice properties of this
framework is that it allows us to
explicitly ask that question because
remember at the end of the day we're
just using this probability
distributions the representation for a
query by visual example and query by
semantic example is basically the same
and we can test what if we completely
mess up the semantic structure by
estimating these so called semantic
models from groups of random images
okay so let's disrupt the semantic
structure and what you see is that so
that's shown in red here the performance
drops are quite significantly and it's
inferior to the performance of query by
visual example so there's certainly
again there which is significant we due
to the fact that we're doing this
representation at the semantic
level okay so that's you know the bulk
of the message there's this question
left which okay there's this question
left which is how do we get this query
images and that's always a question for
query by example and we're just starting
to work with this but because now the
examples are semantic examples we should
be able to kind of be you know find with
examples by just using a regular search
engine like like Google search so here
is any you know the system or building
so there's you know we use Google search
to kind of find a collection of image so
suppose you want to find pictures of
your you know a wedding that you have
attended you know next to the Eiffel
Tower or whatever so we use Google
search engine to find pictures that have
the Eiffel Tower on them then some
there's some intermediate stage that can
I get rid of cartoons and things like
that
graphics and things like that and then
we use these images to query our local
database okay so from the point of view
of the user this is once again just
semantic retrieval so the user doesn't
really have to you know worry that much
about gathering examples which is always
a hard task because basically the search
engine is kind of translating this
textual query into a collection of
images that then we can use to search
our system using this query by semantic
example paradigm and so you know very
this is very recent thing my students
put together last week but we're
literally like starting to build this
and these are some images that if you go
to the Google web page they might still
be the top matches for nesting birds and
so we fed those images to our search
engine and you know our internal
database and that's the stuff that that
comes up so this is a basic idea of you
know using like Google as a translation
engine between like the you know the the
hi
representation at which people like to
work which is you know text or textual
queries and you know a local database
which is not annotated you know what is
not manually annotated but so the basic
idea is you know using query images
themselves as queries to search the
local database okay so somehow it looks
like I lost my my thing there so I just
wanted to finish anyway by with the
conclusions so you know the basic idea I
think is that there's been some
interesting developments in term in this
in this area of image search we
certainly have not solved the problem
yet but I think that this kind of gives
an idea of the progress that has been
happening we I started by talking about
this progression from you know sketch
based interfaces which are not that
useful so then you know visual examples
semantics and now we're proposing this
notion of semantics examples query by
semantic example I'm you know I think it
can be a pretty powerful when combined
with this idea of multiple instance
learning one theme that has you know
emerge I haven't really talked about
that much thing to talk but as if you
know we've consistently seen in our work
is that typically the simple models
using simple models learn from lots of
data and you know and and careful
parameter tuning and things like that
tend to work a lot better than you know
very complicated probabilistic graphical
models and things like that so we've
we've all of our work as you can see is
based on a very simple representation
there's this mixture models of local
features and every time we try something
more complicated the performance
actually drops so that's kind of an
instinct you know observation I guess
and I think there's one can make a case
that there's an intrinsic advantage for
using semantic representations it as you
can see this is a direct comparison the
visual representation is exactly the
same it's just these mixture models of
features and the only difference between
that can explain the performance of
these systems is the fact that we have
these semantic groupings so it certainly
seems to be like an interesting area to
explore further and in my view I mean in
terms of a bigger sort of philosophical
point I think it also points out for a
different way of thinking of computer
vision and what's typically done so
basically here we don't really have very
good classifiers you know we have you
know lots and lots of reasonable
classifiers and then certainly by moving
on to the analysis that in this higher
level semantic space we we're certainly
seeing improvements over whatever we we
have tried along the lines of using
better models to represent the you know
the visual appearance of the images and
so it's certainly something to think
about
that most of the emphasis on vision is
towards building better models for you
know appearance and invariance and and
so forth and and and and you know these
results our representation is quite
simple by the sense of what's being
tried today in the literature and and
like I said we haven't seen big
improvements in in trying more
complicated things</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>