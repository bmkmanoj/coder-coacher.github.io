<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>ZuriHac 2015 - Better Faster Binary Serialization | Coder Coacher - Coaching Coders</title><meta content="ZuriHac 2015 - Better Faster Binary Serialization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>ZuriHac 2015 - Better Faster Binary Serialization</b></h2><h5 class="post__date">2015-07-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/60gUaOuZZsE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay folks again so we now we have our
first of two bigger presentations done
by Duncan probably everyone knows him
Duncan take it away thank you much and
yes fantasies original here um and
everybody help me okay and see the
screen okay I realize it's very big room
and I'm talking to sort of both ends I I
want to make this a kind of the
interactive i want i want questions
always sure so if you're in the far
corners of the reason you're welcome to
either Chantry loudly to interrupt me
with a question or come closer than ever
before so right let's talk is about
binary serialization but it's really
about optimization so it's basically
supposed to be okay study I i I'm not
I'm not just strength tell me how
awesome mr. libraries although it is
awesome but especially about
optimization techniques that are
hopefully transferable drawn libraries
other other places hopefully but because
it's a case study necessarily i have to
understand the details about what this
library does you can't just present
these sort of things in abstract so i
will have to start with a bunch of stuff
about what is all this why under
civilization we're doing so you'll
understand what the code is what the
people sleeping like and how we're
trying to optimize things and so just
quick introduction to the particular
page to find your civilization rotate
what we mean by binding civilization
here and so when you do with binary data
there's really sort of two major these
cases what one is you are dealing with
some externally defined binary format
you need to parse x.509 certificates or
something and the other is ein hasn't
data in hospital I want to sterilize it
send it over the network or dumb people
desk of retired later we're dealing in
this library with the latter case
sterilizing stuff not with the parsing
extended find format now unfortunately
binary serial packages to both
which is the fault of myself and Don
Stewart when we first started we realize
distinction but now G hello so this new
library the short term goal is to
implement civilization so player or redo
the serialization they are quite enough
winery library ultimately like to
integrate it and the other
co-conspirators maintaining the bhangra
library are mostly on borderless idea
the main things it's not much better
solarization format but the thing I'll
be concentrating on this topic is the
performance and it is a lot faster which
is nice terribly likes faster by of
civilization so the briefly the format
is C bore it has recently defined RFC
you can think of it if you've not come
across it before you can think of it as
Jason Bay in binary a compact finally
encoding of adjacent leg if you've come
across map it's very much like that just
slightly better design home side it's
much more compact the existing binary
surreal formats which were terribly
designed by again myself and on and it's
where is well suited to to Haskell's
between my data structures so it's a
reasonable choice if you want to dispute
now with me come toward me later and
that's not a point to the store we're
gonna take take you for reading this as
a reasonable thing to do and see how
fast we can make it so just to start off
with these are the sorts of examples
this is what see what values look like
at least in their text or diagnostic
notation and they look much like Jason
there's some slight differences with
Jason but stop having done so we have in
particular we have lists and maps and
that's what gives us trees right that's
what we used to encode in and structure
and then how to genius of course like
just like a Jason so we're going to
include Haskell arbitrary user-defined
hospital types with this sort of
encoding we're going to each constructor
will be encoders as a seaboard list and
when we have multiple constructors the
tag we will we will use an attack an
integer to say which constructor is
fairly fairly simple this is actually
quite nice because it allows you to do
version
cuz it can increase the type mannerism
but that's that's where different
different so hopefully fanny clear
constructor examples and left and how
we're gonna improve them on the right
yep crushing him the tag numbers have to
match up that started relying on there
yeah yeah oh yeah sorry the question was
what if we dump the seaborne outer disk
and then go and change the numbers and
then try and bring back in again yeah
it's gonna fail well this this
particular way of translating hassle
constructors to see more this is relying
on the the tag numbers but it's be a
fair nicely right sleepy affair with an
arrow yeah in this case if it was the
number that was used somewhere else it
would say what's the wrong length or
when the fields doesn't match but if
they match like if the fields match so
if it's two strings fear but one of them
see Santa nukes there's a sense of that
santa present them then necklace yeah if
it so there's no marchiony so there is
no virtually the wrist version ii but it
relies on you on the tags they used in a
systematic way you could also do these
things with names and maps objects time
but this is the encoding that i'm using
ok so kentucky se borra value it's like
a Jason tree like value it consensually
when it sterilized its represented as a
sequence of tokens and those token to
the binary representation and that is
there for binary representation of the
flattened see what value so for example
here is a list 1 2 3 as tokens that's
list of length 3 followed by integer 1
into 2 into those are the tokens and
then each of those tokens has a binary
encoding and then the overall encoding
is
just a concatenation of all that that's
what gives us the the structure so the
strategy we're taking forward
sterilizing and deserialising is that
we're never going to build an
intermediate seaboard value like a
generic seaport value as a coordinate a
school algebra this time we're going to
go straight from a hassle value to see
what tokens and mr. times binary and
then straight from tokens into a score
value without an intermediate
representation so the sort of instances
that you're right we'll be in terms of
these flattened converting to and from
flattened sequences of see more tokens
and then we'll use a binary pirater and
printer to specialize to working with
those papers so this is in contrast to a
library like a song for Jason closing
where it always uses its intermediate
value type the value type in the ace of
library is the ordinary way in high
school with representation Eric Jason
term and with ace with ace on you
convert your hospital type into a value
and then value goes out to string and
vice versa comes back into a value and
you can go from value back into your own
nascar-type and there's good reasons and
does it that way it makes it into Jason
you have to deal with a problem the
fields can carry anymore joy there are
good reasons why it does that but it has
it has significant significant costs to
have this into the dips structure so a
salon is a good example of this not
vogue taking on it so as I said the very
good reasons why does it but the ground
are big costs to it so in particular
with it's on the intermediate structure
is often larger because it's
representative from generic way
everything is tagged basically so it's
often larger than the final structure
again with ace on it's necessary to
build in tight when you're deserializing
it's necessary to build the entire value
Jenny Jason value before converting
international type so you get this big
memory spike and the cost of allocating
all that and the fact that it's resident
in memory
it's just a bit cut so a real life
example that we had in our project we
were doing the ID back end which is the
thing that sits behind a bfp complete
muscle center that that has to talk to
external gypsy process and it sends big
trees back and forth and we were using
initially a song sending Jason and
values of the pipes and the memory use
was enormous spikes and we switch that
to put to do binary and it went away
that's not because it was binary it's
not because it was you know linearly
faster or linearly using less memory
it's because well I suppose really
though it is linearly less memory
instead of DC rising to the intermediate
structure it goes straight to final the
final structure so isn't this big arisen
this bitch like so that's like the first
your transferable lesson is if you can
avoid it demet intermediate structure
particularly fast up here if the way you
would do it would be to allocate all the
ones that has have them costs and so
here's a slightly more concrete example
I'm getting closer to what what the sort
of code we would use for these instances
would look like for for serializing
we're using a mono lead we have bunch of
functions which give us these tokens
list length word integer string etc and
we just I'm append them and that's the
result and so we do that for each
constructor so if you see the the
example the top again constructive one
has two integers so we're going to
encode it as a list of length 3 with a
tag of 0 and number two body parts and
then similarly 4d civilizing living to
insightly the length of the other list
then look at this tag case analyzed on
tagged depending on which time we got
then we expect further the body parts
and we assert that the length is right
so that's the kind of thing we're going
to expect to see and then expose before
it's a mono lead for sterilizing
no need for G civilizing which is if
you're going to use the binary library
search it something that that's how they
manage so you'll have my word for
sterilizing than that my dad for a juice
arising ok let's look at Sir Isaac
performance first of all because it's a
bit easier on that something sterilizing
is a lot easier to civilize me so binary
in cereal are the existing established
practice in this area they are both
continuation based both for the bill
Romano eight and for the pi zero mad
they're both what we call shallow
embeddings they are directly functions
that all of the competitors it provides
or directly functions that manipulate
butters and pointers and sensitive and
both of these libraries these libraries
actually extremely similar to each other
these days there's almost no difference
sometimes one is faster in those using
exactly the same strategy these days and
they may have exactly the same binary
format as well they probably ought to
just arranged and to with both packages
they do really well on micro benchmarks
and they do quite badly on large-scale
real world lots of complicated
structures records lists kind of thing
you run into a real application if
they'd entity brought badging in fact
for including their only slightly faster
than a something that'll show you
somewhat graphs a minute which are
frankly rather embarrassing especially
as an author of love these packages and
so here here's what the Builder 108
looks like in binary you can buy the
package i think the serial one is
essentially identical its continuation
based and it is sort of state on top of
continuation is that there's a buffer
this being passed around and the these
functions work can i oh and they just
directly right by its into this buffer
and then they call the continuation to
do the next thing so it's always passing
these buffers wrong and the buffer is
contained on the foreign coins are a
bunch of sexist quite what's going on
record
so I said these we say now slow why why
are they slow is is very hard to get a
really good answer for I mean to come
and talk to other the binary library
authors I'll give you all sorts of
different guesses that we don't have a
really good explanation what we can say
is it generates a great deal of code is
this build 01 words and essentially what
you're doing is writing a code generator
when you do this you are writing these
Combinator's which act some you cite get
in line and then have to be combined and
have to produce sort of perfect code at
the new site and what you're really
doing is you're kind of it programming
the GHC in line or you have to deal with
the fact that the in libraries doing
things to your program you rely on it to
achieve the code that you want to
achieve there's all kinds of problems
with continuations there has to be
buffer checks and then in the resume
side of the buffer check there's a code
gets duplicated to do the same thing
again there's lots of buffer has been
allocated lots of other constructors
being allocated but I don't have a
really solid answers to this one thing
is what makes it so slow and partly the
reason reaching that's hard to do is
because on micro benchmarks it looks
great did he go and look at the core for
some of these micro benchmarks at least
the ones that are not obviously totally
floored so this is just showing how it
in particular the binary library varies
wildly depending on GT version the
serial authors have clearly been better
at and when when New Jersey versions
come out going and checking and tweaking
things so I don't think it's because
cereal is just more robust I think it's
because they've put more effort into
tweaking but if you go and look at the
cases like for example look at the
decoding of the binary case in 784
that's the one running fastest there if
you go look at that you'll see that it
has really very good code and if you
look at the ones where it's doing badly
there's something obviously going wrong
with generics
and you can tweet that and you would
make it as fast as it was in the best
case but this doesn't really tell you
because this doesn't tell us why it's
going badly in the large-scale places
because here it's not here it works
really not well this is another
benchmark of a much much bigger this is
actually deserialising a hundred
qualifiers kemal structures are big and
complicated it's a reasonably good
example of the kind of very listy tree
structure you get without any until
arrays and others have done something
it's all just punches and when yours and
winters so and on these big benchmarks
we claim that the code is bad but it's
hard to pin down why it's bad because
there's just so much code you can't
really figure out which bit why why it's
wrong because that and when you get when
you narrow it down to small benchmarks
it suddenly looks good again so we don't
really have we don't have a we haven't
pinned down a precise culprit apart from
the fact that it's really hard to write
this kind of code generator to level
programs okay so a yeah as I
accidentally skip for the slide um time
there is here here's the tape of this is
totally crazy and a couple I think at
this hackathon two years ago when it
maybe you had the crazy idea earlier
than that had it had this crazy idea to
radically different approach to the
sterilization problem which receives a
deep embedding and a description of
these token sequences and then and then
do a separate certain temperature step
that terms of the sequence of tokens
into the final binary data so the
instance is that you write in that case
would provide these constructors
describing the different sequences and
then the apt interpreter just rips
through those and rights rights suffer
and her lover now this is a crazy idea
which you wouldn't expect to work
because it introduces an intermediate
structure which has to be allocated so
it seems like this shouldn't make things
faster it seemed like a lot
works but actually makes it up tight and
faster which is which shop tests and
confused and embarrassed us yes yes
benchmark so here's an encoding shootout
this is again a large-scale sort of
real-world example were civilizing I
think 100 Kemal files when they're fully
in there in memory representation which
as I said is just pointers galore so the
embarrassing thing here is that show Lou
Reed show right is faster the Mason
binary of cereal which is just using
strings you know linked lists of
characters so if I Wheaton in this
before we would have been more
embarrassed before so the new code this
crazy idea that Simon hood is five times
faster than show and six times faster
than cereal which is quite shocking
really so let's let me show you how that
works so we have we have an explicit
this is the deep embedding of the token
stream so tokens can be a word with the
actual word followed by more takers it's
just a certain list but we're
specializing it to the kinds of taken to
the canal and then the encoding is just
a new type is just a standard d-list
different Celeste or in dome one way to
particular took your pic and it's am on
way that that combines by sort of
continuation styling and then including
us in the words just lapsed grats on the
other passing the tail warm to the next
again so now we can we can write down
these instances and very standard way or
in which of these functions just turns
into internet encoding and that that
bridget the generated code new is now
really really simple because it's just
allocatable bunch of instructors so
there's really nothing to do yeah
question again the side without with a
you're not a lot of questions
the ocean do you gain anything by you
mean ye tokens run and i Justi just
eliminates one interaction two min
directions one for the the word is
unboxed straight into the constructor
and we don't have to have an extra in
direction for the list element it's just
directly on the table you could probably
measurable my good one so then you have
to write an interpreter for this thing
and so although the comparables side the
generated code all these constructors is
really simple we can go to town on the
interpreter and we can go as low level
as we feel like in this case we are
using the kind of lowest level of the by
stringbuilder at home late and this
works really nicely you can you can look
at the core and go to some of the stand
of iterations that look the call and
fetch markers on the fiddle them and
then you can make an interpreter really
fast and so that's why this runs well
you have to apply zur to make it on fast
but it's still not entirely clear why
this approach is single trust you have
to really understand whitehead rapacious
a slow so ok something not good at all
what can we learn from this what we do
so the the sort of traditional i call it
the compiler approach to the performance
is it you you write your your Combinator
is using our shower embedding meaning
directly manipulating and functions
directly manipulating the stuff at hand
the Combinator's because you're doing
modular home military style combining
things style approach then obviously
those come later after defined
separately which means that at a
particular yeast site they have to be
combined and then fused in mind
optimized whatever whatever needs to
happen at the new society out to get the
good code that you want and you know
you're trying to achieve some kind of
you know there's some kind of assembly
you're trying to achieve for some kind
of low double c ml or st jean you're
trying to achieve and you're trying to
achieve this with near-perfect code by
fiddling with these
the definitions and inline rules of
these humble motives and work lying on
JC to inline things exactly right and do
everything exactly right unless you are
doing to stage programming you are it's
hardly that you're writing you normal
level one time code but you're also
having to think all the time about how
does the rules were in the in lining and
the unfolding and the optimizations so
it isn't kind of it is it's almost like
a compiler you are you are doing staged
programming but you don't have as much
control as if you were writing the
compiler directly directly the fighter
legend Roger you can do like here you
have to work with the kind of in my new
rule engine rewriting system term
rewriting that the jeweller's so okay
the alternative here is this interpreter
style and you have to pay upfront this
cost of allocating this intermediate
edge structure but then after paying
that cost there's a lot of other things
you can do you can try and now move as
much work as possible from the kind of
compiled side the instances you can try
and move as much work on to the
interpreter as possible and then into an
optimized the interpreter as hard as you
can and so okay so the approach there is
your combinators is a deep embedding I
they are dated constructors or unfold to
Jeff instructors and they just provide
some description of the of the thing and
there's all there's nothing to
optimizing new sites when you do that
which is really nice and so then your
interpreter or multiple interpreters of
course once you've got deep embedding
you can do that your own territories now
a single chunk of code and a single
chunk of code is a lot easier to
organize than this style where you have
to you if you're looking at the core to
see what it is if it looks okay when are
you enjoying the compiler style you have
to look at this isn't it example and you
have to look at any examples whereas
when you're writing an interpreter you
need to look at one piece of paper to
the interpreter so in some ways it's a
lot easier so you've got and you've also
got a bigger sort of total exit
when you're in lighting everything at
the new sites you have to be quite
careful to keep things simple because
that's all you can manage during the
complexities of making all work out
right whether when you're writing a
single chunk of code interpreter it
doesn't it's not pulling more figures
it's very much much simpler you can you
can go to town on tricks and load of
optimization tricks and clever data
structures and other things that you
would not dare to do in line with all
the cool sites and also you can use more
code because there's one interpreter
rather than everything they unfold it as
every call site okay so I talked about
encoding or civilization and we wanted a
slightly harder Tom perdy civilization
and can we can be applying the same
trick to this organization it does look
a bit harder printers are easier than
part of it and so we would require some
kind of description of the binary taken
wiser I have a question yes I don't
grasp the difference between bit shallow
encoding and deepen head cello and
better what's the difference yes yeah so
people people call the deep embedding
when you're Combinator's produce a
description of something meaning in
terms of data constructors its actual
data you can now look at and evaluate
and manipulative they intersect whereas
the shadow embedding is not that it's
when you your combinators director you
work with the stuff at hand so the
beauty charlaine weddings are more
associated with functions new placing
stuff as the implementation of a
Combinator or as deep embedding is more
associated with your commenters being
just data constructors okay so yeah we
would need a description we would into
the description which is different
wedding of our binary of Tekken fighter
if you wanted to try and approach like
this with with D civilization now we
don't have to just go for shallow or
deep we've actually nix the team we can
in bed one side the other we have a
mixed deep shallow what does that mean
in practice that means dance instructors
that contain functions where the
functions are generated specialized at
the yeast site and get embedded into
ejecta constructor so then your
interpreter walks over within a
structure and calls these functions at
appropriate points so that can give you
some of the advantages of both there are
some things which I just slow to
interpret particularly you're just
reading and writing little bits of
memory they're sort of things I have a
high interpreting overhead so there's
something it makes sense to to do a
shower tile and others in the deep style
so you can we can do both and we will do
that here so okay here's a here's a
first go and it's relatively simple it
says okay it's a standard continuation
moon out is our ID code 0 on top of the
continuation monad that produces a data
structure the destruction is this thing
called decode action that's our actual
concrete Debbie constructor which is the
description and you can see in the
consume token case it contains a
function and the decoder continuation
monad is what we use to help us build
that data structure so that let's write
it in a nice straight forward monadic
style than what it what it produces
underneath is this data structures
decode action data structure which
contains lots of functions a term so the
the consume token primitive is a
callback function you pass the
interpreter will pass it the token that
it's just read out of the input stream
and that function will return a new UD
production and interpreted around again
ok so here is a little bit of what the
interpreter looks like does exactly what
it says therefore when we want to
consume taken
check move on the face left will decode
the tape and add a template stream and
if we did get one we go round again we
call ke ke as our continuation we call
okay with the token that we got and drop
some drop stuff off in the stream so
that that's the sort of core of the fast
path if you like completely interpreter
and that's nice because it's quite small
and which is good and there are costs to
this calling this continuation as it
costs as well see the moment allocating
that continuation has a cost we're also
allocating token constructors here we're
not allocating this result token
resulting in streams on to provide fail
staff but we are allocating that
statically limited but we are allocating
the individual well that's relatively
easy to learn HD as we will see it's the
continuations more interesting ok so the
interpreters relatively well to be
straightforward and its costs are not
too high and this actually performs
rather well whoever it's interesting to
look at the allocations because if you
analyze the the benchmarks for the
existing binary cereal you see that they
do spend an awful lot of time doing GC
and if you play around with some of
these things if you try and reduce the
amount of stuff that hella Kate I found
from playing around with earlier
versions of this code that there was
more or less than kind of linear
relationship between the amount that I
allocated in total and how fast we were
eating so here are some series of size
of some charts of what's going on with
allocation in the existing finding
several libraries so this is this
example of here this benchmark is
decoding quite a large structure so we
are ultimately getting into memory about
half a gig of pointers and data
constructors and as you can see in this
graph the GC time is really dominant it
spent on us all this time doing juicy
home is all about 60-percent some people
centers in them so it to be it's a big
factor now this this graph is comparing
the total amount of data that gets
allocated during the run of this
benchmark versus the amount that ends up
in memory of the end so the maximum
residency that's when the whole half keg
or whatever has been d sterilized into
memories versus the amount that's being
allocated but it's just shining through
data that's allocated but then collected
by the college structure so you can see
in both minor incident with a massive
amount of stuff that's allocated that
gets garbage collected that's not part
of the final structure that's been
allocated and this seems like we all try
to do better we're allocating we just
saw but it's been 2 plus it's Angie City
and we're allocating vastly more than is
strictly necessary to bring that data
structure internet so we want to have to
do better allocate massive go faster the
question of you I'm measuring
homogeneous and this is just when
running the program with plus RT s dash
guess it just gives you the time and
spent it gives you the time spent in GC
time expensive mutator and the total
amount of allocation and the maximum
doesn't it that's all just on the
command line from + s joshs plus i vs
dodgers and oh this is the same decoding
benchmark but now instead of
accumulating all that data memory we're
deserializing it but immediately
throwing it away so we're doing the same
amount of mutated work we're still doing
still doing all that encoding but every
time because what we're doing here is DC
rising like 100 records or some than
that so after each of padishah as each
record we throw it away so in some sense
again same time to work but as you can
see in this graph the amount of juicy x
dramatically smaller and so therefore
the over one time this moment so again
this kind of gives us a hint that
perhaps if we could reduce the amount of
publication you could make it run the
whole lot faster in sometimes this
deserialising is really bad for the GC
because you know over your gradually
growing a bigger and bigger structure as
you 20 eyes and the GC is going to have
to traverse that probably several times
and the more you allocate while you're
doing the more times it's going to have
to traverse that entire structure so
that was my intuition is too we should
get pretty good improvement by doing
less allocation of role because will
simply have to traverse that structure
this the persistence structure it just
gets bigger and bigger and doesn't i
calculated you will have to traverse
that many fewer times if we simply
allocate less often and rungy see that's
that's the two I started pursuing this
approach of a las Prince eliminate all
the applications and I'll try and share
some of those techniques with you um
okay so here this is this is some
success from that I'm not going to do
that I had worked so here's the sort of
headline result new continuation based
and new stack based to two different
interpreter two different
implementations of the decoder and same
graph as before of the this one the
total allocation and the maximum
residency so we see with the two new
implementations the amount of allocation
doing is dramatically less and the
maxims residency is slightly less so we
can radically reduced amount of
allocation we're doing and what's better
it runs faster so we've massively we
slightly reduced the music of time but
we've significantly reduce the GC type
so this is pretty good it's running
about 20 times faster it's not as good
as I'd like but top on and interestingly
sort of slightly spoiling my headline
result is that the stack one which is
where I'm ridiculous because its own
sort of interesting and complicated the
stack one allocates much less than than
the continuation baseline which I showed
you the total second grade well go into
more details exactly yeah so it
allocates our best but unfortunately in
this and then in a time benchmark here
and that doesn't make it take less GC
time it takes
same or more juicy time despite the fact
it's doing much less allocation and I
don't yet really understands I'd like to
understand it does it hold onto dated
longer I don't know shouldn't do it's
just maintaining a peristyle insulin and
those are all things that will be needed
later on okay requires further
investigation why do I genuine attempt
sly disappointing but it's it is fast
hey there's a slight side note you with
this deep embedding approach you get
much much smaller object files then this
is an example this is a very large
module with lots and lots of types again
this is battle in fact cut out the
package descriptions the amount of code
for derived really showed but as a Songz
limping hostile version binary and
memories to tune of limitations at the
object code is much much much smaller
which is which is nice okay now my
fanatical pursuit of fewer allocations
okay but they here's another hopefully
Ria's a reusable lesson or Wesley look
for allocations and how do we check if
we're doing well or badly apart from
looking the final results and saying the
total of allocations of 20 how can we
look at the code and see that we're
doing allocations or a few other
features how could we look at kroger
this and spot that this is being
allocated and that something like a turd
with my years of knowing how juicy works
like I know that this will not with this
organic a term that wont but it's it's
much better to to go and look to have
something definitive to go and find out
so let's let's find out so the best
thing to do here is St g-code non-core
scg sdg is a special stylized low-level
form of before that's the final stage of
gypsies optimizer before it goes down to
see them and its stylized in such a way
that all allocations are explained
it and that's that's how we can find out
we can go look at that stuff and see is
it allocating and what's it elevated
where is it allocating stuff we expect
your stuff we don't expect so to get
that output you want to use compile your
module and then we say dump STG and this
DS suppress all reduces the amount of
clutter we don't we don't need types we
don't need there's all sorts of these we
don't lead in this case we just need to
see let in fact let it all the different
floor so in STG every allocation is
either elect or just a simple construct
ratification it's unfortunate quite as
consistent as it always been internet
but it is always at least a constructor
you can just search for the name of the
constructor social net but yeah it can
also carry out of that with just the
just the construction element in and
conversely all let's and these nodes of
the box type not those of my blog site
are also an allocation so what does it
construct allocation the clock in STG it
looks like this let s e-g is very low
level it's got lots of curly braces and
semicolons and slightly funny sometimes
this token work that is the name of the
constructor with a bang is just part of
st g syntax and that means it's
allocating one of those right here right
now and the square brackets again that's
part of the STG syntax that's not
because we're using a list here that is
the arguments that have been packed into
the dark instructor so that is
allocating a TK word containing an X
which is a loan unbox were in fact so
that's what that's what the constructor
negation of taking SG so you can search
for less if you see ones that if they
lists you can see well that's was doing
that's a constructor closures as well
they also get allocated and they look a
little bit more funny and their syntax
this lambda r the r is not a variable
that's just part of the syntax i do not
know what it means and i can't reboot
srt sensory that's okay
I got the so what what this thing this
is this is a continuation k which is a
function really this is lambda x the
square brackets X here that is the
argument to the function and then so we
are allocating a function that takes a
nexus argument and why replicating it
well we are looking for some reason but
the why do they have to allocate those
functions why doesn't work we just
played at the top level it's because
they are capturing some variable this
that's available in the environment and
that's why cast of allocated because the
value fabric was only known at that
location so here we are in this
particular example we're allocating a
closure okay lambda X something
something and we're passing that k well
we're packing that k into the field of a
consumed token data construction so that
that they're actually consumed taken is
also an application that's an example of
where it's just being called as if it
were a function it doesn't it's not
using the left bang syntax but it's also
an education of that constructor so okay
that's what that's what these those
continuations and again you can search
for them because you can search the net
and see if you've expected those
continuations or not okay so let's
eliminate easy ones I said earlier that
we were allocating these terms tokens if
we just specialize this a few times for
the common cases and just pass an unbox
where instead or and unbox into an
unboxed float double etc we can cover
all of those cases all the little
yelling and finite number of the
printers and then things like string
integer box anyway so just imagine okay
so that's that's relative
straightforward you just add more
special cases and extend the interpreter
okay that eliminates a few other visions
the closure ones are more tricky and
more interesting so if you have code
that we stop this at the top decode
standard matic syntax this this is our
continuation monad Mario that produces
this deep embedding so consumed token is
a something that unfolds to the use of
the consumed token data constructor then
what are we what will walk the SDG&amp;amp;E
look like it involves allocating to
pleasures and wives it when's it why's
that it's because t1 and t2 are free
variables in functions that have been
packed into digit constructors and so
those functions have to be allocated
because they because they capture these
three variables because they are
closures so because yeah if consumed
token so if you look at the the bottom
one say we've got claws one and claws
one is being passed as as the arguments
to the final consumed token we're
stuffing here a function into a into a
data constructor and that function grabs
captures variables in the local scope so
that function has to allocate it here so
that it could be passed to consume table
so that means basically every year in
this style a continuation going out that
produces a different value in this file
everybody but basically is going to be a
closure because everything afterwards is
to use that value probably the I suppose
you could if you consumed it immediately
maybe it wouldn't be free later on but
most the time it's going to be you're
going to get every single one of these
basically will turn into a closure
application so it's quite a lot of
closure allocations you know the
temperature is constantly going to be
switching back and forth between running
a little bit of code that allocates
nothing the next closure which goes back
from interpreter and then allocate some
of it every single one is going to be an
allocation and that's dissatisfying I
want to do battle so yeah I'm sort of
said this a loop this this this
particularly comes up this continuations
because the continuation is the last
thing you call therefore the
continuations always a free variable and
therefore it's always a pleasure that
has to allocate it this is the basic
story there and you can see that to go
and look at the
mr. G code you'll see that there's a que
continuation being passed in the top
they used somewhere down bottom there
for all the things in the middle have
had to do allocated because they capture
that K so we will get more secluded this
way so can we do anything about it well
I don't have any general technique it
depends on the area in this particular
case yes yes we can we can use a stack
so instead of writing the left hand side
do you know buying trying find and then
use use of variables in the band's
earlier well we'll use a sack and we'll
say push the thing onto the stack and
then push laughing of the stack and then
locally modify also the top of in order
the stack and that that stack altering
function is now closed it doesn't
capture anything for this environment
it's a local Lander function that
function could be defined at the top
level it's it doesn't capture anything
from the environment therefore it
doesn't have delegated bra back to that
idea holds some promise and then that
means of course at the interpreter
Cassidy below because the interpreter
has to manage this manage this tank and
but just using a sack isn't enough here
we also need to deal with this
continuation argument if we're using
this continuation style and in fact
we'll have to abandon it because there's
nothing addictive if every if our
functions if all our functions are
instances are in the style of decoder
which is this continuation monad then
basically every single one of our
compiled decode functions it's going to
be of the form lander Kelly blah blah
blah blah blah blah blah okay and so
it's always going to be capture and
Katie and always been to delegate
enclosure to just because of cake so if
you want to go to that we have to
abandon this this continuation cycle we
have to as I said out the functions must
be close suddenly as if so they could be
written the top level and then they
won't have to be allocated
so yeah that means we have to stop using
this continuation approach which means
going to be something else to sequence
of stuff so we can use a deep embedding
of sequencing which is not the sort of
thing people normally advise people
especially since it doesn't enforce
something people like to tell you what
if you do a shallow way you guarantee
your men as well as and stuff if you do
it the deep way you can't guarantee it
but we're going to do it anyway so we're
going to have a deep embedding of
sequencing with unit done sequence do
want the first thing and I do the second
thing and this approach in fact does
allow a flooding static description or
stack programs are going to take more
details about hands is going to work
sort of some question marks about the
types here but the we're going to end up
with code other side notes we do we do
in fact let me do it right end up with
Cody looks like that's all these this is
the core or STG more letters only the
stage and the core of you get from
compiling some little program or frames
and program one of these things on top
level statically allocated constructors
which refer to other top-level values so
let's see at the top of d1 is pushed
open followed by g2 yes I didn't mention
that I'm going to chain them each one
but most of the time these sequences are
linear I'd rather not have to use
sequence everywhere I prefer to chain
because it's during directions so I'll
come back to that point in a minute so
we can end up with codes of this g one
fish taken d2 and it does some
sequencing of doing something else for
about 2 3 etc so this is this is perfect
basically there's no allocation
multiplexing it is also allocated and we
can still have a rotational loops such
really nice and let me see what time
what's my time budget when did when did
we start I think you should just tell
you the shout out something
Diamond person Clara cheers okay there's
this if we follow the stock approach we
have to do more or funky things with
types to keep track of using the stack
probably because I stacked it's
necessarily heterogenous you can you can
push you know a thing of Taipei and then
people think titanium and then we have
to keep track of and you can do that
differently in everything in civil place
you write it so we have to keep track of
what's on the stack and types of the
things on the stack so for example push
token is going to go from stack s to
stack token on top of this so let's
let's model that we will say wish tokens
the decoder now decoders can take to
type arguments the initial stack type in
the final stack type so it goes from a
stack of anything s2 token on top of the
front foot off the stack using this data
type just just due to the table to keep
track of things so now competition
Hannah's this pipe decoder hve and b2c
stick them together and we have an
agency so this actually fits the
category class from the base package
identity in composition and this one
that we were going to use it actually
flipped with the composition so this
this is well if you're familiar with it
at all it's you can think of the
category but another nice way to look at
it is a type index mono age because this
as a nice connection with Titan index
boners which we will also be using type
index because we are we've got these two
extra tight for amateurs to keep track
of in our case the type of stack those
other things in the obviously or
instances of Kashmir where it's keeping
track of other things so our decoder
this the bit that's describing the
deccan valium sequencing all the planets
done is s 2 s doesn't modify the some
tall and seabirds the type of
surveillance as I sedimentary force or
primitive most of the time most the time
it's just
operations push this wish this is on the
stack push this to you and without
without branching and truth without
rather than like quartering which is
really wanting the sequence work so we
make we make all of our constructor is
slightly the primitives pushed open
their contain the following chain of
sanctions that's what we want to do it
for performance so we can interpret it
quicker we've just walked down a
champion rather than constantly bouncing
back and forth through the seams but
unfortunately doesn't quite match our
sequence cooperator type we can make it
match on signal prototype by composing
have done to make follow done but now we
don't have the training wanted um but we
can make it work because we can rewrite
it localism using rewrite rules and this
actually works really nicely so we can
pretty much always so given those
visuals and written over a question here
see ya talkative yeah it just will
produce more constructors of the
interpreter has to walk over it's just
shortening chains so given given if you
might somehow this pushed open for apush
like the folder something else 30 rules
wall and eliminate those tons and
sequences and turn that into just a nice
simple in nature and that's just faster
to for the interpreter to walk over but
that's the reason you do we did speed um
so you might say well this is this is
the idly all these rules and branching
sequence and time of change of style but
I want my ganache is quite nice there's
kind of two approaches here one is that
you you right i know your dns style
where it has to be perfectly aligned
everywhere to work and specialized and
you can't let it well in particular if
using the d-list and can't like the
continuation the steady progress we have
to be bounded and abstract him thats it
all goes wrong
so they're slightly these are slightly
fragile approaches whereas they do
something sort of stupid and obvious and
optimize it locally I am thinking of
spoken I sur votre having having tried
both I in particular I had one approach
idea what I was trying to limit the
scope of these continuations and and
then somewhere you wanted to abstract or
something because it was a polymorphic
some instant someone would call morphing
and something the whole thing just was
terrible where is this this kind of
obviously ok but then improve it locally
approach seems fine um ok so now we've
done this deep embedding for doing
things like pushing pushing input tokens
on to the stack now at some point we
need to alter the sack we have to have
these functions that we arrived it
change this time now we're going to do
this with a shower embedding IE
functions that just actually modify the
stack because these these functions are
going to be very low level and things
fast they're going to literally things
like read from here and write to them
and to that kind of operation will have
a very high interpreting ok so we're
going to we're going to use a direct
shadow encoding just directly modify the
stack so for this one so we're going to
use a tape indexed known out stack of
our so again we've got these two type
hunters st step can detect line stack
time the second for the type of the tech
after us so it's exactly the same
through having a known to go but there
is now a next dragon which is the result
that's why it's that's right Simone and
not on one on it but it's still typing
next to keep track of the type of the
sack now the operations been out here is
that you can pop things and you can
return result and then the way we use
that is that the resultant is finally
returned is pushed back onto the stack
and that means that we can guarantee
that I stack of our action never grows
the stack the most you can do is pop you
can pop zip
pop some things and push we thought well
I think you can render stack by most one
that means we likely doing overflow
checks or they were flow checked and all
the diamond interpreter and I projects
in the shallow which is nice that that
keeps me how to code there from it and
25 return so the final code then looks
like this push push and an altar now if
the body of the altar here is this
tightly indexed loan out which is just
the limited operations of popping things
off the stack and returning something
result and note that it is actually a
stack I've got this Iran several times
you have to pop them off in reverse
order you can play I can't go wrong so
many times so yeah this is a index
thrown out unfortunately while category
the type index la noise is in the base
library there isn't one in the there
isn't one for men up to this one of
Edwards packages which will stop whether
it's dozen citizens but none for base so
I reason i'm not using because i don't
want no good binary especially kind of
quite lay down the package document say
we just we just use kind of classic
style no nation type of support but it's
not a glad anyone who is familiar with
no nuts would understand this code so
yeah came back to the headlines does it
work yeah unfortunately not actually
faster than the continuation one which
is kind of disappointing i think there
might be more performance to be squeezed
out of this type of bridge there's more
specializing to be done but i think
really requires now to work out why use
of them still slow or with publicly move
faster of course I don't know Val grind
or Linux power for something to figure
out where where is the remaining cost
otherwise you sort of just poking the
darkness its missile it's a good result
we've got you know two implementations
that are dramatically faster than the
Vista blend of cereal both over twice as
fast yeah as I said earlier the static
version allocates considerably less but
this doesn't seem to translate
to actually doing spending less time in
the game hunter and I don't know why it
is it so I'm yeah I'm hopeful that the
sack approach can be pushed even further
but if not we'll just use the
continuation 11 of us faster so what are
the takeaway lesson two apart from that
this library is going to be awesome so
deep I think think the big thing is the
decent beddings you don't have to be you
should be skeptical panting compelling
video because I do involve extra
allocation costs but when you're done
judiciously and with a very fast
interpreter they can be a reasonable
approach they can be okay for the moment
so in fact they can be excellent in some
places I think it's easier to get more
predictable performance with with this
for this style because of the not having
to do this staged programming stuff and
perhaps that wasn't because of that
practice also easier and justin matter
if you've got a limited time budget
that's perhaps easier to achieve
reasonable performance you might not
achieve better performance five getting
the perfect shadow encoding charlyn
belen but perhaps it's easier this way
you certainly get much smaller object
code size which currently important and
the other thing allocations can be
important and we can track them down by
oblivious to generate the as i said the
final difference between the
continuation one and we stack one
although didn't didn't turn out from
huge but there was still a massive
difference between there's still a
massive difference in the amount of
allocation both for doing compared to
the vision I think I think that was is
weighted little bit so allocation can be
moving and we could easily wanted so
phase i think that the transfer for
lessons there's lots and lots of little
interesting details which i don't have
time dinner time to shove it aside i got
Dunsmore slide it's about
lots of types and lower level riparian
hackery and I better stop that on people
can learn quiz me after class tough
question yes why not the place that do
you have some baseline for the real
world benchmarks I mean but facebook
twitter google also have something
credible so i don't know i don't have
comparisons with like fringe and
photographs and whatnot and I think
there's in those particular I think
they're not the right comparisons
because I want to make comparisons with
things where you can just you know dr
generic and then drive some type classes
for free get civilization hassle values
to to something maybe if you've got a
freighter buff compile the program
doesn't really work well for victory
haskell tree instructions I do I amid
see so I mean so serious like that but
but but is that it's our target is the
end product two times slower or 10 times
the work to it is that she be able two
days see run so comparisons with listen
no I don't have a benchmark program see
it now exactly interesting you've got to
make sure it's a fair comparison is you
got to turn into structure it's not just
to civilize engineering structure or
something yeah it's sort of the strap to
CS track the right so yeah that'd be
good I don't other such as such a nice
run other questions
gentle speak advice you had legend
question Taylor while we're going to the
sides all right well nice top so Bryan
of Sullivan is now working on a direct
encoding feature for I am so instead of
going why I fell in fact like we do now
go directly to do a builder and he asked
me to review the code so but I think
he's using like like you said like the
easy approach directly going to a
builder yeah I think going by this
intermediate structure it's this beam of
construction basically that you can
speed it up more yeah so for the the
encoding sides taking our values
sterilizing after Jason that's kind of
easy way that's the easy side of things
and you said you could go straight to to
text from that Oh going probably via an
intermediate token stream of a sense
yeah that would be fun isn't the hard
one is Delaware is DC lysing parsonages
because fields can occur again order in
binary we have luxury knowing exactly
what would have things are I them but
with Jason you might have to get into
the fire before you get the first feel
the might be coming ladies are dealing
with that but you have to be a common
man not affordable yet good luck will
always be faster than item come on if
not well enough to eat our hats or
something I nearly questions yeah
between code size and font I don't know
I think you like the benchmarks to see I
mean so there's this idea that your
generates you're generating a huge
amounts of specialized coded all in all
the news sites that you're blowing your
instruction cache in branch prediction
what's up and then they will be true but
it's hard to know for sure unless you do
some proper low-level earth but its
power for whatever profiling to find out
it's a reasonable intuition right yeah
so the comment now is that code size
will matter more and more when people
are doing things like using gcj s4
you're compiling to Jason code and
deploying over the web people don't want
to download Multi multi megabytes j/s
files okay size does matter so yeah the
interpreter approach would be with Robin
I set each mind on Finn situation
pensiveness exploit some extra benefits
of using tricks on the writing our
partial you as the like finally like we
can expect the structure and of time
with us too soon so you would need okay
so the question was could you do
something clutter by using effective she
reduced number of chloe gets power
strips that the ability to expect
something in life as it is like you get
more velocity so you would need to use
some sort of rule rewriting or something
to write compile time translate into is
ultimately you have to produce disk
instructors yeah but producing is
consumed
constructors it's all about what did
invented what what value different
reading joined up what constructions to
end up with and what are the functions
in vessels of nodes maybe the Sun I
can't I can't think of anything I can't
think of anything they would let you
because each yeah I don't think the ribs
because each consumed token does give
you a value and it has to produce up
another closure which participant in and
you can't you can't get away with you
know not not producing what is consumed
take instructors it has to be there I
think Medusa value that you need to
consume
right because you so the only way you
can do it is by changing the deep
embedding if you do another you have an
additional specialized perhaps a captive
specialized constructor or constructors
for that you might be able to use with
addictive and then been in Chapter can
do something different again I don't
know perhaps worth exploring because
suddenly we wouldn't want to using a
plaintiff here at most the time and if
there's something that would reduce the
number of closures that might make that
continuation style little bit faster
mrs. alright I think we're probably done
on time thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>