<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Effective Approaches to Video Search and Classification | Coder Coacher - Coaching Coders</title><meta content="Effective Approaches to Video Search and Classification - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Effective Approaches to Video Search and Classification</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iIChCV-NiQc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we're very pleased to have Alex
Halperin in here he'll be talking about
some of his work on the info media
project and actually I I don't know what
else he's going to tell us so when we
all sit and let Alex take it away thank
you so what I'm going to be talking
about is approaches to video search and
classification and specifically the
subtitle is maximizing the synergy
between systems and humans because we
found that well automatically we can't
do all that much so we have to look more
as towards what people can do there are
really four portions to the talk
although not of equal size 1 i'll talk
about the infra media system very very
briefly then i'll talk about one
approach to bridge the semantic gap in
multimedia maining basically trying to
get from undershoot trying to help
understand what images and what video
really stand for you know words are nice
because we have these clear units and
sort of you know a document consists of
words and they sort of relate to what it
means imagery you know you have pixels
but pixels really don't relate to what
this image is all about then I'll talk
about some very serious research active
learning for new concepts and then I'll
conclude with something I call extreme
video retrieval how we really push the
humans to the limit utilizing this so
the info media project has been going on
for more than a decade now and we have a
system that allows you to do interactive
queries on news multilingual news and we
have a fairly fast and simple interface
we allow map kraze where you sort of
select a region on the map and say give
me all the news that comes from this
area we have various filtering
approaches and then there's lots of
browsing filtering and summarizing
because the we're not delivering the
exact
content that the user wants and so we
have to give the user a chance to sort
of look through it efficiently and
different users might want different
things some person might be satisfied
with just a little a little bit of text
and other people might want to see the
face or they might do want to see a
particular action taking place and so
the different characteristics to queries
so what I'll do and this is demo time so
maybe you won't work is give you a quick
example of it so the data I have is from
the first quarter of two thousand five
so it's a year old actually but it's
what I have on the laptop CNN Taiwanese
television Chinese television and
al-jazeera and since I know what's in
the collection I'll give something give
a query that's how half were useful
now that this data has been
pre-processed I have an Oracle database
on there that i'm using for the text
search what you see here is one image
for each shot and i can go ahead and
play some of these octal men met you
know our vinyl prophet epistles re hacia
abajo bomber Anala me only a vatta
tillman Kubek so what we've done here is
we've run a speech recognizer across the
arabic then we've run statistical
machine translation across that Arabic
transcript we got the the right side
there and then that's more or less
aligned you know the machine translation
you can have a word for word alignment
so it's sort of a section for section
alignment the arabic is word for word
lined with what's in the video and while
you can tell from the translation that
this is nowhere near good enough to read
and understand it did give me the right
stuff and there's enough keywords in
there to tell you yeah this is the right
story and visually you can tell it's the
right story based on what what's in the
in the video so this is sort of the very
basic in for media system it I could
give an hour's talk on exactly what the
system what all the features are but
that's not what I was planning to do I
actually gave a talk about this he was
around 2002 here and showed a bunch of
the sort of low-level work it hasn't
it's it's gotten better and it's gotten
more varied but it's sort of the core is
the same so I'm just going to go on you
have an idea how the infamy dia system
works
so yeah and I should give credit to my
colleague Mike crystal who is actually
the chief interface designer for all of
the interface work so that that's not my
work so I'm not spending a lot of time
on it so one of the things that we find
is there's this gap we have video or
imagery at the bottom and there are
low-level features that we can extract
sort of color texture motion features we
might have some data about the video
that was done at capture time you know
we know what format it is what frame
rate maybe we know the title of the
video or something that this is a you
know six o'clock so it must be the news
show kind of thing and then on the right
side here you can't see it very well it
says human annotations so people might
have given put a web page around that
and said well here's some clip of video
that's that shows me falling down or
jumping on a trampoline or something so
i might have things that were annotated
and that those annotations can describe
what it's about describe who's in it
whether it's funny or not it has all
different characteristics so it sort of
spans on the right wide level of things
and then we have the user on the other
side and we don't always know what the
user wants and how to get from these
low-level kinds of things they're not
words to what the users want users tend
to express things in words very rarely
do they actually express things I want
another picture like this that you know
you see a lot of research doing that but
that's rarely what users really want we
find if they had a picture like that
already then you know another one has
only marg small marginal interest so one
of the approaches we've been pursuing is
to define a set of concepts that we can
reliably recognize based on this low
level or mid level data using a number
of approaches in a lot of machine
learning but also you know some human
annotation some computer vision some
exploitation of the capture data and so
forth in the layout and
use that to find these concepts and the
notion is that well you know so we can
do face detection we could do card
detection and so forth but that doesn't
get us very far but suppose we had a
thousand concepts and if you think of it
as a limited vocabulary of a thousand
words that describe video maybe that
will get us there or maybe 10,000 words
is the right level of granularity where
all of a sudden if you if you can use
10,000 words to describe a picture then
all of a sudden you can meaningfully
retrieve things that you really are
interested in so that's a one direction
we're pursuing I won't really go into
more detail than that but it's it's an
approach but I'll go on to the next
section which is actually the active
learning part because if you have a
thousand words that can characterize the
contents of a video and users can search
that meaningfully will still be missing
things and it's impossible to
pre-compute all possible sort of things
a user might want from a video you know
so I want you know to see Britney Spears
left big toe well we won't have a
classifier that does that for you we
probably won't even have a toe
classifier maybe Britney Spears will be
there in that sort of thing but the idea
is that if you're interested enough
maybe we can get the user to actually
assist the system to build a task
specific concept detector one of those
semantic categories dynamically to help
with retrieval and so that's the core
idea behind what we call envy the
extensible news video information
extraction system that users can create
their own concepts and the idea is you
have a user they don't know there is
they label some examples this these
examples positive and negative define a
classifier define training data for a
classifier the classifier build some
models we take the models process the
archive
against with that new classifier get
some new results give those to the user
and the user can say well these things
are good but these are not and we can
keep going that way iterating over it so
just in case I stall did I should have
one slide in there for active learning
so you have labeled trading the label
training data based on that labeled
training data you classify the active
pool then you find some informative data
data which you don't know enough about
but it would really help you if you had
that labeled you get the user to label
that and then you build a new classifier
to try and label the rest of the data it
may be a little bit better than you
could before and the trick is selecting
the right amount of informative data so
that the user doesn't have to label
everything and we tend to use svms so
you know we find a hyperplane that
separates these and the things closest
to the hyperplane are the ones that we
want to label the data points closest to
the hyperplane are the ones that are
most informative at least that's the
sort of standard way you can do it you
can play other games as to how to select
the most informative items but anyway so
that's that's it there is one additional
complication here we're dealing with
multimedia data and so it's not just a
text string that defines this but for us
it's color texture edges audio
information face information motion
information at a different level we have
the text from speech recognition and
video OCR so we have all these feature
sets that all can be used to build a
classifier for a particular piece of
video too so there are different ways to
do that you can do what's known as early
fusion you can take all these features
into one very very large feature vector
and then do your machine learning based
on that but that quickly falls apart if
you have small numbers of training data
and feature vectors that include
thousands of features or maybe even more
certainly if you use text you have very
very large feature vectors then you can
do late fusion which means fusing the
build a model for a color detector build
a model for a textured for texture and
so forth for this particular concept and
then take the combination of those model
the output of those individual unimodal
detectors and combine them and then have
a better detector but in that process
you've lost a lot of information in
between and a plus that also means you
need an additional held out set and
there's some question how to combine the
weights for each of the individual
classifiers that you're building so one
of the new things we've done in NV that
we build a four for each feature set we
build a different sub model then and
this is in each iteration and after
applying the sub model to the unlabeled
data we take from each of the sub models
we choose K examples which are close to
the height of hyper plane for that
particular model so it's a we don't just
pick one set of examples but we pick
examples from each different so we start
to vary things from each different sub
model and then we get the user to label
days so in some sense you're labeling
our times K examples for our submodels
you label k examples and then as your
added so you get the user to annotate
them and then you say well now that I
have this annotated data let's assume
that's my held outset I can figure out
fusion waits for this data for this held
outset that the user just annotated and
then iterate that way does that make
sense okay good so now I'll give some
examples or it gives some some actually
hard evidence this is a good idea not
just
intuitively you can nod and say many
apps it seems reasonable so we try this
over is about 70 hours of video which we
subsample 250 2943 annotated shots and
then there were 20 semantic categories
that were labeled on this they included
things like ships airplanes outdoors I
forget all the categories now but in a
number of those and we're averaging
performance over the 20 topics so
because for any one topic it might do
really well but then don't want to leave
out the other ones for which it doesn't
do well the features that we actually
used for this experiment are to HSV
color histogram zedge histograms Gabor
features and faces face information
includes the size of the face the
location of the face and how many faces
they were and the confidence in the
faces and then we used a support vector
machine with the RBF kernel as our core
classifier so the idea is we trained all
this label data with multi modality
fusion and we compared a different so
the baseline sighting just takes one
long vector of all the feature data the
what we call the base active takes does
the active learning so it takes a
thousand randomly annotated shots and
then each iteration you build a new
classifier based on these concatenated
features so that's the baseline takes
everything with a concatenated vector
the active learning starts doing this in
pieces of a thousand each time and then
the new idea here was to do this multi
modality active learning where we start
with a thousand pic 250 sample examples
from each of the four feature set and
again that number was just chosen
because it you know divides a thousand
by four but you probably can do a better
job and then in each iteration see what
we get
and these are the results the multi
active gets higher and faster and you
see there's some overfitting here that
that happens the baseline concatenated
features concatenated features are just
not that great of an idea and if you do
active learning on them sort of you
still don't get all that far so this
baseline is actually the same because we
did it on all the data I'm just it was
just plotted as a as a straight line
through here it really happened with a
hundred percent of the data so then we
did some upper bound experiments showing
what we what's the best we could do
meaning that if we took all the data and
looked at the four different feature
sets and sixteen different combinations
of these feature sets how well could you
do and then we said well suppose we took
took this active learning and start with
a thousand randomly annotated data
pieces and take again thousand new
examples but each time we use the
optimal combination for the fused model
and then we find we actually could do we
could get to this optimal performance
faster if we did that if we always knew
what was what the best combination was
which we don't but then it turns out we
would we would get there faster with two
percent three percent of the data as
opposed to ten percent of the data but
we're still getting there with the multi
active approach so yeah
yeah I think one of the things that
happens is that after a couple of
iterations the support vector machine
has sort of zeroed in on an area and
even though you're trying to find things
close to the margin they're sort of
still within that area and you're seeing
this over fitting to the particular data
set that you happen to be looking at
closely that's near the margins between
the data set examples that you've you've
looked at and not way out there
somewhere else where the support vector
machine thinks it's not a problem but
you don't even ever get to look at it
that's my guess but using this burger
strategy that you come up to it you get
in your arm will perform for the asset
or and then you know listening app
reaching a maximum it
Vidya I'm you see across to me then
you'll come back up as you reach most of
the data but it'll take a while before
you sort of then get to that best place
the line where you have all the data
so this is comes the moment of great
risk should I show you this in practice
like what it really looks like I can try
so I might not work right so I do here
one of the things that we found with
data from not CNN sources actually
Chinese television is is not bad is that
especially Taiwanese television is all
over the place and it turns out this
person is the anchor here but see the
CNN anchor sit in a studio and you can
always tell you know from the background
that this is a studio this lady here is
the anchor person for Taiwanese
television but she has all this junk
projected on the back all the time and
the question is can we now build a
classifier that picks out the anchors
not just her in this particular outfit
but the anchors in general in Taiwanese
news so I'm gonna give this give this a
try so the thing obviously I have to
start with is label some data I'm going
to put this microphone down because I
need my hands here so what I'm going to
do is select some of these images
see
and then this is it seems to be a new
story and then he or she is again
okay and just for raw variety I'll go to
another show
so okay so now I have a number of
examples and what the system actually
does it keeps track of what I've looked
at and what I didn't select and those
are the negative examples and now I tell
it go ahead and build me a classifier
and let's call it
and obviously it doesn't make sense to
classify anything except Taiwanese news
with us so I'm going to just it does
make sense to classify al-jazeera I mean
you get something but you probably don't
get good results so now it fires off
this perl script because no good way to
start prototyping and it's it's not
super efficient but hopefully it'll
finish before i totally run out of
things to say the first thing it does it
scales all the data to be sort of
roughly in the same same range for for
all the values then it'll try to build a
classifier from that and then it takes
the model that it built and applies it
to all the data in the database now I
should point out that a lot of this
research is University based research we
would not think you could do this over a
billion images in the Google scale
without major major rethinking of the
whole thing so my expertise is not at
all in the in the billion image scale
it's much more looking at what we can do
sort of on this tens of hours of tens of
our scale as opposed to the billions of
images or thousands of hundreds of
thousands of hours of video and so forth
so one reason this is also slow is that
it's all running on this laptop right so
it's everything is local right now I'm
not connected anywhere all right I've
run out of things to say and it's not
even close to being done it once it's
built the model then we're within a
short period of time but we're not there
right now good models trained now it's
taking the data predicting it and since
we're only predicting over like 10 hours
of video it's not such a huge set and
we've precomputed all the basic features
the low-level features so those are
there and now this is the command to
update the database so now it actually
is going into Oracle updating all the
database and it's done
and now I can see what it looks like
here and hopefully worked in no idea so
this is what it thinks it came up with
so this is just a classifier what it did
is is it it actually did a fairly good
job here this one is so what I now I'm
going to do is say well I want to get
this better first I clear out the stuff
I had before and then I'll mark this one
because that's not an anchor yeah that's
probably an anchor this one's not
yeah it's surprisingly well almost too
well now did the active learning won't
be impressive anyway I won't get hung up
on this so now I can go back add this to
the no set if i had seen no good results
i would have added everything to the yes
you know we added a lot of things to the
essay but since i have mostly good
results i append this to the no set for
this anchor to figure the system figures
out oh we just have to add that to the
data and now i can go off and build
another version and so forth and so i
can do this it does the same thing and
now we wait again and so forth i don't
think i'll spend too much more time you
get the basic idea and each time it gets
a little bit better to the extent that
this active learning works and then sure
enough if i do this too much all of a
sudden it gets worse and people in the
audience say oh that's terrible so i
won't go any further i'll just leave it
at that especially since it did well in
the first round we go back to the talk
okay i don't have to show you this
because I show this to you live oh yeah
and then I can also then filter results
I Kate take the results and say oh throw
out all the things that you think are
Taiwanese anchors from this query that I
had on whatever earthquake relief just
throw throw out everything that includes
the picture of the anchor because I'm
not interested in that so I can filter
it okay and now I'll get to the final
part of my talk and this is extreme
video retrieval so we noticed we've been
participating since 2001 we've been
participating participating annual Trek
video evaluations and they have two
tracks that is sort of the automatic and
the interactive track and this year the
best interactive system had a mean
average precision of point for not very
well but you know this is this is
state-of-the-art
the other anchor has finished if you're
interested in seeing the results I can
go back to that but I think you get the
idea so by comparison we also filled it
an automatic system and the out of the
performance of this automatic system
with the same queries was about point
one and what this plots is actually the
performance if you only took the first
ten images what was your mean average
precision if you took the first 500
images what was your mean average
precision so I plots mean average
precision at various depths and now you
you can look at that and say well that's
not very good and then we said well
suppose we had an Oracle that could do
this better and for the first thousand
images put all the ones that were good
in the front and the ones that were not
so good in the back and that would give
you and that's plotted here on this
solid black curve so what we've what we
what we can see is that if you had an
Oracle that looked at 2,000 of the
automatic run images you would do better
than what the best interactive system
was the best interactive is very much
like the classic in for media system
that i showed you at the beginning so
that's so state-of-the-art so he said
hmm what kind of Oracle can we build we
can't build one but we have a person so
all we need is a person to look at the
first two thousand images and then we
should do better than any system that's
currently being fielded I yeah but
that's silly and then the question
became how can we make a person quickly
look at two thousand images there is a
time limit to these interactive tasks of
15 minutes and so you know this is where
you start feeling that oh this might get
to be a challenge and this is where the
extreme part
comes in but first I'm going to make
some comments about the automatic search
here over the video and particularly the
combination okay so for this extremely
well we have the automatic retrieval
baseline which just gives the shots in
ranked order based on its best guess at
this and then we have two methods of
presentation user controlled and system
controlled and I'll show you a demo of
that towards the end the the system
controlled is based on something called
rapid serial visual presentation RSVP
that so psychologists have experimented
with in the past and then we have the
user controlled presentation which
allows the person to manually push a
button and see as many images as they
want you could dynamically resize a page
as you're going along I'll explain that
a little more later but first about the
baseline how did we get this baseline so
the idea is we have all these knowledge
sources or individual experts we have
the closed captions or speech
transcripts we have audio features
motion features colors and then we have
these some of these semantic categories
like faces buildings and so forth that
we've tied our library with and then we
have this query that potentially
constant is text and imagery in the air
in these examples there was we never
used audio emotion but we used the
images that were provided in the text
that was provided and then you know from
each of these experts we get an output
and then the trick is how do we combine
these diverse knowledge sources into a
final ranked list and that's actually
one of my students wrong yen is doing
his thesis on trying to do that and I'll
give you sort of some of the highlights
of that work right now so what's
different is that for normally you only
have this one path we have sort of text
matching to text you get some output and
then that's it maybe you can think of it
you have the text and the page rank if
you want to look at them as two
different sources
anyway so you get this final rank list
and how do you get this ranked list well
so what we have a pro listing model for
multimedia retrieval and the basic model
is logistic regression in that for you
know what's probability that document is
document meaning a video shot is
relevant to a particular query and we
estimate this as the weighted sum of the
individual experts the relevance of the
individual experts gives us the total
experts and this is logistic regression
so it fits in here and the idea is you
want to learn these combination weights
from past results well the problem is
you can't learn it from every query you
don't have a history you know what's the
correct result for this query on some
other collection which would give you a
sense of what the weight should be the
other thing we had to do is logistic
regression is designed for
classification sort of a binary thing
and but a binary classification has a
bias towards irrelevant document if in
the sense that if ninety-nine point nine
nine percent of your collection is
irrelevant then this logistic
regressions will say well my guess best
guess is just call everything irrelevant
and I'll do fairly well according to the
logistic regression you don't want to do
that so we have adapted this ranking
logistic regression where the basic
logistic regression term here gets
modified by a sort of error in what's
not relevant and you take the difference
of them so you're looking at maximizing
the prediction gap between the positive
and negative examples and you can sort
of see it as a lower bound on average
precision and then since that's actually
fairly expensive we can do some
approximations to get this to be a same
complexities logistic regression as
opposed to being N squared
and so one of the other things that we
can do here is so if you take all the
past queries and say well you know on
average for all the queries what was the
text wait what was the image wait and so
forth you get this you get some sort of
uniform waiting over all the queries and
it turns out that's not very good
because queries are all over the place
if you're looking for a person you would
emphasize facial feature and maybe a
person classifier if you're looking for
oceans you would say well I don't want a
face to be big in this because then it's
unlikely be an ocean thing so different
queries have different characteristics
so this isn't very good what we really
want is we have different query classes
and each we map each query into one
query class and then for this query
class we have a set of weights and we
can learn these based on examples of
these query classes that's feasible
because we only have five query classes
so it's doable and then for a new query
we can say oh it's it's most like query
one or it's like a mixture of core of
quarry class 1 and 2 and so therefore we
have the weights be a mixture of those
two and we get results I'm still doing
okay there so the whole idea is here we
have this additional term in the
logistic regression that introduces the
query types or the quarry classes that
we have here and so if you remember this
was the stuff we had before and now
we're just introducing this additional
class that says for this particular
query class we use these sets of weights
and in practice we've used five
different query classes named person
named object general object in seeing in
sports queries there's also some work
that you can automatically find these
query classes for which grade classes
sort of which queries tend to cluster
together to form a class on their own
and then the other thing is what about
these semantic concepts that we don't
really have good query classes for so
for example if we have a boat chip query
great you know then we might say well
you know you want text you don't want
faces the image imagery is important
meaning if you have an image example
then give it this weight but we don't
know how to combine that with outdoors
and ocean because we don't really know
how boat ship that specific query or
even the query class of general object
relates to outdoors in ocean and so for
that ron has adapted something called
probabilistic local context analysis
which he borrowed from the text world
where you take these documents and
basically look at the top view do the
equivalent of pseudo relevance feedback
assume that they are right and then feed
that back but because this is a
multimedia you can't assume too much
about the top things being right so you
have to be very very cautious in feeding
things back because you can very easily
if the top 10 things are bad and you
feedback from that all sudden you're off
on the deep end so you have to be very
very conservative on these things it but
you can control that and so now we have
so this is the term we had before the
for each for each expert we look at the
relevance the weighted relevance and now
we have this additional turn here that
says for this specific query can we also
estimate the relevance of a particular
source to this for this particular query
based on this relevance feedback and you
know it gets more expensive but you can
actually do that and so the bottom line
is does this work you know how well does
it work so we looked at the baseline
automatic system result and then we
applied
these retrieval experts and 15 semantic
features face anchor commercial studio
graphics and so forth and then we used
the relevance Bass Pro holistic
retrieval model basic model and then
this the model with this query analysis
and we present the shots and it actually
act this is the wrong results like
they're the baseline is here is one of
these and if you put all this stuff
together you get significantly better
this is again the trecvid results turns
out these guys here cheated by using
google external information about news
that helped them get a better query
expansion so okay so this is just the an
aside really to lead up to this extreme
retrieval idea so let me first give you
a talk about this system controlled
presentation RSVP rapid serial a visual
presentation the idea is you try to
minimize eye movement you know don't do
anything while don't make the user do
anything that's not relevant to the task
so minimize eye movements all the images
come up in the same location you
maximize information transfer meaning
you put up a lot of images very fast in
theory you can do up to 10 images per
second if you're bringing up a single
image and then we've played around with
one or two images per per page because
it turns out and you have two eyes right
so you can look at two images at the
same time and so you bring up the images
and then after if say 300 milliseconds
it switches to the next one and so on
and if you like if one of them is
correct then you hit either the left or
the right button and off you go and it
marks that one is relevant and then you
can you find that in the beginning it's
quite hard to do this but then after
30 seconds or so your brain adjusts to
this rhythm and you say okay I can do
this and then we have a button said go a
little faster so at the end you can you
can go quite fast so in the two to three
hundred millisecond range per image or
per page and especially if there are few
relevant images sort of you can quickly
eliminate them it turns out not every
query is the same so some queries uh you
need to look more carefully and they're
slower and others you know say is this
soccer well you can tell that pretty
quickly you know can you see a left hand
well you have to look more carefully so
it's it's a much harder kind of task and
so it takes you cognitively longer to do
this what also happens is you're not
always fast enough these things come
whizzing at you at a fairly good clip
and so sometimes as you're pushing the
button it's paging to the next page so
you actually have to mark the previous
one as well as the current one because
you don't know which one did the user
mean and that means at the end you
actually have to have a little phase of
cleaning up because you're not sure
which one was the one you really wanted
so that's the verification step at the
end and i think i'm going to try that
now you see if i can do this
okay so we need shots of Oh marker ami
the former Prime Minister Lebanon
actually let me find something that
everybody knows about Tony Blair I'm
going to just load fifteen hundred
images
so this is the ranked output of the
automated results so right now it's
loading them and since I have all this
other stuff I still have the infamy dia
system up and the Oracle database up
takes longer but this is really pretty
simple because all I'm doing is loading
up a bunch of images into memory I mean
they have to be a memory because you
want to do this rapidly and then I think
the configuration right now such as four
by four so I can use for keys you know
four fingers and say which one is there
and I'm also have going to have to be
quiet because once this starts it's not
like I can do this and talk at the same
time my brain cycles are not fast enough
for that but it's amazing how much you
can you can adapt to it so I mean I just
could do like a couple minutes worth a
couple seconds worth of this once it
loads I did start late so I'm almost
done though okay so it hasn't started
yet but so we see tony blair is in the
bottom right here right so let me start
this and then you can just watch the
images flip flip by because it's going
to go pretty fast so I mark this one as
green and we start ok this is wages I
need to go slower
nope
nope
so you get the idea right and you can
then speed it up to the point where if
you're going if you're doing really good
you can you know go faster and faster as
you're doing this and it is this extreme
in the sense i think i'll just stop you
it's extreme and it said in the sense
that you don't want the phone to be
ringing you don't want anybody to come
to your door and and in fact when I so I
actually did this to and my students did
it several students and two
undergraduates did it and when you're
doing this you actually at some point
you don't want to blink anymore because
blinking takes so much time so you find
yourself you know trying to keep your
eye open and not to blink while you're
doing this so this is on the on the
extreme side of things let me go back to
the talk
ok
so that's what I just showed you and so
on the slightly less extreme size is
when you when you control the paging
manually when you get the next image and
so you have a button that says next page
next page and what we found very
effective is that you vary the number of
images on a page so initially there's
only two and you could say left right
and then as you get less dense in the
relevance things they make sense to have
four by four because you can eliminate
them fairly fast and you only have to
hit one of the four buttons and then
three by three two by two three by three
up to four by four we didn't go further
than that and you have a cording
keyboard so you can hit two or three
keys at the same time and again there
was since you do make mistakes when
you're pushing yourself to the limit
there was this final verification step
so the question is how well did it do
and here are the results so the in this
Trek Widow five things so the classic
intermedia system was here there was
another system from a former student of
mine actually that did a little bit
better this was the manual browsing with
resizing of pages and this was the just
single image flashing at you fairly fast
turns out yet by the way another thing
is you do start to dream about these
images at the end of the day because
they come up so fast and your brain just
can't process them so they sort of stick
around with you for a while it's it's
quite interesting anyway the the of the
very positive news this very dummy
approach as one of my students called it
was statistically comparable to anything
else so once you have this halfway
reasonable baseline this is good now do
I really think people want to do this if
you know if you've ever heard of the ESP
game people play that game for I think
they've put a limit on it for 14 hours
because then they sort of say you've got
to take a break
and you know this is just another
version of this type of a game right and
you know if you played video games the
undergraduates who play video games a
lot they do this very well I have a much
harder time so but the whole point is we
want to do active learning here and then
we actually have some experiments that
show if you can do this so you're
getting feedback while the users doing
this whether you do it with manual
paging or the extreme system based
paging the system can learn and if it
can learn fast enough currently we're
not fast enough as you so if the system
can learn what's relevant and you know
what waiting should be used then it can
rearrange the rest of the imagery to
give you more and more of the relevant
things quicker and then you can actually
do even better than what we saw from the
earlier baselines because you can
rearrange the whole collection each time
and do fairly well so I don't think this
extreme retrieval the way I showed you
is you know ready for a product for
somebody to buy but I do think in
conjunction with active learning it is
one way to sort of make retrieval of
images and video better than it used to
be so the idea is we exploit the humans
ability to do what they're good at which
is identify whether this is something
they want or not very quickly visually
and the computer ability to learn and
rearrange things based on the examples
that's it I'm done thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>