<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast global convergence... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast global convergence... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Fast global convergence...</b></h2><h5 class="post__date">2012-02-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/M5Cj2H1E2NM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">caribbean gets for the real time to talk
to statistically computation best local
convergence in high dimensions okay so
thanks for the organizers for putting
together a great workshop i guess i
don't know how i feel about
acknowledging my stupid ass after that
it's going good ali-a groans come on
Ron's not here so he gets um ok so I'm
talking about class of problems that
sort of popped up in many of the
preceding talks to selfie general first
and then I'll wrote these for some
particular examples reckons many
problems in which you want to estimate
some unknown parameter this could be a
sparse vector it could be a little
random matrix it's somewhat sparse
matrix and many of our methods are based
on solving optimization problems next we
have a loss function that depends on
data some number and on samples and
often we have some kind of regularizer
that tries to encourage some kind of low
dimensional structure and to like a moth
said really seen an explosion of data
and we need thanks to a new force
structure so what about to talking this
I'd like to go through this talk is
interplay between things the
statisticians or learning theorists are
interested in rings what they're
interested in is when you solve this
problem using some method you get some
estimate you get an estimate of sparse
vector you get an estimate of low rank
matrix and they'd like to know how close
is that estimate to some unknown truth
in the world right so that's one kind of
error will call statistical error that
they
on the other hand optimization spirits
what do they do well they have some
algorithms some particular algorithm
politte sufficient that algorithm
generates a sequence of estimates that's
called them to theta tank and what
they'd like to understand is how quickly
does the optimization airdrop was a
function of T so he's often or studied
separately but what we'd like to do is
sort of understand some interplay
between these these two things okay so
the algorithm I'll study one algorithm
come with a simple algorithm but it
takes different forms for different
problems it's an old elements not a new
algorithm I think some of the ideas I
talked about will apply to other albums
let's fix on this one so this projected
gradient descent you have some
constraint set projected gradient said
to send said we're assuming this is
differentiable says take the derivative
start somewhere walk in the negative
gradient direction if you walk outside
the set and you project back on right so
it's a simple algorithm will see that it
takes the form of soft fresh holding or
singular value and vegetable cases now
what we'd like to understand is at least
classically it's very long understood
how this algorithm decades this is all
the workmen to the nesterov and others
if you have convex problems that are
just compact you have no smoothness
conditions all looks fine what that
means in a minute it will converge but
you'll get relatively slow rates who get
sublet your rates what's more
interesting to us here is if you have
stronger conditions like strong
convexity and smoothness you'll get much
faster rates and what we're interested
in
is in these classrooms okay so just to
give you a picture right if you plot
iteration number versus log of the
optimization error if an algorithm is
converging something nearly it means the
air is dropping off like 1 over T or
some polynomial like that so if you take
logs then you'll see a fairly slow decay
what it means obviously if you have
linear convergence it means it contracts
by a constant factor every iteration so
you get a linear rate of convergence or
the right so what happens in practice
when you apply this kind of algorithm to
standard problems so what's one standard
problem one standard problem is sparse
regression right where you have a
regression a column you have a sparse
regression vector that you're trying to
estimate supportive and sparse subset an
estimator many people study is you
minimize the quadratic cloths subject to
an l1 constraint so that's a particular
case of the estimators I mentioned and
projected gradients very nice you take a
gradient step projecting onto the l1
ball is essentially a soft red folding
operation so that can be done none quite
efficiently now when some of them is
interesting and all this talk is focused
on is high dimensions itís mentions
this means that the number of samples n
is much less than the ambient dimension
D right so as many many problems where n
might be a hundred and D could be 10,000
so you won't understand what happens
when this matrix is rectangular in this
way okay so if your white ass out even
with a constant step size you apply four
different sizes of problems five
thousand dimensional 10,000 or 20,000
again plugging iteration versus log
optimization error what you see in all
cases are straight
it's roughly it means letting classroom
in a straight line and here we're using
2500 samples so these are these are very
nice determining problems right so it's
behaving as if the problem were strongly
convex it's converging quite fast but
these problems are not strongly convex
there's no way they can be because if n
is 2500 and you're working in a 10000
National Space this some the has seen of
this function is ranked degenerate ok so
we'd like to explain that good pass
theory on these kinds of problems but it
doesn't really explain this phenomena so
many people have looked at nesterov
methods or radiant methods and have
shown they will converge so than you'll
be that's not what's happening here it's
much faster than sub linear various
people have imposed certain kinds of
global smoothness conditions roughly
that means there's the gradient of the
loss function is richard's continuous
that doesn't old for these problems for
I dimensional problems you don't have
global smoothness so again these kinds
of results don't don't guarantee other
people have have looked at particular
cases where you have arts progression
with restricted isometry property and
they prove some nice results this
problem here is not satisfied the
restricted isometry property so those
results don't explain what's happening
this is a fairly early conditioned
matrix in sideways so it certainly isn't
close to an identity and incense so what
we'd like to do is if I had some general
theory that would explain that we'd also
like to explain low rank matrix problems
we heard I'm in this session many nice
talks vertical toxin involve matrix
completion right so the click here of
many of you know this that what you're
trying to do is you have a very large
matrix with let's say individuals and
items could be movies or for Amazon
things like books or other substance CDs
what happens is you observe in general
probably assumes or noisy ratings people
don't break things and I always this way
but you have a lot of missing data so
it's a very under constraining problem I
think this was first formulated as a
kind of low rank problem by a night raid
Louis his co-authors about seven years
ago okay so what's an estimator for this
well a standard one we've heard as
nuclear norm is a good surrogate for
rank again you can do the same thing we
do the quadratic loss subject to a bound
on the nuclear norm and many people have
studied this kind of estimator for noisy
low rank matrices projected gradient
descent is again reasonable um it's soft
credible Rosa you were values you could
debate whether it's expensive to compute
singular values we've got some nice
talks about how that might be sped up
but certainly it's not an unreasonable
thing to do and what happens when you do
this in practice again with constant
step size well here it's more
interesting so this is a matrix of forty
thousand entries and what you see is
that the convergence rates are actually
convergence non convergence is actually
sample size depending if you have very
large sample size this number is what we
do is generate roughly 25 times the rank
of the matrix times the dimension not a
squared so dimension is 200 so here you
have more observations you have 25 times
less observations it's a larger sample
sizes you get nice when your convergence
do you understand you don't actually
converge you oscillate there's something
interesting some sort of interesting the
phenomenon that has to be predicted here
and it's going to be a sample size
defendant this is our society matrons
what
you had an N by P before you buy right
so here he is two square matrix is 200 x
200 personal appearances or square
meters so we're trying to do low rank
matrix recovery right so terms of this
picture I'm saying that it's a matrix
it's 200 x 200 and n is the number of
observed entries right if you want em to
be less certainly much less than v1 v2
or d squared in my case and what we're
saying is and is roughly the rank times
let's say the longest side without
looking at square ones there is two
hundred in that example probably do
we've studied many different cases so Q
equals zero means it was exactly the low
rank you can do for approximately low
rank I don't remember offhand what it
was maybe tank justice as a low rank
matrix Netflix gonna say might be right
okay so let me if you want a last
example and then I'll also her try and
give you the high-level story I won't go
into this in detail because it's
actually worth it one of my students
polling presented at the main workshop
here but essentially when you have a
missing data problem it often leads to
non convexity and what we'll see is the
theory out present actually has
implications for non-convex objective
functions as well so if you take a
non-convex objective non-convex in the
right way it's an objective that
building constructed for missing data
and again you plot the convergence rate
of projected gradient descent on a non
convex problem you can see over multiple
runs you see a linear rate of
convergence right so that's an
interesting phenomenon it's a case where
you have a non convex problem but the
consequences are all present next is
actually in certain cases you can
guarantee for non-convex problems you
can solve them up to a fairly small
error police even we're going to solving
up to something that's smaller than this
too
okay so those are some examples nevis
introduced the property of estimators
it's something to do with loss functions
and something to do with regulators and
now flesh out some consequences of an
optimization and looking some specific
models okay so if you want to prove in
an objective function is that strongly
convex what you want to do is show that
it's lower bounded by erratic form but
it has curvature in every direction of
the space now you can't do that when
your sample size is less than the
ambient energy and space because if you
look at the taylor series expansion of
your loss function on the truth if you
perturb a little bit in some direction
delta there going to be some directions
that are good a good direction is the
direction of this where you do have
curvature but they're going to be many
many directions a whole sub space where
you see nothin where the loss is
completely flat right so now those are
very bad directions you can't expect to
do anything you can't expect an
optimization algorithm to convert
geometrically or linearly if it ever
encounters these bad areas here so what
you have to end all of these problems is
you have a notion of how the regularizer
might help you right so what you say is
we're going to look again at the taylor
series expansion then look at the error
in that and we're going to ask that
that's lower bounded by something
quadratic but we give you a little bit
of wiggle room right we give you
something that's proportional to the
regularizer squared so if this term was
zero then that's just ordinary strong
convexity but this definition has it's
interesting because we allow you a
negative slop there alright so what's
saying is that as long as your
optimization algorithm walks in
direction
where the regularizer of the direction
is quite small relative to something
like the l2 norm or the Frobenius norm
then you're going to have curvature and
have good behavior and so the name of
the game is you like to be able to show
that an elderly and we will show the
projected gradient descent we're going
to show that we can make it walk in
exactly a space where this term is
dominated by that so I just said that
there's also a an out of this definition
if you flip the inequality sign you have
a notion of smoothness okay so it looks
like a weird definition but it holds for
many problems if you just look at least
squares problems that it's actually
equivalent to what's known as a
restricted I value addition these are
conditions I think first proposed by
vehicle at all they're much weaker than
restricted isometry conditions they're
related they're controlling eigenvalues
of your design matrix in a regression
problem but they're much weaker there's
no sense that you have to be closer to
an isometry they hold for low rank
matrices in nuclear norm you can again
proved high probability that something
like this holds they hold for not just
quadratic losses they hold for
generalized linear models things like
exponential losses or bus song losses or
logistic loss there's a fairly broad
class of problems for which these kinds
of properties hold okay now we need one
more ingredient skip that slide so what
we have so far is we have a sort of
condition on the loss function and it
needs to interact with the regularizer
in a certain way and now we need another
condition on the regularizer that sort
of says that it penalizes that
deviations as much as it can so email
udah to sort of a more general view
many kinds of regular risers here's one
there's others in the literature glad
this is a useful one in some ways we say
that a regularizer is decomposable if
you can find the subspace and that
subspace is typically where you want
your parameter to lie could be a
subspace of sparse vectors it could be a
subspace of low rank matrices it could
be a subspace of sparse functions if
you're doing on parametric problems all
right so what we want to have happen is
that we'd like you to be living in the
blue subspace and we're worrying about
deviations in the red car can you afford
it'll complement and you say that a
regular excellent decomposes if whenever
you take something good so alpha in the
blue the subspace you add to somebody
bad in the red sox faced then the
regularizer splits it splits into two
terms right so one way of thinking about
this is what i get my students is little
tough love triangle inequality says this
is upper bounded by the sum and so what
this is saying is that this regularizer
is going to hurt you as much as it
possibly can if you ever move in a bad
direction right so there's a sense that
it's trying to enforce the structure
that you're after whether sparse vectors
or low rank matrices it's doing that
strongly as possible right this holds
for many things that bolts for l1 holds
for weighted l1 it holds four groups
parson alarms holds for nuclear normals
bolts for sums of decomposable normal so
for robust pcs taking clear plus parts
again this holds right so it's a fairly
general class of norms than satisfy this
if you choose these two subspaces in the
right way okay so here's the main result
what you should take away from the
previous is we're looking at loss
functions that satisfy a restricted
notion of strong convexity and
smoothness we're looking at regularizer
xand satisfied as decomposable property
those are the two conditions and the
main theorem is flowing that if you have
a decomposable regular Iser
then what will guarantee is will give
you a contraction factor something
between 0 and 1 and will give you a
certain tolerance a certain amount of
error such that the optimization error
decreases geometrically with that
contraction factor so this first
something happens that say a half gets
managing quickly it's like point 5 to
the 1 2 3 s exponentially you came but
we're not going to quite get all the way
to the optimum we're going to get to the
optimum plus a little bit of an error
here and this error depends on
essentially the size of the two
subspaces you can make this precise for
various models right that's a
deterministic statement there's no
probability in this right now where the
probability would come in is if you had
a typical some kind of learning setup
you can verify with those conditions
hope okay so i think the picture is
easiest what this is saying is if we
look at optimization there so the first
errand first favorite the second game
third it's saying that it sort of falls
some decreasing paths contracting
geometrically that's what we'd like to
get to right that's the an optimal
solution of the full full problem the
solution doesn't have to be unique
actually and what we're guaranteeing is
it very quickly you'll get to a certain
radius epsilon of the optimal right so
this is going to be an interesting
result as long as we can say that that
epsilon is relatively small right
because we're doing learning we have a
very precise notion of how small we want
that epsilon to be we don't really care
about the optimization problem right we
only care about getting beta hat as an
estimate
truth so the point is what we're
actually after is the unknown parameter
theta star so as long as I can guarantee
that this error of this epsilon in our
optimization is smaller than the sort of
statistical error and estimating the
parameter whether it's a vector or
matrix then this is a useful result and
essentially under many cases what we can
show is that what you'll get is
geometric convergence quickly to a
radius that's little oak of the typical
statistical error in your problem all
right so I think that's an interesting
sort of thing to take away because if
you're an optimization theorists you
might say oh what are you doing you're
only giving me some you know you're
actually solving the problem you're only
solving it up to some tolerance but if
you're in learning then this is exactly
the kind of result you want because you
didn't care about the optimization
problem to start what you care about was
the thing you're trying to ask to make
it so there's a sense that this result
is saying well you don't have to over
optimize there's no point to optimizing
beyond sort of the inherent noise and
learning problem okay so how much time
okay
so this is a general resultant and then
the remainder let me just sort of time
packet in one particular case you can do
it from any problems you can do it from
matrix completion you're doing from
matrix decomposition or robust PCA but
let me unpack it in the sort of easiest
case which is sparse regression if I do
that you'll get sort of a sense a more
precise sense of what these terms mean
okay so we did sparse regression I'm
going to do it with the random design so
it means what you're going to be
observing our excise that are random
they're not from the identities who are
not random projections they might have a
you know a very ill conditioned matrix
Sigma but you're giving standard sort of
linear observations and we're going to
solve we could solve the lasoo with the
quadratic loss or we can do it generally
with any kind of a log-linear loss so
this would include logistic regression
or exponential but small regression
things like that now what our theorem
says in this case it says that if you
belong to the truth belongs to an L cube
all just a fancy way of measuring how
sparse you are if u is 0 just means that
your remarks parts you have a fixed
number of nonzero code q is between 0
and 1 it means that your coefficients
you might have many non zeros with a
decays fast right so what our theorem
says is it will be a contraction factor
will be tracked times some quantity this
blue quantity plus x statistical errors
right so this is what we care about in
learning and so what matters is that we
can show you that this term here is
small that it's got to be small so that
we're saying we're going quickly out to
a radius that's as well as you can
expect to estimate from the statistical
you okay so here's precise form that the
attraction coefficient is 1 minus the
inverse condition number of this matrix
so that matrix could be very bad in
condition this means the convergence
will get slower and we'll see the theory
the predictions of the theory are
consistent and this pre factor here what
is it well in the case Q equals 0 it's
the number of nonzero coefficients times
the log of the dimension over N right so
this quantity here is exactly what's
known as the minimax rate there's no
algorithm that can estimate a sparse
vector to accuracy greater than this
based on n measurements so no no
algorithm can do anything interesting
unless this thing is little over 1 and
when this thing is little o of 1 our
algorithm converges very quickly to
something that seems smaller than that
that error right so that's that's sort
of a good kind of guarantee um right so
let's go back to these pictures let me
just sort of illustrate what this
theorem means in pictures like some
pictures we have 5,000 10,000 20,000
these are sizes of the vector and we're
using the fixed sample size and the
convergence rate is he came right that's
consistent with these pictures or the
this theorem because this theorem if the
sample size expects indeed grows this
term gets larger as this gets larger if
the attraction factor goes towards one
and you get slower convergence right so
if you trust the theory then what you'd
say well if I actually increase the
sample size as a function of dimension
to keep this ratio constant then the
convergence rate should stay constant to
right so let's do that experiment will
do the same sequence of problems except
this time i adjust the sample size i
essentially give you log D more samples
as you go from 5,000
2000 so roughly speaking there's just a
certain constant factor inflation this
is the same data this repotted on
different axes now you see the
convergence rates are identical right
there's a sense that the theory is not
only sort of qualitatively correct it's
actually in some sense in this case
quantitatively accurate what else does
the theory predict well it also predicts
that if you use matrices that have very
bad condition numbers if you do
compressed sensing and you use the
identity then you're going to get a very
very fast rate but you start making your
major Taylor conditioning you'll still
get convergence but the rate should slow
down and that also happens so here this
is the case of compressed sensing this
is a kind of correlation or dependence
parameter as this increases convergence
lows if you make it to correlated and
use a constant step size eventually will
fail to converge but that's again
consistent with what the theories same
okay so fundamentally increase it
triggers to learn about five or ten well
this is a fix this is not a random
quantity it's a fixed quantities you're
sampling from so you can choose that to
be whatever you want if you did
compressed sensing you choose this to be
the identity and then this ratio here is
one there's an eight they're probably
the eight should be not innate probably
get to but the point is that you've you
know many problems that we think about
in learning but we don't have things
that are close to us on
you have things where their be fair bit
of dependence between your covariance
your predictors and this is saying that
you're going to pay a price it's not
saying that everything breaks down says
you'll pay a price because this
condition number will get worse but as
long as your sample sizes sort of
reasonable you'll still get convergence
so that's sort of trade off we pay a
price for core conditioning but it's not
faith that's right I understand a
program tricky because if if you're in
the situation where you had wherever the
hydrogen you're going to the right or
ten space yes and that's not the maximum
eigen values in the reinforcements
that's the million a dimensional case
and the cleveland show sported wash out
all the small because they're small
sorry where's the four that you talking
about the example you mentioned the web
of netflix for 10 or 50 sorry I below
the national this is linear regression
we can netflix those different
conditions so let me show you that this
is for linear regression sparse factors
let's do low rank matrices it's a low
rank matrices um a sort of general model
that includes matrix completion is a
special case is that there's some fixed
matrices X I and what you observe are
tracing your products of X I with some
low rank matrix data star do matrix
completion X I is zero everywhere except
for one in one position but there many
other models that fall under this lots
of people than I swear on multitask
learning identification of auto
regressive processes many models can be
described like this so let's look at
this semi definite program we have a
quadratic loss with a nuclear and one
constraint let's study the behavior of
the sdp not for exactly will rank
matrices because in practice nothing's
exactly low rank in practice what
happens is you have matrices that have
good tale decay the singular values drop
off fast so if we look at lq balls with
the singular values raised to an
exponent
jus between 0 and 1 it sort of says
you've got a fast drop off if you belong
to launching balls and here's theorem in
that case says that you should take the
sample size C let's just think about
acuity busy roads you take the sample
size of your rank times dimension a
little over sampling factor of log deep
and then what you'll get is there is a
contraction factor the Frobenius norm of
your estimate will contract according to
this kappa plus a little bit of error
and that error has the following forms
depending on model you're looking at
right so if we look at this it's saying
that the extra slop that we're not
optimizing beyond is equal to the rank
times the dimension over the sample size
and that's exactly the sort of crash
rate that's again mimics optimal if you
discount degrees of freedom in a rank
our matrix thats d by d has roughly r
times t breeze it for you so this again
is an optimal rate and it's actually
optimal for all cute right so it's same
flavored we're contracting very fast
we're not actually solving the sdp but
we're soaking into an accuracy that's
that's of the order or smaller than the
statistical error so for learning person
that's good enough you might as well
stop there anyway okay so we saw some of
these plots this is for matrix
regression said the matrix completion
but you see the same kinds of curves
reffing speaking that's nature simply
shouldn't be looked at that already okay
so um let's wrap up so he ma spoke about
matrix decomposition problems or robust
PCA neither problems where you have a
sum of a nuclear norm plus a l1 more on
the matrix elements if you study these
in the noisy setting right you don't
assume that the matrix is exactly will
rank plus sparse using that's
approximately low rank plus
approximately sparse plus noise then
it's reasonable to use an estimated it's
worth allows for
yes and you can again get sort of
similar rates if you just look at a
simple algorithm let's say the
alternates between soft fresh holding on
the sparse park and singular value
thresholded on the low rank part that
will converge again very quickly
geometrically fast up to essentially in
statistical error of the problem the
other problem that i mentioned bullying
low that these kinds of properties still
holds for non-convex problems and so
even though in this case you don't know
that there's a unique global optimum
what the theory tells you is they've all
the optimum global or local or a ball
with in a very small ball so any sort of
reasonable algorithm my projected
gradient will go very fast to that ball
and so basically you're solving a non
convex problem so it's non convex but
because it's coming from the learning
setting it's not badly on convex and so
I think that's sort of an interesting
direction to explore to see if other non
convex problems have similar kinds of
favorable structure ok so what I thought
about its projected gradient it's a
simple algorithm that works for lots of
high dimensional problems these notions
of restricted strong convexity and
smoothness or useful I think it's
interesting to be able to solve
optimization problems up to statistical
precision but not beyond there's no need
to sort of optimize the noise I think
these notions are interesting because
they probably are useful for not just
projected Grady's but there's tons of
other algorithms online algorithms dual
primal dual methods i think it probably
would be possible also to say things for
four different kinds of algorithms
here's a few
and the fence pretty much
but what happens yeah I don't have the
sensible well what's the Bill of the
regular army if you're working in a
regime where you have fewer samples and
ambient dimension you can't do anything
unless you have low dimensional
structure so all these regular risers
are trying to enforce some kind of low
dimensional structure all right so
nuclear norm is trying to say that
you're either low rank for your well
approximated by low rank and this kind
of assumption is one way of formalizing
effort that these are matrices that are
very nicely approximated by low rank
matrices alright so the learning point
of view when you asked to me you you'd
like to estimate in such a way that
you're enforcing that law international
structure that's the only way you have
hope to say any process is what is the
role of regular
okay sorry so the role is exactly in
this picture here that I began with
right so the high dimensional problem
your loss function always looks like
this right it's always nicely curved in
some directions and very bad completely
flat in others if you've ever build a
flat direction you can't say anything
there's no way you get when your
convergence so the whole point of the
regularizer either for statistics or
organization is to show that the error
either the statistical error or the
optimization area you always wanted line
in curve directions and when you have a
decomposable regularizer whether it's
Eleanor nuclear norm what you proved
that it never it never goes to the bad
space that's that's why the attraction
team loss and regular Iser is powerful
yo station on corner Shalom is not gonna
vote for all right um it's a very good
question no in general this is greatly
recent work so for non-convex problems
the intuition is that non-congress
problems are sort of like this except in
this direction they're not flat they
have saddles they start going down again
but again if you can construct a
regularizer that interacts with the
blossoms you don't go in the downward
part of the saddle then it's as if you
are a mean a kind of convex problem so
clothings work exploits a lot of
structure she looks at missing data and
she constructs a particular estimator
that's non convex we don't know how
general is I think will be interesting
to think if you look at am algorithms
for other problems with regularization
can you somehow to show there you can
other non convex to not badly so but the
answer is I don't know what do this
condition all the
conditions price for that um well
essentially I suspected as soon as you
have that each other yeah so something
like a strong convexity you don't
actually need is something quite as
strong as this this implies that all
local and up and global solutions are
within some radius it depends on the
size of this and the size of this and
the relation between these two norms so
you're right so the name of the game is
provide enta fying that in showing that
this kind of condition holds for
different problems but yeah I don't
really know these interesting questions
as we last two questions
I'm assuming sort of completely misses
random as you mean this sample entries
this is a poster here that's talking
about more general sampling which I
agrees is interesting because in
practice if you watch one movie you're
more likely to watch other movies at the
related title so that's interesting the
result all results I put our kind of
under the naive we love for weighted
sampling but it's iid it's not dependent
sent me so what's the distinction is
completely at random in this case yes so
each of the properties you mentioned the
goal is sort of to minimize the error
between predicted parameters and true
parameters yes how does the problem
change by now the goal is instead to
infer the support prank so that's an
interesting question there right so this
is very much tailored towards these
kinds of results in terms of for
venience or l2 you're asking if I the
sparse vector if I'd a low rank matrix
do I get the support there you need
slightly different conditions that are
somewhat stronger than this you need
these kinds of mutual coherence or
represent ability conditions because it
would be interesting to also think about
theory for optimization in those cases
in some cases you can use this you can
show that your algorithm stays at a
small support at all iterations that
somehow you're done because the problem
will be strong
on the mound and support but part of the
challenge here is that this is a global
resolve this alts with iteration zero so
it's not just using a support recovery
analysis</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>