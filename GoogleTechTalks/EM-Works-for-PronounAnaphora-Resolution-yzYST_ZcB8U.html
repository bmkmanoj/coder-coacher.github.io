<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>EM Works for Pronoun-Anaphora Resolution | Coder Coacher - Coaching Coders</title><meta content="EM Works for Pronoun-Anaphora Resolution - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>EM Works for Pronoun-Anaphora Resolution</b></h2><h5 class="post__date">2008-10-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yzYST_ZcB8U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have Eugene charniak here today and
I'll give a little story
we don't enough one another for 25 years
in 1982 when I arrived at Yale as a
fresh graduate student I had a choice
between working with Roger shank natural
language processing or drew McDermott on
space and time and I'd really come to
Yale to work with drew but natural
language is pretty cool but I read his
thesis from MIT and got depressed Sally
told Fred that she bought Jack a kite
for his birthday present and Fred said
Jack already has a kite
he will make you take it back
and so I started building parsers and
thinking about how to do that using the
idiosyncratic kinds of parsers that they
had at Yale I was like god this is a
hard problem
I was hanging around with a book which
was going to eclipse the time and and he
hired me to come back to Brad with him
at Brown I thought he was really cool
because and and he was working on
message passing or marker cast mm-hmm he
wasn't really happy with them at the
time and there's this new thing going
around the beautiful was working on
called probabilistic graphical models
and Eugene was one of the first people
to start really embrace that and start
pushing hard on well actually in large
part Glenn was a major influence on me
at that time and Eugene wrote a very
famous paper or at least famous in the
annals of AI and it has some silly title
I can't remember what it was but but he
had a great title it was called pearl
for swine which
unit pearls view of graphical models for
the rest of us and it's still
circulating and lots of people still use
it as their text about seven or eight
years later Eugene was chair of the
department and lo and behold he didn't
get bogged down and all the minutia or
somehow he managed to separate time and
that was the time where he wrote a book
called statistical school natural
language processing physical language
learning language yes which is a classic
today and probably many of you have read
it and he continues to this day to be
the consummate hacker I don't know where
you picked it up probably from MIT but I
can never stop well there's the on the
list machine or Texas Instruments
machines or weave programs in C++ and
and today's going to talk to us about
some of his latest work which I'm sure
he has tacked and built the systems
himself mostly
without further ado Eugene okay well you
will very shortly thank you
but Glenn wasn't it you who told me
about about graphical models
okay well whatever okay I'm gonna be
talking about
pranaam '''l an afro resolution and in
particular unsupervised using excuse me
AM or X to the expectation maximization
algorithm this is joint work with Mika
Elsner a PhD student at Brown excuse me
the previous work on pronoun and Afra is
huge I'm just going to mention a few
pieces that have directly inspired this
work some old work at Brown with new ug
John Hale and I this was back in
nineteen ninety was on this problem some
very recent work at Berkeley was the was
the immediate sort of God that caused me
to start working on this and after it
was essentially done Myka pointed out or
discovered that there was in fact some
previous work that was in many respects
quite similar to what we have done and
this is by cherien Bergsma I'm not going
to talk about it I you know I could if
you're interested at the end I would be
happy to go through the differences you
know why but I won't okay
so an afro is the process of a piece of
text referring back to something
mentioned earlier in the text okay
and so an afro resolution is determining
what that something is
so in Alice fed the dog he wagged his
tail the he refers back to the dog and
which is said to be the antecedent of he
in the literature there are two terms
that are sometimes used interchangeably
but
less and less so one is to say that
something is a referent and the other is
to say that it's an antecedent the term
referent has been taken over by the
logicians and so the referent of the dog
or let's say the referent of Eugene
charniak is this thing right here it's
the thing in the world okay within a
piece of text so text doesn't have
reference okay it's just there's just
the text there and so instead we talk
about an Afra and antecedents which are
just other pieces of text yes that's
right sure except now I'm doing it
statistically and the hard problems like
the one that I proposed I'm just writing
off as too difficult for the time being
okay so we're going to be talking about
pronouns here and in particular we're
just gonna be talking about personal
pronouns personal pronouns are I you he
she if they mine is hers etc we're not
dealing with there are many other kinds
of pronouns we're not dealing for
example with demonstrative pronouns this
or that we're not dealing with
interrogative pronouns the who ate the
cookie okay or several other types
personal pronouns are by far and away
the most difficult / most common and so
that's typically when people talk about
pronoun and Afra that this is what
they're talking about personal pronouns
come in several types when they appear
in subject position there are things
like I he and she so I ate the I ate the
cookie when they appear in object
position then they there was a different
class of pronouns objective pronoun so
Fred welcomed me to San Francisco okay
but I welcomed him
to San Francisco so he and him me and
him are objective pronouns there are
possessive pronouns my me is hers
reflexive pronouns myself himself etc
the important thing to notice that in
these cases the type of the pronoun is
determined not by the antecedent who the
pronoun refers back to but instead just
by the syntactic role it plays in the
sentence and thus it doesn't give any
clues for who the antecedent is and thus
we're essentially just going to
completely ignore it as far as this talk
is concerned there are things however
that obviously are correlated in English
with the antecedent the one is the
pronoun number singular versus plural I
versus we it versus they second property
is person pronouns come in first second
or third person and that is also
determined by the who the antecedent is
gender okay
in English we have semantics gender ie
the gender is somewhat related to the
meaning of the object so we know that
dogs are more likely to be he than then
probably she possibly it things like
that whereas Alice is almost surely
feminine as opposed to pronoun types
these are definitely determined by the
antecedent not by the syntactic role a
huge complicating factor here is that
many pronouns are non anaphoric ie there
is no antecedent in the text and this
can be for two possible reasons one is
the pronoun doesn't have a referent okay
and that occurs for example there's a
class of pronouns called whether it's as
in it is raining so it's used in when
you're talking about the weather is
sort of I don't know it is raining means
what the atmosphere the weather is
raining I don't know but it's not
anaphoric there are things like other
things that don't have antecedents are
things like the it in ok so Alice got
married it is unfortunate that her
relatives do not like him be it in it is
unfortunate definitely has no referent
ok now there are non anaphoric pronouns
that do have reference reference and the
here the him at the end
there's no antecedent for the him
there's no piece of text that talks
about to him but there's only there is a
there is someone who always got married
to and whoever that is that's the
reference ok we are not going to
distinguish in this talk between
pronouns but we're just going to say
either pronouns or anaphoric or non
anaphoric we're not going to distinguish
between why they're non anaphoric all we
have to do is say no this pronoun does
not have an antecedent and I'm not going
to link it up to anything
yeah forget it that I would say that's
non I would say that's not anaphoric
because if if you said Alice got married
to Bill well actually no that's still
not it gets messy anything that
difficult we're just gonna get wrong
I mean wait until you see the results
the results here are very low all I'm
going to claim is is that that they're
better than anyone elses ok this is a
very difficult problem people have this
idea that this is an easy problem that
you just pick the last-mentioned object
of the you know and that's it forget it
that does about 25% if you do the
last-mentioned object of the same gender
that does a bit better but it's a very
hard problem
so we're gonna be talking about learning
to do this task without any marked up
data and the technique we really use is
the expectation maximization algorithm
and to use eeehm you need what is called
a generative model which is a
probabilistic model the sines
probability - all possible model inputs
so in our case the inputs are sync are
simply a sequence of pronouns okay and
we have to assign a probability to those
pronouns based upon the possible and
antecedents and so that's basically what
our input is generally when you talk
about a generative model you talk about
it generating the entire text in our
case the only piece of the text we care
about are the pronouns so yes there's
something that generates all the rest
but we don't have to talk about it here
because I'm assuming that that has no
influence on pronoun and Afra so here is
a very simple generative pronoun model
that's pretty much the way this thing
works and if you want to know some of
the dirt you know more precisely I could
talk about it again but this is the
model that I'm going to assume for this
talk even though we're about to generate
a pronoun we first guess is there an
antecedent with some fixed probability
okay so this is the typical quote
generative story we omit so we flip a
coin that comes out heads we say there's
an antecedent it becomes our tails we
say there isn't and then we're done if
there is an antecedent of course that's
not really not 50/50 but if there is an
antecedent we guess which one of the
previous noun phrases is the mo is the
closest antecedent because very often
there could be more than one noun phrase
in the text we will just be that refers
to the same
Jack we're just gonna be concerned with
the closest one and this is based upon
the position of the antecedent and I
will go into more detail about that
later once we now have made a guess
about what the antecedent is now base up
on that antecedent we guess what if the
pronoun will be singular or plural we
guess if the pronoun we guess the
pronouns gender based on the antecedent
and we also guess the number of the
pronoun for a second or third and then
we choose the antecedent that makes the
pronoun most likely that's the that's
the basic idea so for those of you who
care about the math so this is sort of a
very simple version of we weren't argh
max and I forgot to put in that should
be arc max I so of all the possible
antecedents I we look for the one such
that is most that makes P of a equals I
given the article highest okay
ie we want the most probable and that's
equivalent in this formulation to Arg
max I of P of a given AI given the
previous context times the P of gender
given AI times the P of person given AI
times P of number given AI the
essentially I'm just repeating now in
math form what I said before in in just
a generative model now the second
equation is more problematic with this
equation this is this encodes an
assumption I'm making that the problem
that this first piece when I said
probability of a of AI given previous in
the second line I'm now saying that what
that really is is probability of a
equals AI given all the information
about the previous prone
and noun phrases okay or to put it
another way if the antecedent and I'm
assuming that this is equal to or
approximated by the probability that a
equals I've just given the information
about the antecedent I okay so I'm
assuming that the probability that it
refers to Jorge is independent of what
other individuals are sitting around
Jorge
now this this is clearly false and even
worse for you statistics gurus it means
that when I use the maximum likelihood
estimator to estimate these
probabilities it's going to be biased
inconsistent and bad for your health
okay the problem is trying to fix this
ain't easy and it's not high in my
agenda yes
that just means previous stuff in the
article yes I just wanted to make clear
that it doesn't include anything after
the pronoun itself or the or in fact the
pronoun then okay so that's the basic
equations and the warning about the
estimator hmm
I put that in because Michael Jordan was
very at Berkeley was very unhappy that I
didn't be more explicit about this I'm
going to assume that the input is
completely parsed I'm going to further
assume that any that the antecedent of a
pronoun is some noun phrase in the
current or previous two sentences this
is right about 95% of the time the other
5% some are some of its what's called
guitarra which means that the antecedent
actually occurs after the pronoun more
common is that the antecedent is further
back than 2 then the two previous
sentences but at again at this point
that's the least of our concerns if we
could get these 95% right brother would
we be happy so here's a simplified
example of how this might work
so suppose we so the top table we say if
the position of the antecedent is the
current sentence then the probability
that this is the antecedent is 0.2 if
it's in the previous sentence it's 0.15
if it's two sentences back it's 0.1 oh
okay notice that these numbers don't sum
to one that has a lot to do with that
assumption I made that I said made
everything biased and inconsistent but
now I know you look unhappy
so
I'm not sure what that tables are
problem yeah there's something about
conditioning here that's kind of
bothering yeah yeah yes
okay so it's like probability so
probability that this is the antecedent
of probability that so because you
started with these gently it's sort of
funny it's not right because it's not
that I say no it's a generative model
it's just biased and inconsistent well
the biased part
so can you repeat the point about that
not adding up
okay suppose that it were true that all
an decedent's are in the previous two
sentences suppose English were designed
that way okay and suppose that there
were quite literally three possible
antecedents one in the current sentence
one one back and one two back and
furthermore suppose that this pronoun
has an antecedent it's not non anaphoric
under those conditions these numbers
should add up to 1 ok they won't they're
gonna add up according to the numbers I
made up to approximately 0.45 and that's
because of this assumption I'm making
that everyone the pro that everyone's
probability of being the antecedent is
independent of everyone else around him
or her if I hadn't made that assumption
and I could actually gather these
probabilities then and then they would
sum to one but I but I have to make this
assumption because obviously the
yeah so the reason I have kind of a
little bit below it is because they you
could have gotten around this problem
different way she is to have some kind
of a a factored model where you've
conditioned on the past decisions I
could say well in addition all the
antecedent decisions for the past
pronouns in that case you can have an
model that is not deficient
yes yeah that's
they would be okay don't take my word
well I I don't see why okay so
furthermore suppose that we have the
following probabilities namely the word
Alice is going to generate a masculine
pronoun one time in a hundred a feminine
pronoun 92 times in a hundred and a
neuter pronoun seven times in a hundred
they don't add up to one subject as of
rounding errors those three numbers
should add up and they do
similarly dog is masculine and these by
the way of real numbers these are the
numbers that as opposed to the first set
these are numbers that might um program
dog is masculine 47% feminine 11% over
42% he of course is always missing
so in Ellis fed the dog he ate the food
okay we would look and say well it might
be he might be else or the dog in both
cases they are in the sentence one back
and so the probability mantasy
in position is 0.15 genders however are
quite different and so the probability
that the that Alice will generate a
fifth a masculine pronoun is 0.01 while
the probability that the dog will
generate a nest on proton is 0.47 and
the result is that were much more likely
to generate the dog to generate he from
the dog than from Alice that's a very
simple
question what's the next week man looks
like what's with us those that's just
the product of those two numbers
Oh
okay so I said we've got these numbers
where do they come from well let's start
I'm not I'm going to assume that not
everyone that here knows him if that's
false excuse me
the sentences were parsed but you're not
using any of the carpentry as feature to
disunited to extract I'm using to
attract the phrases I'm using it to know
the parts of speech
oh I'm also that's right I actually
haven't talked about reflexive yeah no
it does not have the full panoply of but
it does know that something an object
position
that's all
okay so let's talk about where these
numbers might come from so suppose we
have a lot of texts with every pronoun
labeled with examples and we wanted to
know given this text what we wanted to
estimate the probability that Alice
would generate a feminine pronoun well
what we would do is we would say take
the number of times the feminine pronoun
has Alice's fantasy divided by the
number of times any pronoun is simply to
estimate the probabilities that a plural
pronoun has a singular antecedent we
would you know do the same thing number
of times a floral pronoun has a singular
antecedent divided by the number of
times any pronoun
these probabilities if we had supervised
data okay so we hire 10,000 graduate
students occasionally these grad
students will be uncertain about the
correct indices so what we're going to
do is we if they are uncertain and it's
about 5% of the time usually what do we
do in those cases and so what we're
going to do is we're going to allow the
graduate student to indicate what the
possibilities are along with the
students estimate of the probability
that each one of those is correct now in
the case where when the student is sure
and we say well was a yes
well here for a if it's point eight now
instead of giving one we give it and so
that's the basic idea and iam just takes
this two is the ultimate generalization
so instead of having a person we're just
we're just going to assign arbitrary but
typically closely similar probabilities
for all the cases so in our case we're
gonna say that initially every pronoun
and every embassy has a one-third
one-third of generating
masculine-feminine and then we can use
our if we have these numbers we can now
use our equation cute what's the
probability that this thing and just
like in the graduates here we're gonna
say well these fractional couples and
then at the end we're going to normalize
them so in each iteration DM first
computes the expectation of how often
the data is generated using a system
very feminine and then it maximizes the
probability the data by resetting the
model parameters to agree with those
expectations in our case we're just
going to use the maximum likelihood
estimator which is more or less obvious
thing with dividing how often it
happened by how anything and so that's
now one thing that sort of funny here is
I said initially all of the antecedents
are going to have exactly the same
probability and that makes you think
well if they're all equal
how will anything ever change why will
the probabilities just sort of always
stay equal and they don't and what I'm
going to do is try to give you some
intuition here considers the probability
of plural pronoun given in plural and
initially given that there are two
possibilities
a singular and plural though both be 50%
now let's think about the situation
where we see a singular pronoun and we
ask what's the probability that say the
nearest antecedent to it will be plural
the answer is plenty less than average
because presumably one of the previous
non phrases is going to be sassy
singular
right since there's a singular pronoun
and so each one now is a slightly less
chance of sorrow simple if we see a
plural pronoun than the probability of
seeing of the nearby one of seeing a
nearby plural is definitely higher so
yes the e/m initially in the first
iteration is going to assign them all
equal but the number of them won't be
there will be slightly more plural noun
nouns near a plural pronoun than near a
singular pronoun and so the total count
even though each individual count is
equal with all its competitors the total
number of councils going for that
combination is going to be M is going to
pick up on these very very clever about
this okay so that's the intuition for
why things are going to move off
completely
here are in fact the numbers that iam
comes up with so in particular this is
the probability of seeing a singular
pronoun given some antecedent so if the
antecedent itself is singular then it's
0.93 if it's plural it's point o4 and if
it's neither singular not plural I can
get something other than a noun might be
a perm sometimes then it's point seven
four
okay notice in particular that plural a
plural antecedent has a nonzero chance
of producing a singular pronoun and that
is indeed correct okay
something will like they can refer to
IBM where they is plural IBM the
singular okay sorry that's the other
that's the other way yeah but I'm
thinking about ah let's see
no no so suppose that so here's my
program is always looking for the most
recent antecedent
suppose we introduced IBM then we talked
about they and then we talked about IBM
again
so okay so what I'm looking for is a
singular antecedent a plural and deceit
and a
I should have done this don't the other
way is there I have you know a good
example in hand I don't have a good
example but it does happen this way it
was one thinking about those numbers I
could think of you know I can't think of
one either it's probably the number of
point oh four is probably too high you
know but I'm sure they I'm sure this
appears he doesn't like them
great possible that mistakes
not that many but yes okay
probability of gender given hannassey so
if the word call it says 96% of the time
it's going to generate a masculine
pronoun almost never a pronoun and never
a feminine or neuter okay
Paul love now it's feminine okay so but
clearly
it's been a lot of interest in genders
of pigs recently according to my program
Pink's are about 45% of masculine 17%
feminine and 30 without what corpus well
this is this is the North American news
corpus from LDC includes Charlotte's Web
no however notice that piggy is almost
always feminine I assume someone here
can guess why
right Miss Piggy
so most of the time that piggy is
mentioned to dismiss the kimchi seven
Walmart is quite clearly neuter so you
know it makes mistakes
but most of the time it gets things
right and you can also see that the
hedges expense it's never quite as
here's another one of the distributions
and what p.m. comes up with this is the
person of the pronoun given the person
of the antecedent and in this case I
also condition on whether the antecedent
is quoted or not and you'll see why so
in the first set here neither the
antecedent nor the pronoun is quoted and
what you observe is look at the diagonal
first goes with first second those in
seconds
7% to the third grow so that's pretty
intuitive that's what you can expect
okay notice however that if the
antecedent is quoted but the pronoun is
not in a quotation that goes away now
everyone goes to a third person so that
will probably go with a third person
someone's talking and the person saying
mister mr. Jones and then in the
unquoted section Jones so there you see
a very different phenomena where
basically everyone wants to be third
person outside and again perfectly
the last of these distributions is the
most complicated and that is the one the
probability of antecedent given context
that was the first list so remember
first we guessed the antecedent and then
we guessed based on the indices gender
and number person so probability in the
antecedent is conditioned on and so many
things okay so the first thing we looked
at before was what sentence other things
that influences include the syntactic
well it's well known that subjects of
sentences are more likely antecedents
and objects and it's generally assumed
that objects in turn are more likely
than the other possible roles so for
example if Alice was talking to Mary she
and she is more likely to be Alice
everything else okay
centering theory and all sorts of other
theories all of which have this is a
basic pendant and sure enough it's true
another useful feature is whether the
antecedent isn't is itself a name a
pronoun or a common man intuitively
common nouns are less likely to be
antecedents and neither pronouns or
proper nouns so all its whole by
distribution is conditioned to hug three
possible sentences three possible
syntactic roles three possible
antecedent types
six possible bins of where the
antecedent occurs within the sentence
three possible positions for the pronoun
and three possible pronoun types and
when you multiply all those together
we're giving p.m. about 2,500 parameters
and if your experience with VM has been
anything like mine this sounds lethal
yeah sure do something
amazingly this already showed you some
other situations this now when you've
got 2,500 parameters it's almost
impossible look at any one and get my
intuition okay so what I've done here is
let's consider the first row here I'm
looking at the probability of that
particular antecedent is the antecedent
given the weather the antecedent itself
is a pronoun a proper noun they're
calming down now notice that these
numbers do not exist in the system that
be in fact what I've done is I've let
everything else vary so the first of
these pronouns 0.09 what that says is
that if you consider all probabilities
in this huge array in which the
antecedent type is a pronoun and you
take the geometric mean the number you
get out is 0.09 whereas if you did this
for proper noun the number you'd get out
is 0.06 everyone
in commission these are averages over
many many different cases to try to give
some intuition about what the trends are
and as you see in this case I think the
trends are mainly pronouns are much more
likely to generate forthcoming pronouns
then than other things and that's
typically because if something's been
simply proper nouns are more common with
common nouns and intensities I think
that worked out so good it looks like I
made up the numbers
similarly for our antecedent word
position word position is measured
starting from the pronoun in the same
sentence you go backwards for it's the
start of the sentence of the words
closest to it are in position 0 being 0
whereas for previous sentences you start
at the beginning and work your way whoa
and sure enough the closest are more
likely than those slightly further away
the only case where I found where things
don't completely match my intuition is
in the last one sure enough some things
in subject position really are more
likely to be generated from them but
objects oh I've got these reversed sorry
that this is a bug the number for others
should be 0.05 and the number for object
should be 0.04 in fact objects are
slightly less likely than other however
the significance of this is small
really really so what I mean I know it
will be computationally a bit of a
problem but I would rather try to
marginalize over those other things then
maybe this we want the power thing then
computers geometric means because I
don't know what they mean they are a
little bit tricky right because if
there's something is one that probably
one of the probabilities is small or not
the others yeah I don't really care I
mean these are just the master - okay I
think okay sorry I just have to say as
illustration goes yeah okay so probably
ignored none anaphoric pronouns things
are very very simple basically there's a
small probability of generating a
pronoun any without any reference and so
basically what this means is if none of
the preceding pronouns look very good
semi-reasonable very and so and there's
the question of who to evaluate against
what I did was I went to the corpora
news group or email group and asked for
all the programs that they knew that are
on the web that did pronoun nefra and I
got this these four I subsequently
discovered one other but it didn't
really work properly and so I'm just
sticking with these four and so the
question is how well does my program
compare to these programs so how are we
going to score so every pronoun either
has an antecedent or not if it does not
it is only count that is correct if the
model also decides that there is no
antecedent okay so that's I think fairly
straightforward if there is an
antecedent then for it to be correct it
must be put in the set from which it
came in the gold data so intuitively
well actually you'll see you'll see this
in a second and this scoring metric is
known in the literature as mid coughs of
resolution etiquette this is what he
calls it and this is his metric so
here's an example alice fed the dog he
wagged his tail so in the gold standard
we have Alice in one group the dog he
and his in another group okay now we
removed the he and the his and we remove
all the pronouns from all the groups and
we're assuming that every group has some
non pranaam '''l noun phrase left in it
which is a pretty good assumption so all
the groups now all the sets are
well-defined and what you now want to do
is put back the pronouns back into the
sets
from which they came and if it goes back
in the right set as in here then it's
correct otherwise it's wrong so in this
case everything is correct now notice
that here is the funny example suppose
that we put Alice in the dog in both in
one set and he and his in a separate set
now in one sense the his could be
claimed to be correct after all it is Co
referent with the he okay but according
to our metric it's going to be scored
wrong because this set that it has to go
into is the one defined by the dog
wasn't put there it's wrong similarly if
he and his were put in the set with
Alice they would both be wrong okay so
is that reasonably clear how the metric
works
they go together so those are anaphoric
but they're there they're not pronouns
and so because so far I'm only looking
at pronouns oh it doesn't this it's this
the metric I have just described only
works for the case of pronouns it does
not work for the general situation which
is much more complicated okay
fortunately for pronouns it's fairly
straightforward but no this this metric
cannot be used for the general case but
it is what is used for pronouns
the you mean for the not for the
complete case beyond pronouns I don't
know but at one point that's what I was
doing and people said that's not the
standard okay I don't know I don't have
any strong intuitions about what the
right thing to do is in that case is and
as far as I can tell the met the two
metrics go up and down together with the
case with your version being about four
to five percent higher because of the
case where two pronouns can go together
but half the wrong thing further back
okay so but they go up in tandem I've
yet to find a case where you know well
that's not true
but anyway I have taken what has been
used previously because it's been used
previously rather than making up a new
one okay so the unlabeled data was about
a half billion words of parsed North
American news text from LDC well
actually yep the North American news
text is from LDC we parsed it and you
can get the 50 best parses for all of
that from LDC which we contributed to
them this consists of about 750 thousand
news articles from various newspapers
I missed this one the experimental that
the development data and the test data
is from the work of knew Yugi back in 99
she marked up the sections zero and one
of the penn treebank with pronoun co
referent annotations and we use zero as
a development set and one is the test
set and the numbers i'm reporting our on
the test set which is section one this
has 900 i has eleven hundred and
nineteen personal pronouns of which nine
hundred and six or anaphoric the rest
are non anaphoric and as i said this is
the training data and here are the
results okay as you can see ours at the
bottom quite clearly dominates all of
the rest it's not particularly close the
best of the others was open NLP which
particularly given that notice that i
should mention that bart guitar and open
NLP do complete coreference okay so this
is um in a sense this is slightly unfair
to them because i'm just taking a piece
of what they do and comparing it okay
but I have no other you know things to
compare against and the one program
there that only does pronounces Java rap
and it's very old and not particularly
good to pronouns Java rap only handles
third person and so I only tested it on
third person evaluations I don't know I
had never worked on that work
a little bit there's some reefs you know
some work that people be doing like
Enderman calamy and then I guess piddle
Dominguez and where they've there's some
sense I guess from must its face yeah
yeah yeah so if you they have some
numbers I don't know of course it's not
just pronoun that's right it could
restrict it pronouncing yes yes okay so
the major problem there is maca days
have almost no pronouns yeah no no it's
a perfectly good question so they are
not very good for testing on pronouns
and furthermore most people who report
their full numbers don't report their
pronoun numbers and finally and this is
off the record I don't trust any numbers
that I can't reproduce myself
and well what I'm saying is I just don't
believe all my my my coworkers okay
which it might be healthy but okay okay
good I I used to have the slightly okay
so basically what I just decided I was
going to do was take programs that I
could actually run and these were the
four now notice that Bart I I don't
actually give a number it's less than
forty percent and I think that's just
what that means is is that the BART
system as shipped doesn't work okay
and in particular there seems to be a
problem not with the program but with
the model behind the program it's it's
it's strange and we've written them
but not gotten any answers that we sing
since you parson you know there's this
old rule-based and Siemens method due to
Jerry Hobbs yes you compare it's easy to
equal them oh yeah uh take my word it
will maybe do thirty percent on this
metric okay
maybe no maybe a bit more maybe forty
percent yeah but no forty percent is so
yeah kind of a nice baseline right you
know I got some a little bit of the
information you might those features of
may it might not be interesting
no no no Jerry Hobson's features are in
particular basically what the hobbs
algorithm implements three features one
is it sensitive to constraints and
reflexive secondly it's it likes things
in subject positions okay
and thirdly it likes things that are
closer rather than further away so the
first one is the one you would want to
have yeah that to you already do it I
have a little bit of the reflexive stuff
but not as much as Jerry does so yes a
full-fledged program that captures
current linguistic theory on reflexes
would improve things slightly
okay so and as I say at the top you you
should be aware that these comparisons
are tricky okay all of the programs for
example have different output
conventions and so it's not as if you
just you know take their output put them
through the same scorer and get a number
because you won't and so what we
actually did is Mike my co-author and I
independently wrote scores for each of
these things and then compared our
numbers you know just to make sure we
didn't do anything obviously stupid so
these numbers at least two and two
independent programs came up with the
same set of numbers or in one case they
were half a percent apart and we took
the higher one so we I think that these
numbers are pretty good but this is by
no means the slam-dunk well and so
anyway so the result is that we have
very very competitive program I'm going
to be putting it up on the web
certainly this was submitted to the
European ACL if they accept it it'll go
up on the web then probably go up on the
web sometime about that period I'm
working as I say right now this program
takes in sentences and outputs F
measures okay which is not what most
people want from a program like this so
the essentially the highest thing on my
agenda right now is to change that so it
outputs parses with coreference marked
up and that's it thank you
yeah looks like this has a nice side
effect of actually deciding to jump at
the top where Sports report intro saying
yes
if you kind of you have some pre likely
currents patterns like say number
possessive usually it's reference
relation mm-hm
and then if you extract all of these
instances from focus right and then you
you get you get dog it's right mm-hmm
and then you you just can't cuz like for
these pronouns you know that gender and
then I just use this distribution the
distribution
is that a benefit well you get probably
the right so do I accept I get them on
more because I'm using every pronoun
here you get like all we want is the
gender information you say here so I'll
just I'll just send you my gender
information okay well what's okay is
there some point to this you can get
these without doing yeah
yes well I okay that's right did you
know
the one thing I do do because its
traditional in iam you know at least
from the e/m work in starting with the
IBM machine translation models is to
start em on a simpler distribution and
use that to initialize and then go on to
the more so I do that and I've never
tried it without it I assume like Mt
you know the IBM one is convex but I be
you know but as soon as you move beyond
IBM one it's no longer convex and you
have local Optima but if you just first
train on IBM one it does very well I
assume the situation is similar
initialize using a uniform distribution
and always jiggered a little bit but
some a it's interesting that yours is
this is asymmetric it's there's no
saddle point there so you don't have to
worry about jiggling not yet the major
feature that I don't have is counting
how often each antecedent has been
mentioned okay and the reason I don't
have it is because this is just pronouns
this doesn't work we run full and pico
reference which is what you need if you
want to have that feature and so in fact
as I said the first thing on my agenda
is to get this program producing useful
output after that I would say the next
thing is to start thinking about a
similar approach to full and pico ref
at which point then I will introduce
that feature as well I mean there are
lots of things that could improve this
program we don't handle cat Afra we
don't handle we the program does not
know that a conjoined NP can be a plural
as I say its knowledge of the syntax of
reflexive z' is very primitive as I also
said it only looks at pronouns to
sentences back if you try to extend it
things actually get worse because it
gets distracted by these extra prone
extra noun phrases way back and
presumably that's because II M is not
giving a sharp enough distribution to
make those sufficiently unlikely but I
haven't really looked at that and that
would be another thing too that would so
there are lots of things ways you can
improve this but the major thing I think
is to make is to go on and handle
full-on picot reference no that's it
it's insensitive basically things
performance improves up to about
iteration 14 and after that it's stable
okay does not seem to over train now
this is going up to about 30
I make no guarantees for what would
happen if you went to 30,000 you know
but certainly from 15 to 30 nothing
nothing happens and it does improve
going up to 15 although at the end it's
the improvement is not statistically
significant but it is an improvement
as far as I can tell but I I mean you
know how these things work you start out
with a very simple model and you add
things and you stop adding when things
don't help okay so as far as I know
everything I presented here helps but
all that says is given the order in
which I implemented them they help
whether if I had implemented in a
different order they would have you know
who knows oh yeah humans are virtually
perfect as I said humans will get a will
agree 95% of the time roughly and the
and then assuming that they disagree in
just one of two possibilities then then
they will get it right
quote-unquote in ninety seven and a half
percent of the time in fact that last
five percent are things like the
following I said that there are cases
where doesn't doesn't really matter
which one you pick or the case is subtle
so one of the examples I was looking at
in the marked-up corpus when something
like this it there were two antecedents
one of a pronoun it one was the XYZ
corporation and the other one was the
XYZ corporation subsidiary subsidiary
okay and the pronoun was just something
about you know they may have some
trouble coming up and presumably if the
company he was having trouble then the
subsidiary might be having trouble and
vice versa you know it wasn't didn't
really matter a lot the carpus decide
new ug when she marked it up said it was
a subsidiary I think that's probably the
intention but who knows that's right no
in fact in fact that was nineteen I mean
that was the walls the remember this is
the Wall Street Journal from I don't
know what year was the tree Bank corpus
done so yes I'm sure they're all out of
business</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>