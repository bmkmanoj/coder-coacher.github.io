<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Net and the City | Coder Coacher - Coaching Coders</title><meta content="Net and the City - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Net and the City</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GhViMkz8qlU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it introduced later Cochrane so we are
from the same one event undergone
college I didn't he did his PhD
chemistry of consciousness right now an
assistant professor in the University of
Missouri onam his research interests are
in the network and rock algorithms
commented organization time series
analysis see relation and modeling I
complexity theory which is here to the
mass of Memphis here he's also held
positions at Northeastern University
artemus knab's knows equinox corporation
MSR Phillips
was that was great in the city what was
the list again Northeastern University
Los Alamos corporation because know what
sometimes known as soulless corporation
a massage that is our some short term
positions yeah thnkx gurgaon so
introduction it's nice to be here first
time in California actually so this is
very strange because I've been in the US
for eight years now okay the talk is
going to be on net of the city this is
ongoing work with collaborators at
Virginia Tech northeastern and Los
Alamos actually I am going to first
motivate why we want to look at City
Nets what those are and then basically
describe how we build our model of City
nets then we I will present some
analysis and how what the structure of
the sitting it looks like and then at
the end present some analysis on attacks
that we have okay so upfront let me just
describe what citynet says we define
what city nights like this no oh I see
so I want that okay all right so when I
say yes when every night when I refer to
a city net what I mean is exactly this I
mean a set of routers the set of routers
end hose and links that are located in a
particular metropolitan area so think of
New York and think of its boundaries but
it's not defined well but let's say i
define the boundary for you and then
think of every router and host and link
that is located inside of that boundary
that's what i mean by the city net again
the connection there that's what i mean
by sitting in it so what we want to do
is to uncover the structure for large
metropolitan areas like new york city
like Chicago and we actually have done
that the reason for doing that is the
following a lot of study has gone into a
lot of you
studied the general structure the
Internet at large structure and also
there's been a lot of study in the
relevant subnetworks for example isp
structure has been steady autonomous
system structure has been studied what
we are doing is looking at the local
structure the geographically local
structure of the internet why do you
want to do that well the internet as as
an infrastructure has grown with humans
and humans have grown the most in cities
so what you want to do is look at how
the infrastructure has grown in dense
areas James both with respect to human
population and in terms of importance
because cities are where the businesses
are cities are where the universities
are cities are where most of the people
are in terms of computer networks in
terms of computer networks what this
will do is facilitate network simulation
studies one you see some aspects of it
where we study the vulnerability or the
robustness of networks what we want to
do is to develop a methodology that is
that anyone having access to you know a
few notes and stuff can create a
structure can create a map of City nets
and then use that for simulations we are
going to release we plan to release
models for the research community so
from what the it will come out of this
research we want to package it and
release it into the network community
because the structure the the knowledge
of City Nets the structural city is not
known okay one thing that I wanted yes
you mean models of data X and you would
leave London we can trees all the data
because some of the data is sensitive so
what we'll do is we'll anonymize and
release it if I what you're asking boy
is mathematical model or is it area we
have what X it's actually data from real
world but we also plan to do research
and you know from that real world data
make mathematical models okay both of
both both but you can to leave all data
that we have because some of it is
actually sensitive n
but we can anonymize in aliso in Rio
later we can anonymize and release so
for example not have any IP addresses
just give them integers all right that
would okay one important distinction
between our model I will describe the
model but before I start our model of
City nets or networks in general for now
but let's go with the signals is a path
view so we don't look at a network as a
graph which is what typically you would
think what we look at a net were going
to say City nets and a model of Sydney I
mean instead of nodes and a set of paths
on that nodes on those nodes and then
you can think of attaching traffic
weights and stuff on that but this is
the crucial difference its parts as
opposed to it's a path you as opposed to
graph you now why do you want to do that
this basically two reasons routing on
the internet is policy-based just
because there is a bath in on the
internet doesn't mean any package will
travel on that path so we believe that
the path view is a more correct view all
right also it has been shown that the
parts on the internet have this dominant
paths phenomena where at steady state
all traffic between this point and some
other point is dominated by a single
path if they even though there are
multiple paths that could go it's
changing slightly because of new
technology and stuff but still internet
shows dominant paths so again when I say
a city net what I mean or a model of a
city net what I mean instead of nodes
and a set of pots over this nose so I
will describe all that rain in a later
slide but roughly speaking this is the
boundary of the city these are nodes
right outside the city so that's what i
call the border nodes be these are the
set of routers are called also our
internal nodes so these route traffic
and these are set of destinations or end
hosts ok that's the basic model ok what
this shows here is that you have a small
cut but we'll come to that but the basic
idea is path instead of graph no it's
not wireless now at this point it's just
and also if you can think of what level
at level 2 level 3 we are looking at the
IP level router level okay people have
done work at the a s level so what is
the AL connectivity okay odd people have
been at even lower level so for example
was the physical connectivity we are
looking at the IP level okay later is it
a good time to ask about the length of
the past typical length of birds I'm
going to get to that good question
matter yeah so length is the two is the
unweighted length in the weighted length
weight is hard so we have not done that
we have some models but it's a weight is
basically traffic or capacity and it's
hard but we have some wall so that will
come come to that so the problem
statement is we want to design a robust
methodology here the either the idea is
in addition to the data we want to
develop a methodology for inferring
realistic path Network views of city
naps okay what do I mean I've described
what path viewers what do I what do I
mean by realistic what I mean by your
lipstick is first of all it should we
define it should be inferred from real
data and secondly even though it's
impossible to have all the routers
there's just too many of them in all end
hosts in your model it should be
realistic from a point of view whatever
metrics you are interested in your your
or whatever matrix common simulation
studies are interested in the model
should have those properties the model
for those metrics should satisfy the
property of real world so our view as I
just mentioned earlier is the router
level view each node think of it as a
router at an end hosts not in in a lot
of different studies each node is an
autonomous system or you can think of
each node as an ISP or you can think of
each node as so this when I say the
router I mean an IP router at the IP
level not at the physical level ok
because logical connectivity could be
different from physical connectivity
there has been work in particular work
by breitbart and I think someone who
will actually add the physical
connectivity but we are not doing that
another thing that we are not interested
in and that is from this is we are not
interested in the transit traffic if you
see all parts start from outside and
inside there could be a path that starts
from outside goes inside comes out
we are not interested in that will not
model that why because we are interested
in how the city is connected to the
outside world and vice versa how the
outside world is going to the to the to
the city so we are ignoring transit
paths okay other thing is we actually
are interested in forward connectivity
and backward connectivity however 50
personal paths have been shown to be
symmetric and because of the way we do
we namely trace routes a backward
connectivity is harder to do so all even
though I'm claiming i will be sort of
implying that it holds for both forward
and backward actually I'm going to carry
it by saying that it's only forward
parts paths okay but if you ask me to
hazard a guess I would say that the
forward and back but would not be much
qualitatively different okay but our
models are not based on backward paths
at all just forward paths obviously the
issue here is that apart from here to
here might be path from A to B might be
much different from path from B to a but
it has but typically it's fifty percent
of the parts are in fact actually
symmetry and lot more are have shared
many other properties okay so some
related work this is a ton of activity
I'm missing a lot of stuff here but I'm
just keep going to give you a flavor of
the kind of work that has been done
different views different via internet
having looked at so the router level
view and this for general internet ours
is only for the cities so there's router
level view you have the autonomous
system view or the ISP view okay also
related to our work in fact a very
crucial part of our work is knowing
which IP address falls in which city so
we are using work that other people have
done on geolocation given an IP address
how do you locate where what is the
geography what is the length logic
latitude and longitude of that people
use different kinds of triangle ation
techniques both in
in open research community and there are
whole companies and we are using some
data from these companies even though we
could in principle do all of their work
and then use that data but we are not we
are using it as a black box geographic
properties of routing hanger so yeah
I'll come to the vaca my digital
m'envoyer over so he used resources and
they are all into all that oh this
geographic properties of routing so if a
packet has to be sent from Mountain View
to New York how does it travel
geographically does it go to Chicago
first there that has been studied that
it has been shown that it's not optimal
but a lot of times it's optimal in terms
of distance again this is ours I we look
upon our work as a part of the bigger
study of understanding the internet and
how it has grown which has a ton of work
in that in particular I note this is the
first work that introduced the shoe that
hinted that the internet graphs our
internet has this power law structure
where you have which is a reason for
efficient routing but also has leads to
vulnerability so that's we look upon our
work as a part of a bigger internet
visual tomography laser is a type of
their part we are not doing any
visualization though and again even
beyond internet there is work on complex
networks a social social networks
internet so on and so forth power
networks so electrical power networks
I've done some work that is related but
it's not part of the stock okay so what
is the basic methodology that we have so
our goal again is to find a path model
so I want to give you a set of nodes in
a set of pots over those nodes for a
given City so let's assume a city like
Chicago for example Chicago is if you
have a specific city in this talk I'll
show Chicago's example okay so I want to
give you that I want to give you a path
model for Chicago so what how do we do
it the first thing we do is find the set
of all IP addresses that lie within
Chicago okay this we do using data sets
so we are actually not doing research in
here you're just using other people's
research now after that what we do is we
sample since we can't run tracers a
basic methodology straight so since we
can't run tracers to all of them we
sample intelligently so that upper trace
route the number of things number of
nodes and edges discovered is improved
okay then we actually traced out from a
lot of different vantage points close to
44 vantage points to every IP in that in
that set that we have in that sample
said that we have we clean data and then
we build a path based model we call it
the bi d model and you sort of saw the
glimpse of it and i'll come back to it
but essentially that's what we do you
have a sample set inside the city you
know all these sources you don't race
off from all these sources to this this
sample set and you get here path base
model and we do analysis after that okay
so the first step was the basically the
IP to geographic mapping this as i
mentioned in an earlier slide it's an
active area of research we are not doing
research in this we just take data from
these are this actually proprietary
later but if you really wanted we could
spend effort into and build our own
system people have done that in the
research community as well so these are
all well-known leaders in their field
what we do is take these three data the
data doesn't match because of obvious
reasons it's not exact we don't know
where the city ends and the sub of
starts one thing we want to know it is
that we want to consider the
metropolitan area because we don't want
to lose out on home customers for
example blocks cut blocks layer to home
customers that's just one so we want to
look at not just not just downtown but
the metropolitan area as well again that
definition is also not clear but
basically what these data give us is for
each IP it gives us which is it gives us
a name which is basically the numeral
city actually they do it for by blocks
but then you have to merge these data
how do you
how do you how do you merge these data
to get one clean data okay the basic the
basic method that we have the basic tool
that we have as have other studies done
is is Trace louts what trace route is is
basically you start from a point gives
you the route from that point to any
other point in the Internet okay so this
is an example of an actual train starts
from one on our data set so you wanted
to find out the path the forward path to
this IP address and this gives you that
you're not using the time information
but you could use time information for
waiting for example what you're not
doing that as of now this is basically
an internal bookkeeping that we have ok
so the trace route is utility that's
been used a lot in network studies it
uses the time-to-live attribute of IP
packets what you do is you say ok the IP
packet should expire in say five steps
so at the fifth hop it expires and then
just by the way the protocols work
hopefully the router will send you a
message saying oh your packet expired so
you know what the fifth router is
essentially they're not really required
to send it but that's why trade routes a
lot of times are not successful in fact
the success rate is only about ten
percent or so ok but that's what you
have to live with so what we have is
yeah now that's fine the translation
remember those two different ways of
doing it with ICMP and yeah me too I am
um yeah okay I don't mention that but we
do all sorts of optimization is the
point of this slide actually there's
lots of different ways to do it we
choose ICMP because you to be raising
more flags so what you have to be
careful about trace routes is that first
of all they can take a long a long time
so if without our optimization which I
am not mentioning here so 100,000 trace
route which is a typical number for a
city will take about two to six weeks
depending it depends on how many
successes er we do trace out we do for
example we don't send three package
which is normally what they do we
remember you're not interested in you
know very fine structure because you
know it's not possible to get all the
it's more important to get more
structure than to get every single small
one of them so we do these optimizations
part of that is using ICMP packets okay
and then we reduce it to a reasonable
numbers like one day 400,000 trade-offs
essentially so that the utility that we
have is basically a utility for
collecting trace outs and it has tunable
options including that but the way we do
it is always used ICMP it works across
machines because other machines because
sources in different locations have
different operating systems it does have
error recovery so if you stop at a
certain point you can start it up
without losing much data at all so I'm
not going to go to the optimization but
some effort was spent there in trying to
make sure that this was reasonable
because this is just not reasonable okay
so what we did was we chose twelve of
the 25 largest cities in terms of
population within the US and it shows
cities such that they were distributed
across the u.s. I'll just so happens
that this there's no big city here
otherwise you see that it's sort of well
distributed so you have stuff their
stuff here of their stuff here okay so
there are 12 cities in our study so the
reason for looking at multiple studies
is because we also want to look at
across city similarities or differences
okay so that's the destination cities
and that's the cities in our study the
data that we use are the following the
data sets that views we have skitter
which is basically a guide as a net
measurement tool and the data that we
used this not all the data that we have
but the data that we use had 20 had
traced routes from 20 vantage point what
skitter does is basically it has a set
of IP addresses that periodically it
trace routes to and some of that were
useful for us so we took that all right
so not all of data is used here when you
took that but they use 20 vantage points
planet labs is just a network of
computers you can get slices on it we
use 14 such vantage points across the
world but most of them in the US and we
trace out to our data and in addition we
used our vantage points 10 in numbers
basically we are so much
friends can you lend us a computer for
you know seven days ten days whatever
and we'll run trace out from that okay
so overall it's 44 vantage points one
crucial thing is that recent studies
have shown that if you do not trace out
from if you do not don't run these trace
route based studies from multiple
vantage points lots of one advantage
points you could get misleading results
okay so that's why a lot of effort was
spent in that one obvious thing to
consider was and we did was rocket fuel
which basically Maps autonomous systems
or actually ISPs okay so they have
mapped for example verizon however when
you look at that data the amount of data
we could use wasn't enough because
they're using very few tracers so we
decided not to use them just to give you
an idea of the amount of data that we
have / City and Chicago in particular so
bigger cities are more data that's why
Chicago is almost twice the average is
about 180 850,000 trace outs that for an
average city of which I said that you
know not all train starts complete only
about 100,000 tracer did complete
however we don't just throw away all non
completed trace routes the amount of
trace outs we use the number of trace
what we use for an average city is all
200,000 that's a lot of data for Chicago
it's about it's more than half a million
okay the first step I said was to find a
set of IP addresses to which to from so
that we could trace out to that now one
thing we found was if we just randomly
sample a lot of IP address space is just
wasteful it's just not used at all if
you randomly sample is that good or do
you need some other sampling what we
found out is that we actually need some
other sampling so what we did was the
following let me describe the result and
then I'll say how we did it what we did
was we took the ID IP address space the
IP addresses within a city and then we
broke it up into blocks and I describe
how the blocks were made and then what
this shows is the number of IP addresses
vs. population and it shows the number
of blocks versus population
and you can see a much better
correlation with the number of blocks
than with the number of IP addresses
what does it tell us basically what it
tells us is that the number of blocks is
a closer description of the complexity
of a city net then a number of IP
addresses so what we did is we did block
sampling instead of sampling randomly
from IP addresses we had these blocks
and then we sample uniformly randomly
from those blocks and again you might
ask well it's just correlated with the
population how does you how do you know
that it's it gives you more complexity
well to tell you the truth I do we don't
however if you look at the performance
in terms of discovering elements of City
Nets nodes and edges forget what this is
written here what we see is that if you
do this uniformly randomly sampled from
IP addresses and verses our block
sampling we get one hundred twenty
percent improvement okay what this means
this is for us for the same for the same
number of trays stops so if you do
10,000 trade routes with uniform
sampling versus block sampling arse is
one hundred ninety percent better across
all types of nodes and edges that we
looked at so this is this time you
forget that this is interior nodes so
these are these are connections between
routers inside Chicago these are total
number of edges these are I 2d edges
means these are connections between
routers and end hosts ok so across every
metric that he looked at we got a huge
improvements give tells us that it is at
least much better than the most trivial
way there could be better approaches for
sampling what we did for the sampling
part was we took the data that we have
and from the run from from the Geo
sources and the data already has some
the idea there was to get to cluster so
this is basically a clustering problem
we want to cluster IP addresses such
that all IP addresses that have similar
routing behavior are clustered in one
because if you do trace out to one of
them you
know that you know the results for the
others as well okay the geolocation data
that we had already had some block
information I in fact the BGP for
example when you do this routing it
already has this block inflammation but
you are still have the problem of not
aggregating too much because if you
aggregate too much then you lose you
know the information because remember
what we would be doing is doing block
sampling here so we have to find nice a
nice middle ground okay so that's that's
what we did for the block sampling part
I have sort of described this bi d model
but just to formally state what the vit
model is we divide we have a set of
nodes and they are divided into three
sets partition in three sets the border
nodes are notes that are just outside of
the city the interior nodes represent
the routers inside the city the
destination Road nodes represent the end
hosts within the city okay so this is
just the every part that we have goes
from starts at aboard nodes and set a
destination node and must go must have
at least one router here okay only some
paths we found in have this structure
but most pass do conform in the real
world do conform to this structure okay
so note that we are ignoring what
happens beyond this because we are
interested in comparing interesting
looking at the connectivity of the city
to the outside world or vice versa okay
that's why the data is our fiscal from
you by net there's a lot of all service
in my house for example where you can't
tell what's inside yeah and all those
are difficulties now and so you sort of
expected lots of endpoints would would
be like that with small numbers but I'm
curious if they're in big numbers that
are hidden from view binder subnets and
corporations or eyes if you have to ask
me I would say yes it's a possibility
but if you ask me when for an estimate I
would not know that you can only know
one thing we did do was not throw away
incompleted trace or even if you have
those you would probably get incomplete
pass that just after that point to just
give up and they can't go beyond so what
we did was we didn't throw that I did
the best we could do sort of attached a
we note there so we don't know which
nodes correspond to you know if
Microsoft has a blocking anything we
don't know which destination would
correspond to that but there is
something that possibly corresponds to
that okay so and and if it's a big chunk
it will have higher weight and we'll see
that later on but yeah does that that's
the obvious problem there's so many
things that are hidden and that is any
trace our study will have this problem
essentially that you could do some okay
i should mention that there's some other
optimizations that we didn't do that we
could do for example people do source
routing okay can you ask us to forward a
packet from here to here we are this
node for example we didn't do that that
could increase the amount of nodes we
see however we stray starting with so
many places if you ask me to guess I
don't think it would matter in our case
typical I would also mention the typical
traced our studies are not done for
these many sources except rocket fuel
but rocket field was traced out into a
small set again the border nodes are
connect City Nets the outside world the
interior nodes are routers inside the
city in destination nodes and end hosts
and we are only interested in outside to
inside traffic okay so and again what we
did was we have the trace route and we
want to go to the BI remodel what we do
is just cut it right at the border
essentially okay also some optimization
we only we only keep trade-offs that at
least go two hops into the city that
will otherwise you can get a path that
has one be node forward destination not
we didn't want to do that we didn't lose
too many nodes because of that though
for now we slept the problem of if I
give you a path I haven't labeled it as
an interior or a destination node okay
because what can happen is the this node
is a destination in one and it is a non
destination node it's a an interior node
in the other but it's a fairly easy
thing to if you think about it suppose X
X is a node okay in our all our traces
that we have
what race is simply a trace that is cut
at the borders so X to the node and if
we have ever seen it being being a
forwarding a packet we know that it's a
router because endows not forward
packets typically so we label it as an
inode on the other hand if it's a dead
end which means that it's the last node
and iden finished on an unfinished
traced out what that means is that
someone in the internet thought that it
could forward a packet all right because
in assuming that someone was right it's
probably a router so we label it as an
inode everything else we label as a
destination node okay so that's our
simple heuristic again there is there is
there is in accuracies in all these
steps but that's just the nature of the
game however because of all this
accuracy in accuracy sorry data
validation becomes an important step so
the steps that we did take to validate
our data we are the following question
is our data enough how our models
converged so we did some statistical
tests for convergence and I will
describe two of them as follows so let's
look at how many network new network
elements we are discovering portrayal
out actually is not portray stout but
it's per source what we did was for one
city in this case we took all the tracer
that we have divided them randomly into
10 buckets and say okay what is the
return for this one and you can see that
it's at that point it's very
insignificant okay so that gives us that
units at least as far as traceroute
studies are concerned we are converging
the other one that is sort of
non-standard standard the other one that
is sort of non-standard is a factorial
test which we basically are asking the
following question here it was just a
simple you know number of nodes number
of elements number of edges here the
question is not that the hill the
question is look at the in degree
distribution let's say in this plot look
at the in degree distribution and is
that is that curve is that metric
converging so what this plot shows is
this is the solid line is the indegree
for four sources okay for a particular
city i think it's chicago for a
particular source adjust for its four
sources
and these the dots here above and below
are two standard deviations above and
below for three sources so think of
three sources and the and the plot that
you get and now go to the four sources
in a plot that you get and what this
shows is that within a band and what
this shows is that the four sources is
between two standard deviations above
and below the three sources okay so even
with three sources we are forces that is
we are sort of converging okay the
standard deviations here for example are
really small compared to the mean okay
each point here by the way has been you
know each point here represents multiple
runs okay so the way you compute this
point is that's why that's the reason
it's called a factorial test is you
choose so you take your set of sources
you take all your desired destinations
divide them into sets oi divide them
into sets here and then take subsets
again you take lots of subsets so that
you get statistically significant
results okay similarly for the outdegree
again in fact in this case only one
standard deviation above and below it's
still converging okay this gives us
confidence that at least as far as we
could do in using this methodology our
results are have to be believed or can
be believed any questions so far before
I go on to the results section okay so
now that we have constructed these
models for City Nets so again our model
is set of nodes and pass that have been
derived from actual traced out
experiments from 44 different sources
okay what can I say about the structure
of city Nets how are they different from
the Internet at large and our one
question that we are asking here is
about the robustness I am going to
describe a little bit of all of them as
expected the number of nodes increases
linearly with the number of blocks in
the city sort of linearly it's well
correlated okay so this is just that I'm
sorry if these are really small but this
is the number of border nodes is the
number of interior nodes so this is B
I the number of destination nodes okay
again the correlation with IP at number
of IP addresses is not that good at all
this is much better than the number of
correlation with IP addresses one
interesting thing is I think you asked
about the depth the the length of these
parts the name of the parts are are
pretty the line a pretty small range so
most of the parts note that there are no
parts less than two because we are
imposing this structure that you must
have at least one at least one interior
node actually three because I am
counting the number of hops the number
of nodes okay so between three and about
six that's the that's the number of hops
for almost ninety nine person of the
parts okay and the different plots are
four different cities okay so there's
one curve or pearl city yes all right
there not twelve of them because we
choose five or six of seven of them for
that but essentially one way to think
about this is the following if they were
if there are lots of parts with long
there's a lot lots of long paths what
does it tell us maybe one way to think
about this is that there is some isp
that is not doing a good job essentially
because then that is P if you are a
customer for that is p we have a long
path okay then people will probably
shift from that I speed to something
that performs better so this is the
parts are the lengths are not that
different they are ok so now let's look
at one difference between the Internet
at large and the sorry the graph view
and the path you this this is the total
degree of edges here and this is the
total degree ranks when I what what I
mean by total degree of edges is the
following you take an edge and you take
the sum of degrees of one side and add
it to the sum of degrees of the other
other node okay that's what i mean by
total degree oh sorry this is actually
this is not this is just for the nodes
this is a troll total degree for the
nodes which means a total in degree the
sum of the indegree and the out degree
okay that plot is coming later on so now
you see that as observed earlier as well
this is not just for the first time you
should know so
and this is log log plot so this is
roughly a straight line again for
different cities so it's a power law
however if you go to the path degree so
number of paths going through a
particular node okay and this holds four
nodes and edges as well it's definitely
qualitatively different because you see
this hump here oh the politic is the
number of paths going through a note in
our yeah this is an this is a number of
edges so so the path degree is very
different because and in particular what
is happening is that there are fewer
larger nodes right because the mass is
distributed towards the smaller nodes
okay and we will see a consequence of
that as in a sense what what you have is
that you have smaller cuts in in city
nets even fractionally because you have
smaller number of large nodes and
because the mask on the in the tail is
small as compared to for example it will
give me ok so the indegree our degree of
the total degree distributions are power
laws I only showed you the total degree
distribution the in in case I didn't
explain this what this is basically you
arrange the the nodes in decreasing
order of their part TV and you plot that
or the log of that versus the path
degree and it has been it is well known
that for the internet it's a power law
so that's why you see a almost linear
relation here yes Oh suggest that I'm
all for this creation of a power law
this copy paste small yes I was
wondering if there are any ideas of such
models for the distribution of the past
equations using we don't have invest but
now this one layer we are the future
reason what do you want to go on with
for example they have the rich gets
richer model you want to come up with
something that has but this is
definitely less heavy-tailed than power
law okay so yeah something maybe as part
about what we have I don't have a result
to show you
for the close cousins of like a
lognormal long normal we have not but
that's at again this is stuff in the
future however at this point is more
qualitative than quantitative for
reasons that I'll also explain later on
but yeah that's again something that we
would want to do is to fit to some of
these curves and one problem but but so
much is clear that it's qualitatively
very different and we'll see some
consequence of that shortly next few
slides and something you would have
already guessed I know why this is
taking so long lots of points is a
scatter plot okay so now let's look at
whether if you look at an edge or a node
and look at its degree is that a good
indication of the number of paths going
through it and the answer is no actually
because if you look at the total edge
degree this is the edge path tell you
what this means is the total edge degree
is the sum of the degrees of so consider
an edge UV the total HD f of e is just
the degree of you plus the double degree
of fee okay so and the dignity of path
degree of a net is just the number of
paths going through it okay so what this
means is just because an edge has a la
lots of connections doesn't mean a lot
of paths are going through it it might
be true for someone but it's not
necessary to you could have an edge that
has only a few neighbors yet you have a
lot of paths going through it okay so
that's what that's what this means
because there's just no strong
correlation at all and you can replace
this with total no degree and still is
quite literally the same that based on
it is an interest built by your eye to
eye or I might but not in this
presentation I might have lots of I want
to finish some time I'm already over
time okay one thing okay so this is a
just uncovering different structures and
I know it's a look it's a bit
unstructured for for for that but again
another property that we have is let's
do some definitions when I say last
router it
is the inode the router immediately
preceding the destination okay and when
we say a block when you think of a block
of IP address which we say it's
homogeneous if all paths to all nodes in
the block have the same last router
which basically means that a block is
homogeneous if basically as far as
routing is concerned is the same route
from everyone okay finding this is not
trivial but you can randomly sample and
final find it and we did do that cluster
router is basically a last router that
serves a homogeneous block okay so what
we found is that cities have a lot of
cluster doubters so fifty-five percent
of Austin for example of Austin routers
are clustered outers and the number is
higher for means roughly in that range
so what that means is that the last hop
router corresponds how is the fraction
of total number of routers the cluster
routers are significant also that pretty
much means that the number of
homogeneous blocks is significant so
lots and lots of blocks a lot of the
blocks are homogeneous which also lends
credibility to first of all our block
sampling because we basically need one
one part for each block but also
uncovers a structure that we like to
call as the apartment hypothesis maybe
what is happening is you have these
apartments where he lots of people live
again this is an analogy I'm giving you
and then if they solve a common router
and basically what the the ISP is doing
is bringing a part to that and then from
there you have connections to all of
that all right as you see you know
almost eighty percent of pretty much
every city is homogeneous city blocks is
you homogeneous again these this was
found experimentally just a short note
on the robustness analysis and this is
the consequence of the the the path
distribution being less heavy tail what
we did was we took the city nation and
the question we asked is how many notes
we need to delete so that eighty percent
of traffic roughly is this is deleted we
did it for two things one is the number
of paths and the other is block uniform
waiting where each block
is assigned a weight of one a traffic
weight of one and then all and it is
distributed equally to all the paths
coming into that block okay and the
results are pretty similar okay that's
why I'm showing when you want so the top
post one is when I do a target node so I
know all information about all the parts
and I'm trying to delete the one that
has that gives me the most so it's like
a set cover prop heuristic essentially
these are just heuristic because that's
its NP complete to find out okay so I do
using the target node so I'm using the
number of paths going through a node as
the heuristic this is the number of
paths going through an edge as the
heuristic this is the number of and this
is the in degree of a node is the
heuristic so you see that you use the
indegree since it's not that well
correlated with the with the number of
paths this is a significant gap there's
only forty percent roughly or sorry
fifty percent roughly this is a close to
eighty percent okay so think of an
attacker if you're just looking at the
graph view okay you are probably not
going to do as well for the attacker as
they're as if you were looking at the
path you again it argues for the path
you these are just random just for
comparison you just pick the notes
randomly and the internet because of the
power log structure is known to be
resilient that case target you mean
adversarial each other yes okay yes yes
yes yes this factor of 10 to the
negative 3 day yes oh yeah this is yeah
so a good point that's my punchline this
is half a percent and it deletes a tea
person or traffic okay for the internet
if you look at the lake philosophers
felicita rafts and stuff like that this
is order of magnitude larger you have
stick the same the same curve except you
might have to I forget their exact
number but it's at least one order of
magnitude larger okay distribution of
nodes and how would be impacted so I
know there's couple buildings in terms
of cells a
also them aah now see that this is very
hard to do if that's the first idea we
have we are going to plot it the problem
is the way geolocation works the the the
granularity is not that much so it will
give you okay it's within 10 miles or so
okay no one have that to my knowledge
well that's an interesting thing to do
because if you can do that we can do
quite a few things yeah we didn't have
that we all we knew was that this is in
the city in this is our jealousy okay
server the east coast from the west
coast for civilian voice traffic by only
cutting for places wow the new you guys
yeah that was in the early nineties they
fixed it okay just some more definitions
again looking at Bitra well a
vulnerability structure when we say the
waste of a city so the structure that we
have is sort of goes in is this sort of
the few parts coming in a few edges
coming in and it branches out okay and
then think of it as you can only have
highways coming in and then here you go
to your home it's sort of sort of
branches off into small streets ok
that's the mental model that I have so
the waste of our cities of this is what
we call the vase and then the other hip
there and then the waste of the city is
just the smallest number of routers that
carry eighty percent of the traffic ok
hit porfa city is the smallest number of
end hose that account for eighty percent
of the traffic okay and what we have is
basically a small waist large hip
structure again this is the uniform
block waiting that I mentioned earlier
as well it's not important the results
are pretty consistent actually even
without that so just to give you an idea
of the ways to hit this is these are
these are not fraction is a person this
is half a percent essentially really
really small except for a few and these
are if you look at it these are small
cities so what is happening is the two
things happening that fewer number of
hosts here compared compared to the
number of routers and is also i'm
figuring there's also some error that we
have related to that we have a dot what
we call a dosimeter index for
basically denial-of-service attack meter
which basically say okay if if adversary
chosen you delete you know K nodes okk
is fixed at 10 notes the adversary where
will it impact the most so think of it
as how many people who is going to
impact so as the ratio of the population
total waste essentially because remember
waste is eighty percent of number of
routers that carry eighty percent of
traffic okay and we see that there is a
correlation between the size and the
population and the size and the
dosimeter here so we're larger cities so
larger cities have smaller doses small
is bad this means that it is like hold
on population so these are larger cities
and small is good sorry large is bad
large is bad which means that with a few
notes you can disrupt a lot lot of
population but larger cities are
actually worse this downtrend here this
is an outlier I think it was Washington
or so however note that all of this is
Paula I related one that you might think
of is then that is the ratio of the
incoming to the outgoing in a certain
sense the ratio of I 2d edges so I 2d
edges to be to I edges okay so mark a we
are claiming that this is much larger
than this it is a sort of similar ones
again this is represent the degree of
disaggregation when the parts come into
the city anyway one interesting thing
that we got is that this is this is not
I mean this is from the data with
plotting the actual data so that is very
surprising or what we found is that look
if you look at the indegree out degree
in total distribution for these values
actually you can have a for these values
you can actually prove that the edge
skew which is which is the ratio of I 2d
to the b2 I it is so ratio of your
branch streets your local streets to the
highways okay so that ratio is actually
for these values login which is not
surprised this is a straight line
because this is
logger this is a log scale okay so this
you can prove not that it's not that
non-trivial pretty trivial to prove
actually okay so I was going to talk a
little bit about this theoretical part
of our work but I'll just end with the
one result that we have so we have shown
that City nets are vulnerable in certain
restricted sense note that we don't have
traffic so we are only looking at what
we have so obvious question is can you
detect large-scale attacks so just
defining one large scale reasonably
large scale attack is the following we
say that an attack is K epsilon attack
on a path network p if the adversary can
remove up to K nodes or edges k network
elements and he must cause a depression
or disruption sorry to at least an
epsilon fraction so if it is 10 and 0.2
by removing 10 edges you must disrupt at
least 20 person or traffic of the of the
paths okay so this is the kind of
attacks that we have been looking at
it's inspired from work by Kleinberg who
looked at the same thing for graphs not
for pots okay so one application might
be that you install monitors on chosen
endpoints a small number of monitors so
that any large-scale disruption shows up
okay what we can prove is that for every
path network and for each K and epsilon
that exists detection sets of size that
polynomial in K and epsilon inverse of
epsilon as independent of the size of
the path networks in de pere of the
number of nodes and in finding the
numbers of edges and in fact we can have
a simple randomized algorithm that can
do that it's based on VC dimension and a
very strong result by house letter and
wells l which i'm not going to define
but that was the proof there but
basically what we get is that the VC
dimension of a set system that we define
again I'm not defining that what VC
dimension is for lack of time but we get
that the VC dimension of set system
relevant sassette system that we define
is K log see and House levels will gives
gives us that it's polynomial in there
and epsilon prime so what you have is
that it scales up very well with your
path network it does not depend on size
the path name only at all it only
depends on your K and epsilon one
crucial thing that we need to know is
that what is the sea I didn't define
what they see is the technique condition
that we need to have in our path network
which is the following think of two
paths they merge and they deviate and
they merge again see bounds the number
of things they can do that so it can't
be arbitrary it cannot depend on n
otherwise we don't have our result so
that's the confluence coefficient
however because internet paths our
destination Bates based we have found
that you never see more than three
confluence okay so once you have that
you have small detection sets I just to
conclude we have studied City nets as
important sub-networks of the internet
crucial difference between our and other
work is that we have a path view and we
have a methodology to make realistic
maps from off City Nets we have taken
pains to ensure the statistical validity
as much as we can the cities have show
structures error that is different from
this structure of the Internet at large
in particular the small waist large ship
structure and they seem more vulnerable
in order of 19 more vulnerable than
Internet at large so if you want to
attack smaller portion you can do it
easily in terms of fractional as well
right remember the direction sets are
fractional okay so that's where my talk
ends sorry from going over time any
questions I would ready to take
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>