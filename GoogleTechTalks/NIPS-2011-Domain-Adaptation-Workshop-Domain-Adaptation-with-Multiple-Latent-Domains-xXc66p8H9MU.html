<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Domain Adaptation Workshop: Domain Adaptation with Multiple Latent Domains | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Domain Adaptation Workshop: Domain Adaptation with Multiple Latent Domains - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Domain Adaptation Workshop: Domain Adaptation with Multiple Latent Domains</b></h2><h5 class="post__date">2012-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xXc66p8H9MU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">oh my work will be by the object and so
in object recognition there's been
increasing interest in demand attention
especially as music to realize that our
days and our realize there's a paper oh
I just fear that
looking at very standards through the
project he'll here i'm showing you an
example to happen with you training tax
on amazon com type of images and here
we're trying to detect trying to
classify object categories here's one
category a column so here we're doing
okay but then if we use models trained
on this data to classify cups in a new
domain like an office robot robot with
webcam then our performance drops when i
walk and this turns out this sort of
bias occurs in many of the common data
set so here are some other examples this
is from Kelton 256
six different categories and for all
these ensuring the person category so
here we see the types of images that we
have they're mostly mine from the web
but they're cleaned up on the right I'm
showing results of raw Bing search image
search for people so here you notice
that it's wouldn't more noisy and for
example it has some cartoons it has some
sort of just graphics in addition to
photographs and here on the bottom is
another popular data set called label me
with people category here you see really
a very extreme shift between these
domains we're here the people are not
even looking at the camera these are
just images taken and outdoor scenes and
they're being photographed so this is
really the problem interested in how do
we train on such data sets and transfer
the models to new domain see visual lens
and especially so I've done some work in
this area and recently so this is just
showing the drop in performance you
train and test
here we have extreme drop thirty seven
to twelve percent accuracy
so recently have been interested in this
issue of mixed days so example hearing
notice that some of the photographs are
actually actually photographs people and
summer graphics or drawings so maybe
we'll really have in this display data
set are multiple League domains and we
just don't know which images come from
which the main so in this talk first I'm
going to describe a basic weapon or the
medication and then I'm going to try and
start addressing so the medication is
domain dependent and but there's been a
lot of work in NLP there's been work in
vision in particular there's some
methods that adopt SPM parameters to
intervene early learning but in our work
really interested in achieving new
edition by learning some sort of
transformation on a future space and the
advantage here is that we can learn the
transformation are using source data
label target examples in some object
categories and then transferred to new
tasks so new categories because we don't
necessarily want to have to have labeled
examples of all possible categories in
the target area so example if you
learned that transfer my cups and
competitive people
so the idea here is that we have some
source domain with a rather source
agreeing source domain in the blue
target domain and the shapes correspond
to different categories and we want to
learn transformation of calling it w
that when applied to the feature space
maps points that go up to the same
category a different domains closer
together and vice versa so that's the
main idea and for that we're going to
use loss functions of this form so the
similarity between point x and a point
why the source and the target domain is
going to be privatized by the major
story so you can think of this as
multiplying y by W essentially mapping
line into the source interface or vice
versa mapping the source feature into
the target
and in this work we use plus functions
of this form where if x and y same label
then this troops should hide somewhere
to be high and if they have different
label the mission
example of a constraint between two
images of the same category but in
different domains and here's a
constraint between a negative constraint
between examples of different categories
ok so our input is a set of these
constraints and
we are going to assume that the loss
functions the general will depend on
just the inner product matrix between
capital X is all the training features
in the source and capitalize all the
target features outside of these
functions and so our objective function
is to minimize the loss was some form of
regularization on going and we're going
to use essentially green you snore a
month early but we're not restricted to
organizer
we'd like to learn nonlinear
transformations so hard when we talked
about a linear map w but it's actually
possible to fertilize this
you have paper that if you're interested
details so here's the summary of the
approach so far we have some training
data and resource in a target you have
some constraints based on this class
labels and at a time get a test point
and you don't know which class it is we
apply the transformation that map said
to the source and
now here as I mentioned before what
we're really interested in is
transferring this what we've learned
about the structure of the domain ship
to new categories so here we have a new
category shown in with the stars that we
haven't seen a training so you can have
constraints for that particular category
but we still apply the transformation
the same way and classified this case
i'm just showing classification to the
nearest neighbors correct so this is
nice because it allows us to serve
transfer our relationship
ok
so back to this problem Lane domains so
imagine that this is our target domain
and we'd like to classify this image of
the face so we're going to compare it to
images for examples in the source domain
for example this particular image and
we're going to use this using the learn
transformation okay and but when we
compare it to this particular image
notice that the images are actually
quite different one is a drawing another
photograph so the intuition is that we
actually you might want to use different
transformations to compare target
examples to different subdomains or late
domains in the chain
okay and the problem is that you really
don't know what they are so we'd like to
learn so this is a very hard problem
essentially this was our previous
objective function so now we're going to
introduce some number of latent domains
so s link domains and so now we're
optimizing over a set of w's 1w per
liter v plus the specter of domain names
game there's one they will put image it
says which would remain that image of
Worcester and I keep saying image but it
could
so so now we want to regularly w's and
want to minimize loss of pork except now
the losses franchise bias
physical problem to optimize so we're
actually going to do
iterate between
so first for mine too
8-9 delete the mailings and fix them and
solve it out so I'm just going to show
up an overview of the algorithm that we
use for solving the case to solve into
the papers so we start with some source
data that you know probably completely
the main and first we separate them by
class by category so here we have people
and here we have rice
and now for each category for each task
we're going to five clustering to obtain
s called superpoints ask cluster so that
means of these clusters are going to
read the super points they're used in
the next level maybe a hierarchical
clustering number so these are some
points and the next step we're going to
cluster these super points now in two
domains but we're going to use some
constraints we do
and the reason we need to be constrained
clustering is because if we just cluster
images without any constraints it's
going to happen if images of the same
category are going to tend to cluster
together so instead of link domains we
might get essentially categories face
cluster and a bicycle posture and that's
not what we want
okay so but with the straight clustering
which I'll describe on the next slide
what we essentially want to get is
Layton doing like this which includes
some people and some bicycles this
example drawing throwing split domain
and another leagues are being at heart
images
so for the constraint clustering stuff
what we want to do is given the super
points that are showing the previous
slide each of which has it mean mij we
want to minimize this objective function
where we minimizing the touch of the
k-means function for these but with
general interest rates and the
constraints basically say that we don't
want to link to super points if they
come from the same category that's very
simple it's a very simple idea
essentially we say we don't want to put
sorry we don't want to put this cluster
together with this cluster because they
go through the same
okay we want you want Lisa points to go
to differently in two weeks so then
should we sort of trying to constrain
this to not cluster of a lot of memories
so on
so you show experiments with the thing
and Calvin six data set that i was
showing and i believe all these days
that they're available so anywhere
interested
and and then we use another day said
called office state set which has 31
categories of common objects in office
and to that so the three domains in that
in my office datacenter amazon domain
DSLR camera images and and then we're
also root apps using synthetic remains
of our took the amazon images and rotate
them put them in which of the webcam
image isn't important so here are the
result of clustering on the office plus
two data set so the white bars is our
method with constraint hard clustering
and then we have two bass lines and is
the
simple k-means baseline and see here
that our method does much better
especially for some of the domain shifts
so here there's just different
combinations of domains two domains each
for each of these bars so here for
example for amazon and for webcam we'll
probably much better at separating those
two domains of course pretending that we
don't have domain names through just
using valuation
so now we've been trying to use our
algorithm
data set here hearing results and so
here I'm showing me the increase or the
difference the increase in the accuracy
when using our method relative to
baseline method it's not use any
transformation does not using anime
notification okay so for some of the
combinations of the main example here
here we're doing much better than just
using a single transformation without
trying to learn late movies and I'm or
some other combinations they're sort of
not all not a lot of
and here's some results on the big
Calcutta so here we're training on the
web search data and testing on the
Caltech six data and the x-axis here is
just showing some parameter that we can
bury that controls how much we want to
optimize the loss function so and so the
red is our method and we're doing better
than you which satisfies the clustering
doctrine oh no sorry this is the
similarities this is the actual data
oscillation so why would you wanna me
it's like a realization
so
um
conclusion we extended our
transformation method to multiple
domains and introduced a way to discover
latent domains and in the future we'd
like to think about so many ways to
include more unlabeled target data
and also like to consider different lost
options for example instead of
similarity losses i talked about abs
Angela
the legends</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>