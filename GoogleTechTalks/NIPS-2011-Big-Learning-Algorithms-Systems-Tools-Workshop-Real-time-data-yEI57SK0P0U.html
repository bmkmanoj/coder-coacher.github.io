<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop:  Real time data... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop:  Real time data... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop:  Real time data...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yEI57SK0P0U" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">result directly for specific algorithms
so first of all thanks to those five
guys who contribute to that see I gave
the most that's why he's in the middle
and like was je and marketers great
inspiring discussions so let's talk a
little bit about okay the thing the
agenda of this so our first explain will
be various distribution and load
balancing algorithm so those of you
who've taken a systems class some of
this may look very very straight forward
those of you who haven't it may look
actually quite surprising what you can
do so I'll talk a little bit about
consistent hashing distr has tables and
so on maybe in the end if we have time
i'll get to peer-to-peer networks but
probably won't have time and then i'll
show you one application in detail
namely data sketches how you can
actually turn the other sketches into
something that is in both scalable fault
tolerant and also deals with temporal
variability in the data and that's
actually three attributes that you
usually don't find in conjunction with
your run-of-the-mill daughter sketch
algorithms and i'll improve quickly show
you wanted two slides how the very same
ideas can be used to distribute
graphical models and if you want to know
more about the latter you should
probably go to the non primitive base
talk workshop in the afternoon but don't
run away yet so distribution strategies
so well let's just look a little bit at
what we would want so one thing that we
would want is we want to have you know a
variable and the load distribution so in
other words the objects need to be
distributed over possibly a large pool
of machines
and well those machines can be faulty
and then the data needs to go across
various numbers of machines the
computational load needs to be
distributed and one way of actually
achieving that is quite nicely done by
what's called consistent hashing and
I'll show you that in the moment so
especially you can actually find
entirely uniform distributions but you
could also have machines with different
capabilities you know some maybe have
twice as fast a processor or a larger
disk and you might actually send more
data to them so this is what's called
proportional hashing now I'll probably
skip over the overlay networks entirely
before going into details let's briefly
talk about hash functions so hash
function is essentially a computational
device to fake a random number I mean
yes we use a random number generators
anyway but I'm willing to go actually
more extreme to say well you know rather
than call invoking your random number
generator you should probably stop the
hash function with a parameter such as
you can just recall that random number
at any time so the goal why you might
want to do that is because maybe you
want to reevaluate that random number if
you want to for instance randomly assign
items to machines at a later time so the
naive thing would be well you know for
each new key that we see we compute a
random number we store it in a big
lookup table and it's you know as random
as I my random number generator can be
but it uses a ridiculous amount of
memory because I now need to increase
that table as I insert more items and it
gets slow as we do more of that and I
can't really merge it dwell between
computers that's a big deal so basically
if I want to synchronize random number
generators between different machines
it's a pain because yes you could
basically start them with the same seed
but then oh my god if you call if some
part of your code calls are in a number
generator more than the other part
you're screwed so you can't really go
back so better idea is to use well a
essentially random number generator with
the seed and you just then invoke it to
each time so the good thing is you don't
really need any memory for that
cause you just you know use your key as
the seed then you just you know pull one
random number out and now you can
actually merge things between machines
which is really handy and well now the
speed of this operation of course is the
independent of how many times I've
invoked it there's a lot more beautiful
theory and analysis for that so I very
much recommend that you actually have
look at this paper here by Julie Mara so
he talks about a lot about cryptography
pseudo-random permutation generators and
so on it's a very beautiful treatment of
you know what various types of
Independence mean in practice use
something like murmur hash so member
hash version 3 is actually very very
fast i thinkI process in the other maybe
four or five gigabytes per second per
core if you want to do something very
cheaply this is probably good enough so
this is like a fast linear congruential
generator based in other words you take
your key x an integer add another
integer to it more the third integer and
those coefficients a B and C you can
look them up on Wikipedia and there's
probably about 10 different sets and you
pick the one that you like so why all
this introduction about hash functions
well the first thing I want to show you
is the aardmen hash this is probably one
of the older ones I could trace it back
at least to cargas 99 paper but it's
probably older than that so the idea is
I have a set of keys and I wanted
uniformly distribute them over a machine
pool and I wanted to have that fully
determined by hash function so what I do
is I take well the laser pointer yeah so
I basically take the hash of the key and
the Machine ID so for each machine my
machine pool I do that then I pick the
machine with a smallest hash value okay
now this procedure will as you can
easily see give me a uniform
distribution of keys of the machines the
nice thing about it is that if a machine
dies
then well for all the keys for which
this machine wasn't the smallest one
nothing happens so nothing gets
reassigned furthermore for those
machines for which the key actually well
the machine had the smallest hash entry
this machine well the next machine is
going to be wrapped uniformly random
again so in other words only a fraction
of 1 over N if I have n machines only a
fraction of one of rain of the data gets
randomly assigned uniformly over the
other end machines that's quite
convenient because it basically means
that if I have a failure my load
balancing is going to spread across my
entire set of machines likewise if I
need to add additional resources again
only a very small fraction of keys will
be reassigned and it'll pull them
uniformly from all the other machines
it's kind of cute you can do a little
bit better than that if you whatever
replications so for instance if I want
to have a distributed file system with
lots of replications and not a single
master then what I could do is I could
just you know take the case smallest
guys rather than the smallest guy and
again the same properties hold and
there's actually a beautiful file system
which does this is called safe siiii ph
and it's a very nice distributive
replicated file system and they do a lot
of other clever tricks in it so if you
interest in their kind of stuff look it
through um there's one big downside to
this as you can immediately see that's
okay if your machine pool is you know a
couple of thousand maybe 100 machines
but from there on it gets really messy
because I mean you need to compute this
hash for each single machine that can be
very very expensive so can we fix that
well the one way to fix it is what's
called a distributed hash table so for
instance Cassandra uses a while slightly
hacky version of that so basically what
you do is you take your machines and you
have well for each machine ID'd well
let's take the AP number i compute the
hash and I put them onto the ring of
Enki so it's the mathematical ring of
Winkies and now what I do is for each
item that comes along
I go and hash it and they then assign it
to well in this case the will rightmost
machine to that key the new item comes
along you know epsilon into this machine
and so on now if you do that here
obviously very fast because you have a
logarithmic time lookup in the number of
machines but the problem is that as you
can easily see those distances here can
actually vary quite a bit the good thing
is out as I'll show you in a moment that
variation can be made reasonably small
but you have a lot more variation the
other downside is if I lose a machine
here then immediately well this machine
here will get all the keys from that one
so I have significant load imbalance and
as I insert or remove a machine and
furthermore if well the overall blocks
are quite large too so let's see how we
can actually fix that a little bit let's
first do a little bit of analysis so as
you can reasonably straight forwardly
siiiick you can always define the
machine that you're interested in as
being a position 0 so then the
probability for that segment to the
right hand side being larger than being
larger than C is but just 1 minus e to
the m minus 1 where m is number of
machines you can work out the density
just by looking at the derivative and
obviously by symmetry the expected
length of a segment is 1 over aim so
plugging this inside into in here and
making the assumption of very large
number of machines you can see that the
probability that the segment is okay
times as long as the average segment is
like e to the minus K so you can use
this fact to then you know say well
actually per machine we don't only pick
one segment but maybe ten segments or so
and then you know concentration starts
kicking in and the average say well
basically the size distribution of those
10 sub segments is much less than you
know what you would have gotten with a
single one and it also be a balancing
operation works better
you can do more beautiful things so this
is called proportional hashing so this
for instance a very nice paper by travel
a towel usenix 2011 which describes that
in great detail for designing a video
caching system and the idea is
essentially very similar to what you
will get in the rejection sampler so you
basically allocate pre-allocated certain
interval and then for each machine in
your machine pool you basically
predefined a small segment size and then
if your machine if a key comes along I
go and compute the hash and ok well here
I hit something then I go and compute
another hash well I didn't hit anything
so I need to rehash I hit something and
I keep on doing this until I get
something so you can easily see this now
gives me a proportional load balance the
distribution however you can also see
that we know once you run out of space
you're done for so you know you picked
app may be a factor of 10 or something
to make sure that there's enough space
for improvement now those of you who
have built samplers would immediately
see this looks awfully like a rejection
sampler right you have a proposal
distribution which is uniform over those
larger interval my acceptance is only if
I hid those bins and I think there's
actually some very nice statistical
combination of all connections between
you know sampling distribution and
machine learning that I worth while
exploring talk to me later if you're
interested in that so a nice application
of that are the random caching trees of
akamai from of cargo at all that's I
think pretty much to pay for that
started are combined and the idea is you
build random trees for each key and then
while you just there for you know are
not creating hot spots because you know
each key is being cached in a different
way so this would be the master and
those next level nodes would only be
instantiated whenever you actually have
enough requests and there's a lot more
fun stuff to it ok so this is what you
get basically you get essentially that
each node does more Leslie you know
uniform work so now that's just the
background a little bit to give you a
bit of an idea of what's going on let me
now show you what we use this for let's
take sketching algorithms so well what
you have is a real-time data stream like
for instance queries and you might want
to get a real-time response of aggregate
statistics and I'm going to make the
following relaxation namely it's
perfectly okay if I don't quite get the
exact answer as long as it's reasonably
good and I want to be able to get
aggregate counts as an i want to be able
to ask you know how many times did I see
a particular query in a particular time
range and unfortunately of course the
set of queries or whatever other items
is not known beforehand and what I want
to make sure is that I have perfect
scaling as in more machines means I can
process more data they mean I have
higher fault tolerance they mean I have
higher accuracy I can also put more
requests out of it and I'll now show you
how those very simple idea say those
hashing ideas that I demonstrated before
can be used to extend something like
count min sketch immediately to such an
algorithm why do we care well you know
if I have stuff that's you know several
days old well okay maybe I can use a
tube or whatever to do offline batch
processing this is for the real time
sketching system and it makes for very
simple system so the three tools that
will need is the complement sketch which
I'm going to explain to you now then
consistent hashing what we looked at
before and then some interpolation
tricks in order to look back in terms of
the marginals okay so here's the
compliment sketch so who who in the
audience has knows how a bloom filter
works cool okay now think of the bloom
filter with integers rather than bits
and you pretty much have the company
sketch now in jail okay so here is what
you do you basically take one row per
hash function so that's a little bit
different from a bloom filter and the
insert algorithm works as follows
for a given key you compute for each row
the hash for this key and increment the
corresponding bin by one at query time I
go and look at all the pins that I have
touched and I take the minimum over
those pins so why is a good idea well
first of all and the assumption is we
only do insert they are interesting
variations on that theme but basically
pour on the inserts we know all those
numbers are non-negative furthermore we
know that each of those bins got
incremented when even we saw that
particular item so therefore each of
those numbers is going to be an upper
bound since all of them are going to be
an upper bound that can take the minimum
it's still going to be an upper part so
this is a very nice algorithm bike or
modem ludacris none which does this so
the guarantees and these are that's a
really amazing thing is I mean okay this
part you can see immediately that the
count that I get from this is going to
be a greater equal than the actual
number of cards you can also see that
this is and this is a little bit more of
a proof but it's actually very
elementary proof that it's basically
upper bounded by epsilon where epsilon
is basically essentially one over a
number of pins times the total number of
inserts that I made furthermore if I
have a power law distribution this is
actually even better so this is a very
very nice sketch because it's actually
the first and I'm not sure with it so
far still the only sketch that has an
order one over a number of pins
dependency rather than one over square
root number of pins and so essentially
the way how you prove it is you okay
globe on this trivial but the upper
bound you first prove the expected value
and then you essentially show the
probability that you have a certain
deviation per half Rho and then you use
essentially just you know multiple you
just boost the probability by
multiplying them in order to show at I
pound so it's a very very immensely
proof and you will enjoy reading that
paper it's beautiful so the problem
however is well you know the accuracy
while it's really great it's like 1 over
r of beans well at some point I run out
of memories so if I want to higher power
higher accuracy what I can't furthermore
this works beautifully if I have a
single machine where I insert if I have
many machines well you know I first of
all need to be able to actually process
the data secondly i need reliability and
then finally this is a fully static
sketch sketch what i want to be able to
do is actually get the similar estimate
for time series data okay so the second
tool we're going to use is now an
application of consistent hashing so and
i'll show you this in about three simple
steps so first of all how do we increase
throughput well that's really easy right
we have a single machine now we have
several mesh servers and I just charred
my keys over those servers in other
words I just you know uniformly
distribute the keys over you know
through this argument hash and I could
use something different but argument is
good enough for away for that use
imagine thousands of servers now this is
really good right because you know the
accuracy actually goes up like one over
a number of servers because now each
server gets only a smaller fraction of
the data and since the accuracy was
amount of data inserted over memory
locations my accuracy goes up subject to
all small log factors but I'm ignoring
them here furthermore the throughput
increases because if I have K servers i
can insert k times as many keys but I'm
really screwed because the reliability
decreases so if I lose one machine
basically my system goes now how can i
fix this now let's look at that part so
suppose you know here we have various
hashes each of them go all of them going
to one machine well there's absolutely
no need for that I could go and
essentially insert one hash per machine
and then the good thing is you know I
would really have to kill all those
machines in order to lose all
information about that data okay so in
other words I now go and insert into you
know k servers rather into a single one
and that way I gained a reliability
this is great the accuracy actually
increases because now each machine needs
to stall a starter the throughput is
more or less constant depending on my
inter-process communications library but
the problem is that you know the latency
actually goes up a lot for queries
because I need to wait until all servers
have returned the argument and there is
no acceleration okay so that's pretty
much the opposite end of what i showed
you before in terms of scaling up now we
increase reliability okay next step can
we actually increase queries throughput
well that's fairly straightforward right
I just over insert into more servers
let's say I need for hashes to get a
good answer i insert into maybe eight
machines and if i have a lot more
queries than inserts you know i just
pick a random subset of four machines
out of the eight that have my daughter
and I'll be fine and on top of that i
gained reliability so this is also easy
now let's just combine the tricks so the
trick is basically you assign keys only
to a subset of machines you over
replicate them for reliability and you
over replicate them for query
parallelism and there'll be a picture on
it in the moment so what you do is again
you use this consistent set hashing so
now for each key there will be another
random subset of K machines that are
responsible for it each of them gets one
hash function make just sure that you
seated with the machine specific key
otherwise if you have the same hash
function on several machines you get no
improvement it's just if you implement
it as useful thing to know and then you
request from a smaller subset of those
machines that can you can use consistent
hashing to have a random subset
depending on the client so then you can
actually prove of useful things for it
so the useful bit here is I mean I mean
simplifying all of this essentially you
need about one extra machine to hold
your sketch if you have let's say ten
failures and at least twenty machines so
the argument goes as follows you
basically can pick you can
hashing randomizes the inserts of the
machines an adversary cannot really
scream in an arbitrary bad way because
so therefore I can essentially
randomized over the failures if I assume
that insert into less than half the
machines in the machine pool I can major
eyes the drawing with without
replacement because basically if your
machine is dead well I can't kill it
again why something where the machine
gets killed over and over so this upper
bounds number of failures that I can
experience and then it's a very simple
three full line analysis where you just
have a by no just a binomial expansion
to get that inequality so it's more
detail in the paper that's at the moment
on the submission it's a very very
simple analysis so just to show you the
picture of what's going on so here is
our machine pool and we decide to insert
into those four machines and for a query
I picked a random subset of those three
machines to get the answer this scales
linearly with the number of servers in
terms of accuracy reliability and well
scalability so this is kind of nice now
I haven't told you yet how to fix the
temporal aspect so the time series
interpolation is in chips as follows so
the compliment sketch is a linear
statistic that's the key idea so in
other words if I want to sketch to say
it's I just some the sketches and
everything's fine so if I want to get
the sketch / long a time interval I take
the first sketch the second sketch and
then sum them up easy so the other thing
is I can also reduce the bit resolution
by essentially folding your sketch over
onto itself and so and the good thing is
I can do that with a process start as
opposed to ever having to touch the
original data game so we first show you
the time aggregation so what we are
doing is something actually very
straightforward we pick an exponentially
increasing set of intervals so but you
want to 48 and so on and so each to to
the at each to to the end time steps
I need to compute okay so it's not to
end but 2 to the N sorry that went wrong
I need to recompute all the bins up to 2
to the air the trick here is that I have
a cumulative somewhere have the once
twice that makes everything work out
nicely such that now the cumulative sum
is basically just powers of two in other
words if I want to compute that interval
I just need to sum over all those
intervals if they were consecutively
aligned so what I do is I basically make
those intervals age until they are
perfectly aligned then I do an
aggregation sweep and I get the next
interval I'll show you that in picture
in a moment the good thing is I only
have to write into the first bin this is
the only one that's really going to get
all the inserts everything else is fine
so I have reasonably memory locate
locality and you can actually show that
the aggregation is basically log log T
time in terms of the aggregation cost so
it's a non-issue the good thing is we
get logarithmic storage in terms of the
time interval that we sketch so let me
show you the intervals let's say first
we have interval 1 then ok ages we write
into another interval then it ages now
we have to now we basically go some of
those two up and get an interval of
links to now this one gets it this one
gets aged ok so now please move over now
we can compute an interval of length 4
and so on and so on and we can go on ok
so we have length 1 2 4 &amp;amp; 8 however this
is stored in memory in that way right we
always right here and these intervals
age the other thing is just basically
going from you know backwards or
forwards in time now I can do a similar
thing with a bit resolution so if i have
my bit resolution Avery 2 to the n steps
you can easily see that therefore you
know a length of two to the N will
always require the same amount of memory
so the first one for this for you know
time intervals two and three I need the
same storage for four five six and seven
in aggregate I need the same storage
so why did I show you those two
techniques right I mean one in a way
gives you a good aggregation over time
the other one gives you an aggregation
of items so we're basically trading off
to two knobs here we can either reduce
the item resolution of our sketch and
keep the time resolution accurate so in
the most extreme case maybe for
historical data that's two years old we
will only know that well in that
particular year basically how many times
that the query machine learning was
issued in a particular year the other
extreme is that for you know one year
ago exactly around midnight I have maybe
a million queries right but they won't
know anymore which ones they are now
that's effectively you know pinning down
the marginals of a joint distribution
which I would really like to be able to
store but which I can do because of
storage costs and this is fixed quite at
least approximately fixed right in just
taking the product of the marginals this
is a good estimate now you would
immediately say well hey days some there
are so many smart algorithms out there
is like you know just any exponential
family sketching a you know estimation
algorithm will do better sure but these
algorithms require more than all the one
time in order to get a number so
therefore we do the really cheap and
dirty thing there's actually an
interesting question of what you would
do at higher order marginals okay let me
show you very briefly that it works okay
so data that we had it falls a nice
parallel distribution and okay so here
are three sketches that you will see and
it's stratified an interesting way so
okay you can't really see the colors but
basically we compare the sketch which
just aggregates over the items you know
which basically at each two to the n
time steps reduces the item resolution
one with one that basically just you
know makes longer and longer intervals
and then essentially predict a piecewise
constant function and then something
that uses the product the green line is
the product and you can see it's doing
reasonably well the other thing that you
can see is
as we get more and more data this blue
line which basically just reduces the
bids does a bit better and that's not
very surprising because if you have a
very heavy hitter if you look at the
original commode muthukrishnan
guarantees you can see that they
basically are absolute deviations and
well I've plotted relative ones so if i
have a very heavy hitter then it doesn't
help me so much if i have collision with
a lot of your relevant in trees so you
could probably do something smart to you
by switching between those two
estimators the fact that these purple
curves have slight peaks here has to do
with the fact that this is a non-uniform
volume so of course if you have a lot of
volume well the error will also go up
especially if I estimate something as
piecewise constant similar figures i can
show you four different error measures
and things behave qualitatively the same
this is probably quite interesting here
is the number of read requests and on
the next slide a number of insert
requests over now between well basically
one and 10 servers obviously if i have
if i want to read from at least three
machines i need to start here and from
five machines and it's right here but
what you can see is it's basically
linear scaling so this is as good as it
gets similar thing here you can use the
same tricks for other sketches so you
have for instance a space-saving sketch
in that case well okay I can't really go
into much detail how it works but
basically you use the very same
machinery ok we're going to skip
peer-to-peer and I'm quickly going to
show you more or less the same picture
here so for distributing random
variables in an inference procedure
again you have essentially a tree while
you basically need to build a replica
tree and that replica tree is
instantiated through a tree which
respects you know the layout that you
have within your cluster within Iraq you
want to be as flat as possible we use it
on a switch you have perfect you know
communication between the racks well you
probably want to use the well
take well whether you respect the fact
that you know you between rock band
which is a lot lower than the width in
black band width and okay I can talk
about synchronization distribution
folder and Joey told me anyway that my
times up here's a very very brief thing
that you sometimes may want to worry
about namely suppose I have you know k
machines they need to talk to another
set of K machines and suppose that the
message is that each of those machines
have is like you know one over number of
machines so basically the more machines
I have the smaller the smaller packets
each machine needs to tell to another
one so if I then just naively let them
all talk to everybody at the same time I
will get a very low data rate between
any pair this is really bad that is
basically by switching my entire tcp/ip
stack will create big overheads so one
way to fix this is to actually you know
schedule who talks to whom in a specific
order now if i were to you could always
think okay let's just pick a nice you
know synchronous order but that's not a
good idea because as soon as a single
machine is delayed all the delays parlor
and bad things happen so what you can do
instead is you just essentially sample
with replacement by picking a random
schedule for each machine now if you do
that and i just picking up one machine
at a time i create two problems one is
some machines will be entirely left out
so that's exactly the 1 over e that you
also know from the bootstrap and what's
at least as bad some machines will be
hot spots because they will actually
receive inserts from more than one
machine now there's a very easy fix to
it well you pick more than one but maybe
two turns out for is a good number and
there's a little inequality that i'll
show you in the next slide and so for
instance with two connections you can
already see that the efficiency of this
communication scheme is between point
seven eight and point eight nine okay so
how do i get those numbers well these
are basically nine machines
well this machine is left out so I
definitely can only get eight nine of
the efficiency to get that lower bound
you basically say well each local
machine spreads its network bandwidth
evenly between all the links and then I
just add up whatever I get here if I
have you know let's say 38 at ages
coming in I just throw one of them away
and that gets you the lower bound in
practice it's somewhere in between
that's just for this specific random
graph in general you can get this
inequality in the knob the limit of
large number of machines and essentially
for similar tiny antennas connections
are enough if you do that that's what
you get and I think now I've really
overstayed my time so it's just well two
billion documents not million billion on
the thousand machines and that's
basically a it's actually you know keep
sampler procedure to well do some user
profiling so the last slide and this is
essentially what you could almost use as
a somewhat of a research agenda is take
your algorithm ask what happens what
guarantees can I get yourself the
machines die furthermore i want to have
linear scaling as i throw more hardware
the problem the third issue is well I
actually have communication constraints
because well I have non-uniform
connectivity structure within my service
centers and I want to keep parameters
local and this gives me a lot of really
fun questions one corollary if you
design the algorithms try to avoid
having a master because masses are
single points of failure and you have to
work hard to make them go away okay
that's all I want to say
it sits
you're solving here
and the stuff that he was done
sighs words
it's very sensitive and well we didn't
be great
we need very small canals of Chuck
changing that actually be able to throw
started basically he was able to an
opera who say anything beyond that is
just a bit worse time anyway yes they
are also vegetable ceramic pelvis
the mac repair people climb
that's
working and so on which we have to worry
about ruin me happy well they see our
whole endeavor dragons are essentially
as well instance of what you give we
have load up your ability are the
requirements at least these scenarios
and what will be playing around
you want to make sure that partitions
the rascal bar usually seem to take down
low 15 and trying to skating service and
i will probably actually work very well
Thanks I mean next to the point of an
hour
I mean a lot of really cool things is
getting a car recently hasn't made
in 5-10 years say
Oh baby
those bitches
let's sing our summertime</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>