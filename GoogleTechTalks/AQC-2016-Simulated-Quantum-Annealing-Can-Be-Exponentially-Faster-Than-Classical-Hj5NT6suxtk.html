<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC 2016 - Simulated Quantum Annealing Can Be Exponentially Faster Than Classical | Coder Coacher - Coaching Coders</title><meta content="AQC 2016 - Simulated Quantum Annealing Can Be Exponentially Faster Than Classical - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AQC 2016 - Simulated Quantum Annealing Can Be Exponentially Faster Than Classical</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Hj5NT6suxtk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Elizabeth is gonna go first and then
Arum but we can adjust to that also okay
Elizabeth okay thank you very much for
inviting me here to speak today I'm
going to be talking about a theoretical
analysis that we did for a particular
class of problem instances that allows
us to obtain rigorous results on the
asymptotic performance of simulated
quantum annealing and also compare that
with rigorous results for quantum
annealing and classical simulated
annealing so the first version of this
work we put on the archive in January
but I just put a updated second version
on the archive last week that made some
Corrections and also expanded the scope
of the results by a bit and just
recently this work was also accepted to
Fox 2016 which is a computer science
conference that'll be held this November
so I'm gonna be taking the approach of
comparing quantum annealing to other
metaheuristic optimization methods and
in the language we heard earlier this
morning these are non tailored
sequential algorithms that is the big
three quantum annealing simulated
annealing and simulated quantum
annealing now how are we going to answer
the big question of how useful is
quantum annealing well we've heard some
about impaired so far at the conference
you know you can build or numerically
simulate quantum hardware and benchmark
it on random instances but another kind
of approach the approach I'll be taking
our theoretical test so here you may
have things like analytically bounding
the spectral gap of a quantum annealing
system in the regime where the noise is
low enough for the adiabatic a theorem
to apply or you could do some work to
theoretically bound the efficiency of
classical simulations of quantum
annealing and in particular I'm going to
be focusing on simulated quantum
annealing which is a classical algorithm
for sampling the output of some quantum
annealing systems including the
transverse field that we care about but
in general the
of simulated quantum annealing is still
a very much an open question at least
theoretically we're starting to get a
pretty clear picture empirically so
let's go over the special properties
that make simulated quantum annealing
possible well we know it's possible when
the Hamiltonian has no sine problem or
put another way the quantum annealing
Hamiltonian has non positive
off-diagonal matrix elements in the
computational basis this immediately
implies that the thermal density matrix
is a non negative matrix it has
non-negative entries everywhere and you
can see this just by expanding that
exponential as a Taylor series and
noting that each of the terms in the sum
is non-negative okay but if we take the
limit as the temperature goes to zero
then the thermal density matrix becomes
the ground state so it's also an
argument that says that the ground state
of Hamiltonian without the sine problem
has non-negative amplitudes everywhere
and this property of the Hamiltonian is
called being stochastic which is a
combination of the terms quantum with
the terms stochastic from the field of
stochastic matrices so let's talk a
little more about the specific kind of
simulated quantum annealing that I'll be
analyzing so these are this is based on
the path integral quantum Monte Carlo
method one way to to quickly explain how
this works is that you want to express
the partition function as a discrete
path integral so you see in the first
step I just expanded the definition of
the trace and I can do this trivially
because the Hamiltonian commutes with
itself and then here I just inserted
several copies of the identity l is some
integer parameter that sort of controls
the discretization that we've done for
the path integral and so the property we
talked about on the previous slide that
the off diagonal matrix elements are non
positive that implies that these these
weights are all positive the terms of
this sum are all positive because there
they're eetu minus some positive real
value so leading to so this creates a
probability distribution on the space of
paths okay I'm not gonna have to get
into the Trotter Suzuki formula or
everything or anything because this kind
of illustrates the fundamental
difference between more general kinds of
quantum systems and this restricted
class of stochastic hamiltonians because
so Fineman told us that when you have a
genuinely quantum process you have all
paths contributing with some faiths and
all those phases of course have the same
magnitude but for in stochastic system
some paths are more important than
others okay so it's somewhere in between
the classical regime where you have one
path that minimizes the action and the
quantum regime where you really need to
count all of the paths so when you have
a distribution that you know places more
importance on some paths than others you
can start to think of exploiting a
general randomized algorithm called
Markov chain Monte Carlo so I'm gonna
break down these terms because you know
we often just say to them say them
together and I want to break down what
they mean separately so Monte Carlo
means that you're estimating expectation
values for a distribution by only
sampling a small number of points
instead of summing over the whole domain
now but this leaves the question how do
you sample these values from some
complicated distribution pi and that's
where you use a Markov chain
so Markov chain is a random walk that
takes place on the domain of your
probability distribution and the way you
construct this random walk or a
sufficient way to construct this random
walk is by designing it to have
transition probabilities that satisfy
this detailed balance property and so I
think in the physics community we
usually characterize the performance of
a randomized Monte Carlo Markov chain
algorithm by something is called
autocorrelation time I'm going to use a
slightly different concept which comes
from computer science called the mixing
time so this is the number of steps of
the random walk that need to be executed
to be close in trace distance to the
intended distribution and as we know
very well in quantum information once
you're close and trace distance this
preserves all of your expectation values
okay
so so that's the formula for the mixing
time but if you haven't seen that before
just keep in mind that the mixing time
controls your convergence to the
intended distribution you wish to sample
okay so now I'll get to the particular
class of instances that will be
analyzing and of course they're bit
symmetric cost functions with a tall
thin spike and in fact so their energy
plot will look like this or this is
maybe the the first example will
actually generalize this a bit on the
next slide but what you see here is on
the horizontal axis you have this your
your domain it's controlled by Hamming
weight so it's important to keep in mind
that this is really a spin system but
we're viewing it as one-dimensional
because the cost function only depends
on the Hamming weight okay and so the
way the cost function works is it's
entirely it just scales up with the
Hamming weight except for this one
special point where you introduce a tall
thin spike in order to create a false
local minimum here and this false local
minimum it was shown in 2002 makes the
problem take exponential time to solve
with classical simulated annealing
simply because it takes exponential time
to even propose or to to accept a
proposal to step on to that spike but it
was also shown in this work that this
problem can be solved in polynomial time
using a quantum annealer and so this was
kind of the origin of all these tall
thin spike problems that we've been
talking about now let's let's look at a
small generalization of this where we
now you know so you might say when you
look at this you might say that well
what if I allow my simulated annealing
algorithm to take steps of size 2
instead of size 1 then it'll still solve
this in polynomial time
so to defeat that we're going to scale
the width of the barrier polynomial e ok
so the height of the barrier is n to the
a and the width of the barrier will be n
to the
and so so now I'm gonna go through an
argument that was given in 2004 which
explains why the quantum annealing gap
of this system is constant when a plus B
is less than 1/2 so da nice argument and
it'll also create a nice parallel with
what we do in the proof about simulated
quantum annealing conversions so the
spikeless hamiltonian h tilde has an
order 1 gap and we know this because the
spikeless hamiltonian means that that
the spike term is not here and so the
cost function simply depends on the
Hamming weight and so it is a collection
of non interacting qubits where each
qubit prefers to be you know a 0 in the
Sigma Z basis so it's an exactly
solvable system when the spike term is
not there and it has a constant gap and
you can also see that the ground state
probability distribution viewed as a
function on this Hamming weight basis is
a binomial distribution and so the
amount of probability that it can put in
any one location is has to has to be
less than one over square root n so now
the next fact we want to point out is
that adding this spike term because it's
a positive semi-definite operator does
not decrease the energy eigenvalues they
all they all go up right so these u
tilde sub I is less than e sub I for I
now putting this together the argument
that Reichardt gave in 2004 is that we
can use the ground state of the
spikeless distribution as a variational
on sots we can use the variational
method of quantum mechanics to upper
bound the ground state energy and so
when you form that inner product that
expectation of the energy you get that
you have the ground state energy of the
spikeless system plus some term that
asymptotically goes to 0 when a plus B
is less than 1/2 so because the first
excited state energy does not go down
this implies that the gap of the spike
system is constant when a plus B is less
than 1/2
and the intuition is that quantum
annealing does not even feel a
modest-sized spike in this problem in
other words the wave function really
really doesn't need to change all that
much in order to minimize the energy
okay so now I'll get to the new results
which has stimulated quantum annealing
for the spike so we show that simulated
quantum annealing equilibrates in
polynomial time and finds the minimum of
the spike cost function whenever a + b
is less than 1/2 and in terms of
explicit bounds we find that it takes
orders n to the 17th time with single
site metropolis updates or order n to
the seventh time with worldline heat
bath updates
now these oat elvas simply mean that
we're ignoring logarithmic factors so
here are some of the key ingredients for
the proof ok so just to say what this
means is that simulated quantum
annealing works in polynomial time
so does quantum annealing but simulated
quantum annealing takes exponential time
so from this from these results you can
conclude that simulated quantum
annealing can be exponentially faster
than classical simulated annealing now
the proof idea is that we're going to
compare the simulated quantum annealing
Markov chain with and without the spike
turn we're also going to relate the
typical proper time that so here proper
time means time along these world lines
these quantum Monte Carlo configurations
we're going to relate this typical
proper time to the expectation values of
the quantum system and you might say
wait that's that's already we already
know that because that's the whole
reason we use quantum Monte Carlo is
that it relates to the expectations and
the quantum system but we we don't just
want the we don't just want the mean
values to agree we want the
distributions to closely agree we want
to we want a concentration of measure
within this within this measure that we
put on the space of paths we want to say
that the proportion of the world lines
that they spend sitting in the region of
the spike is never never that different
than what the quantum system itself does
and then finally on a more technical
note but we really prove is quasi
equilibrium of simulated quantum
annealing within a subset of the Kwan
Monte Carlo state-space and we use the
adiabatic path to guarantee warm starts
in this subset so of course a key
feature of Markov chains is that they
after you run them for a long time
forget where they came from and in fact
that's what the mixing time notion also
says but what we're gonna do is we're
going to where we're only going to let
our algorithm live in this physically
important region of the state space and
show that a quasi equilibrium that we
can keep ourselves in this region with
high probability using warm starts from
the adiabatic path warm starts means if
you don't know that term it means that
you're in a distribution that is close
to the intended one already as soon as
you start running the Markov chain Monte
Carlo okay so now a little bit more
background on Markov chains and mixing
times because you know I want to I want
to say this this is meant to be this
meant to complement the methods that we
saw earlier this morning you know we
heard about escape rates and instantons
and all this beautiful physics and I'm a
physicist so I like that stuff too but
we're gonna use completely different
proof techniques that that come from the
theory of Markov chains so Markov chains
of course essentially you can think of a
discrete mark time Markov chain as
corresponding to its transition matrix
this is a stochastic matrix because well
what that means is that the rows sum to
one and so if you apply this matrix to a
vector that represents a probability
distribution then you get another vector
that represents a probability
distribution a stochastic matrix
conserves probability okay and in terms
of the random walk this means if you
start at the point X you always have a
probability of going somewhere now with
this definition the stationary
distribution will be the eigenvector
with the largest eigen value and as
eigenvalue one okay and you can actually
control the mixing time that's what this
tau epsilon is the the time it takes to
be within trace distance epsilon of your
target distribution you can control it
with the spectral gap of this Markov
chain transition matrix okay so it
scales essentially inversely with the
spectral gap and then there's this fact
one over pi min but it has a log that
makes it typically not not as important
but this one over PI min really
characterizes that your random walk
we're considering the worst possible
starting point for that random walk okay
so let's talk a bit about how so you
know we know in quantum annealing
aficionados that analytically
determining spectral gaps is very hard
and so we need additional tools that
also characterize the mixing times of
these markov chains and one of the most
beautiful tools is this idea of
conductance and bottlenecks so we're
going to visualize our Markov chain PI P
Omega as a weighted graph the vertices
are the states X of our domain and two
vertices are connected with an edge if
there's a nonzero transition probability
between them so with these with this
with this picture in mind of this
weighted graph where the vertices have
weights and the edges have weights then
the conductance of a set is the
probability of leaving some subset of
the graph over the probability of being
in it in the first place okay so this
Phi of s quantity here we have in the
numerator we have this sum over all the
states that are on the boundary of s
that also have an edge leading outside
of s right so so let's think about this
conductance if it's large that if it's
large then you have a high probability
of leaving the set s compared to you
staying in it but if this if this
quantity is very very small then that
means that you have a large denominator
you have a high probability of being in
s but you have a small numerator a low
probability of ever leaving s right so
you can already get the idea that this
is going to imply slow mixing because
you're trapped in the set s and here
just just a technical note that PI of s
should be less than one-half it
shouldn't be the full state space
because if you're going to talk about it
getting trapped well you know there have
to be important regions outside of the
set s as well so anyway if you take the
minimum over all of these subsets that
are less than 1/2 of the weighted state
space if you take this minimum
then cheaters inequality as a result
that relates this to the spectral gap of
the transition matrix and therefore to
the mixing time of the Markov chain okay
so essentially what this says is that
the Markov chain is rapidly mixing the
mixing timescales polynomial e with the
system size if the state space is if the
state space graph has none of these
bottlenecks
now even the conductance turns out to be
pretty hard to work with in practice so
we're going to go one more step removed
and look at a technique called canonical
paths so what you do here is you design
paths such that between any two vertices
they carry an amount of probability
traffic PI X PI Y for those two vertices
and you design these paths in such a way
that they don't overload any of the edge
capacities that is the probability of
being on that edge and actually taking
that transition so this leads us to a
quantity called the congestion which is
the worst-case ratio of the total
traffic of paths the total traffic of
paths passing through an edge to the
capacity of that edge right so in the
denominator we have so we're taking a
maximum over edges of this quantity that
we think of as the capacity and then
we're summing over all the paths that
pass through that edge of the of the
traffic and we also need to account for
the length of the path when you define
this quantity it was shown maybe in the
early 90s that this can be used to bound
the mixing time of your Markov chain
so our proof is going to be very much
based on these canonical paths so here's
the idea the configurations the quantum
Monte Carlo configurations which do not
spend too much of the proportion of
their world wines sitting on the region
of the spike will have will have a
stationary wait in the spike
distribution the qmc for the spike
system that is very close to their
stationary wait for the qmc for the
spike list distribution so this leads to
a picture of the states
where we have this large region Omega G
I'll make a good colored in yellow of
states where the weight of the
configuration because okay thinking
about it so if the spin configuration
never touches the spike then it's going
to be identical in the Spike q MC and
the spike list q MC so most of the state
space actually these distributions
actually look quite similar but there
are also these holes so Omega P is
really the union of all these holes it's
the bad region it's the it's the bad
region where the QC configurations spend
a lot of time on the spike and so
they're stationary wait looks very
different than that of the spike list
case so the new proof technique we used
is that we show that you can use most of
the canonical paths from the spike list
distribution to construct a set of
canonical paths for the spike
distribution with the same with
congestion of the same order opiate it's
inside of a subset it's inside this
subset Omega G which doesn't have
measure one but measure 1 minus 1 over
palling in n so in other words where
we're trying to cut off a small part of
the state space that isn't going to that
isn't going to mess up our expectation
values but does make it very hard to see
that the simulated quantum annealing
chain would mix in this region in fact
maybe it doesn't mix in this region but
at some point we believe the adiabatic
path should be a necessary inclusion to
get stimulated quantum annealing is qmc
plus the adiabatic path right and we
need the adiabatic path to stay inside
the the good subset Omega G okay so now
I'm going to go through maybe some
technical calculation stuff or just talk
it through so when we're talking about
heat bath world line updates so here x1
bar through xn bar xn bar yeah think of
these as the world lines of the n qubits
right so so of course the quantum Monte
Carlo configurations are these length L
sequences of n bit strings but here x1
bar is just a length L sequence of bits
the world line of that
qubit X sub I okay so how does the heat
bath world line update work I know a lot
of you know this you know because you've
implemented so you're going to select
one of the world lines uniformly at
random you're going to leave all of the
other world lines untouched and then
you're going to resample the world line
you selected you're going to delete it
and resample it from the conditional
distribution that is conditioned on all
the other spin values it's completely
standard now the congestion for this
Markov chain for the spikeless
distribution all my tilde quantities are
the spikeless system the congestion
turns out to be order n squared and it's
essentially because the paths have n
steps and in this step where you select
a single I that means you have a 1 over
N probability of selecting any
particular world line so so this bound
you can already see it's not tight
because this is a non interacting
spikeless system we know that the heat
bath world line update should mix in
time n all right as soon as you touch it
touch I mean the world lines don't
interact so as soon as you touch a world
line you never have to come back to it
you're sampling it straight from the
stationary distribution so this bound is
not tight but the reason we need to use
it is because it's the tightest bound
you can get with the canonical paths
technique and our comparison method
doesn't port the mixing time it doesn't
say that mixing times are the same it
says if you have a set of paths for one
of them then you can construct a set of
paths for the other so that's why we
lose a little bit of tightness on this
bound now I also want to discuss that if
we're using non tailored if we're
treating this as a non tailored meta
heuristic method then these heat bath
world line updates actually takes some
non-trivial time to implement with in
other words without using the symmetry
of the problem so the way this works
well first let me say in words how we
implement heat bath world line updates
right so you you've now deleted this
world line after selecting it you now
have to look at all the other
transitions in the rest of the system
and break your world line that you're
updating into these time segments right
and you also need to introduce
so you need to so breaking it into all
of these time segments and then you
compute the effective potential due to
all the other spins that's what the
conditional distribution means now you
also need to generate you also need to
generate some new jumps in the world
line that you're updating so let's try
to let's take what I said in words and
count up the factors to get to this
bound so we know with high probability
there will only be order beta log n
jumps per in and given world line and
this is essentially because it follows a
binomial distribution with mean beta I'm
using the so my adiabatic transverse
field is just order one so that's why
that doesn't appear otherwise it would
be you know gamma times beta where
that's the transverse field but you also
have to account for these deviations
away from the mean so that's why this
Login has to be there if it weren't
there that we wouldn't get the right
expectation values now I said you count
up all the other jumps and the rest of
the system to create your effective
potential whether there's going to be n
beta log n segments there and then
finally when you chain
you know as you're going through and
flipping a weighted coin to decide what
these new blocks are you have to check
all the spin values in all of the other
world lines so again you might say like
what is the point of doing things you
know this way why don't you take
advantage of you know some obvious speed
ups you could get but we're we're really
trying to make a fair comparison and if
you do that then the best you can get is
order N squared beta squared okay so now
we're ready to kind of put everything
together and sketch what the whole proof
looks like so the congestion of the
spikeless chain is order N squared and
the congestion of the spike chain is off
the same order in this set Omega G so
this implies that the mixing time within
the set Omega G is order N squared log
pi min now pieman is just our worst-case
starting point at each step of the
adiabatic path in fact here's how we can
analyze pi min so so if you look you
know if you're familiar with Q MC you
know that the thing you pay for
most the thing that most lowers your
weight of your configuration are having
lots of imaginary time jumps but because
there can only be order beta log and
jumps in any particular world line then
this sort of pieman scales this way once
you've taken the log okay now the other
thing we need is that it suffices to
take essentially this linear schedule in
order to fulfill the warm start
condition let me just briefly say what
that is because so so you're running the
chain inside of Omega G and if if you
start very close to pi then you only
have to run the chain for a very short
time inside of Omega G if you have to
run the chain for too long you would
have to high of a chance of going into
the bad subset so that's why you have
this sort of window not too short not
too long for which you can run the chain
and still stay in the good subset Omega
P and to do that you need these you need
steps at least this small along your
adiabatic path and as we saw in the
previous slide one step of the world
line update chain takes time order N
squared beta squared so finally if you
count all these factors together and use
the fact that that we actually have a
beta that scales with n and this was
necessary in order to sort of defeat the
density of states and be close enough to
the ground state to make those
properties I said about the distribution
beings very close hold and so if you put
all these things together then you can
get this upper bound of order n to the
seventh runtime and and I think that's
essentially the best you can do with a
fair comparison that isn't you know we
haven't optimized the adiabatic path or
anything like that we're just trying to
run it as a black box and I think that's
about as tight as it's going to get with
this method of analysis ok so we've
shown that simulated quantum annealing
can inherit some of the advantages of
true quantum annealing and this allows
for a provably exponential speed-up in
the asymptotic performance of oh yeah
prutte privily exponential separation in
the asymptotic performance of simulated
annealing and semi
quantum annealing can we leverage the
ideas in the proof to better understand
the stationary distribution and
convergence properties of sqa and more
general systems you notice a lot of the
techniques that I sketched don't
actually use the bit symmetry of the
problem really the the best thing that
gave us was that we were able to solve
the spikeless case and we knew so much
about the wave functions
lastly the future is bright there is a
lot to do thank you okay we have time
for questions okay Daniel so are you
able to relate this speed up to
tunneling or some other a nice quantum
feature interestingly we did not
necessarily take that approach so the
previous talk did a very good job of
showing us what the tunneling looked
like in terms of the generation of this
instant-on but here we had almost a
completely different set of proof
techniques so admittedly we don't
actually we do we don't actually see the
tunneling necessarily but we have used
the fact I mean I think what generalizes
about this is the fact that we had a
case we could solve the spikeless case
and we had a case where we introduced
the spike and it didn't change the wave
function too much and so now that we've
shown that the simulated quantum
annealing distribution picks up not only
its mean values but it's higher moments
from the quantum wave function that this
situation if you have rapid mixing in
this you know in the spikeless case your
base case then in the case that's not
too different now you can have rapid
mixing there as well
so tunneling is great but that's not the
proof technique we used okay yes
so hello hi I admire very much the
ability to do analytic bounds like this
if I tried to do that I would not be
able to compete with you the best I can
do is numerically simulate things brute
force and I've done that for some of
these
problems that you discussed when I find
that when you include the interaction of
the system with the bath sometimes we
can get things that are slower sometimes
faster than the adiabatic theorem would
tell us so in one of your last slides
you said that this is the tightest that
this bound is going to get and I wonder
if you have also explored what these
bounds start to look like when you
simulate I haven't seen analytic bounds
when you're when you have a system
coupled to a bath absolutely let me
that's a good question let me say a few
things so when I say this is the best I
can do please let me qualify that by
saying I'm very much following this
black box approach you must run an
adiabatic Li you must converge to
equilibrium once you start relaxing
those conditions you can solve these
problems in constant time you can just
rush to the end in a fully diabetic way
Daniels group has this paper where
there's a diabetic cascade that allows
you to to solve these types of problems
in in constant time so it's you know
it's meant to be a toy problem I mean
the value of the toy problem is to run
our methods the same way that we run
them on more complex problems as well
now as for finding a theoretical
analysis of these diabetic transitions
essentially the specialness of
stochastic Hamiltonians pertains to the
ground state or the thermal state and
once you leave these states while you're
running into sign problems all over the
place and you know we might have to
download that open source package can
you generalize your technique to to
treat other kind of ward line moves for
example worm-like moves where you open
word lines which typically are more
efficient than closed absolutely so
first of all the the proof as written so
when I started this work I didn't know
that you were gonna discover how good
open boundary condition quantum Monte
Carlo are or some people in this room
have now found that that's a relatively
recent development but the same proof
applies and I've also been looking at
things like you have worm algorithms
right these are more for condensed
matter systems yes some of the same
proof techniques do apply I just want to
say that when you have he paths where
line updates these didn't appear
first version of the paper because these
canonical paths you think of them
flipping one bit at a time once you have
these hugely non local updates
it does kind of make the analysis more
difficult but but yeah I have some
projects in the works that are looking
at things like the worm algorithm yes
we'll have just one more question
I've got a Mike Fincke thing huh so
today I got sort of two questions one is
just clarification so you in your final
algorithm you have to run it you want a
quasi equilibrate that's fully so that
means you don't want to run it too long
yes so you've put a lot of effort into
making sure that you're just treating
this as a black box
but how do you know how long when to
stop running it does that depend on the
specific problem specific Hamiltonian
okay I mean I think that's a fair
question and the most I can answer there
is that when you've gotten into the bad
region you know it you can see it
immediately it's just defined as the
time you spend in the spike region so
then you just abort the algorithm it's a
black box how do you even you don't know
if it's a black you don't know that it
has a spike I mean that you're sort of
treating that as a toy problem of course
you could just output the answer but
you're sort of you shouldn't how do you
how you're gonna know is that you're
gonna be in the bad region if you don't
use the specific form of Hamiltonian
that you're okay that that is also a
good question so I mean I think that
similar to the number of steps we need
in the adiabatic path that this is
something that if you have a true true
black box you're going to need to do
multiple runs you know that I mean
that's the only way to see it so so yeah
we do have to be careful qualifying the
order into the seventh but I think it's
a reasonably black box approach and my
slightly less mean question was so you
have put all this effort into making it
relatively black box I mean trying
carefully not to exploit that so can you
then go back the other way and or have
you thought about say okay take your
proof technique and then just see how
far you can push the the how much how
far does it generalize to other
distributions that will where it'll just
go through again yeah I mean I think
that's that's very interesting and in
theory the machinery could work it
doesn't depend on bit symmetry but the
question is can we
these examples so for example these week
cluster instances I thought a bit about
that but it's you know very hard to see
how we can fully understand the quantum
wave functions well enough to apply
these techniques so what I would like to
work towards is making these techniques
a little sense little less sensitive to
needing to know all these details of the
quantum system so that we can move to
less exactly solvable systems but I
guess you could also start with any
exactly solvable system and just
essentially instead of adding a
perturbation to something exactly
solvable and showing this things still
go through essentially you could start
with any exactly soap would and you know
yeah I mean so we should be careful
about calling it a perturbation because
the spike term is tremendously large
when you're sitting on it but yes what
one could think about this and I think
the the other important question is
would these exactly solvable instances
be sort of prototypical of interesting
quantum annealing problems if you know
of any unsure you know talk about it and
we can maybe apply some of this thank
you thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>