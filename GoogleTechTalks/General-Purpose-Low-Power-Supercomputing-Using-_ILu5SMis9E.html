<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>General Purpose, Low Power Supercomputing Using... | Coder Coacher - Coaching Coders</title><meta content="General Purpose, Low Power Supercomputing Using... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>General Purpose, Low Power Supercomputing Using...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_ILu5SMis9E" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi so I wanted to get the talk started I
want to introduce a professor Bob
broderson who is my thesis advisor and
it's very special part of my life bob is
a very distinguished professor at UC
Berkeley he's a member of the National
Academy of Engineering many many many
awards been a professor since the mid
70s when an award for a top10contributor
over the past 50 years to the ISS CC
conference and but an amazing statistic
that Bob told me a little while back is
that he was looking over his statistics
or whatever and found that he had had
had over a hundred and fifty graduate
students I guess is that masters and PhD
over the years and I'm very proud to say
that I was one and wouldn't be where I
am without himself there you go thanks
bot thanks a lot hi right appreciate
that well okay so I'm gonna try to do
today I know this is google is not one
of the big high-volume semiconductor
operations okay so it's a little bit
different take they have usually people
I talked to but I guess what I really
want to try to show you today is I think
what's really going on in sort of you
know where you know computations going
and so that's kind of what the story is
all about today I'm going to start off
with actually is some sort of background
on sort of architectures okay so you'll
what makes something be very efficient
okay and clearly i think one of the big
numbers right now is power efficiency I
think it's not I think it's a problem
you might have probably have here it's
clearly a problem that's in the general
purpose computing world right now I mean
Intel could make faster processors they
weren't power limited right now so we're
in a power-limited regime and
computation so how do you deal with that
and why is it as white as a problem now
so this is actually a chart I did a few
years ago but it's really interesting
okay so let me just kind of walk you
through this with this all about the
chip number thing at the bottom there is
actually I could you it's connected to
actual designs that were done in and
published it is SEC that had all the
information I needed to do this chart
the axis on the other side here is
energy
efficiency this is millions of
operations per second per milliwatts now
that's two rates over one over the other
right so that's really an energy
efficiency number when the time gets
sort of cancelled out so it's actually
energy per operation okay so you think
any function you have to do has so many
operations it needs to do okay well you
can figure out how much energy it takes
that's kind of the fundamental thing you
have to sort of worried about when you
talk about how much power is going to
take okay now was it go across there I
have three basic groups microprocessor
general-purpose DSP and dedicated and I
usually go across this which will see is
that the numbers go from point 1 for the
microprocessor then these are Pentiums
power pcs you know those kinds of those
are what those chips really are I mean I
can give them to you if you're
interested you go to general purpose
DSPs and these are these DSP chips like
text insurance makes and other places
right that have some level of
parallelism is beginning to dedicate it
to a given function okay and then
there's these dedicated chips on the far
end over there okay now notice the range
we go from point 1 4 micro processors
real Pentiums and so on up to 100 couple
hundred for the dedicated chips and this
is actually a couple year ago slide
actually the numbers now are actually i
think the spread is even more that's a
factor of a thousand okay a thousand so
get that in your heads here i mean well
know that dedicated chips that we do are
a lot more efficient than something
that's general purpose and big part of
it because the flexibility you have to
put in but the number that needs getting
your head here is it's a factor of a
thousand potentially okay now
intermediate positions like these DSP
chips of TI or 10 times are so better
than a pentium and that's why TI can
make a big business out of this stuff
but we're still a factor of a hundred
less than this dedicated stuff okay so
here's the question why what's going on
here i mean this underlies the real
argument if we can understand what
architectural e is allowing these
dedicated chips to be so efficient maybe
we can begin to use that in some sort of
general purpose environment so we can
really have a much better efficient
solution something to replace
effectively the von Neumann architecture
ok so let's look at three chips and I'll
do this pretty quickly one's a PowerPC
ones that
neck DSP chip okay and another one will
be several wireless LAN Zhu probably
wireless LANs like you're using here
okay so this is sort of just three
examples and we'll look at those and see
what's going on in those three the
architectures of these three chips we
start off with this powerpc thing runs
about this a few years ago so it's
running about a half 500 megahertz it
only can do two things at a time it has
an integer and floating-point unit so it
has do two operations in each clock
cycle so effectively can do like 900
mips and i kind of roughly let mips be
equal to mops which mops mean an
operation okay so basically we're
talking about 900 mops of performance it
takes about 7 watts you can take that
ratio you get this mop sperm Illawarra
point 13 okay this was the best the
power best of the von Neumann classical
architectures you know in the quarter
micron technology that this these chips
were all common ok let's take the neck
DSP chip and let's look at it for a
second okay mops there it has four
parallel DSPs and this is okay and
inside each of those DSPs actually
there's four operations could go on
there they pipelined it and so they sort
of allow that to happen so you basically
get 16 operations per clock cycle the
clock rate drops actually down to 5 50
megahertz but you still get 800 million
operations per second so it's the same
as the other but it's only 110
milliwatts ok dropping the clock ok has
some big repercussions and how much
power it takes and it particularly it
has repercussions if you can fool around
with a voltage but honored so you get a
factor of seven so this is factor of 10
better than we got with the PowerPC what
was the key thing that happened here
it's not that we drop throughput we
still have the same throughput but what
we did is we allowed ourselves through
parallelism to be able to drop the power
consumption so this is fundamentally
what behind it it's looking like
parallelism is the way to make this
thing go and we see this happening with
Intel Intel after fighting parallelism
tooth and nail for so many years right
finally is decided well we can put down
two cores on a chip okay right
breakthrough yeah all right
the actual layout memory memory is
actually a really interesting thing you
say well jzo most the chips are all just
memory anyway the little small parts
just two processors why you were about
the processor part the reason it's the
architecture that really does that think
what's happening you got this little you
know this the processor unit okay which
is running faster and faster what do you
do you have to queue up intermediate
results the feed that little the
processor part of the things so as the
chip as the clock rates go faster and
faster if you don't have parallelism
like this which f do is you have to
store all the intermediate results to be
able to feed them down into the
processing unit later on so in fact it
still it's actually a failure of that
architecture which requires more and
more memory as you try to speed things
up more rapidly here you have less
memory because you got more processing
units the data is actually closer to the
processing units and you so you begin to
don't have that bottle that going into
memory so that's really actually a part
i'm going to talk about but really a
another fundamental flaw in the whole
von neumann basic architecture so let's
go to the file ask whether this is the
dedicated one ok now this doesn't have
any flexibility are very limited
flexibility but the numbers are
interesting ok so this is a 802 11 chip
ok so this is you might be seen your
laptop basically has 500 things
happening in each clock cycle ok the
clock rates only 80 megahertz but you
get 40,000 million operations per second
on that thing right you divide those two
numbers out you get this mop sprint want
number of 200 right now and 90 nanometer
technology that number is actually
around 2000 if you do something really
dedicated you can get 2,000 you know
mops per Mille wat every milla wat you
get basically to you know billion
operations per second Wow huge
improvement incredible efficiency why is
that happening it's because of this
incredible level of parallelism that we
have here so that's the key now what
made this one so good was its
heterogeneous parallelism it's just not
peril ISM the problem is if you start
putting down a whole bunch of course
it's hard to figure out how to partition
your design or your our Corinne into
this homogeneous parallel architecture
if you have the freedom to sort of do
anything you want just put down
the parallelism that exactly fits what
your problem is and if you're able to do
the communication between it and if
you're able to put the memory where it
needs to be to sort of feed that those
processing units then you can exploit
this level of parallelism but that
requires things to be heterogeneous so
that's a big difference from what we've
been talked about in that so the Intel
model putting multicores down okay
that's homogeneous now you've got a real
programming problem to figure out how to
map your algorithm into that homogeneous
basic parallel structure so the bottom
line is we want to have heterogeneous
parallelism with dynamic basically
communication with distributed memory
that we can put where we need it how do
you do that lasix do it but they do it
for one application okay so that's
that's nice but probably not what's
interesting here ok feel program or gate
arrays that exactly is what they do okay
what a field programmable gate array is
it basically uses a sikh like
architectures dedicated chip
architectures but it gives you full
flexibility and inter connecting these
things up you could make a pentium out
of these things if you want to you could
make a PowerPC out of an FPGA no problem
there's overhead because you know you're
not sort of you don't have designers
going in there and optimizing every wire
you have but the bottom line is you have
full flexibility to put down the
parallelism that just as you need it for
your application and you can change it
and you can change it in about 10
milliseconds so you actually cannot just
have to do it once you can sort of
change it as demanded as needed a
different process company you can change
it really key part of this fpgas work
very interesting very many years ago
three or four years ago maybe fpgas
didn't have enough capability they were
kind of logic replacement things the
thing is you think about happens to
scaling scaling does it allows you you
know as you scale down by lambda the
size of the minimum dimension the number
of things you can put down goes up as
lambda squared okay fpgas just exploit
that fully so what happened is fpgas
just in the few last few years went past
the computational capability of the best
microprocessors and even the multi-core
thing so what's and what's going to
happen it's going to get better rapidly
into the future so this
is the technology that's actually looks
fully exploiting all the Moore's Law
capability of our scaling we have and
you don't need to do it with high clock
rates that's the key because this really
supports multiple parallel high levels
of parallelism like I showed that 802 11
chip that's the kind of architecture you
put on the parallelism exactly it maps
to the algorithm you try to do ok fpgas
where do they fit now they're not as
good as the dedicated chips ok there's
certainly better than even ointment
process or even multi-core processors
right and actually I should just say
something when you put down multiple
processors ok and just put them all in
parallel you actually don't improve the
energy efficiency all right because you
double the power and you double the
amount of computation those two things
divide out so you do not improve energy
efficiency so by putting lots of
parallel cores down are using lots of
parallel processors and some sort of you
know network of workstation strategies
for getting a lot of computation you're
working down at that point one mops
Pramila wat energy efficiency very very
bad so you got to change the fundamental
architecture to get yourself worked up
at a different level ok big advantage
just intermission kid just looks like
i'll say something here this overhead is
large ninety-five percent or more of a
fpga is in the interconnect ok wow
that's really bad but you're still come
off a factor of fifty two hundred times
better yeah useful fpga is up on the l2
died 11
all if you may use an FPGA to make a
pension that they help on still better
okay so this would be that data point
right there is from xilinx chip okay
question is what's that data point
correspond to what's the architecture
behind that data point and the
architecture behind that data point is
the 802 11 like heterogeneous parallel
architecture maximizing the using all
the possible parallelism you possibly
can get on that FPGA ok yeah enjoy great
day intensive operation a little bit
with these pieces for the a6 is easier
for likely lost operation one piece
right I think it especially if I didn't
get it right let me hit me again right
what the right architecture for this
stuff really is is a streaming
architecture actually if that's what
your conclusion was because that is what
these ASIC architectures really are you
stream the data through processing is
going on at every different part of this
thing's all done in parallel right you
can have multiple streams going through
this thing and you can manage the
communication to the memory and the
storage that you have as needed because
you can put it in the communication
links so that's the perfect architecture
for it so that's that what you meant is
it was question
if you don't need to do much computation
for data element then you don't have a
problem seems like right I mean I guess
the problem is I mean this is for
getting high levels of computation and
high levels of memory bandwidth and high
levels of dis bandwidth to because
remember you're distributing everything
so if we forget about computation we
just want to get io bandwidth wow this
architecture is just perfect because you
can just put up as many parallel paths
ports as you want to inside in bringing
data in that to me seems like not
knowing the google search problem very
well but that seems to me the key
problem you guys have it seems like
you've got huge amounts of memory you
want tons of ports into that memory to
sort of be able to do relatively small
amount of things with it let me tell you
this architecture is exactly sports that
very well as well so I'll show you what
our basic machine we're building how it
does that actual chips well things are
getting better and better this is just
showing the scaling thing I talked about
if you these are figures of Merit but
bottom line this is these plots here the
one the bottom line there is a mop spur
mega hertz per million transistors just
shows the efficiency of the basically
von Neumann architecture is beating
worse and worse over time fpgas and that
figure meriden staying constant and so
it's basically it's sort of scaled to
how many transistors you have so it sort
of takes the the fact that you put more
transistors on a given chip out of the
picture this other one here is another
one is just how this raw capability on a
chip and that's what i said what
happened not too many years ago suddenly
you can do more operations on one of
these f pjs you can do on the fastest
the pentium or any von Neumann machine
ok that's hardware ok so we figured out
what really good ok heterogeneous
architectures we got a way to implement
them using this FPGA technology how do
you program it ok and this has really
been pretty much the stopper ok what
happened is this fpgas have been the
realm of hardware no gurus in some sense
people program these things sort of like
when they build an ASIC what's really
going on here I'm telling you now is the
capability has gotten so high on these
things that you no longer have to work
down at that very low level ok that's
the mindset you gotta change right it
doesn't need we do not program this
thing at the hard
we're level we should not program it
it's kinda like you don't program a
processor at the assembly level at the
machine level okay we can move up once
you begin to move up you could do that
when there was enough capability that
you could sort of waste some of it okay
and that's what's the situation is FPGA
so we got to move up to a higher level
of abstraction so we program this thing
now that we can get people who have real
applications to actually use it and not
have to be bottlenecked by trying to
have this thing programmed in some
hardware description language okay so
there's the change that this is it is
critical to things we need a we need
parallel algorithms we need to have
multiple paths to get in we have to take
the architecture and get it away from
the sequential description because
that's that if your if your description
sequential you stick it on fpga you're
not going to be able exploit all the
pair Elizabeth so you've got to have a
parallel road I would guess the Google
problems are very parallel the second
thing is you gotta have a language to
supports that parallel description and
that's really what the problem is use
try to stay and see you got a sequential
description at your core it's going to
be really hard to figure out how to make
that thing parallel okay right and you
know that's been a failure of forever to
try to figure out how you take parallel
you know paralyzed c or a sequential
description people call all sorts of
solutions of that there's a lot of
different ways going on at this course
it's the problem on the table right now
and as we move to multi-core technology
we're going to have to sort of
explicitly deal with multi threading is
one solution but it's going to have to
be lots of other ways we're going to
have you know lower levels of exploiting
the parallelism that's available inside
future generations because perelson is
going to have to happen the question is
what kind bunch of different languages
to do that we actually because we came
at this from a sim a signal processing
background so that's good now this is a
little bit different from what you guys
do but I'm going to follow that path
down just for a little while because of
the success its had in being a
general-purpose computing environment or
general purpose language even though
it's been it's driven from mathworks
matlab is where we use basically and
it's a graphical language called
simulate that we use its we're using it
for all sorts of stuff okay and stuff
like you know circuit simulators and
biological search you know problems okay
and so even though it may think well
geez that's just a DSP thing actually
people use it for all sorts of other
stuff now there's probably better
languages I'm not sure what they are yet
okay maybe
ones I just should there be some
examples okay so what do we have
available to us we have a way to
describe control we've got a way to
describe data paths and we got a way to
describe we can basically build up
higher level macros that people can sort
of program in so it's basically a
graphical programming language that you
can extend and it has the basic idea of
control and data path and communication
explicitly basically what you do is you
put down blocks they can be memory
blocks they could be control blocks they
could be data path blocks and you hook
them up okay right so it's a data flow
like description so that's why i said
streaming is really the core way to
think about this thing I mean what goes
across those lines is datos dated words
okay they move across those lines you'll
pipeline this thing to sort of be able
to want to have these multiple things
happening in parallel or you can have
parallel you can paralyze us in other
ways okay so this is a DSP like thing
but I think if you can see they're all
the fundamentals are here to be able to
program anything you want it I'll show
you some examples of how people are
using this and you can sort of see I
think it gives you gifts mine site that
you probably could program your problems
in something like this language or some
other language might be better okay so
here's just an exciting example this is
kind of what it looks like we actually
build out of a library of blocks it
actually are optimized for xilinx okay
and these blocks are multipliers and
adders and registers and stuff like that
so it's fairly low-level stuff okay
right you can build up out of bigger
things like we can do for our signal
processing world what I we do is build
like FFTs or Viterbi decoder szoke we
could block those up now you might have
other blocks that you would I want to
program up that you would then built use
as your basic you know building library
that you could send bill on top of
doesn't mean you have to do everything
graphical the key thing about graphical
is it explicitly puts in the parallelism
okay because all these blocks are
executing in parallel let's do it's the
hardware execution model and it shows
you how the data is moving through this
thing right and you can how you can
paralyze the memory coming from our data
coming from memory so having the ability
to explicitly define how the
communication works and how to
explicitly show the parallelism is I key
parts of any description you end up with
this is one but there might be others
this is a way to try to abstract this
even more
I'll talk with this project it's called
ramp it's an interesting project we
talked about the oh that point that i
gave you know was that a implementation
of a pentium or was that an
implementation of some really highly
optimized architecture for the fpga
there's a whole group of people across
all the top universities in the country
right now that are trying to go after
the multi-core problem okay basically
talked about thousands of processors how
are we going to program that you know
how are you going to work with it what's
the what's the you know the message
passing architecture going to use for
that okay big problem so what's
happening is MIT Stanford Berkeley
everybody's side let's get a platform to
work on and sort of work on this problem
all the issues will come they have it so
in order to work together with the ideas
well have big blocks will have
communication between use blocks in
order to share thing we sort of have
this any receiving and channel ideas
sort of in a language that supports this
they have people work together so I
reason I bring this up because i think
it's it's these kind of ideas that are
going to get get us how were explicitly
showing parallelism or explicitly
showing communication it's this kind of
stuff that's going to bring it to
applications like you guys probably have
it's not going to be coming from
simulate probably the coming of
something closer to this let me show you
how the excitement of this kind of of
use of these fpgas 24 actually trying to
solve the problem of what
multiprocessors look like because you
could implement the multiprocessor
systems on the FPGA arrays okay so let's
talk about the arrays we talked about
one chip we talked about a programming
model what the or the core is how you
might want to do it one of the hardware
requirements we really want to have okay
we want a reconfigurable platform that's
more than one fpga because we want to
scale this thing up when i have to
connect a bunch of these things together
and we need lots of Io I think most
problems we look at get interesting cuz
there's so much i owe associated with
them so here's the basic computing
element we work with take one of these
FPGAs from xilinx okay you can program
this thing to do anything you know it's
you know as I say it's got computational
logic blocks and you interconnect this
thing through a program strategy that
you know you just program all the
communication links this one is 1 30
nanometer this is actually a couple of
generations old this thing has 70,000
logic blocks has acts
to power pc cores on the chip they're
actually a very small percentage of the
chip okay it has three hundred
multipliers five megabits VES ram what
we did is we took that and we put around
it a basically ram okay so i think it's
like each one of those chips can have up
to I guess eight gigabytes of memory we
can put around each one of these chips
we have a memory bandwidth of twelve
point eight gigabytes per second okay
each one of those chips have 23 gigabit
per second serial links each chip each
tip number that now so here's the huge
amount of i/o bandwidth you have off
these ships available to you there's a
thousand pins on this ship and this is I
say a couple generations ago so there's
huge IO capability that you can connect
to memory you can connect wherever you
want to connect it to now this is the
basic unit we do this take that basic
unit and hook it together now you can do
this a bunch of different ways our idea
was let's just make it look like a
bigger FPGA and make it so it scales and
even bigger ones okay right so with five
of these things around there's a lot of
local communication each one of these
units I show right here basically have
the memory hanging off you can see down
there plus you have these 40 gigabit per
second with these serial links to
connect to other things like other
boards so we got five of these things
okay on one chip then you can connect
through these serial links to other
boards you can scale this thing up and
you can put this thing in sort of a
trees network okay you can then we do is
we have 10 gig Ethernet switches that we
could sort of crossbar and connect these
boards together any way you want to so
this is the strategy to move this thing
up to supercomputer level performance
okay you can put storage off this thing
you can go through these crossbars and
we use InfiniBand or you know 10 gig
either the future probably even faster
so this is what one of the boards looks
like okay this is it each one of those
chips you know can consume up I think
the board when fully everything running
on the board you know in mostly its
memory it takes the memory takes a lot
of the power is like a couple hundred
watts but the computation you do on a
couple of watts and this just shows the
vio you got just thinking Ethernet we
actually put off hdmi so we can look
high-definition video off this thing and
you'll get all sorts of other stuff
that's basically it looks like it's the
hardware bart now you think about and I
think some sense what the Google
architecture is right lots of you know
general purpose commodity micro
processors sort of hook together in
various ways okay yeah that's a good
idea all right I I think that's fine the
problem is your core element you're
using is the wrong one because it has
such low energy efficiency that's the
problem right what you really want to do
is you want to have the core element
based on these reconfigurable units and
configured in such a way that you get
these energy efficiency numbers that
I've talked about right so this is the
old way of doing super computers right I
think the way you want to do them now is
you take boards like I just showed you
you hook them up through very high speed
interconnect you basically are able to
exploit the available parallelism on
each one of these boards okay and that's
going to take you know algorithm work
lets you guys play the experts in or do
is we're building these things up right
now and we can see that we can get up to
sort of Quetta hop kind of performance
on these things xilinx has various these
projects they give us the chips which is
kind of fun right which and then
basically the ships aren't cheap that's
one thing good calling for xilinx here
these babies casa you know a couple
thousand dollars a piece doesn't mean
that has to be true forever because
Islands has got a really good thing
going for it there's not much
competition right and plus the other
thing I should mention about that xilinx
designed their fpga architecture
thinking not at all about general
purpose computing they thought about
this is sort of logic replacement what's
really interesting here the technology
just made this actually architecture not
optimized for this application viable
for this applications you think what
happens if people start to think about
well what really gonna use this thing
for his general purpose computing you
would do things probably differently
right so I think there's just another
factor of 10 or something sitting out
there just people have to start working
on the problem and think about how to
how to use them for general purpose
computing as opposed to what they're
initially thought about so how can you
scale this thing up you make racks of
these things I guess is going to like
your guys stuff right and so basically
put these things together you could put
memory off this so I think I say through
these incredibly high-speed serial links
that are available
on these chips I mean 20 of these
surtees links per chip okay you think
about jeez what you what you how you can
configure this in many different ways so
I guess basically this is getting some
numbers this thing we actually sort of
tried to figure out you know if you try
to build a supercomputer right now and
actually and and do floating point okay
forget about trying to do fixed point
stuff let's go after the supercomputer
guys right at their heart let's do
floating-point stuff okay you can do it
just fine in these and with these FX
chip so I'm tell you so things are
changing here with this technology now
here's a problem we generally sort of
our old way of programming these things
was we would sort of do a software you
know there's a simulink model I told you
this math works thing we would you know
sort of debug our algorithms and then
run it on a von Neumann machine you know
our fastest pcs whoever could get right
to sort of debug the algorithm then we
were downloaded into the FPGA the
problem is we start getting big problems
the six order or seven order of
magnitude different in time between what
it would do on our von Oy machine versus
what I was going to run on the FPGA may
we mean we could not debug the programs
right so you have to debug the programs
on the FPGA array itself well that's
kind of the sort of thing by the thing
as a general purpose computer sure
you've got a really fast computer you
don't you know do all the compiling and
debugging on some slower machine back
compiling for these things what's that
all about compiling for these things is
placed in route in effect right that's
really what that's all out that's really
slow on vanowen machines we've got to
put that on fpga arrays to run them
there as well and that's that actually
is a nice parallel problem it actually
doesn't seem that hard to do yeah it's
got to be done there's lots of things
after you turn this thing into so we
have a software in you know sort of
experience you know like we do with a
von Neumann architecture so we do as we
say ok let's go after this problem so on
the left here is basically the old
software approach you'll basically
design you know basically OS abstraction
of the hardware the old way of doing
FPGAs you had a big bunch of just
low-level hardware CAD tools right in
the center they're just a pain to use ok
and so that's kind of what everybody
thinks of the thing of fpgas well let's
abstract that let's do the same trick I
mean you don't have to know about the
internals of the hardware you could
abstract the way if you could willing to
throw away some of the performance you
don't need to get every last bit of it I
should say we never program these things
up to more than 75 percent of their
capability because once you do that you
start again in place and route problems
actually things slow down so just throw
away 25 percent no problem because you
still got your ending up with these sort
of Terror up level performance so
basically what you need is an OS level
it's living on that array it's
distributed it lives on each of the
different processors maybe the
architecture I showed you we put a the
control fpga in the center is the one we
communicate to to sort of sort of
program these things so we actually use
one of the power pcs that's sitting on
an interval an internal one and we run
linux on that so we actually that's how
we get sort of communication in and out
of the fpga ray so actually you log in
to our board and do the processing there
and actually we can run the place and
route tools on that power pc but they're
pretty slow but actually concerted just
get rid of the von Neumann machine well
I get rid of an external one we have the
ones sitting on the FPGA itself but I
think in the future we got it as we
start changing things will put all this
stuff inside you know the array itself
directly this is just more abstraction
of the hardware or the software flow so
it makes it look more like you know set
of good new tools and programming this
thing and amusing and so and we're
debugging since it's a graphical
programming environment we're using now
we have we're putting debugging in so
it's kind of like doing a dash G option
right we saw you'll slow it down we'll
put in things to keep track of what's
going on we can do break points we can
stop it you know basically we got to
make these things debugging this just
like we would in a software world we
don't want to think of this thing as a
big chunk of hardware doing run in debug
in runtime in sort of a bunch of ways we
can sort of you know deal with this the
out load stuff I got so much memory
available to us we can sort of just
basically capture traces and sort of
follow through what's going on so you
can source a man this is kind of clearly
not a solved problem yet right we're
sort of thinking about how you might
i'll do all this stuff but you can sort
of see where the problems are to take
this problem and turn it into a software
problem not get away from thinking of
this whole thing is a big chunk of
hardware which people did with phenom
machines a long time ago this BOS thing
was what we did this is we have forth is
what the student calls it and that's
fun I mean basically what you can do is
you log in to this and you view the FPGA
just like a file system there's memory
all over the fpga watts of we do file
system calls into that we have a
standard in and standard out that
actually interfaces with the hardware so
we're beginning to break the line down
between what's you know what software
what's hardware and certainly the user
should never know what's the difference
is right just that once you do certain
things boy this thing runs really fast
right okay so that's we started doing
this stuff we work on this project I
don't know basically five six seven
years and there's a b-1 this is the b2
when the B to begin to happen more and
more people got interested in this thing
so we got a whole bunch of different
people that are interested in this now
and I think this is kind of there's no
one doing no google search algorithms
that's for sure right but what I will
point I want to make here is that people
started taking this and taking it in
directions that we had no idea that
would make sense to do right and the
things that are closest to this are
basically there's the Stanford by a
mantic troop they're basically doing
some search things for a protein
synthesis and I mean you can sort of
thing like genome searches and stuff
like that that's exactly this
architecture is very well I'd you know
set for speech recognition stuff there's
astronomy lab the search rester textual
intelligence guys they love this because
they're doing is doing these and I got
some examples that but doing million
point FFT s you're looking for little
you know sine waves coming from the sky
that you know they can sort of there
might be some some sort of intelligence
out there huge amounts of commutation
computation so there's a tremendous list
of these things people doing a basically
signal signal processing stuff got
microsoft doing some things on it so
lots of different people i'll give you a
few examples here so a bit start
building things were actually been one
thing they don't cost very much that's
another thing it's kind of interesting
right we had we had a workshop after
long ago and i know 30 40 people showed
up we're billing these things with
pikeville at about 30 of them sort of in
process to be billed right now and we'll
probably do another run of twenty or so
pretty soon it cost they have the price
on this one
the price these things basically the
boards like four or five K you know from
the assembler and in the chips or
another four or five K right now for the
chips we're using right now so eight or
nine ten K you get something that's up
on the terra op performance level right
wow that's I mean people think what FPGA
is really expensive just normalize about
how much computation you're doing
they're not expensive all they're really
cheap and you've normalize them by how
much power they consume they're also
very very very efficient as I showed you
ok so that's uh what's going on so
here's what people did with them now
some people who are so stuck with von
Neumann like architectures right just
can't get away from it so they take our
be architecture to try to figure out how
to make a chip that does some sort of
this is a VLIW architecture with a has
deal controller on it which is a little
arm processor something like that but
they actually implement that
architecture on the array to sort of
solve the software issues just sort of
so they have something they can try out
then when they do the actual chip
implementation it'll run faster be lower
power if I get another factor of five or
ten i agree but the point is here you
can implement any basic architectures
you want inside these things that's what
point I want to make in fact the
difference between what the ASIC did and
what this would they actually give them
it wasn't five or ten times it was only
like two or three times difference in
speed between what the ASIC would be
with custom design versus what we're can
get just by programming up this FPGA
this is this ramp project which i think
you know i was talking to john wars Nick
I work with clothes in this he's very
closely in the ramp project I'm not so
much involvement but the excitement that
people have in this it's computer
science you know people across the
architecture research people across the
country the excitement they have about
doing this seems out of proportion to
show what we're talking about here
atomic you know putting architectures
down in FPGA raised I think what's made
him so excited about it is the fact is
that we've abstracted away to a large
extent that this big chunk of hardware
people knew FPGAs were interesting and
could do things but the problem was they
didn't know how to get to him they
didn't want to have to work with this
big chunk of hardware that they didn't
know how to deal with right so I think
are abstracting that away is what really
made this very interesting to him plus
the hardware's out that cheaper we can
just give it to him right to the
basically the they're talking about
doing thousand cpu processor emulations
on this thing right xilinx gave us a
half million dollars mostly that's chips
actually up this bill that they actually
helped us build the boards as well so it
was kind of nice and in a staff we got
the NSF money to try to support this
thing so this is kind of getting you
know getting away from where you think
FPGAs do stuff so spice is a circuit
simulator you know it solves matrices
and stuff like that now the message you
do here you can do exactly what you did
on the von Neumann machines you use
those those algorithms but of course
they have problems they're not there
they basically are architected for a
sequential opera processor what you do
is you have a look at it again figure
out how you might do it paralyze a much
better and that's the core with this I
mean we gotta change the way we teach
computer science algorithms in some
sense you can't be teaching you know
metrics that are associated with a
sequential machine you got to begin to
think about how you do it all in
parallel and what the right metrics for
that are so that's trying to spice
feeling this stuff Radio Astronomy
applications ticklish so we started that
one pretty early they have been one of
our you know really strongest really
worked with as carefully all the way
through this because it just saw the
problem they need supercomputer like
performance the I think the next it's
FFTs pretty simple straightforward
streaming kind of computation but lots
of it this is the Allen telescope array
and what he's funded okay right is
hundreds of telescopes sitting out in
the desert you know in the Central
Valley there right and they do beam
forming these things so they can sort of
move around where the beam is and look
at certain places with very high you
know efficiency very you know they
capture all the information from that
certain direction so you get huge you
know improvements in the sensitivity of
the receiver huge computation to pull
this thing off this is really super
computer level performance needed to
pull this whole thing off right and they
looked at yo we're talking like the
hitachi supercomputer area some of these
really big ones so what they started
seeing what they could do with our
arrays right and started putting the
stuff together and sure enough we
have stuff now running out in the desert
not that big array hasn't been fully
built yet but on smaller ones they're
actually using our boards out there in
the field right now to do some of this
beam forming and make it happen this is
guy machine picture this shows the
algorithm mapped on to the FPGA array
you see the two square holes there those
are the power pcs ok so this gym has to
power pc so you can sort of see where
the power if you see size is relative
what the fpga has there the you have to
think about is when you talk about doing
these kinds of algorithms its spatial
processing and suppose you know we
generally think about doing you know
programming your sequentially sort of
organizing the computation you hear it's
a spatial processing you're doing the
other thing is and what our OS supports
is what happens if you stop doing
something you want to do something else
you can reprogram a portion of this chip
and have it do something else so that's
basically it's sort of like you're the
process call that you might have in a
general no standard Illinois Minh
architecture so it's there's a mapping
here of you know time to space that you
really have that's what really allows us
to do the parallelism because once we go
into the space domain then we can sort
of start mapping this stuff across all
these very various a raise yeah
yeah well actually what we do is you can
do anything you want with it his has a
nice tool interface into the hardware so
what we do is we program we go into the
powerpc and we that's what we do is to
all the bit files and stuff then come
from it they go out and program up the
chip so that's how he abstracted away
some of the hardware by using that power
pc so I mean it's low level user
interface stuff just something fine for
a ba noi machine right since that's
where belongs right ok so here's the
last part if you really want energy
efficiency right if you're if this is
even if we got up now a factor of a
hundred over what you've got from the
you know the micro processors and
there's another factor of ten or so on
the table here that you were thrown away
and that's dedicated ok so you can take
basically what our little design flow
does is we take that arc at this a
description of the up at this simulink
level I told you this math works
graphical entry thing and we map that to
an FPGA automatically I mean that you
it's direct from there all the way down
into the chip you don't see any VHDL or
any this kind of stuff that same
description as I told you is the same
one we use in a six so we actually could
do an automatic map of that same
description into an AC so let's say you
figure out your algorithm ok if you want
to get other factor of ten efficiency
boy you just make a chip that does it ok
and boy you got another you then you are
right to the limit of what CMOS can
provide in any given time ok right and
you'll get faster clock rates and you'll
get in better energy efficiency so
dedicated chips ten hundred times more
efficient FPGA hundreds of thousand
times more than a processors do you
think the numbers were talking about are
really huge right and how we do it i
just described out we just have our
little mapping strategy so basically try
to work at that top level we have all
sorts estimation tools but it's not so
important i guess the point is that you
know what people really do this is this
kind of a crazy idea I actually I know
people doing some of the biological
search problems they're doing chips to
support that stuff because boy that's a
big important problem if it's a big
important problem it's worth it to do
the chips to do it right and so I think
you know did something to consider i
mean you know depends on how desperate
you are to reduce the power problem ok
so to me reconfigurable computing is
here right i mean there's something real
interesting going on in this whole area
the technology because just made it
viable in ways we never would have
thought about you know not too many
years ago and it kind of and what I
think even more importantly is that's
going to get better dry twig Moore's law
does I mean that that's what that's what
happened to the processors that was not
an architecture that could continue to
exploit Moore's law right multi-core is
that attempt to sort of do that but I
think just the idea is sort of right
you'll get parallelism but it just
hasn't gone far enough and this is the
way to get the rest of it we had a new
new kind of software engineering that's
for sure I should describe that spatial
processing it's going to get away from
thinking these FPGAs isio big chunk of
hardware that's really different story
that we've had before but you know it's
not we know what we want to get to it's
the environment we've always lived with
okay question is how do we get there
with these raises or underlying
computational fabric i believe all over
time all high-performance computers
going to return to these arrays i mean i
just i just cannot see why you'd want to
throw ten to a hundred times of
efficiency away just for problems of
software which is really probably the
main reason it happens right now okay
that's it sorry
thoughts I mean well how do you handle
faults okay so I guess the question is
you know are we when you talk about a
screaming computing model okay it's a
little different than thinking about a
standard you know software software
model right okay so what happens is as
you go through this let's say you have
some sort of overflow and some of your
computation what you'll do is you'll
deal with it locally okay it's it's not
a that's not a global reset of
everything that's the different thing
because once you have such high
parallelism going on here you've got to
deal with these things locally as you go
through all this big parallel paths so I
think it's a very different way of
thinking about how you'll deal with that
all right the hardest problem that we
found out there was basically
partitioning a chip which is still not
solved like if you're 100 million
elements you can partition automatically
you have to do something manatee so
that's still a bottleneck yeah so the
point about okay now so how do you
really exploit this level of parallelism
and how do you map it on to these arrays
exactly right so actually that language
I just showed you there one thing you
want to do is a hardware idea which is
not a good one to continue in this new
world okay is that thing's data moves
around and sort of lockstep okay you
don't want to do that what you want to
do is you want to sort of 8b couple the
data movement from the clocks of the
FPGA so you do that through basically
asynchronous fifo like sort of
structures and thing like so once you do
that then moving from one ship to
another becomes transparent because yeah
there's some delay go in there but if
you're not really trying to keep
everything in lockstep then you don't
have to worry about it so I think it's
along those lines that you have to solve
this problem you have a large design it
doesn't it won't fit on there like 10
ships it takes my hundred ships you have
a such a big design and you want to do
it automatically you don't want to do it
manually like persons had to come and
say half my program goes in chip one
second half goes and chip too so that's
not a third problem yet yeah I agree so
I've been promised hottie a map really
big problems across to raise it be
efficient about it and not lose I agree
not solved I think there's lots of
things that aren't solved here right but
I think that's probably don't try to be
so efficient I think that's the other
piece of story here then that with what
helps a lot of these problems yeah i'm
just wondering about how how the
adoption of this will
because right now if you look at say
undergraduate education students are
taught first write a program that works
right come up with the series of steps
to solve the problem then your profile
it you figure out where the performer
fall necks are in an advanced topic is
thinking about the motion of data
between the various bits of computation
but you proposed a programming language
here where the first thing you got to do
is you gotta lay out of your data motion
and then a secondary thing is you fill
in the blocks with what computation is
actually done do you think it will be
difficult to teach people to think of
the commando problems you really hit the
really important issue here right it
really is a switch right i mean we are
talking about all as i say you know
basically algorithm complexity it's a
very different model for what we're
talking about here you almost can worry
not worried about the computational part
up you got to worry about the data
movement okay so and when a memory
access and that's the issue I think well
it's got to start from places like this
right who actually can look at this a
while we have a problem to solve okay
we're going to solve it will build up
we're try to do is build up tools that
sort of bring new people on board with
trying something different then I think
what happens you know academia sooner or
later will say oh geez all our students
are doing this and why are we teaching
this okay maybe we should change we
won't do it quickly because I have those
notes and I don't want to redo those
notes again right now there's a lot of
pressure not to change but I mean you
can do it it's possible all right
things sort of the slopes start
equalized the battle
so this has been under the case
so why hasn't this happens we're ready
to get all
yeah I don't think actually i know the
slopes you saw there i think that
actually is a little bit of artifact
just the pieces of data we took right
there you actually get every time you
scale up to new technology actually what
happens right now that the foundries
TSMC the guys doing the state-of-the-art
process what do they use to drive their
next-generation technology it's fpgas
and why is that because they can so
easily design and do a design that
exploits the next generation you scale
down by lambda right you know 1.4 or
whatever it is you get 2 times more
those little competition it's with no
thinking okay then you throw in a little
more yet you actually get little bit
extra clock rate so you actually get you
know sort of two to three times more in
each generation the van Noy the Intel
cores they've stopped been able to
increase the clock rate they cannot
increase that its power limited and that
has really put a lid on them right
but I say now what's the right unit to
put down is the right unit to put down a
ba noi minh core and replicate that
which really has not been designed for
the application or think about or do you
put down allow yourself to put down
exactly what the processing unit needs
to be for the application you're
interested ah there's the problem areas
oh so this is the question you stood
right there that I think the biggest
problem right now it's it's training
it's it's you know it's the history what
we talked to this I think people are
going to solve problems they couldn't do
any other way using this approach that
people can look over there say wow and
they did that it was cost no hundred
times cheaper and 100 times less power
and hmm maybe I should do that too I
mean it could be another company
starting up okay not named Google may be
named frugal because yo they want to
save money and basically they will use a
very different architecture and a very
different approach and won't have the
legacy which really keeps people from
using this I think that's the big part
of it so it's old disruptive technology
argument right so yeah
I see the specter of thousand in terms
of efficiency for ten countries nation
great city again and they sent me to say
yes I mean that question was I showed I
was York with efficiency numbers you
know computation / milliwatts so the
question is you know what's the raw
computation rate increasing and if you
saw I think one of those slides I put
down there basically I mean these chips
are consuming wats I mean effective Lee
we're all power limited ok the power pcs
or power live i mean i mean the the
intel processor limited maybe at 50
watts ok these X's these FPGAs they're
probably limited at 10 watts or
something like that so you're not
talking about much power differences
there right so that's why i say
efficiencies the whole name of the game
because you're really power limited now
right you know the questions
yeah so he's so let's go after if we're
really after power maybe we should be a
little bit smarter about how we do the
fpga design the chip design itself that
is I think really the another big huge
factor like I said xilinx in Alterra
when they designed these things they
just didn't think about this problem
they didn't really worry about power for
the long center right now they're just
beginning to deal with the powers they
it's like processors you know 10 15
years ago people did not care about
power 10 year 15 years ago now they all
they care about the FB Jays are in the
same word he said now they're just
beginning to hit this issue and I think
there is a lot of stuff you can do all
right so yeah okay begins part of the
programming model here okay so if you
really think about screen programming
and basically a locality of computation
and memory access right yeah the
original fpg ideas you put down this big
took a random logic and you can program
it up any way you want you notice how we
that example I showed you of the that
SETI you know at that spectrum analyzer
for the videos Radio Astronomy guys the
computation the colors there were
showing where the computation was you
don't spray that stuff all over chip you
locally basically control it in fact the
way we do the mapping to make this thing
so it's fast and solved I'm sure you
know some of the problems of mapping
into big fpgas you don't just go in
there and just flatten this whole thing
and just start placing routed what you
do is you go in with blocks that you
basically precompiled place those down
which have all the local interconnect
sort of taken care of in the best
possible way then you put those blocks
together yeah you lose stuff between all
these things that's okay because you
don't have to be so efficient the old
model was xilinx designed their chips
right now so you can route those things
100
% that is just crazy you do not need to
do that right if you're trying to do
this kind of application and that is
some of the cost of those chips and some
of the power wasted because they
designed these things to be under
percent even with that happening these
things are still winning big-time
against any other kind of architecture
right so
yes well it is functional programming I
think it's along those lines is exactly
the sort of the programming model that
you really need to think of here so
clickety programming functional
programming I think that's it's those
kinds of programming models that make
sense I think you don't have to get too
weird though I mean you can start off
just you can see blocked if you can
organize your architecture into stream
like blocks and put blocks and handle
the communications upfront make blocks
fairly big that may deus may use some
string compare some things like that
right and may just do that really well
so it's kind of a functional programming
like idea right and then you could
program these things and I think fairly
efficiently right and deal with sort of
non automatically but the mapping issues
right now to put these things on huge
arrays you must be doing some very close
mapping right now onto your machines i
would guess right i mean you're not
letting that just some compiler figure
all that out you must be taking control
of yourself i think the same things
gonna happen here for the time being
until we really have much better tools
right
right
yes the scientists involved double
precision
numbers they also like to do
that 1.5 teraflops was double-precision
floating-point modules not optimized to
the FPGA we actually just took some code
some art you know some our tail code and
just programmed it on there I think if
you go in there and if you want it even
improved a lot better right you could do
sort of more efficient versions of these
floating point units and even do better
than that number so the answer is it's
very good that that's the number this
thing that's floating for that that's
the thing by she's doing floating-point
on ftj we started putting those numbers
of it without could you really do it you
know and you go back five or six years
ago you put one per chip now you put 50
or something I got free chip isn't like
that right from any other veterans okay
oh thanks a lot</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>