<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical Aspects of Data Mining (Stats 202) Day 13 | Coder Coacher - Coaching Coders</title><meta content="Statistical Aspects of Data Mining (Stats 202) Day 13 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical Aspects of Data Mining (Stats 202) Day 13</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-tWS0tN8sW0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome to lecture 13 this is
the final lecture for data mining at
Google there's a homework posted for the
last one that I told you about today I'm
going to finish up chapter 5 we started
into chapter 5 before we're gonna finish
it up today and then I'm gonna do a
little bit on chapter 8 which deals with
clustering just enough to give you a
introduction to that okay so there is
the homework assignment there's some
information about the final the students
are taking next week chapter 5 is on
alternative techniques for
classification we got into it last time
ensemble methods were the one thing that
we didn't finish up and ensemble is a
very general category that talks about
combining different classifiers and your
book says the aim is to improve
classification accuracy by aggregating
the predictions from multiple
classifiers one of the most obvious ways
to do this is something to take a
majority vote so take the straight
average of classifiers right so each
classifier makes a prediction you take
some fixed number of classifiers then
you take the majority vote and the whole
idea there is that if they're acting
somewhat independently then you can
improve your performance and the
examples I gave you or some point
examples suppose you have five
classifiers that are each correct 70% of
a time for a given data point if these
things were completely independent and
you took the majority vote well then the
majority vote is going to be correct if
all five of them are correct if for the
five of them are correct or a three to
five of them are correct then the
majority vote will be correct and you
can calculate this thing which we did
last time and I believe it comes out to
be about 84 percent so you're improving
your accuracy from 70% for the
individual classifier to 80% for the
ensemble
let me just bump up the font size here
to let's say 14 okay so the ensemble
would be 83 or 84 percent accurate even
though each individual classifier is
only 70% accurate now this is sort of
the ideal right where you have complete
independence but as long as you get them
somewhat uncorrelated you can hope to
achieve this type of benefit the next
example I gave you was suppose I had 101
classifiers and each of these was
completely independent and they were
each getting the point correct 70% of
the time
how would the majority vote do well for
101 you just have to have 51 or more of
them correct and the majority vote is
going to be correct so the probability
there again you can figure out using the
binomial distribution and this thing
comes out to I think like four four
nines let's see how this one is comes
out to be point nine nine nine nine nine
nine nine eight seven so if things were
completely independent if you were in a
perfect world you could just take a
majority vote and you would do perfectly
now of course things aren't going to be
completely independent but again to the
extent of the different classifiers
you're averaging are somewhat
uncorrelated you can improve important
performance to some degree okay so what
are the different methods of attempting
to do this in practice well as ensemble
methods your books and book includes
bagging random forests and boosting now
bagging and random forests generally
follow this what I'm talking about in
terms of taking averages and hoping that
things are pretty independent boosting
is a little more complex it's gonna
benefit from this averaging but it's
also doing some some sort of more
structural optimization that we'll talk
about but bagging and random forests are
really along this spirit of straight
averaging so what does bagging do well
it builds different classifiers by
training on repeated samples with
replacement from the data so I know it's
a little little dark I'm just gonna
scribble something on the whiteboard if
I can find a marker so with bagging you
imagine suppose I have a hundred points
right so my training data is 100 points
okay and so I say I want I can I can
build a classifier on a hundred points
but I don't want to build just one
classifier I want to build many
different classifiers so I can average
them together and take the majority vote
so what we do with bagging is we're
going to make different classifiers by
training on repeated samples from the
same data so let me take a 100 points
and sample 100 points with replacement
okay so that would be the exact same
point except I'm doing it with
replacement so what's gonna happen is
I'm gonna get some point 0 times some
points 2 times maybe a few points three
times and sort of like that and then I
can do it again right so the whole point
is that I don't just want one classifier
I want a large number of classifiers so
I can average them together so with
bagging you just keep taking samples of
which replacement it could be from this
of the same sample size you could use a
smaller sample size if you
but from your original data that gives
you a number of different classifiers
there of course not completely
independent they're trained on a lot of
the same data but they might be somewhat
uncorrelated and if you have a really
really noisy really unstable classifier
by doing this and then taking averages
you can sort of smooth it out and
achieve some benefit and improve your
performance so this is the general idea
behind bagging it's a very general
approach right you can use this not just
for classification but you can do any
type of technique you can do bagging on
your data average the results and then
hope to improve performance so I'm not
going to give you too much details about
bagging but you can read about and
basically that's the idea is that I just
take samples with replacement from my
original data build by the classifier on
each one of the sub samples and then
average them together and it has a nice
effect of smoothing things out if you
talk about the bias versus variance
trade-off you decrease the variance but
you haven't sacrificed any of the bias
okay random forests I'm going to go into
it a little bit more detail on those
that's a little bit different it's sort
of along the same spirit of bagging but
instead of sampling the observations
right the rows they often sample the
columns or the attributes so I'll give
you some details about bagging on the
next slide and then the last thing we'll
talk about with regard to ensembles are
boosting which again it benefits from
some averaging but it also does
something a little more intelligent
where it looks at the points that are
getting classified incorrectly and it up
weights them so I'll give you some
details about that but first let me talk
to you about random forests so you can
see here as the name implies it's going
to be specific to trees random forests
forests
forests as a collection of trees whereas
bagging in principle you could do with
any type of classification technique and
in fact with a lot of other techniques
you can use bagging boosting can work
with any base classifier although
commonly these are used with trees but
random forests as the name implied is a
specific algorithm for use with trees
and how random forests works as a
following so one way to do it basically
with random course you want to average a
whole bunch of trees so how are you
going to get a whole bunch of trees that
are somewhat independent well what
they're going to do is the following
right
you start off like you're going a tree
like we did back in chapter 4 and you
have your root node okay and then you
need to find sort of the best
split and chapter four we found the best
split well what random force is going to
do it's not going to find the best split
it's gonna find a split that's good but
somewhat random so if you imagine
instead of sampling the Rose we're gonna
sample the attributes as I set so if
like the sonar data with sixty
attributes instead of finding the best
split from all 60 maybe you just take
ten of the attributes okay and of those
from those ten attributes you find the
best split okay so then that gives you
sort of two other nodes here and then
you have to split those nodes
well again you sample the sixty
attributes just randomly sampled ten of
the attributes and from those ten you
find the best split and over here maybe
you randomly sample another ten
attributes and from those ten you find
the best split so it's not the optimal
tree but the nice thing it has this sort
of random component so then you can come
over here and do it again start with the
same terminal node sample ten different
attributes from the sixty form the best
split on those ten then do it again over
here and so all these trees are going to
be somewhat different because the splits
aren't optimal they're somewhat random I
take a random subset of the columns find
the best split on that random subset of
the columns and grow the tree and
actually they grow these trees and
random forests pretty big right they
don't really attempt to sort of prune
them back too much they just sort of let
them grow pretty big but then you get a
whole bunch of different trees right all
these trees each one gives me a guess
right this one says oh I think it's a
negative one this one says I think it's
a positive one this one says I think
it's a positive one then you take a
majority vote among those trees and that
gives you your prediction okay so he
said trees aren't that useful in terms
of you know decision trees aren't that
powerful of a classifier but using
decision trees in some method like this
like random forests can be very useful
so again these are very effective
they're based on the paper of course
these are the old Ryman's invention if
any of you had the privilege of meeting
him before he passed away he was up at
Berkley really smart really nice guy and
this machine learning paper from 2001 is
a very readable paper where he describes
the algorithm and talks about why it
works
why does it work well because you don't
you still the trees are still unbiased
but you you decrease the variance by
doing this randomization there's a
function in are called random forests
one word the R is not capitalized the F
is in the library of the same name that
will fit these you can read the
documentation on
you can read the machine-learning paper
from 2001 also adele cutler keeps up the
website and sort of gives all the
updates to the algorithm and supports it
on that website is there anything else I
need to say on this slide I think I said
everything on this slide let me show you
in practice so if you look at the sonar
data set here I can fit a random force
this on our data set
basically just install random forests do
library random forests reading the
training data reading the test data now
again I want to have y as a factor so
that our knows that the sixty-first
column of the sonar which was the
negative 1 or 1 is the categorical
predictor categorical response that
we're trying to predict and then the
testing data is the other the first 60
columns and then so the random forest by
default I just say fit random forests X
comma Y right give me the training data
axes give me the train data wise that's
going to fit a random forest and then I
can use my predict function on that
object predict my fit predict it for my
testing data and see how often this
prediction from the testing data equals
the truth the true Y from the testing
data which of course this algorithm did
not see take 1 minus then divide it by
the sum and that will give me my class
of misclassification error rate on the
test data and so let me just paste this
thing into R and so if you remember how
well we were doing all right down sort
of the results from the bake-off before
but this thing gives me 15.3% miss
classification error now of course it is
random right it's growing different
trees each time so if you do it again
you might get a different number here I
got 16.7 you do it again you get 14.1
you do it again you get 16.7 so there is
a random component so the
non-deterministic but of course I don't
have too much confidence that one's
actually really low I don't have too
much confidence in these numbers anyway
because they're just based on the single
partitioning of the data into training a
test if you wanted to get a better more
confidence the numbers you might do it
repeatedly over that too but this thing
looks like it's coming in between you
know 10 and 16% so if you remember the
the bake-off we were having right on the
sonar data set which this is also being
applied to the sonar data sight the
first thing we looked at in Chapter four
were the decision trees from our part
and I said that those aren't very oops
those aren't very accurate and I think
we were only getting about 30% maybe a
little bit better for our part so that
was about 30% on the sonar data we did
nearest neighbor one we got about 20 21
percent I think on the homework I asked
the students to sort of figure out you
know if the nearest neighbor one gives
twenty one percent how do two and three
and four do and they do a little bit
better but you're not you don't do too
much better you're still around like
nineteen percent then last time we
looked at support vector machines and
those did really well those were like
twelve point eight percent so we call it
like thirteen percent and this guy is
arguably competitive to that right
here's a fifteen percent here's an 11
percent so we'll put random forests
random forests I'll put them at about 13
percent - they're very competitive to
support vector machines I mean sort of I
would say our part nearest neighbors you
know not so good just not only on this
data set but in general but support
vector machines are very popular
technique they're popular because
they're effective random forest is very
popular and effective technique it's
about 13% and the last one of course
I'll show you today is boosting which is
also going to be competitive on this
data set and any other tests as well but
again random for it's a pretty simple
technique but there's some strong
motivation I mean the over ayman will
understood well what the failure of
decision trees was the failure of
decision trees is that they tend to be
very sensitive and he said you can
improve the performance by averaging
them he was working on these at the same
time boosting was a hot topic and he
realized that they were sort of both
benefiting from this averaging and he
developed in the end of the day a
classifier that's very competitive and a
lot of people use it it's pretty simple
but he gives a lot of intuition in the
paper for why it works basically he
shows you that you can make these trees
very uncorrelated right you can't make
them perfectly independent but you can
make them very uncorrelated and then
taking the averages will work well and
he actually gives some theoretical
bounds and the performance in terms of
the correlation and the accuracy of each
tree so again that paper is pretty
readable okay so I think that's all I
wanted to say about random for us any
comments questions thoughts about random
forest right so the question is is
random forests appropriate for high
dimensional data set and exactly that's
and and the converse of that is if you
have a low dimensional data set you're
not going to benefit too much by taking
random random columns right because
there's only so many columns you can
take there are are other versions of
random forests you can have different
components that become random but one of
the popular implementations is to select
random columns and if you have a lot of
columns in high dimensional data many of
which are redundant then that's sort of
that's sort of a good recipe for using
random force high dimensional data a lot
of redundant information and so you sort
of randomly select the columns and that
gives you different trees uses all the
information but doesn't over fit and
doesn't suffer so much from the curse of
dimensionality okay other questions
comments on random forests I so the
question is how many trees I forget what
the default is I I think it's generally
in the hundreds let me just see if I can
see here random forests see if there's a
default value oops what did I type it
ran a forest there's only one forest
many trees okay so let's see if there's
I don't know the default value of how
many tree is you spinning is that it
where do you see it there's line first
line and number of tree is probably 500
let's see where's entry number of trees
to grow perfect this should not be set
to too small a number to ensure that
every input row gets predicted in these
two times there you go so default is 500
and you know you can spare a moment
changing these values also one thing I
didn't say about random forests is
they're not only are they good at
classification they're also good at
probability estimates so the number of
trees that predict it as plus 1 is a
good surrogate for the probability the
conditional probability at that point is
plus 1 so if you if you have a bake-off
not just in terms of classification
error but in terms of probability
estimation random forests often beat
things like support vector machines and
boosting where it's not so obvious how
you get a probability estimate out okay
so 500 trees looks to be the current
default the the package is updated
frequently too so you should you know if
you have installed it last year you
might want to reinstall this year and
get the new version okay any other
questions comments on random forests
freak observation just exactly so so
child's question is you know here I have
a split training data and test out I've
been having my bake-off but random for
it's just by itself just on the training
data actually will give you a nice
estimate of its performance on future
data because every time it's not using
the data and it has sort of this built
in so one of the things you get I said
you get probability estimates for free
you also get a nice estimate of how well
you're going to do on future data okay
and how well you do on the training data
has very little to do with that but this
is sort of intelligent enough to give
you a good estimate of the
generalization error to future data okay
any other things on random force okay so
the last ensemble technique to talk
about is boosting which doesn't fit so
well I mean it's it's going to benefit
from this averaging but it's doing
something else to the Oh Brian actually
called boosting the best off-the-shelf
classifier in the world there are a
number of explanations for boosting but
it's really not completely understood
why it works so well people some people
in machine learning have an
interpretation of it they say oh it's a
large margin classifier some people in
statistics say oh it's doing stage wise
optimization of something that looks
like a likelihood function people
disagree about why it works so well but
people generally agree that it does work
well and it's a very popular and
effective technique the most common
version of it when some people talk
about boosting their implicitly meaning
the adaboost version which is from this
1996 paper by foreign ship here and you
know these two they're still very active
in the research to sort of understanding
why this works I mean basically they
have an algorithm they have some
motivation for why it works but people
don't generally agree on why it works
but again people do agree that it does
work well one of the debates around it
is whether or not it over fits and often
it over fits because that's sort of the
surprising thing about that it doesn't
over fit and that's what I say here in
the second point it usually gives zero
training error but rarely over fits
which is very curious because if you
look at the algorithm on paper it looks
sort of like a recipe for a disaster
because it's so aggressive and trying to
classify everything correctly it can use
any classifier as its base classifier
which is called the weak learner so
that's the idea behind boosting is that
you take sort of a weak classifier like
you can think of a very small decision
tree it's not going to be able to learn
all the data because it's just a very
simple tree but it takes that and it
boosts it up makes it stronger how does
it do that well it combines many trees
but not just in a averaging sort of way
in a way that's aggressive at trying to
learn the whole data because it takes a
small tree
wherever that small tree makes mistakes
it gives those weights those points
higher weight puts another tree on top
of that combines those two trees in a
very aggressive fashion so that it can
get every point correctly whatever
points it's still getting wrong it gives
those points even higher weight takes a
third tree puts that on top tries to
learn those points so on paper it looks
like it's doing an optimization in fact
the way it's doing this up waiting it's
it's optimizing something that looks
like maximum likelihood estimation and
statistics in fact the function that
it's optimizing is the exponential loss
which looks a lot like the binomial log
likelihood so even though it looks like
an optimization on paper in practice it
also benefits a lot from the averaging
like random forest does and this is what
some of the debate is about again I said
it works by up weighting points at each
iteration which are misclassified there
do exist our libraries for boosting so
you think okay I could just read in the
our code and say okay here's boosting
but the warning I'll give you is that
these are generally written by
statisticians right R tends to be
something that's popular in the
statistics community so the our
libraries for boosting are generally
written by statisticians
staches excuse me statisticians have
their own sort of personal biases about
boosting so I generally wouldn't
encourage you to use the our libraries
for boosting although things like GBM
are pretty popular one thing to do is
simply to write your code write the code
yourself because the algorithms are very
simple so adaboost is like four lines of
code you can try the built-in libraries
too but at least try something like
adaboost which is very simple and right
at yourself because it generally will do
a lot better than some of the libraries
that are existing in are already so
here's the code this is the adaboost
code so how does this work so if you
start off with a classifier I'll just
call it lowercase G and so this
classifier itself
will give you an estimate of the class
negative 1 or 1 so you can think if it
helps you of G as being a very small
tree right so maybe like 4 node tree so
I just grow a little 4 node tree to the
data
and this is gonna spit out either
negative one or one it either thinks
every point is either negative one or
one okay so it's gonna get some wrong
it's gonna get some right it's just a
very small tree okay the next thing to
do is to compute the error rate for that
tree in fact it's the weighted error
rate but I'm gonna initialize my weights
not as one on it the weights wall is
some and I initialize them at one on end
so I compute the weighted error weight
for this tree with the fraction of
points that it's getting wrong on the
training data okay and the log odds
which is one half log of one minus T
over E and this thing is sort of how bad
it's doing if if it's doing worse than
50% you can actually see this thing will
become negative and flip the sign and so
then what I'm going to do is I'm going
to take the tree times this alpha
multiplied right so this alpha is just a
scalar but it could be negative right so
I'm going to take the tree times to
often help call alpha multiplier and so
that's gonna spit out negative alpha or
alpha so on the first stage I'm either
gonna have negative out for alpha for
every point I initialize n 0 so at stage
1 F is either negative alpha or alpha
okay my final classifier is going to be
based on the sign of F so at stage 1 my
final classifier is equal to the
original class star right because
multiplying by alpha doesn't change the
sign or if it does then it's just going
to be the opposite of the original
classifier so if the original classifier
does better than 50% at stage one I'll
be equal to the original classifier did
worse than 50% it will be the opposite
of the original class ok so stage one is
that interesting Stage two is
interesting right because I'm gonna go
back and I'm gonna refit my base
procedure I'm gonna fit another four
node tree for example to the training
data but again I'm gonna use the weights
and so the whole key is how I update the
weights and the way you update the
weights is you take the old weights and
you multiply by e to the negative alpha
G times y ok so that's the whole key the
weight update the new weights equal the
old weights times e to the negative
alpha G times y so I said I'm gonna up
weight the points that I get wrong and
that's exactly what this is going to do
right if G and y disagree in sign that
means it's wrong that means this would
be negative that means this would be
positive e to a positive number is bit
okay if G&amp;amp;Y agree ensign this would be
positive times negative alpha would be
negative it would get a small weight so
the points that it got wrong get high
weight the points that doesn't get that
it gets correct get low weight and you
can also think of this this G as being
sort of a sari you can also think of the
the alpha times G is giving you sort of
them how wrong it is right so you know
just get the negative one or one you
actually get a confidence weighted
prediction and so you the points that
you think you're getting wrong and
you're really confident actually get
really high weight so it's not just a
binary weighting in terms of right or
wrong it's in terms of how wrong you are
based on your confidence okay so this is
the update for the weights and then
again you go through grow the tree with
those new weights so again I'm giving
trees as an example but it works with
any classifier that accepts weights and
then you go through compute the weighted
error rate for that new tree again half
its log odds and then you update it
again so originally you had alpha plus G
for the first classifier now you add
alpha plus G for the next class bar so
your final classifier right is just the
sum of alpha genes right at each stage
you have a tree G and some multiply our
alpha okay and then you're just gonna
threshold on the sine of that thing so
again it's just a linear combination of
trees that's your final classifier
learning combination of trees where the
alphas are chosen according to this
formula which is based on the error rate
for each tree and the G's are grown on
the training data but using weights
where the weights are smart enough to
update the points that it's been getting
wrong and then of course you go back
through and update the weights again and
so now if you're still getting the point
wrong it's just a going image really
high weight right and eventually you're
gonna get right in fact you can prove
sort of under some mild conditions that
even with a very very simple weak
learner this thing will eventually be
able to get all the data with all the
training down to correct it will be
eventually able to classify all the
training data correctly under some mild
conditions as long as you run it for a
sufficiently long time okay so I said
this algorithm repeats until chosen
stopping time when should you stop well
that's sort of up in the air but
generally people run this for like a
thousand iterations something like that
and again the final classifier is based
on the sign of
f is equal to this linear combination of
trees
so you just threshold at zero everything
that's positive you say is possible on
everything that's negative you say is
negative one so again it's just four
lines of code a very simple algorithm to
run here I'm going to implement it for
you and you can see how to do this so
again I'm gonna look at it on the sonar
data and train it on the train data see
how it Wells it does see how well
doesn't the test data this slide is sort
of uninteresting I'm just reading in the
data here I'm going to initialize some
things so my training error is gonna be
my misclassification around the training
data let me just I'm gonna run it for
500 iterations so initialize that zero
test error same thing the F and the F
test these are my my capital F I guess I
wasn't consistent on the capital
lowercase but this is just my app from
the algorithm and I'm gonna initialize
it to zero it's gonna be a hundred and
thirty because there's 130 points the
train data there's gonna be 78 because
there's 78 points the test data I is one
I'm gonna iterate over I I need the
library are part because I'm gonna use
default our part as the base learner
okay so here's the loop right loops are
slow in are but this is easy to write in
a loop so while I is less than equal to
500 this weight formula should be a
little bit mysterious to you maybe I
should explain why that's the weight
formula I gave you the weight update
formula but this is sort of the weight
formula the weight is e to the negative
YF and that actually does the same thing
he to negative YF it does the same thing
why well it happens to be just look at
how f is updated and look at how the
weights are updated right atha is
updated by adding alpha plus G I'm
telling you that the weight should be
updated by multiplying by e to the
negative alpha G okay so this you know
alpha times G times y is the same thing
there when I multiply by e to negative
alpha times T times y it's the same
thing as F which adds alpha times G so
so I can just you know do that right I'm
going to multiply you need a case I'm
multiplying by Y but here I just have e
to the negative alpha times G and here I
add it here on this scale and here I'm
all
expansion scale so this this weight
formula is equivalent to if I had done
the weight updates so either way you can
do it there the weight should be
normalized to some to one not you know
our part will still tolerate it but it's
nice to keep these on a scale so that
they're not all machine zero fit our
part basically the default call to our
part except I give it the weight
argument which on the first pass is just
one nothing interesting but eventually
I'm going to start up weighting the
points it's getting wrong and of course
method equal class the G here is just
the negative one or one depending on the
prediction from our part so actually in
this case I actually spit out the
probability is threshold 0.5 so the
probably is bigger than 0.5 it's
negative one plus two is one if it's
less than 0.5 its negative one so G is
just negative one or one same thing for
G test I'm just tracking my predictions
on the test data so I want to see how
well I'm doing on the test data II again
it's the weighted error rate so wherever
Y and G different sign is an error times
w sum that up that's the weighted error
rate then alpha 0.5 log of 1 minus e / e
just like in the algorithm then I'm
gonna update the F right how do you
update the F you take F plus alpha times
G just like here right F equal the old a
plus alpha times G so just add on the
tree giving it coefficient alpha and
then I do the same thing on test right I
take my F test at alpha times G just cuz
I'm trying to track my predictions on
the test data so then the other the real
thing I want to know is the training
error and the test error so the training
error is the fraction of points where F
and y different sign on the training
data and on the test data where F and Y
different sign on the test data and then
I'm just gonna increment I by one there
okay so if you do this thing actually
maybe I'll do it in real time for you in
case that adds more drama to it or
something so where is this thing here so
let's see this is just reading in the
data
so really I mean there's only a few
lines of code there that are doing
anything most of this is just sort of
reading in the data initializing things
here and I'll use the default are part
you can use any learner that accepts
weights but I'll just use the default
higher part which is something that you
know is not a good classifier right you
know that it had 30% missed
classification error on the test data
and I'll start my loop here and then
finish it here let's see
and so loops in are slow right this is
doing a pretty simple calculation only
500 times but whenever you're writing
our syntax you know avoid writing a loop
because it's slow as this one is and
then the last thing I'm going to do is
make a picture and I'm gonna make a
picture I'm just gonna plot the test
error with a line put the wise scale
from 0 to 0.5 because the classifier
can't be worse than 50% row won't be
percent and on the x axis I'm going to
put the iterations right you really you
only care about the final value after
the thing is done but it's interesting
to see what it's doing in between and if
it were overfitting at some point you
know I would start to see this this test
error go up line with two then I'm gonna
do the same thing for the training error
plot that on the same graph in a
different color and put a legend so
let's see if this thing is done running
and I can make its picture yep it's done
running okay so I will make the picture
this way and see what it looks like okay
so that's that work yeah okay so first
thing to notice is that if you look at
the training error right a default card
default call to our part will not give
you a perfect fit to the training data
right I guess it looks like it's there
but after about three or four or five
maybe this is ten iterations I can pull
up the training error and look at it but
after not too long the boosting
algorithm has taken the default call to
our part and use the weights in an
intelligent fashion to actually give you
a perfect fit to the training data after
just a few iterations okay so that's
really how the boosting works it takes
some you know the default are part tree
does not fit all the observations
perfectly but when you do this waiting
in this aggressive fashion you will and
so the training error hits zero pretty
early on
now some people would tell you oh the
training under zero you know you've
probably already over fit but the
interesting thing is you know you can
just keep running this algorithm the
fact that the training are zero doesn't
sort of make things stop because it's
all based on this continuous value of F
which is not just negative one or one
it's this compensator prediction so the
algorithm doesn't depend on you know
having errors so you know the way that
error rate still makes sense even though
everything is is an error it's still it
still makes sense and so you can still
keep the algorithm going so the training
error of zero is out really early on if
you look at the test error rate you
actually see it just keeps going down
right so what do you expect to see you
expect to see it like shoot way off and
overfit but it doesn't and that's what's
curious about about boosting is that it
really looks like it should be
overfitting on paper and that the test
error should just go you know just blow
up and it doesn't and in fact it just
seems to benefit more and more the more
you run it so that's one thing that's
interesting the other thing that's
interesting is if you trace this value
back over here and I'll just pull it up
on the screen it's going to be
competitive to whatever other sort of
algorithms you've thought of so let's
see test error there you go fourteen
point one so oh i erased the other guys
but if you remember you know SVM's for
about you know 13 percent so 14 percent
without doing anything very intelligent
other than calling our part which by
itself is a pretty lousy classifier I
actually took a lousy classifier and
boosted it to make it into a good
classifier so that's sort of the story
with boosting um again there's a lot of
research in this area in terms of
understanding why this thing works
people have different ideas why it works
generally people agree that it works
it's a pretty simple procedure but very
effective and it can learn in noisy
situations it can learn in non noisy
situations it can learn in very complex
situations it's a very useful technique
it just needs a base learner that
accepts weights and then you sort of put
in these four lines of code do the
reading and just let it run and it does
well so I think that was probably all I
was going to say about it here's the
picture up on the slide
okay questions comments thoughts on
boosting okay so you'll read a lot of
interesting thoughts on this it's still
an active research area even though this
original algorithm was in 1996 a lot of
people like to argue about why it's
working how it's working a lot of people
argue that it's not working a lot of
people like to change the algorithm to
make it work the way they think it
should work but you know keep an open
mind when you read these things because
people have their own biases and they
often like to map it into their own
research areas and say ah boosting works
precisely because of all the research
I've done you know during my career and
that's the real secret so you know keep
an open mind on this thing it's
basically an algorithm that works well
has a few different motivations behind
it both of which probably makes sense in
different settings but generally people
don't agree on why it works and it's a
lot of sort of debates around it ok so I
think that's all I want to say about
chapter 5 in general any comments
questions on classification so I mean
you could boost an SVM right
generally an SVM is already a pretty
powerful learner they can get everything
you know so you generally wouldn't want
to but yeah I mean you could do
something like okay let me call the
default SVM and then call it without you
know anything that takes weights you can
boost right but it makes sort of more
sense to take sort of really weak
learners and boost them but in principle
anything that takes weeds you could
boost no it actually will do unweighting
so we tried this because we used to try
and so the goal was so Charles's
question is what if you have
observations that already have weights
and you want to keep those weights in
there can you just initialize the
weights other than one so this actually
doesn't work very well because it sort
of unlearned the weights this came up
when we were trying to get probability
estimates out of boosting so a lot of
people think boosting already gives you
probability estimates by taking the you
know going through the link function but
those don't work that well so what we
were trying to do is we were trying to
get probability estimates out by
weighting one of the classes higher than
the other
but that yeah it tends to not work
because it tends to sort of reweighed it
for you so if you want to wait points in
boosting what you really have to do one
thing you can do is replicate them and
then jitter them a little bit and that's
sort of hacky but it tends to work
anything short of that I'm not aware
that it really works because if you just
give them if I just say okay I'm gonna
wait this half the data twice as much as
this half the data it's going to sort of
relearn the weights for you and going to
sort of mess them up and might give you
something as if you hadn't done any
waiting all right so the question is you
know how well can any classifier do on
this to train it out so there is a limit
right and that's that's how well the
Bayes rule would do if you knew what the
Bayes rule was right so if you imagine
data right so suppose I had two
dimensional data and I say okay I'm
gonna flip a biased coin here and twenty
percent of these are pluses and then
twenty percent of these are minuses so
it'd be real silly to say I'm doing
better than 20 percent misclassification
error on that data right because that's
just the theoretical limit so you know
the feeling is we're probably
approaching it on the sonar data right
that that's there's probably some bound
around like ten percent where it would
be silly to say you could actually do
better than that because there's
probably some level of non-determinism
in that data but you know you don't know
what it is for real data which is why
often people when they write papers will
use simulated data because then they
know what the real story is and it's
sort of easier to study on the sonar
data you don't really know what the real
story is so it's difficult to say what
the limit is but we're probably hitting
I don't think anyone's gonna say I have
a method that gets five percent you know
it's it's probably it's not a
deterministic data set there are some
deterministic assets out there where the
limits 0 right but there is a rule but
in this case it's observational data so
let's probably can't go down to 0% ok
any other questions comments on
classification ok so the last thing to
do is chapter 8 which is called cluster
analysis so I'm just going to spend
I guess 20 minutes on this it's sort of
a personal bias may be that I don't
think clustering is quite as interesting
as classification but you can disagree
with me on that but I'm still not gonna
spend more than 20 minutes on it
so anyway clustering divides that
integral
that are meaningful useful or both it's
similar to classification only now we
don't know the answer we don't have the
labels so for this reason clustering is
often called unsupervised learning right
we don't have someone supervising us to
tell us whether we got the answer right
or wrong
whereas classification is often called
supervised learning right we're trying
to learn a rule but then we get to find
out on the test data where we right or
wrong so your book touched on page 491
so you say well how in the world could
you do you know I need any sort of
learning without knowing the answer well
in some cases it wouldn't make sense
right so in this case you know I have
some pluses here and then some pluses
here and some minuses here you know and
they're all sort of jumbled together and
I have to learn some rule to separate
these guys right so in that case if you
didn't know the answer how in the world
would you learned some rule but imagine
the case like this which often happens I
have some pluses here and then some
minuses here and you say oh well the
rule is obvious right there's the two
groups well you could get that rule you
could see that there's two groups even
if you didn't have the labels right even
if you had a bunch of pluses here and a
bunch of pluses here you can say oh
there's some grouping right these guys
look like they live in some group
together and these guys look quickly
live in some group together and these
groups are meaningful because these guys
are all similar to each other
these guys are all some of each other
and these guys are all pretty different
to each other so like the within group
variation is small between group
variation is big so you can define
groups you won't really know is this the
right grouping but it seems like it
stands out I should say here while I'm
talking about
so the supervised learning versus the
unsupervised learning there's also this
area right now called semi-supervised
learning which is sort of a hot area
right now which attempts to sort of
combine both these methods right so if
you had the data that looked like this
and most of the data wasn't labeled but
then you have just like a couple points
here labeled as negative and a couple
points here labeled as positive well you
can do two things right you could
pretend like you don't have the labels
and do clustering or you could pretend
like you do have the labels and you only
have four points to train on or you
could try and combine the two approaches
which is what's called semi-supervised
learning which is sort of a hot topic
right now to combine these two okay so
because there's no right answer your
book characterizes clustering as an
exercise in descrip
of Statistics rather than prediction so
it's sort of in the same category of
stuff we did in chapter 3 where you know
let's make a picture of the data and see
what it looks like
right it's not like there's a right
answer we're just trying to understand
better what's going on in the data
all right look says cluster analysis
groups down to objects based only on
information found on the data that
describes the objects and their
similarities the goal is the objects
within a group be similar to related to
one another and different from or
unrelated to objects in other groups so
that's generally the idea you want to
keep things similar within the group and
different when you go across groups
examples that you've probably seen right
from your high school science class we
always group living creatures into
kingdom phylum class order family genus
and species and in fact this clustering
has a special hierarchical structure
where you know the species inherits
traits from the genus and so on so you
know there's no right answer right you
know there's not like God or some
Creator is gonna come down and say guys
I've had this wrong all along right
these people are actually in this group
and these know you know there's no right
answer but it's sort of a useful way for
us to sort of understand you know what's
what species have common traits and and
things like that it's there's no right
answer but it's a sort of an obvious
grouping that like I'm more similar to a
monkey than I am to a fish right it's
just sort of obvious right
okay information retrieval someone has
the curry movie you'll see you know the
user intense fall into four general
categories some people looking for
theaters some people looking for the
stars and their way some people want see
trailers some people see the views and
you know all the web pages that have
theaters sort of look the same and they
sort of look different from the web
pages that have the stars on them right
so you sort of see this obvious grouping
climate right there's always regions of
similar client that people say this is
this type of climate okay where's the
boundary where is the real boundary
there's no real boundary but it's just
sort of those regions are all very
similar you can look at patterns of
disease and in business you always
segment segment customers right these
are the 20 to 30 year-olds and they have
different bonding patterns than the 30
to 40 what's special about the number 30
nothing but it's a useful segmentation
okay so when we do clustering again
often cluster for understanding those
examples are from the previous slide
sometimes people cluster for utility
right it can be useful or meaningful or
both well when is it useful well you can
summarize data and some different
algorithms will run
faster when you just used the data as
summarized by clusters so some of the
clustering techniques are what are
called prototype methods and so for each
cluster you can say well I'm just gonna
take this one point to represent every
one in the cluster I'm going to take
this one point represent every one in
the cluster and so then if you had you
know 100 clusters you could just run
your algorithm on the hundred clusters
as opposed to the millions of data in
each one of those clusters also just not
just for running algorithms but for
compressing data right if you could just
keep the cluster centers around and
throughout the rest of the data then you
know you have a lot less data to store
so don't take a random sample the data I
just take you know these prototypes from
the data that other cluster centers and
also if you do clustering you can speed
up the nearest neighbor algorithms okay
so how many clusters is always the
question that you can never answer right
because there's no algorithm is really
going to tell you what the right number
of clusters is once you decide on the
number of clusters you can write down
objective functions and minimize
maximize them but you usually have to
decide ahead of time how many clusters
you're looking for so you say okay well
that's obvious there's two clusters
there right but then you say well but
really you know these guys are sort of
broken off and these guys are sort of
broken off so you say okay there's four
and then someone else says yeah but I
see a division here and I see a division
here and someone else is there six right
so there's no real truth to that one you
generally with most clustering
techniques have to have some notion
ahead of time of how many clusters
you're looking for so that's one thing
to keep in mind the most common popular
technique is probably k-means clustering
in this one each cluster is associated
with a centroid this is often the mean
as in k-means although we'll leave it
general and call it centroid it could be
the median or something like that and
this Center is a cluster prototype right
so this cluster here you know where's
the mean it's right here that's sort of
the prototype value for the cluster and
this cluster over here
whereas it's mean well that's right here
so it's sort of like the prototype value
for that cluster okay
each point is assigned to the cluster
with the closest centroid so a new point
is here where does it go it goes here
because that has the closest of these
two centroids that one's the closest
again the number of clusters K must be
specified ahead of time k-means won't
choose k for you okay so how do you do
k-means clustering well this is
of cute so maybe I'll draw a little bit
better picture here so I can point at it
so you minimize this error function
which is basically the distance of every
point to its center squared okay so if I
had two clusters here's all these points
okay and they're all in the same cluster
and they all have center right here
let's make that the center and so I'm
going to measure the distance of each
one of these points how far is each one
of these points to its center square
those things and I'm going to do that
for all K clusters so then the other
cluster is over here and their center is
right here so I'm going to take the sum
of all their squared distances from
their centers okay so now I'm trying to
minimize this thing over what I'm trying
to minimize it over two things over the
selection of cluster centers what two
points should I pick for cluster centers
and the cluster membership for every
point I get to say which Center is he
going to map to so I'm trying to
minimize this objective function over a
cluster membership and cluster Center
selection okay well just saying that by
itself sort of suggests the cute
algorithm right for a given set of
cluster centers right so I'm trying to
find cluster membership and centers to
minimize this thing okay the whole that
whole space but if you told me the
cluster centers right if you told me oh
you know let me just get it wrong like
this is the cluster Center here and this
is the cluster Center here if you told
me those were gonna be the cluster
centers then sort of the cluster
membership is obvious right everyone
who's the closest to this one should go
to this one and everyone who's a closer
to this once you go to that one right
because I'm trying to minimize the sum
of distances squared so you pick the
smallest distance you get for every
point you have two choices pick the
smaller one so once you know the
center's the cluster membership is
obvious okay also once you know the
cluster membership the center is obvious
right if I tell you these points are all
going to be assigned to a single cluster
and these points are all going to be
assigned to a single cluster well then
it's just a problem of saying what's the
single value in that cluster that will
minimize the sum of the squared
distances well if you go around here and
find any single point where all the
distances to it and you sum them up and
square them is minimized that's of
course the mean right the mean minimize
is the single point that minimizes the
sum of the squared differences so if I
know the cluster membership I know what
the center should be it should be the
mean and so this sort of suggests the
algorithm just iterate over these two
things start with two centers determine
your cluster membership Fights mapping
everyone to their closest point then
take that cluster membership and figure
out the new center's by taking the mean
with those new centers figure out your
cluster membership again and just keep
iterating in this algorithm in this
manner and it turns out generally this
algorithm will converge and it will
converge to the solution that minimizes
this thing so that's sort of a very
simple algorithm for solving k-means
clustering okay so that's algorithm 8.1
in the book basically again select your
k equal to in my example points as the
initial centers form your cluster
membership by going to the closest point
then recompute your centroid by taking
you know the mean of your clusters and
the based on your cluster membership
assignment and just keep iterating this
until you converge right it's sort of
like a just converge and you say okay
this is under my convergence threshold
and so I'm done okay other algorithms
exist that are sort of more intelligent
but this one works generally and on the
homework I actually put a problem to do
this in one dimension so in R the
function k-means does k-means clustering
it's in the base package you actually
don't need to read in any special
package or library so just to illustrate
that I have sort of a toy data set here
let me show you my toy data so it's just
a two-dimensional data with like 100
rows let's see and two columns obviously
okay so here's my data and I'm going to
cluster this data and just say yeah
there's a hundred okay and what did I
say to do I said basically make a
picture of the k equal to solution from
the k-means plot the data plot the
fitted cluster centers using a different
color finally used KN n I'm going to use
KN n this is just sort of a trick right
because k-means will tell me the cluster
centers but then I want to know who
belongs to what cluster
well you belong to the close the cluster
with the closest center so I'm just
going to use KN n which defaults to K
nearest neighbor one to find what the
closest Center is and then color you
accordingly
Koller the points pour into a cluster
membership so here I do that basically
read in the data header equal false and
make a plot of the data let me I'll do
that much so you can see what the data
looks like it's basically two pretty
well-defined clusters when you look at
in two dimensions oh you know the one
thing I haven't really said which I
should talk about so okay so there you
see that the two you know arguably this
is a cluster and this is a cluster and
then the next thing I do is to call
k-means and then k-means is going to
give me back the optimal cluster centers
which then I plot here in the color blue
with that see X equal to means to make a
big potting symbol so this is what
k-means found the k-means algorithm in
our said this is one cluster Center and
this is another cluster Center right
there's I don't know if I'm right or
wrong there's no label on the points but
it's sort of identified this is a group
belonging to the center and this is a
group longing to this Center and so then
the last thing to say is well who
belongs to which cluster right does this
guy belong here or belong here well you
belong to the the the cluster with the
nearest center so the trick you know I'm
just sort of instead of competing all
the distances myself I'm just gonna use
KNN and remember how KNN goes it goes
training data test data training labels
so I'm just gonna say the training data
are just the two centers and then the
test data is ax so it's gonna map ax to
the closest point in the train data so
it's going to take every X and map it to
the nearest Center and then I just sort
of create some fake labels for those to
clip those two centers right call one
negative one on the other one and then
I'm going to color the points
accordingly X and color it so if this
thing whatever value this thing is will
be one of them and then plus one will be
the other one and use fine and carry for
equal to 19 and then this should color
in the cluster membership for me you
could go through and compute the
distances yourself but I'll just let KN
do it for me because it's probably
faster whoever wrote that code and
there's the cluster membership right so
obviously everyone's just going to the
closest closest one you know are you
right
well you've minimized your objective
function but again there's no really
right or wrong answer for this it's
right to the extent that its meaning
or useful or both as your book says and
so it's you know one of the reasons
maybe it's not such a hot research area
is because it's not like a question of
who can build a better mousetrap but in
practice you know if you have a
clustering technique that's useful to
you because it'll provide you some
utility and you'll be able to learn
about your data from it the one thing I
haven't said okay sort of doing this
instead of this fake two-dimensional
problem hides a lot in general for your
data and this is true with a lot of
things like nearest neighbor - you need
to have some notion of distance right so
when X 1 and X 2 are continuous valued
and on the same scale Euclidean distance
makes a lot of sense if X were binary or
X or ordinal or X for some categorical
variable it's not obvious how you want
to compute distances and that's sort of
the real trick chapter 2 section 4 talks
a lot about how to compute similarities
and dissimilarities between all
different types of variables and I would
encourage you to look at that we didn't
really spend too much time on it but
it's sort of a difficult problem in and
of itself how you define distances
between X values in an arbitrary space
and to do clustering that's sort of the
thing that you have to get right once
you get that right sort of the
clustering algorithms tend to be pretty
straightforward but you do have to say
what is my notion of distance you know
what is the notion of distance between
these two points in the X space it's not
always obvious how you're going to
define that
I'm not sure so the question is so in
here we're using euclidean distance and
actually you can map this into B it's
the maximum likelihood solution isn't it
if these are vibrating normal I believe
yeah so you can Mac you can map this
into maximum likelihood problem so
Chavez question is if I had different
types of data that weren't continuous
could I map it into a likelihood
framework and say that I ought to
maximize the likelihood function in that
model makes sense I'm not sure but that
would be my approach I think now that
you said it yeah it makes sense okay so
I think yep that's all I was gonna say
about everything so any comments
questions anything I will I will leave
them I will leave them up until until it
gets close to next summer or until I
have some strong reason where I need to
take them down but I think that if I
take them down obviously a lot of random
emails from people asking yeah yeah yeah
so I'm gonna try and leave them up until
sort of the last minute like maybe a
month before next summer starts oh so
the question was is the stats 202 calm
web page is going to stay intact and the
plan is that it will until 2000 what's
next year 2008 2008 about me May 2008 it
should stay intact until I okay well
thanks for coming and take care bye</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>