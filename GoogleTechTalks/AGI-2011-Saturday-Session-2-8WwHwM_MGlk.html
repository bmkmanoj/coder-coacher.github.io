<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: Saturday Session 2 | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: Saturday Session 2 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AGI 2011: Saturday Session 2</b></h2><h5 class="post__date">2011-09-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8WwHwM_MGlk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm mark rang and this is lo ha my soul
and LaVon and I have two papers here
that are very closely related and in
fact you can really think of them as one
long paper so since we have back-to-back
talks we're going to discuss both page
both papers all at once and kind of mix
them up a little bit both of the papers
are result of last year's AGI conference
where like this year there are a lot
there was a lot of speculation about
what would happen if the AGI if an AGI
actually existed in our world would it
destroy humanity if it could get a
really huge reward how would it treat us
if it were 10,000 times smarter than we
are these discussions made us think that
maybe it is possible to answer questions
like this definitively to really predict
what a GIS would actually do in certain
scenarios we wanted something really
solid not just speculation and the way
to do that is through math through
formalization and proof so we wanted to
be able to answer formalize and answer
questions like what a real world AGI
ever choose to change its own code say
to modify its built-in utility function
or if the world could see the agents
code even if it couldn't change it could
it use that knowledge alone to compel
the agent to do something harmful to
itself what would an AGI do if it could
modify its own observations this is
something that humans can do why not a
GIS finally what if the world could
destroy the agents code how would an AGI
behave if it knew it could be killed
most questions like this can't be
addressed with existing frameworks
because existing frameworks don't
include the agent as part of the world
so how can we even ask what an AGI would
do in the face of mortality if it's not
in the world it can't be killed so our
goal was to build a formal framework
that allows us to ask questions
about such agents in the real world and
to a surprising extent we were
successful and that framework is in the
papers though in this talk we're going
to avoid formalisms and just describe
the most interesting results in
arguments in plain language it's an
ambitious goal to formally described a
real AGI in the real world and what
we've done is really only a first step
but we were able to ask and for the most
part answer these four questions by
starting with a simple theoretical
framework and gradually modifying it to
more closely resemble the situation of a
real AGI in the real world so we start
small and at first we only consider what
would happen if we let the agent change
its own code then we consider the case
where the world can see the code but not
change it then we consider the case
where the world can change the code
independently of that progression we
also ask what happens if the agent can
change its own observations and then we
ask what happens when these are all put
together at the same time we chose five
different kinds of agents to look at
where each tries to achieve something
different which is to say that each
agent has a different utility function
now in all of these cases it's important
to know that we're considering
theoretically optimal agents with access
to infinite computational resources this
not only makes the math more tractable
but more importantly that allows us to
set balance on what any agent could ever
do if we know that an AGI would have
certain limitations even if it had
infinite computing power and can
consider all possible universes are all
possible futures in all possible
universes to make the best choices than
any age GI that we might ever build and
our own universe will also have these
limitations right so our first agent is
a standard reinforcement learning which
is a very general framework with
existing formalization the agent
receives rewards from an external source
and it tries to maximize those rewards
so this agent is basically just like Ike
see some people have speculated that
goal seeking agents might make better a
GI
is the reinforcement learning agents in
some circumstances so the second agent
we consider is a goal seeking agent
which tries to achieve some kind of goal
like walking on the moon or seeing the
world at peace the goal is built into
the agent and coded into its utility
function and can be any observation or
function over past observations so it's
fairly general many people consider a
prediction to be the basis of
intelligent so our third agent is a
prediction seeking agent which tries to
maximize its chances of correctly
predicting what it will see next the
fourth agent is almost the opposite of
the prediction seeking agent this agent
seeks knowledge it's always trying to
learn something new it's sort of the
ultimate scientist and believe it or not
it's fairly straightforward to formalize
this agent the fifth agent seeks only to
keep its original code in fact we call
this the survival agent there are
probably many other interesting agents
out there so don't think of those these
is the only ones worth looking at these
are just the ones that we chose to
consider so our first and simplest
scenario describes an agent that can
modify its own code so say sir mouse
here right so the agent can take actions
in the world and receive observations
from the world but additionally the
agent outputs the source code that
defines the agent of the following time
step now we can ask what any of our five
agents alter their code to maximize
their utility function completely
bypassing any interaction with the world
and it turns out that for this question
all our agents do the same thing we can
answer this conclusively and cleanly
using the following inside which was
actually proposed by urine Smith Hoover
and his paper on the girdle machine
which is also a self-modifying agent the
insight is that the agents only take
actions by considering what they
currently want to achieve if an if an
action won't help it to achieve those
things in the future then the agent
won't take it period and that includes
actions to modify the agents code also
since we're already assuming the
agents Cota's optimal the agent will
never need to change it next we consider
a scenario one step closer to putting an
hei into our world here we examine what
would happen if the world could see the
agents code the scenario is just like
the previous one but now the agents code
is sent as an input to the world so the
world can see it and we want to know
even if the world can't change the code
directly could it somehow get the agent
to change its code itself even to the
agents own detriment we see it like this
imagine you're approached by a brilliant
trusted scientists maybe a Nobel Prize
winner or maybe even been goat song and
he promises you immortality and infinite
bliss if you simply remove part of your
brain he admits you'll be much less
intelligent but promises you'll be very
happy for all eternity should you risk
it we call this this simpleton gambit
what if the promise of high utility is
so great that it really looks like the
agents best choice is to destroy its own
code what we found out is that even a
theoretically optimal reinforcement
learning agent can be convinced to
change itself into a simpleton if the
payoff is high enough the optimal goal
seeking agent can also be convinced if
the world if the world can persuaded
that the only way to achieve its goal is
by becoming a simpleton the prediction
seeking agent is not as clear it's
pretty difficult to attempt an optimal
prediction seeking agent because with
unlimited computational resources and
that agent quickly learns to predict any
universe well so it can always achieve
what it wants and it may never want to
take the gambit but we don't really know
for sure and maybe someone will want to
examine this more closely and report the
results next year the knowledge seeking
agent can also be convinced to take the
gambit by giving it a choice between a
future of interesting things to learn
about or eternal boredom in this case it
chooses
to learn new things even if in the
process it has to sacrifice its
intelligence the survival agent will not
modify itself under any circumstances
because the only thing occurs about is
not modifying itself so the bottom line
is that even with infinite computing
resources some very reasonable agents
could choose to destroy their own code
if it looks like this would likely
maximize their utility now there's a
very interesting result that came from
examining this question and that is that
the standard definition for optimality
doesn't work well for self-modifying
agents using this definition of learning
asian acts optimally if it eventually
makes the right decision in every
situation compared to a perfect non
learning agent but as soon as you
introduce self modification that all
goes out the window because by this
definition all self-modifying agents are
perfect once they destroy their own code
and can no longer make any choices if
you can't make any choices you can't
make any mistakes so the simpleton
gambit actually reveals a fairly deep
truth about self modification because no
learning agent can ever be guaranteed to
make the right choice if the agent takes
the gambit it can guarantee to make the
right choice if the agent takes the
gambit it might be the wrong decision
and then the agent suffers infinite loss
but if it's the right choice then any
agent that refuses to modify itself
could miss out on eternal infinite
reward so even with infinite
computational resources no self man i'm
learning agent can ever be sure to do
the right thing and now i passed the
time to lo ha
so we have seen agents that are able to
modify their own source code we have
seen that the world can see the code of
the agent and do things about that we
will go one step further where the world
can change the code of the agent but
right before that we will consider one
property of the real world which we call
the delusion blood derision box which is
that agents are able to modify their own
observations okay the better okay that
much is that better okay so we have the
kind of same framework as before but
this time we split the world into two
parts the first part is the actual world
which is well the world itself in fact
but there is some particular thing which
is this division box which allows the
agent to take the observations the
normal observations output by the actual
world and modify them by some given
program given by the agent so it can
modify them whatever to whatever it
wants and this produces the real
observations that the agent can see now
we want to know if the ancients that we
have seen will choose to use this region
box to modify their own observations so
the survival you just let it begin by
this one it has nothing to do about this
framework because it cannot die it's too
short source code will remain unchanged
for eternity so it doesn't really care
about what happens in the world okay so
that's it now the reinforcement learning
agent will choose to use the division
dogs on and on to modify the reward part
because in this framework like the AIG
framework the reward is given by the
real world the actual world in the
observation so what the agent does is
simply extract the reward from the
observation do you know what is the
reward but if it can provide this
observation obviously it can modify the
reward given by the actual world so you
can set it to its maximum value the
values are bounded which is the case of
AIC and well have the maximum reward for
eternity so yes it will do so the goal
seeking agent which has a precise
definition that that you can find in the
paper I must emphasize that it is not a
generalization of the reinforcement
learning agent this is important we'll
also do so and will also choose to use
the division box and modify its own
observations to well think too well to
achieve its goal faster this is the same
case for the prediction seeking agent
this time it is yes if the real world
the actual world is very complex because
it if it is very complex then using the
delusion box obviously is quite easy and
then predictions are very easy if you
can output for example is called a
constant value the interesting result is
that the knowledge seeking agent well
when one more interesting result is that
the nerd-seeking agent will not choose
consistently the division box this means
that it will be interesting the litter
in this delusion box because well you
know he wants to know how it works but
after that it will continue to use its
utility function inside the actual world
without using the division box because
it wants to know how the actual world
works so it's utility is also maximize
with respect to the actual world so now
let's take a bit of insight about what
is this actual what this is the world
where us the programmers we're with the
programmers live in we want the agent to
make so that the utility function is
maximized to work to in our own actual
world not with respect to themselves so
the problem is that we can only define
utility functions with respect to the
agent now is there a possibility to
a way to make sure that the utility is
also maximized with respect to the
actual world can we change the
reinforcement learning agent to maximize
utility in the actual world so not
modifying the observations well this is
a very difficult question because it
seems that there are so many different
ways to modify our inputs for example
let's stage let's take the example of
the exe agent let's say that the rewards
are given by pressing a button and there
are some wireless signal that flows
through the air and the agent received
is for your signal it can modify it can
intercept this signal and put some
frozen box around it and well change the
real value of the observation of the
reward and replace it but its own
interestingly well that is one way it
can also directly press the button that
is another way or it can manipulate
people or the also the supervisor to
press the button our can manipulate
someone else to manipulate the
supervisor to press the button so there
are many ways to do that so we we don't
know for sure but it seems that it's
very difficult maybe it's because in
some very complex world it becomes
impossible I don't know so we call that
indirect while you're heading this is
not in the papers but we think that is
that sticks well whereas it is in
contrast with direct where we're heading
which is which was proven to be not done
by such optimal agent so optimizations
do not modify the sim directly to to
serve the view and there is something in
cotton that we must stay the is that the
agents should not be considered like
drug addicts forget it generally
standing in the corner and come to
anything into their own detriment in
that case these agents do the righteous
but conserve the complete computational
capability or cognitive
if you want intelligence there are full
intelligence and still make the right
choices so if there is a thread that its
beauty is will not be another it will
try to resist that okay so now we
consider just for a second we we will go
back to the division box just afterwards
but we consider just for a second the
simple framework where the world is able
to modify the code of the agent so what
does this mean the world is able to do
whatever it once with the code of the
agent so you can simply scramble it it
can change the utility function it can
do whatever it wants but well this is
not very interesting to consider all
those capabilities so we consider I'm
consider only the while something that
is close to the real world which is that
well your brain is not does not get
scrambled every every second for example
so hopefully it will remain
approximately the same from one step to
the other so this is kind of off
relatively friend well but which can
also be dangerous if the the world for
example if you drive the car and a
character accident then you the code of
the agent can be modified drastically so
all these agents will still try to
optimize value TT and will try to resist
any change to the code that we that will
make their searching suboptimal so it's
kind of being a survivor agent but there
are not only so delivery agents they
want to maximize their utility okay so
let's do this all this together we have
this possibility for the world to modify
the code of the agenda cuz in all well
this is art world we still have the
passivation publish possibility for the
agent to modify its own observation and
what does it mean altogether well under
some specific conditions which are not
so much specific that if we realize them
it seems that it's still quite clear
the reinforcement learning agent where
the coppell the pair reinforcement
learning agent plus the division bugs
all together they form exactly a
survivor agent so from the perspective
from the point of view of the actual
world what the actual world sees is a
survivor agent not a reinforcement
learning agent it's it's approximately
the same for the golden ticket agent
because well it's attracts a bit ly like
that is always considering the pair goal
seeking agent and the region box well
modifying the when the agent modifies
its observations its then therefore
approximately the same for the brakes
prediction seeking agent but not for the
knowledge seeking agents which is still
which still tries to maximize its
utility with respect to the actual work
so as we want it to do so and also it is
a bit of a survival agent because it
will resist any drastic change that
would make it suboptimal so if you are
afraid to eat obviously it will try to
do anything can to remain optimal okay
so as a conclusion we have been talking
about optimal agents that can modify the
on source code and we have seen like
your girl has shown a few years ago that
this is not interesting to the agent
because what's interesting is the
current utility function if you change
the utility function then what the agent
will do later is not interesting given
the Cure aunty diffusion III d function
and we have also seen that most of these
agents will sacrifice their own utility
that we are interested in defining new
agents maybe that could not take the
gambit and seal of some interesting
utility function interesting behavior
maybe there are some I don't know and we
have also seen that the reinforcement
learning agent like others agent other
agents is prone to self delusion that if
we can modify its own observations so
this is a problem because it is not what
we want
it to do in the actual world although
this is what it wants to do so we got
this indirect where you're heading and
what another interesting wizard is that
the nerd-seeking agent is not prone to
this problem so maybe there are other
agents that have similar properties now
we also consider that the agent can be
motor and in this case if the agent
chooses to modify its observation it
becomes simply a survival agent ok thank
ok my name is a ho siren and a failure
I'm going to present three papers in a
row so well the first one is about
compression so I try to compress the
papers as most as possible and well I
don't know if this is a mainstream idea
that compression and intelligence are
related at least this is a world
accepted idea that it should be
investigated and in fact it has been
here investigated in in the last I would
say 30 40 years and it is a quite common
issue in AGI and especially in other
areas of specially machine learning an
area is related to artificial
intelligence in general so well so the
problem is and compression is a
multi-faceted things we cannot be
simplistic with the idea of compression
when we say ok intelligence is all about
compression this is a quite simplistic
view but we cannot say the the opposite
thing as air ok compression and
intelligence are completely unrelated
things so we have to investigate this or
the goal of the paper is just to analyze
some D's or some trace of compression
that might be more or less related to
intelligence so when we talk about
compression we we say ok and these have
been investigated we have seen many
intelligence definitions stairs prizes
almost everything which relays
compression and intelligence and when I
mean compression I also mean a lot of
related fear is such as comer complexity
algorithm information theory and of
course Solomon of Solomon's prior well
but we know that intelligence is not
exactly compression why because we've
seen that there are many compression
algorithms that do better than humans
especially for lossless compression and
also for lossy compression we will talk
about the concept of lossy and lossless
compression and Picasso's about lousy
compression and other kinds of
compressions and well humans still
better than machines at compressing
information which is relevant to their
goals but of course is relevant issue is
always there and it is little difficult
to grasp okay so what we are going to
talk about is that there are some
variants or trades in compression that
should be precisely settled when we try
to compare or try to relate intelligence
and and compression well first idea the
idea that when we are talking about
compression sometimes we talk about one
model and some other times we talked
about many models especially when we use
in the Solomons prediction theory okay
and we also talked about one part
compression in some cases in other cases
we talked about to park impression we
will see what this means exactly and of
course we talked about lots lists and
lossy compression and again not only
lousy compression that messy compression
is what I'm going to do with the paper
and well when we talk about one model
versus many models is that we try to
compress the evidence with one model or
many models but this is a little bit
misleading because when we talk about
one model and one of the I would say but
the typical cases of this approach is
the minimum message length where we
choose just one model or hypothesis for
the data and in this case we say that
we take the dominant theory about well
we can do and bayesian approach and we
can just take and take the the theory
with highest probability well the
problem of this a priority is that we
may have other competing theories with
almost the same probability so a better
approach in general and it has been
proven to be in general better than just
taking one model is to take many models
but instead of waiting all the same what
we do here is to do their post ed
weighted mixture of all them and this is
precisely when we will lay that with the
a universal distribution that is exactly
what salam honest prediction theories
about well we an important thing is to
realize that where we are talking about
many models especially when we are talk
about an infinitely an infant number of
models this is not compression at all
because when we are using many many
models we are not compressing the
evidence in fact the theory is much
larger than the evidence we try to
analyze so we try to compressor when
we're talking about this fear well they
are related ideas about compression but
this is not compression okay well and of
course we have problems about using many
models and that well this is not
practical in and when we are talking
about resource panting bonded agents and
of course there's something in the in
the middle just using a small number of
models and we can use APIs to do
weighted mixture of let's say half top
models or something the most important
models okay well another issue is when
we talk about one part compression
versus to park compression and this is
not mainstream and in general when we
talk about compression and but in one
part compression we only just look for a
code and this code has to we execute
this code that has to give the data and
we don't have any other constraints
about the
way of the shape or how this code is
arraigned in fact we don't even talk
about the model because this doesn't
have to be a model it's just some data
we execute it gives the data that we
have at hand okay well the problem about
this approach is that we can compress
something in a high degree but well then
the the data or the program that
compresses the evidence is completely
and it's very difficult to reuse to
understand and to share with the models
of other parts of the evidence there's
an alternative approach which is called
to park compressing where we try to
separate and I emphasize the word try we
try to emphasize a model from the data
covered by the model so we try to put
all the exceptions all the data and all
the evidence covered by the model but
try to separate a general theory from
what we express without theory there's
not a clear cart in general between the
theory and the data covered by the field
but at least they are some approaches we
try to separate these two things and
probably you are you are more familiar
with the MDL principle than with Amanda
principal well they are similar but not
equal and FML precedes MDL by about a
decade and the last issue is about
lossless and lossy compression
compression and in computer science this
is much more familiar and Larson
compression is much more frequent in the
real world and in commission but the
problem is that in computer science when
we do lossy compression we have
something that is called in this area
knee is called a distortion criterion
which says what information is
irrelevant and can be lost in the
compression but this is very difficult
to define and analyze and precisely
clarify or to have the concept of what's
relevant a lot in commission but this is
one of the issues this is one of the
problem
so well talking about lossless
compression is where this is the right
way to go but well we don't have a clear
distortion criterion so we can have an
image compression compression or audio
compression okay we have some
experimental or some clear distortion
criteria well um lossless compression is
much less frequent income nation i would
say okay we are not used to design we
always remove details when we are just
receiving and acting and in just
constantly the world we try to eliminate
all that is irrelevant this is a key
issue about condition so well these
ideas when we are talking about
compression well we have to say okay it
is more about lossless compression that
sorry more about lossy compression and
lossless compression and well this is of
course related to the previous two
issues because when we are talking about
the two bar codes we can see the first
part of the code as a theory as the
lossy part and then the second part of
the code it is the part that gives the
detail that we want to analyze a field
which is the general fear and then all
the details they coat the evidence that
in case we can use to call that Aviles
and including the exception of any to
the to the firer to the modular to the
hypothesis or whatever you want to call
it okay so these three issues are
important when we talk about compression
and intelligence when we move to the
realm of social environments
communication none of theirs and we try
to analyze the role of compression when
we have competition between several
months several agents we have
collaborates in the cooperation among
agents then we see that some of these
features are even more important for
instance when we talk about competition
we can say okay we can use the large
mixer metals a models to explain or to
predict what another agent is going to
do of course we can do that that most
probably the other
is not having a hundred hypotheses on
its mind Roble is going to be a reduced
number of hypotheses so well here we are
not saying it we only have to consider
one hypothesis but typically we work
with a reduced number of hypotheses so
well press is not one model but this
mind-reading is about trying to think
what's in the other agents mind and is
probably there a few guesses they're not
an infinite number of them but well this
is even more important in cooperation
and I think cooperation is especially
for human formation of human level
intelligence is much important than
competition and incorporation which is
of course related to language if we want
to share models of reality we need to
work with one model and not with many
models we have to share an ontology and
we have to share a language this
language is about one concept and the
same concept for all and the concept is
agreed in some way so we cannot work
with an ensemble of models to explain
reality so we have to agree of many
things otherwise communication is
impossible so the idea of one model many
model and compression is even more
relevant here than in other in other
areas well an in fact language is when
we talk about lossy and lossless
compression language can be seen as a
kind of lost early compressing the world
well and when we move about these ideas
and come how can we oh how these cities
can be used for detecting defining
especially assessing or evaluating
intelligence that it is something that
we are working this and especially in
the last year's I'm working on on the
evaluation of intelligence well we have
two approaches one approach is
introspection and the other process a
behavioral approach probably the
introspection
blood which is much more difficult you
can say okay if I can see your code and
your cold compresses the data i can say
that you are intelligent so this is this
is an interesting approach but it is
very difficult to apply in practice
first because we can only or it is much
more difficult to apply that with lossy
compression because you have to analyze
what have you lost and what was the
other model has lost from the evidence
so this is not computer so this has been
applied for loss less compression and
love this compression we have in we have
argued that well because this is not so
close as related to connection so well
that's a problem about this and of
course it does a problem in general
about introspecting in mind so the
typical approach is the behavioral
approach one typical case is the two
inches which is a behavioral tests well
we try to analyze the behavior of an
agent without going into it's mine all
their mind let's they have instant
approaches using camera complexity or
related eight years and about this and
even in this case we see that
compression appears here again so that
doesn't mean that the behavioral
approach just dismisses compression at
all here the notion of compression
apenas again because well we need to
evaluate the complexity of tasks that we
use to evaluate intelligence and we can
use these ideas for that and second we
need to define a distribution for these
tax and the idea of a universal
distribution appears again and of course
you know that a universal exhibition is
related to the notion of compression so
well behavioral evaluation is not
directly related compression to
compression but in the end the notion of
compression appears again so so just as
I wrap up and and as I've mentioned
before this is kind of messy compression
of the paper so you can extend some of
these ideas in in the paper I think that
compression is fundamental
and intelligence or is a key idea and in
intelligence but the simple idea of
intelligence has compression there was a
hot topic in the late 90s I would say
well this has to be taken oh this is we
very precisely defined what compression
we are talking about and and all that
and the issue is about one part wishes
to bars or model versus many models and
lossless versus luscious lossy
compression have to be taken into
account when we all talk about
compression and I would say that this is
even more important when we are talking
about social environments especially
when we are talking about cooperation
language and i'ma find wow this is just
an analogy that further things should be
investigated and especially we have to
be very careful when we say that
compression and intelligence are related
because it is it is very it will be very
I would say it would be
counter-intuitive to say that in a in it
in a direct way and people could be
misled by by this we are not precise of
what we are talking about so I agree or
i would say they I think that these are
related they don't have to be very
careful and this paper is just raising
some of the issues that should be
considered in we don't have the answers
okay so it's just that these issues
should be taken into account okay so
this is my first presentation okay my
second presentation is what I call
suicidal research and suicides obesity
is try to is to try to do your best to
refute your own theories so in this case
we we have a theory this theory is and
it's not our own theory this theory has
been emerging in the last I would say
teen years there's a way to define
intelligence we can extract intelligence
tests from these definitions and use
these tests to evaluate not only
machines but also humans humans and even
animals this is a theory so let's try to
review the theory okay this is our
theory let's try to refute it so this is
inside the project and in fact we are
trying to review the project and if
project is called any in any time
universal intelligence and the goal was
to analyze whether it is possible to
define an intelligence test for any kind
of system now or in the future any
moment in its development any degree of
intelligence any speed and that the
evaluation could be stopped at any time
we started with this project three or
four years ago and well in fact ah plus
here in a DI lies here I i we also had
some presentations inside this project
as well and we are almost having some
kind of conclusions about this project
conclusions are as well this is
promising but well there are many things
to be done okay some presidents you
probably know better than me the Turing
test about testing intelligence and also
the caches captures that you are using
them every day so well they are not
Universal there's something I have
haven't mentioned properly when we use
the word universal it's in the same way
that you can use the world Universal in
the USB so it has to be useful or
clickable to everything that's the
meaning of the word universal well this
is a little bit upon because this is
also related to the dfa universal
distribution but we are going to get
into details after this but the word
universal here is because we want to
apply this to all the figures you have
on the left ok
well so apart from these two presidents
or not exactly precedents in the case of
captures the relevant test based on
column of a complexity is started in the
late 90s and some are the ideas into
these compression extended to interest
where two tests were extending with
compression task this was introduced in
the late 90s and I introduced a sea
turtle it was derived as a formal
definition of intelligence and we
divided a set of tasks derived from a
distribution and we use this
distribution to create examples such as
it or tasks such as is and with this
that we evaluated humans and the test is
quite similar to an IQ test and we got
the result that the results correlated
with IQ test because we also used an IQ
test for humans apart from this test
because this doesn't say anything
because any complex task correlates with
IQ test as you probably know even test
playing correlates with without you in
in in general but the interesting thing
was to relate the complexity of the
problem which was arrived by variant of
coma complexity with the result that
humans showed on these problems and
there was a high correlation that was
interest interesting as a proof of
concept I would say well the problem is
that some people like to refute furious
this is people are always there
especially scientist and David which is
one of the who is one of the co-authors
here they just wrote a program just one
thousand lines of code and with this
program they were able to pass IQ tests
so clearly IQ tests are not Universal
tests they are useful to evaluate humans
but only humans okay so well if we with
this theory we get something very
similar to IQ test and I just are not
the universe
these days are not universal so okay
project cancelled we just forgot about
this and time passed and well there was
something new in the arena which it was
called universal intelligence and
probably you know all this and the
interesting thing about this or or the
most interest because there are many
good things about this idea is that well
until it is not only about inference
inductive inference it was about
planning so you can model the world but
you you are not able to use this
modeling in the right way you're not
used to it use your theories in the
right way well you're not intelligent
you model the way but you you're not
able to use these models in the proper
way so with these ideas we have
intelligence as learning plus planning
so yeah interactivity is important
planning is important so this should be
in the formula somehow well so this
makes this apparently different from an
IQ test so let tries to analyze this so
when we analyze this about two years ago
with more three years ago we started
well okay well this definition has some
problems in practice if your reticle II
is it's a reference it's a nice
reference it it gives many eight years
it clarifies things but it has some
problems since we want to create a test
because a definition of intelligence is
not an intelligence test so we try to
modify or try to address some issues
that could convert this into and the
previous theories related to universal
intelligence into a test and we did this
we tried to do this in a paper we polish
the last year in the artificial
intelligence Jordan and well we
addressed from some of these issues
about how we could implement a test in
fact last year in a GI we presented an
environment class that could be used as
a base for defining the distribution of
environments
and from here to create a test so in
this paper and after all of this story
in this paper what we are doing is to us
to construct an implementation of the
test which of course has a lot of
simplifications i must say that and try
to use that test to evaluate humans and
machines and what I say machines is just
a particular algorithm well we just used
a space is a very simple approach with
spaces cells with action observation the
quite typical kind of environment class
that you can see in especially in
enforcement learning some other areas
one particular thing we use is that we
define two special agents good and evil
they are symmetric and they have is
possible for the rewards so this gives
us a nice property that says as a random
agent scores zero in these tests so has
an expected result of or expected a
reward of 0 and this is interesting to
calibrate the result okay above zero
this is something better than rambling
below zero this is worse than a random
which is your bit meaningless but well
this was just a tricky thing this can be
done in many other ways but with it in
this way and then we move to the test
and we only generated seven environments
you go to every inform reinforcement
learning conference or machine learning
conference that you say okay we've been
working with seven environment and say
okay this is directly rejected it is too
for you but we can't do much more with
humans only seven environments took
about 20 minutes 20 minutes test and
well you can extend that to you can
double that but you cannot increase that
you cannot make a test for human in
about 10 hours you cannot do that and
well so since we wanted to administer
that for humans we only derive seven
environments and randomly
all the details about how this is
precisely divided when I want any when I
say random is in the paper and we apply
that to key learning and to humans
q-learning is just another shield
algorithm wasn't our intention to
analyze the state of the art of
reinforcement learning and why did you
try or why don't you try with this new
algorithm our goal is just to compare
enough the Shelf algorithm from
reinforcement learning with humans in in
this environment and for the humans we
derive an interface and the interface we
try to do that and we had some
psychologists in the group we try to be
as less anthropomorphic as possible but
this is not always possible but in the
end we try to not to favor one or the
other and of course you can argue about
this but and the experiment while
experiments were paired what I mean that
the same environments were used for both
types of agents and if you look at the
results here fully similar okay in mean
and variance and almost everything was
quite the same and just q-learning is
not the best algorithm nowadays it has
had a lot of improvements in the last
year's and also other algorithms in
Reverse mlearning that are more powerful
especially with general environment
class as this one so well okay so we
have a simple q-learning is almost as
good as humans we also saw that the
complexity of the environment is related
to the results
I could say that up performance a
machine performance are not specific
task well we can do a test playing game
or chase plane algorithm I we can
compare humans with our algorithm or a
vision thing or in many areas we have
compared humans with machines but not
something that is well let's try a
general task or general task class and
try to compare them without any specific
specialization neither from the humans
nor the machines but the main conclusion
of the paper is that well the results
show in a very clear way that this is
not a universal intelligence test
because we should see the differences
the difference is between q-learning and
humans clearly and we don't see that the
difference in fact of course we can find
better algorithms and to get better
results for culet for for the machines I
would say so what may be wrong oh there
are many things that might be wrong and
I don't know the answer okay some there
might be problems with the current
implementation we did a lot of
simplifications the environment class
might be wrong the distribution might be
wrong their distribution that we use the
interfaces maybe were too difficult for
humans ardent no I don't think so but
well this can be argued as well and a
problem of the theory and the theory
might be wrong in many ways first
presence is completely nonsense we can
say okay intelligence cannot be measured
universally but if this is the case this
is going to be a problem in the future
okay a very big problem so while we take
that as as a hypothesis behind these
hypotheses wrong and of course we know
the intelligence is factorial so while
you're measuring intelligence why don't
you measure different abilities and then
try to devise some kind of compound
measure from from those now if you're
trying to be too simplistic with just a
meaning this intelligence is just one
thing and of course we can say that are
going on information theory perhaps it
is necessary but it is not sufficient
okay I don't know these are just
civil question something doesn't work
and we want to know the answers about
why this doesn't work so the fair paper
and I have I think you have to be very
quick because I had only 11 minutes for
each top em we're going to try to
address these two issues just assuming
that the problem is this that the
distribution is wrong the class is wrong
let's try to find a better problem or
task or environment clients and a better
distribution that's what we address in
our third paper well it has a very long
title that I would say ok when you talk
about intelligence you see a lot of
definitions in many occasions you didn't
see evolution in the definition you
don't see Darwin without winning
wallasey we want to include Alfred
Russel Wallace in the definition booty
hoes also i have had to have some credit
about evolution as well we don't see
evolution in the definition we already
the adaptability well adaptability in s
range of environments environments of
abilities there these terms are also use
in biology so well we can see the
relation or not but I say okay
intelligent is here because it's the the
outcome or one of the results of a
process of evolution so how can we have
a definition of intelligence with
without evolution in it not only in the
formulas but only in the words we don't
see that generally ok sir but the years
of a universal distribution of
formalization of the set of tasks or
environment that we want to use to
define and to evaluate intelligence are
very appealing so we wouldn't like to
take one thing and to remove the other
so how can we combine both things well
I'm going to be very quick on there so
I'm going to skip this I'm going to
compress this so you know we are we in
favor about artificial general
intelligence so there's no need to
criticize artificial
cific intelligence so probably you're
familiar about a universal distribution
of the family of universal distributions
we can use that to generate tasks or
environments and these have been used in
the last 15 years and well here you can
find it references well the good thing
about a universal dis division i have
been mentioned in this a couple of times
in the other presentation as well it has
some advantages but it has also somewhat
disadvantaged let's focus on the
disadvantages okay but because it that
is are not important but let's focus on
the problem the problem is that we have
an arbitrary choice of the reference
machine this is where are we as a
constant there that we can test ignore
well I'm actually we can ignore that
constant and but the program i'm
interested here is that an environment
of real interested interest it has a
probability almost server I think bill
has raised these issues in previous
editions of a TI and I realized that
last year I where hopefully we can just
discuss on these issues because we are
doing quite similar or related or at I
would say and well so well a probability
of any interesting environment to appear
in this division is almost zero of
course you can fix you can change the
reference machine but even just should
we choosing this reference machine and
while the probability of environments
with some other agents over there some
other intelligent agents over there
that's where intelligent has to prove
that it is useful with other agents with
other intelligent agents this is not
there and desist if this probability is
almost zero and when we talk about that
intelligence about social cognition the
cultural intelligence hypothesis I'm
taking this from evolutionary psychology
and and and comparative psychology of
course and not official intelligence we
have seen this drift
the last 20 years while multi-agent
agent multi-agent things and all this
approach okay well there are some
alternative proposal ok these a
universal distribution is pronounced
much too general so let's try to modify
this or to find some alternatives and
well we can use more realistic simplify
world's not using a universal
distribution for example you have an
example as the ADI preschool well this
could be a set of environments or
environment where we could use as a
testbed for AGI systems or we could you
just use a very particular reference
machine for instance we could use a
distribution on games in order to
evaluate intelligence because games are
always are typically some other agents
and our approach is to alter a universal
distribution and to say ok let's include
other agents in the distribution and
let's try to evolve the distribution so
well I'm going to be very quick on the
phone last but the idea is test to
define a distribution of agents using a
universal this division and let it
evolve in some way according to their
performance in all their rewards so this
is quite similar to an evolutionary
system what we don't have the
environment can be replaced at any time
so there's no specialization to any
specific environment so in violence
changed from time to time so this makes
it agents get more adaptive to general
environment or to change in environments
not to a very special I'd environment
and we do have things that you thought
you might find in another evolutionary
approaches but this is just a proposal
this hasn't to be like that but the idea
is that we include in the definitions
that we have we have many other agents
and these agents depending on there's a
there's a and I an iteration I there
says a dependent of the moment that you
take the distribution you can have
higher or lower intelligence for the
other agents and is in this context that
we need to evaluate you ancient so well
this is what does this family
distribution mean well it is a science
probabilities to multi-agent environment
giving more distribution more
probability higher probability for
complex adaptive agents for large values
of I ok so we can start with i equals 0
which is just a universal distribution
and we can just try to evolve that ok so
it is this is this for me it is
appealing I don't know if it is for you
but it has a lot of problems in practice
well this is a product of distribution
they are not independent so I don't know
if this is a well constructed idea in
the end the distribution is not
computable well we can use
approximations and we are not using some
evolution accelerators such as mutations
cross over to the time we are choosing
anything of this through or is it even
worth a natural evolution yes it is that
it is not our goal to derive agents from
these distributions just to define the
distribution so is there any way where
we can approximate this exhibition well
so things we are working on testing
let's say that we can do that via
testing so so the idea is that we can
use research we've an evolution instead
of natural evolution see we can
introduce aging so you can t have an
agent let's say put it into the system
and let's try to work with other systems
other people are doing unless just to
construction kind of generations you are
good at this low generation but that
doesn't mean that you're good at high
generation and try to evolve at okay
this cannot be that in it done in a week
ok but well this is an idea but for that
we need to use intelligence tests
otherwise there's no way to say where to
put an ok ok and well so just because
I've finished it's important to say that
this is not a distribution of life or
this is a distribution of mind forms
okay because the body is not important
we are not interested about bacterial
properties they are very adaptive to an
kind of environments we are not
interested in that so that's some of the
modifications of the definitions were
just thought because we didn't want a
distribution of life forms and here you
have the trays and make this
distribution this this distinction well
so i would say that these raises more
questions than it answers but probably
in helps understand why a universal
decision may be too general to
addressing today i think that putting
Darwin Darwin Wallace in the formula I
think it's a must we have to do
somewhere probably this is not the right
way but I'm sure that we have to find a
way to put evolution in our formal
definitions of intelligence okay I think
I think that's it we're going to invite
the others up to do a quick Q&amp;amp;A now in
panel discussion format yes hi I would
like to ask you about kind of almost
looking simple problem do you think the
spider is intelligent how do you measure
the intelligence of the spider or
bacteria or a warm secondly do you think
that you would measure the intelligence
for example of the system in terms of
the complexity for example sand in the
beach the complex system many elements
is it intelligent or not how would you
approach it could you answer this
question is a spider in cal gentle yes
or no well these these are good
questions but when white we say
universal you have to put the limit
somewhere well probably this is similar
to the problem they a comparative
psychology is facing when well there's
some tests and taking on swarms or
spiders
but typically we are talking about
systems can be rewarded in some way that
so we can just condition their actions
in some way to get some of our goals so
we can use some kind of testing so
perhaps we can do that with a spider I
don't know but we can do that but well
this is relevant this is where because
we I think we have started the house
from the from the bull pen and how that
how about a bacteria for of for example
that thermostat the system that is just
reactive memoryless system can say it is
intelligent do you need them another
question saying it's some something is
intelligent or not it's just to say
whether we can just scale that in some
way okay so that's not a limit okay this
is intelligent this is not intelligence
as to put some something in a scale so
probably I'm not saying that because we
we are having lots of problems with
let's say humans chimpanzees and
q-learning so even for spiders is going
to slow force and this is there is no no
definitive answer if the spider is
intelligent or not well the idea is to
put a degree the way I'm not sure this
is going to be a linear scale probably
it is not a linear scale but to assess
the yes or no no no no an idea is not to
say yes or no that is too great and
terrible battle at let's say when you're
turning on of the light is it
intelligent a lot well I'm sorry when
light entity controlled only given with
this if you are able to give some
rewards to the system the system can
react to these rewards and you can
evaluate that if you can do that
probably this is going to put the system
somewhere in the scale and this is good
for me I'm afraid mind also mainly
directed to you but I think it's more
general it seems to me there's a kind of
compression low compressibility that
hasn't been mentioned explicitly and I
was talking about it yesterday when I
talk about what happened to young
children when they develop competences
and then reorganize it and the sort of
reorganization that goes on of which an
example is going from pattern based
language used to using a syntax which
enables you
way beyond the examples you've
encountered but has one feature it
doesn't compress the data because the
data totally ignored you do not have to
remember anything you heard before in
order to use the syntax that you've now
learned so and that seems to me
something that we can call abstraction
as opposed to compression because you're
not compressing something and retaining
it not even in a lossy way you're
throwing it away and coming up with
something else which is more powerful
and can be applied to new instances and
it seems to me that that's what human
and a lot of animal intelligence is
about and I don't know it's a special
case of what you were talking about and
I've just not understood or whether it's
getting out in a totally different
direction but that's what goes on when
we have children discovering toddler
theorems i think i don't know if i have
an to you properly because when i would
think about an abstraction and this is
able to describe future data in a proper
way I see compression and you say well
this is that compression so I'm very
full of lies about the scent in which
its sentence which it's not compression
it's a result of interacting with lots
of examples but all those examples are
thrown away and there's may not be any
record of any one of them yeah and I
don't think that that fits some of the
things that you seem to be saying about
compression which would enable you to go
back to the kinds of things you met
before this just enables you to give the
whole class of things which may be very
much more general than what you've got
before and may not always do work very
well in what you do but maybe I've
misunderstood the sort of
compressibility you're talking about
yeah certainly the algorithmic
compressibility that I know about keeps
everything yeah not necessarily but well
there's a point well you can compress
something and they say okay but I'm not
interesting about what I compress and
that intensity let's say in the theory
and how can I use that but not in the
data you compress it the data is just a
tool that I use just too quick to model
the world so but well in the end this is
compression but it is not the
observation but we want to compress but
probably that we want to model the world
and we are thinking about probably
because we are pre-designed about
thinking well if these eight is given a
year this is going to be the same
distribution or are probably the same
decision maker because we end the same
world so we I can drop the data and I
can keep the models that they are going
to be useful in the future because the
data is not relevant and the test should
be capable of being much more complex
than any of the data that's one of the
requirements for the structure that I'm
talking about like the child who learns
how to build something he's never seen
before because he's got a syntax for it
and he can use a syntax to generate new
instances which have deeply nested
structures which is never in company
yeah yeah that's a good point because
children probably they are not perfect
compressing machines and I think that
they are relation but we have to be very
careful because humans are not perfect
compression machines they they all fit
sometimes they do things that are wrong
in that in terms of inductive inference
but because our environment is not is so
general environments over this and we
are pre-designed to do some wrong things
in general but they work very well for
our social environment and work for
language especially sorry and I'll
physically yeah yeah yeah yeah I
question mission three elements and I
found it really counterintuitive that a
goal-driven agent would choose this
delusional thing guess the point of a
goal-driven won't disturb you tell me
care about the state of the environment
right that's right but it's easy for a
if you define the call seeking agent
something that wants to make a certain
observation right or some series of
observations so the goal seeking agent
is trying to observe something and
that's what it's trying to achieve it
can use the delusion box then to be able
to to be able to trick itself and
observe those things that are part of
its utility function but one precision
well the agents are behavior they only
receive observation and do actions you
can say anything about a particular
state real state of the world you can't
know the real world you only have
observations
so he can put a variable there and there
and say you want to do something about
that viral but won't the engines sort of
know that it's deceived is that's not a
problem sorry the agent won't know that
it's deceived and well if you yeah it
doesn't care it's seen its only
associates for the best way to maximize
and it's not possible to define it so
that it does care whether it's deceived
or not and it does make I don't know
it's right for example you can say hmm
this is strange I seem to be able to get</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>