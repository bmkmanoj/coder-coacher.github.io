<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Movie/Script: Alignment and Parsing of Video and Text Transcription | Coder Coacher - Coaching Coders</title><meta content="Movie/Script: Alignment and Parsing of Video and Text Transcription - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Movie/Script: Alignment and Parsing of Video and Text Transcription</b></h2><h5 class="post__date">2008-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/BwrnhMZT3Cc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so hi my name is Timothy Kirk I'm very
happy to be here today talking to the
tail of the talk is movie script
alignment and posting a video in text
transcription this drawing work with my
adviser Ben task 5 and I've worked on
other projects but I thought this one
was most closely related to Google so
this is the type of applications were
interested in we'd like to have
automatic collection of annotated
actions in the wild for retrieval and
indexing purposes also video and image
retrieval and video browsing and
summarization the longer term may be
also video mashup where a user would
have databases of segmented in the
indexed videos of objects and he could
plug them and mash them together for
example here Shannon walks away from
side would be able to plug those
different pieces together so we're gonna
use instead of image data sets we're
gonna use movies and TV series so those
are not just arbitrary videos but there
are very structured and the first part
of the talk I'll talk about sea level
parsing where you take a video and some
text data and the output is aligned and
segmented shot boundaries and scene
boundaries aligned with the screenplay
the second part of the talk I will show
retrieval of actions where you can query
for actions from the database of videos
and also segment out and recognize and
name characters and if I'll have time
I'll talk about finer grain segmentation
and alignment using shape so first part
enable parsing solid you read this code
so here's a simplified version of how
you could check to a movie scene you
start from the storyboard you take some
actors oops
you find different camera positions and
then establish establishing shot if I
have a close-ups over the shoulders so
those are the typical shots and also
shown in reaction shot typical of
dialogues and then the goal of the
director is to splice up together
different viewpoints to have a
continuous scene so what we're going to
do now is do a video deconstruction so
we're going to reverse the pipeline
we're going to find the shot boundaries
that's an easy problem we're going to
then find the scene boundaries and then
we're gonna find a recovery the initial
string of shot sequences taken from a
single camera view points to recover
smoothly continuing shots so there's a
lot of previous work most of all I've
dealt without any alignment with the
screenplay so no work we're gonna use
the screenplay
data which is widely available for a lot
of videos let's look at what it's
complete up you have dialogues you have
narrations scene transition levels
that's the one we're going to use for
the alignment and so we're going to try
to align it with the frames of a video
so ideally we would like to align each
shot to a particular dialogue or
narration element here what we're going
to do is extract the scene transitions
and extract the scenes and allowing them
to the videos however is you notice
there's no temporal alignment in the
screenplay so we're also gonna use
additional information of closed
captions which are embedded as ASCII in
the DVD so for the alignments we're
going to use the text of the dialogues
only the Texas is transcribed in the
closed captions to align it using the
simple dynamic programming it's called
dynamic time warping here alignment
between the Dera logs in screenplay to
the dialogues in the closed captions
notice that we don't have any temporal
alignment for the other parts which are
thin boundaries which are interested in
and the narrations which are interested
in for extracting actions so this is
very precise and it's actually a hard
problem
because again unlike dialogues
narrations are not temporary aligned and
also locally there's no difference
between our shots Banner in the scene
boundaries so question is this a scene
break so the answer is no because as we
can see it's alternation of different
shots from the same dialogue now is this
a scene break so the answer is yes this
time because we transition from one
continuty of people and locally which is
to agent as you take people to another
locally which is to other people and
it's particularly hard in this case
because it's a smoothly continuing shot
between the two that transitions between
the two scenes so unlike shot boundaries
scene boundaries cannot be detected
purely locally so our approach is going
to simultaneously group the shots in two
scenes mission in the screenplay and
find a reordering of the shots subset
where you recover as smooth as possible
transitions in the camera position and
this isn't like clustering approaches
because we also want to have a smoothly
varying shots as possible and for
example I'm gonna in the sequence here
we you can see that the camera smoothly
varies from a close-up to a far shot
then to a their different person where
we have also recovered the frame
registration camera registration
and clustering approaches would not have
such a property and it's also hard to
determine exactly the number of clusters
here so we're gonna build a generative
model for simplified running model for
generating a movie start by generating
the scene boundaries given the
screenplay next you generate the scene
structure which is generating a parent
shot for each shot in the scene and this
defines a Markov chain from which we can
sample each shot
independently so generally the first
shot in the scene then the second shot
third shot and the last shot again
modulo this reordering finally you are
going to generate the frames inside it
shot so the winner is the variable
probability of a scene boundary
probability of a parent of a shot given
the scene boundaries and probably this
is the appearance model probably have a
shot given its parent so this is our
graphical representation or I can have
the in top the scene boundaries and then
you have the Markov chain given the
scene boundaries and finally the shot
appearance model to generate each shot
so our prints model is very simple you
take two shots and you're going to say
that they're compatible only based on
the last shots in the on the last frame
in the previous shot and the first frame
in the next shot so we want them to be
similar here this is low because the
second person here is very different
from the first person here and notice
this is a symmetric by definition of the
person last frame on the posit this is
high because those two are similar same
color space so we're defining Indian non
symmetric and probability transition
matrix so let's look at the first
case simple case where there's no scene
breaks and if you do inference in the
model we want to find the maximum
likelihood property of parent shots this
amounts to a traveling salesman problem
in the sequence of shots and of course
this is a very hard problem and so we
haven't solved it yet but we're gonna
use a simplified model where you use
memory limited TSP which is you're only
allowed to jump back and forth from the
limited amount of shots and so for
example here with a width equals to 2
the first one is okay the second one is
not because we're jumping back two shots
and supposed to one-shot we're using
much larger K in practice so properties
of this limited memory reorderings is
that permutation of funds close to
identity also the similarity of shots is
sparse which is good for our purposes if
you want to define more complex shot
transition matrices so bellas has
proposed a linear time dynamic
programming solution for the TSP limited
memory problem where the state space is
the set of cities your visit and set of
shots or cities revisited of to time T
and the current city you're currently
visiting so the dynamic programming
solution which actually would also hold
with a without limited memory but it
would explode would be to minimize over
the previous shot in the sequence of the
costs up to that previous time plus the
cost for jumping from the pure city to
the current city WI I find and so notice
that even though this set
experimentation is is exponential in
fact it has on the number of such
settings is a it's actually exponential
in the west of the ring but it's linear
in
the number of shots and so we can build
a dynamic programming table compactly
again where transition from states with
previous shots and a previous set of
shots to the to the next job and the
computation time is linear in n again
example so we have current city at time
T so we've already seen all the of the
blue or the green checks and we want to
visit all the other ones sorry at city
at three and we cannot go too far in the
gray shaded areas and so next step we
can transition to this state visiting
this city here and you see the
transitions updated as follows so now
let's go back to our initial plan which
was you have to jointly segments into
scenes and also to find our shot we are
drinks so this amounts to doing this for
every single interval finding the best
permutation for each interval so the
naive algorithm would be computed TSP
for each other and then interest to
possible intervals and then combined
using dynamic programming
it's tractable but it's very expensive
so we're going to do a joint dynamic
program over the scene labels and the
shot reorderings and we can see that
we're going to gain a factor n by
including the state space also the scene
level back to our example where it is
city 3 now we can choose arbitrarily to
switch to a different scene so we were
at the beach and we're going to
transition to the jungle here and we're
going to pay no penalty in the
transition which means that there is no
expected similarity between shots from
two different scenes and so the
transition is updated as follows
ok so let's look at some results now
this is the data we've played with so 60
TV episodes 5 movies 6 orange screen
plays so it's more than the movies we
had
so it's a large number of images and a
large number of words so some results
so when typical episode for example for
I've lost would be 60 scenes about six
hundred shots sixty thousand frames and
three hundred dialogues which are
interval elements and also you have all
the other rest narrations which are we
are interested in for actions in the
second part so this is shot detection
that's an easy problem we'll get 97
percent precision F score for president
recall of shot boundary detection the
more interesting stuff is the scene
segmentation with alignments so what
this table shows here is that the width
of the rear range helps a lot so we go
from a width of K equals 1 to K equals
12 and we're plotting here the accuracy
for predicting a scene label of a shot
first ball at the beach or in the jungle
and also the score for scene boundary
detection regardless of the shot label
the scene level and those numbers again
our monetarily increasing for larger
width also now if you look at the second
row this is the uniform model where we
don't have any bias from the screenplay
as to the scene labels the scene breaks
okay the first one was using the
screenplay to find Tobias
where the scene brakes would be we have
large and substitute but it's still
useful bias and the second one is
without it and we can see we have
considerable improvements with using
screenplay this is gonna be very useful
for action retrieval of course time
increases but it's not too bad it's
about 1 minutes in the most complete
model
so see if I could there was those scene
labels you notice there was in brackets
accelerate the beach interior in the
jungle and those I think there's about a
little more than the actual number so
this those are the available scene
breaks for finding scene alignments and
we're gonna use from the Deming time
warping algorithm where we align
dialogues we're gonna say that this
scene break can wiggle anytime anywhere
between those the last time stamp and
the next time stamp and this intro is
pretty pretty large so some results on
the shot twinning shortened reordering
so we Valerie against grunt fourth on
when a positive loss and when episode of
CSI and we get an accuracy for
predicting the prime shot of about
seventy five percent and so as you can
see here well with some example but
where this is before reordering punch
shot is and generating sequence what's
the shot that generates you another way
to view it is the previous shots in the
permutation of shots in a given scene
that's the point shot so this whoops
so this video is again before threading
so it keeps changing three points this
one is much more smooth this is after
threading so we see there's much less
breaks in camera again few more examples
again this shows the advantage of I
supposed to do clustering doing
reordering because we have a smoothly
varying camera position from a close-up
far off to even far off and then
transition to a different person so I
have a lot of examples of those I'm
gonna skip it this is more easily seen
so we have
yeah so we are in the world about half
of the scenes for two episodes one I've
lost one of CSI that's how we got our
75% i Chrissy and then we're also gonna
show results for scene segmentation so
again this is you : wise last frame and
next frame uh and in the shots
hard hard scene more interesting scene
where without just a dialogue okay
finally this is so we have those results
for all the 20 episodes of lost and CSI
we tried on this kind of alignment
results so this is the narration this is
the aligned and we ordered sequence of
shots where again you have for each for
each red thing here this shows where the
scene breaks are and also it tells you
what's the set of characters you should
expect to see in the scene that's again
looking parsing the screenplay this is
useful for browsing summarization so
okay so so far we were just looking at
at the level of scenes we're not going
inside the inside the shots are inside
the frame so gonna try to see what we
can do now with our with the actions and
we can retrieve actions based on the
visual appearance of people track faces
and segments them so this is more
towards retrieval and indexing so we
have a database of movies we have some
queries like points opens the door can
you play it once more oops
and so we collected statistics here from
24 screen plays again downloadable from
the Internet of lost and let's see some
interesting that we can obtain so this
is a screenplay we parse it this is
using simple grammatical rules into
scene transitions narrations speaker and
dialogues and then we do sentence
tokenizer in para speech tagging
fact say the verbs and the also actually
do parsing of the sentence to obtain
subject verb complement and finally
we're gonna sum somatically filter using
word net those instances into like 20
categories adverbs actors objects
similar kills let's look at a few of
them so it was like 25,000 instances of
verbs with like 2000 different kind of
verbs looks is the most comments but any
birds like looks stares glances this is
gonna be the most common type of words
then you have motion verbs like runs
walks are fairly common those are more
interesting verbs because we don't have
such existing categories of databases
for it so those would be the ones or
would be mostly interested in for
example clients punches kisses compounds
like open door so we're using we're net
we have categorized twenty different
semantic categories those are verbs
there's our actors objects instruments
so you can surely define your categories
and then you wrap your query could be
yeah those are it's been by the person
paid not just the raw data okay so
interesting also single Kelly's like we
can have queries for beach jungle site
those kind of interesting queries for a
user and also time of the queries it's
also seeing keywords like internet
services are very very common and like
overlap sound off screen yeah so now
we're gonna try to align it the finer
level the text screenplay that is parsed
and the videos so given the screen
we're gonna do pronoun resolution why
are you doing this because in the
screenplay first of all the interesting
bits are not the dialogues because they
don't convey any information by the
visual scene the interesting bits are
the narrations but about more than half
of them contain pronounce and so we need
to resolve them in order to align them
somehow with people so in this example
the print and resolution and side hold
the interval in his hand becomes sides
and her becomes son given the context
and next we're using verb prints again
that's going to be useful for the verb
queries side holds bottom and side
questions son this is the parsing of the
initial screenplay narration then we
would like to identify people locations
objects and actions so print resolution
I'm gonna skip this this is adapted from
Hobbes algorithm but in our case it
works very well because the sequence of
text is very very structured and so we
can with higher crazy about 80 percent
recover the pronounce for the dialogues
further narrations only so now we can
use this to add a study final level not
yet at the pixel level but it at least
at the frame level the line and query
for different action verbs so this is
using the aligned screenplay and the
line shot boundaries the Alliance scene
boundaries with the with the screen
flight to query for different kind of
herbs for example shouting oops
all right look at focus online and see
that it's more less focus on when I went
type of action and we have this for that
we have tons of instances of each
actions okay for sure it's a bit dark so
now we'd like to improve the retrieval
in order to go to the pixel level and to
the to have face tracking naming so this
is more involved but it wouldn't help us
to have more precisely aligned actions
in terms of widths temple weights and
also special with so one face detection
and tracking cross shots of the
character naming and segmentation and
aside for later like to do camera press
restriction see of course in geometry in
gaze estimation we're not going to talk
about this here so let you read this
quote from Hitchcock okay so people are
central in movies we anything we want to
do with actions is gonna have to pass
through people in particular face
recognition so we're gonna take coach
Cox advice and do find those ovals those
orange ovals and the scenes so that to
do face tracking across different
polarizations in the rich have smooth
tracks so you use your Jones as a
frontal detection which is low recall
but someone highly precision then you
build some discriminative color model
this is very simple here we're simply
using the frequency count of a given
color inside of the bounding box divided
by the frequency counts in the
neighborhood outside and so we're using
this to track face track the face by
trying to fit some shape of the face to
in the subsequent frames to fit this
color this color scoring again notice
the face can turn rotate so we
we'd like to have a lot of variations in
practice we're actually going to use
tons of different deformations of the
the face across scale translation
spectra show and also shape whether it's
frontal or profile so tube is gonna
heavily rely on finding convolutions I'm
gonna let you watch some long face
tracks so again there's like as some
point you're gonna notice weird shapes
this is no it stopped
so those ribs are either non frontal
once the profile once okay in cetera
notice again Villa Jones is going to
miss a lot of those face tracks because
of own non front old Ness and just in
general because there's no truck in both
so this examples here where this is very
dark and we're still able to recover the
face track this is because we have a
discriminative color models here and we
can recover after though the occlusions
from like save the fire okay so this
heavily relies on coalition so we're
gonna propose very fast convolution with
so-called polygonal approximation so
it's relying on only three ideas first
of all this greens theorem we want to
compute a convolution this is again our
face face shape and on the template
which is converting here on the template
which is the score color model and
minus-- theorem tells you that you can
compute a surface integral is a line
integral which is sparse this is good
next we're gonna use a political
approximation of the the contour of the
object and we're going to show how to
compute this efficiently with an
optimally in using dynamic programming
finally we're gonna combine this with an
extension of elegance to 45 degree
integral
which is in order to compute those
integrals not over the initial control
but already illegal an approximated
contour and so this is going to be very
fast convolution in order of number of
vertices in the polygon as opposed to to
make them number one go right because
you have to assume something about this
movie so so here we're using discretized
pixels there's no such smoothness but
however you could consider other type of
so this where it's completely with
arbitrary topologies of the mask it
could work with holes multiple
components already if it has multiple
color then you need to have to encode
each one color separately okay so the
first part is greens theorem again you
all know this I'm going to skip it
no didn't you sing that is the polygonal
approximation so we have a shape that we
want to approximate say with 20 vertices
sorry how much time ten five minutes
before the questions and so what we're
going to do is minimize the
approximation error between the its
initial shape and the shape obtained
with our polygon vertices okay so the
approximation error between the shape
and the vertices is the sum of the areas
here of course it's good to go inside
this for display purposes and so we're
going to approximate this sum of errors
by those shaded here so it's just so
what we're doing is skipping the corner
bits which are small compared to the
rest of the areas so this is this is
going to be positive some that we're
missing
if it goes inside it's gonna be a night
of some so those are a small in practice
so the darn programming solution now is
you're defining
the error of to time up to verse
verdicts M of a vertex ZM as the minimum
over all the previous other vertices of
the approximation score okay and so you
can use dynamic programming here you can
use Rios overlapping repetitions by
minimizing over only the previous
verdicts of the energy so far plus the
energy on the last added vertex of the
last segment and so we're gonna build
this table of number of discretized
positions where you could fit you could
fit the vertices notice we're not
constraining to be having a vertex on
the shape which would result in really
bad approximations and so we're gonna
build this anything programming table
over a number of vertex positions x
number of vertices say 20 in our example
so the complexity here is number of
vertices x number of positions squared
because of the dynamic programming times
number of terms of complexity of each
cell here the competition of the last
segment however notice that the previous
vertex in the next words have to be
aligned in either 45 degree angle or a
long straight lines okay so this is the
key part where we're gonna reduce
complexity into a number of purchases
time number of segments instead of
number of squared positions this is much
smaller finally we can show that the
competition per cell is is constant time
as opposed to something complex so the
key here is to notice that we can
compute the error on the given previous
vertex give a compute the error under
given the previous vertex is e2 we can
compute the error by having a z3 in
constant time by having a bunch of free
competitions so we pre compute the
projections that's using simple distance
transform to compute projections of any
vertex second we're going to approximate
the distance between this the sum of the
scores here to the segment to work an
approximate it to the distances of the
points here to the line as opposed to
the segments so this is most cases a
good approximation
finally the sum we're simply using mmm
cascading operation to compute the sum
of the verses classical can be computed
as a difference of two integrals and so
in the end final term can you compute in
order constant and we have very
efficient ways to to compute the
approximations this is an example that
shows you that linear approximation
depends on the rotation but not on the
scale so for each rotation we're gonna
have to recompute it but that's fine
because this is recomputed prior so
we're doing this for faces and for other
objects yeah lastly is the convention of
this with the greens theorem so in your
we're gonna sum over not just the we're
going to sum of our segments as opposed
to summing over individual pixels also
on the boundary and this is simple
extension of Villa Jones integral image
to 35 degrees angles so in the end what
matters is the computation times goes to
two times number versus I supposed to
village ends which is one time the
number of vertices which is 4 so that's
same order so we have huge storage size
improvement for example Tony by two
hundred template with a efficient
encoding can be stored as 20 bytes with
a political approximation as opposed to
40 kilobytes if you use the binary image
to store the mask so this is factor of
10,000 improvements and also the
commutation time imprisonment is also
very big so if use and again with
convolution can go down to n times
number of vertex s in your polygon with
the polygon approximation and can also
go down to linear without coarse to fine
on top of it coarse to fine again is
well because we can have random access
competition of the dot products as
opposed to a 50 which which is global 3
you couldn't request okay yeah
listen you're solving this individually
for every frame of the video yeah
continue oh so yeah actually do have
temporal continuity this is mostly for
avoiding to launch onto a new track
suppose there's two phases but
computation time would still be
completely doable if we if you did or a
neighborhood or to assume that your
current track would be with some very
dire of the previous track yes we're
only considering a small rectangle to
compute the next track and also some
smooth operations across let's go yeah
good question so so now we also want to
do segmentation some go quickly over
this the ideas want to fit those those
ellipsoids to find our tracks and
segments so this is shows videos shows
also naming so we have a dialogue here
between two people and again notice
where to compute this the naming here
what we're going to do is use the
reordering of a sequence of the threads
and into a constraint sequence and use a
simple voting scheme to vote for each
thread vote based on the co-occurrence
of dialogues between the name of the
speaker and the given shot that is
trying so this is this is very core
scheme but it works okay because we have
larger scope to compute the voting skin
as opposed to something we'll see later
which is shot per shot
so another video whoops another
another that I've seen it's not perfect
of course there's not for Frank last
video again the longer the scene is
longer you can have redundant
information to compute the bias for
being that character this character and
finally this shows the advantage of
using under of using a short rail ring
as opposed to divide chart per shot so
on the right hand side you see the exact
same scheme applied to a shot-for-shot
voting based on the co-occurrence with
dialogues so on the right hand side
again makes a lot of mistakes because
the wording is it's small and there's a
lot of incertitude in though in the
alignments so the love through this work
on finding faces especially enjoyed this
tremendous group and there was Deborah
Menon I've worked on extracting faces
they have pretty impressive results also
too for face extraction for example
David has extracted sixty six hundred
thousand faces and we're gonna skip
camera post registration just some
results where we register posed friends
from the first shot to friends in the
next shot okay how many maybe I should
stop here in like one or two aside I'm
gonna skip the one half okay so I'll
skip this part go to the questions just
some results on so articulated we're
having more articulate templates and
segmentation with articulated templates
so that we want to do again for videos
using this efficient scheme and it is
efficient
but matching so other reason results
were we we compute not only the
segmentation of ism vice man-horse
dataset not only the segmentation of
horses into different parts but also the
labeling of different parts of the
horses alright so in conclusion we
extracted there's a lot of structure in
movies and TV series more than in
arbitrary and constrained videos and it
the structure can be recovered and to
improve tracking labeling and
segmentation so we proposed a way to
recover continuity for tracking through
this wedding using dynamic programming
and finally the alignment with the
screenplay is crucial for having
accurate indexing it gives a cheap way
to align a lot of a lot of data but you
need screenplay and it's available for a
lot of movies you could find on the
internet and also the closed captions
this is very easy to get this is most
DVDs of the produced in the US there's
tons more to do but this is just the tip
of the iceberg just very briefly some of
the projects I've worked on so the new
your time multiscale segmentation some
segmentation of large point cloud data
and on inference side source I have
designed SPECT relaxation for solving
Markov random fields for the map problem
using eigenvectors with the fine
constraints which encode site per site
constraints and will obtain a one of our
number of labels approximation guarantee
and we extend this using an eigenvalue
optimization for maximizing arbitrary
general quadratic energy functions under
under affine constraints and we can show
that we have tighter than your excisions
form a perimeter and terrorism special
so gradient we can talk about this if
you're interested ok so this concludes
my talk if you have any questions
ladies horse model hmm where is where
youth the model for the park this is a
hens line but we're this is again ship
to obtain what we do is yeah we do is we
obtain we have the horse segmentations
from the Weizmann horse dataset and we
just cut into different limbs we just
keep torso head front legs black lights
and that's it and then we simply have to
but we have obtained like 94% decision
or pixel pixel white labeling micro see
and 95 using the articulated model so we
haven't done Sun check yet this is also
an interesting problem especially
because of the weak alignment between
closed captions and and the videos it
would seem it's very aligned but it's
actually not very well aligned and so
using Sun track would definitely help
apparently gender discrimination is well
that's easy so that could help in love
cases however when there's background
noise this could be a little complicated
but I believe with a some work it could
definitely be a big improvement also for
YouTube videos soundtracks might be the
only way to go since we don't have any
screenplay available so you might want
to use on this or single path or your
data
where'd you get screenplays why are
those available online publishing yeah I
haven't like to mention to the legal
issues but so so far we just downloaded
the from websites they're like publicly
available for a lot of TV series
especially TV series there are very
structured and for movies as well and
you can download like there's really a
lot of I wouldn't say most movies I
would say I've seen it for a taste of
thousand of our movies but this is a lot
to start with you can move that be
displayed where you were doing the non
resolution in the word prediction yep so
once you have all of that what's your
actual representation model this this
one yes so you have those things where
shade voids the bottle yeah what's your
representation model
okay so we're subject purse again to
loosely loose grammatical categories
such as subjects verb and objects and
those are slightly loose categories over
a lot for reorderings for you both
jack is helping then helping would be
the verb is opposer is which is not very
informative and object is again again
loosely defined from the front of parse
tree
we're don't use a strict parse tree
which is something a little more
infirmity so that we can literate rip
actions for this right to get a model
for the bottle end no resolve it later
on no we're not trying to model objects
at this point and this is this was a
second to use much larger classes of
object cuz classes of verbs without
knowing anything about it this
really for again a line retrieval so
informative triple as opposed to
detection but again that would be based
on this large data said that we could
collect and you could start running
models so I can think of a bunch of
filmic techniques like shaky-cam
or smash cuts or things that might cause
you trouble have you had enough as your
have a television shows and movies that
you've tried but enough that you can
spot specific problems or troubles and
directors so there's four shot
boundaries we had some problems with a
with I mean the hardest parts about
fixable shot detection was the smooth
transitions the faint fades those for
those we have a simple fix which is
visible when the frame is uniform color
than you at some point you decide it's
like a separate shot black shot and I
say those black shots are also mentioned
in the screenplay
there is for the shakes that's going to
be more problem for whatever we do with
tracking of faces
I mean I've I've showed a couple of
videos where camera was fast motion but
there was no such thing as shake like an
amateur videos or or stylish videos this
is a harder problem yes use the
constraints from the dialogue what kind
of performance to be gained for a
detection so I had no ready for this
so the dynamics was on the top and the
uniform model is only one and so of
uniform is without using this way yeah
uniform only uses their appearance model
and obviously you're gonna do very very
badly at the scene labeling because if
you shift from when you're dead so
that's not very important but the
interesting bit is the predicting the
scene boundaries so that we still do
better
and you drop the last one off just
because it was going down at that point
oh yeah I guess it was just doing yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>