<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Beyond Coverage: What Lurks in Test Suites? | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Beyond Coverage: What Lurks in Test Suites? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Beyond Coverage: What Lurks in Test Suites?</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hoCqnb7_N9o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Sonal Shah: Our next speaker is also from
University of Waterloo. He is an associate
professor of computer engineering over there.
And one of his passions has been making static
analysis tools accessible to developers.
Patrick is talking about what actually makes
a good test suite today.
&amp;gt;&amp;gt;Patrick Lam: Thanks, Sonal.
So of course as a professor, I have students,
and so I have to thank Felix Fang who did
a lot of the work and a lot of the complicated
numbers to find out he did that work.
I'm Patrick Lam.
So the way I thought I would structure this
would be to look at test cases or test suites,
and myths versus realities.
So what we're going to do is we're going to
look at ten open source test suites, and I'm
going to tell you what I could find about
these test Suites based on looking at them
based on doing static analysis of them, based
on doing dynamic analysis of them.
So let me start by talking about the test
suites that we're investigating.
So they come from a variety of different problem
domains. They are all Java programs or libraries.
We have Google visualization which draws graphs
and charts. We also have JFreeChart in this
same domain. It draws charts as well. We also
have graph theory. So that's as opposed to
charts, it's graphs. You know, nodes and edges.
We have hyperSQL which is a database written
in Java. We have the comments collections
library. We have JMeter which is a performance
testing tool. We have WEKA which does machine
learning. It's a machine learning library.
We have poi which manipulates open office
documents, the XML formats. And we have another
tool in the XML area, the JDOM. So it manipulates
general XML files. And we have Joda-Time,
which deals with time and date manipulations.
So these are ten benchmarks, and they all
have test suites. And these benchmarks are
from pretty small, 30K lines of code, to hundred
thousand lines of code in the case of WEKA.
And so we'll see this on the next slide as
well, but what proportion of the code is accounted
for by test cases, and that varies from 5
percent in the case of WEKA to more than half
of the code in the benchmark is accounted
for by test cases in the case of Joda-Time.
So what we'll do is we'll start by talking
about these test suite properties and then
we'll have three myths and the realities or
misrealities that we see on these test suites.
So first of all, we can start by talking about
the test suite size. And we can compare the
test suite size, which you can see on the
Y axis going up and down with the X axis,
which is a systems line of code. And so I've
plotted all of the numbers for our test suites
here.
And so what it looked like to me was we sort
of had two groups of benchmarks. We had the
group of benchmarks with not very many test
suites, so here, and so I did a least squares
fitting here, and so what we have is there's
a -- you know, there is a -- whachamacallit.
There's a constant factor, but then for every
line of -- for every hundred lines of system
code, you have about three lines of code,
of test code, and then we have the benchmarks
with more test cases. And so on average there,
after the coefficient we have about 30 lines
of test code for every hundred lines of code.
So these benchmarks tend to fall into two
clumps.
The other thing I looked at is test cases
which when you run the report it reports I
ran 500 test cases or whatever, versus number
of static test methods you could look at by
just basically grepping the code.
So what we have here is essentially about
one test case per method for a lot of test
cases. So this is a line. This line represents
one test case per method. Some are below that
for weird reasons, but most of them are slightly
above that. And then Apache Commons collection
is way up there. It has a lot of test cases
per test method, and I think I'll talk about
that right now. It is a huge outlier. It does
a lot of reuse. We will also talk about reuse
later on but I thought I would highlight Apache
Commons collection right now. It implements
data structures, maps, trees, that sort of
thing. And so there's a lot of commonality
between the different test cases in this test
suite.
So let's look at this specific example. This
specific example is test map, TestFlat3Map,
and that contains only 14 test methods. And
then when you run it, it's like, oh, I ran
156 test cases. So it's like what's going
on here? That's strange.
So there's inheritance. So when you have a
jUnit test, then it is going to run the test
from a super class as well. And so beyond
the 14 test methods for TestFlat3Map, there
are also 42 tests inherited from super classes.
And then there's this thing that Apache Commons
collections has called bulk tests. So it's
basically just (indiscernible) all the tests
that have to do with maps, for instance, and
so it runs a lot of those tests as well and
that's where the 156 number comes in. And
when you have that for all of the test classes
in Apache, that's where you get the huge blowup
of number of test cases versus number of test
methods.
Okay. So that was the static for the numbers
for the test Suites and sizes. The next thing
I'm going to talk about is the dynamic numbers.
So it's like how many tests -- how long does
a test run, for instance, and that should
be the next slide.
So test cases should sort of run fast. And
as it turns out these test cases are mostly
pretty hermetic. I'll talk about a bit that
later. But these tests run fast, too, and
so most of them run in under 30 seconds. There's
JMeter which takes 53 seconds and then JFreeChart
takes some sort of undetermined number amount
of time, but on the run on the report here
it took about four minutes. It sometimes took
seven minutes as well. So most of these tests
do run very quickly.
So flaky tests. How many flaky tests are there?
These open source test suites are pretty simple
and they do not have very many flaky tests.
Except for HyperSQL. I just couldn't run it
so all its tests are flaky, I guess.
Google Visualization has 76 out of 384, so
that's high. But everyone else is really low.
And so six of these test cases or six of these
test suites have zero failing test cases.
WEKA has one failing test case and JDOM has
three failing test cases. So for whatever
reason, these open source projects probably
have the thing where it's like, okay, you
can't really commit to this project if you
have failing test cases.
I also went and looked for evidence that there
was continuous integration both on this slide
in terms of daily builds and in terms of test
wit running daily. So most of these projects
do have daily builds. Seven of them do in
particular. Google Visualization does not
seem to be (indiscernible) developed and it
doesn't have daily builds, for instance, but
most of them do have daily builds. And almost
half of them have. And almost half of them
have daily tests. They can get other people
to contribute infrastructure to them. For
instance, Surefire is one service that provides
test case running for open source projects.
SonarQube does as well as Travis continuous
integration. So there's quite a bit of continuous
integration, but it's not everyone that runs
it.
Okay. So the next part of the talk is going
to be talking about various test case myths.
So we'll talk about coverage now, and so some
people kind of say it's a really important
thing. Especially in textbooks. And that kind
of annoys me actually.
So I've taught from this textbook, introduction
to software testing by Ammann and Offutt and
I've highlighted in yellow all of the content
in the textbook that's about coverage. So
I was trying to figure out how important is
coverage to developers in practice as measured
by the amount of effort they spend obtaining
coverage of their test suites? And of course
in the textbook, there's a lot of different
types of coverage, but if you run tools, then
you know that you can get branch and statement
coverage, and there's also complexity coverage,
which I couldn't understand what EclEmma was
measuring for that. I don't know what its
definition was.
The number of definitions of coverage in the
book is large. The number of coverage that
you can actually measure is small. So I went
and looked at the coverage metrics, and so
this is the results from nine benchmarks that
I could run, I think.
So the average coverage, I believe -- I didn't
actually compute what the actual coverage
was but you can see what the coverage was
for these benchmarks here. So for some of
the benchmarks you can see coverage is important
to the developers. It doesn't really happen
by accident that you get, like, 95% branch
-- instruction coverage here.
For some of the others, it's really not. And
it also correlates, by the way, with the benchmarks
that had very few lines of code. They also
have low coverage. So WEKA was in the category
not many tests. JMeter was also in this category.
You can also see that instruction and branch
coverage are pretty similar and you probably
already know this.
So from my observations it, seems that developers
act as if coverage is sometimes important.
It's more important to some people than others.
But tools only give limited data about what
coverage we're getting. So I guess maybe one
way to think about it is coverage is one part
of a balanced breakfast, or whatever.
So maybe a guideline is to consider metrics
beyond reported coverage results. So WEKA
has very low coverage, but then you look at
what WEKA is doing, for instance, and the
way you commit something to WEKA and you get
it included in the main integration is you
write a paper. You write a new -- it's a machine
learning toolkit so you have machine learning
algorithms. And the way you get new algorithm
into this toolkit is you publish a peer reviewed
paper about your algorithm. So in some sense
the software is not validated using testing.
It's validated using peer review. So that's
another way of ensuring software quality.
There's also, of course, coverages that are
not covered by QA tools, like inputs-based
coverage. And they're important, too. But
they're not measured by tool, so you have
to hope that you have high (indiscernible)-based
coverage, I guess, is what it boils down to.
Okay. So the first myth was about coverage.
The second myth that I had in my head was,
tests are simple. So I think most tests are
just a bunch of lines of code that do something
and then do some asserts. So I set out to
figure out how complex tests actually are.
So what I did was I looked at two things.
I looked at the complexity of individual test
cases, and then I also looked at the dependencies
of tests, networks, and file systems.
Let's look at the static code complexity of
our test cases.
So the first thing I did was, I looked at
these test cases and said, how many asserts
do each -- does each test method contain or
which percentage of test method contains more
than five asserts in the test code.
So here is an example. I kind of simplified
it probably a bit too much. So it doesn't
actually show the right thing. But it has
five asserts. So I looked at methods that
contain just by counting at least five asserts.
And so here, most test methods, the vast majority
of test methods, are pretty simple and contain
fewer than five asserts. So, in particular,
I believe the geometric mean here is somewhere
in the 30% range. And so Apache Commons Collection
has the most complicated test methods here,
the highest number of complicated test methods.
Google Visualization has test methods that
are very simple. I believe that most of them
just have one -- they do various things, and
then they fail, they have a fail call if the
state is not as expected.
And usually you have at least 70% of the test
cases which are pretty simple.
The next thing I looked at was beyond just
their length, how many of them had branches;
right? So if you have code that doesn't have
branches, then in some sense it's pretty simple.
It just starts in the beginning and goes through
to the end. Here's an example of a test that
has a branch, it has an &quot;if, else.&quot; In the
&quot;if&quot; branch, it is going to call assert equals
wrapped in a try block. And the &quot;else&quot; branch
does the same thing, but it's not wrapped
in a try block.
Another example, another thing I looked for
is how many test cases have loops. It's like
why are you writing test cases with loops?
Well, this is one reason you might do it.
It's JGraphT, which is graph theory. And so
it's iterating through a set of permutations
in the test case. And then it actually counts
the number of permutations and makes sure
that it's what it expects.
So how many test cases have branches? How
many have loops?
Most test cases have neither branches more
loops. So, once again, I should point out
that this is not 0 to 100, this is 0 to 60.
And WEKA has a lot of complicated test cases.
So it has not many test cases. However, the
test cases it does have are complicated, and
60% of the test cases it has is going -- are
going to have branches, and 30% are going
to have loops.
In general, the geometric mean here is 5%
of the test cases have branches -- have loops,
and I think it's about 20% of test cases here
that have branches. So 80% of the test cases,
in general, do not have branches, and so they're
just straight-line code.
Some benchmarks do not have any loops at all,
like Joda-Time and JFreeChart. So all of their
test cases have at most branches, but no loops.
Okay. So that was the syntactic complexity
of these tests. So most tests are pretty simple.
And then the other thing we might want to
know about is, what do these tests use? Do
they use a file system in do they use a network?
So I did a grep, basically, about what these
test cases are actually using.
And so most test cases use a file system in
some way. So what does that mean? I then dug
in and looked at how these test cases use
the file system.
And I would say they're okay. So they do things
like create temporary files and then compare
the temporary files against golden masters.
They verify -- they serialize -- serialization
is a big reason you might use a file system,
because you want to serialize. Even if you
serialize to memory, then you still need to
compare it to something. That something is
usually -- you can put it in your code, but
that would be silly. You would be better off
putting it in a file than comparing it to
a different file. There's another serialization
case for JDOM. JMeter creates charts. It tests
their existence. It doesn't actually -- It's
hard, I guess, to test that the chart actually
looks like it should. And these open source
programmers don't have the resources to actually
test the charts are actually the correct thing.
But they actually check if the chart exists
or not.
There's also some comparisons versus golden
masters as well.
Okay. The other thing I was curious about
was whether these tests used a network or
not. And two of these tests do use a network.
JMeter and poi.
And so the question is, how do they use the
network?
Okay. So JMeter is totally legit. It has a
mirror server. It does stuff which serves
files on demand. And so it tests versus a
mirror server that it puts up on local host.
So that's fine.
The other test is kind of sketchy and it connects
to this hard-coated URL, SC.openoffice.org.
I don't really recommend doing that in your
test feeds.
The other side how you do avoid dependencies
is by using mocks and stubs. And in these
test cases, running these test suites only
Google Visualization used true mocks in the
sense that you have an object for which you're
recording expectations.
There are stubs in four other suites. So they
create dummy objects which have no functionality,
but you can call the right methods.
Okay. So the myth here that I wanted to examine
was, test cases are simple. And I -- I think
I confirmed this myth.
So these test cases have very few asserts.
They have some branching but not that much.
Most test cases do not have any branches.
And there is some file system usage, mostly
to test serialization, and some network usage.
But that was pretty rare.
So what I'm doing is I'm saying myth and then
reality, then a consequence, which you can
take back with you, I hope.
And so the consequence here is that most of
these test cases are pretty simple and they
don't need a lot of high expertise to write.
Of course, you need expertise to design the
test system. And as we've seen here, designing
whatever you want to -- how do you want the
test to work, that's hard; right? Designing
the test system, that's hard. Writing the
tests themselves if they're just simple JUnit
tests, not that hard. So the tests here are
pretty simple.
Then you have -- that accounts for maybe about
80% of the tests. About 20% of the tests are
actually difficult code to write. So there's
something going on for these tests.
Okay. The last myth I want to investigate
is, how do people write test cases? Do they
leverage a lot of reuse? Do they leverage
a lot of subclassing? Let's talk about the
different types of reuse that people might
use when they write tests.
So you might have, for instance -- in JUnit,
you might have setup and teardown to set up
a fixture. You might have inheritance, which
Apache commons uses a lot. You might have
related test classes subclassing the same
parent class. Or you might have compositions.
You might have these classes that are used
by multiple test classes to perform tasks.
So I investigated each of these types of reuse.
So setup and teardown, it's -- again, varies
by test suite. Joda-Time, almost every test
case does use setup and teardown. That's just
the way they set up their code.
In most of the other cases, setup and teardown
is not consistently used. So JFreeChart, JGraphT,
and POI use -- about 10% of test classes have
setup and teardown. For the other classes,
there's maybe about half of the classes that
use setup or teardown. So it's there and it
makes sense for some tests. For others, it
may make sense not using it. I don't know.
Inheritance is heavily used in four of these
test suites. And so in these cases, 50% of
test classes inherit functionality from some
other test class, some abstract test map,
for instance. And so we can see here again
it varies in Apache, hsqldb, JMeter, and WEKA.
Most of these test classes are going to inheritance
functionality and use the inherited functionality.
In a lot of others, they just always inherit
from JUnit test case.
The next thing I want to look at is helper
classes and how many of these classes use
helper classes.
Here's an example of a helper class. So you
have this helper class record inspector, which
is just going to return this list of records.
And so I just went and counted the number
of classes that looked like they were helper
classes. And this is a relatively small number
in terms of the total number of classes. So
there are some at most double-digit number
of helper classes. I think what I can deduce
from that is that most of the functionality
of a class is going to be located in that
class itself and not elsewhere. So classes
are mostly self-contained either in that class
or in those classes, super classes, as we
saw on the previous slide.
One thing I looked at was test cloning. So
another way of doing reuse is by using the
copy command and the paste command; right?
That works pretty well. So what I investigated
here, Divam Jain started this work, and then
we continued this work with Felix. How many
test classes appear to be -- or how many test
methods appear to be test clones of other
methods.
Here's an example of test clone. So you have
these two methods in WEKA, I believe, and
-- so they're, basically, identical except
that you replace nominal in the first method
with string in the second method. And so this
is a form of reuse. How far does it happen?
Let me talk a bit about the way we discover
test clones in our test suite.
So the insight here was that assertions were
key to detecting -- to what a method does.
So if two test methods have the same set of
-- the same set of asserts, then they are
going to be likely clones. So we looked at
the assertions and how they worked in the
control flow graph to determine likely clones
in the test suites.
So the incidence of cloning, on average, 40%
of test cases looked to be clones of other
test cases. I also did some manual classification
about how the false positive rate was. And
most of these are actually not false positives.
They are actually legitimate clones. I think
it's, like, 75% of these are actually clones.
So it's all about the same, except for Google
Visualization, it has very few. But for most
of them, it's somewhere in the 40 to 50% range
of test class -- of test methods which are
clones of some other test method.
Okay. So you have this test cloning thing,
and maybe that's not so great. Maybe you'd
like to refactor your tests. What can you
do? Well, you can use setup and teardown,
which some test methods do. You can use subclassing,
which some of our test suites do. JUnit 4
-- not many use JUnit 4. But if you did use
JUnit 4, then you could program advertise
your unit tests, so you could specify I want
to run this test over this data and this data
and this data. And it will run the same test
method over all this data. There's also notions
from the research like test theories.
Apache Commons Collection, like I said before,
it was weird in the sense that it has some
disproportionate number of test cases compared
to the number of test methods that we have.
And so it has this notion of bulk tests. And
so what's going on here is that you can run
a predefined set of tests, and you give it
a test object, and it runs all these tests
on this test object.
Another way of getting reuse is by having
automatically generated tests. And that's
kind of like parameterized tests. It's another
way of doing that.
For JDOM, what happened here was that they
have a grammar for whatever they have, and
then they generate tests based on the grammar
as well. They actually only generate stubs,
not the actual tests.
So how much automatic testing technology -- well,
you, of course, run the test automatically.
But is there automatic test generation technology?
In our suites, we didn't find a lot of it.
So what I would say is that the principal
automation technology was copy and paste in
these test suites. The reality is that automated
test generation is uncommon in our test suites.
So it looks like people generated these tests
by hand. I can suggest maximize your use,
you can use setup and teardown, you can use
inheritance, parameterized tests. Whatever
works for you, there's a lot of options. If
you think it's a win to get more maintainable
test suites, you can consider doing your tests
that way.
So some examples of test generation tools
are Korat, which allows you to generate structurally
complex tests for random testing and fuzzing.
We also heard yesterday about the Chrome fuzzing
test work as well. So you can definitely fuzz
your tests.
So I can summarize with the three myths. So
coverage is a key property of test suites.
Yeah, maybe. I guess so. It's sort of important.
Tests are indeed simple. So I think I can
confirm that.
And tests are written by hand.
Thanks. I will take questions now.
[ Applause ]
All the data, if you want it, is on the slide
as well. You can look at it.
&amp;gt;&amp;gt;Sonal Shah: (Off mic.)
Any questions? Anything on the moderator link?
Great. Thank you very much, Patrick.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>