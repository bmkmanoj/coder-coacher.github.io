<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical Aspects of Data Mining (Stats 202) Day 11 | Coder Coacher - Coaching Coders</title><meta content="Statistical Aspects of Data Mining (Stats 202) Day 11 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical Aspects of Data Mining (Stats 202) Day 11</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/l4a3e__QzoY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so welcome to lecture 11 today
we're going to finish up chapter four
and start a little bit on chapter five
as I said next week will be the last
week next week we'll finish up chapter
five and then do a little bit on chapter
eight which is the clustering chapter
okay so let's see this is a homework
assignment which you don't care too much
about so chapter 4 is titled
classification basic concepts decision
trees and model evaluation so again the
point of chapter four is basically to
introduce you to the classification
problem we talked specifically about
decision trees which is one method for
doing classification just so we have at
least one example and sort of
historically this is one of the first
methods that people dealt with now of
course we have more modern techniques
that are much more effective which we'll
talk on chapter 5 and then we'll talk
about sort of how do we evaluate
classification models so this was sort
of the picture of the problem right you
have data and there's one attribute in
particular that you care about you want
to be able to pick that one attribute as
a function of the other attributes and
that attribute is a binary attribute
which makes it a two class problem here
it's called yes or no we could
generalize to having multiple classes
but we're not going to generalize to it
having to be numeric because then that
would get into a regression problem so
we're trying to predict the binary
outcome here using in this case three
attributes so we learn a model which is
the induction process then you take that
model and you apply it to new data and
you want to see how well you can predict
the class correctly on new data now in
order to have some measurement of how
what you're going to do because when you
get new data you're not going to know
the class so you're not going to know
whether you're right or wrong but you
want it to perform well so to have some
measurement of the performance you hold
out some of your data as a test set
pretend like you don't know the class
pretend that that's new data use your
predictive model apply it to the new
data get the class estimates and then
see how close those are to the actual
estimates and that would give you an
estimate of your of your generalization
error or your miss classification or on
your test set so that's a picture of the
problem the description is here right so
we have the training set which we call
the attributes X and then the additional
attribute why is the one we're trying to
predict and we use a model to predict
the class as a function of the other
attributes and the goal again is that
previously unseen records right if I get
new observations ought to be able to
predict the class as accurately as
possible and again
test set is used to determine the
accuracy of the model usually the data
set is divided into training and test
sets with the trainings that used to
build the model and the test set is used
to validate it of course you know so
there's more advanced procedures like
cross-validation where you repeatedly
leave out different fractions and use
those as test sets in a number of other
procedures there are many techniques
algorithms for carrying out
classification in chapter 4 again
they're focusing only on decision trees
chapter 5 gets into more modern and
effective techniques and that is sort of
to say that decision trees are neither
modern or effective which is a pretty
accurate statement although someone
asked me last time they said but we use
decision trees when we do boosting or we
use decision trees when we do random
forests yes okay that's true decision
trees are often used in some of the
other modern and effective techniques
but by themselves they sort of come out
of favor and some of the in favor of
some of these other techniques example
of a decision tree we saw last time this
for time to use whether or not someone
got a refund whether or not they're
married single door divorced and their
taxable income to predict whether or not
they cheat on their taxes so this would
be an example of course when you apply
it to new data you're not going to know
whether or not you're right or wrong but
if you pre-plan apply it retrospectively
you can use some data to fit it and then
a new data set to predict and see how
well you're doing in measure your
accuracy so you can determine how
accurate of her picture you have built
most algorithms that grow decision trees
use sort of a top-down approach so you
start off with dt in any node and if all
those belong to the same class that's
the terminal node and you stop if
they've lined two different classes then
you continue to split so you sort of
start at the top and you keep splitting
as you go down so usually the splitting
is done in a greedy fashion greedy means
that you simply choose the optimal split
at each stage according to some
criterion now that may not be optimal
even according to the same criterion
because you're not looking ahead right
you're only doing what's locally optimal
however the greedy approach is
computationally efficient so it's
popular and then we just mentioned last
time that we had sort of three questions
to ask right what types of splits should
we allow how do we select the best split
and when do we stop well for number one
we said let's just use binary splits to
be consistent with what the are part
algorithm is doing so nominal attributes
you split sort of all the classes some
of the classes on one side and the rest
of classes on the other ordinal you
might want to preserve the
order although our part doesn't bother
with that and then numeric attributes
you basically choose a cut point
everything less than that cut point goes
on one side everything greater than that
cup one goes on the other side for
number two what type of criteria should
we use well we can either use miss
classification error Gini index or
entropy I mean you could imagine other
criteria to all of these are within a
terminal within a node sorry not a
terminal node within any node that would
be a terminal node if you stop there
within any node how pure is it okay and
in really in reality this is like the
lack of purity right so if I have a one
of the classes seventy percent
probability than one minus point seven
is thirty percent miss classification
error so thirty percent is a measure of
how impure it is okay Gini index and
entropy will talk about to all these
measure the lack of purity or the the
impurity within a note in fact entropy
it's intuitive right how much entropy is
there the the maximum of all these of
course would occur when you're at 50 50
for the two class problem okay so why
would we use misclassification error to
determine the splitting on the nodes
well we care about Miss classification
error right in the end of the day you
want to know how low your miss
classification error is on your test set
so it might make sense to use it as the
splitting criteria the reason we don't
often use it is because it's sort of
discontinuous and I'll give you a nice
example than a second but it's a simple
thing to use 1 minus miss classification
error is called the accuracy and then we
left off with this example last time
where we said suppose that this is the
data I have three attributes a B or C
they're all either true or false and
then the question said well which
attribute should I split on and actually
we figured out that you should split on
a right because initially your miss
classification error is 50% right you
have twenty five percent sorry fifty
positives and fifty negatives so
initially it's a 50-50 case so you have
miss classification or at fifty percent
however if you split on a you can see
that the positives for a let's see where
a is true so this is a true and this is
a false if you look at just the examples
with a true you see there's 25 here 25
positives and zero negatives for a true
where when a is false you see 25
50 negatives and 25 positives so 25
positives and 50 negatives so splitting
on a true or false in one node you could
call this the positive note because it's
25 positives and negatives and the other
node you can call this the negative node
because there's 50 negatives and 25
positives and you see that you have one
pure node and one node that's sort of 50
out of 75 pure and so you're only going
to classify 25 with 100 observations
wrong so the misclassification error for
this tree is now gone from fifty percent
to twenty-five percent and that is the
better split and B or C if you check
them out now the one thing I what the
reason I sort of brought this up again
is because if you look at those three
measures right they're all measures
within a note right they're all measures
within a note all these three and so the
question is if you have a measure within
a note how do you combine them to get
the overall measure well let's just take
for example this one right in this
classification air is 1 minus the
maximum probability within the node so
here you have two probabilities right
one hundred percent and zero percent so
the maximum is a hundred percent so the
Miss classification error in this node
is 1 minus 1 or 0 write this note has
zero missed ratification ER the other
node you have two probabilities you have
a 25 out of 75 and a 50s 75 so 50 out 75
is the max so Miss classification er is
1 minus 50 out of 75 so the mist
classification are in this note is 25
out of 75 so I have two nodes one is
Miss classification error 01 has miss
classification error 25 out of 75 so how
do you combine them to get the overall
in this classification error well you
take the weighted average right 25 of
the 100 are in that node which has miss
classification number 0 and then 75 out
of the 100 are in the other node which
has the most classification of 25 on a
75 and so of course obviously right you
wind up with 20 50 100 which you knew
because you said well you're it's
getting 25 of them wrong the reason I
just went through this though is because
in general that's how you're going to
combine these metrics right you compute
them within each node and then you take
the weighted average over all the notes
okay so it's obvious from this
classification error but in general for
Gini index will compute Gini index
within each node and then take the
weighted average to figure out the
overall Gini index same thing with
entropy so that's miss class
canary we did this example Gini index
okay this is actually what gets used in
our part in our why does it get used in
our part in our because our part in our
is based on the cart algorithm which
suggests you should use the Gini index
for two classes you can see Gini index
actually turns out to be proportional to
the binomial variance after you compute
it in each node again you take the
weighted average right rated average
within each node is the overall Gini
index so you compute Genia with a known
and you take the weighted average so
let's see um here you can just see no
dewyze computations of course this would
be a completely pure node right 06 so
you take 1 minus 0 squared minus 1
squared you get 0 right so Jeanne is is
measuring sort of the lack of purity so
it's 0 here because this is completely
pure and then of course jeanie will
increase as you get closer and closer to
fifty percent ok because it's simply
measuring the lack of purity so of
course the highest one would be if I had
three of each which isn't up here but
you see it increasing and again you just
take all the probabilities you square
them and then you subtract from one
after you add them up okay so this is
the example I don't know if I'll sort of
go through this one the full way just to
save time but this is one of the
problems I the textbook where they
basically ask you you know should you
split on a one or a two according to the
Gini index maybe I'll just do one of
them of course you have to compare both
of them and then say which one is better
again you just you're going to compute
the Gini index within each node and then
you are going to take the weighted
average across the two nodes to figure
out the overall genie of X so let me
just do a one and so suppose you split
on a one I guess it's again either true
or false if i split on any one true or
false well let's see a one true I have
one two three sorry one two three
positives tell me if I count these wrong
and a one true i have one that's it one
negative so that a one false I have won
only one positive
and so a 1 false I have one two three
four negatives is that right that means
there's nine things here okay nine
things good okay so three two one vs. 3
2 1 and 1 and for too long ago so sort
of both nodes are almost pure right this
one has sort of 25% and 75 and this one
has 20 and 80 so both nodes are almost
pure so then how do you do the Gini
index well for this note here right
you're just going to take 1 minus 0
point 2 5 squared minus point 7 5
squared okay and then for this node here
you're going to take it as 1 minus point
2 squared minus point 8 squared okay and
figure out each one of those and so then
the overall value right it's going to be
the weighted average of these so let me
just sort of write the overall value so
4 tenths of the observations are here
right so you're going to have four tents
here sorry I always think there's 10
thanks four nights here for whatever
this first value is plus then the five
nights here or whatever that value is
okay so i can just sort of do that in
our in one shot and let me do that and
then check it with the number on my
paper to make sure it's right i think
the only thing you can mess up is the
signs in this one let me see if i can
make this a little bit bigger GUI let's
do 14 okay oh my god it looks bigger
course I can't see my cursor okay so
what did I say so four nights four
ninths times that node plus five ninths
times the other node and then the first
node we had probably 2.25 and point
seven five so 1 minus point two five
squared minus point 7 5 squared and then
the other node was point2 and point
eight right 1 minus point 2 squared
minus 0 point 8 squared that there's
some funny front assess their there's
too many parentheses Oh point 2 squared
minus point 8 squared that looks right
so point 344
see if that's the right answer sounds
familiar Gini index for this one is
point 344 yep and it turns out so a one
gives me 0 point 3 4 4.3 44 if I split
on a 2 i'm not going to do that one just
to save time but if i split on a 2 turns
out i get point 4 8 9 a-2 is not such a
good split so that would favor splitting
on a 1 in that problem so that's just
sort of one of the textbook problems
that tells you if you were to start with
the decision tree and finding your first
split and you were using the Gini index
as our part does it would split on a one
right away and so decision trees the one
nice thing about them is that they sort
of have this sort of built-in variable
selection right they sort of consider
all the variables which you know sort of
might all be correlated might all do a
good job but they sort of pick out you
know at this stage which is the best one
to use and once they pick it out they
also tell you what is the best split to
use so not just saying I mean here I
have to split true or false only has two
values but they find the best split and
to tell you which variable to use so
there's some nice features about them
which will exploit later when we use
things like boosting and random for us
okay any questions about that problem
okay so that's sort of what our part is
doing internally when it starts growing
the tree from the top down now the one
thing we wanted to talk about is why
would you use something like the Gini
index to grow the tree when you really
care about Miss classification error
well I think this example is sort of a
one of the simplest examples that can
show you why that might be useful again
the the Gini index sort of has like a
nice gradient where it kind of points
you in the right direction with regard
to the probabilities whereas miss
classification error really throws out
the probability information and is sort
of discontinuous and only looks at how
many are right or wrong so if you take
this example for instance you start out
with a parent node that's 723 right so
you 7 C 1 and 3 C 2 so initially your
miss classification error is thirty
percent okay and so then you're going to
split that suppose you have that one
attribute that you are considering
splitting on attribute a it could either
be yes or no okay and when you split on
that
one becomes 30 and node two becomes four
three now the interesting thing is right
if you were to pick the majority class
in each well first let me see let me do
the Gini index right so Gini index for
30 is 0 right because that's a pure note
1 minus 1 minus 0 0 right so that's a
pure node Genia licks for this one four
and three well 1 minus 4 7 squared minus
3 7 squared point for and I know it's
pretty big fact it's bigger than the
parent node but when you take the
weighted average three tents are in here
with 0 7 10 * in here at the point for
not actually comes out smaller right so
Gini index says you know this is
actually a good split to do decreases
the gnex I mean you can compare to other
possible splits but you know this one
gives you a decrease from point four to
two point three for three however if you
are splitting only on Miss
classification error as we did last time
you would see this node would be called
class 1 and this node would also be
called class 1 and so you're still going
to get those three wrong so your miss
classification error would start at
thirty percent and it would stay at
thirty percent and so Gini index doesn't
really point you in the direction of
this split it doesn't really tell you
that this split is any better than what
you started with so with sorry miss
classification error doesn't tells you
that this split is justice goes the
originals but it doesn't tell you that
it's any better whereas the Gini index
goes down from point for two 2.43 then
this classification error stays at
thirty percent so this is why we often
want to use some sort of surrogate loss
functions such as the Gini index even
though in the end of the day you care
about Miss classification error the Gini
index is nice because it sort of carries
along information with you to sort of
help you grow the tree just like when
you do logistic regression right you do
maximum likelihood even though you care
about this classification error in the
end of the day still the likelihood
function gives you some extra
information and turns out to be optimal
to use that same thing with the Gini
index when you grow the tree even though
you only care about this classification
error the end of the day the Gini index
will sort of point you in a good
direction because it will use the extra
information from the probabilities okay
any questions about that slide okay so
then the last thing we had to talk about
is entropy entropy similar to Gini index
right I mean it might suggest different
splits but just like Gini index it's
going to use the information the
probabilities this sort of helps you
grow the tree
II so how do you compute entropy well P
times log P negative and you sum those
up log base 2 just to be specific
there's of course theoretical reasons
for why log base 2 and we can talk about
those you can motivate in a lot of
different ways but entropy has a lot of
nice interpretations that you can come
out from different angles similar to
sorry it's used in c-45 right so if cart
and our part use Gini index than c-45
uses entropy which is one of the
differences between those algorithms
probably not the biggest difference but
it sits there after you compute the
entropy in each node again you take the
overall weighted average the decrease in
entropy has a special name is called the
information gain right so if it's just
you know when you're using Gini index
you just look at the decrease in the
Gini index when you look at the decrease
in the entropy that's called information
gain just sort of a special name and
they talk about that on page 160 so it's
sort of similar to Gini index in that
it's a measure of purity or rather
impurity and then you take the weighted
average to turn how to do this plate
here of course you see so okay so you
say well why is this 0 where we're going
to define 0 log 0 to be 0 just because
you know the limit as X goes to 0 of 0
times log X is of course 0 so we define
that to be 0 so then you get the answer
you want the perfectly pure node has
zero entropy okay that's of course what
you would want and then one fifth one
and five has 0 point six five entropy
two and four has even bigger entropy and
of course three and three would have
entropy one which is the largest entropy
you're going to get so that's how you
compute entropy within each node similar
to Gini index it sort of starts off at
zero then increases as you get to the
50-50 case so here's a problem from the
textbook it says for the following data
set for a binary class problem calculate
the information gained when splitting on
a and B which attribute with the
decision algorithm choose so maybe I
will run through this one depending on
how much time there is the first thing
though you have to know is when it says
calculate the information gain they're
not just saying choose which attribute
is better they specifically want to know
what the information gain is so the
first thing you're going to have to do
is compute the original entropy of the
tree before you do and
split so this is number 38 so you would
say entropy for the original tree is
something like well the original tree
has outfits at nine or ten one two three
four five six seven eight nine ten this
time ten things four of which are plus
so it would be like negative 4 10 log of
4 tenths minus 6 tenths log of 6 tenths
right that would be the original entropy
and of course this is log base 2 which
you know it's just going to be a scaling
but to be specific we should do that so
let's see let's see negative 4 tenths
for tens times log of 4 tenths now of
course our is not using log base 2 so
I'll just divided by log of 2 minus 6
tenths times log of 6 tenths divided by
log of 2 and that looks right point nine
seven let's call point nine seven 10 so
that's the entropy for the original data
right before you do any splits at all
and so when you're computing information
gain you're going to take the difference
in entropy between the split and this
original value point nine so 10 so let
me just do the I'll do the split on a
just to see I won't do the split on be
so suppose you were to split on a and a
can either be true or false again now
again these are all simple examples
because there's only one once you say
I'm going to put on a thereto or false
but again a if it were numeric you'd
have to look at every possible split if
it had multiple categories you have to
look at every possible way of putting
the two categories into two groups in
this case if a is true i get 1 2 3 4
plusses and it is true I get one two
three negatives okay so that's a pretty
heterogeneous node if a is false I get
one two three negatives and nothing else
so that's a perfectly homogeneous node
oops i should say positive zero not yeah
positive zero okay so there we go that's
sort of the story for a and again how
are you going to do this well you're
going to actually maybe I'll just do it
here and in our to save time so let's
see I'm going to again take the weighted
average within each note so it's going
to be four out of the 10 observations
sorry seven where the 10 observations
are in that node so seven tenths times
the entropy and that node plus three
tenths times the entropy and the other
node 3 10 times the entropy in the other
node now the entropy in this note is 0
right the entropy for pure note is 0 so
that that's not there that's just 0 so
basically I need seven tens times the
entropy in this node the entropy in that
node is going to be let's see negative 4
out of 7 times log of 47 s divided by
log of 2 and then minus you don't want
to mess up the signs here three out of
seven times log log of three sevens
divided by log of 2 right so it's just P
times log of P oh sorry negative P times
laga p- p times log of P ok I think
that's right guys mess up so that says
it's point six eight nine seven point
six eight nine seven and so if i take
the difference here 10-7 is three 10-9
is one sixteen minus eight is eight
eight minus six is two point two eight
one three should be the info gain the
information gained when splitting on
attribute a let me check my notes and
also check the question is there really
an attribute called a yes there is so
when I split on it the information gain
is point two eight one three because i
went from my original entropy down to my
entropy point six eight nine seven so
the information gain is point two eight
one three first building on attribute a
for question 38 and that is correct and
if you're curious if you split on
attribute be the
mation gain would be 0 point 2 5 6 5
which is close but not as big so
actually attribute a is the better one
right because this is information gained
right so this is the one with the
smaller entropy and so the entropy when
you do be actually comes out vigorous
the information getting any a
subtractive smaller so attribute a in
this example gives me the bigger
information gained so c-45 for example
would choose to split on ashtabula over
attribute be in this example when it
starts drawing the tree okay great any
questions on that one okay so the last
thing to talk about with regard to
growing trees then at least in terms of
the criteria for splitting is what do
you do when you have something that's
numeric right so attribute a 3 for
example which is not only numeric but
allegedly continuous here right even
though why has one decimal and okay so
completely compute the information game
for every possible split so what you do
for this now I said a lot of algorithms
use midpoints and that's actually what
the the textbook authors had in mind for
this one was would be that you would
just use the midpoint so what you
actually have to do I mean you could do
something sort of you know an
intelligent sort of partitioning or
gradient search but arguably you know
every every partitioning is is
legitimate right that uses a midpoint so
if I were doing this exercise right
number 39 one thing I could do just sort
of for brute force is to go through and
write down the possible midpoints
because every midpoint is arguably a
candidate for being a split so I have a
three at one and then I have a three at
three point oh so two point O is a split
candidate and then one and I have a 3.0
and then there's a four point oh so I
ought to can entertain the possibility
of splitting on 3.5 so all these are
split candidates and then for each one I
could compute the info gain right which
would be the original entropy minus the
new entropy when you split on that one
well let's see what is the original
entropy for this one let's see I can
you that should be one two three four
out of nine so the original entropy
would be the entropy for four out of
nine before i do any splitting so let's
see what's the entropy like for four out
of nine it's trying to recycle some
stuff here entropy let's try this I can
do the entropy for four out of nine not
using this thing here and say it's going
to be four out of nine times log of 4 to
9 minus five out of nine times log of 5
out of 9 so the original entropy looks
like point nine nine 10 if I were that's
always the case right point nine nine 10
is the original entropy so for each for
all of these right I would take the
point nine nine 10 minus whatever so it
doesn't really matter it's not going to
affect which one you choose but
technically if someone says what's the
information gain it's the old entropy
minus the new entropy okay so the other
question is what's the new entropy if I
split on two point oh well if I split on
two point oh then I split off exactly
one guy right and so that one guy right
is going to be the one guy that's less
than two point O which has a value of
plus but doesn't really matter that's
going to be a pure node so it's going to
be one of the nine guys which has an
entropy of zero is going to be a pure
node and then the other eight out of the
nine guys I would figure out what is the
entropy for them and so you see for them
let's see so this guy is gone saw the
other eight there's one two three pluses
and five negatives so it would be
negative 38 times log base 2 of 38 minus
58 times log base 2 of 58 and you can
kind of see I mean you can imagine if
you were to make this table it actually
might sort of be instructive to you
because these numbers these fractions as
you go down are going to become more
balanced which is good right because
these criteria they want sort of two
nodes that are both big and pure right
so you're going to trade off you know
this note is pure but it's not very big
and it would be better if you could get
sort of get more on both sides but still
keep the purity so all
all these criteria are ways of trading
officer to the size of the nodes with
the purity within the node and for that
reason you can imagine why Gini index
and entropy might give you different
answers okay but in this case I have one
pure node but it's small so that's good
but bad and then sort of this node which
is big which is good but not that pure
and let's see if I were to actually
compute those things instead of just
talking about it then what would it be
so it would be 0 point 9 9 10 minus
let's see oh I can pry recycle something
let me recycle this so point 99.99 10
minus the entropy in that node times
eight nine so actually this should be
distribute the negative through right
okay point nine nine 10 minus the
entropy in that node which should be
taught wait eight nine times the entropy
in that node which is they give 38 times
log of three eighths oh sorry 38 times
log of 38 minus 58 times log of five
eighths that looks right i think i have
my signs right so Oh point one four to
six would be the information gained
their point 14 of 14 26 is the
information gained if you split a three
bigger than 2 or less than to let me
just exactly thing what is this number
39 ah yes Oh point one four to seven I
wrote down before so point 14 to
something is the information gained when
you split on a2 a3 plus or minus two and
then you can go through and again you
know do the information gain for all the
other split points and that will tell
you you know what the best split point
is and then if you were really growing
the tree you would compare that best
split point to the information gained
using either a one or a two so that's
sort of what the tree does it has to
look at every attribute and for every
attribute it has to look at every split
point but in order to save time of
course it just does this in this grief
question where it doesn't sort of you
know we look ahead or look back because
already there's sort of a lot of
computation to do just in this greedy
fashion okay so that's number 39 any
questions about that so if you sort of
want to see the solutions for all these
this is one of the homework problems so
they'll be up online but it's just a
case we're going through and computing
the information gained for every
possible split point okay here's a
picture you know just to show you that
they are different now they didn't scale
this picture so that's arguably you
should scale it you know because miss
classification and Jeannie max out at
point five and entropy maxes out at one
arguably you should scale it but if you
just look at Genia miss classification
error which are already scaled you see
sort of why they're different right
Jeannie sort of has a steeper derivative
near zero and a flat of derivative nero
point five whereas miss classification
error is linear so the extent to which
you value sort of purity for each one of
them is different I mean they're both
monotone obviously you know but this one
is steeper here in flatter here so you
can see why Gini index would favor
different splits than this
classification error would and that's
the point of showing this picture now
you might ask how in the world did you
draw this picture because it only
depends on P and I thought there was
like a p1 and p2 well of course p 2 is
simply 1 minus p1 and so if you look at
something like the Gini index which says
it's you know Gini index is like 1 minus
P 1 squared minus P 2 squared well you
know let's just call p 1 p and let's
just call p 2 1 minus p right because
for the two class problem everything
we've been doing has been general enough
that we can handle the you know k class
problem but for the two class problem
that's all Gini index really is and so
that's 1 minus P squared minus 1 plus 2
p minus p squared so it looks like I
lose my one but I have a minus 2p
squared + + 2 P so if i factor out the p
squared and reorder sorry I threw out
the P and reorder let's factor out the
two also and that's just going to be P
times 1 minus P right
p times 1 minus p so you see that right
that's right right negative 2 p squared
and positive 2 p yeah so you see that
this thing here if you're sort of a
stats person or i guess anyone can see
this that that's the binomial variance
right if you remember from your
elementary stat class you form a
confidence interval for the binomial
distribution the variance for binomial
is always p times 1 minus p so that's
really what Jeany entropy is so it's
probably not a big surprise that
statisticians when they're growing a
tree minimize this type of thing because
it's simply simply a generalization of
the binomial binomial variance okay
that's all I want to say any questions
comments about this picture question yes
yes I think there's an example in the
maybe there's an example of
classification error but yeah Jeanne and
directs and entropy can get lead to
different splits yeah so one of them
values purity relative to size more than
the other and if you if you normalize
this to be on the same scale you could
see that they're not that one of them
has more steepness on the other I mean
the question is like you know legitimate
people or if the reasonable people can
disagree if I say okay I give you two
nodes of this size and both of them have
this much purity versus I give you two
nodes of this size and they have this
much purity the question is sort of you
know is splitting off three points with
perfect purity you know when three is a
very small number better than splitting
off a lot of points with not so perfect
purity depends how you measure purity
and if you measure purity differently
than one might favor one over the other
so you can get different answers
depending on which you use and it's not
too hard to construct them someone
someone had a comment in the back case
for entropy and bearing the genie or
vice versa it's hard to say sort of
absolute terms right i mean you can sort
of motivate both of them there's some i
can give you statistical arguments for
using Gini index i can give you probably
entropy you can sort of argue it from a
maximum likelihood optimality point of
view you can give some a family
arguments for either one but sort of in
practice you know some people prefer one
over the other it just happens i think
that the Gini index was popular in Stan
and entropy got popular in the cs
literature which led to see four or five
but i don't have the best historical
perspective on that but there's sort of
two competing measures and both of them
have strong motivations but it's you
know we would not say one is absolutely
better than the other you can you can
drive optimality properties for for both
depending on how you started okay any
other comments questions on geni entropy
trade-off okay in general though very
few people use miss classification error
when they're actually fitting these
these models they only use it in the end
when they want to see how good it did
how well it did okay so the other
question from this chapter of course is
when to stop splitting this is a tricky
question you know there's if there's no
answer on what measure to use there's
really no answer on this because the
idea of like how how big should you grow
the tree that's really a question of
model selection and it really says you
know you don't want to over fit you
don't want to under fit and you know
that's a hard thing to do because you
never really know whether you're over
fitting or underfitting you know it best
you can leave out some data for testing
but that really depends on what now do
you leave out for testing and it's hard
to really you know compared to trees of
similar size one thing you could do in
terms of answering this question of how
to grow the tree would be look at your
miss classification error or even if you
want your Gini index or your entropy on
the test data and stop when that begins
to increase right so start growing the
tree look at your miss classification
error on the test data it's going to go
down down down but eventually it'll
start going up up up so right where it
starts to go up that's it you should
stop growing the tree because you're
overfitting that's one thing people do
another thing people do often in
conjunction with that is what's known as
pruning which is it also gets back to
the fact that you're growing the trees
sub optimally so you really want to sort
of look ahead one step or two steps so
pruning actually in some way looks ahead
all the way pruning the idea with
pruning is it just basically it grows
the whole tree right so your book calls
it post printing a large tree is fit
grow it all the way down grow it as
large as you want according to some
criterion maybe Gini index or something
like that and then trim it back maybe
according to a different criterion maybe
even this time measuring the air on the
test side as opposed the training set so
if you look at the documentation for our
part it actually uses pruning like this
it actually grows a large tree and then
starts to do some intelligent ways of
growing it
back when it grows the large tree it
just minimizes Gini index on the on the
training data but when it trims it back
it tries to do some more intelligent
things with cross validation and things
like that so you can read the
documentation on that of course our part
generally is trying to follow the cart
algorithm as outlined in the famous text
called classification and regression
trees from 84 and so when I say these
algorithms are old you know that's what
i mean by old 1984 but again they're
still used somewhat they have some nice
features they do nice variable selection
and they're the basis for many other
modern algorithms such as begging and
people often use these trees for
regression or for boosting okay so
that's what i want to say about pruning
I think that's it yeah question right
right right right yes so you can I mean
a lot of problems here right if I say
like I'm gonna look at the test data
every time and use that as my criterion
then argue that I'm overfitting the test
data to right which is one problem yeah
I'm using it to decide how big the trees
so I've already seen it many times and
now I'm cheating again right and and so
you know it gets euro eur / fitting your
test data as well as your training data
right so you know that's why this is
sort of a subtle business and to really
know exactly the perfect tree that's
going to do the best on the new data
it's really hard to know and it's
question is how efficiently can i use my
training guide on my test data i don't
want to use the same data more than once
but then if I you know split off if I'm
going to consider ten trees and I split
off ten test sets and all those test
sets are going to be small and small
isn't good either and it's just there's
no right answer to how to do this people
have their own favorite ways you know
10-fold cross-validation is very popular
things like that but there's no sort of
right answer to do this and you know in
the end of the day when people have
competitions for for you know machine
learning problems and they're trying to
do a classification test you don't know
if you're going to win the contest right
before before it's over so there's no
sort of right answer this but different
people have different things they like
to do the one thing you wouldn't want to
do is just always believe your training
data because you're always going to
overfit your training that you can
always kind of work fit your training
data so you can also refer your test
data comment
so pre pruning is just like stopping
early i don't know the book
differentiates pre pruning and post
pruning and says pre pruning would be
like stop growing at a certain point
which to me isn't really pruning pruning
is when you pruning is post pruning to
me I don't know why the authors write
that but they do and you can see on page
185 they make that distinction ok so I
think it's all I wanted to say about
this chapter although one thing that
sort of goes along with this chapter
which I don't know why they put it in
chapter 5 is to talk about sort of this
class and balance problem so i'm just
going to spend the last 15 minutes
you're talking about this which are you
Lee I think should be in chapter 4 so
we'll just do it here so so far we've
talked about should the two classes
being equal right and by that I mean
we've always sort of assumed the same
type of loss for both types of errors
right if it's a rock and I say it's a
piece of metal or if it's a piece of
metal I say it's a rock I lose a dollar
either way right if a patient is going
to die tomorrow and I tell them they're
okay it's the same cost as if they're
okay and I tell them they're going to
die tomorrow and you know that's not
true right in that case um I've always
used fifty percent as the cutoff right
when I was looking at the probabilities
and I said method equal class its
threshold 0.5 I always pick the majority
of class in the terminal node okay so
this is reasonable if the following
three things are true right first of all
you have to have the same cost for both
types of error you're only interested in
the point 5 cut off and the ratio of the
two classes in your training data will
match that in the population for the
most part meaning that you don't start
off with a population that's 30 70 and
someone says here I'm going to give you
a hundred of both well if you know the
populations 30 70 but the way they
sample the data was to give you a
hundred of both then you know that you
sort of have to rebalance thing if you
want to rebalance the things if you want
to do the inference on that population
so you sort of need to have all these
things true and in many cases they
aren't and the most striking cases when
you're sort of predicting a class that's
very rare right you're trying to do you
know find the terrorists in the whole
database or find you know the few
documents that match this query and the
whole corpus of documents out there so
you know a lot of times classification
is not symmetric at all and it's very
imbalanced and in that case you know
miss classification error isn't always
what you want to do you don't always
want to train the classifier to have
equal equal costs for both types of
errors so again if anyone
use three condition is not true you
might want to sort of turn up or turn
down the number of observations been
classified as positive so you should
think of every classifier as having a
knob and you can either turn it up or
turn it down to get more or less of the
positive class okay this can be done in
a number of ways depending on the
classifier now some methods give you a
probability estimate right logistic
regression spits out a probability
estimate cart in our part they tell you
how many observations are in that
terminal node so you have a local
probability estimate some don't give you
that right but some give you some
continuous threshold right like for
adaboost right sometimes people try and
use that that value that it's computing
that continuous value that's on negative
infinity to infinity and they sort of
threshold on that you know a lot of
things don't really give you a
probability but they give you some sort
of ranking of the confidence of you know
this is the most likely to be the
positive class this is the second most
likely and so you can sort of move your
threshold along there you can sort of
turn the knob up and down depending on
how many positives you want to get over
under sampling kind of works right if
the class of our has no way to turn the
knob at all you can always trick it and
you can just you know give it you know
repeat the positives twice as many time
or or hide some of the negatives from it
or things like that that's over
understanding you can always sort of
trick the classifier into being into
seeing more more or less of the positive
class by controlling how you do the
sampling ok so when dealing with class
and balance you hear a lot of people
they don't really care about you know
the accuracy or the Miss classification
error by itself they would rather look
separately at recall and precision and
of course this comes from the
information retrieval and I'm sure
everyone in this room has probably heard
these terms at some point before recall
is simply you look at all the ones that
you that are actually of the class
you're interested in and you figure out
how many of those you got right so
there's all these documents out here
that are relevant to that query how many
of them was actually able to pull up
okay that's where this comes from in the
information retrieval case precision is
you look at all the documents that you
pulled up all the ones that you said
we're yes and you see how many of them
you actually should have pulled up right
so all the things you got how many of
them are actually right right so two
different questions out of all the
things that are out there that you
should have got how many did you get how
many did you recall versus all the
things that you said you thought were
the
yes class how many of them actually are
okay so of course you can make one of
these you know one would by sacrificing
another right I can say Oh everyone's a
terrorist one hundred percent reek al
but low precision or I can say you know
right you can you can sort of do that
that's trivial right so this is sort of
recall and precision before we just look
to accuracy now some people do like to
compute recall and precision but then
combine them into a single number and
that single number is called F your book
calls it f1 because there's sort of a
weighting parameter and one is the case
where you've set them both equal F is
simply the harmonic mean of recall and
precision right to / 1 / recall plus 1
or a precision and it turns out actually
you know the Wikipedia article on
information retrieval is pretty good or
else i should say i know so little about
information people that i think that
it's a good article so maybe you can
find fault with it but i thought it
looked like a good article and it talks
about recall and precision even talks
about the f measure and so that's talked
about on page 297 the ROC curve is the
thing I wanted to talk about
specifically though this is stands for
receiver operating characteristic
there's some history about why I has
such a funny name because you can turn
up or turn down any classifier in terms
of the positives or negatives it's
really unfair to diss compare two
classifiers at one point on that dial
right if I say let me just compare the
two classifiers at the point 5 you know
at the 50-percent point well one might
be do better than the other but what
happens then if you pick that classifier
as you turn it up it might turn out that
the other classifier would have done
better when you're trying to get a lot
more positives out so you can sort of
compare two classifiers simultaneously
as you turn up and turn down the knobs
by looking at the ROC curve which is a
plot on the y axis of the total positive
rate and the x axis has the false
positive rate so sorry I said total true
positive rate on the y axis and false
positive rate on the x axis and you say
well how does a single classifier to
give me multiple values for the true
positive rate in the false positive rate
well you turn up and you turn down the
dial right as I get more positives as I
get more positives my true positive rate
is going to go up and so is my false
positive rate as I turn it down they're
both going to go down so what you want
of course is this
to be big in this to be small there's
going to be a trade-off they don't vary
independently and so you want to take
the classifier that you know sort of
consistently does better with respect to
both of these as you turn up and turn
down the knob especially if you're going
to be using that classifier at different
thresholds right if you just want one
classifier that works at the point five
threshold or the point seven threshold
okay you just look at miss
classification air and compare that way
but if you want to classify that works
sort of as you turn up and turn down the
knob and you want to compare them the
ROC curve is useful for looking at that
so again true positive rate on the
y-axis false positive on the x-axis so
therefore the diagonal is sort of as bad
as you can do right the diagonal would
be like a random guess if I say let me
call everything you know I'm going to
flip a coin right or thirty percent of
the things I'm going to call positive
well then you're going to have a thirty
percent true positive rate and a
30-percent you know false possible right
because everything is going to be thirty
percent positive right so you look back
at these right everything is going to be
thirty percent positive so both of these
would be thirty percent so you know
anything like that the diagonal is just
simply random guessing good classifier
is going to be on the upper left right
why is that because true positive rate
on the y axis false positive rate on the
x axis you want this to be low you want
this to be high so you want to be like
up here diagonal is random guessing okay
so everything should be above the
diagonal Trillo that you should just
flip a coin a good classifier lies near
the upper left these are useful for
comparing two classifiers the better of
classifier will be on top more often
right sometimes someone will say this
class Prada completely dominates the
other one on the ROC curve meaning that
it's always above it but you know if
someone just computes misclassification
error and so you could just do better at
one point so you like to see if one
really dominates the others if they
cross then it's sort of it's sort of
questionable two classifiers that clock
cross one way you could use this to
compare them is to compute the area
under the curve right the diagonal has
area point five you know perfect
classifier would have area one so you
could compare the area you know under
the curve for two different classifiers
and you know maybe they cross but maybe
one of them has a bigger area than the
other see if it come with a bigger area
so those two classifiers would both be
really good right but to choose between
them since they cross I might look at
the
under the curve okay so I wanted to sort
of get to this example I only have seven
minutes but at least maybe I can start
it so this is one of the problems from
the textbook basically they're asking
you to compare two classifiers and these
two classifiers m1 and m2 happen to spit
out probability estimates right this
point seven three means that classifier
m1 thinks there's a seventy-three
percent probability that observation one
is really from the positive class and I
actually sort of put busy and spin on
that one but you understand these to be
probability estimates the point is
though it actually doesn't matter all
you really need is numbers that you can
order these things by so the point seven
three doesn't matter so much is that
that's the largest number so this
observation if that classifier had to
pick just one thing if you turn down the
positive all the way to the point we're
only getting one positive this is the
guy that it's going to stay as positive
okay so whereas over here you know point
six eight is the highest the other
classifier m2 would have picked the
third observation as being the thing
that it thinks most likely is positive
and actually what have you been gotten
it wrong so might be a bad class bar so
what they're asking you to do here is to
make the ROC curve so what that means is
that it every cut off you should go
through and compute the false positive
and false negative rate so the first
thing you just have to do is simply
order these things so this is the
instance and again let me just do it for
classifier m1 okay so I'll do em one
let's see false positive rate and m1
true positive rate okay so if anything
is pot so the first one is one right
point seven three okay so instance one
gets classified as positive so now my
false positive rate well I haven't said
anything so false positive rate is zero
out of five right because out of the 55
negatives I haven't called any of them
positive yet but the true positive rate
I got one of them right right one of the
five positives I got it right so my true
positive rate is one out of five the
next guy let's see who's the next guy
would be observation to write because he
has 0 point six nine second-highest so
it's a V instant to
his false positive rate still 0 out of 5
true positive rate is now two out of
five and you can see that as you plot
these again false positive rate true
positive rate let's see then we go point
two point four point six point eight one
point 0 0 point two point four point six
point eight one point 0 right
everything's out of 5 so now i'm up to
the point 0.4 with this guy the next
instance would be instance five right
five has a point six seven that's the
third highest and again i got that one
right to that's also positive so now i'm
still a 0 false positive rate but I mean
three out of five on the true positive
i'm now up here so so far my roc curve
is going straight up which is perfect
however when i get to the next one point
five five that's a negative i get it
wrong that's a negative i get it wrong
so that's case for so now my false
positive rate is one out of five my true
positive rate is still three out of five
so now i start to move horizontally
which is bad right ok the next case
would be number six which is a point for
7.47 for number six okay I get that one
right so my false positive rate stays at
one out of five my true positive rate
state now goes up to four to five so I
get to go up again here the next guy
would be observation nine that's also
positive I get him right because he has
probably point four five so now my false
positive rate stays at one out of five
true positive rate is now at five out of
five right so now I'm all the way to the
top and then you can imagine after that
what's going to happen you're going to
stay at a hundred percent but as you
calm one more thing as positive all
that's going to happen is that your
false positive rate is going to keep
going up so that's the ROC curve for m1
and the exercise asked you to make the
same thing for them to you can see that
one actually does pretty well because it
shoots straight up goes over a little
bit but then keeps shooting up so what
you want is like you want this thing to
get to one really fast you want this to
get one really slowly they're both
obviously starting at zero if you don't
call anything a positive they're both
zero but what you want is this one to
grow faster than this one because you
want to get up fast and over slowly and
so this classifier actually does
well and you can compare em to on the
same thing and so it's measuring as you
turn up the positives right as you turn
up the positives how well are you doing
you want to be high and you want to be
to the left and in particularly want to
be above the diagonal so that's that
exercise and then the last one here says
suppose I choose a cutoff of 25 so where
was point five point five would be after
observation number four right because my
poor guy was 0 point five five but my
six guy was 0 point four seven so
suppose I choose the cutoff here okay
and this was uh i think it was plus plus
plus negative right so i suppose i
choose it there then it asked me to
compute simply the give the precision
recall in a pleasure okay so precision
precision recall an F measure so
precision well I've called for things
positive right because I'm calling all
these things positive three out of four
of them are actually right so my
precision would be three out of four
point seven five right out of all the
things I called positive well if i call
it everything about point five positive
that's these four three out of four of
them are right recall well you know that
in the data there's actually five
positives out there right one two three
four five how many of them did you get
you got three of them so your recall is
3 out of 5 or sixty percent now of
course that's just a precision and
recall 4141 threshold you could compute
it for all the thresholds or you can
look at the ROC curve for all the
thresholds and the final thing it asks
for is the f measure f-measure is just
the harmonic mean of these two guys so
that would be 2 times 0 point 7 5 times
0 point 6 / 0 point 7 5 plus point 6 and
your f measure comes out to be 0 point 6
67 so if you had to report sort of you
know one number ahead report your
atheleisure you would be point four
point six six seven but again that's
just for a single threshold you can get
different recall and precision and neff
measures depending on you as you change
the threshold the ROC curve is nice
because it sort of evaluates the
classifiers over all possible thresholds
ok that's it any questions before we
take off
okay i'll see you on Tuesday</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>