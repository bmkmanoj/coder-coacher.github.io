<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Brains, Sex, and Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Brains, Sex, and Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Brains, Sex, and Machine Learning</b></h2><h5 class="post__date">2012-08-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DleXA5ADG78" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome I'm really honored to have to be
able to introduce our speaker today
Geoff Hinton is from the University of
Toronto he graduated from Cambridge in
1970 with a Bachelor of Arts in
Experimental Psychology and from
Edinburgh in 1978 with a PhD in
artificial intelligence we barbarians
south of the Canadian border don't take
much stock in pedigree but as a computer
scientist I was I couldn't help but
notice that Jeff is the great great
grandson of the logician George Boole
very cool among his many achievements
Jeff co-invented Boltzmann machines
along with Terry Sejnowski and
introduced backpropagation for training
multi-layer neural networks with Paul
where Bush David Rummel heart and Ron
Williams he's the founding director of
the Gatsby computational neuroscience
unit at University College London which
is one of the premier places to to study
computational neuroscience in the world
his students comprise a stellar list of
computer scientists in machine learning
and computer vision
they include Ric's Leske Richard gentle
Radford Neal Karl Rasmussen Brendan fray
and eyt J all research scientists well
known by many of us he was elected to
the to the Royal Society in 1998
received the itch Chi research
excellence lifetime achievement award in
2005 and despite these numerous awards
and popularity as an invited speaker and
thesis advisor he continues to be
amazingly productive and inventive if
you haven't heard about deep belief
networks contrastive divergence products
of experts the wake-sleep algorithm
semantic hashing or any of a host of
other inventions of Jeff you probably
don't know what ICML or nip stands for
either
back in January yawn Laocoon one of the
many computer scientists that Jeff has
influenced started a machine learning
related mean by writing geoff hinton fax
this is an analogue of Jeff Norris fax
which I didn't know about before I found
this out but so here's a couple of them
geoff hinton doesn't need to make hidden
units they hide by themselves when he
approaches geoff hinton once built a
neural network that beat Chuck Norris on
em nest after an encounter with Geoff
Hinton support vectors become unhinged
and for today's introduction I created a
mashup from two of yawns facts it goes
never interrupt one of Geoff Hinton's
talks he will make you regret without
bounds that's really unfair because just
one of these guys I know doesn't have a
mean field in his body and you'll have
ample ample opportunity to test this
because he's gonna be around with us for
most of the summer so without further
ado yeah
so today I'm going to talk about a new
idea and machine learning and it also
has implications for a couple of major
problems in biology so the main theme of
the talk is going to be this new idea
machine learning but it was partly
inspired by some work on the theory of
evolution I worked on it a long time ago
and gave up on it because they didn't do
the experiments properly and then
Papadimitriou came to Toronto and gave a
talk and I thought that's the same idea
and so I went back into the experiments
properly and it really works nicely so
to explain something in the theory of
evolution it also explains something
about how neurons communicate so in the
theory of evolution there's a problem
with sexual reproduction and the problem
is that it would be nice and fit you
need your genes to be working well
together so you find a nice set of genes
as are well Co adapted and then you mate
and half the genes disappear this is a
simple model of what goes on hatha jeans
disappear and so now you don't have your
big set of code Apogee genes anymore so
you don't work so well and that is a
real puzzle why doesn't why isn't that a
bad thing
so the paper by living a type of de
metrio and Feldman in 2008 claims that
the whole point of sexual reproduction
is to break up these code applications
and there's two reasons why that might
be good idea one is for long term
optimization so in the short term maybe
it's bad but he gets you out of
situations in which you have to change
lots of things all at the same time to
make any progress so might have long
term optimization you don't have much
evidence of that from the machine
learning perspective yet the other thing
is it might make you more robust against
changes in the environment so if the
environment changes then if you have a
big complex card app tation that
produces an effect that you want by
depending on lots of different genes
it's liable something you'll get messed
up but if instead of that you have lots
of smaller card applications that are
achieve the function in many different
ways using a few genes working together
then when you mess with the environment
some of those ways are more likely to
survive and it turns out that's a big
effect
the other biological issue is how
neurons communicate so cortical neurons
do a lot of signal processing and
they're very good at it and yet they
don't send real numbers to each other
as far as we can tell they send these
spikes of activity that are 1 or 0
and the timing of the spikes is random
now this is completely crazy
because for the same amount of energy
they could send a spike with precise
timing relative to some oscillation say
and so they could communicate an analog
value by the time of the spike and the
question is why don't they do that um ok
am I going to try and answer both those
questions when I told you about this
idea machine learning um one reason why
we sort of need to understand why
neurons can communicate with single
spikes like this is because we would
like to make really big neural nets that
implemented on many cores and so the
states of neurons some neurons are going
to be a1 corn some euros are nautical
they you need to communicate the states
in neurons between cores and so you'd
like that to be low bandwidth and if
they only really need one bit to
communicate their state that's a win by
a factor of 32 over what people are
doing at present so on the face of it it
seems very hard to believe that one bit
could be a better thing to send than
analog value I think that's because as
engineers we're used to sort of doing
signal processing you do things like you
you fit some data by assuming there's a
linear dynamical system and you fit a
linear dynamical model and to do that
you want to use real numbers that's not
the kind of problem the brain solving
the engineer cells form where they come
to the problem with a good idea what
model they want to fit and now they want
to sort of identify the parameters the
brain comes to this completely
incomprehensible world with huge amounts
of input data it's a real sort of
confusing mess and the right thing to do
in that situation is to fit a gazillion
different models you don't want to fit
one specific model really nicely you
want to fit a gazillion different models
and then use the wisdom of crowds but it
when someone asks you a question you get
all the
gazillion different models to give you
an answer and sort of take a consensus
and that's a very different style of
computation from fitting one precise
model carefully so I'll come back to
that issue now I'm going to jump back a
long time back in the mid-80s people
developed neural networks that have
multiple laser feature detectors and we
had a learning algorithm called back
propagation that was reasonable at
learning multiple laser feature
detectors so you put some input in run
it through the net compare that with
what you wanted then go backwards
through the net using the chain rule to
figure out how to change the weights on
the incoming connections of each neuron
it was very exciting to begin with it
never worked very well for deep nets
except for Jana Kahn's deep nets which
used convolution but for the other ones
that never worked very well one problem
with it was back then it was very hard
to get very big sets of labeled data you
could get images or you get sine waves
but getting accurate labels for them was
really tough so we couldn't get enough
label data one solution of course it
just worked very hard getting label data
and people in speech did that they got
big label datasets another solution is
to say let's see if we can learn most of
the layers of feature detectors maybe
all but the last layer by trying to
model the input data by trying to build
a generative model of the sensory input
rather than by trying to decide what the
right labels where it is and it turned
out that approach worked quite well so
since 1985 a number of things have
happened computers got a lot faster
that's the main thing that happened
label data sets got a lot bigger so now
you can train big back propagation nets
we also found better ways to initialize
the weights of these multi-layer nets
by using unsupervised learning and the
combination of all three factors means
that we can now initialize Nets sensibly
we can then turn them loose on a big
data set we initialize use the unlabeled
data turn them loose on a big data set
and train the hell out of them with a
big fast computer or a whole bunch of
big fast computers and they do very well
so here's some propaganda probably the
most impressive application so far in
speech recognition some time ago some of
my students and me um but mainly my
students showed that on us relatively
small data set a three hours of speech
that was a standard benchmark alt image
we could beat the existing speaker
independent benchmarks by using a deep
neural net to map from a window of
acoustic frames to predictions about
what pieces of what phoneme were the
middle frame was representing so in
speech use hidden Markov models to deal
with temporal alignment for each phoneme
you have a hidden Markov model that has
a few states to be three and sometimes
four one phone you have many different
hidden Markov models depending on what
phonemes you think of as I did but the
problem is to get from the acoustic
input to a prediction about how likely
it is that is this piece of this phoneme
and that's what the deep neural nets can
do much better if you then feed those
predictions to a decoder which uses all
sorts of knowledge about language and
stuff it gives you word strings that are
more accurate than if you use the
previous standard kind of acoustic model
so she has some results from a recent
review paper which is notable because
the paper has authors from MSR research
IBM and Google as well as University
Toronto and this is on much bigger data
sets so MSI used quite a big data set of
three hundred and nine hours and they
got the error rate on that data set down
from 27% using the standard approach
before these deep neural nets were
applied to 18% that's a huge improvement
and then on a different test set they
got it down for 23% 16% IBM who probably
had the best speech recognizing the most
carefully tuned one on a somewhat
smaller data set got their aerator there
very carefully tuned system down from
18.8% 17.5% and this was a very
carefully to insist on being voice
search they got it down by a lot
um Google used what Google thinks of as
a small data set they got the error rate
down to twelve point three they got it
down to twelve point three from 16%
which was trained on what they think it
was a big data set um I don't know how
big that is but it was much bigger so
you give it much more day to get sixteen
percent you give it less data and you
give this better this deep neural net
and it gets 12 percent and this is now
down below twelve percent and most of
this progress was made by what one
summer intern with a lot of help from
the speech group of course and that's
one of the best examples of deep neural
nets doing something useful MSR is
already armed announced that they're
going to put their system deep neural
net live okay so session is is anything
we can't do with big deep neural
networks
why don't they the answer to everything
that's what we first thought when we did
back prop and begin to think it again
well here's one thing it's tricky to do
to train a big deep neural network you
get a whole bunch of cores and you split
it over cause and you and run for a long
time and after you've used sort of
thousands of cores for several weeks
you've trained your great big network we
say 1.7 billion parameters and then
someone says hey why don't you train 500
of those guys and average them well that
seems like hard work but we know that
averaging a whole bunch of models is
always a big win so if you want to win a
machine learning competition like
Netflix what you should do is take lots
of different models throw away the ones
that are no use and keep the ones that
work reasonably well especially ones
that are different from each other and
then average them and so the winners
were averaging more than 100 models and
we really like sure that with big deep
neural networks that would presumably
make them work better the more evidence
for that is you can take a rather wimpy
machine learning method called decision
trees and just one decision tree by
itself doesn't work that well but if you
take a whole bunch of
then they work really well that's called
random forests and I believe they
connect the thing that gets you how
you're dancing about from the 3d data
that's using random forests and the
point about decision trees is they're
fast to train and they're very fast to
test time so test time you can afford
drivers you're whole bunch together and
also you can afford to train a whole
bunch and we would really like to do the
same with these big deep neural networks
well on the face of it that's going to
be tricky so before I say how we're
going to do that let me just tell you
two ways of averaging models the sort of
most standard way is if each model
produces a probability distribution
across classes it's three classes here
what you do is simply average those
probabilities we shall give you a
distribution that's softer than what the
models predict and is guaranteed to be
is a better bet to use this average
distribution than to pick one of the
models at random of course the best
model might be better than your average
but with when you've got a lot of the
models activity won't be the other way
we can combine distributions is to take
a geometric mean instead of arithmetic
mean we just multiply together the
probabilities that the models give to
the different classes and then we take
these sort of ends route if we have n
models then we get a bunch of numbers
that don't add up to 1 so we have to
renormalize but that also has the nice
property that if you use this geometric
mean that will give you better
predictions than picking a model a
random okay so here's the main idea of
the talk what we're going to do is we're
going to take a slightly oversized
neural net and I'll begin with a neural
net that just has one hidden layer and
each time I present some training data
to it I'm going to randomly emit each of
the hidden units with a probability 0.5
so you just pretend they're not there so
now we have a space of 2 to the N
possible architectures if there's n
hidden units so H to the H architectures
is 18 units and we're sampling
architectures from that space so we're
going to sample a whole bunch of
different architects
and each architecture is only ever going
to see one training example and it's
only going to see it once because the
chance of sampling the same thing twice
is negligible and almost all the
architectures will never be sampled but
all of these architectures are sharing
the same weights which is regularizing
them a lot and so the question is sort
of how well will this work also the
question is okay you do that at training
time so when you're training for each
training example you sample an
architecture and then update the weights
of the hidden units you're actually
using
slightly then go to the next example but
would you do a test time because how do
you average all these things a test time
and it turns out if you're willing to
take a geometric mean that's very easy
we can think of having these this very
large number of possible models we've
only trained a small fraction of them
but still a large number as many as the
number of presentations of the number of
training examples um so you think of
that as very extreme bagging where
you're making your models different by
giving them different training data and
strongly regulan and then at test time
what we can do is because each hidden
unit is there with a probability of 1/2
during training we make it be there all
the time during tests but we have its
outgoing weights so the expected
contribution you get from it's the same
and now if you do that then you have our
soft max output group that's computing a
probability distribution across classes
you can show that that exactly computes
the geometric mean of the predictions of
all these two to the H different
networks so that's a nice property it
means we just run this one mean Network
that's twice as big as any of the
networks we run during training and so
for that factor of two we can get we can
average all these networks here the
including all the ones we didn't train
we're never trained right on the other
hand they're sharing weights with the
ones that were trained so even the ones
that weren't trained you're going to do
sensible things because it is massive
weight Cherie
so it's a funny kind of model averaging
where we combine model averaging with
massive weight sharing if we have more
hidden layers we just use dropout and
all of the hidden layers and we always
use nearly always use 50% dropout
because that's not a free parameter
anymore
I mean 1/2 is sort of it's not really a
fudge we just use 1/2 we've of course
experiment should be using different
numbers and any number between like
point three and point seven has very
similar behavior it's probably the case
that if you have a very big net if you
can afford that using much more droplet
is better um that is keeping a smaller
fraction on each case if you have
multiple hidden layers and you use 50%
dropout in each layer then when you use
the mean neckla test time that's not the
same as running all possible Nets
through an averaging them taking the
geometric mean but it's a pretty good
approximation to that so you can do
experiments where you run it
stochastically a lot of times and
average them and you'll find you get
something very similar to running the
mean field net once you can do the same
thing in the input layer and that also
helps a lot that's already being used by
people in yoshua bengio group and
elsewhere where you simply omit some of
the inputs so if it's an image you just
set some of the pixels to zero and that
acts as a good regularizer that of
course hurts you during training but
makes you generalize better it's a good
kind of noise to add and this is just a
generalization of that to all of the
layers in the input layer you probably
don't want to leave out half the pixels
you want to leave out 20% of the
ensemble there is actually a familiar
example of dropout or particular kind of
dropout for people who know about
logistic regression if you're doing
logistic regression and you don't have
enough data to fit your model really
well so your Sprinter overfitting all
you can do is you can drop out all but
one of the inputs so now you're going to
do logistic regression on just this
model here and learn this weight and you
do that on a whole bunch of data and
then you drop out then you focus on one
run another weight
if you do logistic regression like that
that's called naive Bayes a test time
you use all of these things together if
you want a distribution that you really
ought to take the geometric mean of what
they all predict but if you're just
interested in the most likely class you
don't need to do that
and so naive Bayes is actually an
example of dropout where you're using
dropout to avoid overfitting of course
that immediately suggests that um why
only consider the possibilities where
you use them all or you just use one of
them why not consider sort of learning
dropout rates for these guys using a
validation set so that you're doing sort
of subset selection but probabilistic
subset selections and you're not going
to say I'm going to go for one
particular subset of the features and
try and find the best subset which is
what statisticians promotion that I'm
doing you say I'm going to have
gazillions of subsets produced by giving
each of these a probability that's not
point five anymore so that sort of
useless features will have a low
probability being included youthful ones
without have a much higher probability
and now you can average all those models
together and you'll get the big win out
of model averaging and that hasn't been
as far as I know that's hardly been
investigated that kind of idea so in a
deep neural network our experience is
that if you take any deep neural network
that's showing overfitting so for
example in speech any neural network
that you stopped early because if you
don't stop it early it over fits you can
make it work quite a lot better by using
dropout it works better than early
stopping and it makes it cause a big
decrease in the number of errors um
people at Google say yeah but I don't
have overfitting because they've got so
much data and the answer to that is well
you should have overfitting because you
may have a lot of data but that just
means you should use a much bigger
neural net I mean you can take the limit
of infinite data but I can take the
limit of infinite computation okay so
here's some initial experiments on a
boring old task called M NIST which is
the kind of Drosophila of machine
learning and people say you shouldn't
use em nice because it's boring and
that's like telling a biologist you
shouldn't use Drosophila so many
biologists use Drosophila why are you
using Drosophila
when you're using for something like as
so many other bandages use Drosophila
you can compare things um a lot's known
about it so it's recognizing handwritten
digits and if you look on yeah lakhan's
website the best reported result for a
kind of vanilla neural net is 160 errors
on the test set you can do much better
than that by putting some knowledge in
you can put knowledge in either by
making the net convolutional or by
transforming the data using the fact
that if you shift something it stays the
same class so you can put knowledge in
revising with the data and you can get
down to about sort of 25 errors now by
doing all those tricks but if you don't
if you don't do that if you do a form of
learning that would still work if
someone randomly permute to the pixels
then the best reported result 160 errors
um
that's this line here this 160 arrows if
you try various different neural nets
deep nets um so here's a net the blue
one with 800 units in each layer that's
more than you normally use for M list
and two hidden layers and you train it
with a big learning rate but with a
constraint on the weight so they don't
blow up that already helps you do better
than hundreds Akira's by the way then
here's a whole bunch of different LEDs
and here's what happens if you train
them with this big learning rate and
then you decay the learning rate so you
see they don't over fit um and they all
do about the same if you make the Nets
bigger and give them more layers um so
this one has 1,200 units per layer three
layers so that's much too big for normal
kind of training that would over fit
horrendously um that one actually does
really well that gets down to just over
100 errors which is really good
performance on amnesty for a method that
isn't being given extra information so
support vector machines get about 140
errors they're back here on that list
and it was this gap between back proper
support vector machines that originally
made people sort of switch to support
vector machines
but now we've sort of reversed it okay
but M NIST is a sort of boring task to
make this work we do use weight
constraints and if you're using l2
penalties on weights to keep them small
or l1 penalties to keep them small um it
tends to work better to say take each
hidden unit and decide on the maximum
length for the incoming weight vector
using a validation set and then during
learning if the weight vectors smaller
than that leave it alone as soon as it
hits that level when it goes beyond that
then shrink it by division so it comes
back to that maximum length um that
works quite a bit better than using l2
decay in the things we've tried and
that's what we're using here it's just a
hard constraint but I can give you one
one way of thinking about it when you do
that division to normalize it back to
the length you'd stay at that length if
you had Lagrange multipliers that we
just write when you were doing your
optimization and those Lagrange
multipliers are like the weight costs
the penalties on the weights but these
are penalties that change so what'll
happen is as the weights grow when
they're small there's no penalty
once they hit the constraint then if
some of the weights want to grow a lot
and the others weights don't want to
grow March you have to put in big
Lagrange multipliers and that'll
actually push the ones that want to grow
back down to a zero so in effect the l2
penalty on each connection adapts to how
much the other connections want to
change yeah yeah okay so here's
experiments on Timmy which was um more
interesting you you train the neural
network on a window of acoustic frames
you're trying to predict which states
which hmms the central frame corresponds
to um withstand a fine-tuning
um you get 22.7% after doing after doing
this pretreat unsupervised pre training
and that's already a very good result we
can do better than that but what we did
was we went to a different way of pre
processing the data so we hadn't already
over fitted it a lot we're using dempo
Viscardi system for this and that was
sort of the first time we used it then
when you take this net and apply dropout
you get down to nineteen point seven
which is a record for speaker
independent methods that's a big drop so
here's that's the phone error rate you
can also look at as you're training you
can look at how well you're classifying
the individual frames which is called
the classification rate and the
classification error rate if you just do
normal fine-tuning where you're not
doing dropout you see it comes down and
then it over fits so early stopping if
you could stop at just the right point
we'll get you one of these points for
these different nets we've drop out it
comes down it just keeps going down if
you look at the cross entropy area the
classification rate doesn't go up the
cross entropy error goes up very
slightly but basically it's hardly
overfitting at all and all these
different architectures do very similar
and all much better than what you will
get with ODIs time so if you're using
only stopping stop um just to show it
was Universal we thought we'd do
document classification um it didn't
work so well for that but it worked
so we took a FET what we think of as a
big document set of the order of a
million documents which are represented
by the counts of the 2000 most frequent
words there's our sort of little
hierarchy of classes we simplified it by
taking 50 non-overlapping classes that's
sort of the second level of the
hierarchy we removed one class and
accounted for about a third of the data
and we removed a few classes that hardly
had in Europe branches so then we had
data that had 50 classes and we could
see how well dropping out and
if you training without dropout you get
your classic kind of thing that will
favor an early stopping and with dropout
you get sort of a percent or two better
and you don't over fit so for document
classification it works it's not as big
a win as for the other things but in
general our experiences pick a task at
random and this will make give you five
to ten percent less errors just by using
a somewhat bigger net and using dropout
here's a more interesting task this is
classifying color images they're small
color images so I designed this task to
be as similar as possible to M list but
nevertheless to use real images from the
web so they're 32 by 32 color images
there's ten different classes these are
examples in the test set of birds and
you can see so the images are fine
the images are producing the following
way first search the web for bird and
things that come under bird in arm
subnet of classes and then take all
those things and give them to Toronto
undergraduates and say the instructions
are roughly this is the one dominant
thing in the image and could that
plausibly have the label bird you
discover after a while look you're
paying these undergraduates and they're
actually playing games with each other
on their laptops and not labeling images
because labeling images is incredibly
boring the only person I've ever found
who like labeling images was Radford to
Neal Radford Neil's daughter who would
look at the display 100 images and go
cat cat cat I think she was about two at
the time and so image labels ought to be
aged too they really get into it they
would probably pay you to do um so Alex
Khrushchev ski trained a great big
neural net which is convolutional he
tried lots of things his best net was
getting about 18 percent error which is
about the record on this and he then got
that down to 16 percent error he's now
got that much lower by using
translations of the images
but if you don't cheat if you don't use
transform data or you put an extra
knowledge that's quite a big win these
numbers always changing who keeps
improving things here's a more serious
test of object recognition so I got fed
up with the computer vision people
saying that um you you neural-net guys
are just trying simple sets like Caltech
101 so I called up Malick and I said um
who has has been saying that you know
deep neural nets haven't really proved
themselves object recognition because
they're always using things that simple
so I talked to Jeetendra and Jitendra
agreed that if we could work work on
image net then that was a real result
image net would be impressive if we
could make it work on an Internet um
there's a 2010 competition where they
disclose the test set so you can try it
yourself um it's got so um 1.3 million
training images so files in different
classes you're having to a thousand
weigh classification you've got about a
thousand examples of each class so it's
going to be really important to sort of
share features the winner of the 2010
competition got 47 percent error for
their first choice if you say it's okay
to get in your top 5 choices then they
got 25 percent error so that was the
state to the out in 2010 the state of
the art now is a little bit better they
down to 45 percent so there's another
competition the winner that competition
went back to this competition I showed
that they get 45% on this competition so
the state of the art as far as we know
is 45% less um Alex tried a whole buncha
neural nets um is he ended up with a net
with seven hidden layers not counting
the max pooling we're the only layers
were convolutional so they didn't have
too many parameters the later layers
were fully connected so they had lots of
parameters they had like four thousand
units fully connected so that's 16
million parameters right there um and
there's two of them he used this trick
of transforming the training data so he
reduce the images to 256 by 256 but then
didn't use all of that image he use
patches or a bit smaller so he got lots
of different patches so he's getting
things in lots of different phases so he
never had the same training example
twice and he used a bunch of other
tricks that I learned trying to go into
an answer irrelevant to the effect of
dropout so he got down to about 48
percent error so a bank the state of the
art using all these methods but then he
applied dropout so the globally
connected layers and he's now down to 39
percent error for the best one and 19
percent error for the top five and
that's a huge improvement I mean people
do computer vision I'll tell you that's
a big step forward here's some samples
from Alex's net this is before he was
getting the very best results so this is
a slightly worse version of his net but
just to show you what this task is like
you're given a picture like that and you
have to say which of a thousand things
it is and Alex's neural net says it's an
autumn now I'm actually an apologist for
deep neural Nets so I think you're not
as a jolly good thing to say because
notice you haven't got the tip of the
bead and you got the wet fur of an otter
here and I think it's very clever to
recognize the wet fur of an otter it
doesn't it does get quail ii and it does
get all the things that i can't
distinguish them require like a ruffed
grouse and a partridge after that this
one it's obviously a snow plow um and
then you look at its errors the air is
always more informative sort of drilling
platform and garbage trucks you find
life for wiser thing is like oh well you
know this might not be snow it might be
sort of foam foamy water and if you look
at look again here's the flag at the
front of the lifeboat here's the at
the back of the life but here's the
bridge of the lifeboat it really does
look very like a lifeboat
like I say I'm gonna apologize for your
or Matt's this one it gets completely
wrong okay that doesn't get it in the
top five the right answer is scabbard
it's bizarre it says earthworm
guillotine oozy what says guillotine two
big vertical bits this is presumably
something to do with color on the forest
um
broom you can see what says broom but
earth well might I say earth one but if
you look in a bit more detail you can
see a couple of earthworms here now you
may think this is just fantasy but what
Alex can do is you can back propagate
from the class label to say tell me
which pixels are having the most effect
on your confidence in this class label
so you can see the sensitivity of the
label to the pixels like he hasn't done
with this one but there's one where the
object is a mite which is very small in
me which is a leaf with a mighty and
it's up to one side in the image if you
back propagate there it's the pixels are
the mites that are causing it to say
much so this isn't just totally fantasy
I enjoy showing these let's show some
more image that has a wide variety of
classes of things and you can see why
they allow you to get in the top five so
what's the right answer here well I
would have said probably microwave if
not microwave dishwasher if that's what
that is that's exactly what it says the
right answers electric range I don't
know you wouldn't know where the
electric range is I think it must be
over here or it might just be you know
there must be one there cuz where
there's a dishwasher in a microwave
there must be an electric range and
there's a wash basin so the right answer
there is very dubious and it actually
gave better answers incidentally if if
the test set has some answers that
aren't the best answers then you give
bet if you got them all right that will
be zero error but if you give better
answers and test set that's got to be
negative error so my aim is to get this
thing giving negative error agent um it
has distributed things like turnstiles
that it can recognize it has things from
catalogs like bulletproof vests here's
my favorite error you shirts um I I
phone headphones
and it says it's a corkscrew or lipstick
or a screw um many sit and a wild earth
would it think they're an ant but if you
look at it this is the view you don't
want to have an ant
it's an enormous and and it's just about
to bite you it's looking down line
yes Santelli okay this is an aphid slew
of an ant now we can't prove that that's
why it said an the wiles wood is and
there's another way to think about
dropout so I shown you dropout works
another way to think about it which is
like what the evolutionary theorist
thing is that run thing even in terms of
model averaging think of it in terms of
what a hidden unit has to do so a hidden
unit in a net I would like to do
something useful if he knows exactly who
he's going to be collaborating with then
he can sort of tune what he does to what
he can expect them to be doing and you
get these complex code rotations but if
you don't know who else is going to be
there um you're out of luck if you try
and tune to them you better do something
that's sort of individually useful it
turns hidden units into rugged
individualists s-- except that they're
rugged individualists would like to do
something that's useful given what the
other guys are up to in general so given
all these common literally many sets of
other guys you might have to work with
do something that's generally helpful
that'll cause you to do something
different from what they're doing but
not relying on which particular ones are
there and that makes you far more robust
against then changing but also against
the environment changing and being
robust against the environment changing
is what provides overfitting the machine
learning the environment changes when
you switch from the training set to the
test set and what you really want to be
robust against is those changes so
here's a little example of that here's
two training cases
uh those are the inputs and those are
the desired outputs now here's a set of
weights that does perfectly fine um you
see 5 plus 11 gives you 6 and if you had
a needs you get 4 but the 5 and 11 are
co-adapted
they sort of this minus 5 is sort of
going in the wrong direction
and it's relying on this plus 11 being
there otherwise is saying something
really stupid if you do drop out and
there's plus 11 sometimes isn't there
you'll see that this minus is a really
bad idea if you do drop out you'll get
why it's more like this um actually you
get weights of twice this but when you
put them into the mean that you have
them and then you get weights like this
and then these weights and licensable
um so in machine learning the sort of
two ways of regularizing things one is
model averaging and the other is adding
noise people shown that adding Gaussian
noise to weights in a linear system is
exactly equivalent to an l2 penalty um
adding guessing noise to the inputs is
exactly equal to internal to penalty
sorry yeah guessing noise to the inputs
that's like having an l2 penalty uh as
you have a more complex net it gets a
bit more complicated but people
generally think of adding noise is
different from doing model averaging
what drawback shows is it's a case when
you can view it either as adding noise
or as model averaging they're not really
different methods then I want to go into
one more advantage of using dropout
which goes back to evolutionary biology
I guess when you train a net with
dropout the neurons are very robust
against their co-workers changing they
don't know who's going to be there and
so they better do something that's
robust against these changes that means
these Nets ought to be very good in
genetic algorithms you ought to be able
to take two different nets and take hot
let's suppose you just have one hidden
layer take half thousand units one net
and half million units from the other
net already these hidden units are
robust against their co-workers changing
so when you take half from each net
you'll get something that doesn't work
properly yet but his works already works
pretty well and then you set skeevers
tested this it works pretty well and
then a little bit of training it works
almost as well as the parents so now
what you can do is you can take a big
cluster or big farm
and you can have do a lot of computation
with almost no communication so you're
running lots of networks on a little
bunch of local cause and every so often
one of these networks advertises for a
mate she considers the possibilities
chooses one and then the the you get
half the hidden units from the mother
and half from the father the mother
network then gets suspended mothers know
about this and that core is used to run
the child Network the child net we then
do some more training and then you make
a prediction about whose ground at best
the mother or this child the child may
not yet be as good as a mother because
it hasn't finished training but you have
to make some prediction is this child
likely to end up better than the mother
if so you kill the mother if not you
kill the child you may ask what happens
to the father's well it's very important
in this algorithm to have rapid gender
changes so a network there's a father
one time better be a mother another time
if you're a father you never get killed
um and so to make it all to get rid of
them you have to do gender change okay
um yeah indeed one is just sort of
standard dropping again one wouldn't
work so well it may well be that in
biology you have to cuz organizing group
sex is too complicated but I don't know
we haven't done the experiments we
haven't even we just on one very
preliminary experiment to show that if
you make two of these things you get a
child that quickly learns to be good so
the robustness you get from drop is
really helpful and the thing about
genetic algorithms is you can use a
million of your spare cause and you can
be using them sort of fully without the
only communication okay now for
something that's not completely
different um if you think what drop ID
is doing is taking a neuron and
the neuron is logistic neuron my Nets so
computes of probability P and then the
commutes a number P from the logistic
that's the output of the logistic shoot
all the neural activity he then sends
this number is real value with a
probability of not 0.5 to the layer
above that's what dropouts doing well
that has exactly the same expected value
at sending a naught point 5 with a
probability of P okay um the variance is
slightly different but I so why not try
that arm so take when you're on that
when we pre train these neural nets we
use to casting by new neurons anyway and
instead of doing back propagation by
sending these real values why not just
send a single bit chosen stochastically
so you compute P and then with
probability P you send a 1 um it's sort
of bizarre because we always thought
that the the breakthrough and back
propagation was realizing that you need
you to send those real numbers um
actually if you do that it works really
well the neural net is obviously slower
to learn and you need to make it a bit
bigger to be able to learn the same
amount of stuff but he generalizes much
better and from preliminary experiments
I've done the improvement you get in
performance is similar to what you get
from droplet um so what this is saying
is if you've got a big deep neural net
and you're relying on regularizing it is
big right it doesn't have enough
training data you want to regularize it
by using something like dropout or noise
it's actually better for the neurons to
send a single bit than it is for them to
send an analog value um in dropout your
variance if you send a half
if you send P with probability half your
average values a half P and so your
value is going to be 1/4 P away from
from that so when you wanna go um that's
the variance you get with with a
stochastic bit but where you're sending
a point five this is the variance you
can see that when peoples are half
they're the same that's good because
they have to be when P is big this guy
has a lower variance in this guy but
when P is small this guy has bigger
variance this guy when P is small goes
as P squared this guy goes as P and so
when P is small using a scatter bit
you're getting even more variance and
you do with dropout and that's the
Poisson limit when P is small you ignore
this term and the variance of R or short
P and that's sort of roughly what
neurons are like a lot of
neuroscientists will tell you what a
neuron does is it computes a price on
rage and then it emits spikes according
to impress on you have to take into
account the fact that it can't
immediately meet or not the spike in
that makes life more complicated but
ignoring that applies Ahmad's a pretty
good model of a neuron and it's always
been a puzzle why don't they use the
times of spikes why why do they do the
all these randomness and the answer is
because that's what they want to do
that's the right thing to do if you want
to fit models to data if you send real
numbers you do worse so here's an
amusing piece of history which I already
alluded to when we trained first
discovered that you can train lots of
laser features unsupervised we're using
things called Boltzmann machines a
stochastic binary units cherry Sinofsky
insisted on stochastic binary units in
1980 when I first met him because he
said that's what neurons are like and so
we tried to develop ways of computing
with them um because that's what neurons
were like we discover we could train
layers of heat changes like this but
after doing that we knew back
propagation was good for fine tuning so
we pretended they were deterministic
neurons which is a horrible intellectual
fudge and we use back propagation it
turns out if you don't do that if you
keep running them as stochastic binary
neurons in the forward pass but run the
standard back propagation in the
backward class so in the backward class
you're using those peas in one - P
but they never need to be communicated
outside the neuron then it actually
trains slower but it generalizes much
better so it's a better thing to do so I
want to end with some explanations of
why cortical neurons don't send analog
values um because that's always been a
puzzle and a lot of people say well
there's no way they can do it
efficiently but actually it takes some
sugar to send despite because you have
to send this wave of depolarization it
takes the same amount of sugar to send
despite with precise timing that's not a
good reason not to use the time what's
more we know that there's neurons that
do use very precise charge so when I
localize a sound I'm relying on the time
difference in sound time difference in
time for the time to get to my two ears
now that's a millisecond per sound and
the difference in time to my two is is
more sort of like this that's the
difference in distance so we're talking
about a small fraction of a millisecond
and your neurons do that by sending a
spike this way and spike this way and
seeing where they overlap so they're
using spike timing very precisely so we
know evolution can do that if it wants
to also in your hippocampus you have
well if you were a rat
you'd have which basically you are you'd
have you'd have cells that fired when
you're in a particular place and so that
the fact that the cell fired would tell
you you were there but when it fired
relative to an oscillation tells you
where you are within that place field
there's some debate about that still but
it's pretty well-established so there
you're using the precise time of the
spike to tell you where you are and the
fact that it occurred to tell you you're
somewhere around right here um so we
know they can use spike times the spike
so I think this just isn't a plausible
explanation they can do it there's an
efficient way to do it and they don't
another argument is evolution just never
thought of it I mean it just never
stumbled across this idea and I find
that highly implausible evolution can
take the same cells and turn them into
teeth and eyeballs
if it can do that why cut you know if
using the time of a spike seems sort of
pretty obvious we can even think of it
so I don't believe that I'm my current
explanation is neurons don't sense send
analog values because they don't want to
they're better off sending stochastic
spikes and that's because it's such a
great regularizer and what neurons are
really concerned to do is fit a
gazillion models to this weird data
they're confronted with average what all
these models are saying and sending
stochastic spikes is a very good way to
do that okay I'm done
so we have time for a few questions but
if you want to ask a question you have
to talk into the microphone so no one oh
ok one one person and in their argument
why cortical neuron stones and analog
values is that it's more power-efficient
to do it stochastically like ah why is
it more perishing because the the part
the energy is in sending this wave of
depolarization down the axon you have to
sort of your sodium pump has to pump the
ions back in again no not sending a
spikes like doing the work related to
like aligning the spikes and like
looking at variants along the wave it's
not that hard I mean one thing that
really convinced me of this is I dreamt
up a little scheme so I could send you I
could you could arm time spikes
carefully and you could for example do
scalar products where it was a scalar
product of a vector of strengths and a
vector of spike times you can do that
scalar project and think it's quite easy
to convert that back into the time of a
spike for the next layer so it's not
hard to dream up machinery that does it
it's just that the cortex doesn't seem
to be using it so I think there is
there's an energy efficient way to do it
yeah so in in the drop out case you you
you turn the stochasticity on during
training as a regularizer but then a
test time to to get the power of those
models back out you averaged them right
but if this is the conjecture for how
the brain works and stochasticity is
always on it's as if it's always
training when can't cash out how can you
cash out and actually average those
models they tend to begin well at test
time like pricing so to have you to the
models what it would need to do is run
for a while and use spike rates
basically you need to integrate over
time being stochastic and if you look at
people making decisions they
more like a with time so the way the
brain if if this is why it's using
stochastic spikes the way it would have
to do the model averaging is in the
correct way which is to run it several
times and so you get more accurate if
you spent more time by averaging results
over time and psychologies have lots of
evidence so you do that when you're
trying to make decisions that's why you
get more accurate if you're given more
time high so has a method like that's
been applied to general Bolton machines
more general models like this well of
both machines sort of already doing this
right the bolts machinery is all leased
is all stochastic spikes master um
I don't know if the spikes that the
intensity of the spikes I mean in your
case is zero point wins the constant but
in observations like be very diverse
right no no spikes pretty much all the
same size so spike is propagated by
sending a wave of depolarization down
the axon and it's it's a digital thing
that is the digital thing up at each
stage so that if you if you
hyperpolarize it
sorry depolarize it at one end so that
the wave starts traveling what comes out
the other end is entirely predictable
and what comes out the other end will
stay the same even if the action has
branches and so on so it's a sort of
digital system you put a one in this end
and one comes out the other end and
they're pretty much always the same size
now there are some models like people
have made sort of computational
neuroscience models where you have
spikes of different heights Mike Loki
has a model like that but that's not
what real spikes are like if a mother
point one more one thank you a support
victim machine could be viewed as one
neuron so job Al's could be applied to
support victim machine have you tried
this no I haven't is trying its tangible
I like the view that a support vector
machine is one neuron
you can explain it to me um so if you
treat a dropout or view it as a regular
riser it shuts out half the nodes at any
given time another straw man would be to
simply have half as many nodes that are
always um you've tried that and it's oh
yes drop as much measure okay so if you
look at that one sixty on Yan's web page
a lot of different people who know a lot
about tuning backpropagation try jump
lat tried lots of things and john plank
was the guy who got 160 by running a big
hidden layer just one hidden layer and
training it very very slowly he managed
to get 160 ones
um lots of other people tried and did
worse if you don't sort of transform the
data or pudding knowledge so that's been
very carefully explored and there is a
result on Yan's web page of 153 that's
an unpublished result and it's by me
um and that result was obtained by using
these weight constraints so weight
constraints actually you can beat 160
you can get down to about 150 just by
using the weight constraints but you
won't get anywhere near sort of 110 120
another thing is the ones with drop out
learn more slowly yes um have you
thought of you know not using drop out
at the beginning so this is fairly you
know stuff we we've only written one
paper on it was just rejected and so
almost everything to be tried we haven't
tried judge we've just got the initial
results that suggests it works really
well all right so we're kind of at a
time so let's thank Jeff
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>