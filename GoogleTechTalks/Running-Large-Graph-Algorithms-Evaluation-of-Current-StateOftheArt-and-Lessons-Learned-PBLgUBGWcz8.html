<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Running Large Graph Algorithms: Evaluation of Current State-Of-the-Art and Lessons Learned | Coder Coacher - Coaching Coders</title><meta content="Running Large Graph Algorithms: Evaluation of Current State-Of-the-Art and Lessons Learned - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Running Large Graph Algorithms: Evaluation of Current State-Of-the-Art and Lessons Learned</b></h2><h5 class="post__date">2010-02-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PBLgUBGWcz8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so I'm pleased to introduce Andy you
today this talk sort of gets got started
when I went to give a talk about our
graph infrastructure at Livermore a
couple months ago and discovered that
there's a lot of interesting very large
graph work going on it at Livermore and
Andy agreed to come talk about what
they're doing here are there he's he got
his PhD from penn state while ago you
went to Livermore and spent I don't know
five years or so working on very large
systems machine systems and then moved
into the the large graph area and has
been doing quite a bit of work in that
area since and and he's going to tell us
today about some of his experiences
there Jim oh thank you all for coming to
my coke a my name is Andy you I am a
computer scientist at laura's liberal
national laboratory probably uh you may
be aware of this but our lab Lawrence
Livermore Lab is known for a large
scientific computation and a big
machines but the past few years of the
new breed of science called data science
has gained a lot of attention this data
at the lab the data science is pretty
different from the data that used in
large scientific computation in that
they are highly unstructured data like a
text our webpages sensor data images and
one that the idea is that you want to
fuse all this information from different
sources into a form of graph and run
graph mining algorithms to extract some
useful information out of there so our
our first question that was that can we
use those large images we already have
at a lab to run large graph algorithms
and it was kind of mixed but the answer
from our early experiments was negative
so we try to find all the ultimate run
logic where five buildings
so I'm going to talk about the other we
don't stay we gained from the series of
it I'll tell me the file is it okay can
you hear them tell you baby both the
remote sites please mute your mics
please ok so the all-star talk with the
graph that we are interested in and the
how they are used and some of the
challenges involved in running a large
graph algorithms and one by one I'll
talk about the other systems and
probably models who have tested in our
experiments and the other results and
the the learned the lessons we learned
and I'll finish the talk or by sharing
some of the thoughts that I have gained
of throw this experience ok so the
graphs that we are interested is called
scale-free graphs I also known as a
power-law graphs or complex networks
these graphs are actually random graphs
that take deconstructed are over some
here at a time without any external
control or of the intervention so verra
VAR c and n physical has shown that
scale-free grabs is can be constructed
or built by connecting a new vertex to a
popular vertices with large degree so
this method is known as a preferential
attachment and and in the preferential
attachment method if the continues will
ended up having a big fat nodes are with
a high degree those we call hops and
like a Google or Yahoo was a lot of
connections and the existence of this
this Hobbes actually explains a lot of
known properties of scale-free graphs
for example because they are this
because it will because of large air
hops the digger these visions if you
plot them will follow the other
political distributions meaning that
there are lot of small degree nodes but
there is a small number of very high
degree vertices as well and because of
this hubs have a large connectivity is a
lot of connections to other vertices
what it does is that they actually
strong shrink the other diameter of the
graph so it called make raise all kind
of the known here properties of
scale-free graph like a small diameters
and the 60 group's operations a small
water above the gear characteristics and
also these graphs also tend to have some
car form of communities and if you look
at the inside of the communities you
will see as some kind of self
similarities between the communities a
backrub structure and / or grab
structure so the scale free graphs are
everywhere a lot of them and the most
common ones internet graphs and web
graphs or social networks like the this
of friendship graph or citation networks
and some of the groups are being used
and bioinformatics like a
protein-protein interaction graph ok so
dr up these crabs actually are usually
very large and they carries a lot of
good information so graph mining is a
technique that actually you can extract
this information hidden deep inside of
graph then and there's a lot of TI uses
for that and some of the example the
most not the obvious one is the google
pagerank and and his kind of our the
graph algorithm that belongs to a class
of problems known as spectral graph
theorem ok and many related applications
like a personalized web search and
random will restart and all the related
to finding I game value and eigen vector
problems
graph mining is being used a lot of in
areas of social network analysis and the
information they look for in this
particular application is closely
related to a structure of the graph for
example the graph that we have is what's
known as a decorous carrera clock graph
that described curry club that exists in
a u.s. college to the 70s and what
happened it is is crop is that there's a
feud between a founding member and the
coach so eventually they split into two
groups right by running a graph
algorithms you can actually tell which
belong to which groups and for other
centers of these two groups and who are
the people that work as kind of a
liaison between these creepers and
welcome on this social net one is I've
made out of the cages for example you
have like a in run graph which is very
messy but if you're wrong are the proper
group mining algorithms you can actually
determine the rules of each individual
with a few other secretaries who are the
CEOs and who are the people with the
power and similarly this is what I call
the center of the Union universe problem
I you may want to find a single guy in
the middle who has a final word and it
controls everything like OD mr. can lay
and similarly you can find a people are
due to power some some some information
people from these two guys that spending
their leisure times in jail now also
craft social networks is incorrect money
getting a lot of attention from us of
the government's especially intelligence
of the communities because this is a can
be actually very useful tool for
counterterrorism and count eight legends
so they are getting all kinds of
information from the year the field
reports and they fuse that information
and build a graph like this and this is
actually are the events graph that the
person is doing what and what engraft
mining algorithms can answer is buttons
like its food for the food and food
those who are if you know this
information actually you can destroy
this or the terrorist organization or
analyst
dropped the operation by taking out some
of the key individuals or blow to the
flow of the man that's so this gave very
valuable tool and also graph mining has
a lot of the commercial and academic are
the interest for example we have the the
social networks are it is a stray de
Graaff average select big bowl of mass
and and correct mining algorithms can
actually take dissect this kind of messy
a draft into a form like this and you
can answer the questions like this of
who hands out who by dividing this group
of friends and who are the most popular
dudes amongst these people I don't know
how we can make money out of this kind
of information but after we send a point
of view as very interesting and
fascinating to find the scholarly
information and the graph mining is also
used in bioinformatics are this is a
protein-protein interaction graph and
units come out the algorithm called the
community finding and what it does is
that if I you can find the a cluster of
a protease that that closely density
into each other that means that and if
you know any of the functions of the any
of the star the protein what can you
tell is that these group of protease
probably do the same kind of functions
so that's how they find out another area
of bread mining is pattern matching in
pattern matching is the European of
pattern and unit you want to find all
the instances of sub graph that matches
this pattern this is an only one example
of the AB prep aromatic the pattern says
that you have the two guys living in the
same house and barbarian monitoring and
watching some factory or national
monuments and one guy went to the truck
and the other guy aboard a large
quantity of fertilizer and if you find a
parent's than match this kind of an
instance that you may want to notify
someone either deal in inwood law
enforcement agents and the same kind of
techniques can be also used to detect
fraud and there to detect the atoms of
skin is in cyber security areas for
intrusion detection
like this the mas Gading so the problem
with running a large graph algorithms is
that they're usually very big I'm sure
I'm not sure they how big of your web
graphs you guys study here but I'm sure
is more than 10 billion vertices house
it's not as but is still reaching the a
billion our vertex graph range the other
problem in the graph mining is is that
the algorithms that we are interested in
are usually high computationally very
expensive I mean even those are running
a simple graph like a breadth-first
search on graph of this size can be very
challenging and also the other many of
the graph algorithms are tractable graph
algorithms has the N cube or N squared
type of complexity the other
lesser-known our problem is that the way
these algorithms works they tend to
generate a lot of intermediate results
and this is the source of a big
performance problems for example this is
the the size of the level sets in the
prep research as you increase the levels
the the size of the intermediate results
shoots off very rapidly so what other
people are the approaches that they take
to solve this problem government labs
and the the people with the resources
they use the high-performance computers
and people don't they use the relational
databases or other of the free source
proposals are the tools like a how-to
MapReduce or data flow systems okay and
we are evaluated all these different are
the platforms and programming models
using different references but in this
talk I will mainly focus on the graph
search because this is the computational
least expensive and and and that's why
we were able to run the graph search in
all these different platforms in the
first commercial system with hazard
distance is IBM they are the merchant
orbit out from IBM
is massively parallel high-performance
computer the idea is that they wanted to
use a low-power very slow processors for
loss of them it appears bad this was the
largest and fastest supercomputer in the
world at that was Livermore Lab it has a
64k 64,000 nodes each one as a duel
course and if people have 130,000
processes and has half gig of memory the
memory the 32 terabytes of memory in
total and all the nodes are connected
the value back past the BD tourist
network and some true networks and for
their programming environment we use the
message passing using MPI libraries so
how you run a graph search on blue jean
emotion like this is very similar to
what you believe the prego iran's so you
have a graph you have chopped rd chop it
down into the independent sets and you
distributed all these nodes and at the
same time you give the source of the
vertex and the source of the search then
they start reading the other Jason two
vertices and and send out the the next
level sets to the proponents of the
those vertices and have to receive this
up messages they do the same they found
the next level says send out here to the
other vertices and eventually the
communication pattern will become altar
communications and this is very
expensive type of communications until
they find the one life under ok
destination is there and then send out
the message is saying stop get done ok
the problem that we found was the
message size as I shown they are in
earlier slides that message by volume
has actually increases and because of
that we will not a we had a lot of
problems actually making it 12 a graph
of the larger size so we had a lot of we
have to do a lot of optimizations I'm
not going to go through the details of
the what we did optimum addresses but
basically what we did was that we used a
some people to the edge partitioning to
reduce the number of communications
and we theoretically put up upper bounds
on the message size so they actually
turned out to be great so that we could
actually save the memories space that
used as a message buffer okay and we map
the MPI tasks logical processes into a
physical processor of the blue Jenelle
to tell but in such a way that you can
take advantage of fast interconnects and
the we also notice that the it the level
set contains some tomorrow under the
vertices and and we actually get rid of
them before sending out to the technical
team okay so we did a research on graft
with a 30 million edges on blue Janelle
and we deal with scaling study rescaling
meaning that you've increased global
program size as you increase the number
of processors it used so the local
problem size are constant okay and in
this search we randomly select a source
and destination when it will run hundred
of those searches and later the average
time and as you can see it's not I mean
I deal which scaling curve should remain
almost flat but this is not that bad for
the graph of this size it took less than
five seconds in worst case mr. okay and
what surprising to us was that the
communication time overhead line is very
small is about 5% this is not something
that what we expected the reason I
believe is that this is not a
full-service past and the single source
shortest path type of search is as st
type of that you have source and
destination so they are working on very
small very small diameter graph already
so path lands should be very like a tour
I have not seen the president is large
enough that to see they are the results
will go off and also all the optimizers
that we did for the communication and
running Andre has network it actually
helped us their communication time
and this work was actually first attempt
to run the graph with a billion edges
and it was nominated as a finalist for
2012 will award eliminating I mean I
told you that there are a lot of
duplicates in messages I'm the editor
level sets in worst case eighty percent
of the vortices were duplicates right so
we actually before sending out those
messages we can cover it up all this to
palakkad matter what it does what it did
was that it's not only reduce the
communication time and communication
volume but it reduces the amount of work
the receiver has to do I mean West the
receiver receives these messages then
you have to read all the memories to go
into the next level so this one or this
graph shows are there problems with the
gray typical graph algorithms we ran a
random walk with restart on blue Janelle
or that were the smaller scales and
there's a log-log scale the search of
time and it's not really that good it
shoots up and the reason is that it
shows the communication was father like
this I mean this cover our augen value
problems is known to be notorious how to
deter scale have a problems but well
this kind of shows that that is true
excuse me no we you are not able to do
that because we use the existing
packages to run that problem so we are
not able to optimize that that package
no so the messages I mean either take a
message that lessons we learn is that
the programming for message passing
produce not that easy and make it
scalable was even harder so and as I
just mentioned that i don't think the UH
it will be are you useful for other
grandfather's because there are no tools
or craft mouth packages that will scale
that obama we try to many different
packages but nothing was successful and
the other of the thing is that we use
random graph for the CDC experiments
because we didn't know back then how to
generate the scale if it grows fast and
random graph and scary graphs are really
different and i don't i'm not sure with
its all this optimization that will
scale just are the very large scale
three graphs at all and we may have to
actually have the Empire libraries but
they follow up to my vision okay so the
next machine that we were interested in
it was a shared memory of
multi-threading architecture like a
crazed MTA but we did not have creme ta
okay so we chose a very similar
architecture called Sun Niagara as it
was ultra Sparky too okay what's special
what's interesting about this
architecture is the people many people
know that the memory latency is
following right so you single thread you
have competition and we wait for the
memory computation anyway for the memory
so in traditional of the architecture
they try to reduce this memory latency
time by improving the fcat utilization
in this architecture they I'll take it I
mean they they accept that there will be
a memory are there delays what they did
was that instead of trying to reduce the
of the memory latency time they in
increased they put a lot of large number
of threats and interleave this excretion
so to make sure the eldest one thread is
running at all the time so if you
consider the other computation time the
the is you need a really good
utilization of the processors and they
use the hard way actually for the
context switching of the threat to
reduce to overhead so we ran a very
simple venture mouth which is called RMA
of the simple benchmarked up there
basically the accesses the random
location of the big table in the memory
and as you can see the more threats that
we are add to these benchmarks the
actually populace will get better this
is the beauty beauty of the empty
architecture
of course it will eventually dear the
flat out because adding more threats at
certain point we will not have that much
so it was very good results so this is
how we'd won the graph search or nigra I
mean is basically the tip direct
translation of the message of parsing
algorithm hillgrove you chop it off and
outside the other so into memory and the
process of this array is a cyclic threat
right the same as the sources give it to
one guy and they start reading these
vertices and move on to the next year
the level sets and they distort in the
shared information this dynamic
information into a are different
portions of the memory but until the one
guy assessed upon the other destination
body vortex but the problem is that this
shared information the disinformation is
a shared by many different threads I had
to use a locks so this is the other week
scaling study of this tia the graph I'll
go to that i just showed and its really
bad is a low blow scale and it improves
the shoots up a lot and the problem was
that it was a the large contention will
check the pod where performance counters
and ensure they are as we add more
threat that the idle cycling time was
was getting bigger and bigger and the
only particular chip they use the other
software locking mechanism so for the
one single thread i ran the code with
and without the locks the difference is
about 4.5 x times so the biggest mistake
we made for this one was we try to
translate a message passing algorithm on
this particular architecture and that
was mistake because because of the lock
contention you cannot get the good
performance out of it and and come to
think of it I think yeah there for this
particular architecture we have to
develop algorithms in some asynchronous
ways and try to minimize the locks ok I
don't know how to do it but this he has
to be different type of flooring minds
that require two to run the divergence
on this
protection so many people use is a
relational database because as chip are
and and is canceled a lot of data by
push and we chose a motion coordinate
Lisa R which is a state-of-art para
database machine in to run the graph
algorithms and this motion is designed
for the data warehouse type of
applications and and it is kind of
active disk file approach you push the
computation to whether there is in stock
pull the data out from the disk and it
uses a fpga on each data node so the
user some of the quarries gets compiled
and portions of that the quarry is
loaded into the FPGA to expedite the
quarry process and also this technique
or make made this motion very expensive
the graph search on the using SQL is
very simple you stop of the rotor graph
the system and you start with the source
vertex which is a single table single
road table and the you expand by
accessing the auditors the bonuses to
the next level and from this level
system which is a stable pivot table you
come come up with our bigger tables
until you reach a table that contains
destination vertex so what's how you do
expand the level sets in the search
black teeny it's scruffy in the yellow
circle is actually join operation okay
you join the current levels the other
just of the tables and you come up with
our next level says so and graph are
good and they're like yeah in the graph
algorithms the involves a lot of this
choice so when you run SQL code for the
brave algorithms their performance is
dominated by joint performance and
unfortunately that the relational
databases we tried that petroleum
performance was not that good because it
was not designed to do that one nice
thing about the relation arabic you can
store a large data so just to try to
test that we are
repressor a skill great research on
synthetic graft with a 300 billion our
bodies edges and on the paper this was
the largest grab that searched are out
there I'm not sure you guys did a bigger
than that but unload 60 under 70 to our
network performance server and the net
is a poppin server at their headquarters
in Massachusetts and the disease results
they see claims that eighty percent of
the search is finished in less than five
minutes okay but there is a catch they
used a date actually is that something
called a bidirectional bread for search
you actually start from search from
source and destination and you move to
the different directions so what it does
is that you actually a cut the path
lines in half so the other the size of
the tables they have to join is pretty
small okay even considering that this
numbers are doesn't look so good that's
not so impressive and I don't think it
easier this SQL type of algorithms will
scale to more general graph algorithms
at all so I mean relational database is
easy to use our and it's really cheap
but again joy performance em near the
joint operation proculus was a
father-like and grab involves a lot of
joint approaches and the other problem
that we had was that we didn't have many
choices for the optimizers because it's
taken care of by the SQL compiler and we
didn't know what was going on behind a
curtain of this t-sql compilers and
their behavior is different from one
compiler to another so there was the
other problems we have for the year
relational database systems so we ask
these out of the questions how about we
customize the systems the graphs engine
systems for our needs so we have
developed it our own are the aggressor
search engine called mssg I'm here s
grace and mssg is a parallel version of
that okay and this system is actually
consists of some front-end and back-end
the input to the system is the streaming
edges and we try to call out the these
edges into into an audience to set the
memory at the front the front ends but
eventually this memory will be a memory
buffer will fill out their full so at
the time you send out to the back end
notes for the stretch but because these
are the other set of vertices come in as
a blog as a park in undetermined on
schedule time what we did was that we
just changes flaps of the the vertices
to improve actually locality it turned
out to be good okay and the running of
Professor childhood is the same same way
as the message passing algorithm does
difference is that we read the data from
the disk instead of memory ok so the
strengths clarin castro scaling is that
is a reversible week scaling you fix the
global program size while you're adding
more number of processors okay and and
as typical is pretty good number
although we are the size of the graph
which right is pretty small is only a 10
million vertices as we add more
processors your search of time declines
but the the communication overhead is
actually increases as it's natural
because the you add more processors that
means there are more data to be
exchanged ok which Scalia's test is kind
of interesting because I mean is almost
flat it's not that bad for this small
size graph but what's interesting is
that the overhead disk right overhead is
actually shoots up as well as the
communication overhead and this is
because you add more processors then
they grow up the overall problem sizes
gets a bigger and more data read and to
communicate and if you want to run graph
our witness on on any of the dr out of
co way this is something that you should
expect i'll skip this ellison is a
power-law graph at the after the
polluting experiments we finally find
come find a way to generate synthetic
weapon really
fast so with after that experience
everything the synthetic is rigorous we
learn that we try to actually try to
partition the graph so that improve the
localities and reduce the communication
but it turned out to be that it was not
that great so we ended up using random
partitioning but kind of round robin
type of partitioning it worked fine
because the many of the graph algorithms
11 character is that they do not have a
locality so even trying to improve the
locality of the application yeah it's
not going to work so the messaging is
the learned lessons to learn is that
well designed custom graph engine like
ours can actually outperform very
expensive commercial relations layer of
law systems and maybe I show show these
results this is the results of the same
experiments way we ran on the net is
emotion the relational database machine
that much in the small rack is actually
close two million dollars is very
expensive machine and the of course the
size of the graph which fly is very
small but also the size of the clusters
that we try our run on this experiments
was or one tenth of the net is emotion
size and in this experience we can
finish ninety percent of searches in
less than under 90 seconds just pretty
good so we learned that the
communication and disk writes a bottle x
I'll see you for the logic graph
algorithms you have to take care of this
one very carefully and the one thing
that we regret is that we didn't have a
good programming model to start with so
it is more like the the plug-in type of
software development any to the longtime
developed system and if you if we have a
clearly defined probably model while we
developing these these products the week
Lev computer better okay and the i'm not
sure if maybe I'll easily
for the more general type of graph
algorithms is made for the search good
many people are looking at the guard the
other cloud computing the map Buddhist
based approach because it's out there
and is a simple method simple program
model and be able to process a lot of
data in in cheap way and I'm not going
to go in details what this is because
I'll will roop what I will appear very
stupid try to explain what emeritus is
to people at the google so and the this
our search is based on the single source
shortest our method on task force
algorithm and I'm not going to the
details because you have we must have
seen these slides million times okay
basically what i would like to point out
is that one map and reduce space is the
the operation that that actually goes
through one level to the other is you
can you can think of it as a one joint
operation in relational database SQL
type of skin right so you have to repeat
a lot of map and reduce faces to the
search the knife implementation i just
showed are often really bad because the
reason is that in neither implementation
you have to actually carry out all this
year static and dynamic information from
one place to the other and deaths this
officer so what the year with optimizing
the way that we distributed all the
static and dynamic information to vertex
the oldest nodes and let the other map
and this test is worried about their own
local files and walk only on that and by
doing that we were able to get like a 3x
process spira and even our with ya dis
dis optimized code we ran a actual data
it's not synthetic data pubmed graph as
our 30 million but assists and five
million edges okay and this is the
results of course the MapReduce
outperformed the the net is immersion
relational devs machine pretty big I
mean this guy's is actually twice s as
the the
we used to run the memories but a it was
a vast thirty percent slower considering
the number of nodes thirty percent slow
then escalates architecture but
considering this is a MapReduce and it
really easy to program this was it is
good however we learned that this type
of the problem model is to limit it it's
more it's better suited for
embarrassingly parallel type of
applications in the Patent unnecessary
their corrupt type applications and we
could be just are the implementation
issues but this is the way of is
handling this intermediate results is is
really bad I mean from one phase to the
other they have to store all this
intermediate results the level cells in
a storage lb them again I mean it could
be just in case of the the huddle
MapReduce that we used but there was I
mean we're not the only ones to actually
find out these problems and the days a
lot of people some people in academia
they reported their problems in their
papers but this was big performance
model exit so eventually I believe this
year the MapReduce will have to be
evolved into some formal but data
parallelism like a data flow model and
the it has to have our the ability to
stream the data instead of throwing the
intermediate results in risk and we did
again it has to have some kind of a
streaming data and in coupled with the
pipeline processing of this coming
incoming data are in ink or in memory
okay finally we are not the only ones
that that that are recognized the
problems with a data flow mode I'm the
other how to maybe this model and the
people are looking at the data flow
model are the Microsoft have believed
they developed some people to try it and
it looks like this so I would say the
data flow system is I mean it model is
the MapReduce on steroids it in a way it
is similar to emeritus the way how it
runs but it's more flexible and more
awful than beverages because the people
can freely construct their own data flow
knows I'm near the tasks data processing
task and freely a construct and data
flow diagram like this okay and the
indenter flow model there are a lot of
tasks called agents or the actors and
they start execution as soon as the data
are arrives into the inputs and so in
that way there is no way actually you
can control the other flow of the data
okay and this is a natural way to
realize a data parallelism and we turned
out to be really big factor to improve
the performance was so have we happen to
have a actual data flow system running
data flow system called data analytics
supercomputer is from a company called
LexisNexis so we chose this machine
simply because it's over there are not
many are the data flow systems out there
and this is a parallel theraflu engine
that runs on any commodity clusters so
in hardware wise is nothing special
about it but it's actually their
software that makes a lot of difference
and as a ability to swim the data
through the data flow diagrams and and
they do process this data in in the
memory in pipeline fashion and their
access them in their access to the disk
is all sequentialized to reduced in our
movements the faster disk accesses the
designer of the system actually really
nice thing they realize that there are
manipulation operation is actually
actually very only two very important
approaches which is sort and join so
they optimized heck out of these two and
build a system around these two
constructs one other nice thing about
dataflow machine are compared to other
relational database or maybe this is
that you have a flexibility you just
have a flexibility to a construct there
there there there there are management
and notes
in any way they want and combine them
together for the better up tour better
performance and put the relational
databases and we do not have we are
completely rely on the SQL compiler and
in a sense I'll compare the relational
database systems to the modern autofocus
or everything kind of camera that is
easy to use and does their really decent
job compared to other by Dow system
which is more like a my father's 1930 c3
camera we just really able to use we
have to control everything dials and
knobs but if you know how to use this
camera you can still track incredible
pictures so corrupt search on the task
is very similar way to run in on SQL 0
the sister graph into the system but
that the difference is that as rest is
no start arriving the task based on the
execution it's a waiting until all the
data gets loaded so but the way is
implemented is still very similar to
synchronous ways so they have to create
audio level sets and pass the next level
next level and next level but thing is
that the other path they don't have to
wait for this to come so the old things
is running apparel so we ran are both
very directional and bidirectional
search using our actual craft the PubMed
graph and this is the results and the
for the x direction research motion size
is pretty small we ran 20 node cluster
and we were able to do search in less
than one minute and compared to this
performance to other a big table I mean
feat data kind of systems that does
stands out it's all relative the
performers are the dance system is
actually five times better than the
second best which is a square system and
I contribute to this fact that we were
able to this data parallelism were able
to run of the process data in peril and
also their underlying library that
mised for certain joint and other art
images will really work well so
combination of those two techniques we
were able to get this kind of good
performance so Darren search ah we
wanted to I mean our management wanted
to actually to see how this much it can
be used to for more complex type of
graph algorithms so we came up with you
are a suite of 512 core is the parent or
is this very complex type of course
because of this one's this type of
course that generate a lot of
intermediate results it's NP complete
problem it will sub graph isomorphism
and that kind of large intermediate
results can push the system's capability
to to its limit right so we there are
five choristers and we ran those five
corazon pubmed and this is results we
have and looking at the airport III and
44 we got about the 250 a tremendous
speed ups compared to Netezza and other
relational database systems and this one
this particular to cory's the
intermediate resources are really really
big that's the power of the data flow
model compared to relational data model
so i found i personally other experience
in writing a functional programming
myself so i didn't have any problem of
looking at his proper near the graph
algorithms in data flow way so there was
relatively easy to program for me and
the system this model actually enables
asynchronous data parallelism and it
combined with the highly optimized data
may a pleasure constructs and witnessed
combinations the performance really good
prop day the best amongst the oldest bit
data type of platforms okay the thing is
that Hardware may be cheap but their
software was very expensive this motion
was not
vidor is about comparable to our
Nettie's emotions on the software is
very expect that the library was very
expensive Yeah right in using I mean our
code is written new details of their
constructs but they're soft was very
expensive that's the point okay and up
there the problem that we found doesn't
like a MapReduce it because it's pure
data flow model there's no way you can
actually control the flow of the data
stub here-- Cordero ever there and I
found it very limiting factor almost
showstopper for this type of model to
apply to the graph algorithms we need to
be a really are you need to have some
kind of a control from the mechanism
embedded so I believed in the eventual
data flow model to be something like a
control flow type of application the
other model to be useful for graph
algorithms so so this is uh I got two
slides left good so this kind of
summarize dear experienced we have a
billet I mean bottom line is that no
system is perfect when it are in terms
of price performance our scalability and
then provide ease of use the thing is
that there are two camps actually can
divide when it comes to logic graph
analysis one is the people who wants
almost real time of of graph search time
for those people they have no other
options but to use very expensive one of
coins have a high performance compares
like Apple Janelle or Cray MTA but these
people are usually what for government
and there are other people who care more
about the cost muslim visit and academia
said sectors and and the the coast and
the data capacity and those people tend
to stay in this up i well called big
table a big data are their area and in
this region i'll give up MapReduce on an
edge for the price and ease of use and
when it comes to pawn
was i given ages to RS grace and data
flow model right I don't know but my
guess is that none of the system will
scale to a large graph because number
one as I tell you that the way this
graph is work algorithm works in it
generates tend to generate a lot of
intermediate results the way this
program is implemented I believe we have
to think differently you cannot stick
into our looking at the Commerce
algorithm book and try to directly
translate the algorithms for large graph
I don't think it's going to scale so
this is my personal code filling
eventually MapReduce model will be need
to be involved in to the target area of
low moral or some kind of Kentucky a
workflow model with the other oldest
requirements that that I mentioned in
earlier and that will be ideal case in
terms of when all these things company
considered but again another option the
very interesting option is prego this is
nice option for the people running on
the message page time application
because it gives an ability for the
users to write program very easily I
found it very interesting and I also
believe that this one can be easily
extended to a shared memory type of
architecture as well there's no point no
technical difficulties of doing that the
question is can we push this model to to
this region model is nice is simple and
I don't think we have to change the
model itself but these things should
have to be there to support this several
models certain degree so this is my
final slide I would like to share off
some of my clothes this is my personal
opinion and in so it's highly subjective
but eventually again I believe that
asynchronous their apparel model ah as
it's ideal of the platform the
programming models to run logic graph
algorithms okay like a MapReduce for
data flow models and it has to be
coupled
the very efficient and scalable
substrate implementation to support all
the streaming and pipelined processors
and whatnot so again this is very
important up the thing having said that
asynchronous data peril model is the
future could be wrong then you the
people they writing software do
algorithms they have to think
differently and that's the thing you
have to solve this problem
algorithmically the problem again is
that these graph algorithms use a lot of
global states and this is the source of
the bottling and can we do really do
I'll write a program without dealing
with the global States program for the
search I don't think that deal with it
its current problems right so you have
to think differently and and and and we
regulate different type of algorithms
and again I'm the prettiest very
interesting option I'm pretty sure
considering the fact that this the
cradle are is from google in few years
this more that will dictate us it will
tell us how to write a rap algorithms
and personally I'm very interested in
learning this new technology and and I'm
dying to write some of the algorithms
using this model I'm sure they'll i'll
discuss this was in meetings in
afternoon with the spread of people here
that's it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>