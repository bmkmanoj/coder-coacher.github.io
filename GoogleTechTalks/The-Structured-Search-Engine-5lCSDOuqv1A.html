<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Structured Search Engine | Coder Coacher - Coaching Coders</title><meta content="The Structured Search Engine - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Structured Search Engine</b></h2><h5 class="post__date">2011-01-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5lCSDOuqv1A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good evening everyone glad you guys
could not make it so when we talking
about what I pretty months ago decided
to call the structured search engine
when when already asked me for an title
and an abstract for this talk I'm
actually kind of debated a bit you know
what I was actually trying to talk about
here as you mentioned it's what we try
to do in my group here in New York
search quality is try to really
understand and you know we said the
queries understand the documents try to
put them together structured isn't a
great word for that it has a lot to do
with what we're we're doing but you know
it's not perfect and I thought about
calling a semantic search engine this
wasn't this wasn't perfect either I mean
semantics is kind of laden with little
semantics you know people semantic web
other things like that it means a lot of
been things to people the understanding
search engine doesn't relie have a good
ring to it intelligent search engine
just kind of what we try to do but not
not perfect that one's taken
so as we often do in this this kind of
situation I turn to Gary Larson to
explain my feelings so I mean I love
this cartoon I mean basically you know I
think this is actually kind of what
what's happening with search engines
today right we we want to say something
we want to ask the search engine the
question we want to explain ourselves
when to give it a lot of background
contexts and things like that
and and you know we wish we could do
this but really all the search engines
really hearing is ginger ginger you know
it's not nobody getting what we're
trying to say funny story actually
happened just yesterday on our internal
quality mailing less someone brought off
the query he's trying to figure out what
the sixth Lane on a running track what
the length of that was unlike an Olympic
sized running track now this is this is
the guy that basically wrote our index
you know run our indexing serving system
in Mountain View um so he tried the
query Olympics Olympic track distance he
tried the query track distance Olympic
track running distance he tried a whole
bunch of these queries he was doing
ginger ginger ginger right he was he was
basically giving like these really broad
queries and hoping that Google would
magically come up with the answer turns
out if you put in the query how long is
the sixth Lane on an Olympic running
track top answer it's right there so so
bad that even the best engineers at
Google have been trained to say ginger
ginger ginger tuna to the search engine
but but in reality like we're actually
getting to the point where Google can
try to answer some of these questions by
more deeper understanding longer queries
but more people understanding the
documents and the content that that go
with them so so today I want to talk
about some of these efforts that are
going on most these are going on in New
York on the search quality team here key
topics I guess we want to what can you
understand about the world kinda where
we start like what things are in the
world can it better understand the
queries that we're getting from our
users can we understand the content of
the web documents user reviews things
like that and then finally can put all
this together and actually like we will
do some of the work for you and kind of
put all this together so that you don't
have to work quite as hard to get the
answer to your questions so I won't read
it but you know going to kind of go
through a couple of technologies we're
developing here and how they try to
answer these questions
so the first question is you know can we
have to understand what's going on
outside the search engine shouldn't
operate in a vacuum you know we should
be actually understanding that there are
real things in the world it's not just
documents and engrams and so on so as
many of you might have heard we acquired
a company called freebase over the
summer they build a structured database
of everything in the world you know this
is a graph it allows links for between
things so we know that you know Bono and
the edge are members of u2 and so on so
for those that haven't seen freebase
before if the database of entities as I
said except connections it's got
properties it's got strong semantics so
there's actually a schema for freebase
it knows what a company is it knows that
it has employees and it was it as a CEO
it's got about twenty to twenty million
topics right now
it started off when we acquired it
actually about thirteen million we've
made a strong commitment to making it
bigger you actually I added a whole
bunch of new music data that's been
published now so Google's really
committed to making sure that this
becomes the canonical reference point
for high quality 99% precision you know
entity topic data on the web this is all
this data is all publicly available just
as an advertisement it's creative
commons license you can you can do
whatever you want with it
attribution to freebase this is this
great API is I mean it's got a very
simple to use JavaScript query line
which and as there's some very good
tools that they have this thing called
grid works which was recently renamed
Google refine we're basically pulling in
new databases of information and
reconciling it into freebase if you've
got a great database of you know ancient
Mayan art or something like that pre but
it doesn't know about any of it we'll
give you the tools to actually help you
import that make sure that if pre base
already knows about some of it you're
reconciling it properly and merging it
properly and then you can make this
better for everyone so just to give you
a little bit of an example of the things
that the freebase knows about you know
it knows about buildings you notice
about molecules it was about Aztec gods
you know this is this is a place where
we start to get further away from
standard kind of databases of everything
like Wikipedia it was about candy
international candy us candy knows about
art of course it's a quick you know
example of how frequent represents
information previous knows that there's
an entity called Blade Runner onions
it's a film there's about other names
for it you can see that it's not only
Blade Runner but there's a name in
Russian and various other languages for
as well it known as that Harrison Ford
acted in it you'll note that the
relationship between a movie and an
actor is actually not a simple one
that it's not just an actor as a
character that he's playing freebase
represents these by something called a
compound value type which is a way of
kind of mediating between an entity and
kind of complex information so it's not
just a simple attribute value store it's
a way of actually representing complex
real information about the world so
freebase is cool we can do a lot of
stuff with that you know we can build
interesting search engines but as I said
our goal here is to actually have a
database of everything learn about
everything in the world so you know 20
million enemies is that's cool you know
what's really cool a billion entities so
so how do we get to a billion entities
right I think really the only way that
we can think of and we can we can do
this manually you know we can kind of
walk through in and try to have each
person to contribute you know their own
one of the thing that they know best but
really we're going to have to do
something automatically and there's a
lot about what we concentrate on here in
New York so what can we actually extract
out of documents the other simple stuff
things we can recognize with patterns
dates and times and measurements phone
numbers and things like that some things
are harder because they're kind of
ambiguous things like locations you know
Starbucks has many locations which one
is this document actually talking about
people obviously very ambiguous but can
they actually recognize which people are
being discussed in certain documents and
then even more complex types like as I
was saying with freebase no factual data
like what are facts about an entity so
here's an example of a page that is an
entity this this floorstanding speaker
that previous has no idea about right
this is a long tail entity and it's
actually pretty good page I think we can
pull a lot of information out of this
well actually the manufacturer and the
part number and so on you know it's a
good description of this thing we'd love
to create an entity out of this if you
scroll down the page there's a really
nice table here you know it's got all
kinds of obstruction information be
great to get this stuff in so our
teammate of New York works on pulling
this information out we use a whole slew
of techniques we look at kind of look at
tabular data maybe that looks like it's
organized in some way we look at data
that looks like attributes that we've
already heard of so maybe we've already
heard that weight is an interesting
attribute lots of things have weights
freebase knows that so whenever we see a
weight followed by a measurement let's
let's pick it up and prove it into the
database
in crazy things once we've picked up a
couple of attributes we can get a
pattern we can induce a wrapper for the
for the data and actually pull out
things like input impedance you know
that's something that pre base might not
know about we might not have in our
schema what's still useful information
so the approach here just we're not
going to go into too much depth at
probably this talk it's going to be a
huge fire hose and you're going to
probably have to join Google in order to
find out the real details but so the
approach here as always the Google they
have a huge we have large amount of
scale we know the whole web we're going
to go for you know a very high coverage
system here that we're going to give up
something in precision but we're going
to hope to get it back by aggregating
data from lots of sources those are the
look tables look at you know things like
attribute : value we know certain values
take certain types
so so depth is always a measurement of
length and if we can recognize a
measurement to blade and the word depth
is nearby it's possible it's quite
probable that someone's trying to
describe an attribute here of some
entity we look at things like standard
techniques like page segmentation we
look at rapper induction to induce the
pattern for what we're sitting on the
page so all of this obviously as I said
not much depth here but lots of machine
learning going on behind the scenes to
produce this table you know lots of lots
of valuation and we all want to find out
how we're doing and kind of iterate on
on the information we're seeing and
what's what's working what's not and
we're trying to eventually build up this
this database so you'll notice at the
bottom here by the way just to point out
the high coverage low precision meme
here you know it's not just the good
step we've ranked this and we're doing
pretty well I'm going to take questions
at the end if something very particular
right yeah sorry so you got the question
was about the number on the left hand
side so that's just a confidence that we
assign it's you know you can think of it
as maybe like a probability of being
true although that might be a horrifying
it a little bit so so they're saying
we're you know this is just the cream of
the crop here if you look down a little
further you know start to get a little
sketchy there's a going to lower and
lower confidence values but you know
these things all kind of look like
attributes and values to our system but
thankfully we've been able to kind of
pull out the wheat and the chaff here at
the top and that becomes the useful data
that we have so that was that was the
first firehose that was kind of some how
we understand the the world outside the
web and what can we do to kind of build
up this database of everything and
hopefully make it to a billion facts
we're not we're not quite to a billion
net but we're trying so the next the
next step is can we actually understand
queries you know can you understand what
what users are saying when they're
trying to look for document information
and so in this in this main I want to
talk a little bit about question
answering so this was actually my
starter project at Google you know six
years ago still doing it it's it's a
tough problem as the sign says here this
is a great shot snap on one of the guys
on the team on the subway Google some
answers aren't found on Google but we're
hoping the ones that are found on Google
that we
we surface them in a way that you know
people can find useful so just give an
example I'm talking about talking about
this this one box here at the top if you
asked when with Martin Luther King jr.
born you want to service the answer
right away and kind of give you the
correct answer the team is also
responsible for for highlighting answer
in the web search snippets especially if
we have lower confidence that this thing
is right we want to still give you some
indication that maybe this is a date
that that's useful for you
so they take through a quick worked
example of how we understand the query
like this so the first thing we do as I
mentioned before we have a lot of
systems here for understanding simple
text we call them annotators things like
dates and times and measurements and so
on one of them is very good at
recognizing natives so first clue we
have here for the query is that it
mentions a name that's that's that's a
good piece of information once we've got
that bit or even if we don't we next try
to split the the query into a thing that
the user is asking about we call it the
entity and an attribute you know what
what property of the enemy are they
looking for if we can't find that we're
done you know this is probably not a
question but if we do we're going to try
to figure out how to retrieve a good
value for that the next thing we look at
you know we have the large database of
entities we know something about how
they're related do we know any other
names for the enemy being mentioned in
this case we know that mother King is
jr. is also referred to as Martin the
King often MLK and so on one of the
things when somebody asks for someone
when someone is born what what other
ways could our database be representing
that same value date of birth date board
and so on um you look at the whole query
to try to give us some other clues as to
what the answer might be Mew have a
bunch of you know to be honest regular
expressions that kind of say well if you
say when was it's probably looking for a
date even if we didn't have that because
we have this large database and we can
kind of do a lot of aggregated analysis
we know that date of birth are born
often looks for elephant has the value
of a date so we think it's probably a
date board can also mean where they were
born so there might be a little bit of
confusion about that but we're going to
try it with a favorite dates in this
example so now we go when we look at the
database as I mentioned we've got a big
big table full of them for me
like this you can see that we've got a
couple of potential answers here the
birthplaces there as the birth date and
we're going to go in now the sorry that
the the big insight here is that we're
not just going to look at the table if
we just looked at the table lots and
lots of queries would match you know
we'd be able to answer pretty much any
query with something out of tables I've
shown you before there's a lot of low
confidence values in there but the big
insight here is we actually look at the
top search results google.com search is
very good at delivering on topic
information for pretty much any query so
when you say when was Martin Luther King
jr. born chances are the top if it's
question the top documents have the
answer and so in this case we go we look
up our suggested answers in the
documents and try to figure out if they
appear in this case obviously the dates
appear you'll notice also that the
birthplace appears as well but because
of two truth factors first we find the
date more often and second we are
expecting to find a date
what is up happening is you know we
actually answer the question correctly
we get a lot of cooperation to the top
search results and we try to give the
user some some some ways of actually
backing this office which is we're
probably going to be wrong five 10% of
the time we want to make sure the user
kind of tracked check us and figure out
where this answer came from so that was
again another quick firehose another
project we're doing here in New York
about question answering so next thing I
want to talk about a little bit is what
we can do to understand content so we've
looked at the world outside you know we
can understand entities now we can look
at queries and understand how users are
asking questions and we can look at now
we want to look at how we actually
understand the documents the content on
the web I'm going to pick a particular
slice of this we've already talked a lot
about extracting information from
documents and how we understand that but
I want to talk a little bit about
sentiment analysis so set of analysis is
the field where we're looking at can we
help me understand what users are saying
about let's say a business products a
person is it positive or negative
are they happy about their experience or
the upset you can put it another way
it's a difference between you know Eric
and his googly eyes or double facepalm
right I mean like what our users really
how are they feeling about this
so just an example this is one of the
things that's actually out there on
Google
today on the place pages which is part
of local search we we have the listing
here for the Carnegie Deli and you'll
notice that we're actually summarizing
the reviews for the Carnegie Deli it is
1500 reviews almost for Carnegie Deli
and if you really want to get a good
feeling for what's going on and should I
go there is it really well it's cracked
up to be you could read all 1500 reviews
but if we can we can actually summarize
and try to give you a feel for it one
thing you'll notice is that the star
rating for the Carnegie Deli is three
and a half stars that's kind of like an
aggregate average of all the review
texts that we picked up and the snippets
that we've put up here call them Frank
and it's because they're kind of you
know sewn together they're trying to be
balanced in same proportion as that
three and a half star rating so there's
good stuff obviously great food great
service so not so good stuff some people
think the waitstaff are unprofessional
and you pay the price for it too so
we're going to try to balance the
reviews somebody has five stars when
interesting the mostly positive reviews
one star mostly negative reviews I'm
just trying to give you a good feel for
it you want to get into more detail we
also try to parse out what aspects what
things are people discussing about this
business you know in the case of a
restaurant that some standard once
obviously everyone talks about the food
and the service the carding Bella deli
is known for its corned beef so we
actually are able to identify that pull
it out and reviews automatically and
determine that corned beef is a pretty
big hot topic everyone's talking about
it in positive or negative terms you
have to dig in even further find out
about the desserts and so on and you go
even further right I mean we're really
trying to give you a tool to really dig
down why is it that everyone's talking
about
the corned beef what's wrong with the
service why is that you know almost half
red and so on so so that's just a little
bit of a set up this is why we want to
analyze sentiment and summarize so um
some analysis is a great field for
machine learning and NLP there's so much
text it's such a you know an interesting
kind of deep natural language problem we
have to deal with a whole bunch of
different issues here obviously this is
a basic one which is you know what is
what is a positive sentiment what is
negative what words represent positive
stuff whatever words represent negative
this aspects as I discuss what are
people talking about you know this
deeper natural language problems what
happens when somebody negates the word
you know wasn't the best you know we
don't want to give it credit for
positive just because the word best
appears there and as well we don't we
also don't want to apply the negative
sentiment to the latter half of the
sentence and then finally scope you know
oftentimes people say things
excuse me like we came here because we
couldn't stand the lines at the other
restaurant we don't want to give
negative credit to this restaurant
because of its lines it's not it's not
the right scope they're talking about
some other restaurants so these are the
kind of things we're dealing with if we
want to accurately summarize information
from reviews so I might be able to go
into all of this I want to just go
through a couple of worked examples here
of how we deal with the positive
negative words and how we deal with
negation so so this positive and
negative problem we call classification
such as classification so given a
sentence given a review is it positive
as a negative house user expressing
their opinion there's some pretty broad
goals this is Google right so we want to
be the possible use this in many domains
we don't want to be just doing local but
new products new there's people whatever
we can do
it's got to be international we have a
ton of traffic coming in from from
outside the US and UK and the English
language countries we don't want to just
make a solution that works for English
and then stop it's got to be robust a
lot of content on the web is misspelled
case nobody noticed you know it's got
people don't use proper grammar so this
can't be like a really hardcore you know
parsing and so on that that's going to
mess up a lot just because somebody
misspelled the word and then obviously
it's got a scale I mean we're dealing
with with millions and millions of
documents easily I want this to be
updated every day I want to build taking
reviews very quickly and so on so the
approach just for classification is to
build a lexicon we call it which is a
set of words that have some meaning
associated with them of words that have
positive negative associations and we
want to do that quickly from a small set
of seed that's how we're going to crack
the internationalisation problem I'm
going to say you just start with a small
number of words and we'll be able to
expand upon them so really quick to
build a lexicon you start off as I said
with a small set of seed words you know
these are simple things like good and
bad fantastic and so on we've got 100 of
them actually is enough as it turns out
to just kind of give the system
sense of what where to put the dividing
line between good and bad and then you
also take a large graph of engrams so
you think of this a little bit like word
net you know it's a set of words or
phrases that are associations between
them in our case we actually have
something a little more powerful than
word net we actually take the whole web
corpus that we compute what's there's a
distributional similarity metric between
all the engrams that we can find since
basically says what kind of context to
these words appear in and how similar
all those contexts we compute this over
the whole web corpus and we end up with
a very very large lexicon several
hundred million phrases and edges with
weights between them since the big graph
it kind of says which words related to
each other as far as how they're used in
the language nice thing again about this
you notice there's nothing English
specific about this all we need is a
bunch of engrams and a bunch of
documents in that language and we can
kind of go ahead build this graph then
we run returns label propagation over
this so we start off by labeling the
notes that have positive or negative
sentiment and then we kind of
iteratively propagate those weights
through the graph until we reach some
sort of steady state that says this
portion of the graph is positive this
portion of the graph is negative and
this portion the graph will all we don't
know it's probably somewhere in the
middle remember this isn't just words
this is all kinds of phrases you know
this is things like truly memorable
right not just truly not just memorable
but truly memorable is a fairly positive
word one of a kind turns out Mac fanboys
is slightly negative so pain in the ass
you know sweetly internal bleeding
clearly negative if somebody mentions
internal bleeding in the review of a
restaurant probably don't want to go
there right so so this is just a really
small sample the weights associated with
these kind of range from negative 5 to 5
yeah it's just based on how positive or
negative we think that these words are
based on the context we find them so
it's just to give an example of why it's
good to use a lexicon like this that's
built from lots and lots of phrases
instead of just a simple dictionary we
can actually tell the difference between
nouns and other kinds of typically non
sentimental words based on the context
that they occur in so things like dog
dog is not a particularly sentiments
word unless you have one but dogs
barking is negative dog friendly is
positive right self-sufficiency is good
in the terms of self but self serving is
a bad bad kind of phrase painstakingly
is different than painful
attention-grabbing
turns out to be a good thing
money-grabbing turns out to be a bad
thing you know and even even great even
positive words like great sometimes mean
negative things like great expense is
not necessarily a good thing
so this is kind of the power of having a
lexicon it's built from lots and lots of
documents lots and lots of you know
different ways of representing the same
information the other sentiment
kind of tasks that I want to talk about
briefly is is negation how do we
actually handle when a user negates a
particular word so they say not great
how do we actually tell that great and
not and actually positive in this case
but is actually negative so we could do
this with a lexicon right it's possible
that we'll see the word the phrase not
great or wasn't great enough in our web
corpus that we'll be able to identified
as a as a negative thing but it's better
it turns out to be a lot better if we
actually build a specific tool for
identifying negations so again just to
work a quick example on this a review
piece of review text here is pretty
standard service wasn't the best but the
food more than made up for it and this
is what our negation system tries to do
with it so the beginning of the sentence
zero here
or green is is basically it is not
negative zero means that there's no
probability that this is a negated part
of the sentence and as we go towards one
it means something that's probably
negated so that that wasn't the best and
best is not actually a positive
sentiment here with high probability the
user is not talking about best is
talking about the opposite of best and
then as you can see as you go down the
sentence the the probability of that it
is still negative kind of to indels off
you know this isn't this isn't a perfect
example here right we actually want to
treat food as a positive thing and we're
just barely you know kind of making it
down towards where it's probably not
negated but if you said the right
thresholds here we actually do fairly
well identifying negations so the way we
did this we being the team not me
we took a golden data
that we basically hand labeled about
2000 reviews and Penhale labeled the
negations in the you know we tried to
get good agreement between the people
that were labeling these so that
everyone kind of said the same thing was
negated it we built what's the user
technique all the conditional random
field which basically outputs
probabilities it takes a bunch of
features and outputs probabilities of
whether or not what you would like to
happen is happening in this particular
section of the text and we trained it
based on these positive examples so just
kind of to give some of the results here
this was published last year in a
workshop and this was actually on the
public benchmark or something called
Bioscope bunch of biology papers this is
actually the best results that had been
published so far it also had a dramatic
effect on our internal metrics on where
we train when we looked at local reviews
I don't know if it's easy enough to see
on the presentation but there's a red
line which is the old system a blue line
which is a new system good better is up
and to the right so we did a lot better
here another thing just really quick
point I want to make is kind of meta
point which is that Google does want to
you know kind of work and publish some
of the stuff that we're doing I mean
it's not just kind of a black hole here
we're actually trying to get out
published papers that at workshops at
conferences and so on
I think the sentiments team is a great
example of this both the lexicon work
and these this negation were published
this year and we're applicable I mean
we're really trying to get the data the
the results out there and share them
with the community just a few more
examples of negation some that worked
out well and some that didn't the
underlined bits here are the things that
are being negated so you negations are
hardly a simple problem with detecting
sentiment you know a simple problem is
it's something that's not not a positive
example here you can see you know we do
well on certain things like don't cry
for me Argentina you know that's a good
example system does have some some
issues it doesn't do well on the kind of
yo to speak it looks it's very dependent
on finding evidence to the left it
doesn't do you know very well to the
right so if you flip and you invert the
negation we just don't see a lot of text
like that
sure we could trade a few examples of VO
to speak but they just don't come up
very often in actual reviews of
restaurants so unless people start
writing reviews and do to speak I think
we're all right
okay so so that was the section on
trying to understand content both the
stuff that we talked about the beginning
about understanding and extracting
information from content as well as
understanding content at a deeper level
of natural language processing with
sentiment analysis so this last section
I want to talk a little bit about can
you actually help Google do some of the
work for you we've got all this great
stuff going on the back end but most of
it still ends up being a single query
and doing one kind of bit of information
processing for you can we actually do
something that actually does lots of
queries upon your behalf saves you a lot
of effort so it's maybe maybe this
doesn't happen every day but maybe
there's some problem that would take you
a lot of effort to put together the
information that you need and the
question is can we actually save you
some of that time on these very complex
tasks so I will talk about about Google
squared which was a last product that we
launched about a year and a half ago for
those that haven't seen it Google
squared is trying to tackle the problem
of kind of hard decision making problems
that don't occur every day but that you
know kind of high value right so if you
multiply frequency by value these are
still about as you have the same in
factor as a typical everyday you know
question is during query because they're
much more valuable even though they
don't occur as often so these are things
like buying a car planning a vacation
choosing a college things like that just
as a personal example I my wife or not a
couple years ago we're buying a car
right and we this is before squared
launched before was even kind of being
developed so we did what probably a lot
of people would do we actually you know
we need we kind of wanted a big car
we've got kids we actually made a
spreadsheet you know we put in that
spreadsheet we put a list of cars you
know we put maybe we're interested in
you know the Toyota Highlander you only
were interested in
whatever put that down the left side and
there's things that we care about we
care about the crash-test rated we care
about the price we care about number of
seats things like that but those across
the top and then we did a search for
each combination of those values you
know we actually went through and we
found out what the crash test rating was
of each of these cars and the problem
with this is that there's no one site
that quite does this exactly what you'd
want that is flexible or not to have
both crash test rating and fuel economy
and you know like another the price
let's say you know like the some sites
Edmonds has a lot of its information
that's not as configurable as we wanted
and so we ended up using a spreadsheet
and kind of manipulating the data
ourselves um so these are the kind of
problems that Google squared is going
after the magpie here we chose as our
internal corn a magpie because it's a
bird that collects lots of little bits
of you know oil and hay and garbage to
build its nest and you know some good
some bad but you know it's a process of
collection and kind of aggregation which
is very similar to the kinds of tasks
that we were going for so for those that
haven't seen it Google squared just kind
of an example you type in one query 300
Win Club then Google squared goes out
builds this whole table for you it's got
a list of things this case baseball
players pictures descriptions facts
about them it's not just people it can
do antibiotics can do things like
cheese's actually going to try and do a
quick demo see if this works so just see
it in action so this is just an example
we type cheese's it builds this entire
square on and so on one thing to note
about Google squared is that it's not
trying to do all the work for you we
know in the case of information
extraction open domain information
extraction about the whole world here as
I said before this is a high coverage
possibly low precision kind of you know
domain so we're actually going to only
show you the things we're pretty
confident of but we don't we actually
aren't sure that that we have the right
value we're going to either hide it or
you know kind of dim it out a little bit
and we're going to work with the user to
try to help them understand what
information might be right for their
task so now I liken this to google.com
search if Google like home search were
perfect every query would be and I'm
feeling lucky right wouldn't have 10
results it's not right I mean like you
it's wrong it's wrong
a lot it's a church how it should trust
me so the idea is you give lots of place
you give a lot of feedback and even for
things like you know effect what's
effectively question-answering here we
want to give the user some flexibility
to kind of correct things so the media
you know won't actually go and look at
Wikipedia and find out yes you know I
believe Wikipedia that that gorgonzola
certainly coca-cola is soft and crumbly
and that's fine so I'm going to include
this in my in my Square and Google
squared learns from this feedback we
kind of you know look at users that are
adding and deleting rows users and users
that are correcting values changing
values and these are all editable so I
can go in and just change I don't know
maybe I don't like this one you know
squared is good at kind of adding new
items let's say I noticed that Swiss
wasn't in here so they won't add Swiss
cheese and it'll go off and it will pull
try to pull in new values for each one
of these and kind of fill in the table
Swiss cheese's turns out it's from
Switzerland you can also you can also
add columns here so let's say that I
want to know what kind of wine to pair
this with so this is the tough this is
how query Google Square doesn't really
know exactly you know what what I'm
talking about this probably isn't a
standard attribute in freebase you know
we can't just go to a high-quality sorts
of information but we do have to some
you know some information here we picked
out the wrong part of the value but you
know champagne that seems like a good
values and help put it in here my
computer we're working alright you know
this one fruity wines beers and ales
Gouda cheese so this is the kind of
process we want to give people away very
quickly pulling in information and then
building this new we all want to decide
what she's dessert of my party and kind
of figure this stuff out
Google's where do we start from what are
the key premises here you know why are
we building this so and then how we
going to build it rather so one thing we
noticed as I spoke about before with
question is sorry and so on web search
is a great a great tool for information
extraction and trying to find the right
information we do a search on google.com
it tends to get relevant documents
enough that we can use it right so we're
not going to just use a database
direction and use all that information
inside google search all that pent-up
experience and so on one another point
here is a key step to making decision
you've got to collect data right I mean
if you're buying a car choosing a
college you need lots and lots of data
and you want to kind of put all that
together another premise as I mentioned
it's not we're going to be a hundred
percent accurate we're not going to even
claim that this other person accurate
we're lucky if we get 70 percent
accuracy on open domain information
extraction that would be world class so
we're going to make a trade-off here
we're going to try to go for high
coverage but we're gonna provide users
with tools to correct the data when they
when it goes wrong when finally can we
actually make use of this the search
engine here and if we do a lot of work
on behalf of the user can we save them
all about typing and putting in each
individual query to try to get this the
square or table built then just as a
kind of fun fact I mean Google Square to
build that Square but cheese it did
about 200 queries to Google com all
information together so just to talk a
little bit about how this gets done
first phase you got a broad query let's
get a list of names this is the thing
that goes down the left side what what
kind of changes are there do we also
want to expand that list so if you
already have some cheese's what other
cheese might be interesting what kind of
attributes are interesting for this I'm
gonna go into this in a bit more detail
into it and finally can we actually pull
out a value to put into the square
so the first step as I said finding a
list of names so the approach here we
want to take the query cheese's and go
to something like brie or Gouda we're to
do a search as I said that seems to be a
pretty good a good good tool if you look
at the search for cheese's these are
pretty good results as it turns out you
know this result has a really nice
comprehensive list of cheeses so does
this one one thing you also notice
though here is that these are really
comprehensive lists this is not actually
giving us much information about which
cheeses are actually interesting which
are the popular cheese's you know I the
Wikipedia entry has on the order of a
thousand cheese's listed or something
like that and if I'm just trying to
figure out what I want to serve tonight
with my wine this is probably not the
best way to go you know go about is just
kind of clicking on each one of these
links so we're gonna try to help the
user organize these things does that
require some ranking so we've got a
candidate list of maybe a thousand
cheese's or things that might not even
be a cheese and we're going to try to
rank these things
one approach is to try to get more or
less as it turns out not less so we run
other queries we're on list of cheeses
kinds of cheeses top ten cheese's
popular cheese's some of these are very
good for getting comprehensive lists
like list of cheeses some of these are
good for getting popularity of cheese's
so top ten cheese's is actually a great
way to get lists of people's favorite
cheese's which gives us some ways of
separating out the interesting stuff
from the kind of longtail it's also a
lot of user feedback that we can use
here so squared users as I said edit a
lot of tables so if someone's done this
query before you know possibly they've
added a row that's a pretty good signal
to us that you know this is probably a
good cheese or whatever we're talking
about web users also type in the query
cheese's a lot of it turns out we can
look at what they type next did they
refine cheese's to the brie or Gouda
much more likely they're going to do
that then they're going to do something
much more rare and then we just have the
raw popularity more pages contain Brie
especially pages that contain lists then
danbo the type of danish cheese that i
just kind of randomly picked from the
list but i've never heard of before so
so that's kind of how we get the initial
list of names we kind of do this
two-step process of pulling out lists
and then ranking them now we want more
names you want to be able to suggest
additional ones especially if you've
done some modification of the square and
you've added your own things maybe
you've gone down a particular path
you're just using you know putting in
european white cheeses or something like
that can we actually suggest more things
that fit into this category so in this
case use something very similar to
google sets same basic algorithm we go
out and we look at lists on the web in
aggregate offline and we say given that
you have several items what is the most
likely other set of items that would be
in a list anyone on the web so given
that you've already seen brie and gouda
and you look at other lists on the web
that's a brie and Gouda what other what
other items tend to occur in the same
lists then that basically give us a
probability and we use that to kind of
write suggestions for the pop up at the
bottom and kind of suggest other things
you might want to add to your square
next we want to actually have some
things to go along the top so image and
description is kind of again in every
square is going to have an image in the
description but every other than that
there's almost no
ahman allottee between domains as far as
what kinds of attributes might be
interesting people we wanted their
birthday for cheese's you want to know
their you know where they come from and
so on so so obviously the first thing
you think of go and look at the fact
table we've got this big table hopefully
a billion facts what what does it say
about Bri in this case is actually
pretty good you know it's got things
like source milk and aging time which
turned out to be pretty good this
doesn't always work
cheddar turns out to be a fairly
conflated name you know we don't have
you know kind of a strong idea of which
one of these it might be it turns out
it's a town in South Carolina another
town in kingdom also apparently a album
no a band I guess or maybe a artist we
know who cheddar is so this doesn't
always work I mean we can aggregate that
helps me look at a lot of cheese and see
what their attributes are but I think we
want some other sort of signal here to
kind of help out so there's a theme here
the second approach is to go and look at
web search so we actually take each one
of the cheeses that we do a search on
google.com we do a search for Brie we do
a search for Gouda we do a search for
and we look at the original search for
cheese's as well look at the tables in
that search it turns out that there's
certain attributes that kind of get
mentioned over and over and over again
things like texture and country and this
is a way of kind of narrowing down the
context right
not only did its web search kind of
beyond tend to be on topic most of the
time for these queries but it also kind
of allows us to aggregate across many
many different queries of the same type
and disambiguate so even if there's some
ambiguous things in your list like if
you're looking at cars or car
manufacturers and you've got Ford then
you might mix it up with for the
president and get data birth the
likelihood of that happening over and
over and over again for every individual
car manufacturer is fairly low and so by
doing this throughout the entire list we
end up with a good set of attributes and
the final step I basically already
talked about when you want to find the
value to go into the cell that's
question answering and actually what
ended up happening was we took we
launched Google squared about a year and
a half ago that launched our labs that
was great we got a lot of good feedback
for it but it's labs you know we only
get a few you know tens of thousands of
users per day you want to get the stuff
out of google.com so we took the same
back-end that that runs the
the cells from Google squared let me put
it on Google comm and that's now our
question answering system so this is a
example of where Labs is kind of the
breeding ground for the good stuff that
comes out next on the web search and
we've got a lot of more good stuff
coming down the pipe from here on out so
we just want to talk a little bit about
what we've learned in general from all
of these tasks so first and I've kind of
created several times now web search is
really powerful it's a great way to do
information extraction Google comm knows
you know if you say cheddar it means
you're probably talking about the cheese
even if broad you know strings the
strings in hard fact database sometimes
we're talking about town or a musical
artists web search kind of stays on
topic it helps us stay on topic for the
different things that we're looking for
it also is very very deep it it has
things for the long tail has lots and
lots of documents that are all on topic
for given for a given time subject
another thing is scale as we saw from
question answering from extraction as we
saw in the with the way that we build
sentiment analysis scale let's do a
grenade you know having this many
documents having as many queries having
this many machines allows us to do
things that kind of unable to be done
anywhere else we're tackling kind of
these kinds of NLP and information
extraction problems that I don't think
could really be done on any other
setting another thing is that I talked a
lot about you know coverage and so on
and the trade-off but it turns out that
precision is actually really key we ran
a survey at the top of Google squared
the kind of ask users how how they were
feeling about their experience was
squared and whether it was useful and
whether they solve their tasks we did a
release at one point shortly after we
launched that improved our measured
precision internally on our evals about
10% absolute turns out that the
satisfaction of the users also went up
by 10% so it's you know it's not about
doing this in a vacuum it actually uses
out there and improvements in quality
actually have substantial improvements
and user satisfaction as well another
thing is that coverage very very hard we
want to get into the tail you want to
understand the whole web understand the
whole world but you know it's a very
very hard you know difficult thing to go
into the tail here but it's critical
these are some of the queries that we've
gotten a little Square people actually
typed in titanium rings designs
software artificial tears you know these
are queries that people really want to
get and build a square out of and we
need to be able to find this for them in
finance and how people solve the
problems another thing we learned and we
already knew this is that if you fail
you can ask Li ask the user don'tyou
shouldn't be shy about you know kind of
being perfect and right every time that
kind of makes a hard brittle experience
it isn't likely to succeed you know
every single time somebody tries
something slightly outside of the domain
of what you plan to build it for so you
should build systems that are kind of
robust to user feedback and accepting if
user feedback so they can correct it you
should learn from it on the final thing
another pitch this is this kind of work
on open domain information extraction
it's it's hard you know but it's pretty
exciting I think this is a place where
we can make a lot of impact by having
the scale that Google has so I'll repeat
our each pitch we are obviously hiring
and I wanted to take questions if you
could use the mics actually that would
be great that way the video and the
everyone else can kind of hear them
freebase is an acquired technology right
that's correct for them from freebase ah
music it was a company called meadow web
dollars Danny Hillis it's people isn't
it Danny Hillis founded it yes yes
thanks come on I'm gonna have to bribe
you with schlagg are you going to take
the data they they users enter into
squared and put it back into search
engine we do use the data for feedback
yes we look at user Corrections we look
especially at when they're adding and
moving rows they're adding columns when
the correcting values obviously it's a
source of spam right I mean people can
you know go and input that you know
their name is the current president of
United States that's you know it's a
great wish but it's not true so we have
to look at an aggregate and we obviously
have to compare it to signals that that
exists outside of squared but it does
turn out to be a very good signal you
know and you can actually identify
spammers are the ones that fill the
entire column in with their name you
know they're not being subtle as it
turns out so yeah hi um do you guys do
any normalization of the data that
goes into the squares like structure of
the sorai or anything along those lines
or you just pull it straight from the
web we definitely do normalization on
nothing kind of hand curated like a
thesaurus necessarily but for instance I
mentioned the annotators that we have we
understand measurements we understand
dates we understand locations even so if
we see somebody refer to London and we
see somebody refer to London England you
know we know that there's a high
probability that those could be the same
thing in the right context or if
somebody says 39 inches and something
else is a metre we can kind of normalize
this thing so we do that kind of
normalization especially we have some
information about the semantics of the
string that that's on the normalization
question or do you guys do some
tokenization in your extraction
algorithms so what do you mean by
totalization that breaks it like a word
sure um so we do some of that especially
with getting the lists you know and
informing the queries so obviously we if
somebody says cheese's you know we need
to know that's the same as genes and so
on so we do get roots in that sense I'd
say that that that that's probably the
the main extended background of parse
and we do a little bit of it in
sentiment analysis as well with the
aspects so if you're talking about
service and servers we can kind of
collapse those two aspects since this
the same kind of discussion but yeah
that's that's probably the extent of it
it's question of how do you lose your
experience with with by grams and
trigrams and how did you wait the two so
if you look at the by grams more than
the trigrams or you would see if you can
find an up data for three words
sequentially would use that more the
answered question I'm so I'm not sure if
everybody covered here the questions up
through the mics that way so there was a
bit of length tuning there obviously
there's a but it comes out more and
actually the overall frequency in the
corpus right so we do some tf-idf
for instance like the phrase a great
night you know turns up less often than
great
or you know because it's just a more
complex term and obviously it's going to
show up less often so so we do that kind
of realization we have found that it
does help sometimes but actually you
should probably talk to Isaac sitting on
the steps up there wave Isaac he's the T
helper the sentiment team if anybody is
really interested in that kind of NLP is
a great guy to corner after the talk the
last question is are there any papers in
the field that you found useful in
classification um I'd actually refer you
to Isaac again so I think you probably
check the references from the papers
listed in the talk but okay cool thank
ya
good run let's go backward hi just
wondering how you measure user
satisfaction in search that's a good
question so we actually construct a
survey that basically asks both broad
questions about the overall experience
with something like squared so you know
did you solve the problem that you were
trying to tackle as well as very
specific ones like did you use feature X
did you add a row to the square did you
know that you could add info to the
square we also asked questions about
specific updates that we make so we
initially launched square we weren't
color coding the cells based on
confidence we added that later on but
kind of like you know gray squares and
low confidence squares we asked people
did this help you you know we we measure
the difference between those questions
as we as release updates thanks for the
wrong in the early part of your talk you
talked about open source Creative
Commons content of course we've got
Google Square which is Google property
of course what's the server relationship
or current planning to do to say create
your own sort of corpus that you're
allowed to sort of give back to the
community so yeah yeah sure so freebase
is still open right I mean I think
that's kind of our plan is to keep
rebase open and Creative Commons license
as I mentioned before you know it
started off well start very small but
when the summer we acquired it is about
thirteen million concepts since then
we've added a ton of new data through
Google kind of sources that we've been
able to also creative commons-licensed
things like comprehensive music database
with artists and records and rocks
so so that's our plan it's kind of keep
that open and that that'll be the
resource it also provides kind of a
reference point if we make another point
here you know when you're talking about
an entity in the world we want to be we
want freebase to be the place that you
point because it's open so that we're
not you know worried about it changing
they strive to keep it kind of like
Wikipedia very you know kind of even
keeled and steady and to always have
identifiers is that are kind of unique
so we would like that to be kind of the
way going forward
so is there regularity to two additional
corpus material or is it just whenever
your legal team gets its let's do a
question so there's a couple answers
there so first of all obviously yes we
need to figure out how to get the data
if we're doing it ourselves I mean that
that's a big that's a big problem and
then we're working through that but it's
a community effort as well right I mean
like we have passionate users just like
Wikipedia and people that are experts in
steam locomotives and they add a whole
you know freebase category for steam
locomotives and there's a an active
discussion board about item and
information that's how freebase is
actually grown up till now and I think
it's working pretty well it just took
bustin on top of this any ApS in plan
for the deaf community to build products
on top of Google Square that's a really
good question um so to become really
honest I hadn't thought of it yet so as
far as the api's employees we we have
been looking at api's for a while we do
have the ability to export a square tomb
of spreadsheet removal spreadsheet into
to CSV and so on it would be pretty easy
to build an API on top of that allows
the kind of iterative you know querying
and so on and obviously we have all the
tools on google.com search as well I'd
say that most of the effort right now
and the team is focused on bringing what
we've learned from squared onto Google
comm and I think you know from there I
think hopefully the tool to be available
to the deaf community on main Google
search you know which are costly being
improved as well will get the benefit of
all the stuff that you learn on squared
thank you well one of them most
interesting problems that I've seen
Google's queries determine the set of
attributes if understood correctly you
will determine the settlement if is
conduct query and from the set of and it
is you try to find the salient
attributes and users comes could you
tell us about your ideas to determine
attributes from the query for example if
I ask for Arctic explorers I really
don't care when they were born and when
they died jaren mostly interested in
when they explore write Antarctica right
so so first of all I think that the
approach that we use which is to first
build up a list of actual explorers and
then look for their attributes should
get some of that information right
because I think most of the time when
you're discussing Arctic explorers those
are the kind of attributes that you'd
have but as well as the fact that I
mentioned when we do the initial query
list of Arctic explorers or you know top
ten Arctic explorers we look at those
tables as well in those pages so maybe
Wikipedia has a great table that
includes a list of Arctic explorers and
the dates that they actually got to the
South Pole or whatnot so we look at
those attributes as well it's kind of
another signal into picking good
attributes for the square do that I mean
those are the two sources we have now
which hopefully get you focused and the
last one obviously is user feedback we'd
like to see users add that column to our
square also I believe that the point can
be interpreted so that for to the Google
squared that could cleverly answers cars
with more than seven seats by fuel
economy ax yeah everywhere
well attribute also entity set
construction should not take into
account your fuel economy and do not
eventually take to account the number of
seats but gzip is a post filter yeah now
that'd be really awesome it's definitely
something we thought a lot about and we
see these queries on Google as well and
obviously this is something as I spoke
about the beginning we want people to be
giving us more information in their
queries not less like don't tell us the
broad thing you want tell us the
specific thing so if you're really
looking for a seven passenger vehicle
with a good fuel economy be great if you
can tell us that so one of things we
have obviously in
direction we haven't used it yet is we
have a database of attributes we know
what enemies they're associated with so
we can actually look at parsing the
query the same way we do with question
answering and say like Oh
fuel economy seems to be an attribute
not you know a category let's see what
we can do about fuel economy and try to
use that for parsing for picking which
items to list but you're right it's an
open area I think I'd be pretty cool
famously is strong in terms of
statistical machine learning in the
sense the original Google indices really
sort of graph theoretic models of
observe Authority and then the entity
and value extraction models that you're
suggesting also seem like they smack of
parsing an L and they'll keep on ending
and statistical back end you're curious
one of the as I understand it benefits
of building things semantically the so
called Semantic Web is the notion that
one can attach inference models and
logic models underneath Alice Ike does
Google have any intention of attaching
systems that in a sense would allow
allow a sort of reflection once a set of
assertions are made instead of an entity
and values base a system could then sort
of query itself reinforce it based on
existing corpus of data and draw new
instances of assertions you have any
comment towards that yeah I mean I think
it's definitely an interesting line of
research and I think you're right that
the approach that we're taking because
it's so kind of almost anti ontology
with the exception of the freebase the
heart the good schema that freebase has
does isn't actually very amenable to it
we're much more inclined towards
building these kinds of broad
statistical frameworks as opposed to
building a very specific inference
engines I think part of the reason is
that in our experience we've actually
found this to be somewhat brittle you
know like you can only get as many rules
as you have time to write up and we
haven't found a good way of learning
them obviously you know there's other
domains where that's fine if you have a
very specific task that you're trying to
accomplish and there's a smaller set
even you know a thousand wills or
something like that that might
accomplish that task then that's great
that's a great domain for that but we
found that trust me the types of queries
that we get you know it's just you would
never have thought of writing inference
rules for them so eventually we just
found that using using the scale to kind
of build that built from the bottom up
is a lot easier typically than kind of
writing rules that go top down so that's
just kind of an approach I think but so
I guess my answer is that we don't
really have any soon
pick you no plans to build these
consonant prints engines is it at all a
route of exploration on Google's part in
terms of its R&amp;amp;D I mean there's the
notion that four domains in essence
tumble out of collections of entities
right by the notion of cheeses of course
I come out that you know you may be able
to then talk about the notion of dairy
manufacturers of dairy product so is
there at all any interest so yeah
actually maybe I shouldn't I shouldn't
say that we're not working on because I
think there is a lot of interest in this
there's a lot of folks in the in the
Google research you know kind of area
that do think about these things a lot
and as well as freebase honestly a lot
of guys that they're because they're so
steeped in the scheming on the Near East
and ontology is a great language called
MQL his meta web career language that
actually allows you in JavaScript or and
Jason to construct queries that do very
much you know would look an awful lot
like in principles they say show me all
the Googlers who have written books
published since 2000 and you just kind
of expressed that as a you know as a
bunch of Jason that has stars in it milk
and wild cards so I think there's a lot
of tools for doing that I guess within
my group it's not really the approach
that we consider do have any plans for
usages that the more enterprise
overrated so enterprise tools yeah so
within my group now we've also focused
on on doing things for for the open web
we obviously do have an enterprise group
and actually reuse a lot of the basic
parsers that that I was talking about
before for dates and people's names and
so on and there been some some great
ideas there about looking for looking at
those signals to do a better job at
ranking of smaller enterprise corpora
where you don't have the kind of rich
PageRank signals and other things like
that like you do on the open web so yeah
there's some talk about that as far as
like kind of Google squared level you
know complex analysis I haven't haven't
seen anything like that okay going back
to something you said on on sentiment
analysis at one point you said that you
are using a conditional random field to
determine if whether in a certain
sentiment if something really is
happening and things like that and use
the word that you were actually training
this
so my question is by training you mean
you actually have someone tagging text
corpora you know like I had a great time
and this is you know a great phrase yeah
so from the this is specifically for
negation detection and as I mentioned we
had label a set of 2000 sentences that
had negations and didn't have negations
in them but one of this part of the
sentence that was negated and then
trained on that so that was a case where
we actually went and you know spent some
actual linear time to build up a golden
corpus rather than kind of learning you
know from the ground up okay yeah and
sort of my my follow up question is kind
of similar to what the vidal mention is
like you know a lot of the problems with
you know a lot of these problems with
these with sort of sparse phrases like
you know for example like saying i had a
great night is pretty much the same as
having saying i had a great evening and
you can determine the that the evening
at night is is the same from some other
cinema database of minutes or a idea of
like you know training imprints engines
and stuff like that to actually reduce
the sparseness or classify shares to
poca yeah it's actually a really good
point and so yes we've had a lot of
thoughts down that path I think there's
obviously context as it's not just the
words in the lexicon the sentiment words
that we can use this distributional
similarity metric did identify when
they're being used in the same context
and all something about doing it for the
aspects for the nouns other things like
that so yeah it's a great idea and I
think figuring out that when people say
evening and night they mean roughly the
same thing because they're used in the
same context that's one approach you can
think about it once based on you know
the actual new text and so on
but well good evening yeah I have two
quick questions the first one is on
Google squared how do you keep track on
the backend what people are searching
for and also to make better searches for
that same item in Google squared and
then the follow-up one to that one is
how do you keep track of overseas how
there are different phrases like when is
one thing to do within the US or a
country that has you know us type slangs
what about you know all around the world
yeah it's a good question so on the
first question which is about how we
actually keep track of things that we
have logging right I mean we have a logs
policy where we keep the logs for a
certain amount of time Google squared is
nice in terms of privacy and that we
really don't need much information about
a particular user to kind of learn from
the logs really all we need is their
query so we can strip all kinds of
personally identifiable information out
and just use the raw data the person
searched for this then they added this
then they corrected this you know that
kind of you know brief session is enough
information for us to kind of say Oh
Swiss is a good cheese people keen to
keep adding it when they search for
cheese's does that kind of get it I mean
yeah yeah and then as far as the
overseas I think so first Google stores
only only launched in English right now
it's a pretty tough to get a launch in
English we haven't actually you know
tackled it in other languages yet but in
general the nice thing about the lab is
that for many countries and many
languages there's a really rich set of
documents out there already in those
languages and any of these statistical
techniques that I was mentioning where
we learn from a large corpus of
information if we can get it to work in
English in a general way it's pretty
easy to run an in German or French or
something else like that other places
have different slides because they have
a similarly large corpus of data as long
as we can figure out which documents are
in which languages so I kind of get it
what you're asking about
thank you cool Thanks yeah hello I was
wondering if you guys like another
person asking you guys we're gonna do an
API within that API would you like do
you think you would be able to like open
up Google Square algorithms kind of scan
other data that's not necessarily on
Google I kind of like or maybe they
maybe bundle it up like a like on Google
search
lines like thing that's interesting
questions so we we certainly could I
think that the problem of Google squared
as I mentioned is very reliant on the
kind of information that we get by doing
a Google search right and it's very
reliant on Bulacan being topical being
longtail understanding lots of lots of
information having good ranking so if
you have another corpus that has similar
properties about you know having that
kind of comprehensiveness yeah I
certainly think that that would be
possible
as far as the the underlying techniques
the ones that aren't based on web search
those are completely open like
identifying a date and document into
identifying a fact at a document those
things are much more generalized and
don't rely as much on web search and as
I mentioned before I think that in the
enterprise situation for the Google
Search Appliance is already people
looking into adding those kinds of
things yeah so the they'll be able to
like search and turnable often in some
databases stuff sure yeah exactly okay
all right thanks well so you mentioned
there were about 20 million things in
freebase and most of them were hand
inputs is that correct
um hand is it by hand you mean like big
script scraping a site and you know
reformatting it and dumping it into a
database in yeah I had it so my question
is at what point do you think that
essentially you're going to be able to
set out and just sort of scrape the open
web try to populate more of previous how
far off yeah away from that sort of
thing and how much human intervention is
going to be necessary to sort of vet
algorithms on that yeah that's a that's
a really good question so I think as I
pointed out I mean the difference
between freebase with its 20 mil
identities and the goal of a billion
identities and you know many many more
facts even than that of our kind of fact
extraction tabular extraction of other
techniques I think that we're already
now that you know freebase is part of
Google we're already looking at how we
can take that information and find the
best nuggets of it the the easiest thing
to do is just look to augment existing
freebase entities so I mean we probably
have more facts about the Eiffel Tower
than they do just we did the web talks
about a lot more things and perhaps
they've been able to import so that's an
obvious one it's probably pretty easy to
identify
is overlap and then the harder problem
which we've just gotten started on is is
trying to identify the new things you
know and trying to fight whether it's
tough to tell whether something is just
a different name for a thing or it's
actually a completely new concept that
you haven't seen before but yeah that's
definitely the direction you want to go
is taking the kind of script writing and
scraping out of the equation and looking
much more of the general techniques
which would hopefully scale scale much
further
okay so at some point you mentioned that
there was sort of a value to doing all
the extra work like a value to each
quarry how to figure that out
like how valuable quarry is that's a
good question I wish I had an answer
that was not kind of foofy so I was kind
of using my own internal metric right
and if I just need to know when Martin
Luther King was born the value is
probably somewhat low but the time to
execute that query is also low whereas
if I'm trying to decide where I'm going
to go to college I mean that's a that's
a 10 you know a hundred two hundred
thousand dollar question these days
right and that's a--that's a pretty big
deal and so I'm willing to invest an
awful lot more time in putting that
information together so does the
algorithm decide how much processing
power to put into a certain query yeah
again I wish that we I wish that we
could a priority I kind of identify how
how important a query is but we can't I
mean I think that that's you know I
would love if we could figure out an
algorithm for that I think I can think
of ways to start to estimate it you can
look at the amount of clicks and other
things like that and how much action to
the query get how long do people spend
on it but now honestly we don't actually
have a very good metric for that I was
using more kind of a made-up theoretical
metric Oh diziness is integrating with
advertising instantly and what like well
I guess advertisers could buy no spaces
in the greater ah that's a good question
now that one I hadn't thought of
honestly yeah you could certainly
imagine I mean having paid placement but
Google Square is completely you know at
free right now but yeah
okay all right there's been a lot of
buzz about Korra calm and how it's just
getting a lot of people using as a
resource do you think that like what you
take on that site and you think it'll be
like a destination resource for search
and sure as they progressed people the
right answer yeah that's a good question
I think for is very interesting I think
it has some quirks right now I mean I
think obviously it's much better at kind
of tech questions you know if you go and
you're asking about some esoteric you
know how long is the Olympic track thing
you probably get an answer obviously but
this far as the existing content down
there it's much better and as telling
you how Steve Jobs is dealing perhaps
then there are kinds of delays as far as
I think what they do definitely have is
this ability to to do two things first
of all changes are much more complicated
queries we kind of look out of answers I
mean other things like that and that's
always been done before but I think the
social aspect is actually really
important as well and the idea of kind
of trusting the person that that is
answering a question and having some
sort of identity to that yeah I think
that they're doing a great job with that
you know obviously aardvark which we
acquired is also doing some things in
that domain well yeah I think that this
part is becoming a destination site I
don't know it's probably at the whim of
every other you know social startup and
whether or not they get enough traction
but well yeah they seem to have a good
take on it well it's just I was just
wondering if there's anything coming up
with Google Finance because now you
create a huge context you know is any
idea about integrating it to to improve
Google Finance Article II associational
categories yeah that's actually a really
good question I mean I think we
definitely talked with the team a lot I
think there's a couple of things working
against us in finance but not to say
that we don't want to work on it but one
is that a lot of the interesting
information is proprietary it's a little
harder to get so Finance tends to do
more things like deals to actually get
the data that they want another thing is
that because of financial information
they have a lower tolerance for that
precision you know if you report you
know the incorrect value of some asset
or whatnot you know it's a pretty big
deal whereas if I tell you that Martin
Luther King jr. was born in 1930 instead
of 1929 chances are not going to lose a
million dollars over it so so I think
those two things are working against us
but would you're right I think this kind
of open extraction and be able to do
this kind of analysis over large pieces
of information especially sentiment
analysis as well I think is there it's
definitely interesting area so to
summarize it's because it's it's a bit
of a risky market at the moment to
summarize it's something that we're
interested in working on I guess I'm not
saying we're not working on it I'm just
I'm just trying to point out some of the
issues with actually getting something
out the door yeah thank you like the
silo rooms popular hi um so given how
social the web is and it's becoming an
reliance on social networking is it
possible for Google squared to be
personalized to people based on the
opinions or beliefs or habits of yeah I
think that's actually really interesting
direction that it could go I think some
of the things we try to do when we
launch Square and we allow you to share
tables we allow you to kind of edit you
know see another person table we didn't
have like kind of a readwrite kind of
systems you know a little more
complicated and want to tackle for a
labs product but we definitely had this
idea that you know I'm planning a
vacation
my wife obviously has some say in that
you know probably more say you know so
we want to be able to collaborate on a
square I think that that was kind of an
important thing the idea of saving state
and neil to kind of iterate through it
the sort of getting feedback from your
network I mean I think to the point
about kora and other social exercise I
think it's actually a really interesting
area where you know google square deals
a lot with trust do you trust the site
providing the information to school
teachers use it to as an example of you
know how to gauge whether or not a lot
of our email feedback you know is from
teaching they're using this way to teach
students to gauge you know the
trustworthiness of a site how to
actually do deeper research and not just
trust everything you see because Google
squared is wrong a lot that's stuff like
a teaching moment there but so we deal
with the trust in that way I think
dealing with trust in terms of the
people that you now providing the
answers is another great way it's not
something we explored but I think is
we're pretty
they're squares are there yeah yeah and
ranking that I mean we do have a social
search product it'll look at your you
know you'll profile and so on and try to
surface results that your friends have
shared on reader or whatnot and
obviously that those results would be
available to square it as part of a
rights documents hi yeah so if you are
dealing with sort of relatively
straightforward topics and say services
are at a restaurant no service at a
course and doesn't I think sensible
analysis should be very straight
forwards and you can do very simple
machine learning on that soon but if you
do what's more some specific questions
and on a certain chopping off since a
drop or like an engine snow how do you
handle this and I mean so to pick out
much more specific things now do you
have any any ideas of how to handle
medicine so just make sure I understand
what you're suggesting you're asking
about the difference between kind of a
broad domain with lots of evidence like
local businesses versus with a narrow
domain
yeah I'm like I don't know car with auto
repair or something else something yeah
specific part of an engines no yeah yeah
I mean do people express strong opinions
about specific parts of engines nobody
should be valuable phone for a lot of
people you are deciding what to do about
you know sure I mean I think some things
are universal right I mean like there's
most sentiment terms kind of you know
the way that people express frustration
or happiness tend to apply to all
domains and it's the nouns and the
aspects that that change after eighth
year they know they'll maybe be
referring to the carburetor and one
domain or the waitstaff in another
domain so I think the nice thing about
the way that we approach tenem analysis
is that lexicon that tells us what's
positive and what's negative should be
universal once we've developed it and
then it would just be a matter of I
don't know if there's different words
that you might use to us what I'm having
is this is an engine I mean like maybe
positive is a very specific
was coaching put your angel eyes ever
like see like you're Pete you you're
yeah like Pete work or something like
that a certain rage I was really happy
about yeah yeah yeah yeah that's really
interesting I think we I could imagine
using some of the tools that we have to
build something like that but as I said
most of our work is focused on the
broadest possible applications so yeah I
think we have the tools to build that
it's not necessarily an active active
direction that we're going right now
oh thanks all right do you ever think
about incorporating videos and their
content into Google squared like you
know if I wanted to look up fitness
videos yeah all right the first time we
showed Google squared before we released
it to our vice-president here in New
York Steve Feldman we let off with the
query rollercoasters which was kind of
cool because you can show the heights
and top speeds and do that and his
suggestion was to add videos he's like
would it be cool if you could like watch
a video of each coaster going down it's
me and drop ok yes exactly so yeah I
think that totally I mean I think the
main issue they would be getting a video
that's on topic right I mean like we had
a lot we still struggle with square
there's a roller coaster called mantis
and it's really tough for us to get a
picture of the roller coaster and not a
picture of the book so I think video has
even fewer signals would be my intuition
and we'd have to work a little bit to
make sure they're on topic but I think
we could probably do it yeah a little
bit I discussed with you earlier but
just to give the background that similar
application I've been working based on
social networking and one thing I just
wanted to check with you how did you
guys improve the intelligence from 50 to
60 or the satisfaction because we found
working on this that improving it
manually very specific to particular
queries was pretty tedious job yeah and
not every time relevant because if it's
a time sensitive query somebody is
looking for a game on a particular day
and improving that quarry one makes
sense right
so we try very very hard and you
probably got the gist during the talk
not to do this kind of manual one-off
he spoke kind of fixes right we have a
little bit of white listing and black
listing we try to avoid you know racial
slurs and things like that it's you know
we want to make sure to get those things
right but in general we're really trying
to do the most general possible thing so
yes about how we went from fifty and
sixty percent we've actually gone
further than that
since then it's through you know looking
at the broadest class of errors that we
might make and attacking it in the
broadest possible way so for instance
maybe specific approval one at that
point but we did things like add a new
general extractor you know we added
something that looked we were only
looking at tabled and now we're going to
look at those clear prompts you know
attribute : value and try to find more
data that supports that we might look at
a new signal into the mix like user
refinements on google comm that might
give us more of a signal as to what's
relevant and what's not so typically
when we make those kinds of improvements
we try to look at a very broad case and
we cannot do any manual fixes with the
exception of these kinds of like you
know worst-case scenario kinds of black
lists and things like that and as far as
the freshness issue this is certainly
things that we get wrong question
answering Amit Singhal our Google fellow
for search submitted that we were
getting the I think was the Prime
Minister of India or something like that
rumor was today the the area we had
anyway what we were getting some
question wrong he sent it to us and it
was it turned out that he had just
gotten reelected and the new table that
we're pushing is going to fix that so
we'll fix things kind of only in a
general way but but time-sensitive stuff
is still something we struggle with sure
yeah alright thank you okay Square to
deal with life critical or
mission-critical questions like my dog
just swallow rat poisoning what should I
do
Google squared wouldn't do with it at
all I hope Google squared is mostly
about categorical queries so I mean like
maybe would answer ways to kill my dog
or something like that but maybe not but
you know google.com is actually probably
a much better source of information for
that and there I think we're going to
try to angry at the wisdom of the web
hopefully would go to answers Cora
I think would be a great site probably
would be a great site to answer
like that but it's not really domain for
squared I know it's it's an interesting
point we might want to she had the
question was whether we should turn
users away if they ask something like
that one problem is that it's hard to
identify those you know as you know as
we were talking about before like
classifying queries like that can be
difficult maybe if there's certain very
very obvious buzzwords we could build a
classifier these kinds of things are
very tough you want them to be 100%
precise and 100 percent coverage and
that just doesn't exist in these kinds
of domains yeah my hope would be that
user would never find themself on Google
squared for a question like that to be
completely honest yeah all right going
forward how do you see Google squared
integrating with mainstream web search
or I guess in other words if you see
that a user has searched for something
that could be better represented by
Google squared do you ever see do you
ever think you would see a squared page
showing up in place of URLs yeah I mean
it's certainly something that we would
love to do as I mentioned we're taking
parts of squared that we believe to be
really useful like the question is doing
that and we've already launched that
we're pushing that on google comm we're
taking other pieces and they're kind of
an active development is it have to be
re kind of restructured and we thought
because they don't quite make sense in
exactly the same way we think that
they're very valuable to answer these
kinds of queries as far as Google
squared knowing enough to kind of take
over the whole page when you type in you
know hybrid SUV it's a little bit of a
stretch but I can certainly imagine at
least being kind of there being a way to
get to it immediately from your search
results but Google is mostly about
getting to the destination of where you
can best serve your information so and
do you have any figures on how many
google web searches could be represented
by squared it is it is a common tab
searches come in that you would say you
know let's show this in a grid that's a
good question I don't have current
figures I do know that they're fairly
frequent you know I don't know I want to
say exactly but you know they're you
know they're not it's a non-trivial
portion of our our query screen right
just had the user set direction is like
you're aligned with
the precision then you can measure the
user satisfaction by the Khedira so do a
survey for the how do you how do you
measure all in precision yeah so measure
your own precision
this is 'le we do kind of a cross search
quality basically it's a it's a we build
up a set of queries that we care about
we build up a set of answers we believe
are correct and then we kind of measure
we automatic and query our system and
check whether or not we're getting
things right and we measure precision of
that and recall that just like no any
other kind of machine learning tasks so
that's not good wait system before I'm
sorry that's not go existing way before
the Google squared
oh sure yeah yeah we do this for all
types of quality problems and the other
question is Google Square is like a good
tool for maybe academic research yeah
yeah in fact I says I mentioned that's
one of our key demographics is school
teachers and so on but yeah I mean that
I was definitely the book report or you
know kind of research report domain is
one that we excel at yeah exactly that
you have images images of like embodying
the meanest what about the
other read a PDF those sure yeah I mean
as I said if we can get people the
information that would help them we do
pull facts from PDFs because they're
part of the Google index but surfacing
them yeah that might be an interesting</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>