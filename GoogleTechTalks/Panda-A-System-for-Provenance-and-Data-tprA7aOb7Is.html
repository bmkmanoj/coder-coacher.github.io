<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Panda: A System for Provenance and Data | Coder Coacher - Coaching Coders</title><meta content="Panda: A System for Provenance and Data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Panda: A System for Provenance and Data</b></h2><h5 class="post__date">2012-12-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tprA7aOb7Is" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and Jennifer is the athletic Jones
professor and chair of the department of
computer science at Stanford University
I hear research interests are with data
management systems of all non
traditional aspects of data management
systems I see the member of the National
Academy of Engineering and the american
academy of arts sciences and he has won
many honors and awards and today she's
going to talk about the system for
provenance at data which they developed
at stanford it is called panda and
jennifer hey thank you very much I guess
I'll introduce my former student and the
issues on the screen here at the front
hi anish were all looking at you
anisha's worked at Google for a while
he's um he was my PhD student he's now
working in New York okay so thank you
photo for the introduction and thank you
for having me today I like talks that
are interactive so at any time you can
ask me a question even here on the title
slide although I doubt there's much to
ask panda stands for provenance and data
and so I'm just going to be talking
generally about work we've been doing in
the area of provenance recently so i'm
going to start with a example it's
obviously a fabricated example just to
motivate the use of provenance in a
setting of what we call data oriented
workflows so let's imagine that we have
a workflow where we're going to take in
some customer information and some
buying information catalog and predict
what we're going what customers are
going to buy or what the demand for our
products are going to be so we'll start
out with some customer lists and they
might be of varying types of customer
lists we might have gotten them from
multiple places so the first thing we'll
do is send those customer lists through
some kind of deduplication operator
after the deduplication we're going to
canonicalize the lists in some form and
we're going to take the ones that look
like they came from Europe and
canonicalize them in one way if it looks
like they came from the US will do it
another way this is just to demonstrate
different shape of workflows once
they're in the
canonical form will Union them back
together that's when we'll throw in
information about our catalog and some
of the buying patterns that we've seen
in order to make predictions about what
the customers from those customer lists
are going to buy will aggregate together
our predictions to determine what type
of sales we'll see ok and we after we
run this on our customer list we
discover that we expect to sell a lot of
cowboy hats ok so I'm the analyst I've
done this that run this workflow and I'm
pretty surprised that I'm going to sell
a lot of cowboy hats because that's not
my customer profile that's not my
demographic so now this is where
Providence is going to come into play if
I just ran the workflow I finished it
and I have my output data and
everything's thrown away I can't really
figure out anything about why I have
cowboy hats so now i'm going to use
provenance to trace backwards and see
what happened so i will do backward
tracing i'll send that cowboy hat
prediction back one step so back through
my aggregator and i'll discover this is
obviously there would be a large number
in reality but I discover i have three
customers which is a lot who want to buy
cowboy hats who I predictable by cowboy
hats with a relatively high probability
now that still seems odd to me because I
really didn't think that I had a lot of
people from the south for example who
might buy cowboy hat so I have to keep
tracing backwards again hopefully I
capture the provenance in order to do
that so I go back and I find that these
three customers are from Paris Texas so
some of you can probably already see
what's going to happen if I keep tracing
backwards I will send it back and I'll
discover that the data that I'm looking
at went through that us a trance the USA
canonicalize are there and the data
actually this is what it looked like
before it went through there so I'm
starting to get suspicious that maybe i
sent the data through the wrong way
through my workflow and i did that by
tracing back with providence so i go
back entirely to find out where those
customers came from obviously those
customers were from Paris France and so
I'm being came from this customer list
so i can either fix that customer list
to say that these people are from paris
france or i could fix this splitter
which sent them
accidentally threw the USA and that's
what added Paris Texas to the data okay
everybody follow that okay so after I so
I've used Providence to figure out what
rent rent wrong this is a use of
Providence for debugging and after I
fixed the data or it fixed either the
customer list or the splitter then i can
send the data back through and so now
this is using forward propagation will
also see how providence can be used for
that so i send the data back through
this time it goes through europe the
europe canonicalize ER instead of the US
and instead of at making it Paris Texas
it makes it Paris France and then we
keep going goes back through and out
comes a beret instead of a cowboy hat
okay so that's just a toy motivating
example of how we can use provenance to
explore what happened when we sent data
through a workflow and that's sort of
the whole topic of the talk okay so
let's go back and talk about provenance
a little bit i'm going to give you a
definition but different people have
definitions very broadly provenance is
where data came from how it was well
where it came from in the first place
how it was derived how we manipulated it
combined it and so on and also
potentially how its evolved over time
and there's plenty of uses of providence
and probably everybody's sitting here
has some use of providence in their mind
probably a bunch of different ones
broadly speaking you can use provenance
for explanation so drilling down on data
if you have a bunch of derived data you
want to find the source of the data you
know it's a deeper understanding of how
the data was derived debugging is a
particularly common use as in the
example that I showed you where we have
data that's been processed by a complex
workflow it looks wrong we want to find
out why is it wrong is it because we
made a mistake in the workflow itself is
it because the original data had bugs in
it we can also use provenance to figure
out how errors propagate so if you have
we find an error if you say get some
derived data you go back to the source
data you find an error Providence can
help you find out exactly how that
errors spread through other data that
other processing that you did on the
same data and provenance can be used for
auditing and it can also be used for
recomputation so prob
is the key in some sense to doing
incremental processing in this
environment okay application domains
well sales prediction workflows but
again that was a toy example I think
scientific data workflows are one of the
original drivers for Providence the
scientists were recognized decades ago
that they needed provenance they do a
lot of gathering of raw data and then a
lot of processing at audit layers and
layers of processing they look at the
outcome it doesn't look right they need
to find out what happened where the the
source of that was and that also
includes by the way human curated data
and i made this slide before
crowdsourcing became popular but humans
getting involved in the workflow really
complicates the problem of provenance of
course and again evolving versions of
data is pretty relevant for scientists
but pretty much any analytical pipeline
can make use of provenance ETL processes
I know information extraction pipelines
are used here and in other companies so
that's just some examples okay now let
me say third time's a charm first of all
one thing that may be going through your
mind is isn't provenance the same thing
as lineage so people talk about lineage
a lot now I'm talking to about
Providence and for those who know some
of the work I've done at Stanford over
the last couple decades you might ask G
haven't you worked on this problem
before okay so first of all yeah
provenance is pretty much the same less
lineage some people think of lineages
maybe more smaller scale or something
but let's just call those synonyms
provenance and limit lineage and
provenance seems to be the term people
feel like using these days and have I
worked on it before absolutely actually
this is the third time I've visited the
provenance problem so the first and
that's why third time's a charm maybe
okay so the first time I worked on it
was in a data warehousing project this
was long ago I don't know 15 years ago
maybe something like that so in that
project we this was back when data
warehousing was kind of a new idea the
whole idea of having small operational
data sources and then bringing that data
into a big warehouse for analysis was at
some
time relatively new so we did research
in that area and the research we did
that had to do with Providence or
lineage was simply if you define your
big warehouse as a relational view over
the little relational operational
sources then then you might be
interested in your warehouse when you're
looking at some data of finding the
origins of the data in the operational
sources and there were some actually
quite interesting problems that we
worked on there we worked on you know
caching issues what exactly do you have
to cash in the warehouse so you can go
back to the original sources and so on
so so various system issues also some
formal foundations we also looked a
little bit at that time at ETL pipelines
to extract transform load pipelines
because those were used in warehousing
as well okay so that was a PhD thesis or
two quite a while ago much more recently
but still a while ago was the trio
project and the trio project was called
trio because it had three components all
working together in one system data
uncertainty and lineage and those three
compel the uncertainty and lineage sort
of looked orthogonal to each other when
we started and i'm still not completely
sure why i started a project with those
two components but it became very clear
from a technical perspective must have
just been some intuition or something or
random luck that actually lineage is
very important when you're trying to
manage uncertain data because when you
bring together data that's uncertain if
you want to evaluate the certainty or
the confidence of that data you actually
have to go back and look at the
confidence of where it came from and I'm
this is not a talk about trio but in
that proj and I but I'm happy to talk
about it offline but in that project we
had a pretty narrow definition of what
lineage or Providence was but that
definition that use of lineage was very
key to the system we built for managing
uncertain data but it was mostly in
support of uncertainty so that was time
one then there was time to okay so now
here's the third time's a charm sort of
which is the Panda project and that's
very recent although we're kind of
winding down or have wound down but
that's what I'm going to tell you about
today in this project we've been
focusing particularly on the
things called data-oriented workflows
not on uncertain data not on data
warehouses at like the example I showed
you but I'll be a little more formal
about it and then we try to explore some
of the sort of maybe more fundamental
aspects of Providence fine-grained
Providence versus coarse grain
provenance Providence defined at a
logical level versus a physical level
what to do when some of the
transformations in your workflow have a
known structure you know what they're
doing what to do when you don't know
what they're doing all you see is the
input data and the output data so that's
the types of things that we explored in
the most recent foray into this area and
we worked on foundations we worked on
algorithms we built a system a couple of
couple of systems three systems actually
and I'll touch on all of those today
okay but actually I'm going to make a
bold statement about provenance which is
no time is ever going to be a charm okay
actually a fairly strong opinion about
the area of data provenance which is
that you're never going to solve it's an
unsolvable problem okay there's never
going to be a general framework so I've
tried it three times I tried one area
another another I don't think there's
ever going to be a solution it's an
impossible dream so if you want to leave
now everybody want to go nope okay so it
is an impossible dream I just as I look
at the different applications for
Providence different applications need
different things I've had so many people
come to me and say I need provenance and
I say well what do you think Providence
is and every time they say something
different so i think that the common
denominator is actually quite small okay
so actually that's fine for academics
doesn't bother me very much there's lots
of relevant research to do as you can
see I've had three waves of PhD theses
in this area so no problem that's great
but it is more of a problem if somebody
thinks they're going to build the
Providence system because I actually
personally think they're not going to so
again you can leave if you want I've
said exactly the same thing about
another research area for the last two
decades exactly the same i could just
replace the word Providence with
something else does anybody know what it
is I knew it loan would know yes data
integration
he says the data integration problem
will never be soft thank you yep I have
given a variety of talks where I say I
really don't think data integration is
ever a solvable problem the common
denominator is just way too small
everybody's view of what data
integration is is different from
everybody else's but actually again in
the academic setting no problem tons of
great research to do in data integration
okay so that's my introduction so for
the remainder of the talk i'm going to
give some fundamentals i'm going to talk
about capturing providence exploiting
providence some of the actual concrete
results we've gotten and follow-on work
any questions or discussions at this
point in the talk okay looks like we
only lost one person who's decided
they're not going to solve this problem
so yes right exactly yes and we solved a
category way back with our data
warehouse in that category was if you
have a relational view of some
relational data sources how do you cache
data to make Providence efficient great
no problem when you have that but that's
not actually most people's problem
unfortunately it's a great academic
problem but it's not the main real world
problem second time we use provenance
behind the scenes to build an uncertain
data base great that was actually
Providence was fantastic but we weren't
solving anyone's provenance problem we
were solving their uncertain data
problem and so you'll see I'm going to
tell you some settings where we have
some results but are they actually what
people need not sure my it's what some
people need but there's I don't think
there's anything that's going to solve
even fifty percent of the provenance
problems in one system yeah yes
if i take uncertainty out of trio and I
just have data and lineage oh but I'm
doing something that's unrelated and so
in some sense what I'm doing what this
project is going to tell you about is
almost unrelated to how lineage was used
in the trio system that was a very
specific use where you were running
relational queries and keeping pointers
to wear things came from so that you
could resolve confidence values you ask
you if I months excited about uncertain
data of course why wouldn't I be
uncertain data is everywhere uncertain
date is a great problem but I never work
on anything for too long right so it's
time to move on to something new but
that's not that's uncertain data is
extremely important from and actually
more solvable than provenance in my
opinion I think you can that uncertainty
you can you know sort of build a system
that has uncertainty in it that will
solve some reasonable fraction of
people's problems where I still feel a
single system for Providence is only
going to solve you know a small fraction
so I think uncertainty is a more
contained problem that can be defined
and solved more in a more contained way
yes
do I know nope not in anything you're
going to see I'm not going to quantify
why data is wrong it's just wrong yeah
okay so why don't I move forward and
tell you a little bit about what we've
done ok so for fundamentals I'm going to
define data oriented workflows i'm going
to give you extremely simple underlying
providence model and then i'm going to
talk about capturing provenance and for
exploiting provenance i'm going to talk
about different things you do with it
sort of like I showed you back we're
tracing forward tracing forward
propagation also define refresh and I'll
talk a little bit about ad hoc queries
that involve provenance so that's all
introductory material then I'll talk
about the concrete things that we've
done and follow-on work okay so let me
define a data oriented workflow so
data-oriented workflow is a graph let's
make it a directed acyclic graph of what
we call processing nodes each of those
round things as a processing node and
data sets move through on the edges the
reason those are different colors is
because some of them might be opaque
some of them might know exactly what
they're doing so and I'll talk about
that in a little more detail so a couple
things to assume one thing I'm going to
assume this is a you know is relatively
strong assumption is that we define the
workflow statically so the workflow kind
of sits there and then we send data
through in batches sometimes there are
environments and this is a good example
of you know different people need
different things sometimes our
environments where the workflow is sort
of created as the data goes through the
workflow it's more dynamic but I'm going
to assume that this is just a statically
defined workflow don't assume anything
though about the data sets or the
processing nodes I'm going to try to be
very general about those so the goal is
that when we know something about how
data is being processed we will try to
exploit that in Providence okay but if
we don't know much about what they're
doing we still want to fall back so the
whole system kind of works that's the
basic idea so what kind of things would
we know about a processing node well we
might know that it's doing a relational
operation or some query in a language
that we can analyze we might know that
it's monotonic so that's a little
broader property monotonic
in case people don't know if you put in
a data set and you get output then if
you put in a superset you'll get a super
set as output so it's just a nice
property and a lot of times when
something is monotonic you can have
better formal foundations we might know
that it's a one many operator so the
when you put one thing in many things
come out or many one goes the other way
around we might know that it's a map
function or reduce function for example
so those are the types of things we
would know about our nodes and the
general principle is the more we can say
about our notes the finer grained we can
be about the provenance that we collect
so if we don't know anything about what
a note is doing we put in a big data set
another data set comes out all we can
say is the whole output set came from
the whole input set but if we know sort
of more details about what it's doing
then we can make more fine-grain claims
about the provenance and I'll just give
you some examples right now from my
workflow so the known relational
operators in this workflow are the Union
operator and the aggregation operator so
those could be written in a relational
language we know that our deduplication
operator is a many one operator so I'm
going to find copies or near copies of
customer records and I'm going to merge
them so each group that gets merged into
record makes it many one but it's
non-monotonic I if I add more data I'm
not necessarily going to add things to
my answer might change my answer in
unpredictable ways okay the split and
the canonical izing transformations are
very simple they're just 11
transformations or monotonic so those
are good ones maybe the predict one we
don't know what it's doing at all that
might be just completely opaque okay so
our idea is to try to get provenance
working with these known properties and
I'm going to use a very simple
provenance model extremely simple it's
just going to say that underneath
provenance is just a mapping between
input data elements and output data
elements okay so that's what I'm going
to sue I'm not always going to know what
it is but that's going to be my
underlying model so that's a very
simplified in some sense and this is
another example where some of you might
say Lou that's not what I needed I
needed something completely different
but this is what I'm going to work with
it I'm going to call that data oriented
provenance and it's sufficient for many
purposes I put most here sure why not
okay most of them ones that I think
about okay so the idea then is so now
I've defined provenance for you I've
motivated that different transformations
have different properties the idea is
that as the data is processed through
one of these workflows somehow those
processing nodes that are going to
provide the provenance along with the
output they're going to have to do
something if they don't do anything that
I just get my output at the end and I
don't have much much to do so broadly
speaking there's two ways of capturing
provenance one is eager and the other is
lazy so eager says as the data moves
through one of these processing those
I'm going to generate the provenance I'm
just going to generate physically that
mapping between input and output
elements maybe I'll generate something a
little broader and we'll talk about that
the other possibility is that you have a
tracing procedure so the processing node
and in fact this might not even have to
do anything specifically when the data
goes through has a way of tracing data
back and basically has an inverse okay
so those are the end the inverse then
would be you'd be probably doing most of
your work at the time that you're
tracing providence we're eager you're
doing most of the work at the time you
send the data through the workflow okay
all right so as an example how
provenance might be captured in our
workflow if we look at our relational
operators the Union and the aggregation
there's been a lot of work on relational
provenance including that part one of
the three times I've looked at it so it
can be done you automatically you can
take sequel and you can automatically
generate Providence for most sequel
queries you could do it eagerly you can
do it lazily both work okay you can take
a sequel query and essentially inverted
in many cases okay for deduplication
it's kind of an interesting one so think
about these entity resolution algorithms
they go through a big set they might
have sort of complicated things that
they do eventually they decide that
these different blocks of Records
represent the same entity and they merge
them and so it's pretty easy at the time
that you are doing a deduplication for
most of them to capture which block of
Records who decided with the same and
got merged into a single record you can
just have like backward pointers or
something like that it would be much
hard to harder to do that lazily to take
output record it depends on the
algorithm if they take an output record
and find the ones that you decided to
merge so this is an example where it's
pretty good to do things eagerly not
that much extra work would be hard to
invert the 11 transformations are very
easy and it's one to one very often
there's a key that you can just trace
back and everything works well you can
do it eagerly or lazily and then the
prediction of course depends entirely on
what's going on inside there the worst
case is that for some of these operators
or these processing nodes you don't have
fine grain provenance but that's kind of
a problem because if at some point when
you're tracing back all you can say is
the provenance is the entire input set
then you've lost anything useful okay so
that's it for capture I know I'm still
kind of general any questions at this
point all right so let's suppose now
we've captured provenance yep yeah so
they're not all invertible the simple
queries are invertible that it's there's
also plenty of work on how to augment
queries to capture what you need and so
you would do that with the deduplication
you can actually you can just sort of
write out identifies you there's known
ways to capture what you need that's it
let's put it that way often yes that if
you're doing eager so and for
deduplication is a good example where
you probably wanted to do it eagerly
then you will modify your processing to
write off some information about
provenance yeah and so it's known for
sequel for most of sequel this for
standard sequel queries it's known how
to do it both yearly and lazily how to
augment the query to write off
Providence or to invert the query when
it's possible and there's been a lot of
work in that area okay so let's suppose
that we have our provenance now what do
we want to do with it what are the basic
operations for provenance so there's
backward tracing and forward tracing
that's what people think of most so
backward tracing
is exactly what I showed you I want to
know where that cowboy hat record came
from so I want to trace provenance
backwards from the output into the input
forward tracing is the other direction
and Providence is useful for forward
tracing as well so I have a particular
customer and I want to know which sales
predictions she had an effect on so I
want to take data here at the front and
figure out how it affected things in the
back okay forward propagation says maybe
I changed some data at the front and I
want to propagate those changes through
but I don't want to rerun the entire
workflow and Providence can be very
useful for that I mentioned that already
for incremental processing refresh is
kind of a combination of backward
tracing and forward forward propagation
refresh says let me look at my output
and let me say there's one one element
that I want to refresh I want to read
this isn't a great example but maybe I
want to refresh all the sales of the
sales predictions for hats so some small
subset Providence can be very helpful to
refresh just a portion of the workflow
and again that's basically tracing that
data back to its origins which the
origins might have changed and then
propagating the new values through it
I'm going to go into that particular
operation in some detail in one setting
you might also want to ask queries that
involve providence and i'll just give
some examples just a sort of declarative
query says how many people from each
country contributed to the cowboy hat
prediction so now I'm combining queries
over the data with queries about the
provenance itself other examples which
customer lists contributed to the most
to the most to the top 100 predicted
items so who are what are my most
valuable customers again combining the
data with how the data went through the
workflow for a specific customer list
which items have higher demand than for
the entire customer set so what that's
you know who do i advertise to or which
customers is so slightly different this
talks about how things went through the
workflow which customers have more
duplication the ones that went through
USA are the ones that went through
Europe for example so there's various
goals for a query language you'd like it
to be declarative you'd like to have ad
hoc queries like a database system
combined providence day
data and data and be optimized about and
I'll just say right now we never got to
that it looks like fun stuff but it's
we're kind of winding down and so that's
an area that I'm not going to say much
more about okay so let me just talk
about the concrete results that I am
going to tell you about I'm going to
actually give you three different
settings and you'll you'll see how they
relate one of them is where we express
Providence's predicates and this is
probably the most general way of
expressing Providence that I'm going to
tell you about today and we this was the
work we did here was actually motivated
by looking specifically at that refresh
problem saying I have output I have an
output element I will that might be out
of date based on changes to the input
how do i use providence to trace back
and then push those four words there's
going to be a very hard problem and so
that we're going to use Providence's
predicate for that problem and after all
of our initial the first prototype that
we built then I'm going to talk about
logical provenance and this actually
turns out to be a nice way to specify
provenance in the settings where it
works doesn't always work and that drove
the second iteration of our prototype
and the third thing I'm going to talk
about is what we call generalized map
and reduce workflows and this is a data
oriented workflow like I showed you
except every node is either a map
function or reduce function so that's
even more specialized in all cases we
focus on defining the logical
foundations and we focused on the
operations of tracing back we're tracing
forward tracing and so on and did not
work on the ad hoc queries and
optimizations okay i will say that what
i'm going to talk about goes from most
general to least general okay so
remember i said that when things you
know we know very specific things we can
capture fine-grained provenance and
you'll see that so the predicates are
the most expressive provenance language
I'm going to talk about logical
provenance is a little less expressive
and then these generalized map and
reduce workflows are the least
expressive but then we can do the most
okay so everybody good any questions
alright let's talk about provenance
predicate snow and this is a kind of
formal model you'll see so we're back to
our data oriented workflow with the
different colored nodes and we're let's
look at just one node and let's talk
about what happens if that one node then
we'll talk about workflows so we have as
so we start with our data here things go
through at this point we have some input
I to this black node here and let's
suppose that some element came out oh so
eyes are input 0 is our output and 0 is
one element of our output okay so what
I'm going to say in this model is that
we're going to have some way of
capturing provenance that describes the
provenance as a predicate that predicate
is P here we're going to associate with
0 a predicate p that allows us to find
the provenance of that element in the
input in other words the provenance of
the output is apply that predicate this
is the relational algebra selection
operator but it remembers that from
college hope we apply that predicate to
the input and that gives us the output
okay this is a model in the general case
too we have our whole output every
output element is going to be have
associated a predicate that gives us the
provenance of that output element ok so
the worst case we can always have a
provenance predicate for every output
element the worst case is that that
predicate is the predicate true that
says that I really don't know where this
output element came from when I apply
the predicate true i'm going to get the
entire input set so all I can say is
that entire input definitely had
something to do with this output I can't
be more specific than that ok this is
you have to think of this as a formalism
to be instantiated ok so this is the
underlying formal model that we have the
idea of these predicates very often the
predicates can have a compact
representation so for output the
predicate that can just be for example
select the input item that has the same
key as the output item ok then you don't
have to write a separate predicate for
everyone and this logical provenance i'm
going to show you effectively does that
gay people are starting to look blank
every sort of ok more or less and
sometimes you can prep generate those
predicates automatically ok but this is
going to be
formulism I'm going to use to study the
Refresh problem we're going into this
good news for academics part of the talk
now where we get to work on fun problems
okay yes that's the extreme case and
that's absolutely allowed yep find the
record on the input that has ID 9 36 24
or whatever yep yes it doesn't tell
anything about the transformation that's
right so when I define provenance I
defined it as a mapping from input
elements to output elements we're
focusing more on the data than the
transformation itself I mean we know it
went through the transformation that's
partly also there's the various hidden I
mean these various assumptions I made
one is that we have a static workflow so
we know the transformation I can tell
you that it went through that black one
there because that's static yeah okay
and so we can give a natural recursive
definition if we want the r if we want
the provenance of an element here all
the way at the front we apply its
predicate to get elements here and then
we trace them backwards all everything
I'm going to do is going to have this
natural recursive definition of
Providence where you go backwards
through the Providence yes yeah so this
does I think right there thank you
extends to multiple inputs and outputs
sets yeah you have to have a predicate
for each one you have to have separate
predicates four separate inputs that's
the way we model it yeah and that
generally is expressive and I and I take
it you have to run into the worst case
quite a bit right if you have
aggregation well aggregate a typical
group by aggregation you can just go to
the group that it produced so it's not
that bad right but sometimes things get
tricky if you have joins and you didn't
save the joint attribute for example
that's sorted so think of doing a join
on an attribute but you don't save that
in the output that can actually be kind
of tricky to find the Providence in that
case yeah
um the function goes this way the
predicate finds the data but yes if you
can invert the function that's more like
a tracing procedure if this isn't it if
this is running a function and that
function is invertible then you could
just have this backward tracing the
thing is it's not always invertible
say
yes it is at the same time but you have
yeah you can have to see an example to
see how it works but if you have
multiple inputs I mean if they're doing
a union or something it's pretty easy it
depends what they're doing with the
multiple inputs yeah now we can talk
about that a little more off like that
gets a little hairy we're gonna get to
that that's next you're asking all of my
you're ahead of me here ok so you so if
you look back in the literature at the
rather vast literature of provenance in
different ways of expressing it most of
it could can be captured with this
language with this thing of it is a
formalism ok so it's going to come to
what you were talking about but now
let's talk about the Refresh problem so
the Refresh problem says I have some
output elements and I want and my input
might have changed i don't want to run
the whole workflow again but i want to
refresh those particular output elements
and this I like the Refresh operator
because it stresses provenance it says I
need to be able to trace backwards but I
need to also be able to propagate
forwards and actually tracing backwards
and getting the data that will propagate
forward in the right way sort of tells
you you had the right provenance it's a
little hard to explain but so let's look
at what we would do with refresh let's
say that we just want to refresh an
element and output through one
processing note ok so we have our input
we have our output and we want to take
oh I whatever it is 0 for and we say I
want to find out whether the value of 0
for has changed ok so we've got a new
input I want to see if the value has
changed i'm going to backward trace by
applying my predicate associated with my
output element which is pi/2 the new
input ok that's what I'm going to do and
then I'm going to propagate that new
input so that that's going to give me I
star some subset of the input ok so i
probably my predicate I get a subset the
input i'm going to send that input
through the through the processing node
and that's going to give me my refreshed
value okay that's very simple very
straightforward example I am doing my
egg regev
prediction that my beret is going to
have high sales here's my predicate my
predicate says the provenance of my
beret prediction is going to be all
input items that are brazed so this is
your first example of a province
predicate pretty simple right okay and
now I've updated my input and I want to
read fresh that so I apply my predicate
item equals beret to the new input
that's going to get me all the new
values for beret I send it through and I
might have a new prediction okay that's
how a refresh works very simple okay the
question is does it always work actually
doesn't always work you have to have
certain properties for it to work period
and I'm going to tell you those but
we're not going to go through them in
any detail the others does it make sense
it actually does generally make sense
but it depends on certain properties of
the processing notes and the Providence
itself we can sort of generalize the
whole idea to go through an entire
workflow so we have output here and we
want to go all the way back to our
original inputs which might have changed
so i do recursive backward tracing and
then I send it all the way through the
entire workflow okay and then the
question is does this work and does it
make sense and is it efficient actually
maybe it's not efficient I'm not going
to answer those questions for you today
entirely I can refer you to a paper it
again depends on properties of the
workflow i'm going to give you an adjust
one second i'm going to give you an
example where it doesn't work then i'm
going to state the properties for you
and we're going to leave this topic yeah
yeah yeah that actually there's been a
small amount of work in that area I
think it's a great area the question is
I look at my output and I expect
something to have been there and it's
not and I want to ask why is this not
there there's been maybe one or two
papers on that topic in a very
restricted setting of relational queries
I think it's a great topic I haven't
worked on it yep okay so let me just
give you an example of where this
refresh does not work and then I'm going
to tell you the properties that you need
for it to work so it's not going to work
for every workflow so let's suppose that
we have a very small sales workflow here
we have people who live in a city and
they sell things in Euros we convert
euros to dollars and then we sum up for
the cities so we have these two people
who sold the amelie and Pierre from
Paris each sold 10 euros we convert it
to dollars at the 1.3 rate and then we
do a sum oh sorry let me add the
provenance predicates here we always
have to remember our predicate so the
first couple came from the first couple
has the predicate person equals Amelie
so that predicate applied to this first
table will give us the first couple and
that'll set up the provenance from the
first couple in the first table to the
first one in the second table okay and
similarly we've got our predicate Pierre
here to find the provenance of this
couple back in the original data ok now
i'm going to send those and i see that
for paris i sold twenty six dollars and
the provenance predicate here pretty
simple city equals Paris so this Paris
couple to find its provenance I go back
here and I do select city equals Paris
ok so now you can see how these
predicates work it's all pretty simple
ok so now I've updated my data so Amelie
went out and sold some more things and I
know my date has been updated so i
decide to refresh my output couple so
i'm going to follow my procedure and
backward tracing and forward propagation
so I traced backwards by applying my
city equals Paris predicate and I get
those two tuples there ok i trace those
backwards so the first one's going to
give me Amelie the second one is going
to give me Pierre ok so I've got these
now and then I'm ready to forward prop
gate so I just send those two tuples
only those through the workflow that
updates the sales here this 26 value and
it updates the total and everything's
great okay here's a simple example where
it doesn't work let's suppose now that
we have a new salesperson marine Maurice
a good salesperson and so we have new
data we want to refresh our output so we
asked to refresh the tupple we follow
the provenance predicate and we get the
two tuples for Amelie and Pierre you can
see what's going to happen we follow
those provenance predicates and we get
the first two tuples here we send those
through the workflow and nothing changed
ok so the problem is that we missed this
third tuple here and probably a lot of
you are thinking of all kinds of
solutions to this problem and there are
all kinds of solutions to this problem
well we focus on is addressing when this
problem doesn't arise actually ok so
what we should have done here is we
should have added Marie and then we
would have gotten the right answer ok so
I'm just going to tell you with screen
snapshots from our paper here are the
properties you need in order to
guarantee that refresh works and that's
pretty much what we studied for that
problem was how how to do it efficiently
when you can do it and the properties
you need in order to do it ok all right
we built a system based on this I'll
describe the system very briefly we
built it using the sequel light library
we put our data in there we had our
workflow in there we stored provenance
predicates actually brushed under the
rug that you need predicates that go in
the other direction to actually didn't
tell you about that but it's not adding
a huge amount and then the user can
create tables the user creates workflows
by defining transformations currently
the system has sequel transformations or
Python transformations you can backward
trace you can forward trace you can
refresh and that's the basic panda
version 1 ok that's part one I'm now
going to know go into part two logical
provenance any question about the
predicates ok if you're interested in
those properties you can get the slides
or look at the paper ok logical
provenance is actually this of the most
recent work we've done
provenance instead of specifying the
provenance at the data element level
specifies it at the transformation level
so it's much more compact okay just one
specification for the transformation no
matter how big the data it is and it
turns out that provident specification
is often equivalent to specifying it at
the data level so underneath the model
is still this mapping from input to
output but we're going to specify it in
a logical level okay and physical
provenance of course would be specials
specified as a data element level so
what we're going to use for our logical
provenance specification language is
what we call attribute mappings and
filters and it's pretty simple stuff and
pretty effective as I said okay so let
me explain what an attribute mapping is
and we've seen cases already you might
have even thought of this already where
we could have used this mapping between
input and output attributes to specify
our provenance okay so let's say again
we have our input set and we have our
output set and let's say that we have an
attribute and this they don't have to be
relational as long as they have some
different you know record served fields
or something that we can identify so
this one has a attribute a this one has
an attribute be an attribute mapping
says there is a correspondence between a
and B usually it's an equality
correspondence but it doesn't have to be
simple case I preserve the key right
that's a very simple case but very very
common so we want some way to capture
that even if you know it's not going to
happen in all of our processing nodes if
it happens in any we'd like to make that
be extremely simple case so what this
mapping says is that in this sort of
informal if we have an output element 0
then the data that contributes to 0 is
the data in the input that has a value
that is equal to ohs b value okay so if
I have you know a bunch of hats in the
output then I'm off it came from all the
hats in the input for example okay very
straightforward a more more generally if
you have an attribute mapping but it
says if you like formal stuff is for
every possible value of x and the bees
the if you take the a values that have
the a's that have that x value take that
portion of the input and you send it
through the processing node that will
give you that will contain actually the
output that has experts b-value kind of
think of as partition take the input
partition it by its a values send those
things separately and you'll get the
output elements whose B values are the
same as the a values okay that's the way
to think of it okay and I've specified
that just all I wrote was this I no
predicate snow pointers know anything
okay so at a simple example if we have
our customer and item the probability of
buying that item we send it through the
aggregator this from my original example
then we have an attribute mapping that
the output items this is aggregated on
items now go to the input items very
common if you have a sequel query with a
group by any of the group by attributes
are in your Select clause that those
group by attributes are your attribute
mappings works beautifully ok so we've
proven various properties of these
mappings you can merge multiple mappings
you can split mappings they're
transitive all kinds of nice things like
that the worst case remember we have
that worst case provenance predicate p
equals true the worst case here is your
attribute mapping is empty that says
that my partition is the whole input and
output any element in the output its
provenance is the entire input that's
what we're always degenerating to with
all of these formalisms now we can
actually generalize these attribute
mappings to more data log like rules
this would say you know the output item
attribute here came from the input item
attribute for example or we could add
predicates we could say okay I have two
outputs output 1 and output to output 1
came from the inputs item whose
probability was greater than 0 point 95
and probability was less than 9.95 you
can also have functions you can say you
know I mapped a name in one to the
capitalization of the name in the other
so actually attribute mappings can be
pretty nicely extended to be fairly
powerful this is the only other sticky
note I'm going to say we actually never
got to that particular aspect of it but
I think it's doable okay so that's
attribute mappings that's part that's
half
our logical provenance specification the
other half is predicates and predicate
SAR really simple but this makes our
language fairly powerful when we add
filters filter just says basically I'm
going together I'm going to apply a
filter in this processing node and none
of the data that fails the filter is
going to come out what it says is that
if I apply a filter if I have a
predicate f a filter and I apply it to
the input the output I'm not going to
lose anything in my output okay and let
me just give you a simple example so if
I have for example person country sales
and I'm taking out the French sales I
only am keeping the ones that are from
France then the predicate country equals
France is part of the Providence it
tells me that I'm only going my output
is all going to come from the French
input so that would be a predicate there
okay and I can add to this particular
example also attribute mapping from
person to person right so i can if i add
an attribute mapping what that is
telling me is now this is sort of a
combined those two things says when I
want to find the provenance of an output
element I apply the attribute mapping to
the input and I apply the filter okay so
in this case my attribute mapping is
people and the important thing is that
if I want to find the output of the
provenance of a person I'm going to find
that same person in here and i'm going
to apply the filter that country equals
france so trust me this all works and
it's actually quite simple okay so the
general case of logical provenance is
that i have a set of attribute mappings
and a set of filters and i describe that
at the transformation level and I don't
do anything at the data level okay and
so we've again developed a theory we
build a theory of when these providence
is correct when it's precise the minimal
provenance using these descriptions I'm
not going to go into that we've you
start again with the transformation the
processing nodes and then you extend to
the workflows recursively one thing
that's interesting is that in the case
of sequel for most sequel you can
automatically generate the logical
provenance specifications and they are
correct and they are minimal and by
minimal I mean
capture the smallest amount of
Providence in the input so you've got
your output your transformation with
sequel you've got your output I can give
you a specification that will give me
the smallest relevant input okay yeah
yeah yeah but the do the predicate in
the first one was per element the
filters for the whole transformation so
that's a big difference yeah and let me
just mention just one more thing when we
implemented the logical provenance so we
changed we did a second version of the
system based on logical provenance
really the most interesting thing in the
implementation is if you have a big
workflow with a these logical provenance
specifications at each node the basic
ideas you would trace backwards we you
know step by step using those
specifications but what you can actually
do is sometimes not always you can merge
specifications from to processing nodes
and you can skip the intermediate data
so i could just take the you know i have
two processing nodes that are adjacent
to each other I have specifications for
each I can just put those together and
make it as if it's logically one
transformation and ignore the
intermediate data and that's that's a
nice property and that was that was sort
of the most interesting thing we did in
the implementation so let me just
mention version two of the system so a
version two now instead of the
predicates down here we have our
attribute mappings they can be inferred
automatically for some types of
transformations or the user has to
specify them if it's an opaque
transformation and we have the filters
and then we also relative graphical user
interface for version 2 and it but
otherwise it supports the same
operations ok questions about logical
provenance yes
yeah so I mean all of these things you
all of these things you could try to
generate by observing the behavior and
there has been a fair amount of work on
that if you're generating a filter you
might not it might not be a hardon
guarantee but on the other hand we one
thing about Providence is sometimes you
don't need everything to be exact it can
be approximate we've mostly worked in
the exact world but if you're willing to
be approximate then you can do that you
can observe and start you know getting
heuristics about the provenance ok now
let's go to the most narrow topic which
is what we call generalized map and
reduce workflows we're still in the same
picture of a data oriented workflow but
now we're going to ask the question what
if every transformation was known to be
either a map function or reduced
function so now we have very specific
properties of how these each processing
node is going to behave and with these
very specific properties it can be
easier for us to define providence
capture provenance and exploit it and in
fact you can write an automatic wrapper
on top of Hadoop that will capture
providence when you know these are map
and reduce functions with very little
intervention of the person who's writing
the job and it doesn't interfere with
the behavior on Hadoop so this is really
again very narrow and it was one sort of
one student one year project or less so
let's look at map and reduce function so
a map function this is my definition of
a map function and I'll just say right
now I'm assuming that functions that
don't have strange side effects or store
things a map function says if I have M
is my processing node then the output of
that processing node M is the union of
applying it to one element at a time ok
that's definition of a map function
everybody good with that so the
provenance is pretty easy the Providence
of an output is the single element such
that that output was produced when I
applied the map function to that element
okay very pure definition everything
works reduce function is the same basic
idea except you're doing this grouping
or this partitioning so I'm going to
define a reduced function
to be a processing node are so such that
i can separately apply our to each
partition based on a partition key okay
and that's the basic boof that's the
idea of the reduce function so as a
definition the reduce function I
partition on these this reduce key i
have n partitions i apply our to each
one separately and then I give basically
the same definition the out provenance
of an output element is the partition
that produced that output element okay
and again I'm assuming no storage or
side effects here okay so now I've told
you what the provenance is for I've told
you how you find the provenance for a
map function are for a reduced function
they have defined it for you and again
the same old recursive definition if you
want providence of an output element
you'd go one step at a time to get the
provenance of the input okay so if we
have a workflow that has a set of inputs
and then we have an output element let's
say that we apply our definition and the
providence gives us these input subsets
I one through I n11 property that we
would like and some of you may see
there's a relationship though not
exactly to what we want to happen in
refresh one property we'd like is if I
use these intuitive definitions of
provenance for map functions and reduce
functions okay I do a backward trace
through using those absolutely intuitive
definitions if i get my input and i run
it through the workflow i sure hope i
get my output right that should be a
good thing if I don't get my output from
following the natural provenance
definition something's probably not
great okay actually usually holds but
not always so following the exact
intuitive definition doesn't always work
its kind of though some of the sort of
fun interesting academic oriented stuff
that you can do with Providence so I'm
going to give you an example and then
I'm going to again give you the
properties that a workflow needs to
satisfy in order to work okay so in this
example we're going to be analyzing
tweets so people are going to be posting
Twitter they're going to be tweeting
about movies we're going to infer their
ranking a rating of 0 sorry the rating
of a movie based on their tweet we're
going to summer
those into median ratings for movies and
then strangely we're going to count up
our mu our movies / rating okay this is
pretty strange but I needed to do it for
it is not to work and you'll see what I
actually needed was this series of
non-monotonic and won many
transformations to make this procedure
fail actually so most of the time
tracing back with the simple definition
works but here's an example where it
doesn't okay so this is a map function
for the scanning it's to reduce
functions map reduce reduce and gives us
our output so let's just look at how it
works so let's say that our input is a
bunch of these this will tell you
approximately when we did this work when
the Twilight and avatar come out like
two or three years ago I guess ok so we
tweet about these movies someone says
avatar was great they hated Twilight and
so on we're going to take those tweets
and we're going to turn them into
ratings somehow so we avatar got a
twilight got a zero I hated it and so on
all right everybody with me I'm going to
take the median for each one so for
avatar the median rating was seven
because i had an eight seven and four
and for Twilight a 2-0 2 &amp;amp; a 9 so I get
a 2 ok and so that tells me now my last
thing is to say how many movies have
each median I know this is weird to move
its median of two I had one movie median
of seven I had one movie okay you need
this double aggregation to make things
fail as a matter of fact okay so but
actually right now with this data things
are fine if i trace backwards using my
definitions of map for Providence for
map and reduce functions this median of
two is going to go back to Twilight
because that's the one that has a median
of two it's going to go back to those
three Twilight records it's going to go
back to the Twilight tweets and remember
what I the depth what I wanted to happen
was after i trace back using my formal
definitions and I then applied just to
those i apply my transformation that
like my workflow to just those I want to
get my output that's the property I want
ok so I send these threw up sorry I said
through and I will get the same output
so everything is good in this case
alright so just again to emphasize I've
got a simple definition of Providence
for map and reduce functions at the very
natural definition I use that definition
to go backwards through my map and
reduce functions I get to the front I
get a subset of the input and I want if
I just ran that subset through to get my
output all right so here's an example
where it doesn't work this says I've
changed my tweets and now I have one
that's about both movies it says I
enjoyed avatar and twilight too okay so
this is going to be a one many but one
mini is fine for map functions that
functions don't have to be one to one
and what won money is actually crucial
to this counter example okay so now i
actually get these ratings here okay
from this but now we see that this for
this one here is generating 2 ratings
right there avatar and twilight now I
get my medians they're the same same
medians okay and I get my same count at
the end all right so now I'm going to
trace backwards using the definitions of
the map and reduce provenance here we go
I'm going to trace this one the median
of seven it's going to give me avatar
avatar is going to give me these three
those are going to give me these three
okay so now I've traced back using the
intuitive definition now what happens if
i push those through the problem is well
let me just start here if i push these
33 through i get avatar aight avatar 7 i
get this little twilight couple here
right and avatar for I design this data
so that now Twilight is going to become
have a seven if it's me as a comedian
and I get a 2 here okay so didn't work
now what are the properties that caused
it not to work it's pretty esoteric to
tell you the truth the first one is a
one many function that's key to the
counter example if it were 11 function I
don't have the problem the second one is
a non-monotonic reduced function and the
third one is also a non-monotonic
reduced function believe it or not you
need to have one mini followed by
non-monotonic followed by non-monotonic
for it not to work and we've proven that
so there you go there's the theorem but
you don't need to read that now anyway
otherwise it'll work if you have only
one non-monotonic or if you don't have a
one minute you're you're in good shape
okay so we built a system based on this
this completely separate system we built
it as an extension to Hadoop it's a you
know with academic proof-of-concept it
was mostly based on backward tracing we
didn't we didn't tune it for forward
tracing or for refresh you automatically
you can submit a Hadoop job will
automatically wrap it it will generate
default IDs and we can have user-defined
schemes we can add the indexing I'm
getting late on time so I'm not going to
talk about it in great detail we did
some preliminary performance experiments
we had roughly fifty percent time
overhead on a 500 gigabyte workflow and
it takes a second to trace things for
demos the most fun I would say is
actually pig scripts so what we can do
is take a pig script that generates a
pretty complex map and MapReduce
workflow and it will automatically
capture provenance for that and it has a
bit of a WOW factor i would say for
people so this was a separate project
but I think maybe in some sense the most
practical all right so I'm just going to
spend a couple minutes on follow-on work
and then I'm done why don't I just do
that and then see if there are any more
questions okay so first of all I
actually you know some of you probably
observed I sort of showed you three
separate things and they have some
relationships to each other that we
haven't even completely understood
frankly predicates are the most general
logical provenance less so these
generalized map and reduce workflows the
least so that's the relationship we
haven't really unified them second we
used a pretty specific provenance model
in a way if the underlying model was
very fine-grain data item to data item
we also focus as someone asked on data
provenance not on process provenance so
we're really focusing not on the
processing nodes and what nodes a data
item pass through but rather the
relationship between the data that they
itself and when you have dynamic
workflows what nodes data paths through
becomes much more important and I also
didn't do anything with time and
versioning I think that's actually
pretty important too for all of these
you'd have to extend capture tracing and
propagation I discussed ad hoc queries
actually think this would be a really
fun area to work and I don't know you
know how again there's this sort of
common denominator whether you can get a
Providence query language that satisfies
lots of people I don't know but it seems
like fun to design a language that
queries data together with provenance
and you optimize it in ways interesting
ways i also think if you want to query
provenance and you know what you want to
query before you run the workflow then
you can actually tailor what providence
you capture to the queries that you want
to answer about it so that seems like a
kind of fun area to then there's also I
think a very practical problem we worked
on this a little bit about optimizing
what you do when you're capturing
Providence and exploiting provenance and
the whole question of eager versus lazy
do I want to do the work as I've send
the data through the workflow or do I
want to will do the work when I need to
trace provenance is actually a pretty
pretty hard question there's space-time
trade-offs it's the query update
trade-offs the usual type of trade-off
that also depends on what kind of nodes
you have so I gave the example where
deduplication it's very easy to capture
provenance eagerly and very hard to
capture it lazily whereas aggregation is
simple aggregator if you do aggregation
you're doing a big say group by
aggregation it's you really and you're
keeping the group by attributes it's
pointless to in your output capture all
the pointers back to all the elements
that that generated the output it's a
huge waste of space so you really don't
want to do eager in that case so
different nodes have different different
needs let's say there's a question of
whether you want to retain intermediate
data when you're operating on Providence
I just gave a brief discussion of
wherein the logical provenance we have a
mechanism for you know sort of with
formal foundations eliminating keeping
intermediate data for provenance tracing
in the extreme case when you think about
whether you want to keep intermediate
data you run the workflow once so you
have all your inputs you run it once you
all your outputs nothing's ever going to
change and you're going to do a huge
amount of provenance tracing then you
should do all your work up front you
throw away the intermediate data sets
capture your mappings from the original
input to the output and you're done ok
so just decide there's a big area of
theoretical decision problems here
actually as well as system issues ok
related work huge many decades of work
in Providence tons and tons that kind of
goes back to my original opinion that
the problem will never be solved but
there's a whole bunch of fun research to
do and I'm finished so thank you
questions yeah
in the predicate I might mostly
mentioned
Oh conjunctions now you can have
disjunctive predicates without a problem
I think yeah I could come up with cases
where you it's more like if you want to
identify a group of things if I for some
reason I'm computing my hats and shirts
together into one prediction then it
might be you know it equals hat or
equals shirt that would be an example
yeah so I think they can arise fairly
naturally yeah right
since they're me
oh how do i how do I justify my claim
that the common denominator is small I
yeah I can't justify it from a technical
point of view I can more justify it by
just saying when I talk to people
everybody wants something different
there's just some tweak that makes to
their problem that makes this not quite
the right solution yeah I mean I I don't
have a mental image of actually a Venn
diagram with small intersections even
though I claim that I more have an image
of here's the solution oops doesn't work
for this here's the solution oops
doesn't work for that we're all of those
boobs are sort of different reasons I
know it's kind of a fluffy answer that's
my gut feeling for it makes a
general-purpose Providence solution I'm
more than a dollar I've item lunch
dinner yeah other questions okay thank
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>