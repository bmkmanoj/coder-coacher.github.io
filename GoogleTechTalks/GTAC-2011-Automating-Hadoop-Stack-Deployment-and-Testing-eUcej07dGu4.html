<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2011: Automating Hadoop Stack Deployment and Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2011: Automating Hadoop Stack Deployment and Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2011: Automating Hadoop Stack Deployment and Testing</b></h2><h5 class="post__date">2011-10-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/eUcej07dGu4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have Andre our cilia from yahoo
interesting company for us to hear from
and Andre is actually I think you're a
committer for a hoodie price yes yep and
he is going to talk about the Hadoop
deployment work that he does at Yahoo
okay all ready for this please welcome
Andre thank you thank you very much
Alberta had a very entertaining talk so
now I can bore all of you to death and
not feel guilty about it so my name is
Sandra helium and today I'm going to be
talking about the meeting had abstract
employment and tasting and we are
talking about things like Hadoop stack
deployment Hadoop stack testing at a
meeting all of these things these all
have very big topics and I don't think
there is possible to create a it's
possible to create a single system
that's going to satisfy everybody so in
my experience what happens when
companies are faced with these issues
they're basically looking at what kind
of IT infrastructure they already have
and what kind of business requirements
they have and then based on that they're
coming up with very different solutions
to these problems that's why today in
today's talk what I'm going to do it
consists of two parts in the first part
I'm going to look at the common
questions that arise when you start
thinking about these topics and then in
the second part i will i will present
how did the yahoo chiam looked what was
our take on this on the solution and i'm
going to present our solution which is
when if ever you are faced with the same
set of challenges it going to be looking
at the same questions but most likely
going to come up with a different
solution but hopefully my presentation
is going to help in that you're going to
be thinking about some of the same
questions that everybody has to face so
Hadoop leading data-intensive
distributed applications framework
hopefully it doesn't it introduction
although maybe at google it does so they
it comes from the famous Google paper
happens to be the engine that powers
Yahoo grids that's how Yahoo does heavy
lifting for all of its properties news
sports finance and so forth and like any
other product it needs to be tested so
when we started looking into how do we
deploy well how do we test head open and
the deployment is a natural part of the
process very quickly we realized how
much truth there is in this famous
Confucius statement that if the language
is not correct and what it said is not
what is meant we started thinking so
okay testing Hadoop testing Hadoop let's
see so what is hadoop is it just the
three components that constitute Apache
Hadoop proper are we talking about extra
configurations libraries some other
components what do we mean by Hadoop
okay even if you have some ID like each
particular group organization they have
some idea what is hadoop for them and
these things are different but what
version are we talking about again there
are some versions out there and Hadoop
Apache org but these versions are not
necessarily the ones that are in use so
we need to define what are we talking
about when we are talking about
deployment what is what is what is
deployment what will be deploying and
very deploying it a another question any
kind of software complex complex
software system faces what does it mean
to test it how many sort of body
movements do we need to make to call it
a proper testing and how much testing do
we actually need we need to be need to
decide on that and another point to
consider is that depending on whom are
we talking to we can help get very
different answer to the same question
for example for people who do Yahoo
Finance website a notion of Hadoop and a
version and deployment and test is very
very different and for people who do
HDFS hardcore development so we need to
think of all these things so let's look
at each point and term let's try to
define how do how can we define
a scope of the software Hadoop we always
sting / Hadoop not as a single project
but there's an ecosystem there is
something called Apache Hadoop which is
like a core on top of the not on top
about in addition to that there are
always some extra stuff that that goes
into the score Hadoop things like
compression libraries configuration
files some small scripts this and that
then there are other components that are
closely working with the Hadoop things
like age race or high for a zookeeper
and if your core Hadoop developer you my
argument say well wait a second
zookeeper is not Hadoop something
different for me but for people who are
doing for example ads for them they
don't really care about bare-bones
Hadoop they need services and for them
if they don't have for example HBase or
they they cannot do queries and hive day
they cannot run Hadoop so for them so
for them Hadoop also includes these
components at a sort of close to the
core head on top of those components we
have other components things like big
Eugene proxies on top of that we have
platforms like yahoo has proprietary
platforms for serving ads for security
and so forth optimizations there are
more platforms on top of that platforms
and finally we are getting to the top
level where there is things like Yahoo
Finance there is like this huge software
stack and the painting and where do we
look at this stack we can define Hadoop
using GM we can think of Hadoop using
different terms like well even when we
are talking about Hadoop core some some
groups think of Hadoop course a project
to Apache Hadoop that's called Hadoop
core where for us for other Hadoop's i
will sorry for other groups which use
handle for their data processing for
them Hadoop core means the whole Apache
Hadoop and extra libraries and security
and configuration and so forth the next
subject that we need to think about this
head of version so Hadoop is a
combination of korb build and release or
released libraries other components
configurations and so forth and not the
one all artifact subversion sometimes
they're always a small set of files that
are just being copied over because you
know developers or operations know that
they they're needed so although
technically we can define Hadoop is a
combinations of all the versions this
huge can contain a shin string in
reality what you find out that users
generally do not just grab random
versions of Hadoop components level the
place throw it together and expected to
work rather what happens is there is
some kind of decisions which versions of
the software going to be going to be
deploying and this this sort of stack it
gets developers have it in mind when
they developing their software so these
are the versions we're aiming for and
then he is certifying this stack and
putting a stamp of approval and then
this this tag yet gets blessed
vibrations and and then this is sort of
our version one of the versions that we
are using so versioning is a thing to
think yet another important aspect when
you are thinking about Hadoop deployment
Hadoop really scales and it scales up at
Yahoo we are running well over 4000
nodes and it also scales down I can
deploy Hadoop on my laptop so different
operational scales define different
concerns when I'm thinking about
deploying hand upon 5 nodes the concerns
are very different from deployment of
500 nodes and there are different
challenges and i'll be talking and the
next slide about it so when we and also
when we are thinking about tasting we
also have to think about the right scale
because if you're running production on
4000 notes it would be nice to run tests
4000 notes however for one thing is
going to be very expensive for nothing
even if you could even if you had 4000
notes just standing there waiting for
our test it probably will be impossible
to just stop the whole production
replicate the whole data to the test
environment and do something like that
so there's a cost associated with
tasting at user scale at the same time
deploying small too small toy sighs
clusters of course it's convenient but
but then we are not tasting what the
users actually using and we do not
address the scale issues and it actually
managing 15 hundred node clusters
in terms of animation well in my
experience and managing 105 node
clusters there's costs associated with
having lots of small entities so
something to think about and and and
there's no again for all these things
there is not a single answer for example
I know that there are issue at hand up
there issues of scale that come up above
hundred nodes we're running a hundred
node cluster everything works fine we
run 500 node cluster and suddenly we are
noticing that the node start dying after
in a certain condition on the other hand
they're scaling down there other
problems resource scheduling in that is
notoriously crappy when we are trying to
run hand upon a very few number of nodes
sorry so I hope your vision is ok if you
can read it it's actually better than
than normal but basically what this
table tries to present this you can
think of when you're thinking what when
you're thinking about deployment if
you're thinking about the system that
this is going to be doing the deployment
you can conceptually split the scales in
two different bins like for example
deployment and one on two nodes and this
this is what this table presents for
example deployment and one or two nodes
I would call it walk deployment it just
to see that it comes up and kind of says
hello world something like that
proof-of-concept deployment maybe three
to 10 nodes where you can you wanna run
something like a some small MapReduce
job to see that you know things I'm
working then the small deployment is a
it's actually serious cluster like 10 to
15 notes 50 notice i'm already cluster
people doing actual important work on
the scale but there is still no need for
like this massive automation then there
is medium scale fifty two to three
hundred nodes I'm just it's not an exact
science it's roughly there are different
issues and then there is large scale up
two thousand notes and then there is
finally in the price scale which are
very few companies has yeah so all these
different scales they have different
things too
to you for you about when you're doing
the deployment i will give some example
for example when i'm doing more
deployment my first concern would be
that it just should work i mean i click
a button and it works so there should be
as little environment setup as humanly
possible and another thing is when I'm
doing when I'm installing it in my
laptop and I'm doing the damage i'm
dragging the laptop around they don't
want to be tied to corporate network so
really it should be working off light
when i'm going into the proof-of-concept
the things to be watching out is usually
for such a small cluster at not have
fancy I tea set up so I would look be
looking about that I did that my
deployment system only depends on some
basic linux it up nothing fancy know
some kind of complicated ldap entries
dns setup d be instances set up things
like that they all make using proof of
concept very hard thing to do if you
look for example medium scale deployment
completely different set of issues we
have things like it at medium scale
deployment we already cannot be having a
root password being passed around the
team and doing the management or
MapReduce role so we need some kind of
user role management and one form of the
other hardware layer management if you
have 200 nodes they keep failing left
and right so you know we have a full if
you do not have some kind of ability to
manage the host reimage them quickly
manage what state they're in what image
do they have we end up having full time
rolls people just going around making
sure that the hardware is working or
things like full automation for for
small cluster i still can get away with
writing the small PDSA scripts that go
to let's say 30 nodes and and run the
same commanded 30 notes for 200 note
it's not it's not practical it becomes
it comes to the point where the error
rate is too high so we need to think
about full any iteration we can run on
all the notes within a cluster we need
to automate it and make
its robust and ours checking and so
forth so this so the point of all this
slide is basically as you're thinking
about your system deployment for
different skills they're very different
issues you can think about and I heard
sentiments from Hadoop developers where
they're saying well we're going to
create we applied we're thinking of
doing a deployment system and when asked
what scale are you planning for they say
well anything from single to 20,000
nodes so well you know there there's a
lot of costs associated with it lots of
thing to think about now a little bit
about tasting well how do you test
Hadoop with of course again another huge
subject to primary approaches that I so
one is the right way and I'm using the
double quotes of course there is no
right way but it's my feeling that it's
best to test based on use cases so
basically you want to test the complex
Hadoop stack so what do your customers
actually use you know have them have
your their use cases and basically it's
going to be tasting what the actual
testing versus the bulk of the code the
problem with that approach is of course
most organizations do not have this
level or organization I actually I don't
think I've so once a case where Hadoop
usage will be having really good use
cases and and to test out of the use
cases you have to have a very detailed
spilling out how you process work how
you just to ski to start so what the
most organizations they actually do is
the organizational way they say okay
let's assume that the existing version
of Hadoop works for the new version we
get some Delta so let's find out what
features go into this Delta we're going
to test the Delta features and then we
will declare that the new version of the
product works as well well this approach
is also k although this approach
requires it's sort of implicit things in
a closed source world this approach
requires you to have a control what kind
of Delta you getting and then a close
source world yes we can also go always
go to let's say developers so director
of development and ask what kind of
features they are planning for the next
release and they're gonna give you a
list of features that there's going to
be signed off so key goes ahead and
happiness starts doing the test planning
and test cases and all things an open
source world this model breaks down
because really no single organization or
team has control over what goes into the
next release so we can try influencing
open source community or you can try
controlling what patches go in but
there's no single answer and their
challenges either way so generally
speaking I feel that basically we need
to test what whether stour customers and
that's what we are trying to do it yo
and dancin these questions really
depends on what kind of you know what's
your situation and I will show what we
do at what decision will be made at
yahoo so and so so much for generalities
now talking about specifically yahoo
case when we started looking into what
how do we test Hadoop we quickly
realized that we we have a challenge in
our head because on one hand the Hadoop
development realities at Yahoo were that
there was there were many core Hadoop
developers it was researched like
environment first innovation place and
product growth lots of tinkering
experimentation basically the Hadoop
development he was busy churning out
versions adding new features building
what we call rocket parts you know the
the new large important functionality
and the same at the same time in the
production land we would like to have
stable services stable yahoo services we
don't want finance yahoo com going down
so we have large production cluster that
must be rock solid that in turn means
that the any kind of change should be
controlled there should be deliberate
change process planned well in advance
continued separation quality monitoring
we cannot go in
break things and you know expect to be
happy afterwards so there's basically
sort of this clash of attitudes so on
one hand we have fast-paced development
environment neither had a very solid
production environment so the solution
we came out we call it hit this is the
system basically at Yahoo we have a pool
of sand box we call it sandbox
environment several thousand of nodes
that I used for development key release
engineering and so forth so we developed
a system that allows us to allocate the
chunk of these nodes set set them aside
and use them for Hadoop deployment so we
provide the ability to deploy Hadoop
stack on the set of nodes we decided
that we're going to be deploying what we
call Hadoop searching components so
we're not going to be deploying the
whole stack fro from core Hadoop all the
way to financial or advertisement code
when you're going to be deploying the
components that are sort of generic and
used in many different parts of the
company the user has the ability to use
one of the reference configuration so do
you own but actually using reference
configuration is a very important
feature because most of the time users
don't really know what are the versions
of all these little component go into
Hadoop stack they have no idea so we're
just offering them this is what this is
the major configuration says that were
blessed by key you know can you can use
the latest or one of those we have the
ability to choose which components to
deploy choose configuration so provide
your own Hadoop deployments they always
have there is a Hadoop cluster itself
but there is always a client part we
were all the client side code goes so we
deployed that one as well and then of
course when we deploying the stack you
would like to see that it's kind of
breathing in up and running so we do
small smoke tests and then we generate
the manifest which is useful which is
documentation item but that's useful one
for example key process you know runs
around and we end up certifying
something we'd like to know what
specific did we certify so the records
are important
this is the the diagram of the typical
heat deployment everything inside the
dotted box is a Hadoop cluster if you
look at the green this is a different
square so different note so we're
deploying regular data nodes we deploy
name no job tracker hive wuji and so
forth some of the components required
database storage so we are providing
database instances on the side also this
big green box is the client side we are
deploying all the client portions like
she lies or things like pig and then if
we choose if you opt to do the actual
tasting much is the hashtag deployment
but to run the test cycle we are also
deploying on the client side tests for
different components we made a decision
to deploy all the tests on the client
side going from the the idea that we're
going to be running tests just like a
regular user would so if we if on the
other hand did the test concern for
example with the server part let's say
I'm an Uzi key team and I'm packaging
packages to do to do with the server
tasting I can either run it from the
client side or if I want to go to the
server that they can just do a cessation
and do whatever I want on the server
side so once we deploy it all the
software on this let's say 20 notes or
however many the first question is is it
up if we deployed in is it working is it
breathing or just you know dead in the
water so what we found out that the most
common problem that we are facing in our
early deployments is that components not
work with each other so what happens is
there are different teams they're
developing their components and they're
tasting them separately so we see team
is developing which the component and
they're tasting and their little
environment and it works and then pink
team is doing the same thing for their
environment and networks and then
they're bringing everything together in
one cluster and it's just
fails because they're they they're using
different assumptions and they're using
different configurations we had cases
where component a always assumes to be
working with a default q whereas
component d crashes when there is a
default q assumes that there is never a
default cube because this is how our
actual production environment is set up
so clearly these two components that
although they work by themselves but
they can never work together so this is
what that's why we are aiming for
integration testing so basically we
decide well we decided that each
component should be certified by their
appropriate key teams so we're not going
to be concentrating and that but we'd
like to make sure that the whole stack
works and that's we're going to be
focusing on the integration testing so
the way it works is that each key team
provides test package because there are
so many components there is no way one
central team can can be responsible for
all the testing so it's the job of the
team's responsible for individual
components to provide the test so we get
on the stage together and the guidance
we give to those teams is think about
major interfaces the qu component has
with other components and test for these
scenarios so again picking on noisy
again so if I'm a developer or key for
ug component what kind of interfaces
dari touching the other parts of Hadoop
universe well I'm creating and checking
for files in a GD file system so I'm
using that interface I'm submitting in
checking the status of MapReduce jobs so
i'm using this interface so i'm using
some kind of maybe i'm using some kind
of special stubs in a pig for example
that means i'm using that interface so
all these major interfaces should be
covered as a part of the integration
testing also the basics like a tent
occasional security running making sure
that cameras tickets work and things
like that so they're all needs to be
tested and that ultimately bring the
stack together and that will
sure that everybody is working with
everybody and also we are aiming for
common case scenario again the purpose
of integration testing is not to test
the heck out of a system but just to
make sure it's working fine as a whole
and then the the API is such that test
generate results and g-unit format and
then they get picked up by Hudson so
here is a diagram of a typical hit
workflow we it gets triggered by hudson
and i will show you screenshots later
but basically we're choosing which tag
to deploy from a tag which is a
description of a set of component
versions that hopefully has been either
been tested by somebody or at least
vetted by somebody before so we are
deploying based on the stag when
deploying a Hadoop stack and then if you
decided to the actual testing we
deployed in step number three we
deployed the tests the run integration
tests and then report the results so the
cycle is very simple and this is how the
interface looks like on the left you can
see the simplified versions that's what
most people use and that's what they
like because it's simple all you need to
do is you can you need to choose one of
the predefined cluster there's a bunch
of those you choose the tag and the tags
are they circulated throughout the
organization let's say developers
assemble a certain stack and it seems to
be working so then he can pick up the
tag and do the testing and when the
quirky is done quirky sends out okay
this tag seems to be working okay for
core let's let's look at the components
and so forth so basically tags get
passed around and then you choose where
to email the results that's it you click
building the new tack at the nuclei the
old cluster gets wiped and the new one
gets installed and the tests are being
installed and run and the results being
mailed to the interested parties if you
want to be fancy if you want to do some
specific types of testing on your test
new version of your component there is
an interface on the right where you can
see a lot of different options for a lot
of different components contribute
teaching the Stags you can choose
whatever version of the component you
won whatever test you want and all the
other parameters and basically do your
testing this way and Hudson has the
ability to display the results pick up
g-unit results in display the interface
is made in the chip which is using
Hudson so in the summary we were we were
looking for a way to make Hadoop stag
deployment and test simple it is such a
big complex problems we were trying
looking to carve out some subset of this
problem to make it simple so we
addressed commonly used Hadoop centric
components of the stack and there it is
this these are used by a lot of teams
that they deployed the base of the stack
let's say there's a team that that's
something specific with I don't know
whatever advertisement for example they
might have all the expertise in the
world about how their attachment
software works but most likely than not
they don't know how to do with the rest
of the Hadoop stack so this system
allows them to deploy the basics and
then they can put their software on top
and do whatever they want we were
gearing towards flexible small scale
deployments which is the opposite of
what we have on the production side
which are large deployments but less
flexible we suggest reference versions
the configuration which is very useful
practical item as it turns out again
most developers or more groups that no
they do not know 25 versions of
different software pieces that go into
the Hadoop stack they want to know it
out of the box when deploying fully
configured working clustered minutes and
provide interface for integration
testing and a Jenkins 30 an interface
and reporting so thank you very much
we have a few minutes for questions so
we've got microphones around over here
okay this is what's new from ingenuity
systems I was wondering when you said
you have individual components being
tested and then you don't know if they
will all work together don't you have
concept of like versioning of a pis or
compatibility of ApS so that when a when
a component is promoted to be a released
version it should be backward compatible
so you don't have to run it to set up
everything in one single environment and
test them together so you asking about
versioning of AP is right so we have
well we have versions of components
themselves the interface the heat view
of the components that are coming in
coming is version packages so we have
versions of the components coming in the
package form we do not have versions API
just because i'm not aware that Hadoop
has documented API so there is no
documentation for a period s naught
Delta there is no versions for the IPA
it's actually it's actually a big
problem a thing and it would be nice
maybe one of these days you will get
some something like a Java compatibility
kid but for Hadoop where we will have
defined API yeah but no not today hi I'm
on of Durrani from orbitz just a
clarifying question which deployment
tool were you using to deploy Hadoop so
yahoo uses packaging system that can
deploy individual packages on top of it
heat actually has the mechanism to
deploy Hadoop
multiple nodes so I'm not aware of any
packaging system right now that that's
going to allow deployment of coordinated
packages of many different nodes so
there should be some kind of deployment
we have we have our own hi this is a
shish ankle from adobe systems so I have
a couple of questions actually one is a
when we integrate such components and
carry out such a acceptance test and say
it just fails so usually in my
experience it's the root cause like
which is manifested by the failure
usually lies in a like in it in a
component which is not very which which
can be counterintuitive so is there any
matrix you track at the integration at
the i would say intuition abstraction
which can point to the root cause and
subsequent debugging of the problem
faster and the second question i have is
i mean how do you kind of you know i
would say predict the scale of the
system say if a system works for the X
number of nodes and should it works for
the 3x or 100 x node so do you have some
kind of parity model which have created
by which you can predict the you know I
mean if based on certain kind of
heuristics like how much or how good the
system would scale I say so so the first
question was if you have a company if
you have a failure in a stack how do we
attribute the would cause usually well
there is no single answer to it the
general principle is of course if the
cause is obvious you know that's the
team that gets paged in system like heat
if test for a particular components of
failing that system gets the
notification that something is failing
but beyond that we just have a general
principle that
downstream projects look at the problem
first for example we have pig that's
relying on HDFS and the pig is failing
usually the big team gets to look at it
first even though it's quite possible
that actually HDFS is the one that's
ultimately causing the problem so it's
just the reality and all the developers
aware of it and we just take it into
consideration when we are allocating
resources for scheduling as for the
second problem do we have any models
that would predict failures of scale
again it's a very complex question so we
do we don't have again any kind of
universal models we have a sort of
corporate experience at which scale
points things break where so when we are
scheduling our testing we are scheduling
the regular most of the types of testing
is happening in a 20 node cluster which
is just you know enough for for the most
applications then we are doing scaling
200 and 500 nodes so and and then of
course the higher the skill the more
expensive with this we attract by by the
time we getting to 500 we are tasting
for you know certain subset of tests
that we know causes the problems you
know things like performance a load or
bringing components up and down things
like that also we do performance testing
separates like sort of separate sub
tasting branch sort of activity and I
think there are some models in
performance specifically some
considerations well this is how much
memory each node takes and this is how
long did it take so we can kind of do
some rough estimate isn't going to scale
or not and test for that if you have any
doubts but it's only about performance
so this was all about kind of big large
scale integration tests and you talked
about working out the deltas for what
needs testing and that sounded quite a
manual process and quite a high level
process there's only if you have any
techniques any technologies for working
out any integration tests got a large
black box tests what you actually need
to be testing in a more automated
fashion so this process had described it
actually quite automated to then that
was our aim you know we the hit is fully
automatically really click on add a
statistic so you're asking about so
black so heat is aimed for integration
testing which is a particular type of
testing it can be used for other types
but it's not its primary aim so so you
so what are you what kind of testing are
you asking specifically so sounds like
your tests where does this system work
for all these components still and like
you run quite a few tests in that
process and how to minimize just the
number of components you're testing in
the number of things you're doing with
them well that's what that's what the
integration testing basically does we
bring them all together and that's black
book black box testing so I guess one of
the finer points that you didn't mention
although the tests are provided for
individual components but because it's
integration testing virtually all of
them they exercise the whole loop for
this component so when we when we do it
with we talking about big tasting the
test for peak the exercise pig and the
rest of the stack at the same time so
the old real test you're crunching real
data or rescheduling real jobs or
recopying real data things like that so
the exercising the whole the whole thing
and that's as I said we it's up to
individual teams to choose what kind of
tests they use because again there's so
many components and no single person
knows everything but we are giving them
guidance make sure you do the whole
cycle and you test interface how long do
we run our integration cycle so this
particular system it's used for
certification so we're basically
certifying stags that they are ready for
to go to not a production but things
like research basically go out of
sandbox we are trying not to run it for
unstable code basically our contract
with key TM says that you test you
component first and make sure it's
working first when you're sure it's
working first and then we are going to
test the whole stack this way we are
waiting a situation when we are getting
the code that's not ready into the whole
second of course everything will fail
and then you know we will endlessly be
in the cycle when we're trying to patch
things up and they will not work so this
system is run toward the end of key
cycle right before the the set of
components is headed to production yes
kind of following up on that point of
when Tessa now after you test so my
experience with integration testing is a
lot of times when issues do come up what
you're looking at is not in the system
that you're interested in testing but in
the rest of it so whereas you know like
you have kind of contract with teams
that you know by the time you're putting
something in its stable the reality is a
lot of times when you know I'm trying to
test my component in the system fifty
percent of the time I am looking at
something that has really nothing to do
with my system you know how do I then
balance that yes I need to make sure it
works but I'm spending a lot of time
dealing with you know other people to
problems I could say yeah so how yeah so
if i do development of my component i
end up debugging other people's
components and wasting all this time
just to find out that something else is
not working so how do i how to work how
they deal with this situation the answer
is a blessed stack configurations so
when you're using heat for example
you're deploying configuration some of
them are really have been thoroughly
tested so as a developer of your
particular component we deployed that
configuration and the stack and then on
top of what you deploying your component
which you're working on so you have much
higher confidence that if something
first it has been tested already so you
have higher confidence that when
something breaks is probably your
component it's still not a guarantee
that's true but you know that that
really helps you also run the risk that
then when something is blessed and you
know you tend to be running on a really
old version so you integrate testing on
something that's probably dated yeah
well heat environment is pretty close to
the development so we are not and then
everything that goes into research your
production is running through hit so
there's not that much delay between
things being sort of ready for key and
things being available
the system so it's not as much of a
problem hey we have time for one more
question yeah hello my name is your
slick person from that secure how much
time are you actually spending on on the
deployment issues and and like random
test failures and fighting sons but
issues instead of like focusing on the
really integration tests like you must
have if you do larger deployments all
kinds of like deployment problems as
well or then you have a really good
system how much time do we spend on and
sort of release engineering issues
versus key issues something like that
pretty much yeah yeah well because I'm
developer a system i'm spending
essentially all my time and release
engineering issues because if it's a key
issues I'm handing it on to key Tim and
that's basically dancers the key teams
they don't care about how the deployment
works the deployment I mean it just did
care about how much time the process
works the process of deployment is like
something less than half an hour or
something like that action of less than
15 minutes but there's a small there are
a couple of people who deal with the
deployment system and the rest just key
organizations that deal with their tests
thank you thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>