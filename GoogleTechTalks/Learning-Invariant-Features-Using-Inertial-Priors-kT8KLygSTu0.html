<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning Invariant Features Using Inertial Priors | Coder Coacher - Coaching Coders</title><meta content="Learning Invariant Features Using Inertial Priors - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning Invariant Features Using Inertial Priors</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kT8KLygSTu0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">ok
is primarily a technical talk and i'll
be presenting a mathematical model of a
portion of the neocortex I hesitate to
claim that that you'll be able to
understand everything in this talk but
the mathematical model doesn't include
any equations it includes only graphical
models which I'll explain as we go along
and those graphical models have a
mathematical semantics that I hope
you'll find natural to understand there
was a second or alternative title to the
talk why Google might want to be in the
neocortex business and the idea behind
that is to consider much in the same
vein as why my general motors be in the
business of extruded plastics for
example and the answer of course is that
they use more extruded plastics in their
automobiles than anybody else in the
country so here we're talking about an
enabling technology that is just in its
early stages and that Google might want
to invest in so this is joint work with
Glenn and with rich Washington so I'm
going to go and with Jim Lloyd so Geoff
Hinton and I talked that he gave a bitch
kyanite in 2005 on the event of him
being awarded the research excellence
award asked the following question what
kind of graphical model is the brain I
think Jeff would probably agree with me
on the first two bullets it's
hierarchical in the sense that it
represents a hierarchy of concepts or
representations it's Beijing he might
block at Beijing but I don't think so
too much anyway it's stochastic and
generative in the sense that you can use
this model to generate the kinds of
stimuli
that the brain normally receives the
third item though the idea that its
temporal and predictive is something
that Jeff might resist however another
Jeff Jeff Hawkins would thoroughly
embrace such a model and finally the
idea that it's continuously adaptive I
think anybody would agree too
unfortunately from a purely mathematical
point of view we're off in deep waters
once we consider this more variety of
model so why should get google care
about these types of models right now my
understanding is that Google is very
much interested in image understanding
of a wide variety they need it for such
tasks as detecting whether or not
there's pornography on a web site
filtering spam and the like and the
techniques that are used currently or
primarily ones that involve what's
called discriminative learning in
particular these techniques are
supervised and luckily for these cases
there's lots of labeled data so
supervised learning methods work quite
well the kind of models will be talking
about today are primarily unsupervised
and so they go beyond what you would
normally be able to handle within a
discriminative framework and they
necessarily generate concepts that are
not necessarily accessible in the sense
that since the machine will be learning
these new concepts and new concept
hierarchies they won't be ones that will
necessarily accord with our intuitions
about the internals of such models
so I think that people will agree
another pajamas that's great only two in
the audience though I think that
everybody will agree that Google is in
some sense the grand content addressable
memory and the kinds of generative
models that we're looking at certainly
have that flavor we're also interested
in what I call coincidence driven
associations for example two things
residing on the same webpage can at
least if there's more than one
occurrence of this indicate a
relationship that you might want to
encode the models that we're looking at
are fundamentally sequence-based and
even though a lot of the models that
Google currently uses are based on
models that are non sequence-based sort
of bag of words more and more Google is
moving to taking advantage of the
sequential nature of of text and video
and the like and finally the things that
Google is most interested in involve
multiple modalities
so this is an attempt to compare text
based inference and image infants in
particular video inference and in the
middle I've shown the primary channels
or the visual pathways in the cortex
leading back from the eye and the retina
to the back of the brain where the
primary visual cortex exists and on the
left and right are supposed to be the
sequence of our the hierarchy in reverse
order of abstraction of concepts and
I've gratuitous ly included three
textbooks on the left two are ones that
I wrote and one responded Peter wrote I
thought better throw and just for good
measure and and and these are classic
examples of linear text and if you break
them down to the most primitive
constituent pieces that corresponds to
characters and then we build them up in
more and more complex abstractions by
compositing the more primitive elements
so the sort of of metrics and signatures
that we use for web pages today are
often in terms of things called term
frequency vectors and the next step
beyond that would be to start taking
order into account using em grams and
the like in terms of video sequence the
smallest constituent points would be
pixels and then a somewhat more
amorphous concept than words but still
relevant in the machine vision worlds
the notion of a patch and then the
analog of term frequency vectors would
be to take some basis that you would use
to filter images or image sequences and
use that to construct a set of
coefficients and those coefficients
would correspond to a signature in much
the same way that a term frequency
vector would you could then composite
in various ways and if you think about
the structure of the brain in many ways
the information that's available as it
as its fed into the cortex is much the
same as the level of term frequency
vectors in this case there would be a
basis that would correspond to haare
wavelets for example or one of our
numbers of families of such filters but
beyond that you start to build up a
complex hierarchy it takes into account
features that span larger degrees of
both space and time and that's the
primary topic of today describing how
those features could be constructed in
an online learning mechanism I'll start
with some terminology and essentially
I'm going to take the neuroscience ideas
and reduce them to Sperry simple
elements and then build them back up
using the mathematical models that I'll
be presenting so everybody's familiar
with the idea of the cortex as a sheet
that is crenellated and stretched over
the more primitive aspects of the cortex
that sheet has a number of properties
that make it a ideal target for turning
into a computational model for one thing
it's the striations and a horizontal
vein are very regular the cells that
occur at different levels or layers
within the cortex are very our uniform
very homogeneous second in a vertical
mode there are what is often been called
columns these were first referred to as
such by mountcastle back in the 50s and
have since given rise to essentially a
cottage industry of people coming up
with different ideas of cortical columns
there are now many columns and how
columns and a variety of such structures
they are they tend to be anatomically
distinct and they've been posited to be
functionally distinct as well though
they're still somewhat controversial in
terms of being a fundamental element of
the cortex nevertheless that's the
framework or that's the that's the unit
that we consider of most interest so the
representation on the far right here
shows a sort of honeycomb a set of
columns that that would correspond to
essentially hyper columns in the
parlance of mountcastle each hyper
column would be about a hundred thousand
cells about 60,000 neurons and as we'll
see it can be used to recognize some
relatively interesting but still
primitive features within the primary
visual cortex the next notion is the
notion of a receptive field and this
essentially you can think of as a
portion of the periphery of the body in
the case of the eyes this would be some
portion of the retina that maps back to
a cell and on which the cells response
is dependent the the as you move further
and further back along the visual
pathways the receptive fields tend to
span a larger and larger portion of the
overall visual field I don't know how
well that shows up but we're interested
in a particular portion of the visual
pathways and it's called the ventral
visual pathway and it corresponds to the
strike cortex or area v1 v2 v4 and IT
it's not necessary that you know what
those those mean in any sense other than
that they are areas that have a distinct
anatomical structure and
that that Google and easily discovered
that as the cells transfer information
from one area to another they manage to
maintain what's called a retina mapping
which essentially preserves a lot of the
spatial and structural characteristics
of the images as they first appear on
the retina so this area this pathway the
ventral visual pathway is the area that
is most aligned with the notion of what
you see rather than where you see it so
it gives you features that characterize
the what the what kinds of things that
are out there but not necessarily their
overall spatial organization the last
concept that will draw upon in the
following is a distinction between
simple on complex cells this was
terminology that was introduced by you
balloon vessel back in the 60s and it is
often misunderstood but really it's a
fairly simple concept all of the purple
cells on the bottom things those
correspond to simple cells there's only
one complex cell shown here and it's the
orange on the right above each of the
lower graphics there's a a small graphic
that shows the response of a given cell
to a stimuli that is inside of its
visual field so the larger box there the
gray box corresponds to the receptive
field of a cell and the white bar
corresponds to the stimuli and in this
case for the simple cell the simple cell
is going to respond to a bar oriented at
a certain orientation and position right
in the center of the visual field and
that's all that it responds to
and you can see off to the right a
little bit the spike train or a
simulated spike train of what result if
you exposed the cell to that stimuli
oops on the right it shows a complex
cell and this complex cell also responds
to a bar at the same orientation as the
simple cell the only difference is that
this in this case it responds to that
bar wherever it appears within the
visual field it's said to be invariant
with respect to the position of the bar
within the visual field but not the
angle right so it's specific to some
things and invariant to others so the
characteristics that we're looking yes
to connect each other we've all the only
technology simple cells connect to
simple cells so you can have a simple
cell all of whose receptive fields in
some sense correspond to the input of
the output of other simple semester and
then complex cells take simple cells as
input complex cells also take other
complex little simpler so we're
interested in these models because they
provide a hierarchical model hierarchy
in terms of the set of abstractions
those abstractions tend to be primarily
in the first levels of the visual cortex
spatial and temporal abstractions these
things are capable of pattern
recognition of a sort that we would very
much like to see in modern AI systems
and pattern completion in the sense that
we hallucinate in some of the controlled
way portions of objects that are
occluded by other objects the invariant
that are learned in these systems
provide an alternative to the kind of
iterative computation that we would
typically do and an algorithm device and
variants allow us to get away without
performing a great deal of sub shirts
and as before the idea that we'd be able
to represent input from various
modalities this stack on the left and
the row on the right is meant to
represent that visual or the temper the
ventral visual pathway that I mentioned
earlier and of course even though it's
arranged on the flat sheet of the cortex
of as a set of separate areas
interconnected by neurons I like to
think of it as the stack on the left
with information feeding in from the
bottom the information from v1 would be
via the retina and the lateral
geniculate in which the raw input and
that input I shouldn't say raw it's
processed in a number of ways and in
particular we're going to assume that we
have a basis
and a set of coefficients that we've
used to pre-process the images as they
would appear in the retina so that they
are both contrast and an illumination
invariant and that's the raw input to v1
and I'm going to introduce for design
elements and by the way each of these
design elements has an Associated paper
and if you can read and understand those
four papers I think you'll have a good
grasp of some of the main ideas in
neuroscience today the first is a paper
by Fukushima on what he called the Neo
cognate Ron and at the time it was
perhaps the most ambitious model for
representing visual information and
characterizing the early parts of the
visual pathways now Fukushima's not the
first person to suggest that we would
represent complex objects in terms of a
hierarchy of conjunctions of simpler
patterns but it was one of the primary
ones in the early days of machine vision
I've been told that in the area of
understanding speech there are
precursors to this that have much the
same flavor of the work of Fukushima the
second one may seem like a trivial idea
but it's essential in the earning that
we do Peter full deck in his 1991 PhD
thesis at Harvard described a phenomena
whereby the way that we perceive our
environment is that we try to slow
things down we try to find
representations that persist that are
stable across both space and time and
the intuition is that the worlds of
blooming buzzing
busy place moving far too quickly for us
to account for and so we need to filter
out that and extract features that move
much more slowly more recently the idea
has come to play in work by Lauren whisk
got and and says now ski in a framework
that they call slow feature analysis
which has much the same idea as the
earlier work by Peter fold yak the third
is yet another simple idea that executed
correctly makes for a very interesting
and necessary balance a relative to some
of the other mechanisms when you perform
in variant recognition what are you
doing you are recognizing things and
ignoring certain aspects in the case
that example that I gave of the complex
cell a second ago what you're ignoring
is the exact position within the
receptive field that kind of sloppiness
as it were allows you to recognize
things quite easily but also results in
in false positives so Ullman insulative
suggested a method that they showed has
biological plausibility and which very
simply you first recognized by using
these invariant methods and then you
filter by looking at how different
receptive fields overlap to make sure
that they're consistent with one another
so the fact that in one area you see a
horizontal line and in an adjacent or
overlapping region you see another
horizontal line those lines have to line
up in order to provide a consistent
overall representation and the final
idea is a work which has the paper that
I would suggest is a paper by tai seng
Lee and David Mumford but David Mumford
has been working on this basic idea
since early in the 90s
and it is an attempt to cast what's
going on in the visual pathways in terms
of Beijing inference and the propagation
of information up and down a hierarchy
now to some extent this is not received
as much appreciation as I think it
deserves in part because human beings
are able to recognize things within an
amazing ability which does not appear
given the timing information to require
any feedback one way to think about this
is that when we're asked to do a simple
recognition task often we will be given
a series of images and then later on
asked to recall whether or not we've
seen those images at all and it would
seem at first blush that you're actually
analyzing those pictures and you're
realizing you're seeing a picture of a
cow and a farm and that's the cue that
you use to realize whether or not you've
seen it before but another explanation
is that you're learning it in much the
same way that you extract a set of
wavelet coefficients from an image or
term frequency vectors from a document
that is to say that you can establish a
signature which is pretty good at
recognising whether you've seen it or
not in fact quite good but not at all
good in terms of understanding the
content of that image actually is so
we're going to work our way backwards
from those four topics and we're going
to start with the basic idea of Beijing
inference in a hierarchical model on the
top graphic you'll see three boxes that
correspond to say three areas of the
ventral visual pathway area v1 v2 and v4
and the the framework is that each box
represents a a random variable
actually a large set of random variables
and the those random variables represent
a marginal distribution and the
distributions taken as a whole the three
distributions can be composited to
construct a grand joint distribution and
it's that grand district joint
distribution which characterizes the
operations of the cortex each of these
boxes or random variables corresponds to
is receives information from the areas
below it and the area above it in the
hierarchy so this is our first example
of a graphical model the individual
boxes correspond to random variables and
the arcs correspond to information that
would flow back and forth capturing the
dependencies between the different
random variables I'm going to use a
really simple graph as an example
throughout the rest of the talk and
you'll have to use your imagination to
to think that this in this characterizes
a cortex but the simple cartoon will
make it much simpler for us to
understand the pieces down the road so
there's only nine variables in this
graphical model the arrows point in one
direction but the arrows simply
represent a conditional dependence and
you can always use Bayes rule to reverse
those arrows so even though the arrows
point in a single direction information
moves in both directions there are no
arcs between the nodes within a level
and that is because initially there are
no arcs we learn those arcs in the
process of constructing the hierarchical
model
use your imagination so that the the
variables they think of those is the
variables corresponding to hyper columns
in area v1 there won't be just five of
them they'll beam something more on the
likely of more on the on the order of a
hundred thousand or so there are on the
order of 10 to the 11th cells or neurons
in the neocortex and 10 to the 14th
connections but as I said each one of
these variables is going to represent a
hyper column which consists of anywhere
from fifty to a hundred thousand cells
so given that as our basis for our
simple hierarchical models and now
remembering that instead of having nine
variables it will have nine hundred
thousand variables or something on that
order we're going to have to decompose
it into smaller components and those
components each component is referred to
as a sub network and the different sub
networks are going to overlap that's a
good thing and those subnets are going
to be sized in such a way they can be
handled all the inference involved with
respect to that subnet can be handled on
a single processor on a single core and
we're going to distribute the process
among a large number of cores or
processes processors so that the subnets
can communicate through the variables
that they share the variables that they
overlap so now given the previous slide
where I showed you one subnet imagine
that that the subnets all correspond to
a node in one level and all of the nodes
that points to all its children on the
level below and what we're going to do
is we're going to explode that
that graph into a graph of subnets where
again the subnets communicate by using
potentials or marginal distributions as
it were that they contribute to one
another in a message-passing
architecture now I showed you a
hierarchy that was pyramid in shape but
in fact the models that will look at
will be anything but that you can think
of them is pretty much squares or
truncated pyramids and the nodes at the
different level will have the following
behavior which is drawn from work
brayton Berglund shoes on the structure
of the other visual cortex in cats &amp;amp;
mugeez as you ascend the hierarchy the
in and out degree of subnets or of nodes
in the graph doesn't change however if
you think of the hierarchy the graph
that we showed of 9 nodes or 9 million
nodes as embedded in the obvious
embedding in the three-dimensional space
as you ascend the hierarchy those the
the the receptive field for the subnets
spans a larger and larger space and this
means essentially that as you ascend the
hierarchy the width of the graph
restricted to a given level becomes less
and less the diameter of the graph that
is the shortest longest path within that
sub graph so at this point you should
have in your mind a picture of a
hierarchy corresponding to the first
three levels or so of the visual cortex
looking something like this it's a it's
essentially a three dimensional graph
where each subnet is described as a
pyramid and the pyramids the base of the
pyramids gets larger and larger as you
ascend the higher
but as I mentioned in the second slide
that one of the main things about the
graphical models that we're looking at
is they're essentially sequence-based
and so we want to think about how we can
handle time so this slide is meant to
represent essentially three dimensions
where the two spatial dimensions are
compressed into a single line so every
horizontal line of cells corresponds to
the entire input at a given time so it's
two dimensionals corresponding to the
two dimensions of the retinal field and
then time recedes into the vanishing
point and at any given time the system
model will encompass a certain set of
those random variables so this is a the
grand model as it were that represents
all the space and time that's available
to the system to make inferences on so
the task of each level and essentially
each subnet is to take its receptive
fields shown here in the little red box
and abstract that and given the model
that I mentioned before of Wis cotton
says now ski and Peter full Jack's model
to extract a feature of that that
changes slowly over time and the blue
nodes on the top are meant to represent
the nodes in the next higher level and
to capture that kind of an abstraction
is there some criteria beyond this is
a tangent I know beyond that there must
be some criterion beyond features I
changed slowly over time he pits
not strangers you have to at least avoid
the obvious there must be some sort of
informative we'll see it just a second
so this is meant to abstract from the
model that I showed you before with the
receptive field for a given subnet
before the nine nodes below and the
process whereby that information is
filtered to ultimately get the slow
feature and there are three pieces to
this and each piece can be thought of as
a form of inference and what we'll be
doing over the next nine or ten slides
is going through the details of each
piece of this inference and then at the
end we'll have a single graphical model
that will characterize all the behavior
and all the inference involved in these
three steps so the very first step is
one in which we take a set of variables
in this case they are spatio-temporal
variables so the receptive field covers
not just space but space and time and we
learn using unsupervised methods the
distinct patterns of spatio-temporal of
change within those variables we can use
a method such as mixtures of gaussians
in order to capture those patterns or in
the case of nominative variables we can
use a method such as naive bayes or
augmented naive bayes the next step we
take in some sense the those patterns or
those prototypes that we've learned from
looking at the spatio-temporal receptive
field and we look at their dynamics how
they change over time and we actually
learn a dynamical model that describes
that that evolutionary behavior in doing
so we learn a transition matrix that
describes the evolution of those
features and we analyzed that in the
third step in order to to figure out
features that were move slowly
essentially what we do is we look at all
those patterns and their change over
time and we group them in such a way
that they now accord with slowly moving
features and I like to think about those
in other top of the framework which
we'll talk about in just a second so the
obvious to anybody who works in speech
or graphical models the obvious model to
use for this is something called a
hidden Markov model and there are two
characteristics of the hidden Markov
model that are essential to our interest
here the idea is that you take an input
space usually a relatively high
dimensional input space such as the
pixels in a small patch of an image and
you reduce it or map it on to a lower
dimensional space and so the downward
pointing arrows correspond to the
observation model or the emission
probabilities for a hidden Markov model
having reduced it to a smaller space we
then analyze the way that those those
lower dimensional variables change over
time and so the vertical arrows
correspond to the transition
probabilities of the hidden Markov model
how would we do that within the
framework that I described in the
previous slides well let's take our our
single subnet as we in our exploded view
from a few slides back and we're going
to construct a graphical model from this
over a series of steps so the first step
to think about is we're going to take
the graphical model within the subnet
and we're going to replicate it in some
number of slices and the number of
slices will roughly correspond to what
we believe the ability of
hyper column to store sort of traces of
its past inputs and then given those
different slices each of which
corresponds to the simple model within
the subnet we link them together with an
arc that indicates that those variables
of the X sub i's in this case are
dependent temporally on one another now
in the graphic that we had four or five
slides ago where I showed you the three
steps two of those steps involved
compression so we took a receptive field
and we extracted the features mapping a
high dimensional space on to a lower
dimensional space the net result was a
compression in space and in the top or
the third phase we took the transitions
of those patterns over time and we
compress them temporally when you
compress time you tend to induce
additional dependencies between your
spatial variables and when you compress
space you tend to increase the the order
of the Markov process that is the number
of past steps that that the random
variable corresponding to the present
time depends upon so somehow we have to
account for these additional
dependencies in our model so back to the
model that we had before we start with
this and the first thing we're going to
do is we're going to introduce a latent
variable and that latent variable is
going to capture the evolution over time
so in the next slide X sub I is going to
disappear for a moment and we're going
to replace it by a latent variable here
the latent variable is of a variable
Sigma and I use that word
the Greek Sigma to indicate that Sigma
ranges over an alphabet an alphabet of
prototypes in some sense the signatures
for the patterns that are recognized
within the receptive field now these are
the receptive field variables and we how
we learn this is we use a method called
structural am and what structural am
does is in addition to learning the
conditional dependencies or the
probabilities that essentially quantify
these higher these graphical models it
also learns the structure so it learns
which variables depend upon which others
and introduces those variables those
dependencies by adding additional arcs
to the graph so when we're finished with
this pup this process we have a model
which you can think of sort of as the
observation or omission model for the
underlying temporal model the next step
is we're going to learn the dynamics and
and so now we have the replication of
the observation model in every slice and
we have arts between the Sigma nodes
indicating that those are temporally
evolving variables the variable at time
T in this case is potentially dependent
upon the variable at t minus delta t
minus 2 delta t minus 3 delta etc and
again we don't know that a priori we
have to learn it in general this can be
a very bad idea when trying to represent
a model in that the size of the
distribution that's necessary to
describe the transitions or the
evolution of this process would explode
in terms of the number of past stages
that it depends on so whatever the
number of prototypes we have
the distribution is going to be the
product of the number of prototypes
times the number of prototypes it
depends upon in previous stages which is
exponential in the length of the entire
chain and so what we do instead instead
of learning a giant table whose
dimension would be the same number of
slices in our network we learn a tree
structure which tends to be very sparse
but still accounts for the essential
temporal dependencies you can think of
what it's learning are the essential
n-grams that are statistically regular
and commonly occurring all right so now
we have our model our temporal model and
we're going to make the Sigma's
disappear for a moment and that's
because given AK Markov model you can
always turn it into a first order Markov
model by simply storing additional state
information within the state variable
which in this case is Sigma but what
we've done by learning just the N grams
the essential n-grams we've constructed
a very compact space and so we'll turn
that into a first order model by having
Omega in this case range not own over
the prototypes but over the n grams that
we learned in the previous stage and now
what we want to do is we want to learn
the slow-moving features we've learned
all the dynamics we figured out what the
commonly occurring patterns we've seen
how they evolved over time and so we're
going to add back our x sub i's which
correspond who as in says the output of
the subnet and what's the role of the X
sub i's using the terminology that Peter
fold yak introduced on the notion of a
slow feature we're trying to extract
things that
and to persist over time and at the same
time that account for the essential
features and the inputs so a brief
digression this is not a graphical model
this looks like a graphical model or not
a graphical model in the strict sense as
it's used in machine learning today this
is a state transition diagram so in this
case the circles correspond to the
states and a simple process and the arcs
correspond to transitions between those
states so I'm sure you've all seen state
transition diagrams this is a simple
example but this example illustrates the
idea of a slow feature so what happens
when you set this automaton loose well
if you started it in state 1 you see it
bounced back and forth between state 1
and 2 most likely for some portion of
time and upon entering each state or
re-entering the state if there's a self
transition the the state essentially
emits an output so as it's popping back
and forth between one and two you would
see it out put something that looks like
a a BBB AAA BBB AAA until finally it's
going to jump out of those two states to
state for and then with high probability
it's going to bounce back and forth
between three and four for a time
emitting a different signature in this
case sequences of a's and sees so in
terms of a slow feature what's going on
you can abstract away from the nitty
gritty details of the low-level states
and say what's really interesting is is
it in a mode where it's emitting A's and
C's or is it a mode words emitting A's
and B's and those are the signature
States or meta states of the overall
process
and that what for years that we'd like
to learn now what would that actually
mean in terms of the kinds of things you
would learn and say learning visual
features while the bouncing back in 1
and 2 might correspond to for example us
very small portion of an image or a
pattern moving across your visual field
so imagine that you're watching a box
move across your overall visual field
each of the individual cells corresponds
to a small receptive field a small
window onto that larger image and so
what you might see is a small Chevron
the corner of a box moving through that
little receptive field and what you want
it to learn is that it doesn't matter
where that Chevron is within your
receptive field you always want to admit
the same thing on the other hand if you
see a chevron moving down maybe you want
to admit something different or if
instead of a chevron you see a straight
line or an end stopped you want to learn
something different so you learn what
those features are and essentially
you're learning and invariant your the
invariants correspond to the modes of
the process that's illustrated here so
back to our model the grand model that
results from all of this is is shown
here and i've reverted back to showing
things in terms of sigma so i can make
explicit the fact that sigma depends not
just on the previous pattern but on
possibly the pattern before that and
before that and the model as a whole has
only one arc that you haven't seen
before and that's the mark the arc
between X of T and X of t minus delta
what we would like to do in order to
provide a precise characterization of
the model is to to specify the graphical
model which we've done
and then to be able to use an inference
mechanism in order to learn the
parameters of that model from the data
the data corresponding to the
information that's fed to the subnet
from its children within the subnet
hierarchy and so that arc between x t
minus delta and xt is meant to capture
the dynamics of this slowly moving
variable in order to make it work that
is to ensure that in fact we do learn
slow features we play a little trick for
those of you who know about using priors
in in learning graphical models or
learning of any sort typically in
graphical models you use what are called
deira schley priors and the way to think
about that is the probability of x of t
given x of t minus delta can be
described as a table the transition
matrix essentially for the the X of T
and what we do in terms of specifying a
prior is we populate an initial table
that looks that essentially provides
what are called pseudo counts and those
22 accounts reflect the bias that we
have to discourage the parameters from
moving too quickly during during
learning using for example expectation
maximization all we do in order to
ensure that we learn these slow moving
features is we take this matrix of
pseudo counts and we look along the
diagonal the diagonal corresponds to the
self transitions and we fatten it along
the diagonal so we increase the number
of pseudo countless the numbers that
correspond to the pseudo counts right
along the diagonal and that encourages
you the system to learn features that
move very slowly so we've been
experimenting with a number of different
methods that rely on these techniques
both using eam applied to the entire
model is showing he
but also learning by composing the
different pieces of the learning process
that I mentioned in the previous slide
namely the emission probabilities use
the emission probabilities in order to
then learn the transition probabilities
for the speed the Sigma's and then
finally learn the transition
probabilities and the class variable the
X sub i's by simply looking at the
transition matrix for the Sigma's and
doing a form of spectral analysis so
that's the internal model that's what
goes on within a subnet and that's how
the subnet learns these slow-moving
features but the subnet is just one part
of a much larger model and this is the
same graph that we had before the
exploded version of the graph and I'm
going to now say how what the semantics
for this entire model are and in order
to do so I have to introduce this notion
of a hierarchical hidden Markov model so
this is again an automaton except it's
not a meat on that's factored into
components so the semantics of this are
that think of the top otamatone
corresponding the states one and two as
being a the top of the hierarchy and
when you're in a given State you do one
of two things either your terminal state
like this state here in the second level
or you're a state that involves a think
of it as a procedure call such as the
state here and the dashed arcs
correspond to the procedure calls so
what would happen in this case is it
would start off in state 1 you would
call the procedure here in state 1 of
this automaton you didn't mid an a then
you'd go to state 2
you may at that point even go back to
state 1 or go down to this otamatone but
eventually the lowest otamatone is going
to go to its final state in which
control returns to the calling procedure
and it continues until it hits a final
state at which state returns to the top
level state and you continue with that
so that's the semantics for a
hierarchical hidden Markov model each of
those automaton will correspond to the
components that are learned by a subnet
and we can cast the whole structure in
terms of a graphical model as shown here
so this is the same this is a
representation of the same atomic ax
that we showed in the previous two
slides it has three levels and these
corresponds to the state of the
different levels in the hierarchy so X
super n the N is the level in the
hierarchy and the sub is the point in
time and we've also introduced some
additional variables that correspond to
the final state and all they do is
indicate when the system finally arrives
at a final state and that is then
dependent upon the next higher level
that models the characteristics of the
procedure calling semantics that I gave
in the previous slide so what we expect
is that every level in the hierarchy
only keeps track of a very short trace
in time we want the model as a whole to
capture concepts that range over our
entire visual field and that span large
portions of time because for example
we'd be like to be able to classify
videos in such a way that we can capture
very interesting features that may
involve not just a single frame but many
frames or span many scenes in
the overall of the overall video so
here's the semantics essentially for
what happens within the level that
allows us to determine how there's a
different temporal resolution at every
level so imagine this is the input to a
given subnet at a given level and each
of these boxes are meant to correspond
to a binary distribution so this is a
binary 0 and binary one and the black
lies in this case correspond to the
lowest level inputs to the system and
they happen at every think of it at
every clock tick but in a given level
that the system essentially has smooth
out the signal so it's capturing a
feature that corresponds to its sampling
at a much lower rate and so the green
indications are the sampling rate for
for this subnet in receiving information
from its children now given the
semantics and the model that we've
described for a subnet as a whole
essentially what it's trying to do is
extract these slow-moving features and
so the output is one that will be
sampled even less frequently and it's so
its spatial characteristics are not
essentially as a as a slow a low-pass
filter but rather that are capturing the
essential components at a somewhat
slower frequency so there's a potential
problem with this if you imagined as we
had in the previous slide a slice
corresponding to every tick and we want
to be able to spot span large scenes
within and a video that might involve
hundreds or thousands of frames or
millions of frames we would have to
build a very large network and in
particular if if you could imagine that
it
every level we have the frequency that
in this little network here how much how
many slices would we need in order for
level 3 to be able to even have two
samples of its of its input and the
answer is that if if we're assuming that
every subnet essentially has four slices
or small n that and that every sudden
that above it samples that half the
frequency then essentially in some 369
slices that's all the information that
our network would be able to get however
we can go at at higher and higher levels
and note that all of these portions of
the graphical model that are shown in
dashed we don't actually have to
represent all we're representing are the
ones shown in black and so we have a
very compact representation that scales
linearly and the number of levels that
are hierarchy okay
so that's the basic idea the models have
been just begun to be tested these four
key ideas there's a lot known about the
cortex but a lot of it is not easily
translatable into engineering principles
we believe that these four design
elements are easily translatable and to
design principle into principles
engineering principles and that's what
we've been doing over the last nine
months or so the progress so far we have
matlab code that runs on in a single
process on such examples as the nist
national institutes standards digits
libraries and data sets the data set
that's being used by dilip george at new
mentor a company whose basic business is
to build neocortex's and we've also
built an MPI prototype that allowed us
to explore some of the issues involved
in building distributed algorithms which
Glenn and rich and Jim are working on
now we've translated the matlab into a
serial implementation that's in c++ and
we're working with a variety of people
in order to build the filters that will
provide the first couple of layers of
the cortex which will essentially be as
I said a method for invariance with
respect to illumination and contrast and
a wavelet filter of a particular sort
and now we are trying to develop the
last bits of the distributed system and
hopefully we'll be able to debug that
over the next month or so and beyond
that I hope to come back here in another
six months or five months and give you
some examples of
stuff that we've been able to do with
this model we are actively pursuing
people to help with the project it's
finally gotten to a point where I think
we really have some concrete tasks that
people can work on and we're seeking
collaborations with people in Google's
actually quite robust machine vision
community to help out with the process
and with some of the component problems
like to talk thank a few people that
helped out with this Ethan Schreiber and
theresa view were the ones who did the
translations to from the matlab code to
P&amp;amp;L dilip George and Bobby Jarrah's new
meta are good friends and certainly have
helped us and they really are in the in
the business of building cortexes some
folks at sanford and kevin murphy for a
piece of code that he's developed was a
great contribution to the community it's
called the Bayes net toolbox which has
also helped us enormously so thank you
and if there's any time for questions
I'll be glad to take them
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>