<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Theoretical Limits of Statistical High Dimensional... | Coder Coacher - Coaching Coders</title><meta content="The Theoretical Limits of Statistical High Dimensional... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Theoretical Limits of Statistical High Dimensional...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/4J9dXvuV1jQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">since I my own host I will not introduce
myself besides saying that as you can
tell from the subject i'm interested in
mathematics now this law that were a
theoretical I put it on purpose to make
clear that this is interesting theory
for reasons that I'm going to explain I
don't believe this particular thing has
immediate glaring applications it's more
of nice to know now it is related to
very practical things I did write quite
a successful program to find the nearest
neighbors however the practical aspects
are different the reason why i chose
talk about this is because the
mathematical interest it's a very my
opinion very beautiful theory so let's
go first introducing the problem and i
would use it by an example suppose that
we are given a set of in bed sequences
and most of them are pretty random but
there are few say there is a pair i put
in them here one next to the other so
that the comparison is easily that are
more likely to be the same than the rest
so the way to find it is they if we go
through all the pairs and compare
between each pair compute the Hamming
distance will find that this pair is
exceptionally close now when you are
talking about small numbers of course
going through all the pairs is very easy
but when you have tens of millions and
if i'll be able to make even billions of
things then you need to do something
better than
looking at all pairs because there are
just too many pairs so this is the
general problem now here I can define a
very particular model problem by saying
that all right i have n sequences of
beads the beats are random with
probability half 0 half 1 and there is
one special pair such such as the
probability that beats are similar is
not a half but p where p is greater than
half so the way to present is this there
is a probability matrix of showing in
that special pair then if you had the
probability of having zero both here and
there is p / 21 the same while while
having zero in 1 and 0 in one sequence
and one in the other sequence is 1 minus
p over to now this matrix tells the
probability of the special pair but if
you are computing the marginal
probabilities meaning summing this and
this we have half summing these and
these we have half so that pertains to
all those sequences they have
probability half half so this one matric
encodes how such a thing is generated
all right now this is an example problem
completely defined now what do we know
about it first thing is that we need
enough data in order to be able to
pinpoint the correct pair even if nobody
will limit our computation because they
it could be that we just can't can tell
it the if P is close to a half for
instance it will it will be so went when
can we do it so we have information
theory the amount of information in the
pair is about to log N or log of n
square or half and square
now the amount of information that i did
that i have in each single bit is the
information a function of p well well
known function so it is zero when p is a
have no information up to a log to when
p is one which is the perfect thing now
we have D bits so the total information
is d times I of P so I take the
difference now what does this number
mean if this is negative it means that
yes I have enough information to
pinpoint the pair if it's positive it
means no I can't pinpoint it but maybe I
can restrict it I can restrict it to a
set to a size of n possibilities this is
standard information theory of course
the question is how computational how
much effort do I need to do so this is
again standard so what what we do is
that we randomly select a subset of the
coordinates subset of K coordinates
where K is about log base two of n why
did because we are going to look at the
values in this coordinate their how the
number of value is possible is 2 to the
K which would be approximately n so it
means that each of the sequence s will
get sort of a place of its own those
Finkel so we have buckets the buckets we
throw in a sequence to the bucket
according to the value of its
coordinates so let me go back so if if i
take the bucket to be the first three
coordinates you see that this one and
this one would fall to the same bucket
and together with this this is a bucket
of size at least three
give another bucket of size 1 and here i
have another bucket of size 1 okay the
little n is the thing that we are going
to see a lot this is the number of
objects between which we are trying to
find the pair's the big n is sort of a
flint acquaintance it is when I'm
looking at the information theory how
many possibilities of for preparing I
have left once I analyzed fully analyze
the data so it's the difference between
the necessary amount information that
they want in order to determine a pair
and the amount of information that I
have in most cases the this difference
is negative so though it's written as a
big end it's actually a very small
number don't do one day yes because
they're here p plus 1 minus p is 1/2 so
this time is a half the sum is the half
and together the sum is one now this is
not a transition matrix it's a pretty
for inter probability matrix
alright so with we sorta too weird worth
choosing random k coordinates and there
was what are the chances that the two
members of the pair will indeed fall
into the same basket ah it's P to the K
because the the chance of the agree on a
single coordinate is P which is about n
to the log base 2 of B so this is a
probability it could be quite small and
this is the chance that one try that we
made we chose those coordinates we threw
into baskets what are the chances that
we found something the changes are
something so in order to be able to find
a correct pair we have to make several
tries in each try we randomly pick some
subset of coordinates and throwing two
baskets how many tries do we need to do
exactly the inverse of this number so
that's the number of tries now in each
try on the average we have to make n
comparisons because in each basket we
expect very few elements 12 no more so
number of comparison would be of order
of n so all together the number of
comparisons that we need is n times this
which is together n to the log base 2 of
2 over P so we can we can check now that
if P is one that everything is perfect
then this is the work is of order n
which is means which we just have to go
and read the data this is the best case
the worst case when P is the half then
this is n square meaning we have to go
and compare everything we can't do
anything so these are the extremes and
the
ability goes between them now I
highlighted here two things that I want
to stress one why do I talk about
comparisons I mean you would say that
the natural unit would we just say bit
operations the reason why I talk about
comparisons is because for instance if
if d the length of the sequences is very
very long and we will say that then with
when we go and compare one to another
it's work d then you would say that the
work goes there to infinity with d but
actually we need to take only few of
them till that we are certain so usually
we just need to take a look at we took
number also it could be the data is
spares so that we have an efficient
representation so I don't so this is the
reason why I don't want to go to go and
count how actually I'm making the
comparisons and just counting the
comparisons this is one thing the other
is that I'm claiming that this analysis
is valid only when the amount of data
that I have the length of the sequences
is very large why is that I mean
apparently what I did before the
relevant no matter what this is the
reason why this is necessary is that the
analysis that we did is correct once we
make our first try but when we make our
second try we might pick on the same
coordinate so that day our our tries are
not independent so when these large they
are independent and then when it's valid
when these small they are dependent and
the actual expression is more
complicated alright so this is the basic
throw a model problem and it's a
standard simple solution and this is the
founder
for everything else so if there is
question about this way to ask it now oh
yeah of course I mean when I'm now
saying standard meaning there is nothing
there that have contributed hail in any
way I'm just presenting the basics it's
the assets which well known for a long
time definitely yes change when you have
not good
alright so you can you can see that you
can do the same thing if we had instead
of beats a three possibilities so it
wouldn't matter now what would matter of
course if you are getting to a more
complicated matrix and and then when the
magic is small enough you can say that
things work sort of the same principle
but they're just more complicated but
you can go to extreme cases and have
continuous distributions etc and then
though some of the principles remain it
might be quite harder to work the
analysis in practice this like many
other things this is an analogy to what
you have an information theory formation
theory they start to presenting you
those nice bit examples but then summer
further the way of electrical engineers
working with all those continuous
signals so in principle you could have
this here too all right we started with
one example now I want to present
another example this is a very important
example and this you would you wouldn't
find a so prominent in the literature
and so this is spares data now the kind
of problem that I worked with is spares
data problem and I believe that this
sort of problem is very very common so
the matrix there is something of this
nature what does it mean it means that
most times we have zeros 00 everywhere
with a very large probability and the
probability so that they if you look at
the modular probability 1 minus 2
epsilon 0 and just 2 epsilon ones
all right now what's your impression
suppose that you are given a large
amount of data as d as large as you wish
is this problem easier or harder than
the previous problem with a reasonable P
say P three quarters what's your
intuition
Oh God well this is the surprising thing
go not so surprising if you think about
it it's actually easier there is
powering in their spare city and i want
to show you why so again i'm sure i'm
the metal that I'm presenting is a
standard method that's done long before
me it's usually attributed to broader
though actually there is an earlier a
publication of a demon bear let's
somehow he didn't get it his name
attached to it but he says that there he
went to search and good on the whole is
not story that they went to search but
this should be attributed to him too and
again the approach is marketing but here
you have to be a bit more careful
because if you just take a random subset
of coordinates it might well be that
they're all zero because there are so
many zeros so you want to have sort of a
certain number of ones now a good
practical way to our way to arrange it
is the following you rearrange the
coordinates in some random way and then
for each of the sequences you look where
are the ones starting in that order so
the first one is here the second one is
there the third one cell and you're
looking at the K of them and the
locations of the oldest case this is
this is what determines the bucket it's
a very very very reasonable system
now let's let's see how it performs this
is actually a in an upper bound I'm not
trying to compute exactly or forms but
showing that it does at least this well
so we denote the number of cord
coordinates that we take is log n
divided by log 2 Epsilon why is that
because what's the probability that two
unrelated points fall into the same
bucket those it could it could depend on
a large number of coordinates but but we
know that at least K of them say look at
the first at least care of these are
ones now this one's they must be once
also in the other so the probability is
it most to epsilon to the K now this
choice of K is so that this is the order
of 1 over N so that again we expect the
buckets to be of a small size now when
we do this now what what's the
probability the two related sequences
fall into the same bucket well let's
look at the first M coordinates M is
arbitrary I can choose it whatever I
want now if I know that those to agree
on those M values and that K of them are
ones then it's fine so we have at least
the probability of choosing those k ones
out of em this is the probability of
having those two values once the
probability of having those two epsilon
well not the most pleasant expression in
the world don't see much worse and we
can we can rewrite it like this which
looks even worse but the point here it
is that a choice of em a reasonable
choice this is so that the expected
number will be K gives this by normal
expression that is sort of or
unity or 1 over m so you do the
computation gives that the amount of
work is like this you have n to the 1
plus something which is small now this
is what I want to stress the larger
epsilon is the smaller this edition and
the better we are now even though notice
that each one the probability is please
suppose that I know that there is one
here the problem is it's preserved in
the other sequence is just a half
nevertheless because those things are so
rare I can make a very very good a
termination they are very powerful
indicators now also thing to notice that
and again you can think of the distances
here is having distances but if you look
at the probabilities really quite funny
things happen so for instance the
sequence that with no one's whatsoever
is as close in probability to something
as itself because see that here this
epsilon this epsilon are the same and we
could of course it take an example of
making this day to epsilon and then it
would be even worse so the major
considerations become a bit tricky it's
not such it's not really right to look
at it as a metric space at all all right
so this is this is the spirit example
any questions about it all right now
since i put that worked here in there is
no way for me to avoid mentioning the
leading theory attached to the subject
most papers take this approach and if i
don't mention it say i would be asked
hey well how do you compare with this
now the project I take is a
probabilistic while this approach is
worst case so it's apples and oranges in
one sense nevertheless there are some
comparisons to be made so what this
theory says actually the algorithm that
it's based it's a even a simpler version
of the bucketing algorithm that we saw
it first except that coordinates are
chosen with replacement so you can have
the same coordinate chosen few times
which seems the bad idea but it makes
the analysis easier because then you
have independent choice and what turns
out is there is the following suppose
that you have you have a problem of
trying to find the nearest neighbors and
there is a ratio see between the random
distance distance between just two
points anywhere and the distance of the
right pair now if you have if you have
this ratio see then the way you can do
with work n to the 1 plus 1 over C so
the larger see is the larger the ratio
between the random distance and a pair
distance the better you can do now this
this is sort of a quote I mean those are
not showing whities though though it's
quite simple now also there is a lower
bound quite quite an interesting paper
and this lower bound is actually valid
not only from this point of view but
also from the random point of view and
it shows it in some sense I do some
assumptions you cannot do better then a
power which also goes near one when C
gets large now let's see how it does in
the particular examples that we saw so
far so in the first example the the
ratio it's very easy to compute because
the random distance is half D two random
points will agree in
and really different in half while the
pair distance is 1 minus P times D so
the work that we get is like this this
is worse than what we had before but you
can say okay this is sort of a general
worst-case thing so well we might have
something worse now let's take the
spares problem so here then we have we
have we have dogmatic so to two random
ones each each one of them will be will
have to epsilon once and there will be
little intersection because epsilon is
small so the distance would be about 4
epsilon D while to correct ones each
each each will have 2 epsilon 2 but
epsilon of them would be the same so the
distance would be about 2 epsilon D so
the ratio SI is to which means a word
like this and notice that there is no
factor that drives this to one when when
epsilon goes to 0 so I'd say that this
is a quite unacceptable now of course
one can say that all right this is
because this is and it is a very general
bound and the worst conditions but maybe
when we look at the actual method things
work better now you can tell me if I'm
wrong but I really did my best to look
and see which methods they recommend so
I went and googled locality sensitive
hashing and there they point a paper
local except ashing scheme based on p
stable distributions i look in there and
there is a particular metal they
recommend exactly for spares data so i
believe i like i did my best to see a
direct the recommended thing and it is
quite a clever scheme what it does
instead of looking at coordinates it
does render projection that the idea
goes to the Johnson their leader Strauss
theorem but here instead of fair using
no normal or discrete variables they are
using a cushy variables see which have a
distribution without variance and the
reason why they're using them is because
they go well with the l1 matrix and
they're so so what they do is that for
each sequence they are taking and can
computing a sort of a signature which is
one bit that is computed out of the
whole sequence by taking combination of
values and looking and looking at the
sign of the thing and you need to take
again k of these random signatures with
different values project into the
buckets and see what happens now the
idea of random projection seems very
very compelling you start with a problem
in 20,000 dimensions you do any
projection you have now a problem in 20
dimensions with the distance structure
about the same this is what the theory
guarantees and this is what is actually
delivered could anything be wrong with
that well if what we were to do is to go
choose pairs of points and compute the
distances then there is nothing wrong
with that the distances that we would
get in a 20,000 dimensions is about the
same as the distance that we will get in
the 20 dimensions and everything is fine
however notice that the methods that we
did before did we go just to pick pairs
of points and compute their distances no
we looked at particular coordinates now
once you go and
do a render projection those coordinates
are lost forever now the effect that it
has is quite dramatic so if I think that
I would I wouldn't go now through the
details they are written and they are
not very complicated but it turns out
that this particular method on this
example has performance not only that
this doesn't tend to one it's even worse
than what we had before now I actually
heard from three different people
independent which ride the random
projection approach not necessarily
using a cushy variables but the this
thing doesn't depend on the Cauchy and
each one of them found that it didn't
work this is why
any questions about that how many
projected down into there's a large
number positive example
no problem
well it's then no it doesn't matter much
what what matters is how we do the
projection if if by doing it you go and
you scramble your bits then if I think
things are bad of course if if you if
you compute a 1-1 projection to a large
number of bits and do it in an old
scrambling way then this is fine but
this is again there the approach that
that we started off the idea of going
and they combining all sorts of
coordinates together by slamming them up
it is the destructive thing and then if
you think of it it's it's quite obvious
actually how could we think otherwise
all right so this was a sort of negative
part and be and believe me this is the
sort of thing that I can't avoid because
it is the reigning theory now the way I
prefer things is taking the
probabilistic model of having a matrix
now it's not necessarily two by two and
and I and I have a data that is
rendering generated by it and and I have
a certain pair or could be many pairs it
doesn't matter that we are related the
others are not related and it's a
question of what we can do under the
circumstances
now the general question of what we can
do is is just too difficult I mean if I
were able to prove that any algorithm
here couldn't do better than something
non trivial then for me to prove that PA
is the different than NP would be quite
simple since it isn't no i will not do
this so i must restrict my class of
algorithms I can't allow just everything
the class of algorithms that I concerned
there are well the sort of the things
that we saw before bucketing algorithms
meaning I have buckets I show things
into buckets now all all the algorithms
that are actually practice our bucketing
algorithms however I insist not just
that I have bucketing algorithms but did
I have the buckets fixed in advance
independent of the data now many
algorithms are actually construct the
buckets using the data now I cannot
prove things about about this situation
because you could do a very complicated
construction and then who knows by
miracle things work I certainly couldn't
prove that they wouldn't however when
you have random data I'd say that using
the data in order to generate them the
buckets doesn't seem to to give much
value because if think as long as them
we are not using our special pair and
you are using everything else then
everything else is just random you might
as well have a fixed buckets so the only
game that we have is by how this special
pair affects the buckets the intuition
is that it doesn't help much but of
course I can prove such a thing all
right so what does the pathetic a luta
means I have a list of buckets now here
the back
actually pairs because this is this is
an another regionalization instead of
having just a single set of sequences I
have say two sets and I want to compute
pairs between one and the other now this
this allows also to consider a
non-symmetric problem when we have a
large set and a small set so when I'm
taking buckets I am allowing buckets in
one sequence and bucket in another
sequence now an example of why this sort
of thing would be necessary is that a
thinker if you have a really general
problem you could have that a special
pair are anti-correlated where one of
them is one the other one tends to be
zero in order to design buckets there of
course the buckets for the one and the
bucket for the other must be different
otherwise there is no way so in so in
general I really forced to do this now
so I have a list of bucket and also
before I describe the bucketing
procedures composed of two rice i do
first try for into buckets to be second
try etcetera here the description is I
put all the twice together here is the
audience of buckets I go and try out
them all right now what are the
properties of such a fee first of all
there is some success probability and
they're just that if I'm if I'm looking
on those those sets the probability that
they might my pair would fall into them
this is a success probability I have the
I've probability that both of them fall
into this bucket or into this bucket or
into that bucket this is this this union
is the set that I succeed and its
probability this is a success
probability this is obvious
now the work probability is a bit less
obvious I have you this expression what
does it mean now this i have i have n0
is the number in one set and one the
novel in the other said you can just
think of them both as equal to n if you
don't want to think of different sets so
n 0 the probability of bucket 0 how many
elements of the set 1 I expect to fall
into to this bucket this the number that
expect to its companion so the
multiplication is the number of
comparisons adding all these things is
the total number of comparisons so this
is this is certainly an element of the
work necessary however this is not the
only thing if we allowed only this we
could make very very very small buckets
and do it seems a little work there is
another work of just throwing things
into buckets how men so this is how many
elements are thrown into this into the
first bucket and we have to start and we
have to some because even if we do
things and we don't succeed in order to
match pairs we still have to throw them
in so all this expression is certainly a
lower bound now can-can such a thing be
realized now the this this thing is
really this is just a number of
comparisons but throwing things into
buckets it might be necessary to do more
work you can you can think of of an
algorithm that you have a prepared list
for each element to which bucket seat
belongs you see an element you make a
lookup and that's it then this is really
really the amount of work necessary but
it is a bit cheating because they the
num the list their size is a
financial in n so in order to realize
such a thing we actually need to do a
sort of a tensor product make a make a
projection depending not owned all the
coordinates but on some of them make a
projection on the others etc and combine
them in a tensor product and then such a
thing can be approximated at least I
hope that they did that such a thing is
an absolute lower bound should be clear
hopefully all right and now the result
and this is something that I find quite
surprising now I wrote the information
to rebound that the thing that we had
before so that we can compare so now I
remember that before we had the times
the information now I allow that to have
different matrices in each place sort of
generalization then the information
about this the log of the number of
possible pairs minus the available
information when this is negative we
have enough information in order to
determine which is the correct pair now
this is my result it says that the
amount of work is supreme over some
things of something that depends on the
size of the set the success probability
and i have to subtract information
functions now this information function
depends on the matrix but it has three
parameters and with particular values of
these parameters it is the same as the
original information function
this expression certainly takes some
time to get used to first of all notice
that though it's quite complicated it's
neat in the sense there are no funny
constants there is no big Oh little Oh
see not even two point three everything
is just right now yes mm-hmm well you
can say that you is infinity when s is
one probability successes is one now in
in actuality in order to ensure
probability of success one dude it's
necessary to do a large amount of work
say N squared so the realistic thing is
to put s not one but say one minus one
over N or something and then you
wouldn't be infinity it would be just
something large
now what's the meaning of of the of this
sort of statement now we have lot of
pieces of information coming from the
different coordinates so now think that
we have we have this bunch of bits and
think of it as sort of given fixed thing
for this fixed thing we have those fixed
parameters and now you just look at how
making a small change in here changes
the behavior of the expression so for
instance if we increase the size of the
set from n 0 to 2 and 0 what happens to
to the work it increases by a factor of
2 to the lambda 0 so this lambda 0 is
actually the exponent of showing how the
work increases with the size of the
first set now then again this how it
increases with the second set meal how
how it behaves when you when you're when
we are changing the requirements of for
success and this is how much we gain
from each piece of information now the
how much we gain is a bit tricky because
think of the situation that we have lots
lots of data when these in sort of goes
to infinity then each particular piece
of data has very very little value not
only that each one of them goes to 0 the
Sun altogether would go to 0 2 because
the information that you have in each
one coordinate is almost contained in
the others so in that situation when
these large then the meaning of this of
all this expression is really the
question of for which values of lambda
this information goes to 0
misinformation function
of course you could have data off of
different quality so you could have some
some pieces of data for them the
information has an Onan a large value
and they would use the amount of work
considerably now it's not this necessary
to work with all those parameters you
could take the n 0 equal n 1 lambda 0
equal 1 lambda 1 and they take the new
large towards infinity and you are left
with sort of one parameter but what one
parameter is necessary it cannot go and
be just like that now this is a lower
bound and there is also a statement that
this is asymptotically correct in some
sense so the way it's proven is also a
bit reminiscent of information theory
that the lower bound is proven by sort
of information like inequality and the
upper bound they claim that we can
achieve such a thing is by constructing
a random bucketing code so i will show
in a very cursory way the idea behind
the construction the proof and then they
all try to answer questions it's not
it's not a simple thing so certainly
takes time to get used to now they this
information function is not a simple
thing not a surprise by now lower the
expression does have a sort of beauty of
its own aim
they a cool back leave Libra divergence
or you can just say the entropy distance
place here a very large role this is
sort of very information theoretic
expression and it goes here to a large
extent now how such a thing is is proven
so in order to get the idea just just
let us take that the case when you have
lots of data and then the inequality
this is also a quite interesting
inequality that the work is less than or
equal success probability x powers of
the size of sets
so when we take this example and we ask
Lucas plugging those things how well we
can do turns out that instead of what we
had n to the log base 2 of 2 over P we
can approach n to the power of 1 over P
and that's the optimal power
another thing is that if we have an
unbalanced set one size N 1 the other of
a smaller size and one to a smaller
power &amp;amp; windus power is small enough we
can do with linear amount of work also
quite a surprise Howard are these things
done so the construction so for instance
in this case as I said it's just a very
random buckets we are we are choosing a
random random points drawing a humming
ball around them that contains about one
end for the data so this is this is the
basic thing and we have to do tensor
products of this and it works so the
same approach of the building random
code information theory works in here
too in the more general matrix the
construction is more elaborate but again
it's a random construction as for the
inequality house such an inequality is
proven so the crucial step is showing
that if the information function is 0
for 2 matrices then it it's equivalent
for it being 0 under tensor product this
is working some inequalities none of
them is is complicated but there are
some of them so this is not a thing to
present here and then you know in order
to prove the sort of thing I want this
this by the way is a finger of interest
by itself look at this inequality what
it says is that the probability of
having a and rectangle
the rectangle is less than or equal
products of the probe day of its
marginal through abilities to some
parameters that have a relation between
them if you know if the teller grand
inequalities or other probability
inequalities you will find the this sort
of thing interesting and this is proven
now very simply and that's like that we
had before that that we show that from
the information of a single matrix I get
information of the tensor product then I
can just look at the tensor product and
reduce to dimension one insert into the
definition and I get the inequality
immediately and then there is another I
won't go for this but this is all what's
left very very simple and neat
inequality so this is essentially it as
you gather there are certainly a non non
trivial details the information function
is not it's not simple to compute it's
not like what you had the information
that there is just a for a formula
involving logarithms here it's a thing
that you really need to run a small
smaller computer program to compute but
it is it is computable and it gives the
balance as to what you can achieve with
using buckets no matter what you do so
old all this thing is sort of a parallel
information theory Rock more complicated
than the study information theory valid
for this particular case
so I got at the end i have a list of
bibliography people are interested in
the theory of the Sphinx but first
questions questions
well I don't interpret this is that I
made everything clear well there's
neighbor problem
base
passions on as we get them get a new
video
forgot it
that's like your own valve set properly
set aside
sighs answer
try to find if this one
yes but in this situation they the large
set is being processed have such a
scheme where you it seems to me like
this one of the advantages ls8 states
can free process
queries
is that the right interpretation and how
much work is their buckets of all been
computed knowledge work
that learned said
eat its its its dealer it's still fine
there the bounce that you have if you if
you divide by n it would still be the
work of finding one using the prayer
computation however the way the way it's
presented here they the amount of of
data that you have to keep is very large
so usually when you have the situation
that you / compute things it's important
for you that the pre-computation will
result in a compact data not much larger
than the original while this can violate
it and the this is certainly a drawback
of this general approach
oh ok so I Fink's and for cup</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>