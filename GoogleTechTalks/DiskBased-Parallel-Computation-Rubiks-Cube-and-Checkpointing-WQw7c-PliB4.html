<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Disk-Based Parallel Computation, Rubik's Cube, and Checkpointing | Coder Coacher - Coaching Coders</title><meta content="Disk-Based Parallel Computation, Rubik's Cube, and Checkpointing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Disk-Based Parallel Computation, Rubik's Cube, and Checkpointing</b></h2><h5 class="post__date">2008-03-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WQw7c-PliB4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's a pleasure to introduce
professor Jin government from
Northeastern University I am Greg
malevich from a produced him a present
government has worked on high
performance computing and he will give a
talk about his recent research results
with that thank you hi so as you see i'm
running the high performance computing
laboratory at Northeastern University
these are six of my PhD students who are
also working on various aspects of this
effort so when Greg invited me to come
here and give a talk I offered him a
choice of three topics and he said that
all three were interesting so I sort of
gulped and then I thought about it a
night then I thought yes actually there
is a single story really that explains
all three topics so that's why you see
the three-part title so this is another
one of those talks which says that the
world is coming to an end I'm sure
you've heard many of them so in this
case what's happening is in some sense
we're reaching the end of Moore's law
but that doesn't mean the end of
progress it just means we have to go in
different directions to continue to make
progress so it's becoming a very
exciting time for example memory chips
are no longer growing and they haven't
been in terms of density very fast at
all and this has been the case for many
years now memory chips certainly are not
growing in speed anymore and that's been
true for over a decade we do have large
Ram available on server class
motherboards but the commodity market
doesn't want to pay for that and so if
they don't pay for that then we're stuck
with commodity motherboards that only
accept four gigabytes of RAM but what's
happening at the same time well CPUs are
not getting faster but in some sense
we're getting more of them on the chip
we're getting more and more multi-core
and so this is putting more and more
pressure on the RAM and to make things
even worse one of the other things which
we find very interesting to investigate
is the the heterogeneous computing
so in particular for example right now
I'm teaching a course on the
general-purpose GPU programming and this
simply puts even more pressure on the
RAM now we're asking the RAM to do more
and more so we need a new way to write
our programs at least for those that
don't run solely in cash if you're using
RAM you have to think harder and if
you're using RAM and you want to go to
eight and 16 cores then maybe even the
RAM won't be enough and now you have to
think harder about where is it all going
to spill over onto so our view is that
disk is the new ram and so we just have
to take all the old solutions we have
and try to reorganize them at a lower
level in the hierarchy so disk is the is
going to be the new Ram we claim and RAM
will be the new cache and then the cash
will simply be an ultra fast cash so the
argument here is that the bandwidth well
before I go into this argument let me
just say a little about what our group
has been doing for the last decade or
two so we start out working a lot in
parallel computing at a time when the
assumption was that the bottleneck is
how do you get more cpus to work on it
and then at some point because of these
issues that I'm talking about we feel
that the the issue has shifted over and
so while there are many groups that are
very interested in pedda hurts computing
in our case we're very interested in
petabyte computing and in fact what
we're very interested in also is any
groups out there for example here at
Google which might in fact have datasets
approaching or beyond petabyte because
then the ways to manipulate those
datasets efficiently they start to
change and while you can always use the
old programming methods to do it using
the old programming methods is very
error-prone it's extremely hard to debug
now when you're working with many disks
you have your one second bug your one
minute bug your one hour bug your one
day bug and then you get to the one week
bug
and the one week bug says that on press
on node number 27 there's a problem with
file number 133 now what do you do to
debug that so you need new tools and
this really is the general direction
we're going in so let's look at the idea
that disc is the new Ram why do we say
this if you do it correctly the
bandwidth of a disc is about 100
megabytes per second so if you take 50
disks a 50 node cluster these days
that's just a departmental cluster five
gigabytes per second so but what's the
bandwidth of RAM also on the order of
five gigabytes per second so in terms of
bandwidth if you can use the 50 local
disks of your cluster in parallel then
you have the same bandwidth as a single
Ram subsystem so we want to view the
distributed disks of a cluster as if we
have a new kind of machine a machine in
which we have perhaps 10 terabytes or
more of shared memory shared ram the
shared ram is on disk so and then as far
as the CPUs if you have 32 CPUs think of
them as 32 cores or if these are quad
core then we have a hundred and twenty
eight cores accessing your 10 terabytes
of shared RAM and that's the way we want
to think of the machine well of course
we have a problem when you have 50 disks
and you try to put 50 disks on it the
latency doesn't get any better so what
do you do if the latency doesn't get any
better the answer is you have to
reorganize your data structures and your
low-level algorithms your access or
algorithms you have to do it in a
careful way so you don't overload the
network and we've been studying how to
do this now for about five years we've
gone through a series of case histories
in especially in computational algebra
but then more recently we decided to
showcase the example of Rubik's Cube to
say that these methods are now working
and we view our future now as we know
the techniques we know how they interact
now we want to develop a kind of mini
language extension that's going to
automatically call
techniques and make it easy for the
non-expert to use the 10 terabytes of
disk on their cluster as if it's just
Ram and there's an interesting story
they are two but before we go into that
story let's talk about rubik's cubes
since it is a topic which is popular
among many people it certainly managed
to hit the press in our case when we
were able to announce that 26 move
suffice and at the end you'll see
there's even a short addendum to to that
story but let's begin the story so the
story is that the Rubik's Cube was
invented in the late 1970s by a man
named rubik already by nineteen eighty
two people wanted to know well how many
moves do you need to always solve
Rubik's Cube what's the worst possible
position and how many moves will you
need to solve it then well people
already know of one particular position
in which case you need 20 moves and in
fact there been some people who've been
exploring it and now come up with
thousands and maybe millions of cases
where you need these 20 moves so at this
point the best guess is that either you
need 20 moves all ways to solve it or
maybe 21 but there may be there only a
few examples of this 21 case and we just
haven't found them yet but most people
believe that's where the story would
stop okay what can we do today well
starting in nineteen eighty-two they
were looking for this number how many
moves does it take to solve Rubik's Cube
and they this was called God's number
and they were figuring already 1982
essentially without using computers in
any significant way had already decided
was between 17 and 52 by nineteen ninety
three of us at Northeastern actually did
the problem for the two by two by two
cube so this is a case where we had a
four megabyte workstation and we needed
to squeeze the
the million states or so on to on to
this it's equivalent to solving the
three by three by three with just the
corner cubies if you want to try that
and what we did is we found a data
structure that allowed us to do it using
only one megabyte of RAM and so we're
able to show the number of moves needed
going back to 3 x 3 x 3 read in a
landmark computation show 29 moves in 95
just about a year and a half ago or two
years radu showed 27 moves last year we
brought it down to 26 but we brought it
down to 26 by taking a different view
than read and radu you have to use a
divide and conquer where you split the
problem into two pieces they tried to
split it into two equal sized pieces and
they got 29 and 27 moves we decided to
split into two very unequal pieces one
of which was much too large for the RAM
much too large for the aggregate RAM of
a cluster and another piece which is
quite small and very easy to to analyze
and so the large piece required us of
course to use the parallel disk-based
computation so this is what made it a
showcase for us and we're not at the end
of the line there we believe we can go
lower still and as I say at the end of
this portion I'll tell the rest of that
story but in a case last year we had
this paper 26 move suffice using this
different approach where we just take 20
we divide split it into a very large
problem in a very small problem so there
are about four point three times ten to
the 19 states or people say that I
should call it I think it's 43
quintillion solutions by human beings
are more or less along the lines you
would think for example solve one face
then memorize a number of mu sequences
that preserve one face then using those
sequences go back and try to solve the
rest of the cube while always preserving
that first face so in the computer we
try to do something like this also
accept the computer has a much bigger
memory so we can play that game in a
better way so here's the way one would
present the cube generators up down
front back left right so the reachable
states are just everywhere you can
arrive at by making those moves the
number of states is given as before so
we consider so we need to break it into
two problems to break it into two
problems will take a subgroup of all the
moves this subgroup is simply going to
be babies bet if I point is up here well
let still allows ourselves the moves up
and down but only left squared right
squared front squared and back squared
so this means instead of making
90-degree twists we insist it has to be
a hundred eighty degree twist now you
can't reach all the states so this is
what was done in 1995 using this idea
one can then take the cube the size of
the subgroup and this gives you your two
problems a problem of size two times ten
to the ninth and two times ten to the
tenth to problems of about equal size
and at that time in fact running on a CM
five at the time if I remember so one of
the supercomputers at that time Reid was
able to find a fine bounds on this and
get the solution of 29 moves suffice use
you find shorter solutions up here at
this level and then down inside the
subgroup you will you you can allow
yourself to make all moves so roughly
speaking this is how we're dividing it
the language comes from a mathematical
group theory but the concepts I think
are quite quite accessible regardless
it's just the names are a little
different so the full problem we'll call
that the group we're going to split all
the states those four point the 43
quintillion states into what are called
cosets cosets of a subgroup in some
sense we have this subgroup which is a
subset of states but it's preserved
under a certain subset of the moves and
then we have various images of that when
you allow yourself to make any move and
one can easily show that you can
partition all the states into these
cosets and so this already from the
you can already see what the game is
going to be the game is going to be that
wherever you are say you're over here
and you need to find the shortest
solution maybe it's too much to search
43 quintillion but you can jump through
the cosets and find a shortest solution
in the coset space then jump down now
that you know you're inside the original
subgroup from which everything else is
an image look at where you are and
proceed to make moves here so figure out
the worst number of moves up there the
worst number of moves down here and this
and this and you have a new upper bound
for Rubik's Cube so okay so the idea is
pretty simple and and so what happened
is in this case in 1995 they decide to
break it up into two roughly equal size
problems because that produces only a
billion cases to check and if we're much
more than that they would have run out
of storage they wouldn't would have run
out of ram it would have been a problem
so by making the two problems equal size
that's great if they're unequal then one
problem is going to overflow the
capacity of the Machine and of course
that's done purely in RAM at that time
people we're not thinking of using disk
for the computation so these are some
details which aren't that important
roughly speaking the key things to take
away from this is we say we're going to
get to upper bounds we're working cosets
base number one and will show that the
worst case is twelve possible moves then
once you arrive at the subgroup will say
the worst possible case is 18 possible
moves and then we'll add them adding is
pessimistic because in general you won't
always arrive at the worst case here you
usually arrive at some average state but
all we can prove is opposed you land in
the worst case 12 plus 18 well 30 moves
advice but in fact you can isolate a few
of the very worst cases and they're not
too many and then you just solve them
one by one and so now you can show well
one really the worst cases don't count
and we can cut it down to 29 moves and
then radu playing a very similar game
doing more extensive computations cut
down the same approach to 27 moves so so
then what did we do as I said we want to
break it up into two problems of
different sizes so here we see 6 times
10 to the 13th vs 6 times 10 to the 5th
6 times sin sin to the fifth is great
now we're practically doing the whole
problem inside cash nevermind ram so so
q is incredibly small and then on the
other hand the 10 to the 13th is a
problem it's a problem for RAM it's a
problem for the aggregate Ram of a
cluster one needs to do something
different but if one does something
different there's pay dirt at the end so
again we we have our two problems will
say that we can consider using old
generators and we'll build a subgroup
saying that we're going to allow
ourselves only the squares of these
moves this means we'll only we will only
allow 180 degree rotations if we only
allow 180 degree rotations now the
reachable set is this small number six
times ten to the fifth and but the nice
thing about this also is the these also
it's a very symmetric set of moves
whether you choose to make the move u
squared first or l squared first it
doesn't matter just turn the cube on
another side and u squared looks like l
squared or if you choose to make d
squared first turn the cube on the other
side and it looks like d squared so
you're going to get the symmetries of
the cube and that's going to cut down
your problem and read and radu also knew
this and they used this in their case
but in their case they didn't have quite
as many symmetries since you does not
look like l squared you and l squared
are different so now we have in some
sense the most possible symmetries and
we get to cut down the problem even
further a little bonus for this approach
so once we cut down by symmetries now
after symmetries we're looking at large
problem taking 10 to the 12th so 10 to
the 12 let's see 10 to the 9 is a
billion so now
are saying a trillion cases and the
small problem is tiny a trillion cases
to check well that's not so bad for
computers not at all we just the in
terms of CPU time it's clearly doable on
any reasonable cluster no problem except
if we're going to find those shortest
paths over here and now if there are a
trillion of them where do we store them
in RAM so that's the key question so we
have a trillion cases to check the CPU
time is not a problem it's the storage
that's killing us and what we claim is
that this is symptomatic of many of the
newer problems that we're facing it's no
longer the RAM that's the problem is the
storage where are we going to store it
all and if you don't store it in ram you
can stored on disk if it's just video
great streaming access you can store
your seven hundred megabyte video file
but if you have a lot of small pieces of
information how do you store that on
disk and in our case we do each state
can be represented in a very tiny way
ultimately we use some tricks to
represent each state in only half of one
bite so we have all these states each of
which is half of one bite and if we're
not smart about we have to continually
do random access through these states of
which are half of one bite each and that
would be a disaster on this complete
disaster you want half of one bite you
need to load in at least the disk block
and maybe a hundred kilobytes just to
amortize over the bed latency so in any
case let's continue with Rubik's Cube
before I go back to the disk based
computation so for Rubik's Cube we
divide we get our two answers sixteen
moves in the coset space and then 13
moves in the tiny group once you're done
with a cassette space 16 and 13 is 29
this is without any further processing
role just from the pre-computation alone
so the pre-computation alone already
gives you 20 29 moves which in some
sense ties what read got after he was
done then we do the sum of the post
computations 15 and 13 this brings it
down to 28 moves
and then on another side we can actually
bring it down a little more this will
make more sense in the later slide where
I'll point out how we finally get down
to 26 where we see our savings but in
any case we go from 29 to 26 moves by
getting getting rid of some cases where
we just do a case-by-case examination in
fact my student following this original
approach probably is going to get down
to 25 moves what will here before the
end of this though is in fact there's
competition somebody else just recently
just before I came here announced and
provided paper saying that he's he's
also reached 25 and using a different
method and what we're excited about for
the future is now to combine the two but
let's get to that so let's first
understand what is we we did and then
we'll talk about a little about the
future and then get back to the parallel
dis based computation so here we see
what's going on in the coset space this
is the large problem in the large
problem we said we were able to cut it
back to 16 cases well already up here
you can see one place where there's
clearly something good to do to get
below the initial 29 moves from the
pre-computation we can definitely
examine these 17 cosets one by one and
show in each case look these really
don't really take 29 moves they take
much less and that's a very fast
computation extremely fast yeah even up
here there's a lot of hope because this
is only 300 million so effectively what
we did is at this end we did a
case-by-case analysis and cut it down
and then at the low end we're able to
cut it down also at the low end for
example level 2 well at level 2 we have
only three elements and one can examine
each of those three elements in co
headspace and show directly right here
that in principle for all those cosets
at look at level two we would need 13
plus 2 or
15 moves but in fact you do a
case-by-case analysis and your show no
you only need 14 moves so you can show
that you don't need 15 moves total when
you include these small property need
only 14 moves so therefore this needs
only one move less here we need only one
move less here here we need one move
less here one move less here one move
list here we did a more intensive
analysis and we actually show you can
get away with two moves less than the
naive estimate so and so there we are
one two three moves we save 29-3 is 26
good and and so this is more or less a
summary of what I did what I said it
happens to use something called Topsy
task oriented parallel C C++ it wasn't
essential to our computation but it was
convenient it's a task-oriented package
for easily doing parallel computation
and because we already knew it in-house
we knew how to use it very efficiently
so the parallelism didn't stop us at all
the hard part was actually debugging the
disk space computation and here you see
the two moves lined up side by side in
fact one can even say the two approaches
and as I said the two approaches just
recently Thomas Ricky Key has shown how
to move this down lower even to 25 and
so now in our column we want to take his
idea and apply it using the large cosset
space just a little bit about what the
techniques were and then we'll come back
to the parallel disk-based computation
so fundamentally there were two ideas
one we've discussed a lot about point
number two we use we're in the end up
using large amounts of intermediate disk
space seven terabytes for a hash array
and why do we do this well a
fundamentally we're doing the kind of
breadth first search when you do breadth
first search you get a frontier when you
get a frontier or open list you have to
check is this a state that I've ever
seen
before if it is then throw it away well
how can you check that the usual way
we'd do it when we're in RAM is we use a
hash array so already this points to one
of the problems that we're going to use
disk as the new Ram we can't use hash
arrays directly we need to do something
different if you've seen the abstract
you've seen what the trick was there
instead of doing standard duplicate
illumination elimination we're going to
capture a lot of accesses of the hash
array and then we're going to use
something called delayed duplicate
detection but before going on to that
I'll just say the other trick that we
needed to do what we were doing is we're
using these symmetrized cosets that's
this co set space where we've already
taken account of the symmetries and
we're getting and we're able to get 10
million multiplies per second so and
this uses a certain amount of
mathematical group theory certain
computational techniques some of which
have some small added novelty but
fundamentally the the major ideas were
already known before so it's in any case
what we're able to do then is we're able
to do a kind of table based
multiplication and the tables are kept
mostly in the l1 cache since the and
that's also critical for getting 10
million moves per second we have to work
with an l1 cache if we even go to l2
cache it's going to slow down and at
that point the CPU time really does
reten to become a bottleneck but by
keeping this in l1 cache we can make
moves blindingly fast and now the only
problem is the disk storage so we don't
need to do too much of this this is
simply meant to convince you that there
are easy ways to decompose the problem
of making a move in and so instead of
needing instead of having one huge table
saying here's the list of all cosets and
we want to make this move we can break
it down into pieces we can break a group
into corner cubies and edge cubies and
then if we do it in the right way there
nicole details we can set up a table for
what happens to the corner cubase when
you make a move and what happens to the
edge cubies now you have two tables that
are approximately the square root of the
size of the original table well let's
take the edge cubies it turns out you
can split that up for example also for
the edge cubies you might split it up
into the possibility of just flipping
the faces but not actually moving the
vases or moving the edge cubies to a
completely different location and so you
can split this up into two different
tables so you continue to split a huge
table of state comma generator into many
states the state based on edges looking
at flips only the state based on edges
looking on how you move the edges the
states based on corner cubies and you
end up with about under ten tables all
of which mostly fit in l1 cache or
potentially always fit Nell one cache it
depends what cpu you're using so this is
just more the same and now what I was
saying is a bit of the future where it's
going to be even more interesting I
think so Thomas wrote Kiki recently
announced just before coming here as I
said in this paper that he has a new
complementary approach that can produce
an upper bound of 25 moves so that's in
the other column and what's exciting to
us is that we believe that we can do
something like what he does and we can
get down to 24 maybe 23 maybe less we
haven't had time yet to analyze it and
see but fundamentally just to not leave
in suspense and I encourage you to look
up his paper I assume he'll be putting
it he'll be making it fully public
sometime in the near future
fundamentally what he's doing is he
looks at the worst case up here he looks
at states which at levels in which there
are so many states that would be very
hard to analyze each one case by case
and what he knows is you don't have to
analyze each one case by case
you can just take States at random and
analyze them and show that these states
of these states at random you can do
them in very few moves and they say hey
if you can do this state in very few
moves all the states and neighborhood of
that can also be done in a reasonable
number of moves and what we haven't been
sure but what Thomas Roth Kiki has shown
is that that works the topology here in
which states are connected to other
states by moves the topology is dense
enough that it really works to just take
random states find nearby states and
show that if a random state can be
solved in very few moves there are a lot
of nearby states a lot of it and
eventually you have to cover every state
but it turns out that that really can be
done using the other approach of using
this approach and so we believe if it
worked over here it should work over
here to stay tuned to see what happens
on that end okay next what are the
longer-term goals so it's it's all very
nice and doing Rubik's Cube but that's
not really what's going to solve solve
recurrent problem of limited RAM and
more and more cores on a cpu so so why
do we do it well because it's there
that's certainly true in fact another
reason to do it is simply it provides a
crossroads where people from different
disciplines can get together and compare
their techniques so they're people from
AI who do search there are people from
operations research who do branch and
bound which is closely related there are
even formal verification methods where
you try to represent many states
biological formula and using various
methods like this the different
disciplines can actually attack the same
problem and it's not so often that you
find an example like this so therefore
it's a very nice example for this
purpose it's no long instead of having
the different disciplines each working
within their own discipline and saying
we have a good method and we don't talk
to the other people now there's a common
case where we can actually compare next
the other thing
that is because the world is running out
of RAM and this is the case that I made
earlier so if the world is running out
of RAM we we want to show in fact
contrary to what i said in my first
slide that this is not the end of the
world that there are ways to get around
it so this is an example of how to get
around it but we can do better so the
world is changing this is what we said
before well so so essentially the claim
is what I said orally suppose we did
have a single computer with 10 terabytes
of RAM and maybe 200 CPU cores would
would that satisfy your need for more
space and we think in most cases it
would so what our claim is then is just
look at a departmental cluster may be 32
quad core nodes it's very cheap these
days certainly under a hundred thousand
dollars and getting a cheaper all the
time want to put a 500 gigabyte local
disk on each one it's they're very
inexpensive now and at that point the
numbers work out so so let's look at
that so when is a cluster like a 10
terabytes shared memory computer so
these are the little riddles that's fun
to ask so we can assume so well in this
case we'll make a very modern assumption
assume only 200 gigabytes of your local
disk is free and the rest of it's being
used after all part of the thing to
realize is when you're designing a new
cluster you can buy a 512 gigabyte disk
you can buy a 200 500 gig or a 250 gig
disk there's very little savings in
Boeing the 250 gig disk so usually what
you'll do is you'll buy the 500 gig disk
and say maybe we'll have expansion in
the future let's just pay the extra two
or three percent five percent on the
cost of the whole cluster as insurance
so the cluster can grow organically well
in that case you now have 200 gig per
node
free and now you have your attend
terabyte a shared memory computer it's
just sitting there being unused those
disciples are just falling on the floor
assume 50 nodes so again it's a
computation very similar to the one I
just went through and here i make the
argument about economics again so let's
go to the other side when is a cluster
not like a 10 terabytes shared memory
computer that's very important we
already spoke about the latency problem
here but in addition to latency the very
first thing we have to observe is now we
have to write a parallel program we
can't just say all we wanted to 10
terabytes will use one core thank you
very much that's all we need you really
have to write the program as a parallel
program and then you do have to worry
about the latency and then finally if
you're going to write it as a parallel
program then you are going to be using
the network and if you use the network
then can the network keep up with a disk
so give various hand waving arguments
just to say that this is reasonable
ultimately for us we do it because it
works we've done it for five years on a
variety of examples and none of these
issues have ever stopped us yet but
looking at it introspectively and trying
to figure out why we offer these hand
waving cases so as I say answer number
one well we used this architecture and
it works the empirical answer we've done
a number of computations in
computational algebra than Rubik's Cube
down here in progress we're doing it's
the technical name is co set enumeration
but effectively just think of it as a
pointer chasing problem and in spirit
it's sort of the same as the problem of
converting a non deterministic finite
automaton to a deterministic finite
automaton a classic problem in computer
science that that everyone in computer
science would know so so what we're
doing is we're trying we're looking for
cases that should be hard and then we're
trying to wipe them out one by one and
then at the end we say well we think we
have good coverage and that's really
going to be the argument not that this
will always
we can never guarantee that it will
always work but if we can demonstrate
good coverage across most common
applications then maybe that makes the
point okay so coming back to these three
questions we'll ask them again so
question number one we plot require a
parallel program so we have to access
the local disks of many nodes in
parallel so our bet and in a sense we
still have to prove it is that any
sequential algorithm that wants to
create gigabytes of RAM based data if it
really wants to create gigabytes of RAM
based data probably there's some
parallelism in it it's unlikely that it
plans to create the gigabytes of data
byte by byte or word by word one after
another and it cannot create the next
word until it seemed the very last word
and the gigabytes of data this I think
would be a highly unusual example so if
we are creating gigabytes at a time
there should be again it's a hand waving
example but in every case we've seen so
far there should be some natural
parallelism we really don't depend on
building a gigabyte of data a word by
word in the way we work day to day then
there's the latency problem of course
and and that's that's probably the big
question that's the one that where you
really have to do some work to get
around it so the observation here for
the latency problem is that solutions do
exist so in the case of Rubik's Cube we
basically we're looking at the frontier
in some kind of state space search and
there the solution was delayed duplicate
detection ddd so it's not a term that we
invented it was has been around before
but we've tried to extend that idea
intensively so it implies waiting until
many nodes of the next frontier are have
been discovered and then once you have
and you can remove duplicates how can
you do this well let me just give you
well well what you would do is you'd
probably want to use a hash table so if
you're going to do this you want to use
a
table aha hash tables those don't seem
to work well with discs do they if you
want to make one hash access by
definition you're using a pseudo-random
function by definition you have the
worst possible locality so for hash
tables what do you do well the answer is
yes it's highly non-local and if it were
local then you have a bad hash function
but you can wait until there are
millions of hash queries or billions or
trillions depending on what level you're
playing this game and then once you've
done that then sort on the hash index
and scan the disk to resolve the queries
so hopefully it's clear we have our
billions of hash queries we apply the
hash function in using streaming access
we proceed to produce a hash index for
each one and now we sort the data based
on the hash index and now we can read
the hash array and in our case for
Rubik's Cube it might have been the
frontier compare the frontier where the
hash array in hash index order it all
goes very fast and and this is this is
what worked for us to in the Rubik's
Cube computation the whole computation
ultimately took something on the order
of some number of cluster days let's
call it a cluster week and that was it
and the longest time was typically just
waiting in the queue to get access to
the cluster so good so of course we have
to sort but as is well known there are
techniques like external sorting which
allows one to do that in fact for
Rubik's Cube we didn't use this method
we didn't sort there are more efficient
methods to which you can find if you
want to read our papers more closely in
this case there is a Anna bin based
approach in which you take the hash
index you look at the high bits of the
hash index and say are the high bits of
the hash index that will be my bin
number and I'll put it in that bin and
that bin of course is simply a separate
file on disk so you segment all of your
queries into different bins and now you
can load up each bin and only one
portion of the hash index if you do it
at the hash array if you do it right at
all fits and RAM and now you don't need
any
morning and fewer accesses to disco
celoso the whole thing speeds up pointer
chasing you can play the same game yeah
if you have to chase just one point or
you have a problem just like if you have
to do just one hash index but if you're
willing to chase billions of pointers at
the same time you can make this work and
so for us that's the solution to pointer
chasing tracing strings again it's
something very similar wait until there
are millions of strings and tracing
strings becomes very much like pointer
chasing so what we claim is the
low-level details can be worked out and
part of the problem now is just to make
it easily accessible to the programmer
so they don't have to write it and debug
it themselves and that's what we want to
do in the future finally there was a
third question the third question can
the network keep up with a disk and
again it's a hand waving case but we've
looked back at what we've done and it
hasn't worried us and so we're trying to
think why it was it never a worry for us
and so ultimately the argument for us is
that the point-to-point bandwidth of
Gigabit Ethernet is 100 megabytes per
second the band with a disk hundred
maggot megabytes per second they match
so the only issue is the aggregate
bandwidth of the network how many
point-to-point streams can we manage at
the same time if we need order all
communication than we would be sunk well
do we need all the oral communication
for that answer we go back to other
parallel programs the world has a long
history of many parallel programs by now
so let's look at parallel programs which
are operating solely in RAM and not on
disk when they were operating in RAM
they never seem to be bothered too much
by the fact that the network couldn't
keep up if they do all to all
computation or to all communication so
in fact in practice or most previous
parallel programs did not overload the
network and so will make that
hand-waving argument that is that on
fast Ram they were not overloading the
network we want to use slow disk instead
SRAM if we use slow disk now for sure
we're not going to overload the network
because we can't push the network as
fast the discount can't take it in as
quite as fast okay so long term goal as
I've said is that we think we know what
the building blocks are can we build a
language on top of it a language and a
runtime library and that's the natural
thing to ask next so now that we have
the experience here just a few of the
techniques that are well-known there are
quite a few others can we take it to the
next level and then make it really
accessible where anybody can start
writing this and running it writing
these types of programs running it on
their own departmental cluster and we
believe that's doable my student is
working on that now first thesis Daniel
Konkel who I should give him credit
Daniel conco as the one also who was the
first author on the Rubik's Cube result
and what he's doing is he's building on
some previous experience in our group so
there's this other joint paper with
another student of mine erik robinson in
which we make the following observation
if this is what you want to do then
there are some space-time trade-offs for
using the additional discs and these so
think initially just to be a traditional
space-time trade-offs that you've heard
everybody talked about forever in
working solely in RAM and what we've
done is we've identified a number of
techniques and we've observed that in
each case you can view it as a spacetime
trade-off and this is very natural
because if one of these techniques was
better in space and better in time then
we would simply ignore anything that's
slower in space and time both so the
only techniques that survived this
initial filled so filter a technique
that is slower in space and slower in
time is not competitive we throw it away
we therefore want to just filter through
and keep those techniques which are
either faster in time or faster in space
and what we're left with is a hierarchy
and in this hierarchy we see that for
the
the names don't don't matter you can
read the paper if you're interested in
what these things do if this were a
longer talk I might be tempted to talk
about some of these techniques because
they are quite nice some of them were
known in the past some of them were
invented by our group but the key idea
here is well here we have growing space
but hopefully less time and in the past
we would cut it here and say when you
run out of RAM that's it so use
whatever's closest to this dividing line
in our example here we'd stop at the
landmarks method and use and use
landmarks and said we have no more RAM
we can't do any better what we're
observing here is well actually there
are two cutoff points here and here if
you're willing to go to disk you also
get these space-time trade-offs and in
our experience we have very often found
that the best dis based method is
actually faster than the best cram based
method because the RAM based methods are
having to do all sorts of dynamic
on-the-fly compression or in order to
squeeze it into Ram whereas if you just
lay it on disk and sometimes even have
duplicated data so you can access what
you need more efficiently you can do
better so so therefore the philosophy of
our group is to take this dividing line
and say this is what we'll use we can't
do any better because we've now run out
of this space and so it's a kind of it
seems to be a bit of a paradox we're
saying that any time that we run out of
ram and we want to do a computation
faster then the solution is to use some
of that very slow disk this that is a
thousand times slower than RAM or more
and by using this very slow disk instead
of the fast Ram we will make the
computation go faster but it works it
works as long as you think about
space-time trade-offs and and so if the
world had only one algorithm then yes
you would stop at the end of ram and
that would be it because ram is faster
than disk but the world has more than
one algorithm the world has a bunch of
algorithms and nicely ordered in fact
according to the space-time trade-off so
and often this is the winner not the one
based on RAM it's just that
traditionally we always thought of disk
as being too slow so the old uses of
discs were as a file system as a
database and as a swapping region what
we want to say is this now have a fourth
use this can be used actively as a
substitute for RAM and RAM becomes the
cash so it's no longer just a file
system database and swapping region but
an active substitute for RAM so I'm
going to skip this you're welcome to
read this slide later or even the paper
if you want the full details but it's
just an example of one of the techniques
we used because what I really want to
come down to come to now is another work
in our group which are very excited
about so this is the third part of our
title the user level checkpointing so
checkpointing has a very long history
there have been in some sense successful
checkpointing programs for at least 20
years condor with its process migration
effectively introduced one kind of
checkpointing very early on and so but
and and we find we need checkpointing
but in fact the kind of check pointing
this out there and available doesn't
really do what we want of course we have
a long running parallel computation long
running parallel computations crash when
they do crash you have to figure out how
to restart there are right nows and and
plus its we do need the parallelism we
do need distributed checkpointing many
of the solutions previously were either
sequential solutions or else they
depended on specific software like a
particular dialect of MPI message
passing interface we want a general
solution that will solve the problem
once and for all because then it opens
up still other opportunities for the
future that we'll talk about in
so the kernel level checkpointing has a
problem the kernel level checkpointing
requires that you modify the colonel
makes sense and if you have to modify
the colonel you can no longer bundle
your checkpointing package with the
application and just distribute as one
large package because that means the
user is going to put it on some unknown
colonel out there on some unknown
cluster and maybe they don't have the
necessary kernel level support maybe
they haven't modified their colonel
maybe they don't have root privilege so
maybe they're not allowed to modify the
kernel on many clusters the typical user
does not have root privilege so at this
so our thought is well let's try it with
user level checkpointing this means
operate only in user space no root
privilege needed and we can embed the
whole thing in a library in user space
well now checkpointing divides into two
problems problem number one copy all of
user space memory to disk well luckily
that's easy problem number two copy the
state of the kernel to disk that depends
on the API for the colonel but luckily
the Linux kernel today has a very rich
API not only system calls but the proc
file system interface and using this we
can find out everything we want to know
about the state of a process the colonel
will tell us will copy that information
into user space now we copy the user
space data onto disk and we want to
restart we invert the whole thing we
bring it userspace memory back in and we
then tell the colonel please put the
process in the following state we want
these file descriptors open we have
these pseudo ttys please give them back
to us these this is where we did an M
map don't forget that and so on and so
on the package is the MTC p it's freely
available open source you're certainly
welcome to all look at it and our plan
in fact is that in the next month or so
we want to produce Linux packages so
this is still beta level software let's
say but soon we want to have our version
1 point 0
so what what is it what is the strategy
that we used here well one one or two
points maybe are worth mentioning
because they have some novelty perhaps
foremost is the idea that we use a two
level architecture so we had earlier
work on something called em TCP
multi-threaded checkpointing which would
take a multi-threaded process and
checkpoint it so this is one step beyond
the original single threaded
checkpointing packages that we've had
ever since the 1990s certainly and now
rather than extended into DM TCP our
thought is let's build the check
pointing at two levels n TCP knows only
about its own process and nothing else
it knows nothing about sockets for
example or pseudo ttys UNIX domain
sockets and so on it knows nothing about
forking child processes and the MTC p is
going to handle all the extra things so
and then bmtc p what is ready we'll say
I've now copied the state or the state
of your process that relates to
multi-process computation sockets pseudo
ttys and so on now m tcp you should go
ahead and do the rest and m tcp knows
about things like open file descriptors
because that's unique to one process and
it just does the rest so by having these
this two-level strategy we also have
greater portability we hope on any
system which already has the equivalent
of nccp working we can then build the
MTC p on top of it so and we use in DM
tcp we use a coordinator process that is
mostly stateless this could be a
weakness so therefore we want to keep
the end the coordinator very fast so
it's mostly stateless and has only small
pieces of communication with the
processes the main job of the
coordinator process is to run through
five barriers and tell everybody we're
now up to the next barrier go
so that scales very well you can easily
imagine a scaling to a thousand
processors maybe even ten thousand
processors and the coordinator can
probably handle it and in fact one thing
we're very eager to do is if we can find
some some guinea pig some groups who
really would like to check point a
thousand process computation that would
be a wonderful opportunity for us we
believe we can do it right now but we
need the right collaborators on which to
demonstrate unfortunately in-house we
don't have our own thousand node
facility so we don't have a thousand
node computation going on that we can
just try it on but we're currently
searching for good partners there and
the other comment is the distributed
part turns out not to be a hard problem
at all other people have also thought of
these ideas using sockets you can send
each sender can send a cookie and then
flush and when you flush it you're
flushing the kernel space buffer into
the user space of the remote process the
remote process keeps reading until it
reads this cookie the remote process
then says okay we're ready I've copied
everything into user space we can check
point when we restart we will simply
invert the process the remote process
will send the buffer data back to the
sender and the sender can resend it and
then wake up the application threads we
use the idea the technique well we have
transparent initialization because we
take advantage of LD preload in Linux so
we can wait this allows us to create a
checkpoint thread per process we can
create wrappers around certain
infrequently called system calls clone
bind except listen by doing this we can
detect any kind of network communication
or inter process communication as it
happens we can fork it would be another
example as we detect these infrequently
infrequent calls will simply record ok
there's something else for us to worry
about when it comes time to check
pointing we can also slightly modify
these if we need to this also gives us
easy virtual is
where we needed and the same time we
make sure not to put wrappers around
something like read or write because
those are used constantly and we want to
keep those fast and untouched okay so I
think this gives you an idea maybe I'll
just say one other thing one thing we're
excited about for a user-level
checkpointing also is that what we're
doing effectively is we're modeling the
colonel and then we recreate the state
in the colonel upon restart this is an
approach that we can use not just for
standard checkpointing of ordinary
process like this but you can use it in
other domains to you can use it to
checkpoint graphics if you view the
graphics as some black box in which you
just need to recreate the state gdb
sessions so the approach itself I think
is a very powerful one by working with
in user space instead of kernel space we
can now interact with these other user
space libraries and rather than
checkpoint the entire library by brute
force we can inquire about their state
and then recreate their state so
advantage now is we can create very
small checkpointing checkpoint images
much smaller than anything you would get
from say the excellent technology by by
VMware and by and now you can have many
small images you can put these images on
your USB key you can save them for two
months and come back and look at a
scenario you're looking at and so on so
just as we have operating system images
the idea that you should be able to have
process images that you carry around and
reload on to any computer even images
that involve more than one process
that's something that we're very excited
about too ok thank you any questions
yes to make some of this automatic is
that is the cat coaching what you were
referring to so the question is that i
mentioned language extensions to make
some of this automatic and is the is the
check pointing what as i'm referring to
so primarily the check pointing is
helpful but it's not the main technology
the main technology we're thinking of is
we already have runtime libraries that
we've had to build just in order to make
our own programs work can we extend
those runtime libraries and then put a
nice and tactic layer on top of it so a
user who doesn't know the internals of
our library can just write something
very simple and along these lines
there's an interesting idea called the
tilty desktop which which for me is
something of an inspiration the Tilted
desktop says that in some lecture halls
they don't want the speaker to put his
coffee on the lectern because it can
spill and ruin electronics so how do you
do that this lectern I guess is is a
little slanting and the answer is well
you make the lectern slanted and now if
you put the coffee on it would obviously
spill so you can't put the coffee on it
you can only use the lectern for what it
was meant for and so in some sense this
is what we want to do for the parallel
disk-based computation we will make
accessible the possibility of using disk
but we do not want the user using this
for one word operations at a time that
would be horrible and so we want to bias
the language in terms of streams and if
the user does try to build do one word
at a time will force them to build an
entire stream first and say okay now you
can write your one-bite and so users
hope it will be biased into using this
the right way not the wrong way yes
very well with long delays and yeah
absolutely it would be very interesting
to to think about lazy function lazy
lazy functions in language design I just
say that our group is primarily in
high-performance computing at
Northeastern University we also have an
excellent programming languages group
and if we can persuade them to get
interested then there would be a very
natural collaboration there but with an
our group our expertise is not in that
direction yes
no yes do you want to ask the question
or so I just assume it good I think yes
I think I know what's going yes exactly
so the point made was that we're
assuming that all sockets are
intracluster and what happens when the
sockets leave the cluster and so
informally we like to talk about this is
the problem of checkpointing the world
ultimately you start with one process
that has a socket open to somebody else
maybe a a demon for for routing or
addresses or something and so before you
know it you're actually going off site
and trying to check point some some
database for for addresses for routing
for other protocols so this is why in
fact we think that the user level
approach is has even some advantages
over kernel level approach in a kernel
level approach we say that we want to
keep the Colonel's small and simple we
have access to everything in the kernel
and we'll just do the right thing but
then you get into the issue of what
happens when the colonel ignorantly
tries to say I have a peer over here
outside of my cluster I should be
talking to him what do you do you have
to teach the colonel something now
you're mixing kernel and user space the
design becomes a little harder and it
can be done and there's some excellent
people who have done excellent work in
that direction but in our case our
argument is that unlike the colonel paid
people we don't expect to get a hundred
percent coverage will be quite happy if
we can get ninety percent coverage the
most common applications and so what we
do is we look for these cases and then
every time we find a common case like
this we simply patch it we say oh for
example nscd was one that we had a
problem with namings think it's naming
service caching demon I forgot the
initials exactly this is a demon that
will get get addresses from you for
off-site and it's it's obviously outside
of user control
gee lipsy depending on how you configure
it will automatically go to this demon
and ask for the information and then the
demon can cash it for everybody on the
whole cluster this is a disaster for us
so we detect when we're talking to the
nscd demon and when we are we use other
routes instead in order to get the
information we need so we have to add a
wrapper around the system call it says
wait a minute you're talking with the
rest of the world that's that's not
allowed here's how we want you to get
the information instead yes we can
restart the computation to get
information again but our computation
will not be exact in the sense that we
have to go get the information one more
time and the demon is allowed to give us
a different answer the second time that
that's completely correct but the
alternative yet the problem we had was
that on restart this demon was actually
creating a shared memory a shared memory
portion in our memory which is shared
between the demon and us and when we
restarted we would look in the shared
memory region which we're sharing with a
demon and look to see if we had the
information there and if not we'd ask
the demon please add it to the shared
memory region and of course it's the
demon said what shared memory region
you're a new process I never saw you
before so ultimately we're forced into
these workarounds yes any other
questions yes Greg
in order for for the power computing
system why they adopt it should be easy
to
Express programs the model system so
Marcus is an example of this base
computation where the reduced price is
delayed however the latency in fact I
cylinders is amortized over a large
amount of data that gets to transfer the
providers so you have measure the one
example repeat your presentation you
also mention a few building blocks that
remove the data so I comment on the
model of computation that successful
good so the comment is that if you look
at map reduce its employing a similar
philosophy in the first you do the map
and then after a certain amount of
latency you do the reduction and so so
here also you're you're taking advantage
of this and you can try to implement
things likely late duplicate detection
and so on that we discussed in this talk
and I agree MapReduce in fact has a
philosophy that is very close to my
heart rather than try to solve every
single problem possible in a way that's
guaranteed that no one can ever create a
program that will fool your system what
we hope to do is try to get this 1995
ninety-eight percent coverage and so I
think my impression of the MapReduce as
it exists today is it wasn't designed to
do all possible parallel computations
what they did is they took a certain
model of computation which who knows
maybe it's eighty-five ninety percent of
the needs of the users I don't really
know and i'd be interested discuss this
in here to find out exactly what
coverage you would say and then they say
okay we're going to do that portion of
the applications very well and very
easily for the user and so our goal now
is to take it up to let's say
ninety-five percent or 98
% but this is work in progress we don't
have a full solution today by the way I
should also say as I said earlier we are
very eager to if anybody does is dealing
with say petabyte datasets even hundred
gigabyte we would be very interested in
working with you and seeing if we can
help solve your application because for
us this is would be a wonderful test bed
and it's exactly this problem of
coverage that we're facing ourselves if
we're going to get to the ninety-eight
percent coverage level then we need
these examples with where you're using
petabytes of disk and we need to
demonstrate their techniques work there
also or else we can never reach the
ninety-eight percent level another
question I petabytes of data nice
contracting well I want to reduce the
computation from taking four days to fix
drop of that time to walk through it all
two hours yes that means switched it's
exactly reversing into what you're
talking about it d paralyzing and doing
things incremental okay so yes as I
understand you have many petabytes of
data and it takes days perhaps four days
to walk through it and be nice to reduce
it two hours
good good or just get the answers
walking through it is not absolutely
required so yeah and and I'll repeat our
philosophy and then I'd be very
interested to talk to you after the talk
to see where it would or would not map
onto your problem but for us what we
would say is first of all if if we're
going to need to look at most of the
data then we have to get around a
hardware ball to neck and the best way
to get around the hardware bottleneck is
to spread the data over more disks so
then we can do that and so and so rather
than take the four days maybe we can
bring it down to less than one day by
looking at this in parallel in a special
case maybe we don't need to look at we
might need to look at only one petabyte
or 100 gigabytes which would be
relatively fast and then at that point
we need to organize the computation now
that we have much more parallelism than
we started out with we better have the
right tools to deal with this extra
parallelism which is higher than we
started with and that's the that what
we're trying to analyze yes you have to
increase the parallelism of your
application to
to basically hi Delaney says you're
hiding latencies to provide a large
amount of parallelism I did you have the
increased trial is that one question the
second one is you just mentioned
something which is kind of interesting
increasing the size of the distributed
deployment to increase speed you can do
the same thing using memory instead of
this two or perhaps even stages have you
thought about going in that direction
okay so so the two questions as I
understand it are number one in our work
did we have to use additional
parallelism by spreading the data even
further among the disks and number two
have we considered the opportunities
using solid state disk or or or other
sim yeah competing technologies so so
first in many of our computations we on
pencil and paper we analyzed the times
and it turns out we can predict the
running times very accurately in advance
it's hard to pick an exact number but
let's say within about twenty five or
thirty percent we know how long it's
going to run and the reason we can
predict it so accurately is because we
know how much data we're dealing with
and we know what is the speed of the
disk and the speed of the network and
the speed of the RAM and the speed of
the CPU on the internal on the CPU
intensive portion inside the loop so we
can analog determine the time very
accurately and we know where the
bottlenecks are surprisingly on many of
our computations the ball donec is not
the CPU and it's not the disk it's very
often the ram the ram is too slow and
and this is rather funny to us and this
happens partly because of the specific
algorithm we use in the time-space
trade-off I referred to earlier so
depending on where you are in this time
space hierarchy you may actually need to
do extra CPU computation in order to
squeeze the data down onto disk and
that's but that's often our preference
rather than expand the parallelism to
instead try to make the data fit in a
smaller space and that's another way of
getting around the problem when we do
that we have to spend more CPU time of
course if this is raw data there may not
be opportunities to do this but in
structured data like we're dealing with
we can
and then what happens is the CPU we
spend more CPU time but in today's time
of multi-core computing spending more
CPU time pushes the bottleneck not into
the CPU but actually into the RAM itself
so the other question is what about
competing technologies such as SSDs as
opposed to disk so these are very
exciting to us also we're certainly
keeping an eye on them this is the
mature technology today that it is
already on every departmental cluster so
in some sense it's free because they
already bought it in advance where they
needed it or not and it's just sitting
there in terms of SSDs for the future
and other technologies SSDs from what I
understand today have a bandwidth that
is similar to the band with a disk you
don't really win on the bandwidth side
but where you do win in some sense is on
the latency side because SSDs have
smaller natural blocks that you're
dealing with and you can access those
blocks with Leslie with better latency
so yeah it's hard to comment further
except to say that this gets rid of the
one critical objection that we keep
facing and using this as the new ram in
reducing the latency so so this would
simply take the algorithmic tricks we
have and leverage them still further so
that hopefully the coverage and
applications would go even further but
it's hard to say more at this stage
without actually doing specific
experiments are there other questions
well thank you very much
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>