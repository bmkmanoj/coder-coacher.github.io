<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Research and Education in the Clouds: Experience at the Univ | Coder Coacher - Coaching Coders</title><meta content="Research and Education in the Clouds: Experience at the Univ - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Research and Education in the Clouds: Experience at the Univ</b></h2><h5 class="post__date">2009-05-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/euX9Ti_mFV8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everybody thanks for coming
I would like to introduce to you
professor jameel in from the University
of Maryland Jimmy has been working with
us and participating in Google's
academic cloud computing initiative
program and he is here to tell us more
about it thanks very much all right well
thanks for the introduction drea um so
today I'll be talking about my
experiences uh doing research in
education in the cloud sort of sharing
with you guys what I've been up to over
the last couple years okay so what do I
start off by telling you guys a little
bit more about myself so I I got my
undergrad masters and PhD all from MIT I
loved it so much they decided to see
their whole time and they had to
actually kick me out but I worked in the
computer science and artificial
intelligence laboratory I finished my
PhD in 2004 and and since then I've been
at the University of Maryland in the
information school there and recently I
was just awarded a promoted to associate
professor with tenure so that's a big
milestone in my own career um thank you
thank you uh so I think of my own
research as lying at the intersection of
NLP natural language processing and
information retrieval and for the last
couple years a lot of my work has been
in this buzzword worth no dia field of
cloud computing okay so how did I start
getting involved in this I'll start
involved in the academic cloud computing
initiative so many of you might
recognize Christoph vasilia who is I
guess the brainchild behind this whole
program this started two years ago the
Google IBM academic cloud computing
initiative with a few pilot institutions
Marilyn was among one of them and we're
fortunate enough to to be invited to
participate in this and I'm that
basically a lead faculty of Maryland on
this and more recently this
collaboration has been extended to
include the National Science Foundation
in a in the cluster exploratory research
program okay so um what is what is this
what is what is this all about so for
those of you that hack pearl this were
sort of a one-liner might sort of
kick-out it right so the idea being you
know when in doubt throw more data
editor right so take a global
substitution for for knowledge and throw
data at it right and this is something
that that the NLP and the I our
community has loaned for a long time now
so this is a work that's often cited in
the literature by Michelle Benko and
Eric Brill in ACL 2001 so over here we
have a the performance graph of a super
super vies machine learning task on the
x axis we have the amount of data
training data that you that's thrown at
it and on the y-axis here you have
accuracy I think this is a this is a
class a grammar grammar correction so
naturally you see the more data you
throw at it the better the performance
get holding else all else equal so more
recently there's been work by your
colleagues at in Google research so this
is this is a brand said L from google
research and the same idea applied to
machine translations on the x-axis here
you have the size of the collection you
use to train your language models and
the y axis this is on a machine
translation task you have the blue score
which is a standard metric for
evaluating machine translation
performance ok so once again you see the
same trend the more data you throw at it
the the better the point for the
performance gets ok so I've given
variance of this talk several times
before and and this is actually the the
the the first time I've given it at
Google and so I think literally Here I
am preaching to the choir right so so
what's interesting here I mean what's
what's different about this particular
talk I think what I want to sort of
focus on is this part of it so if you
look at the X scale that's on the order
of trillions of tokens ok so actually I
was just having dinner with one of your
colleagues yesterday and it was
remarking oh I just go and start up a
language modeling server and I can just
get access to all of this but but the
problem I want to focus on today as well
how do we get the rest of the world to
that point how do we get the rest of the
world to the point where web-scale
web-scale processing natural language
processing and information processing is
available for the masses
and sort of the the question that goes
hand in hand with that is you know how
do we educate future computer scientists
and this is something that's given some
thought over last couple years about um
and so I'm going to break this down into
two problems that have sort of
recognized and I want to share my
experiences on these lines the first is
the prerequisites problem the second is
what i'm calling the the resource
problem and i'll go in them one at a
time ok so the prerequisites problem
it's actually fairly simple how you tack
log sheet of problems right you apply
divide and conquer this is sort of
everything that we learn in cs101 right
you take there's some amount of work you
divide it into little work units you
send them through each worker you get
intermediate results and you have to put
everything back together again right so
this is naturally breaks down into a
partition problem on the one end and a
combination problem on the other end ok
but in reality it's a little bit more
complex than that right you in order
forget to get these things to work you
need to this is our these are the things
you need if to learn if you take a
district of parallel distributed
computing course right you learn about
things like fundamental issues right how
to worry about scheduling data
distribution synchronization
inter-process communication fault
tolerance you worry about architectural
issues right this is where you learn
about Flynn's taxonomy uma versus Numa
etc etc you'll learn about different
programming models all right message
passing and shared memory models you'll
learn about common problems and they go
by very lively names as dining
philosophers sleeping Barbara's etc and
finally you learn about various
programming constructs to deal with
these common problems that you learn
about barriers conditional variables
mutexes etc etc and you learn about
about things like a master-slave
architecture / Sumer producer-consumer
cues etc but the upshot of all this is
the reality is that the programmers
shoulders all the burden of managing the
concurrency right at a very low level
right so this comes this creates a
problem if you want to teach web-scale
NLP or I are right so so what are what
are the options if you want to teach
web-scale text processing or information
processing well you have a couple
options the alternatives are
one you sort of do are you sort of
brushed aside the topic and say okay you
know we're work on reasonably sized
datasets that you can fit on your laptop
and everybody goes home happy but of
course this is not a good solution
because this is exactly the problem that
the academic cloud computing initiative
was designed to to address in the first
place okay the second alternatives you
is you set up a set up a separate
prerequisite chain right so before you
take my web scale text processing course
you go and take a separate course in
parallel and distributed systems or
something like that but for the issue
with here is it sets up a long
prerequisite chain and you're going to
lose people along the way okay and the
third alternative is well you you
essentially integrate the prerequisites
in your own class right so on I would
teach a web-scale NLP or I our course
and I spent a few weeks teaching
everything that the students need to
know in order to be able to work at that
scale of course the downside here is
that while you spend half your course
teaching them the fundamentals of
exactly the things I showed on the slaw
on the previous slides ok so the
alternatives are we're stuck between a
rock and a hard place in another hard
place right so I think you can see where
this is going right so MapReduce is
comes to the rescue here right so allows
you in abstraction that allows you to
design algorithms at a much higher level
abstraction right you write the mapper
and reducer and that's it the runtime
handles everything else and the nice
thing about it is now now now that there
is Hadoop it's really accessible to to
the rest of the world right so who do
implement open source implementation
MapReduce in instead of sawzall there's
pig sort of the roughly comparable
equivalent in the open source world
there's hive the data warehousing
application from facebook so there's a
vibrant open source community that's
evolving around this okay so by using
MapReduce Hadoop as the underlying
substrate allows me as a faculty in a
course to focus on teaching and LP and I
our algorithms and and move away from
system level details right so so that's
sort of a nice solution to the problem
number one the prerequisites problem
okay so I'm going to talk about a move
on
talk about the the second problem the
resource problem okay so it's been said
that with cheap community of commodity
clusters and simple distributed
programming models like mapreduce you
add them together and you have data
intensive computing for the masses okay
so you should ask me well what's the
problem here well there's still several
problems one of them is that cheap is
relative right so we've lowered the bar
a little bit right instead of hundreds
of thousands of dollars for two to play
in this game you now only need tens of
thousands of dollars you know these
numbers are really rough but you get the
idea you still you've lower the bar but
I still don't think it's slow enough so
that's that's issue number one the
second issue is the difference between
research and education so this is sort
of a mismatch between funding models so
in in in the university in academia
hardware resources are for the most part
k for by research grants right and as
such they should be used for doing
research and so so in actuality all the
hardware that we at university skin can
acquire are procured in this manner and
so it's a little bit tough to say to
turn around and use these cluster
resources for teaching and in fact very
few places do actually have dedicated
teaching clusters for doing web scale
processing so that's that's another
issue okay so what are those solutions
so I'm going to talk about three
separate solutions the first is charity
all right get somebody to give it to you
for free and to that in that respect
we're fortunate enough to be at Maryland
be part of the cloud computing
initiative so we we got our we got some
resources for free the second is well
what if we could bring research in
education more into lineman and thereby
leverage of leverage clusters that were
purchased under research grants for
educational purposes okay so that's an
interesting idea that I played with and
the third is the idea of utility
computing so why not use ec2 or are some
utility computing service to for
teaching purposes okay so what I basic
you have been doing in the last couple
years is exploring these various
approaches so they were operationalized
and two separate cloud computing courses
basically a Hadoop courses I taught the
first one in spring of 2008 and the
second one in fall of 2008 essentially
using and trying out these solutions now
now here it's of course I have to
recognize that of course not the first
person to be doing this and others have
been doing this as well so recognize
excellent work that's been ongoing at
the University of Washington at Berkeley
and a few other places yeah rat lab okay
well i have i've haven't heard that
feedback before okay all right okay so
let me give you an overview of these to
cloud computing courses so and spring
2008 Iran this is the first time I ran
it and I had the explicit goal of well
what am I integrate research in
education so the basic idea was to put
and create together team small teams of
graduate students peach PhD students
leading undergraduate students and the
ideas to tackle open research problems
and I wanted them to generate
publishable results and they did and
I'll be presenting a couple case studies
from from these courses and so for the
most part I encourage PhD students to
choose a problem that they were working
on as part of their dissertation and to
bring it into my cloud computing course
and that actually worked out quite
successfully so the actual setup is for
a 15-week standard semester I spent
three weeks on a Hadoop boot camp this
is like drinking farm of fire hose you
just dumped in the in the deep end and
you just show them everything you need
to do know about Hadoop in a couple of
weeks and then the rest of the time was
spent on actually working on the project
okay and so the resources we use for
this iteration of the course were was
was a cluster from the academic cloud
computing initiative and and
and for the second cluster we actually
got a loaner cluster from a research
group on campus from the center for
bioinformatics and computational biology
so what happened was one of the students
in that group wanted to take my course
and had access to clusters as part of
the research so I said well here this is
an alignment of research in education
and since you're going to devote the
cluster for the semester to Hadoop
anyways do you mind if some of the other
students use it for their for the course
projects and they were they willingly
obliged and and so but so i think this
is a very nice case when once you bring
research and education into alignment
you can get better utilization of
resources okay so there were 16 in the
spring 2008 iteration these are the
projects they worked on I was a
relatively small scale project 13
students seven PhDs three undergrads
three masters and they came from all
many departments on campus so from the
information school my home department
computer science linguistics and even
geography we had a student that showed
up from Geography Department wanting to
participate and involved many campus
labs of course the labs on campus
working on natural language processing
information retrieval human-computer
interaction computational biology etc
alright so the second iteration which
happened in fall 2008 had a more
traditional setup so whereas before i
had a three-week bootcamp and spent the
rest of the semester on the project had
a more traditional setup with lecturers
labs problem sets etc so i essentially
flip the flip the order around so there
was a three-week final project and the
rest of the time was spent on these
lectures and labs on problem sets uh the
the final project in this case could be
research focused I did encourage them to
take that route but you know they were
they were okay I was okay with it if
they wanted to play with other things
and the resource I used this this was
amazon web services so through through a
grant from AWS they provided each one of
the students with some free ec2 credits
and they used it for their courses and
that actually worked out beautifully
and and so for for the fall 2008
iteration I had 21 students total I
designed it as a intro intro grad
student sort of a master's level for CR
PhD level andorra Vance undergrad so I
had a I had students from from from he
masters in undergrad also okay so what
am I going to do now is spend the next
few minutes presenting some case studies
of some research that actually came out
of for the most part since the first
iteration in spring 2008 was more
research focused these are these are the
projects that came out of it so the
first one I want to talk about is the
problem of pairwise similary comparison
the student is Tamra alcian and he's
actually he's actually uh finishing up
his dissertation in a in a month so this
is I think a success story for the
academic cloud computing initiative he's
a PhD student in computer science okay
so the problem is this a pairwise
document similarity problem so you have
a bunch of documents you want to
represent them as feature vectors bag of
will convert them into bag of words
convert them then into a weighted
feature vector and you wanna and then
you want to compute a pairwise document
similarity all right so you want to
compare every document with every other
document okay so why would you want to
do that couple reasons so you might
wanted to do it for clustering as an
initial step for clustering you might
want to do it for cross document
coreference resolution which is actually
the application that the student was
examining so you want to see if this
mention of Clinton in this document is
the same as this mention of Clinton in
another document because the reference
might be ambiguous well what you do
there is you can convert the compare the
context in which those those references
appear in and so that's a parallel
similar problem when you're doing more
like that queries you can treat this as
a pairwise similary comparison priests
or the results and stirred serve them up
online okay so we looked at similar
functions of this form so in our
products between inner products between
way to feature vectors and so in
actuality you can replace it with a
second form here so the the difference
being
a feature will only contribute to a
final similarity comparison if has no
nonzero values in both feature vectors
right so you can place it within
intersection okay so we came up with a
with what we think is a cute two step
solution in MapReduce one is building
the inverted index first and second is
directly computing the pairwise
similarity from the from the postings
okay so building the virtus index is
very standard so for this toy example we
have three documents a triangle document
the circle document in the square
document you you MIT your postings very
standard doing sorting and shuffling and
you gather up the postings list to to to
create the inverted index okay so then
you do then you then you take the
inverted index and you map over it and
what you do here is you essentially
cross each postings with itself you
generate the cross product between all
all all postings in that postings list
and what this does is it generates all
the pair's in which that feature will
contribute okay so you're you're
computing the the partial future
combinations there's the grouping and
shuffling process and then once you
group the features together you can sum
and sum them and you get the and you get
the final similary comparison scores so
this breaks down nicely into a map phase
on one end and a reduced face on the
other end so and so in action so what
you're doing here is essentially
generating all the feature value pairs
in this in this when you're crossing the
postings and what MapReduce is doing for
you is essentially bringing bring
together as the summation bring together
all the partial partial feature
combinations that you need and in the
reduce phase you just some across all of
them all right and so the idea here is
that you map over the postings just once
and so and let MapReduce keep track of
where all these partial feature weights
need to go so we also implemented with a
infective pneus efficiency trade-off by
what we call DF limiting so there's this
if ian distribution of terms what
happens is a long time a lot of times
you're comparing the in one document
with the in the other document right and
the feature weights going to are going
to be very small
on that case so what you can do is you
can essentially throw it away so you can
establish a document frequency limit and
say okay if it fer occurs too frequently
we'll just throw it away and this
actually has a large effect because
these term distributions follow how
follows if ian curves and so you can
save a lot of effort that way so the
student wrote a paper that was published
as a short paper in a CL 2008 last year
and I did some follow-up work that's
going to be presented at sega are this
year okay so here's the here's the
second case study is quite interesting i
had a student from the computational
biology lab on campus coming here and
wanting to explore applications of
sequence alignment in the clouds and
actually there were two of them one
participated in the spring 2008
iteration and one participated in fall
2008 iteration so Mike Schatz and bing
been dling ameed um okay so i think it's
easiest to UM to explain this
application by analogy and i do this
completely tongue-in-cheek as you see in
a bit the analogy breaks down very very
quickly okay so let's say we have
Shakespeare and here we have the soul of
famous Eloqua from from Hamlin again
let's say for whatever reason instead of
composing the play on normal paper
Shakespeare wrote it on a single very
long strand of tape right so it's just
the entire the entire play is just one
line okay and that's say you know he was
worried about backup and so he created
multiple copies you know nevermind I
photocopiers didn't exist sort of bricks
this story but okay so so chic spear was
carrying around all these all these
different copies of the manuscript and
and uh he tripped and there was happened
be a shredder next the next door nearby
and all the manuscript fell into the
shredder okay oops okay so now he has to
put everything back together again okay
well how would you do that well you sort
of like putting together a jigsaw puzzle
right you know you you take this part
and then okay this goes here okay well
this must go here because they line up
that way and okay
this part must then follow here and so
on so forth and oh that's that's a
duplicate so you know they must be
coming from multiple copies and and so
and so forth ok so you get the idea
right this is essentially the problem of
a DNA sequence assembly today except
instead of doing it with words we're
doing it with atc and GS the basic
alphabet of DNA ok so what happens today
is you start off with a subject genome
so this comes from a cheek swab drop of
blood whatever this is what you're
trying to sequence this is what you're
trying to find out so what you do is you
you prep the sample and you run it
through the sequence of machines so here
just examples from three different three
different manufacturers of them and out
on the end due to limitations of present
day technology you get back a whole
bunch of very very very short sequences
okay which then you have to put together
again these short sequences are on the
order of tens of tens of base pairs okay
so you have this problem now you had it
and these are called reads so you have
to take the reeds and put them back
together again sorry on the order of
billions of sequences right so you prep
the sample run it through a sequencing
machine you get back several gigabytes
worth of / / experiment and you know
these experiments are happening 24-7 on
dozens of machines and large genome
sequencing centers okay so you have to
put you have to put a you have to put
Humpty Dumpty back together again okay
so how do you do this well um as it
turns out it's already been done once
it's been done once as part of the human
genome project and the results of wood
or published in 2003 so your tax dollars
at work this is on with President up
with a previous generation technology it
took 11 years and cost three billion
dollars okay but now we essentially have
a reference sequence this serves as a
scaffolding for us to put the rest of
these reads together right so we have
the reeds and we basically want to put
it together and then we can do this
alignment process and then we can read
out the target genome but of course it's
not going to match exactly it's not
going to match exactly because if it did
there would be no point to doing
sequencing and but the bigger point is
that all of our DNA sequences are
different right i mean we're we have we
share mostly the same DNA but there are
some differences so there are
differences in terms of insertions their
differences in terms of deletions your
differences in terms of mutations and
from a biological point of view these
may be significant right so there are
genetic diseases that can be traced to
insertions deletions mutations etc okay
okay so how how do you do this how will
we solve this a problem in MapReduce as
it turns out one of the students that
participated in the course Michael shots
came put together an algorithm he called
it clovers okay so the idea is
relatively simple so you start off with
the reference genome and you start off
with all the reads in enough map phase
what you can essentially do is do a
sliding window over it and you meant
what he calls k-mers but in the natural
language processing world these would be
essentially n-grams okay so you you
essentially slide over it and amid all
lots of n grams as as key value pairs in
the sort of sorting and shuffling stage
this is essentially a poor man's version
of a large-scale distributed hash map
right and in the shuffling stage you
have the sub sequences from the
reference genome in the query sequences
being brought together and basically in
the reduced stage you can from these
alignments extend them computing
mismatches into account for the
insertions deletions a setter that I
talked about in the previous slide right
okay and then then you can and once you
have the alignments you just sort of
read them off and by n and end-to-end
and get out the get out the alignment
okay so this is a rough sketch of the
algorithm there are some general details
of you know keeping track of where
everything came from okay but i think
this conveys the idea fairly wet okay so
here's some here's some performance
evaluation on the top you have a running
time for human chromosome 1 and on the
bottom human chromosome 22 so this was
relatively small cluster 24 core
in-house cluster the different lines
represent how many mismatches you allow
okay so the more mismatches you allow
the loser your your matching scheme and
therefore the longer takes to compute
the possible alignment so the nice thing
that that we see here is that we get
linear speed-up as their number of our
reads goes out which is nice this is
exactly the result that we're expecting
and that's nicely it's a nice result all
right and so this was just recently
published it's coming out to press in
and bioinformatics a journal all right
there's also a third case study that I
won't go into but it's a it's a it's
about statistical machine translation
and chris dyer was a PhD student
linguistics that that participated in
this and he actually spent the summer as
an intern in google research last summer
so so some of you might already be
familiar with this work if you're
interested I'll be happy to discuss this
offline in more detail okay okay so I
want to spend the sort of the final talk
about the final part of my talk sort of
reflecting about you know what what does
this all mean so I think the importance
of this harkens back to basically what
we learn in computer science when it won
okay so you know cloud computing is full
of it's a buzzword right so so what what
is it when you get down to the core so
here's what I did um here are notes from
the first computer science course I've
ever taken so you can even see the date
up in the upper right hand corner six
double one at MIT and so this is imagine
me as a you know young freshman you know
going up in this is so uh and this is
basically what I learned in the first 15
minutes of class and you'll see down
here um this is Gerald Sussman a Gerry
Sussman lecturing he's telling us you
know what is what is computation about
what's complexity control while
computations about means of combination
a means of abstraction and there that's
it I think I I think this is really what
it's all about it's all
appropriate levels of abstraction for
this for this changing world of
computing so here's a problem I think
this is a problem the von Neumann
machine right so this is a standard
architecture I'm sure we've encountered
in our own in our own undergraduate days
right this is the model that's been the
standard model for 40 50 years this is
the model that's been ingrained into our
heads and when we are developing systems
this is the underlying abstraction that
we have in mind all right so here so so
the problem becomes okay now we have
live in a data which world that we need
to think about not only individual
machines but entire clusters okay so
what have we done we essentially taken
each individual abstraction and and
cloned it multiple times okay so so
we're still thinking at at the cluster
level but we're still think still
thinking at the individual detailed
abstraction level okay and I'm arguing
that this is this is a wrong this is a
wrong level all right so this is what
gets us into issues like this it's a
real live example of deadlock is this
trying to think at a cluster level and
trying to worry about the individual
machine level at the same time right so
what we need is and what really cloud
computing to me is all about is about
the appropriate level of distraction so
give you some examples what I think are
really nice levels of abstraction right
we need abstractions at the level of
data centers right this is a blueprint
from from yer ah from your Oregon data
center we need so that that's the right
level abstraction entire data centers so
here's some nice level abstraction that
I've that are that are worthwhile so
moving from our traditional stack to
your way to a virtualized stack right
other examples like Amazon Web Services
right so abstract away the operation and
maintenance of of machines and basically
being able to say okay give me give me
10 linux boxes give me a thousand linux
boxes ok Google App Engine my scalable
API in which people can then develop on
top of and at the highest level the the
idea behind Salesforce right apps on
demand
in the clouds my application of clouds
okay uh and so so here's here's I think
we're we're moving to is in future I
think we're sort of moving into the
world where we're going to have
applications driving from the top and
and continued advances in both
information retrieval natural language
processing machine learning etc and
we're going to have continued advances
in in on the autumn bottom layer on the
system player in terms of architecture
networks etc but what's going to be
important is the mediating level of a
program and meeting lee mediating layer
programming models right MapReduce is a
start but know what's next they're
surely are going to be new appropriate
models that followed MapReduce and so
there's going to be both downward force
right so new applications create new
demands for programming models that
drive demands on hardware and there's
also going to be upward pressure right
so advances in in hardware drive new
programming models that drawdown enable
new applications okay so uh so yeah so
so I want to I want to conclude with
this final thought write the theme of
this this talk has been you know how
does the rest of the world get there
right and so you might be wondering well
you know i'm at Google I have all the
access to all the computing resources I
need what do I care about the rest of
the world right and I think I think this
is this is the important reason this is
an import response it's sort of the
principle of you know the rising tide
lifts all boats right so what's good for
the community in general molon again be
good for google right i like to thank
you for your time and i'll be happy to
hate comments and of course i have to
acknowledge my sponsors for this
research okay I I believe you want to
ask a question I think you're supposed
to approach the microphone so Jimmy what
what other sort of that you give the
good side of the story others or things
frustrating like what what are the
things too
he's like using like mapreduce in
academic environment um so I don't know
if the microphone picked picked that up
but I'll just repeat the question so
their kongs question was what about what
are the frustrations I think the primary
frustration is just immaturity right the
immaturity of the the open source
equivalents of all of these technologies
we're talking about right so Hadoop
still has a ways to go but I think we'll
eventually get there so all right um so
the question I have is that you know
when i'm teaching particular
undergraduates i mean even just getting
them to understand how to use a debugger
and understand relatively simple
concurrency is a challenge and i just
wondered that you know with something
like hadoop that you know what's your
experience with how hard it is for
students to actually understand it and
particularly when things don't go right
and there's either a correctness error
or performance bug in Hadoop code how
well the students do in terms of being
able to diagnose and understand those
problems okay um with respect to the the
the particular population you talking
about intro undergrads I actually don't
have any experience the students that
took my course were all upper level
computer science undergrads and to some
extent they were self selecting so
they're self selecting because they are
they're interested in and they want to
know about it so there I I think they're
much more highly motivated so I don't
have a good sense of for that well how
about for those students what was their
experience with um I think it was a it
was a lot of frustration at least in the
in the beginning and the frustration
both stems from the the immaturity of
the platform there you know certain
things like you spend a lot of time
doing more configuration than they
should systems level configuration you
know for example in the ec2 in the in
the in the the course that was taught
using ec2 I think they spent a lot of
time mucking with config scripts booting
up ec2 instances then then they should
have so that's all folded into
the actual learning about concurrency so
it's a lot of heart a lot a lot more
difficult to tangle than one might
expect but um yeah I think that's an
issue we need to think about in a more
detail sorry I can't give you a more
detailed answer than that but yes it's
an important problem could you comment
on the difference between CPU and I
utilization because oftentimes I found
in academic environments disk i/o and
network i/o were really not sufficient
and tended to keep you from being able
to use the CPUs that their full
potential um so we actually don't have a
lot of experience with that and I that
actually might be a good thing so when
you're working when we're working with
ec2 we basically have no idea what's
going on behind the scenes and to some
extent that was experience with with
using the ACC I cluster because it was
managed on on our behalf so we don't
actually have we don't actually have
enough information to be able to
understand the trade-offs between I oh
and and and bandwidth and and CPU cycles
but uh in general yeah and in general I
think I think everything went quite well
and resources were more than adequate
for for what we were doing ok
congratulations on your success on that
sounds like you you really did well let
me ask you a question like five years
from now or 10 years from now what model
do you think best fits sort of the
academic environment for this kind of
usage would it be is is the Amazon model
the right one the App Engine or should
this be part of some kind of Salesforce
thing where you just load it into a big
spread you know spreadsheet and just
work on and what would you what would
you want to have happen or what is your
prediction of where it should go um I
don't know I think this is part of my
part of the reason for my visit here is
too
is to tell you guys on about these
things that have been up to and sort of
hoping to engage in a dialogue you know
plotting plotting and planning for the
future but I think they both have their
they both have their advantages and
disadvantages so I might actually answer
your question by talking a little bit
about both of them so with the cluster
model where we're using the ACC I
cluster I mean other other other people
have taught the courses I've have
observed the same thing is that cluster
usage is not even so for the most time
for the most part there is no usage
until the night before the problem set
is due right and then then everybody
clobbers each other right ok so and that
that's exactly what the ec2 model sols
because everybody gets to play in their
own virtual cluster ok so so that's that
would be for the that would be a win-win
for the ec2 model however there's always
something to be said of having a cluster
that's up all the time that yeah if you
have access to so in terms of
convenience having the aacc I clusters
much more convenient because with Amazon
you have to do a lot of data copying
back and forth from from from s3 on to
HDFS there is a cost associated with
booting up the instances shutting them
down messing with config scripts if
things don't work out Oh correctly and a
lot of these issues will work themselves
out in time you know so amazon has new
services that allow you to run MapReduce
directly so so I don't know so both I
can see merits in both models
so there's been some migration of things
that we would consider traditional HPC
high performance computing tasks on to
the hadoop world I understand there's a
their components of physics and
astronomy experiments that have moved
into this whole Hadoop model of
computing and I think it's worked out
quite well I think another issue is the
traditional HPC world already has a hall
already has a way of doing things right
i mean so there's always going to be i
mean and it works I mean and so there's
always going to be some inertia in
dealing with those issues right and
there's always you know some people find
this debatable you know what it is it
really data-intensive I mean we want to
say that Hadoop is good for
data-intensive processing but in a lot
of traditional HPC tasks are data
intensive also right so it's unclear at
this point i would say that's exactly
the point I wanted to address which is
you know we've got MapReduce and now it
seems like it's a hammer and every nail
starts looking like a MapReduce problem
but that often creates very convoluted
codes that would be much more
straightforward what alternative worker
models are applicable to the sort of hpc
classic HPC style calculations that
normally are being run on these big MPI
supercomputers with low latency you know
where I've found that often it's easier
to just have no communication between
the nodes and then run it an
embarrassingly parallel but not in
MapReduce I is there something better
than MapReduce we should be thinking
about uh yes I and I think this is where
I was trying to get to with a slide is
that um you know MapReduce is the first
of these abstractions that allow it's a
very convenient one it's basically the
only one that we have that works at the
scale right now but sandwich between
system level developments and drivers
from
applications there will be there there
will be there will be new programming
models that emerged I'll give you some
examples so i think at sigmod last year
there was a proposal to modify MapReduce
into something called MapReduce merge
where they added a third phase in the
end and that gave you some additional
power there is a and I can name a couple
more none sort of escapes me at this
point of other people that have been
playing with similar abstractions at
that scale and I and I think this is
this is a this is where the innovation
is going to come from in this is where
we're with the academic academic cloud
computing initiative and NSF NSF's
efforts as part of the Klu Klu
exploratory cluster exploratory program
is trying to push the field and you know
i guess a natural question is you know
where where would Google be in the
picture right so that's a question to
you guys where where do you where do you
see future collaborations along these
lines between academia and industry
um so I think this is something that NSF
is trying to work out right now so that
the previous model is that as part of
the grant to NSF you would request
resources through IBM and google as part
of as part of the as part of the program
and I understand amazon is thinking of
doing something similar so they just
recently announced a whole educational
thrust along these lines also where
they're soliciting grants to 24 ec2
credits so those are those are just two
currently existing models but I'm sure
they'll will come up with something or
NSF will come up with something in the
future that that lowers the bar even
more right okay all right thank you very
much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>