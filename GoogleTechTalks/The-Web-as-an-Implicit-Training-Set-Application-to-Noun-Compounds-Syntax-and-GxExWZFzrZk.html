<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Web as an Implicit Training Set: Application to Noun Compounds Syntax and... | Coder Coacher - Coaching Coders</title><meta content="The Web as an Implicit Training Set: Application to Noun Compounds Syntax and... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Web as an Implicit Training Set: Application to Noun Compounds Syntax and...</b></h2><h5 class="post__date">2007-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GxExWZFzrZk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm going to talk about using the web as
a source as an implicit training set
which is using it not just as a source
of Ingram frequencies but also for I
mean really as a corpus and you see what
what I mean by that during the talk
essentially so what were they going to
do is first I'll give like a brief
introduction and then I'll walk you
through three problems and I'll spend
most of my time on the very first one
because you are looking for details but
okay if I go out of time for the other
two problems too so first is now compile
marketing which is a syntactic problem
paraphrasing now compounds which is
essentially a semantic problem and then
machine translation which is an
application I mean this is how you how
those two things can be applied to real
task so what we do in natural language
processing dota mate go is to have
something programs the right programs
that can understand language of course
this is hard so what we do is that this
week focus on particular sub problems
and the dominant approach recently has
been statistical corpus-based one where
you get a very much collection and then
you compute some statistics operate
there has been this informational paper
cup of years ago 2001 by Blink oh and
banco in braille scaling to very very
watch corpora where they have this very
simple problem spelling correction in
particular context so if you have I'm in
the third year as a principle of animals
in high school should your spirit that
way or that way principal okay and
essentially if you used to have a
training Tatum then you can probably
work have a classifier that works in the
surrounding context and you know tells
you whether which which of the two
options is more likely the problem is
that mean where does this train Gator
come from and the they were working that
I mean for this particular problem you
can get as much strength as you want by
just assuming that in well edited text
the correct word will be used so what
they have found is that the problem know
that probably should not spend that much
time
okay I have it so should i skip all of
this or like continue okay so I mean the
main idea is that it's better it's
better to get more training data as
opposed to find you in Calgary written
and essentially okay essentially what
they found is that you know the the word
toga rhythm here is kind of the best
over here and the best becomes worse and
actually he is not much difference
between the different algorithm so
essentially the more the more the
manatee is the more data we get the
better your algorithm works for this
particular task maybe not for tasks
which is great idea how we can extend it
to other tasks so there is another in
you know famous paper using the episode
based one by a patent hour and they have
these eight particular different tasks
and for some of them I mean using very
simple interrupt frequencies they find
that I mean using the web just because
of the scale you can get resource that
are much better than the best supervised
algorithm published so far for others
you did don't find like significant
differences and fedoras they could not
improve so their conclusion is that the
web and ground should be used as a
baseline so it is the very first thing
you should try essentially I have built
on this work now compound bracketing I
have multi from from this group into
this group and prepositional phrase
detachment from this group introduced
group using techniques that I'll talk
about in a little while so I mean the
mint is that probably the potential dcd
is not fully realized there are more
things on the web you know in addition
to the Ingraham so there are other sorts
of information and what I'm advocating
is really using the web as a corpse I
suppose just as a source of Ingram
frequencies so the very first task is a
noun compound bracketing and he is a
particular example so you have like this
configuration of to work down compounds
and you have liver cell wine and liver
cell antibody and from a syntactic from
you know part of speech point of view
they're all sequences of three nouns and
even like the first true two words are
the same but the syntactic structure is
different here you have like an antibody
that targets deliver cell which means
that liver cell is grouped together in
Daniel
antibody and here we have sell wine you
know this is derived from the river so
you have like you know you have like
essentially a right bracketed versus
Allen bracketing structure and if you
want to find out the semantics of the
Nile compound you probably need to solve
this problem first okay so you need to
make a bucketing decision between left
and right structure so and previously it
has been addressed using probabilities
you look for the weather the association
between the first two words is stronger
or the CH the association between the
second word is stronger or you can think
of in terms of dependencies and then
like you look for an association between
the first words as a possible
association between the first and the
third one and then Noah patent ke RS
already said they use the web instead of
a fixed corpus and then their days also
some supervised to work that makes you
know unrealistic assumptions of asking
for the particular coordinate senses
what I going to talk about is how you
can use in addition to web grams also
paraphrases and surface features that
can help you solve this particular task
so this is the old staff your staff is
ok so you have these three words and the
adjacency model task ok is the
association between these two words
stronger than the association between
those two and the dependency model task
ok is the association between those two
words stronger than the association
between the first and the third and I
mean the idea behind the dependency
model is that I mean watches the head
and what is the modifier as opposed to a
quiz adjacent to whom and you can do
this using like anger by graham
frequencies or probabilities and this is
all old stuff and it essentially like
for this conditional probability how you
can estimate it it's very easy you just
can just go and queried you know the web
for the frequency of the diagram and the
frequency and then unigram and you can
take the ratio of course you can also
like do inflection variabilities awake
for porn singular and so on ok but there
is more on the web you know that can be
used to to help you solve this
particular
ask and for example they are like these
surface markers that alters in some
particular context can use to tell you
what it should go left or right because
I mean where they make it consciously or
not I mean they leave some particular
surface markers and you can also use
different kinds of paraphrases and the
good thing is that the normal size of
the web makes this service which is
frequent enough to be useful and here is
a particular example so the very first
feature that I have if you have cell
cycle analysis and if you find it
somewhere else with a little touch
between the first two words it probably
tells you that you know its cell cycle
you know it's a left bracketing you know
there's some stronger session between
those two words it if you find you know
between the second two one donor t-cell
it's probably right bracketing odo the
right the right dash is kind of
ambiguous because it can eat scope cam
the whole thing fiber optic system it's
really targeting the whole so they are
not as reliable on the right but on the
left they are and you can have like
dashes between everything in some
particular context which are unusable
another thing is possessive marker if
you have brain stem cell brain stem cell
is really ambiguous between because this
a part of the brain called the brain
stem and is like also the stem cell so
it's not clear whether it is like a left
or right bracketing but if you find it
elsewhere brain stem cell it probably
tells you that stem cell go together and
if you find it brain stems sale it
probably tells you that the left you
know the left words are together of
course I mean this is not deterministic
I mean you can just use this in a
particular probabilistic model but this
is some evidence that can tell you and
then these features can can occur
together say right brain stem cells as
well you have bottle dash and possessive
marker another thing is capitalization
if you have postmodern film reviews
malaria I mean the fact that malaria is
uppercase and vivax is lower case and
you don't care about the first one it
probably tells you it's a left it's a
left bracket because you know these two
again I mean so
and trying to find elsewhere the now the
noun compound that is spelled in in this
different way I mean I don't want the
original one to be spelt in this way it
also like for the other one there's a
prob sorry new companies and products
that have random capitalization in the
middle of the words yes you can work
here you have like different kinds of
organisms so in biology I mean not all
models will be used I mean I mean you
not have this particular features these
particular patterns you'll not happen
for every single noun compound but for
some of the material and for some of
them some will be better some other will
be better for something else and you
have some prompts vitamin D deficiency
geez geez really convention I mean you
cannot really use it I mean the there
are some problems with this one to
Andromeda jiu-jitsu also came to a
problem here's another one say see we
have the non compound look immediately
reform yourself ok if you find it
somewhere looking and slashing for my
cell this means it's an alternative
detail so right bracketing you know for
the for the original one because you
cannot it cannot be the case that
leukemia modifies lymphoma if they are
alternatives I don't think it's just
like if you get growth factor bet I mean
like different kinds of brackets can
tell you you know that it goes like left
to right and essentially even even like
even things like one cancer colon
patients i mean i have found that even
such punctuation I mean that goes inside
diagram are you know good week predictor
stuff which way it should go another
thing is you can have like you can look
at the surrounding context say like
mouse brain stem cell this probably
means that those two go together and if
those two go together maybe those two I
mean should go together too so you have
y carrete bracketing the problem is that
the search engines ignore poor creation
so I mean for different reasons so you
cannot really have a query that you have
brain stem cell you cannot buy query for
for this little dash here so what I do
because I mean I just have access to the
search engine as a regular user I
usually say like four brain stem cell
and then I I take the top 1000 documents
and I try to find it I mean the
population signor but I I want to see in
the little snippets whether the
punctuation that I'm interested in will
happen in tiffs oh you know i extracted
sorry why don't you use the google
groups i think that the goo a house
doesn't get his penetration foundation
thought it does does enjoy if you know
but i was always in the canary
impression that he does okay but anyway
this is something that they did back in
2005 so it was not available anyway so
okay another another thing that but for
some of the features you can we can
create a directly situation so he is
another very very highly highly accurate
feature say white tumor necrosis factor
if you have tumor necrosis TN is
abbreviation happening here okay this
means that these two go together okay
because this abbreviation happens in
this context in this predicts a left
bracketing if you have like tumor
necrosis factor and then in parentheses
NF right it tells you a ride bracketing
because it only abbreviates the second
tool and you can what you can do you can
go in google and you can say tumor
necrosis quien factor or tumor necrosis
factor NF I mean okay you don't have the
upper case you don't have the
parentheses but it's still kind of does
the work of course we have problems with
state names i mean if if those two
what's happened to be a state name or
like short water or you know Roman
digits but most of the time you know it
works so another another very important
in a highly accurate feature is
congratulation so if you have health
care reform okay health care it's very
likely to be spelled together I mean
just you know for just because it's it's
almost lexicalized I mean in English ok
it's spelled but wait and carry form not
that much hell 3 4 noded much and you
can like test the adjacency model okay
do I cg stores to concatenate are
supposed to do ecgs towards order depend
on similar
I see those two words concatenated as
opposed to do I see those two or you can
write have it in the context like health
care reform so you want to concatenate
the first we in the context of this of
the third one or you concatenate the
second 2 in the context of the first one
okay so another thing is ok you can just
like see you use the Google star
operator and see ok if does it happen
that you know they're they're kind of to
get close to one another but like those
two are together in the other one is
separate and I mean if the first two are
together this is like left bracketing if
if the second two are together this is
right bracketing another thing that you
can do you can just like try to move say
like the first to hear you have a care
reform health health care reform you
just like move those two in front or wyk
reform health care and this will
probably happen more often than this one
so it will be an evidence for a live
bracketing another thing is internal
infection viability so if you have bone
mineral density and if you see
inflections here bones mineral density
this means that probably those two are
going together so you have like a right
bracketing structure and if you he most
of the time inflection of our ability
over here you know it tells you that you
have a left bracketing in each of these
models what I'm trying to do I mean I'm
looking at these models in isolation so
for example if I if I want to see the
internal infection variability feature I
see how many times I see this pattern
okay say like I have it five times how
many times I see this pattern say like
10 times then this model wins then you
know the whole model volts all right
left in this particular case another
thing is sometimes when you have an ID
bracketing Fink say like adult male rat
sometimes you can like switch the
modifiers you can also see male adult
rat this tells you are attractive
because you cannot do in case of left
bracketing
you said it votes that means it's
considered a binary intro yes each of
these creatures this pod or not
depending on the actual relative
frequency of okay is there any reason
why he turning um okay so this is the
reason and the reason is i mean i'll
show a little bit later but I mean the
reason is that I wanted to be
unsupervised of course I mean it's not
entirely supervised because I give the
polarity of the feature I say okay this
particular feature votes left or votes
right this is some sort of supervision
right i mean and in addition to that I
give the particular features i mean so
they stung some sort of supervision but
it's it's I mean it's not super vision
in the sense that ok here are hundred
examples in this particular example we
have a left bracket link in this
particular example have a ride
bracketing the amount of supervision
that they have it's like very white and
kids just like the party of the
particular feature and I didn't want to
use training data I wanted to be
unsupervised so that's why I have this
using the way passing increasing
training set and also I had only two
hundred forty four examples and with
that many features I mean I need a lot
more in order to have wait for the
particular methods but I mean if you ask
me how I can do it I mean to not really
have like a classifier I mean a
supervised model I can I can talk about
that you know again to the doc or if you
want now but it will interfere what I
have planned for look at fun sorry font
information so like both the decision
bold and italic or or a hypertext anchor
make sure it okay so I haven't but I
mean the thing is when part of the
reason is that when you have like brain
stem cell-like in quotes it will make it
bolt in the snippet so I mean what was
it before I mean it's kind of lost I
mean that's part of the reason you have
to go to the original yes but but this
is expensive and I didn't want to do
that so ok so another another important
kind of sorts of
mission sputter faces okay so if you ask
a human what's the brain stem cell they
can say well those are stem cells that
are in the brain or data from the brain
so this is a way to paraphrase it how
you paraphrase it you paraphrase a noun
compound using verbs or using
prepositions okay most of the time there
are other kinds of purpose of paraphrase
or you can have like a cop or
paraphrases Sehwag skyscrapper office
building his office building that is a
skyscraper okay so what I'm doing is I
am say suppose I want to bracket brain
stem cell I would generate output for
example stem cell together our generate
different kinds of prepositions and
maybe here determiners maybe I'll do
some inflections singular puro here and
here and now see how often you know
maybe this happens two times when we
describe him as hundred times so I have
some some frequencies for that
particular bracketing and then i have
also some that particular I'd bracketing
of sorry this is like this is right so
he have cells from the brain stem in
here because brain brain in stem are
together for brain stem cell this is
like a long bracketing okay and what i
am doing i just sum up the left
predicting butter faces versus the right
reading paraphrases and I see which
number is bigger and this is the
pervasive model yes I was wondering how
much the next to just see whether it
will gel together this is this is this
feature right now regular but it is a
really different yes it is because
because this one just this one doesn't
work very well I mean then we will see
waking in the results in the other one
works much better so okay so in
prepositions they have been proposed to
you know us source to get the semantics
of the noun compound in terms of the
positions before but and not I don't
want to predict the semantics i am
trying to predict the syntax and all I'm
doing as I already said I just count so
cells in the bone marrow disprove this
you know predicts left I have that much
this predicts left I have that much and
I have some evidence for right
bracketing I just sum up separating the
left-hander I did I compare the two okay
so about the evaluation what I have is
this a standard data set of 244 examples
put together by our they extracted from
the gorillaz encyclopedia and in these
particular experiments I limited the i
used google exact phrase queries and I
limited the language to English so he is
how the lower dataset looks like so he
have like three pots of words sorry
couldn't be in certain context it would
be a three it will definitely make sense
to make it in a particular context but
the data set that is considered the
standard one and that you know people
compete against doesn't have context
it's very similar to the way that the
data set for prepositional phrase
attachment it sits in the same format
you just give the for the four words
outside of any context and if you want
to be comparable to somebody's our
previous work I mean you have to use one
of those datasets I mean it's an
artificial task I agree because you
don't have the context and when you have
the context it probably changes a lot of
things but yeah this is how how it has
been handled before so you just have the
three words in a left or right decision
okay
so and he is how the standard models the
Ingraham models work this is like
adjacency the adjacency model I mean
remember the the model chicks when at
the association between the first two
words is stronger than the association
between the second to sew frequencies
probabilities point-wise mutual
information chi-squared so you make
prediction for a hundred percent or
almost and you have precision 75-73
something like that and with the
dependency model which checks the first
tool as opposed to the first in the
third you have something 79 up to eighty
percent and i have here the confidence
intervals sorted and then using these
stars they don't work I mean okay the
precision is better for the Jason see
they depend for the independence us and
then you was a lot on coverage okay so
here the interesting ones the
concatenations the concatenation triples
remember why concatenating the first to
work in the context of the second one
they can be extremely extremely precise
96 point twenty percent it's it's really
good but it all only covers thirty-two
percent of the examples because you
cannot expect to see this all the time
and but other kinds of concatenation say
by the dependency it's it's again eighty
percent here's what happens with the
paraphrases it's 82 and it is eighty-six
percent coverage the surface features so
you I mean the surface features remember
the ones with the little dash and like
little commas in the bracket inks you
have eighty five percent position
eighty-seven percent coverage so the
abbreviations and the possessive markers
which for which you can you can have a
quick query to Google because although
the possessive marker is removed you
still have the S say like bonds you
don't have the positive attitude you
still have s so you can still use it you
still have pretty high precision
so what I'm doing I have a majority vote
here and again because I don't I want to
trade it to be unsupervised in this
particular okay so I have a majority
vote of the ones that are in both mean
those who these ones and you know the
chi-square so what you get is ninety
percent precision and ninety-five
percent coverage and then if you default
everybody else to left which is most of
the time we have left this is your base
one it's left bracketing you get
eighty-nine percent so how this relates
to other work the baseline sixty-six
percent lower who use the fixed
collection of the grounders encyclopedia
he had sixty-eight percent position with
the Jason's model 77 with the dependency
my Christ we are using the web is 79
almost 80 then you are hesitant model
which is eight point seven and this is
mine in this difference you know this is
statistically significantly better than
this one and it's all statistically
significantly better than these models
this one is supervised introduces also
additional training data which is mixed
with this one so it's not directly
comparable this one is partly directly
comparable because they used half of the
training data for tuning Toronto pata
and test only on half of that okay so
about non compound pocketing so what i
have done is i have introduced the
surface features and these paraphrases
as an additional source of information
and i have obtained state-of-the-art
results on this particular task so now
i'm going to move and there I mean this
particular task although it's kind of
artificial it has applications to auto I
mean it's a lot of this sort of these
features that I have developed I mean
they have applications for other tasks
but even this particular task I mean
people have been he fused this there was
an easier p paper this year about adding
dependency structure to the flat noun
phrases in
working in the pantry back in the penn
treebank you know the non fairies are
what it's not clear what word depends on
which one so they used a lot of these
features that they have developed to add
a dependency structure and then this
another thing that has something to do
with what a search engine probably has
to solve is this career segmentation
problem in days like a direct relation
to now compound bracketing so okay of
course it's a little bit different
because in a secure segmentation for
used car part which is like three words
you have four different structures and
because you also have the option of
keeping everything together or of
separating everything and in bracketing
you only have two options and there was
an e/m NOP paper and if you want i mean
i can i can go into more detail about
that i mean how the particular you know
the kinds of features that i have
developed apply to this and this
supervised model and they have also
interesting findings but essentially
have like a binary decision between
every single pair of words okay should i
split here should we split here okay so
the next task the Tau I'm going to talk
about is semantic one part facing down
compounds so consider around compound
like malaria mosquito and they have been
like different kinds of semantic
relations proposed for this particular
you know if you if you want to
characterize semantically the noun
compound malaria mosquito maybe it's a
constellation all is the source relation
I mean those are like abstract relations
other people like wow where he proposed
using prepositions say like it's
mosquito with malaria or its oil from
olive okay and they there are some
approaches that are somewhere in between
that he had verbs and prepositions and
what I want is I'm looking for the verbs
that can be used to power face so that
malaria mosquito is mosquito that
carries spread causes malaria
transmitted and so on in these verbs can
be useful for
they can be used for example like in a
search engine I mean if you kept like
malaria mosquito if you if you have
found on the web page if you have on the
web page works like scary spread causes
transmitted and so on you probably are
more certain that it's about this
particular kind of malaria mosquito and
this I mean they can be used also
instead of authority you can use it in
ranking or another thing is if you have
malaria mosquito mosquito that carries
malaria they are the same thing okay and
when the user asks for the noun compound
you can use for the other one or vice
versa you need to decide when noun
compound is equivalent to a particular
putter face so how I extract these verbs
it's a very simple technique essentially
we have okay you have a noun compound
the first now in the second noun and
here we have a primary modifier and what
i do i turn it into a post modifiers so
which is the relative clause instead of
noun 1 now to say like malaria mosquito
it becomes like mosquito dead something
malaria and then you work for the
intervening verbs which is pretty simple
and I use the Google star operator to
that and you find things like say why he
is for migraine treatment its treatments
that are out there for migraines they
should be given and so on and if you
just extract the words you end up with
things like you know me a migraine
treatment is treatment it prevents
migraine is given for migraine and so on
and if you look at different things like
cancer treatment migraine treatment
wrinkle treatment they're interesting
things for example cancer treatment okay
it treats cancer preventing cures that's
good but wrinkle treatment it's reducing
wrinkles and smoothing treatments but
does not prevent them so there's
something about the semantics and he is
how it compares to previously specified
you know high-level semantic relations
there like malaria mosquito has been
poor positives really calls but if you
think about it it's an indirect
causation it's not that the mosquito
doesn't doesn't really cause malaria
it's like the the particular
virus that the mosquito transmitted is
causing the malaria so okay here in in
both I have verbs that sub you know
according to me support this particular
relation and the ones in talak are ones
that don't directly support this
particular relation but I still consider
them relevant I me nice to think that
they correctly paraphrased another
bounty and you see that this not I mean
not that much over web sometimes say
like mosquito that carries malaria
spread malaria transmission malaria
brings and so it's not direct causation
okay and then here you have I have an
agreement with humans i mean the kinds
of words that I extract I mean I have
set up a task with Mechanical Turk
amazon's mechanical turk asking people
to give me works that can be used to
paraphrase on our compound and this is
what they gave me like for malaria
mosquito carries causes transmits
effects and so on and this what i have
so you can see this like kind of
agreement i have done this for for a big
collection 250 examples but i'm still by
collecting the data from the Amazon
Mechanical Turk so I cannot give you the
final results right now okay so you can
you can apply this for finding relation
between between complex nominals this is
this was the same about task and this is
what the task is so you are given a
particular sentence which is your
context and you have two entities that
are marked in 21 and mg2 so he have like
vessel and tools they are also given
there were net census which are not
using then the particular relation
content container you have to to make a
binary decision about whether vessel in
tools in this particular sentence are in
a content container relation or not and
then you are also given the particular
query that has been used by the
annotators but by the people that put
together the task I was a cot ask
organizer that was used to extract this
particular sentence so here examples of
the several relations that were used in
this
to competition so we have cost effect
instrument agency particle and so on so
content containers they like Apple
basket plane cargo origin entity like
Desert Storm grain alcohol and so on so
what i'm doing here i I just I just go
on the web and I work for sentences that
contain both words so here I have like
one star or actually essentially up to
eight stars and I just try to find I
mean sentences that contain this word in
the other work in any order and then I
extract the intervening verbs in the
intervening prepositions in the
intervening coordinating conjunctions
because they somehow have to do
something with the semantics of the
relation that connects those words and
you say he is what you get save for a
committee member for committee member
you have it's a member of a committee
committee includes members members or so
no committee member chairs the committee
committee has member consists of members
and so on he have the frequency here
here like what kind of entity Jesus like
a preposition or a verb or a
coordination conjunction in the
direction because whether it was first
known before or the second known after
and this did you know you can use this
as a distribution of you know the
distribution of these verbs and
prepositions and coronations you can use
them to make classification essentially
in this particular task you had training
data 140 examples in the testing that
date of 70 examples and what I have I
have a nearest neighbor classifier you
know I have for each of the training
examples i have of a vector like that
okay and then when i have a test example
i produce a vector like that too and
then I compared it to each of the of the
individual vectors and I choose the
label or the one you know that is
closest to my vector and i'm not using
like a class based model because they
don't work very well and this is
something that Peter ternion telling
people working in this task have found
because I guess that the relation
it's not it's it's kind of fuzzy I mean
it's not very well-defined I mean what
is really like content container watch
3d my part whole and so on so the
instance based models work better for
this particular task and the features
that they have the work the web features
ver prepositions were purple plus
preposition coordinate conjunctions and
then also the context features why the
sentence words entity words and so on
yes I have like a que nearest neighbor
classifier in the desk coefficient and
hear the results UCB this is my system
under the condition that I don't use the
word quit I mean the the query I mean by
query I mean this query I was allowed to
use its work too and I don't use were
net I did not want to use word that
because I think this is unrealistic
assumption to have the particular
correct were net sense for this
particular now in this particular
context okay and I have the second best
system using up to eight stars in
between the words I mean remember when I
was fishing for them and if I use 10
instead of eight I mean I get even
better results which are better than the
best system this is not statistically
significant but still and then in the in
the condition where you i'm i'm using
also the query words i have the best
system and you get even better when he
like 10 starts which is again a support
for the history that you know hearing
more data you know gives you better
results yes
I I cannot tell from the top of my head
but I mean they days a paper to
dissipate by describing i can i can open
and i can show you if you want right now
i mean i have it but i cannot found out
okay so okay and now I'm movin to the to
the final task machine about machine
translation so machine translation is I
mean I guess is anybody here familiar
with machine translation I mean okay yes
ok so then then I mean then I should
shouldn't to expend much which time I
get but anyway it's its taint on
parallel text and phrase-based machine
translation you you extract pieces of
text which you call phrases that are not
necessarily mystically phrases I mean
they're just some particular piece of
text and you have some probabilities
associated associated with them and this
is the backbone of the translation
process I mean when you have like a
particular sentence that you have to
translate you are trying to find you no
good pieces of text cause translations
to you know the problem is okay so
suppose you have this phrase spain's
economy okay it now in your training in
your text that you have to translate
supposed to hit the economy of Spain I
mean it's not exactly the same thing
it's syntactically it's it's kind of
equivalent but it's a little bit
different so you are not much it and and
in this particular case which is
probably not not much of a problem but
in other cases when we have a non
composition of translation you can miss
an opportunity to correctly translate
you know some something no non
compositional so my G is ok maybe you
can go into back you can paraphrase the
the fridge table ok so you here in half
likes spends economy economics painting
the economy of Spain spain's economy
Spain economy for Spain Athens pain of
the Spain some of these are crap I mean
some of these are wrong some of these
are correct and you can probably use the
web to filter the bad ones
so here are some examples of what
happens when when you para is the first
table that way so for example we have
like percent of members of the Irish
parliament it's person to fight
experiment members percent of Irish
parliament members I mean they're kind
of the same thing I mean from from a
syntactic point of view okay and I'm
paraphrasing the source the source side
you know the source language and I am
translating from english into a foreign
language and in this particular
experiments i have from english into
spanish and here's another one say like
the danger of infection with AIDS okay
danger of AIDS infection aight infection
danger aight infections danger I mean so
what I'm trying to do is essentially
okay another thing that you can do is
probably paraphrasing the phrase table
is not the right idea because the pieces
of text are kind of mean they're cut you
don't really have the context when you
have like a some wonk noun phrase I mean
you probably have just the beginning of
it and it's it's just wrong toto I mean
you can you cannot paraphrase it
correctly so maybe it's better to
paraphrase the whole sentence ok so the
next idea is ok you have your training
corpus we have this particular training
centers and to paraphrase it in
different ways and you pair each of the
paraphrases with the original Spanish
translation translate you know do you so
you here we have this you have you have
for example we must cooperate interation
this should be should include you and
initiatives this is like an English
sentence it get some spanish translation
you have several paraphrases of the
sentence ok and you pair you say that
this sentence has that each of these
sentences has the same spanish
translation as the original one because
the kinds of paraphrases i'll show you
shortly what kinds of Perez I'm doing
they are very likely to be meaning
preserving so he is essentially what I'm
doing what I'm doing is when you have
something like an instrument of economic
policy which is like a noun phrase with
an internal preposition I'll turn this
into a noun compound so it will become
bad economic policy instrum
it in here when I half an hour compound
I'll do the reverse I like split it out
I'll paraphrase it with a preposition so
like a budget becomes budget of the AU
ok so those are the kinds of paraphrase
is that I do and so what I'm doing I'm
parsing the English sentence and then
I'm I'm looking for patterns where you
have like a preposition prepositional
phrase in size and MP or when you hear
when he half an hour compound and I'll
turn this into this and this into this
ok so and these are these are the kinds
of paraphrases I have so here you ok the
lifting of the beef import ban the beef
import ban lifting so I'm removing the
preposition making into a noun compound
here i have like putting a possessive
marker the lifting of the beef import
ban the beef import bans lifting or
something that has a possessive in all
genitive marker I can just remove it
commissioner statement Commissioner
statement or Commissioner statement
statement of the commissioner and all
these are kind of syntactic and then
when you have to break an hour compound
you have to use the web I also i'm using
also the web for this ones because not
all of them are always good say like
inquiry committee chairman inquiry
committee chairman you can like put a
genitive markets and positions or like
the beef import ban ban on beef import
will generate a particular preposition
so how do i do a parent / up the
parfaits say suppose I need to break a
noun compound like beef import ban
lifting okay so I try all possible
groups like beef import ban lifting beef
import ban lifting and so on I mean why
keeping like putting a break at each
position and then I try his like the
word to the left in the particular
sentence energy is the word to the right
and one is no geez peace n 2 is this
piece and then I try putting the
possessive marker here preposition and
optional determiner debt copula
determiner or dead Kokua preposition
terminal and you know I adjust
choose the most frequent ones and i
generate a couple of different
paraphrases and i can have multiple
options there like you an inch
initiatives it's like initiatives of the
UN at the UN in the UN they are probably
all good or maybe not i mean it's I mean
it makes mistakes some time so the
results what I have is this is this is
the baseline for 10,000 sentences you
have a blue of 22 point 38 if your
paraphrase without using the web you wus
but sorry ok this is the europe europe
parallel corpus 10,000 and this is the
2006 where you have like the workies
data which is already tokenized yes this
is jump only one ten-thousandth and
consistent but i have a hip hear
something con more okay so what happens
how much time they have but anyway I'm
around you yes I have like two or three
slides much yeah okay sorry so okay if
you I mean here i have only like the
first four rows so which means that i'll
turn something into a noun compound but
i'll not split now compounds okay and
then if you do that yours but teach you
if you also split now compounds then
you'll get some improvement if you
paraphrase the phrase table this is this
result you still have some about the
same improvement ok but what what is
really good is when you're so okay this
result is like so s is like the original
system so you your train the original
system you end up with some phrase table
and then this one is the phrase table
over the paraphrase corpus okay which is
kind of big away where you paraphrase
the sentences and then you merge the two
take the first the two phrase tables and
so
and you can merge merge them in
different ways one way is simply to
merge them and to prefer if you have
like a phrase that is coming from the
original corpus I mean you prefer this
one because the the probabilities are
better than the ones on the paraphrase
corpus and another thing that you can do
is you can have additional features that
tell you whether it came from the
original table or where it came from the
from the new table the paraphrased
corpus or whether it came from bot and
you can also like in addition to
paraphrasing the corpus you can also
paraphrase the phrase table and they
have experiments but you don't have much
much of a difference so okay and then
here this is like the size of the phrase
table 180,000 and here we have 300 so
280 which means it yes you do generate
more phrases I mean we do have more face
in the face table and yes you do use
them because here we have 40,000 used
and Hugh here 56,000 I mean this is Miss
how many phrases you here after the the
face peel drinking Moses or in I mean
when we do the recording so mean when
you when you test I mean you do use on
testing more places yes
yes finally the way yes so okay here's
what's happening I have I here i have in
the face table something with some
particular weights right so the problem
is that when I paraphrase and 20 train
the probabilities are kind of wrong
because for this particular phase now I
have 10 paraphrases and for this one I
have only once and now like the lexicon
weights are different and also like the
weights you know the probabilities for
the particular phrases are different and
I don't know what is correct I mean I
can I have no way to to calculate it
because this particular phrase never
happened for some of them some of them
happen so what I do is I output here an
extra feature you know the decoder can
can can have additional feature so I
have a headphone extra feature it tells
me okay this came from the original
phrase table or just came from the
frontal from the second you know from
the paraphrase corpus or I mean it came
from but because this can also happen
and you to like somehow one preference
is not before one to the other however
even even if I just merge them okay and
and give in the case when you have the
same phrase coming from bottom to
originally from the paraphrase corpus
out kiddo the probabilities from the
original one because I trust them more
if i do that i mean i have 23.5 if i use
this extra fish i had twenty three
pointer teen the difference is very
small this tells you that you know the
kinds of paraphrase it i'm doing there
pretty much you know very meaning
preserving okay so here's what happens
when you when you work with bigger
corpus I mean you don't learn that much
here we have one 2238 to 2305 on 20,000
from 24 30 feet 25 0 1 140 you know I
mean the game is get smaller okay but if
you walk also here I mean this gain also
the game
of doubling the training data also gets
more so I mean and essentially what's
happening is what I'm getting from here
to here I mean from the baseline to what
I do with the paraphrases gives you the
effect of between 50 and thirty-three
percent of the game that you would have
you know gained by doubling the amount
of training data sorry bigger when you
double the teenager going 22.3 a 24
points you see that's less than 2% as to
what it's more than two percent which
one from 10k to 20k the improvement is
less than two percent right yes and from
20k to 40k it's more than 2% right yes I
I don't know yeah did you just say I
mean I I don't know why this happens I
mean because I mean I just kept like we
know Marilyn training sorry the
improvement slows down so far oh I see
okay unfortunately he oh you're saying
he it's not let's see oh jeez hehe it's
almost too he's almost too but but here
you know it's kinds of it's only half so
but but then you I mean huge expected
with with double training data at some
point I mean you don't you don't I mean
from you don't earn that much by diving
the training data at some point I mean
and sorry that would be replicating you
would not get much from the train yes
and essentially I mean I kept I kept a
fraction of that okay it's just a
generalization because I guess the
question is by the time doubling the
training data start improving
yes yes so so what what what what what I
think is happening is that I mean when
you have a lot of training data just
paraphrasing you problem not help that
much so it's probably helping for for
the cases when you don't have that much
train data may be like you have like
fifty thousand examples or like the
sentences or a 30,000 or something like
four for Less covered languages I mean
then you should expect improvement you
can try to do that ok so I'm
paraphrasing the source side and not
perfect by the target site because the
target side mmm it's I mean you can
probably get some improvement by
parfaits target site but only because
you can produce probably something more
fluent I mean you can only like improve
fluency but really the power of this
thing is to to to find you know too much
more phrases in the input this is where
what what you essentially want and
another thing that you can do is you can
probably paraphrase the target sentence
that you want to translate so and you
can you can think of maybe like
normalizing the training data for
example like if you if you want to
translate something from english into
spanish then I mean because Spanish
doesn't like now compounds okay it works
prepositions you can prob like
paraphrase it sorted in the training
time you can have like you can get rid
of all non compounds and you can have
like you know prepositions that and if
you were a model like that that really
translates from english without no
compounds into spanish maybe you can get
that improvement or i mean you can when
you have centers to translate you can
just regenerate several different
options I mean several different
paraphrases translate each of them and
but then but then you need a real anchor
I mean you need something to change
which one of them so you need some
training data I haven't done this yet so
and yeah I participated in the sure task
of WMT
and from English to Spanish will miss
corpora I hear the best system on a
couple of measures unfortunately i'm not
using this particular paraphrasing
technique because i didn't have time to
for the manometer a training and so on
to complete sorry i have like a best
cordon bleu meter or in the cup of
others and if you're curious what i used
for this particular submission so ok
this is the baseline system as described
on the WM g you know website and then
just by changing the tokenization you
give like half a point of law and the
way I'm changing the tokenization is
essentially ok for example when you hear
something with a lit with a dash I mean
like Sehwag brainstem together I mean
output spaces around the dash so I just
like put more spaces around the
separators and this seems to help and
then i'm using quite sentences not of
length up to 40 but went to up to 1000 i
use of course lexicalized or drink to
language models one trained on the
europe parliament in the other one on
just decide and I have like very high
order I'm not sure I should have fused
dead higher order in thai also use
cognates this is the kind of law that
they have but unfortunately i couldn't
use this paraphrasing this one because
the experiments did not complete in time
so ok to wrap it up what i have done is
i have described you i mean i had
proposed to use additional sources i
mean using the web not just the source
of Ingram's but I mean using different
surface markers in using different
paraphrases to get better results and I
have achieved results that for the noun
compound bracketing a heavy achieved
results that are better than the best
published algorithms so of course there
are probably more exciting features on
the web and I mean there are problems
where you can apply such kinds of
features and it's already happening
there are people that use some of the
features that I have proposed for curry
segmentation there was this easy on
paper this year and yeah I mean what I'm
really advocating is using the web as a
corpus not just sorts of angles
really like getting into the text and
you know maybe like stacking the box
from the exactly prepositions so on
thank you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>