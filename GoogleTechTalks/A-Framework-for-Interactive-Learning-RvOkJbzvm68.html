<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Framework for Interactive Learning | Coder Coacher - Coaching Coders</title><meta content="A Framework for Interactive Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Framework for Interactive Learning</b></h2><h5 class="post__date">2017-10-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RvOkJbzvm68" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">from as a social professor in university
of southern california and it's done
lots of interesting work in algorithms
game theory and social network Thanks
thanks for having me on such short
notice on a holiday in all this so I
want to tell you a little bit about work
that I've been doing with my grad
students animam Tuam is a day over the
last couple of years this is resulted of
mostly the first two papers one from
stock 2016 one that will be in this
upcoming nibs and depending on how we
have time I might I have a couple slides
on how we use basically the same
framework in a different context and
that's a paper that was generally
accepted to skoda 2018 as all of you
know most of the work is always done by
the grad student that's no different
here on the stock paper there was also
an undergrad contributed a little bit I
felt that this is so much more essence
work than big fronts that I'm only
listing really asked on primarily here
also he will be looking for internships
if you like what you hear let me know
and I'll make sure he sends this resume
not just to Mohammed but so there's a
quick motivation I think most people who
are the ones of you who I know are these
theorists or just plain to your wrists
so I'll keep the motivation somewhat
short but feel free to ask for so we
frequently need to deploys of an output
of a learning algorithm before it's
fully learned so it's some kind of an
online setting so for instance you're
learning a classifier such as will I
like this song before I really have
learned the users preferences fully or I
deploy ranking up States and search
results what are the most relevant pages
for a particular topic before I know
actually what criteria to use this
specific user is using to evaluate
search results or I need to provide a
clustering again before I've learned a
particular users clustering objective
obviously if you're over enough data and
you have users that are all identical
you'll relatively could
we have enough data to really learn this
well but otherwise yourself in an online
setting and gradually you receive
feedback and that feedback can let you
improve what you would you learn you see
that the users are somehow attracting
with your output in a way that reveals
that something was perfect about what
you learned and that presumably lets you
come up with a better solution and
that's really just all that I gave you
is a very generic motivation or the need
for online algorithms maybe with a
slight twist of online learning and so
little stolen in SML 1988 people really
defined a whole online learning
framework and we are more specifically
working in the other seminal 1988 online
learning paper in the equivalent query
model of Dana Angwin and so that model
sort of works so that was specifically
formulated for learning and basically a
binary classifier although Dana surely
also knew that it would extend very
easily to non binary classifiers so the
algorithm proposes a binary classifier
and receives one of the following two
response from the user either a good job
your classifiers right to learn it or
here is one point X that you are
misclassifying please put that X on the
other side and using that feedback you
can improve your classifier and repeat
and so the goal is to do this with
relatively few career queries if you
know nothing then in principle in the
worst case you're going to have to
correct each point individually you will
need n iterations of this but so I'm
gonna call it the main results of the
main result that one remembers although
there are some harder results of later
as a paper is a theorem by Angwin but
really it's sort of happenin in the air
and around and some papers I think out
of Russia also earlier that if the
classifiers from some class offenders
then using improper queries you can
learn and using at most log base two of
this class many queries sorry what's
improper quite improper I mean the
following suppose that what if you have
a bunch of points that are in a 2d space
and you know that your classifier is
going to be a hyper
and Euclidean spectrum no your
classifier is a hyperplane so there's
only lie the VC dimension is low the
class is small so you should be able to
learn this fast but in order to make use
of this and actually learn fast I need
to sometimes propose a classifier that
is not a hyperplane so I will give you a
classifier that's not a hyperplane I
know that there will be a mistake
because I know the ground truth is a
hyperplane but I'll learn more by
deliberately giving you something that's
not a hyperplane the information I get
out of it it's more useful and then
eventually I will learn the correct type
of line but this theorem is not you're
just asking it's not true because if I
cannot choose the point like it is not
about to learn I'd say tell me what in
the sense that I go tell me what is the
correct label for this point I propose
something and then you choose a
correction but I have no control over
which correction you propose all I know
it's something that I did more wrong
exactly okay so so this is the
equivalent query model of Dayna hanwen
which I sort of had a bunch of papers
early on by Dana and a couple other
people that seems to have been a little
bit forgotten over the last twenty years
and I mean part of our point here is it
should probably be rediscovered because
it's actually a really nice model I
think it's almost I like that model
almost better than the little song on my
model though in some precise sense
they're kind of equivalent so here's our
generalization of this the algorithm
proposes a model and we're using model
as a generic term that subsumes say a
classifier or a ranking of items or a
clustering of a bunch of music so think
of any kind of learning task and the
proposed thing that is to be learned we
are just calling a model as a very
generic term here so it doesn't mean
you're learning specific parameters on
the class of models it's just sort of a
very generic term that hopefully isn't
too loaded and the user again provides
one of the following two answers either
your model is correct or here is a small
local correction to your models earlier
this was saying this is a misclassified
point but there could be other local
Corrections that will give you a bunch
of examples in the next few slides based
on this the algorithm uses feedback and
improves the model and then proposes a
new model and so now the question is why
are the suitable local corrections in a
domain of moms that you're interested in
and when can this bount that Dana and
will improved for the classifiers be
achieved for a more general setting so
this is really the questions we're
mainly answering realisable setting
where us nervously because there is a
very important exactly we're assuming
that there is a ground truths that you
need to find right and obviously if
you're dealing with say a population of
users there might be ten ground truths
and you don't really know which user
you're interacting with so there's all
kinds of wonderful extensions to this
for the purpose of this talk there is a
ground truth and your goal is to
discover it okay Oh rats
I forgot to recompile so that you don't
see my grid for aligning things sorry
this is what's supposed to help me in
drawing all the animations for ranking
so I don't know whether you can try to
read this so imagine that you're
searching and so my my students made
this animation right around the time
when the travel ban came out example so
you're googling travel ban executive
order and so here are the topics that he
found and so suppose that you click on
this one well somehow I think this
conveys the information that this clear
link might actually have been more
relevant at this and I mean there's sort
of some earlier work of course new items
that uses this and I assume that most
modern search engines probably have some
version of using this click information
that's user feedback says probably this
should have been ahead of this one if we
have the same user again maybe we'll
rear axles so this is would be the kind
of local feedback that you get that
these two should have been in the
opposite order or generally say after
clicking here on the next click is down
here well that maybe means that this one
should really have been ahead of these
four so that's still local slightly
slightly less local
so so that this is the kind of feedback
that one would get maybe in ranking that
would be this of a local correction to
your rank so here is some written more
formally I mean I just showed you my
picture so the bubble sort feedback
would be that you reveal a pair of
adjacent elements that are out of order
so the user tells you these two here
should really be in the opposite order
insertion sort feedback is you get two
positions inj such that the element from
position j really should be in position
I and all of these should be at least
one position further down so you get the
information that change should proceed
all of these red elements and the
motivation is what I showed you earlier
the user clicks on I minus 1 and on J
but are nothing in between so probably
this was more relevant obviously an
abstraction but I think reasonable model
it's or for some theory the other cities
but so you're treating the insertion
sort order as a deterministic statement
that Y is definitely above each of those
four and there's complete information
there and no information about I versus
I minus one and up there are correct so
this would be the feedback since you
were stressing the word deterministic in
a few slides
about ten fifteen slides I will loosen
this and say that the user isn't always
right about this so instead of
deterministic this will become
probabilistic information but yes it
makes the statement that the element
that right now is in position J should
precede everything in the earlier
positions and it makes no statement
whatsoever about their relative order or
the relationship between this evergreen
element Y and everything else okay
thanks
so the other one is clustering so when
you interactively with the user want to
learn how to cluster say an image and by
the way this slide is borrowed from
Pranjal Abbas T is Simon's Institute
talked earlier this year so yeah I
wouldn't make such beautiful pictures so
you have a user and the user sort of
helping you learn how to cluster this
image
and so can give you local feedback say
that yeah you have too many customers
you have too few clusters they should
look different in particular ways so the
model that was proposed in two papers by
Balkan and Blum and then subsequently
used and refined by us the ins Adi is
the model is now a partition of the N
items into K clusters where K may not be
given so and it's maybe adaptive and in
local there's two types of local
Corrections that they propose one is you
reveal that there's a pair of two
clusters that must be merged and the
interpretation is in the ground truth
all the elements that both of the
clusters belong in the same cluster so
here you had to read Clause two clusters
unbeknownst to you in the ground truth
they're both read so the user will mark
both of them and tell you everything in
here should be in one cluster so this is
one type of feedback the other one is
the user reveals a cluster that really
shouldn't be just one cluster it has
elements from more than one branch with
cluster so here this cluster has three
different colors so that should be split
up the user doesn't tell us how to split
up it doesn't tell us how many clusters
are contained in here but it has at
least two colors in the ground choose
that's the other type of feedback so
that's the feedback that they consider
provide algorithms for and actually some
did some studies on like actual image
segmentation so these are so some types
of examples of how what would local
Corrections would be beyond
classification and so here's our
framework from from the nits paper so we
put all this together as follows we have
a graph we define a graph G whose nodes
are the models so the classifiers or the
rankings and factorial rankings two to
the N classifiers or bail number of n
many clusters and G must contain at
least one edge designated for each local
improvement
so for each classifier you have some
outgoing edge that somehow is labeled
with if the user gives this feedback
that's the edge we assigned or it
doesn't mean anything about what the
user actually meant but we have to have
at least one of those edges we are free
to put extra unlabeled edges in the
graph
and we are free to assign positive
weights to the edges in fact I will show
you examples where you need that so so
this is how we're going to model this in
yet for those ages the second bullet
point those have to be edges they're
consistent with well saw so that that's
the key property so you get to define
the graph any way you want if you want
to use our theorems this is the one
property that you're the graph that you
define actually has to satisfy that's
exactly the notion which is suppose that
there is an underground truth model C
star which is a node in the state graph
and there's a proposed model that you're
showing the user right now that's called
V and the user suggests an improvement
among the edge V u so BU is the leading
edge that you labeled in such a way that
it corresponds to this improvement then
this edge needs to lie on a shortest
path from V to V star with respect to
the edge planes so in that say that's
the sense in which runs for the hyper
tube you need to have the edge label in
a correct direction so that you don't
accidentally move away from this one
point R so this has to hold for every
target least R for every proposed model
and for every response that the user can
give you so this is the one key property
that we need off the graph but other
than that you get to define the graph
any which way you want it to be we
define the loss function or something
and the new major cost so here we don't
assume anything about it there is no
objective function so we're really in an
online setting trying to learn what the
user has in mind that we're making no
assumptions whatsoever about what's
going on in that mind and what
objectives they have what weights they
have not only one basic equations you
map maybe the degrees and loss so you
might be of loss so in some sense I mean
for classifier you might say the loss is
I don't know the fall specifications and
so none of these edges would give you
decrease in loss but of course then
yeah the shortest path in some sense
would be I think it would be decreasing
in terms of the distance so you could
use the distance as a loss function if
you wanted okay so this is this is the
one key property we need to hold and let
me just go through the example so for
the classifiers we have one node for
each binary classifier and an undirected
edge when they differ in exactly one
label so our graph is just an
n-dimensional hypercube and to verify
that this has this shortest path
property that I just gave earlier so a
shortest path from some classifier let's
say this bottom left corner here is the
one that we are proposing right now and
the front top right one is actually the
correct one well it needs to correct
these two labels and it doesn't matter
in which order it does so so if I get
this edge it's on a shortest path if I
get this engine sin our shortest path if
I get this edge it's not on a shortest
path but then that couldn't possibly
have been the correct pacifier because
we just made one label wrong so this is
sort of the intuitive argument that
every label Kretsch actually does lie on
a shortest path so we can just use the
hybrid shoe and I mean you just from
that would get exactly the old 1988
algorithm from our framework
particularly interesting for rankings
you have one node for each binary
permutation and so in the bubble sort
model we have an undirected edge
whenever you obtain one cranking from
the other by one adjacent swamp and so
this is the graph on four elements with
24 nodes for the 24 permutations these
edges are undirected there and the
shortest path from a permutation B to
another permutation T star must face all
out of order pairs it only fixes one per
move so the order is arbitrary and the
distance or the number of swaps is
exactly the Campbeltown
distance and so as a result every swamp
that's proposed does lie on the shortest
path because it decreases the candle
towels or if you want to think the same
terms it gets the distance gets smaller
interestingly if we allow the user to
suggest non-adjacent swaps like three
and seven are in this order they should
be in the other order and I don't mean
to say anything about what's their
relationship is with things in between
we can actually prove that there's no
graph directed or undirected weighted or
unweighted that has the shortest path
property so so there's a little bit to
what this framework can do for you so
why can you just take the Delta candle
Tao like the the cannot ever notice
label to the candle tower distance to
the um the problem is it's not really
clear what would be the next what notes
this should lead to because if you swap
them if you just went ahead and execute
at the Swap that the user proposed the
candle tell might actually get larger
because I thought you mentioned as
probable as the shift no that's also the
shape there's gonna be on the next slide
so this is if you can do not adjacent
swap and so in that case you're gonna
just executed that one if you
interpreted it as what the user might be
meaning to swap those two then you might
go further away so you need to somehow
infer from this what is the better model
of the you dratted mind and you can't
guess that one right so we have some
some argument of some I mean both of
these situation this is another
potential function that corresponds to
the shortest path I guess what cases so
what it does they go the potential and
when we say that it lies on the shortest
path that means the potential function
goes down so in some sense that
statement is certainly true but we feel
that you don't really need to talk about
potential from because it plays no role
other than being a distance okay so now
the other ranking one so again you have
one for each permutation and now you
have an undirected edge from V to V
Prime when V is obtained from V Prime
the other way around B prime is obtained
from B by moving the element from
position J to I and shifting everything
so executing this shift and we give it a
weight of J minus I so this is the first
example of a case where you actually
need to weight the graph and notice that
if we allow only shifting forward from
the clicks
no shifting backward from the clicks we
actually we have an undirected edge
where really the other direction
shouldn't be there right there is no
move that the user can suggest to move
an element back and shift everything
forward but it doesn't matter as I said
earlier we're having extra it just
doesn't hurt us so long as it doesn't
destroy the shortest path property which
it doesn't hear we're fine so thus
longer move can really be just
decomposed into a bunch of adjacent
swaps obviously and so you can basically
recycle the previous argument to show
that as a source property for clustering
it's kind of interesting you have one
node for each partition with any number
of partitions and you have a directed
edge of weight to from a clustering C to
C prime when C prime is obtained from C
by just splitting one any one cluster
into two clusters and leaving all the
other clusters intact and now
interesting if a user says this cluster
needs to be split it has elements from
multiple other clusters it's not really
clear how that split should happen so
what we can do is we can put a directed
edge of weight one when one cluster is
replaced by a single the clusters of all
its elements we're not saying that
that's what the user actually managed we
just said we'll insert this directive
edge and label it with that action it's
our choice so long as it has the
shortest path property so even though
the user didn't imply that he wanted to
break this cluster into singleness we
have the choice to interpret it that way
and add this edge and it turns out the
proof is a little bit more complex I'm
not going to outline it here but you can
actually prove that any edge that the
user suggests now lies on a shortest
path and notice this graph is directed
so early on all our graphs were
undirected this is an example where
actually need to have a directed graph
okay so so this is how you would apply
the framework here so connect another
question so your example for swaps that
the reason is I didn't mean faster swap
between two and also no Jason these two
gives an information it gives you
information and in fact there is an
algorithm it achieves the same
guarantees I'm going to show you for the
craft based framework all I'm saying is
that we cannot model it in our framework
you learn information and you can do
very
well in that model of feedback union but
not using the techniques I'm showing you
I just felt that it was useful to also
see the limits of our technique because
that often showed us you like where
exactly the technique is situated
so here's know an abstract version of
what we want to solve we have a known
graph G with edge weights and capital n
nodes we have an unknown node T which is
what we call the target and the
algorithm is given a node set s
initially that Garan is guaranteed to
contain T for instance if you know that
your classifier is a hyperplane then you
only have a subset of fiber two but for
the purpose of understanding stalk just
think of s initially as being in a set
of all nodes of the graph and so the
algorithm queries no it's V and gets two
kinds of possible feedback either good
job you found the target or otherwise
you get an edge e out of the query node
that lies on the shortest path to this
unknown target and if there are multiple
such candidate edges e then the one that
has returned is chosen adversarially
just like when there was a classifier
get multiple things that you missed
classified you had no control over which
one was returned and our goal is to find
this target key with few queries and
what's a design parties working in
design then so far what can you design
in this point so so now we're now this
is a pure algorithmic question I come up
with an algorithm an online algorithm
that does well for this task on an
arbitrary graph that you're given so the
first there's the design choice of how
do you build the graph G to model and
you're learning a problem and I showed
you the answers to that on the last four
slides and now we have an algorithmic
question to solve find an online
algorithm that decides in what order to
query these nodes based on what feedback
it gets so what came in the maybe I'm
sorry let's talk somebody so by the way
this was our stock picture which
preceded the newspaper so we just
defined this here as a cheap algorithmic
question that we like and then so the
referee is rightfully complained that
our work didn't have really any obvious
applications it was just a few theory
question and then shortly thereafter we
came up
all these applications to learning so we
kind of did it in the wrong order but so
hopefully I've motivated why this is an
interesting question resolved now that
I've shown you all the learning
applications so a few teasers you guys
know somebody you have weeks in the
graph you want a few and Q will mean log
of M allowed log of capital n or log of
the size of the set s so that's what few
is gonna mean you could also have the
goal of just looking at the graph and
trying to minimize it and I might
mention a few things we have some
results on this but we're here
our goal is logarithmic in the size okay
let me just before I move on on
something on the whiteboard and everyone
except Mohammed already knows the answer
because a fourth over lunch gets too
much I just want to make sure that
everyone understands the question so
suppose that this is my graph it's a
very easy graph G and I tell you I have
one known on this graph in mind I'm not
telling you which one but aren't some
who use what also that people over there
have seen so here's here's my graph it's
a very complicated graph one of these
nodes is my secret target you want to
find it and if you guessed right I will
tell you if you guess wrong I will give
you an answer on the shortest path to
the target which here happens to be
unique
what is your first guess right Oh
excellent
if I say it this way okay you all figure
it out that you need to run binary
search here excellent
so we were interested basically in
generalizing binary search to arbitrary
graphs so our theorem is for arbitrary
undirected graphs the targets have
always been weighted undirected graphs
weights don't hurt us the target can
always be found in at most log base two
of your initial sense or at most log
base 2 append queries now sometimes you
can do it unless if you have a star you
query the middle and you're done
but for arbitrators our ass
you can always put in at most log base
number of notes and so we called this
binary search and graphs as you just all
figured out on your own when geez just a
path the optimal algorithm is binary
search when G is a tree you want to
query a node such that if you remove
that node all remaining components that
size at most n over half of the notes
that you have exceptional it exists by a
paper of Sheldon from 1869 fools people
or I guess is older and so we want to do
this for arbitrary graphs and again we
have an animation so anytime we have an
animation we have this annoying grid so
for any so here's the algorithm it's
fits into lines for any node said let
the media be any node that minimizes the
sum of distances to all the other nodes
that are still active right so you have
to set s and it will change over time
we'll rule out notes as we go along you
have a set of candidate nodes you
minimize the sum of distances from your
note to all the candidate notes and we
so that's a median of the graph and it's
possible that the median itself is not
in the set s so it's possible that you
query a note that has already been ruled
out you can construct inputs where this
happens and then who result in what I
called earlier improper learning or you
proposed a classifier out of the class
that you're trying to learn and so you
just repeatedly while you have more than
one node in your set you clearly the
median of your set and then you update
the set by removing all nodes that are
inconsistent with the answer so let's
just try this out on one little graph so
we see what happens so here is our graph
and say we query this node and we get
the response that this is an edge on the
shortest path from the one we query to
the target this means that all these
nodes over here cannot be the answer
because you would never go through here
on any shortest path so all of these
white nodes have now been ruled out and
we have a set s that only consists of
three nodes now we query a median of
that set that happens to be this node we
get the feedback that this is the
direction so now we've ruled out both of
these nodes and this is the one
remaining nodes that has the apart so
this is how you would do binary search
and
for a graph and the analysis I won't
give you the full lot but I'll give you
enough that each of you can figure it
out probably in ten minutes on your own
now so lucky lemma is if you have an
undirected graph then whatever possible
response you get to the median query
eliminates at least half of the nodes so
you saw earlier that a lot of nodes got
eliminated this was no accident
anytime you query the median of your set
at least half the notes get eliminated
and the proof idea is basically if you
have one response that didn't eliminate
half the nose you follow that edge and
clearing that known and that actually
has the smaller bound so that your node
wasn't actually the median just like one
or two paragraphs of calculations for
directed graphs things are much worse
obviously so far everything I showed you
was undirected for instance if this is
your graph well if you query this node
and you're told follow this edge it
doesn't reveal anything because if it's
not this node then you're gonna have to
use this edge so you kind of need n
minus 1 queries in that case so then
we're can asking ourselves we want to
use this for directed graphs or
properties of the directed graphs graph
would give us something close to what we
have on directed graphs and so the key
property for directed graphs is we call
it the graph s see almost undirected if
every edge E is part of a directed cycle
of total weight at most C times W so
basically if you have an unweighted
graph every age every directed edge must
be part of a short directed cycle and so
in some sense the cycle is of course the
worst graph because the cycle is long as
undirected graphs in the sense are to
almost undirected so weird form a
formulation and what we the lemma we get
is that if G is see almost undirected
and whatever response you get to the
queering the median eliminates at least
the 1 over C fraction of the nodes and
when you apply that repeatedly you get
that if G's see almost undirected and
you can always find the target using at
most C times natural log of your initial
side so I said it's probably not quite
tight we have a lower bound of C minus 1
over log base 2 of
log base 2 of the size of the set for C
equals n this gives you the right answer
so I'm pretty sure the lower bound or
something like that lower bound is tight
and there's substance log C gap between
the two we spent a couple days on this
and weren't able to come up with an
algorithm that improves the upper bound
by this log seam factor that's kind of
an interesting question too
or someone else may dissolve okay so now
let's go back to where we came from
namely learning applications so first we
get Edwin's theorem from 1988 that a
binary classifier from a class F for
instance possible OBC dimension can be
learned in at most log base 2 of the
size of your class equivalents queries
ok and that's nothing new we just
recovered something that's been known
for a long time for sorting I don't
think that had been anything yet so what
we showed in this paper is that a
ranking of n items can be used using it
learn to be able to log base 2 of n
factorial so that's order n log and
equivalence query queries if you get
insertion sort for bubbles or a few
thousand for both of those we can do
that you can also do it with non
adjacent swaps but it doesn't follow
from from this framework you have to do
a separate proof and we can actually see
you can actually get there all right is
that bubble so it runs well don't you
see you're not executing bubble sort
when you tell me your elements 4 &amp;amp; 5 are
out of order that doesn't force me to
next just swap those two and ask you
about that so then I will just be
running bubble sort and need n square
queries but when you do that I swamp
those two and I swap a whole bunch of
other things too and ask you another
query and so by doing that I can get it
down to n log n right that's cool
so you can also get a lower bound of
Omega n log and in this case it's not
just simply the standard counting
argument lower bound because the thing
that earlier on was annoying us that
there are many different answers we can
get rather than being able to ask about
one specific bit it's still annoying us
here now for the lower bound because
there's actually any different answers I
could possibly get and so that means I
get more than one bit of information on
each query so the standard just how many
bits of information do you get argument
doesn't work but we can explicitly
construct an adversary that forces you
to use half and half queries yeah so
probably also my hold your thought for
about eight slides because I do have a
couple slides on this this is actually a
really interesting question so right now
we're only optimizing queries this is a
big graph and we're doing a lot of
computations on it and yes this was a
question we asked ourselves that we have
some parts or results that I'm gonna
show you and so we can also recover
recovers of the main theorem from
elastins on this paper which sort of
says if each of the K clusters in the
actual grand truths are from some class
F then there's at most F to the K
clusters for against pink lobe we see
dimension then the clustering can be
learned using at most order K log F
equivalent squares so that falls out of
our directed graph and starts so I mean
we only really have one new result but
we kind of like the fact that it just
unifies to old results gives us one new
result for free it's sort of just a
clean framework of what's happened okay
so earlier the person over the speakers
asked deterministic and I said well I'll
tell you a little bit about what happens
when answers come in correct
so people who really make mistakes in
their feedback time so how well can we
recover from people making mistakes so
here's our noise model for each query
that you asked independently the coin
gets whipped with probability P which we
assume is strictly greater than 1/2 the
response is actually correct and follows
the rules that we had earlier with
probability 1 minus P the response is
adversarially incorrect and this is
independent of x they say no and you
probably
oh why don't you just do that so the
easy operon is repeat each of your owner
log and Ferries order log n times
that's enough that your probability
bounds are good enough that you can take
it Union bound over you log in
iterations for each of the queries just
take the majority mode that will be
correct by Chernov bonds and you get now
the correct result with high probability
so we went from log in to log n times
log log n times some term that depends
on and we didn't like that long walk we
wanted to get rid of it
so in some sense when you're doing
binary search you queried the middle
node and I tell you go to the left you
say well I'm not quite sure I trust this
answer because only correct 3/4 of the
time but you wouldn't query the middle
node you get you wouldn't you would not
go all the way to 1/4 now but maybe you
go to a third or 3/8 or something like
that and say I'll move in a little bit
but I'll hitch my bits in case the next
answer says that I need to go to the
right and so people have studied this
for just binary search on the line and
we now wanted to do everything noise
tolerantly in an arbitrary graph so the
main result with noise and this was the
one that the undergrad contributed to a
bit was that there is an algorithm with
the following property Skaven again a
weighted undirected graph and again some
of this generalizes to directed graphs
but it's unclear and some Delta greater
than 0 which is your error probability
in the final answer and you're again
given a candidate note said the
algorithm finds the target with
probability at least 1 minus Delta using
no more than this many queries so what
are the terms here we have a 1 minus
Delta over 1 minus the entropy of P so
the better your signaled the fewer
queries you need times the log of s so
this is sort of the term we had earlier
gets multiplied by this there's a little
o of log S term here and then we have a
term that depends on Delta it's the log
square of 1 over Delta we'd really like
to get rid of that square
we've tried quite a bit and so far not
succeeded but we still believe that it
should be possible to get rid of that
square this is matched even on the line
almost perfect completely by so there's
a lower bound on the line which is
essentially this dependence on on
yes is actually the information
theoretically optimal including the
constant you cannot get a better
constant here and this was by a paper by
Ben Oren has seen him from 2008 so they
basically did a did something very
similar for just binary search on the
line or on for path and they prove
matching lower bounds the one thing that
they can do differently on the line is
they get they don't have to square and
it's of their reasons in the analysis
why that square pops up and we don't see
an easy way to fix it but there may be a
not so easy way to fix its own sense go
again going back to our applications we
now get robust interactive learning
algorithms for the three applications
classifiers ranking customers
so basically anytime you can model it
that way you get the robust version for
free which we kind of liked for instance
that was something that wasn't in our
skin saw it's not it it might have been
harder to do that from first principles
then once you just have the framework in
place and a note that the algorithm
needs to know leave so this is a
question that s1 has gotten when he's
giving a talk on this so the algorithm
has to know what's the correct bus
probability features in a number of
places yeah the algorithm is actually
not that complicated the analysis has a
few subtleties in it um but it's
basically something a binary search
meets multiplicative weight update
algorithm so the algorithm keeps track
of node weights and those are exactly
likelihoods the probability then I
observe the responses that I saw
assuming that V is the target so every
node has a weight and that's its
likelihood of being the target given the
sequence of what we have seen and
initially inside s all of these r1 and
outside edge they're all 0 and then we
define the weighted median that's a node
V that minimizes the sum of distances
times the weight of that note so if
something has been ruled out it's a zero
earlier an otherwise it's a 1 now you
have sort of these fractional if a node
has very high likelihood then you should
have your median close to that node all
right and you just repeatedly query a
weighted median and update such that if
your if that node is consistent with the
answer you pointed in one direction that
node can be reached through this edge by
a shortest path you multiply its weight
with P if it's inconsistent then you
multiply it with one
so that it just stays the likelihoods so
let me illustrate that again on the
Peterson graph so here we have our graph
let's say we start again by querying
this one we pointed this way so now
these things that we ruled out earlier
just have a lower likelihood so they
become less likely the other three
become more like say we query this one
next we again get this response this one
will increase its likelihood but also
this one and this one will increase
their likelihood because the shortest
path to them goes through here for these
two the likelihood Goes Down and for
these here the likelihood goes down even
further so now let's say this one is the
median it's not always that the median
has the highest likelihood but in this
case it's true so we created just wanted
to say the answer is yes you guessed
correctly that it's also not completely
reliable but again it increases the
likelihood here and it decreases the
likelihood for everyone else and we just
keep doing that for a while so this is
really the core of our algorithm so for
the analysis here a few key observation
each operation iteration the targets
weight is multiplied and expectation by
key to the P times 1 minus P to the 1
minus P so because an expectation you
have P responses giving the correct
answer and 1 minus P response giving the
wrong one so P to the p8 says this is
the targets weight gets multiplied this
way and so long as the targets weight is
that most 1/2 the total weight decreases
by at least a factor of 2 in the whole
system three lines of calculations and
so this is like the standing kind of
analysis for multiplicative weights and
so after a large enough number of Ryland
the targets wage should be sharply
concentrated by eternal bounds around P
to the P 1 minus P to the 1 minus P to
the K power whereas unless the targets
weight at some point exceeded a half the
total weight is now 2 to the minus K
which is less than this so that's a
contradiction the target has more weight
than the entire graph so that means at
some point the targets weight must have
exceeded a house so at this point what
we would like to say is oh as soon as
the nodes weight exceeds a half that's
the target unfortunately you get a lot
of false positives that way because it
could be that after a future
enough bad things happening that some
other note that it's maybe very close to
the target actually gets out you just
needed a few wrong wrong answers so we
can't quite do that so what the whole
algorithm does instead is it runs in a
few rounds first we run the
multiplicative weight update binary
search algorithm for some number ten of
log and rounds carefully chosen constant
and we look at all the nodes whose
weight exceeded 1/2 at some point and we
keep all of those around one of those is
the target we're pretty sure but we
don't know which one so then we run
multiplicative weight update again only
on that with those as the starting
center for some number theta of log log
and off rounds now because that's how
many rounds we need given if we only
have log and nodes remaining at this
point and so now again we keep track of
all the nodes whose weight exceeded at
some point and we're saying those are
the only candidates it might still be
long log n of those the long log n is
not too many so we can query each of
them log log at times that's enough for
turn of balance to give us dead if you
ask an old log log n times are you the
target are you the target are you the
target if it is the target it needs to
tell us so at least a P fraction of the
time and so turn off balance give us
enough that only one of the nodes will
tell us the majority of the time I'm the
target and so now we can take a Union
bound over all of this tune some
constants deal with a few minor
difficulties and that basically gives
you than 0 so the know that the target
will survive all three phases and so
it's the one that will be returned
someone had a question so much of
egocentrism would be created with the
exponent the knowledge of but is just
sent as an exponent of the knowledge of
P so when you update your weights you
need to multiply with P and 1 minus P
and otherwise so you don't know what the
likelihoods are if you don't know what
to you so the algorithm really can't
quite and if you don't know what the
likelihoods are you can't compute the
weighted media so knowledge of P is very
essential
do something where you run it for a
bunch of candidate values of P you have
a lower bound and then you come up with
a fine enough discritization and run a
bunch of algorithms but I'm not sure
what happens if yeah I I wonder what
would happen if you have P plus Delta
instead of P whether you could sort of
show or that you find that you get close
enough that if you maybe put your
threshold at 0.3 instead of 0.5 that
sounds somehow something good would
happen we haven't done that calculations
of iteration okay so so L so this is the
main result from ours and so you asked
about how do you do this computationally
so let me tell you what I know so as you
probably observed as a motivation for
your question the graph is often large
and it's given implicitly right we're
not writing down all n factorial
permutations we just know that this is
what they are and so the question is can
we find the median efficiently in time
that's only poly logarithmic analysis so
to write your roots
yeah surely in general is impossible but
under what cases can we write that sort
of we were interested in them and so I
can give you a sufficient condition so
this is in our newspaper so we need a
few assumptions we need the assumption
that the maximum degree in the diameter
of the graph our poly logarithmic in the
number of nodes which they are for the
three examples like hypercube the degree
and the diameter of both log of nodes
and it's true for the other spot we also
need we assume so that's the key
assumption we need to assume that there
is an efficient sampling Oracle then it
returns a node V that's drawn with the
probability you're approximately
proportional to Mew ideally we would
like exactly to sample from you but so
long as we're reasonably close to Mew
that's good enough and so in that case
we can compute an approximate media
efficiently and poly logarithmic in the
size of the graph and the stable Hellman
can be made efficient so what we need a
sampling Oracle that samples from you so
then the question is for the examples
that I gave you when do these sampling
Oracle's exist
and we have a positive result for
ranking I think it's a really exciting
open question for the classifier and I
have no idea what to do for for the
clustering yeah so when you said that
you could if you could sample from you
then you can get the the median that's
yet from surveys we proved it from
scratch I don't think it would be I
don't know which survey result you're
referring to but basically what the
intuition for our algorithm is appalling
you start at some arbitrary node and say
well let's assume this is the media and
so now you sample all these others and
you see among all my neighbors is there
anyone who is actually doing better in
terms of some of distances to everything
that I sampled then the one I'm
currently thinking is the median and
then you move there and if again among
though that guy's neighbors you look and
so if anyone so you basically do some
kind of a hill climbing or descend on
some of distances to all the things you
sampled and that would that leaves you
sharp enough estimates that or for the
actual sum of weighted sum of distances
is that you can so I don't know what
Sergey is result is but so that's
basically the algorithm and there's
again a few subtleties but that's the
high-level idea
so for ranking there so then you can do
this is actually very cute so when you
have ranking and you get no errors your
answers to find a partial order on the
elements and the consistent rankings are
just linear extensions of that partial
order sum total order that's consistent
with a partial order and it turns out
that this is a very well known problem
and the math community linear extensions
can be sampled efficiently the first
paper that did so was cars on of cation
from 91 they defined a relatively
natural chain where you guys if you do
bubble sort moves so long as they're
they don't violate anything and then
there's a beautiful paper like the path
coupling paper by bubbly and Dyer it's a
gorgeous application of path coupling
that a slight variation of cars on/off
catch on you can analyze in basically
one page instead of 10 pages and then
Hueber mark kubera and 2006 showed how
to take the bubbly Dyer result basically
and instead of getting an approximate
Sam
from the Markov chain basically use some
extra tricks on top to sample exactly
from the linear extensions so without
errors you can implement interactive
sorting in time that is polynomial in
little n the number of elements which is
logarithmic at the size of the graph if
you do have errors your graph is not
necessarily a partial order and so now
what the question becomes some
interesting about how you quantify
things if I give you something I say
sample something that's as consistent as
possible with what I gave you and P is
really close to one so basically errors
are insanely unlikely 1 minus 1 over N
factorial so error probability 1 over N
factorial but I give you a graph that
has a handful of errors which should
never have happened but it did then if
we could sample even approximately 2
with the total variation this is 1/2 so
that's about as in approximate as
approximately can be from you we could
solve minimum feedback arc set so for
this value of P unless our P equals NP
you're not going to be able to sample
but of course you might say well for
this value of P I don't really need to
solve that problem because I will always
schedule in your extension so in that
case I'm good oh we don't know what
happens for smaller P like maybe for P
equals 0.8 the sound thing is possible
we spend several months on this and we
didn't really get anywhere it doesn't
seem trivial ok how are we doing we're
pretty close to out of time so I
mentioned at the beginning briefly the
problem finding a career strategy that
minimizes the worst case number of
queries for a particular graph I thought
it would be something optimization
question for undirected graphs our alibi
because there's at most log and queries
needed this can be solved in
quasi-polynomial time we don't know
whether it's an N n P or in Co NP we
don't know if there's a polynomial time
algorithm for it probably not because
the crossing polynomial time is needed
under the e th and the specific quasi
polynomial that we get out of our
analysis it's needed under the SE th so
unlikely
when you go to directed graphs or have
non-uniform query costs which we didn't
have before
problem becomes pspace-complete and when
you look at abashed urge where you get
to ask multiple nodes at once for
several rounds then it actually ends up
being complete in the polynomial
hierarchy so as you get to rasp for
seven rounds and each time you get to
ask some queries we can show that it's
whether we don't know actually
completeness we know that it's hard if
you say you can run seven rounds we know
it's hard for Sigma 15 and it is
contained in Sigma 19 so we believe is
probably in Sigma 17 but we still we
haven't found completely matching upper
and lower bound but it's hard for some
classes of polynomial hierarchy and it's
contained in two steps above that so we
kind of thought this was cute it was a
very natural question that sort of fell
somewhere in the middle of the
polynomial arity and then sort of the
application from the soda paper gonna do
very briefly only so there we want to
learn a hierarchical clustering and I'm
going to change the query model a little
bit so there's a ground truth binary
tree whose n leaves our elements think
of them as animals and the algorithm
should learn this tree which is of the
hierarchical clustering of these animals
and it gets to ask ordinal queries I get
to ask Erin what do you which one do you
think of shark lion and there are close
most similarly which pair of shark line
and bear is more similar than the third
and so I get to ask these kind of
queries repeatedly and by asking few of
these which would mean the most common
ancestor I want to recover this tree and
again using a few queries and again we
can assume that the answers to queries
can be incorrect and if so adversarially
with probability 1 minus P so basically
here we have two hierarchy the line and
the Puma are together dog is more
similar to them than either owls crowds
or dinos and so we might want to prove
in this way or you might want to group
them differently because you think a
dino is a dangerous animals maybe it
should be grouped with lion and Puma or
maybe you want to say dinos are extinct
so owls and crowds should be grouped
over here and the Dino should be far off
so I want to learn what clustering you
have in mind
a genetic and basically on our results
from the soda papers we give two
algorithms one that's based on a
adaptation of quicksort and one that's
based on an adaptation of insertion sort
that both use order law and lock in
queries and again there's produces lower
bodies well there is a lower bound from
other people already and the insertion
sort based algorithm use the some of the
other ideas so you built a hierarchy one
element at a time so I started with like
two elements in the hierarchy and I've
insert the third one correctly with
respect to those two and so on so for
each element I need to find its neighbor
in the tree at the time I'm inserting it
and that turns out to correspond exactly
to finding an unknown target in the tree
which is the unknown target is just a
neighbor and I want to do this with
these queries and it turns out that you
can simulate the previous type of query
with these ordinal queries so if I want
to know does this element belong in the
left subtree in the right subtree or
above a given node I take one element
from the left subtree of the one from
the right subtree of me and the element
X itself that I'm trying to insert and I
ask which of these three is most similar
if X and Excel are more similar you go
into the left subtree of x and x are
more simple you go in the right and if
XL and X are our most similar you go a
bomb and into another part of the tree
so you can simulate a vertex query
modulo a few subtleties and by then you
can exploit the known algorithm so we
take we use a modification of the
algorithm by finding a ragam an elegant
old file from 94 and so we can find the
node but I enough probability to handle
all there's taking advance etcetera so
summary I showed you a general framework
for interactive learning using these
equivalence careers of a generalization
of the model from the data England and
so the main result is that these models
can be learned if the user feedback can
be encoded in a weighted graph that has
the shortest path property means the
correct feedback corresponds first edges
of shortest paths and the results can be
extended to nearly undirected graph and
for a feedback with independent error so
nearly undirected short cycles and so
that gives us as a corollary in Korea
interactive learning algorithms for
classifiers ranking cell offsprings and
can be made computationally efficient if
you can sample efficiently from
likelihood distribution and the analysis
techniques are useful for learning a
hierarchy from ordinal queries and it
also can handle errors cultures boxes
open questions so there are some gaps I
mean the one in though how long a
hierarchy the one between the C log N
and C over log C times log in and
support some gaps that it would be nice
to close you might want more realistic
error models so if I ask you between a
lobster and lion and a tiger which one
all more similar we'll probably have a
lot of agreement in this room whereas if
I ask you between a Puma and a lion and
a tiger which two are more similar you
might they might not have as much
agreement so there might there some
queries when the answer is very close
which one is closest where you're much
more likely to see mistakes there's for
various when you're looking at a
classifier you just deploy it you might
get false positives people telling you
your classifier about it right because
they've never had to run it on the ones
where I have a mistake so you might get
a lot of false you've got the right node
answers but not a lot of false
Corrections so there's a lot of
questions about different error models
here as I mentioned one could try to
generalize it beyond graphs so for the
non adjacent transpositions we can get
the same result but not using
opportunities what are other learning
tasks where we could apply these
techniques and for the all the domains
that I gave you for which ones can we
get positive or negative sampling
results without errors I think that the
learning a classifier question is really
interesting I'm surprised no one has
done it with errors all of these seem to
become really really hard but they also
seem interesting so are there some nice
Markov chain arguments here and of
course sound being able to sample is
sufficient but not necessary too
yeah so we've spent some time so I maybe
I'll be attributing a slightly different
question to you then what do you attend
address that might be so
so in some sense whenever you have some
learning clustering whatever our problem
and you define an objective function
what you really want to do is learn the
thing that your user really wants or
needs or had in mind unfortunately don't
have a good way of telling you we define
an objective function you say that is
probably a good surrogate for what my
user really wanted but of course inputs
that most people know implicitly even
though it's not often stated in papers
that this objective function probably
doesn't capture 100% of what the user
one so then the question is how could I
minimize the user feedback I may be
using the objective function as an
additional piece of information I
already know weight of the objective
functions going maybe I can ask the user
a lot fewer queries and we've spent some
time on this we have some partial
results or some clustering question we
don't have the really beautiful result
yet some of these seem to become pretty
hard but maybe we're looking at there at
the wrong model of combining that you
but I think and I mean son Joey has said
goal would be to simplify the
competition about it by saying that it's
fine if I find that now you're finding
basically one target yeah I mean so I
was thinking about it the other way
around I was trying to
say I don't want to ask to ask user n
log inquiries if I can get most of it
right by just using some kind of a
decent objective function maybe I can
get it down to asking the user just long
and queries rather than n log inquiries
so that to me seems also an interesting
question and I think some joy has been
come pushing this new point that
posturing really should combine an
objective function and some user
interaction in order to really get what
the user wanted but without bothering us
or too much but I mean yeah the other
one you don't need to get exactly the
right one who on something that's close
and what's the notion of being close to
the correct one that might be an
objective yeah we haven't thought about
that first
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>