<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC 2016 - A Fully-Programmable Measurement-Feedback OPO Ising Machine with All-to-All Connectivity | Coder Coacher - Coaching Coders</title><meta content="AQC 2016 - A Fully-Programmable Measurement-Feedback OPO Ising Machine with All-to-All Connectivity - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AQC 2016 - A Fully-Programmable Measurement-Feedback OPO Ising Machine with All-to-All Connectivity</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C0CMydhdRbM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">first of all I'd like to thank the
organizers for giving us the opportunity
to talk about our work here as John
mentioned I'm going to talk about
something today that's not just
different from superconducting qubits
that in fact is not even a QC or even
quantum annealing so I'm sort of in the
wrong venue for this but we're actually
doing something that is very similar in
spirit to what everybody in this room
does so I hope you'll find what what
we're doing interesting even though it's
a little bit of a different approach to
the general problem so the the
high-level picture is that we're trying
to build a physical machine that can
find ground states of classical Ising
spin Hamiltonians just the same is
essentially everybody else here is
trying to do what we're trying to do it
in a way that's essentially completely
different from from adiabatic quantum
computing although it does have some
similarities which hopefully you'll get
to see as we go through the talk so this
the work that I'm going to present today
is primarily new work that has been done
with my main collaborator Stanford is a
Lear isomer on D and this is all in
essentially Yoshi I'm going to screw up
and I'm also co-advised by today I'm a
Butchie some of the preliminary results
that I'm gonna going to show you is
basically a lead up to what we what
we've recently done is from this paper
here in nature of tonics that early race
had published a couple of years ago and
I recommend that as a reference if you
want more of a sort of physics based
intuition for the mechanism that that
we're using in our in our system and
this benchmarking paper here from
Yoshitaka is also perhaps interesting to
some people and then I should also give
a large shout out to all my other
collaborators who have been helpful in
discussions and so on and Shuhei is
actually here and had a nice push to
last night so you can find him too if
you if you want and then of course the
funding comes from from the impact
program in Japan so first of all it's a
let's quickly get everybody trying to
get everybody on the same page I'm gonna
try and do this in a reasonable pace so
we can get through to the more
interesting things are the actual result
of on you machine that we've built but
just so that we have some sort of common
terminology and people aren't lost when
I talk about our POS ya guys
so our system is is we build everything
we do with with circled optical
parametric oscillators and an OPR is a a
funny sounding name but it's actually
not a very complicated thing so
basically if you have a nonlinear an
optical nonlinear material and you
design it things appropriately you can
pump it with lattice it's at this blue
frequency here and you can put in a
signal beam at this green frequency and
this device will act as an amplifier and
transfer power from the blue pump
photons into both the green and another
color red photons it acts as an
amplifier and just like when you build
the laser if you take a gain medium and
you put it in a cavity you can take such
an OPA you can put it inside a cavity
and form what's called an opium and for
our purposes all that matters of the
following things one is that you can
think about what's happening here is
when you pump the system with these this
blue light it basically the system
basically takes one photon of that blue
light and converts it into two photons
at half the frequency and in our case we
operate in the circle degenerate regime
where really it is it is half the
frequency you can design it so that it's
different frequencies and this system
has a threshold just like a laser has a
threshold so as you as you pump it
initially you don't get much output
light for the given pump input but at
some point there's a a large kink and
you you suddenly see a lot more light
coming out and the third property is one
that differs from lasers and this is
basically the reason we use our POS and
it has to do with phase property of the
light so in a pure below threshold if
you look at the in-phase and quadrature
plot of the light that comes out below
threshold you generate squeezed vacuum
but if you go above threshold you end up
generating squeezed coherent states and
if you're sufficiently far above
threshold it really just looks like
coherent States and the interesting
feature of this is that you always get a
coherent state that's either in phase or
exactly out of phase so PI phase with
respect to the pump this is something
that's very different from a laser laser
you can get a coherent state that lies
anywhere on this circle and this is the
reason that we lack or Pio is because
this gives us a very natural binary
variable the phase of our pulses and
it's so if you think about it in spin
terms the the zero phase is our spin up
and the PI phase is I'll spin down and
I'm an experimentalist so this this PRL
showed that this is what happens in
theory here's some experimental data
actually showing this happening in real
life where early Raisa did an experiment
basically showing that if you measure a
success of opium pulses you either get
this high value out of the from the
homodyne measurement or this low value
out corresponding to the zero and the PI
phases and you see you never get
anything in between so that verifies
that we really do have this binary phase
property so one way that you could try
and build a system that uses a lot of
different opioids as as spins is you
could make a lot of different cavities
physically spatially separated with many
different nonlinear crystals but that's
expensive and not really necessary so
the approach we take is using time
multiplexing so we use pulse tokyo's and
a pulse to appear operates as follows so
you if you see in this diagram here we
have these formers that comprise the
cavity and inside we have this little
nonlinear crystal and you can design
this so that this cavity can hold just
one pulse so if this laser here has a
repetition rate which spits out pulses
every whatever the period is one
nanosecond of 10 nanoseconds for example
and you can design the cavity so that it
holds exactly one pulse at a time but
you don't have to do that this is the
normal way that people who bill post
post appears do things but you don't
have to do that you can make the pulse
the cavity longer
and you can have the cavity actually
stole multiple pulses at the same time
and in the first initial sort of proof
of principle experience you did this was
with with four pulses so at the moment
if you just have this you have
hamiltonian h equals zero
you just have four random bits
essentially for random spins how do you
how do you introduce coupling it's the
way we do this is by introducing in this
case beam splitter in the in the cavity
and a delay line and this delay is set
to be in this case exactly the delay
between two falses so you're taking some
light from for example the first pulse
and you're injecting it into the second
pulse and depending on whether it's in
phase or out of phase when it gets back
to this thing locked
I see you'll have destructive or
constructive interference and you can
add more delay lines and introduce
couplings between not just nearest
neighbors but next nearest neighbors and
and so on and in the case of this in
equals four experiment that what was
implemented was basically couplings
between all four spins in the
experimental results if you don't have
the coupling so on you see that the your
outcome is just random every every state
every possible configuration of spins is
just equally lucky but when you turn the
couplings on only the ones that
correspond to the solution of this icing
problem which we always consider as
maxcut problems there's a direct mapping
between Mac's Kaduna and the Ising model
and we always see that we get one of
these ground states of this maxcut
problem so that's nice but how do you
how do you scale this thing up so one
one thing you can consider doing is
instead of using a free space cavity
using a fiber cavity and in that way we
can make a very long cavity so that's
what's depicted here
we then need if we follow the same
scheme in -1 delay lines if we want to
have an in spin system and because we
don't necessarily want to have all the
couplings on all the time we need to put
modulators in each delay line this has a
number of problems one of them is cost
delay lines or maybe not that expensive
but the modulator czar not cheap and
then you have to face stabilize the
entire system and you also are eating up
your power that's inside the cavity by
splitting it up so many times so then
you have to introduce an amplifier and
this introduces additional problems so
this is more or less where I left off at
the end of a QC last year and I told you
we were going to try and work on a
machine that tries to alleviate these
problems and so for the last year and a
half or so we've been working on such a
machine and I'm going to tell you what
it is now and give you a little bit of a
hint of of heart deforms so the basic
idea is that instead of having this
large network of delay lines we are
going to replace that network with a
measurement feedback scheme so we have
the same setup here of it's a fiber ring
cavity and there's a people in waveguide
and B going to pump this thing again I
have pulses in the cavity and then in
our case at the moment we have 160
pulses in the cavity it's which is about
320 meters long but instead of having
this delay line network what we have is
a beam splitter that taps out a little
bit of the power and we measure the
phase of each pulse using this balanced
homodyne setup here we feed that into an
FPGA and then we get the FPGA to compute
what would have happened essentially if
we had that delay line network and then
we inject life back into the cavity that
is modulated by these two intensity and
phase modulator is driven by the FPGA so
what is the FPGA actually doing well the
calculation is doing it's not
particularly complicated it's
essentially if for every pulse it needs
to compute this vector vector dot
product so that's really all there is to
it so just to make it a little bit more
concrete
what we do is in one round-trip of the
cavity there are a hundred and sixty
pulses in there we actually only use 100
of them these are the ones marked in red
here each of these has a phase that we
measure is the c1 c2 etc in the FPGA we
compute this vector vector dot product
and then when the next pulse one comes
along
we feedback with this vector vector dot
product that's being computed and so on
for the next one so our information so
every clock cycle what we call what I
call a clock cycle the separation
between optical pulses every clock cycle
we have to compute this this this little
sum but that's and it's doing it based
on the previous round trip of all phases
so it's a little bit different from the
the fully free space fully coherent
system because it's not using the the
phases as they are right now it's using
the ones from the previous round trip
but that's the that apparently seems to
be a and not an unimportant difference
at least from what we've been able to
tell so far so just to make it clear
that we're actually not just drawing
pretty cartoons and we're actually an
experimental group here's an
experimental diagram of what the system
actually looks like the bit in the top
right is the the part that I just
discussed the rest is largely the to do
with phase stabilizing the entire system
a large part of the effort that goes
into this is basically making sure that
it that all the phases stay stable and
we unfortunately have some lag of people
in our building we had to consult with
to find out how you stabilize a 300
meter
I think the fiber - let's call it a
hundred nanometers but probably we
weren't better than that and so there
was a there was a big chunk of the
effort what it actually looks like and
in reality is this this shot shows a
subset of the table where there's this
little fiber there's a people in
waveguide and and some lasers and some
delay lines and so on it's it's just
like any other optics experiment if you
just look at the table it looks like a
mess but there you can at least believe
that we have something so now if you're
if you're falling asleep because there
was too much physics to do much
experimental detail now you can wake up
because
there will be no more now we just have
the computation results so the first
thing we tried on the machine was doing
max cut and incidentally everything I
will talk about from now on will be also
be max cut on a very simple graph it's
this 16 vertex ring in spoke graph I
think officially mathematicians call it
a mobius ladder but bring in this book
so what happens when you when you ran
this on the machine so as a function of
the computing time or the round-trip so
every time the pulses go around the
cavity we measure them and we keep track
of it so we can watch what's going on we
see that it starts off looking just
randomly randomly jumping around we're
not we're not improving and then as the
feet we turn this feedback on as we turn
it on we see it start to drive the
system into a lower and lower energy
State so closer and closer to the
maximum cut until it eventually reaches
the maximum cut and it just stays there
so at least for this graph and this run
it worked which is great I was a little
surprised to even see it working with
the first time I turned it on because
you spent a year and a half building
something and doing all the FPGA
designing and calibrating all the delays
and then you turn it on is like what is
it going to do
ok good it did something did something
reasonable so if we if we run this again
that doesn't do it again is this just a
lucky shot
and in the case of this problem the
answer is definitely no that this
problem is actually a very nice one to
run with because it's very robust to
almost all experimental parameters that
we have and we almost always get 100%
success probability with this particular
problem instance is this particular
problem instance a lucky shot well here
are a couple of other examples that have
chosen essentially at random so this
first one is the one that you saw before
here's another graph that ended up
having 58 percent success probability
and the interesting thing that I took
away from this histogram of energies is
that even though we don't get a hundred
percent success problem to you we still
get something that's what I consider
high you just run this a couple
then you have with high probability the
right answer but more importantly I see
that when we don't get the exact round
state we get something that's close in
energy to it and the same is true for
this graph see here which had even lower
overall success probability so are these
are these problems special data points
that I've just picked out and the answer
to that is again no met the max cut
problem has a nice feature that on cubic
graphs it remains np-hard and cubic
graphs are nice at least when you have
16 vertex vertices because you can
enumerate all of them and they're not
that many of them there are 4060 cubic
graphs at 16 vertices and we we ran
every single one of them on our machine
and this histogram shows that we were
every single problem instance we were
able to solve every instance with the
probability of at least 20% so at least
for small instances so N equals 16 the
Machine doesn't just solve special cases
so it can solve everything that every
every cubic graph so then of course
that's nice it's a good toy but what
happens when you when you scale up and I
think pretty much everybody in this room
is going to be able to guess with at
least one feature of of the next plot is
so this is the scaling plot and for now
I encourage you to ignore all the colors
besides blue well that's the thing that
I guess most people would be able to
predictive as you scale this up with
cubic graphs you see as you get larger
and larger the success probability drops
exponentially so in some sense this is
encouraging because it means that my
analysis code is not broken because if I
saw this thing just being constant I
would be very suspicious that that
something was wrong
the main encouraging feature I take from
this graph is from the colors that are
not blue and what these mean of the
following is that I'm working with
undirected and unweighted graphs and so
there's a very nice metric for how good
a solution is so you know exactly what
we know what the ground state is
and it has some number of cuts that's
the maximum cut for that for that graph
and we can because it's unweighted we
can evaluate something any given
solution and say is it within X percent
of that maximum cut and that's what each
of these different colors means so for
example a brown is did you get a cut
that's within ninety percent of the of
the maximum cut and the most interesting
thing to me in this plot is showing that
nine ninety percent basically stays
unchanged as you scale so within this
range we don't see a significant
significant change in the success
probability to get a ninety percent
solution now is that interesting is that
useful I don't know but at least I found
it interesting I can't yet answer if
it's useful the other thing that it can
point out is that the even the ninety
five ninety four percent are not
dropping that significantly and of
course we can turn this graph into a
runtime graph by just saying how many
times do you have to repeat things and
you see the same story here that on this
now log plot on the y scale to get an
exact solution you have an exponential
increase and the exponent is quite large
in the sense that if I that if we
extrapolate this out to N equals a
thousand or two thousand I can already
see that the runtime is going to start
becoming very significant in terms of
seconds minutes hours
however for 95% certainty for ninety
percent accuracy it seems like we can
extrapolate this pretty far and and hope
that such a large machine will actually
be able to still solve problems
approximately in a reasonable fashion so
so much for cubic graphs keep it graphs
are nice because of this NP hardness
property but who cares about them in
practice the the I guess for most things
people are doing the graph that you look
at it looks either kind of random what's
definitely not cubic so here's an
example of us trying something that's a
random graph here is a 100 vertices 495
edges or 990 depending on how you count
and this is a plot of showing
the dynamics where each one of these
lines is one of the opioids and you see
the homodyne measurement output for each
of them and you see it crossing between
these various bands there's some very
interesting structure and if we do the
same plot as before showing what is the
evolution of black as you drive this it
goes it solves the problem in a hundred
microseconds in this run this one it
doesn't have perfect success probability
but it's about 5% for that particular
graph and we can analyze what happens as
you change the density of the graph and
the rough result we get is that we can
more or less solve graphs of any density
things get a little tricky when you get
out to extremely dense graphs where we
think that's actually an artifact of the
other Soumya of the experimental
parameters that we're using and I have
some simulation results in suggest that
we can basically fix that as well
so in summary we we are building and
have built initial experimental systems
where the purpose is to use cutting-edge
works of couple doppio is to try and
physically emulate and find ground
States or approximate grounds that
approximate solutions of classical Ising
spin hamiltonians we have a hundred spin
work a hundred spin system working I
should emphasize that this system has
also all connectivity because of the way
we've designed it so we actually have
10,000 spin spin connections right now
we can find in some cases exact
solutions in some cases just approximate
solutions and this time division
multiplexing and measurement feedback
techniques that we've we've developed
for this we don't have any solid ideas
for how they may be portable to a QC but
at least in this community I think maybe
somebody has a good idea for how you can
try and use these techniques in for
example a photonic aqc I'm not sure if
any of these those ideas are a little
baby far-fetched but I thought I'd throw
it out there something for people to
think about so with that thank you very
much
thanks a lot for the very interesting
tack I was a little bit curious about
the readout system this device because
to me it's a little bit vague it might
be a very stupid question but to me it
seems that you have actually a
continuous system and then you kind of
project it to a discrete system at the
end when you are doing the readout so
you are actually solving something that
looks like an icing system but in the
continuous space which and then it's
being projected so it would be great if
you could elaborate on how this process
works and then how is the readout so
that's that's exactly correct that
essentially the system is continuous
until at the end we basically make a
projection we just say what is the zero
point if it's above then it's been up
and if it's below and it's been down and
we could imagine for example in the
measurement feedback process at each
step we're reading reading the phases
and and computing these feedback terms
we could imagine doing the projection
there as well it turns out we don't
actually do that we actually use the
amplitude information during the
computation it's still at this point not
actually clear what is the best way to
do it this is this is the first sort of
brand of big machine we've built and we
just we had to make some design
decisions and we made them and we saw it
happen but there are a ton of different
ways to do things and it's absolutely
not showing rigorously which ways are
best so hopefully there'll be more
interest in the signage system going
forward and we'll have some theoretical
help to try and work out really what is
the best way to to do things I was
wondering if there is any way to
characterize the runtime of your system
lake in the traditional adiabatic model
the inverse-square the spectral minimum
spectral gap tells you you know how fast
it runs so in this case is there a way
to tell what scaling is like we don't
have any good and
results like this I think there's some
intuition that well it's not really a
minimum gap during an annealing
equivalent but there's a minimum gap at
the fat of the final state and that will
have some bearing on and the sort of
density of states or these six of these
first excited states and now will have
some bearing on on our success
probability but the connection between
that and run time is something that we
haven't we don't have good analytic
results for it almost everything that we
have at them over as numerix thanks well
I'm impressed to see that you can find
the ground states of hundred cubit
hamiltonians I wonder would your would
your system be able to find the ground
states of Hamiltonians that have multi
cubit interactions so at the moment we
we've only implemented these two two
cubed M interactions and then same on
terms but in but in principle we can
change the FPGA design to implement for
example three body for body etc
interactions there's a large technical
challenge there because then you're not
just doing a vector vector dot product
game where you have to do a matrix
vector and and higher and higher so it
becomes computationally very demanding
but in principle as possible does that
mean that the overhead is only in the
classical computation that supplements
the experiments you're doing that's my
best guess is that if I had to implement
that the first thing I'd be worried
about is FPGA what happens after that
who knows what roadblocks you'd run in
so we haven't thought about it very
carefully but I think the most immediate
when I get roadblock I can think of is
the FPGA great thank you thank you I
wanted to ask a question it seems that
with the pickoff of the classical light
signal and the amplification and kind of
a classical feedback circuit what you're
really doing here is building a mean
field solver and that if that's the case
a really interesting thing to do would
take these problems and solve them with
a conventional numerical means hill
field solver and see if there's any
difference that to me is the real
benchmarking which was done on the
d-wave machine 4.com
anything right and that would be good
and then my prediction is that that
probably would happen if you see pretty
close and that you could make a much
faster system a much cheaper system by
getting rid of all the electronics
except for the FPGA and just solve it in
there and then it'll be even faster and
more reliable and you can do anything
you want so that's my prediction mm-hmm
so this this thought is not really
escaped our attention you and I first
started working on this project I
started trying to understand what really
is the optics doing what versus the FPGA
and can we just throw away the the
people in waveguide and just make the
FPGA do everything and it's still an
open question we've done a lot of work
on simulations and trying to understand
what's really going on what's really
important that we still don't know I
think your point is completely but the
literature on testing the d-wave kind of
went down this road right who do it and
that seems like the next thing to do
yeah that's absolutely next steps for us
is basically we don't have the expertise
internally to come up with all sorts of
different models and tests but we're
very happy to work with people in the
community and we would love to sort of
engage and try and do the same kind of
systematic testing now that we've got
the initial physical system sort of
basically going
I think the optimal algorithm for
approximating maxcut on a classical
computer is this go men's Williamson
algorithm based on semi definite
programming right and it achieves an
approximation ratio of something like
88% for general graphs assuming
something called the unique games
conjecture and like 92 or 94 percent for
three regular graphs and it has some
similarity with what you're talking
about because it's it's based on a
relaxation from binary bit values to
real values that you then trend came
back two bits at the end have you done
some kind of comparison between you know
correlations right yes so Grumman's
williamson is basically I was our
initial benchmark during the simulation
studies and
right to this paper here by Yoshitaka
has a large set of benchmarks in
numerical simulations of our system
versus both SDP the SDP relaxation based
methods as well as other heuristic
methods for solving Mac's guide it it
turns out that humans Williamson is
maybe a little bit it's a little bit
unfair for us to compare ourselves to
them because they give a performance
guarantee on any graph whereas we don't
give any guarantee we just heuristic lis
can say well if some benchmarking set we
can do well but compared to two SDP we
we do better but it's it's not really a
fair comparison really the thing we
should really benchmarking in some
things like simulated annealing all the
other range of annealing methods as well
as branch-and-bound methods and so on
there's the large literature from
operations research people that we
compare ourselves to as well okay thanks
okay thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>