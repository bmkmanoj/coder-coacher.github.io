<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Capacitated Covering, Scheduling to Minimize Energy and Min Edge Cost Flows - A Natural Convergence | Coder Coacher - Coaching Coders</title><meta content="Capacitated Covering, Scheduling to Minimize Energy and Min Edge Cost Flows - A Natural Convergence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Capacitated Covering, Scheduling to Minimize Energy and Min Edge Cost Flows - A Natural Convergence</b></h2><h5 class="post__date">2013-03-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sgBrc5y4n98" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">
SAMIR KHULLER: Thanks, Han.
It's indeed a pleasure to be
back here after many years.
So I'd like to try to keep the
short talk to about 45 minutes
and give you an overview of some
of the research that my
students and myself have been
involved in for the few years.
And as she mentioned,
last summer I
became department chair.
And I learned several
philosophical lessons in life.
As a PhD student, as [INAUDIBLE]
said one day, you
know more and more about
less and less.
As a chair of the department,
you know less and less about
more and more.
It's an interesting thing.
Anyway, so let me talk.
Some of the research started
a long time ago.
And what it is I've tried to
cover, three different topics
and show you how they're
tied together.
And in my mind, this convergence
happened only much
later in our-- initially, when
we started seeing the common
lens in which to view all
of these problems.
And so I think once you have
this viewpoint there are still
more interesting things to do.
So let me start off with a very
basic question that some
of you might have seen before.
This is covered, typically, in
courses on scheduling theory
or courses on approximation
algorithms.
It's a very, very basic and
simple model which is
something known as the Unrelated
Parallel Machine
Scheduling problem.
So in this problem, basically
you have a
collection of machines.
And you have a collection
of jobs.
And the goal of this is about
finding a good schedule for
these jobs on the machines.
Now, these machines
are not identical.
So there is a matrix, P
matrix, which is the
processing time of jobs
here on machine i.
So there is a Pij value that
shows how long it will take to
run jobs here on machine i.
And you could even say Pij is
infinity, if you want to
disallow a job potentially
being done on a machine.
And the goal is to somehow
assign all these jobs to
machines and also keep
the maximum load
on any machine low.
That's the goal.
So for example, you might have a
job, J1, which has different
running times on different
machines.
So it takes running time 2 on
machine M1, running time 4 on
machine M2, and can't be
done on M3, and so on.
And so the question is we
want to assign this job.
And we don't want to assign just
one job, but we actually
want to come up with
an assignment of
all of these jobs.
Now, we're going to see trying
to somehow bound how big the
load on the machine might be.
So this, for example, is a
schedule of makespan 4.
And then, here is another
schedule for this collection
of jobs with the type
of makespan.
So we just have to come up
with the assignment.
So this problem has been studied
for over two decades.
The first people that actually
delivered some interesting
approximation algorithms
for this problem-- not
surprisingly, it's NP-hard--
was a classic paper by Lenstra,
Shmoys, and Tardos,
going back to 1990.
But this actually showed that
you can formulate this problem
as an integer program, look at
a fractional relaxation.
So rather than trying to find an
integer algorithm solution,
you look for a fraction
algorithm solution.
And there's a very simple,
elegant rounding technique
that they came up with, which
shows you that the maximum
load on any machine is, at most,
2T, assuming there is a
solution where every machine
has load M plus 2.
So this is something that you
might be familiar with.
And in the same paper, they
also showed there is a 1.5
lowerbound and approximation
guarantee on this problem.
So there's still a gap here,
which is that maybe we can
improve the approximation
guarantee to 1.5 or better.
And a few years later in a solo
paper in '93, Shmoys and
Tardos made a further
generalization of this problem
where, along with the job that
we assign to a machine,
there's also an assignment cost
cij, which is cost that
you pay to put a job on
a particular machine.
And then you have some global
budget, C, on how much money
you are willing to spend
for this assignment.
And you want to do
the same thing.
You want to get a assignment
which has a very low makespan
value for all the work done.
And then, they showed that
even that problem, this
generalization is a true
approximation of the makespan.
But you don't violate the
cost budget at all.
So if there's a schedule where
the maximum load on an machine
is T and the total cost of the
assignment is C, you also find
an assignment where the cost is
at most C. And the maximum
load is M plus 2.
So this is a classic problem.
And some of the work that we are
doing has to do with the
generalization of
this question.
So here's one generalization
that motivated us to think
about the following question.
So what if machines
are not free?
So these machines, you think
of them as some spectrum of
choices that you have
available to you.
But every machine has a cost.
So you have to pay some money
to buy the machine.
Either you're building
a data center.
And you want to decide which
sort of machines to build to
do your work more efficiently.
So there's a cost.
Or the cost can simply be
use as the energy cost
to activate a machine.
The other machines are idle.
So there now are two or three
different things going on.
One is that you have to assign
these jobs to a machine, same
problem as before.
But also, every machine
has a cost to it.
So you have a bunch of
different options.
And you want to decide
which machine to buy.
So formally, the question that
we arrive at is, you have
machines with some cost.
And then, I give you a cost
budget on how much money I'm
willing to spend to
buy machines.
And now, we'll select some
subset of machines with the
properties that cost doesn't
violate the budget.
And then, once I choose
the machines, I'm
back to the same problem.
I want a fixed set
of machines.
And I want to somehow assign
these jobs to the machines.
Does this question make sense?

AUDIENCE: The cost
Cj is I'm buying.
Am I?
SAMIR KHULLER: The cost now
is I'm buying the machine.
AUDIENCE: Did you mean Ci?
SAMIR KHULLER: No, just
Cj, it's the fixed
cost to buy a machine.
So once you buy a machine, then
you can allow it to load
the machine.
The machines that you
don't buy, their
load has to be zero.
You cannot assign any job yet.
So now this should also
perhaps remind
you of another problem.
So not only does this generalize
the problem I
talked about.
If you have an unbounded budget
of the cost, then it's
exactly the problem that we
had before, but it also
generalizes another
classical problem,
known as the set cover.
So a quick recap of what is set
cover-- so set cover is a
very elementary problem
where you have sets.
These are shown despite what
I picture of where
this goes on the right.
And you have some element.
And then, every set has a cost,
which we have to pay to
acquire the set.
And now, the goal is that,
again, we have to buy some
collection of sets.
And then, the edges now
correspond to a membership of
a job in the set.
So I can set that
Pij value is 1.
And infinity is the elemental
value of the set.
And now we went to choose some
collection of sets and have
the problem where we can assign
all of these jobs for
the chosen sets.
But now, I find I don't even
constrain what the load of the
machine is.
So it's unbounded load.
And that's, again, set cover
problem, because the budget
that you spend on buying it is
excepted in the cost solution.
AUDIENCE: Actually,
going back, to
the model for a second.
So the cost depends upon--
can you go back?
SAMIR KHULLER: Sure.
AUDIENCE: So I think the
confusion was there is C--
SAMIR KHULLER: No, so it's
not the Gap model.
AUDIENCE: No, is it
Cj for machine i?
Or is it Ci for machine?
SAMIR KHULLER: Oh, sorry.
It's for the machine.
AUDIENCE: I think that's Cj
on machine, so that--
SAMIR KHULLER: Oh, I see.
This is actually Ci.
I'm sorry.
Thanks for your point.
It's Ci for this, thanks.
The cost is exactly
on the machine.
Yeah, I understand.
So again, this is a very
classical problem we've
studied since the early '70s.
And it's a very simple greedy
algorithm that gives you all
current log n approximation.
And those were studied for the
unweighted case where every
set is cost 1.
And there's an easy
generalization for the
weighted case where, instead of
just rereading the largest
sets, or the sets that have
the largest number of
uncovered elements, you can look
at the ratio cost of--
how many more elements
can I cover complete
with the cost of set?
And you can still get a log n
approximation for this NP-hard
problem that way.
And what is interesting is
that, even though the
approximation algorithm is
extremely simple, there is
also a matching lowerbound, in
some sense which proves that,
there would be a certain
complexity class collapse, if
there was an approximation
algorithm with factor
[INAUDIBLE] log n.
So there's some evidence that
log n may be the right answer
to this problem.
So now that you've seen these
two problems, we're back to
question of generalizing the
unrelated parallel machine for
this machine activation
question.
So the main reason that we were
able to show this sort of
paper in 2010 was that, if there
is a solution where you
pay cost at most C to buy some
sort of machines with maximum
load T, then we can actually
find a solution cost log n
times the budget C and with
maximum load [INAUDIBLE]
plus 2.
And in some sense this log
n term is unavoidable
due to the set cover.
The thing is, this is
unavoidable due to the machine
minimization problem,
the UPM part.
So the paper actually
has a bunch of
separate different results.
And let me talk about one of the
very simplest algorithms.
So we have two basic
approaches.
One of them is based on an
NP-routing approach, which
gives you slightly worse
approximation guarantees.
But the algorithm
is much faster.
So it solves a single
linear program.
And then there's an anonymous
routing of a linear program.
And it shows that you can get
some nice behavior with those
properties.
But the bound is just
like it was.
So what I'm going to show
you today is a much
simpler greedy algorithm.
And the bounds will
stay just like it.
But the algorithm is not going
to be nearly as fast as the
ones in the single
linear program.
So it's a greedy approach
in some sense.
What I'm saying is that the way
to think about the problem
is to think about the Set Cover
Greedy Algorithm, which
says, at every step, I want
to somehow choose
one set or one machine.
And then I can ask the question,
how beneficial is
this choice.
How much more can I do if
I buy this machine?
And again, you get the ratio
costs of how much more you can
do with a certain machine, with
the cost of the machine,
and then greedily try to
choose the set machine.
So that's the basic idea
in this approach.
Sorry, is there a question?
So now, suppose I have already
chosen some set
of machines to open.
So that's the set S, with the
property that I don't want to
assign more than T units
of processing
lanes to that machine.
So this second set, I'll think
about as some integer program
or a linear program to capture
that this way.
And I may want to now decide
which new machine to open.
So every machine I open has a
cost and maybe some more jobs
that I might be able to apply.
But the problem isn't
this problem.
It's NP-hard, even to find one
assignment, let alone find
what is the value of
the new machine.
So what is the incremental
benefit of opening a new
machine, Mi?
So that's an NP-hard problem.
But what we're going to do is,
we're going to relax the
function fS to simply take off
a fractional schedule.
So we can write a certain LP,
which will define some
variable which tells me how much
of a job is processed.
And then, we can use that LP
as an oracle to tell us how
much more fractional jobs will
we be able to process, if I
open this new machine.
So I might have 10,000 jobs
that I want to do.
I might say, I'll open
these four machines.
They have a budget of
T on each machine.
And then, I want to write a
certain linear program that
says, fractionally, how
much T jobs can we do.
We want to maximize that for
this set of machines, even if
that strains the load.
And then, for every other open
machine, we can ask the
question-- how much
is the benefit of
opening a certain machine?
And how many more fractional
jobs would I be able to
assign, if I open this
new machine?
And then, at every step,
we've just again
got to use the ratio.
So it turns out that you can
use the classical result of
Wolsey on the advantages of
the scheduled greedy field
work, using submodular functions
that just gives you
a log n approximation of the
same [INAUDIBLE], except that
here it takes a little extra
work to prove that this
function is submodular.
And that was shown by
Lisa Fleischer.
And there's a simpler proof
in my work with Jian Li.
So that is the basic
greedy approach.
So we start off with an NP
set of machines and then
incrementally, until I have
assigned fractionally at least
the n minus 1 units of work,
where n is the number of jobs
that I want to do.
At every step, we incrementally
choose a machine
with best ratio cost to keep
going, until, fractionally a
least, n minus 1 jobs
are scheduled.
And these are all being done
with the assumption the load
on any machine can never
cross this budget of T.
And in the end, we've got to
use the technique by Shmoys
and Tardos, which is a technique
for converting
fractional schedules to
regular schedules.
And then, you create a factor
of 2 in that conversion.
So you get this nice log
n bound of the cost.
And you get the bound
of 2T on the base.
So that's the basic idea.
But this algorithm is not very
practical, as I pointed out.
Because if you think about how
you're going to implement this
algorithm, at every step you
to solve a separate linear
program, just to choose
one new machine.
So eventually, in trying to do
some experiments related to
this that I might mention at the
end of the talk, we found
that this approach is completely
impractical, even
for medium-sized exercises.
It's too slow to solve.
And there are various heuristics
and tricks
that you can do.
This kind of question shows up,
for example, in cases of
social networks where people are
trying to do the spread of
influence maximization.
And again, there's this greedy
step, which has to do this
incremental computation
every step.
It makes it slow.
And then a lot of researchers
in the AI community, for
example, have looked at
heuristics and speeding up
this computation.
So a lot of those ideas might
be applicable here to
eventually give a faster
algorithm.
But the algorithm around a
singular linear program is
certainly fast.
So I already mentioned
this result.
And I also wanted to say that
there's a related problem,
which is a related set.
So this is a model where every
job has a processing time, PI.
And every machine has
a certain speed.
And the processing time of the
job just depends on the speed
of the machine.
So if there's a machine twice as
fast, you can do the job in
half the time.
So in that problem there, all
these machines are related.
But in some sense it's a simpler
model, because it
allows you to run any job on any
machine, as this machine
on the street.
So in that problem, you
can do much better.
You can get a Polynomial Time
Approximation Scheme, for
example, the lowest cost shown
by Hochbaum and Shmoy in the
'80s, and then subsequently
the simpler proof by
[INAUDIBLE], which also
gives a polynomial
approximation scheme.
And we were able to extend that
same style framework to
get a PTAS for the [INAUDIBLE],
with a budget on
the cost that you want to spend
on buying machines.
So that concludes the first part
of the talk that I wanted
to talk about.
So our motivation was
sort of two-fold.
One was looking at the energy
cost of buying machines and
machines are not free, or just
the energy cost or the actual
cost of deciding, I want to do
this computation here or this
computation here.
And different vendors might
charge you a different amount.
So let us move gears and talk
about a completely unrelated
problem, which is something that
we studied a decade ago.
AUDIENCE: Is T known, in
the previous case?
SAMIR KHULLER: So we kind of
assume that, if there's a
schedule, this is budget T. So
in some sense, maybe we only
start with a P. We have to guess
at T. And then, if it
says, infeasible, then we know
that no solution existed with
that [INAUDIBLE], for example,
even if all the
machines were working.
AUDIENCE: But you can
do binary searches?
SAMIR KHULLER: Yes, sir.
So there is this parameter T,
which has to be guessed.
And that's there in all of the
previous literature as well.
So I'm going to go back to,
again, a very simple problem.
And the motivation for looking
at this question on Vertex
Cover with Soft Capacities looks
completely different.
So our motivation for starting
this problem had to do some
biological applications.
So let me briefly mention what
is the Vertex Cover problem
and then talk about this
generalization.
So a Vertex Cover problem is a
very simple graph problem.
So forget what's written
up there.
So you have a graph.
And you want the vertex to cover
all the edges of the
graph somehow.
So I want to pick some nodes
that cover all the edges.
So that's the Vertex
Cover problem.
So for example, in that graph,
if I pick the blue nodes, I
should be able to cover
all the edges.
It's just a covering problem.
So a very simple covering
problem, which is NP-hard.
There are lots of different ways
of solving it, lots of
dual approximations, and
so on and so forth.
So we want to close
in on a couple of
generalization problems.
One is that, of course, the
nodes have [INAUDIBLE], which
is therefore the standard Vertex
Cover problem as well.
But in addition, we
have a capacity
constraint for every vertex.
So if I pick a vertex, the kv
of the vertex might be 500.
But if its capacity is 100, then
if I pick one copy of the
node, I can only cover some
subset of 100 edges incident
on that node.
I cannot cover all
of the edges.
But we can still pick a second
copy of the node and cover 100
more edges, and so on.
But you have to pay for the
second copy of the machine.
So Refeal Hassin was [INAUDIBLE]
one time.
And his student, Kanat Or, was
working in a biotech company.
And there was actually a pretty
good application there,
which had to do with testing
the presence of some
substances in a chemical.
And they could do
certain tests.
And their test, involved
picking this node which
corresponds to something in
their biochemistry experiment.
And then, you can test for
various other substances that
coexist as other substances,
which are some set of edges
hitting this node.
And you still have to decide
which collection of
tests that you do.
And then, you can do a second
test to test other
edges and so on.
So this was their original
question.
And they were wondering whether
or not we can design
some algorithm for this general
edition of Vertex Cover.
Now, again, all of these are
related to set cover.
Because, after all, I'm trying
to pick nodes to cover edges,
which is like picking sets to
cover elements, except that
things are a little
simpler here.
Every element belongs
to only two sets.
So like in the Vertex
Cover case, it's a
special case of Set Cover.
We can get factor
2-approximations.
So we might hope that we can get
a factor 2-approximation
here as well.
So if the capacity of the node
is unbounded, like I said,
it's the standard Vertex
Cover problem.
So this problem, it turned out,
actually has a couple of
different ways of solving it.
So there's an NP-rounding
algorithm that we came
up with later on.
Initially, get a factor of 4.
And then, using some more
complex techniques, we were
able to show that you can still
match the bound of 2.
But their simplest algorithm, in
some sense, was based on a
very simple primal-dual
framework.
So we had to write a linear
program formulation or an
integer program formulation for
the Vertex Cover problem.
So it's a little bit different
from the standard formulation
that you might be
used to seeing.
So if you see, there's a
variable up there in the
objective function, which is X
sub v. So X sub v here models
how many copies of vertex
V do we want to choose.
And then, every time I pick a
copy of the node, I have to
pay the weight of the node.
And now what we have is
assignment variable, Yeu plus
Yev, for every edge UV.
So this edge, in some sense, has
to decide which endpoint
it wants to be assigned to.
So the sum of those variables
has to be at least 1.
And then there's a certain
capacity constraint which says
that you cannot assign
to many things.
So if I pick X3 copies of a
node, I'm allowed to assign up
to Kv times Xv edges incident
to that node.
And this says that if I
have actually zero.
And I never choose any copy of
this node, then I cannot
assign anything there.
So it's a very standard program,
very similar to the
Vertex Cover program.
But this generalizes it
naturally to deal with
capacities.
So that's the tour of this.
And so that is a very simple
primal-dual algorithm.
There's a few tricks
that happen
here, which don't happen.
There's a few things
that are needed.
If you don't really care about
the approximation guarantee.
This will give you an
approximation factor
of 4 that is easy.
But there's some issues
that happen.
So for example, nodes, we are
using dual variables.
This is something that would
make sense to you if you're
familiar with the primal-dual
schema.
If you're not, that's fine.
But as nodes are being chosen,
lots of edges are
incident to this node.
And when we decide to do this
node, we don't really decide
how many copies of this node
we want right now.
So we just say, all these
edges are covered.
And later on, we'll figure out
how many copies of this node
we actually need.
And the reason is that you don't
want over open too many
copies because we can't afford
to pay for them.
So what happens is that, if a
node is opened and this edge
is assigned here, suppose the
other node that this edge is
incident on never gets chosen.
Then, in some sense, we have
some spare capacity to charge
the dual variable
for this edge.
If the other end gets chosen,
actually we have to reassign
this edge to the other side.
And then, there's some
interesting case analysis that
you have to do to analyze it.
But in the end, it give you
the same bound of 2.
So we can match the bounds of
the standard Vertex Cover
problem for this more
general problem.
But now, I want to show you
something that we didn't
really explicitly think about
earlier, which is that, I can
think about this problem in a
very similar framework that we
have with jobs and machines.
I can think of every edge that
needs to be assigned as a node
on the left side of my graph.
And I can think of every machine
on the right, which is
the vertex.
And then, if I choose three
copies of the vertex, it has
some capacity.
It's like picking three
copies of a machine.
And then I pay three
times for it.
And then, I can assign
jobs to it.
Or we might say that
I can only pick
one copy of a vertex.
So that gives a more general
problem with hard capacities.

So some interesting observations
here-- so like I
just said, if I only pick one
copy of a node, rather than as
many copies as I want that
happen to be in this problem,
things become actually
a bit more involved.
So for example, a generalization
of the problem
that we studied is this
vertex cover with
hard capacities problem.
Now, this problem is interesting
because it begins
in some very funny ways.
So if I have weights on the
nodes, then it actually
becomes set cover hard, which
is somewhat strange.
Because each edge is still
incident on the nodes.
So we still have that property
which was very useful for the
Vertex Cover Dual Approximation
compared to the
log n guarantee.
But here, if I have weights on
the nodes and I still want to
assign all edges to nodes and
have a hard capacity concerned
with every node, this problem
becomes Set Cover.
So you can't do anything
but log n.
This was shown in a past paper
by Chuzhoy and Naor.
So that's a little bit
depressing because you can't
have any hope of getting a
constant approximation.
But in the same paper, what
they showed is that the
unweighted problem behaves
very differently.
And that's interesting, because
for most of the graph
problems, you don't see the
separation between the
weighted case and the
unweighted case.
So for most of the approximation
algorithms, the
best results for unweighted
problems, typically not
always, are sort of the same.
But you can take an algorithm
with the unweighted case.
And massage it or generalize
it to get this algorithm.
But here, there's this huge
difference in behavior.
So you get a 3-approximation
for that one.
And the algorithm is
somewhat involved.
And I'll explain at least a
couple of the simple ideas
behind their algorithm.
And then, I had a paper, soon
after their work, we were able
to prove it through
a 2-approximation.
But that algorithm is
really very messy.
And I want not to
worry too much.
So here's the central
idea behind the
Chuzhoy and Naor scheme.
And I think this is interesting
because it's a
very simple process.
But the analysis is somewhat
complicated.
So the basic approach is
the same, to again
drag down some [? IP, ?]
which is going to capture
the problem.
I now have these variables Xv
for every node, which, as you
know, you either choose a node
or don't choose a node.
And then, we have some hard
capacity constraint.
And once you relax it and get
a fractional solution, their
central insight is
very simple.
It says that, if you look at
any edge, then one of the
assignment variables, one of
these, two has to have value
at least half.
Because this edge could
be split fractionally.
It could be assigned 0.6 to one
side and 0.4 to one side.
But you can't assign 0.4 to both
sides, because it has to
be assigned somewhere.
So one of the endpoints has
a value of at least half.
So what this shows is that
think every vertex with
property twice Xv.
Now, the nodes are fractionally
chosen to some extent.
But that also tells you that
if the Xv value is at least
1/2, this node would
be chosen for sure.
If the Xv value is less than
1/2, then this node may or may
not be chosen.
So this all means two
classes of nodes.
Some nodes have Xv value
at least 1/2.
Some nodes have value
Xv less than 1/2.
The one that has value
at least 1/2 is
already chosen for sure.
The ones that have value less
than a half there's this
randomized process that
you choose them.
Now, we do know that every edge
has at least one end on a
node with value at least
1/2 that's chosen.
But that doesn't necessarily
mean that node is going to
have available capacity
for you.
So you're not done yet.

After this randomized process
terminates, you still may have
to choose more vertices to get
a feasible solution here.
But in some sense, what they're
able to show is that
the expected number of vertices
that you have to add
to convert this into a feasible
solution is very low,
because the probabilistic
[INAUDIBLE] events
will come that way.
So the analysis is
really messy.
So if you look at exactly this
algorithm that I described,
you could automatically here get
a bound of something like
an 8-approximation.
But it's not really
a 2-approximation.
And then, you change
the threshold.
And you change the parameters.
Then you can finally, with
some more complicated
analysis, convert it to
a 3-approximation.
And then, there's even more
complicated analysis, almost
20 pages long in our paper,
to find our way to a
2-approximation.
But the basic take-away
idea is this very
simple randomized process.

So one of the other things that
I wanted to mention is
that their approach, strangely,
does not generalize
to multigraphs.
So there are multiple edges
between pairs of nodes.
The aggregate process
is the same.
But there's no way to extend
the analysis to that.
And more importantly, there's no
easy way to extend this to
the case of hypergraphs.
So all the stuff I told you
earlier about large-scale with
soft capacities, there are very
easy generalizations with
hypergraph case.
So if an edge is not incident
as to two nodes, but is
incident to, let's say,
a subset of vertices--
up to ten vertices--
then you can get a
10-approximation.
So the size of the hyperedge, in
some sense, determines the
approximation guarantee.
So in Barna's thesis, we were
able to show that you can get
an Order (d)-approximation
for the [INAUDIBLE]
hypergraphs, if the edge
is of size, at most, d.
And this analysis also extends
to multigraphs.
And it's a little bit more
involved, because not only do
we have to consider the kind
of fractional solution that
they consider.
But actually we have to analyze
the structure of this
fractional solution, a little
bit more deeply.
So let me say just a
few words about it.
And then I'll again change gears
to a different problem.
The main idea here is that we
have to look a little bit more
closely at the fractional
solution.
And it turns out that-- well,
maybe the next example will
illustrate it to you.
We have to partition.
So I'll take a look a this
example for a second.
So here we are back to the same
problem, where we have
edges that have to be assigned
to vertices.
And after you solve the LP,
you get some strange
fractional solution.
These values tell you
that this node was
chosen to value 0.4.
This node was chosen to
value 0.9, and so on.
And then, these are the edges,
which are all fractionally
assigned to these nodes somehow
so that we convert
this to an NP-hard solution.
The question is, how
would we do that.
So some of the things that you
have to observe here is that,
for some edges-- if you look at
the top edge, for example,
the assignment variable for
that node, which is 0.55,
matches exactly the Xv
variable for the
corresponding node.
So those two are the same.
But they don't have to be.
It could be smaller.
But if you look at some
other node, this
assignment variable is 0.45.
But that node has value 0.6.
And this separation actually
becomes important when you try
to do something like
pick every node
with probability Xv.
And then, the capacity of
the node is going to
be KvXv or so on.
Or you get a capacity of Kv.
So I have a little example
here to show you
what's going on.
So dealing with the case when
the assignment variable is
actually equal to the Xv
value is much simpler.
So here's a simple example.
So suppose some node has
fractional value 0.7.
And it has capacity 6.
So now, notice that in the
constraint this is giving you
a budget of 4.2 for assigning
variables.
And then there would be a whole
bunch of edges incident
on this node with different
fractional values.
But notice that the total
budget is still 4.2.
So both these fractional
values are exactly 0.7.
They can't be more than 0.7,
because that is a constraint
in the program.
But some of them are
exactly 0.7.
And some of them are smaller.
But now, if I end up choosing
this node in my solution, I'm
basically scaling it up.
And I'm willing to pay the
financing to go from 0.7 to 1.
So I'm willing to pay
that financing.
I scale it up.
But the capacity I'm going
to get is only 6.
I'm not going to get
more capacity.
So notice that the number of
edges incident on this node is
a lot higher than 6.
And I cannot assign all of it.
But when I do the scaling,
these guys which have the
scale value of that, they
can scale up and
be completely assigned.
But I still have to
worry about these.
So there's some large sum very,
very messy details that
you have to deal with.
So the central idea was, that
you can look at the structure
of a linear program and then use
that, in some way, to help
you do the solving process.
And the constancy
wasn't large.
And I'm sure much better
algorithms will
eventually be obtained.
But at least we were able to
show that there's a constant
approximation, if you have
constant size hyperedges.
So now I'm going to change gears
one more time and talk
about a completely different
problem.
So one of the motivating reasons
for looking at this
hyperedge generalization of this
problem was that, in the
machine activation problem, if
you think about a very simple
model like vertex cover,
it says, you have jobs.
And these jobs can
only be assigned
to these two machines.
And then, I have to decide which
machines to activate,
which is by choosing
nodes to open.
And then, each node
has some capacity.
That's the [INAUDIBLE]
picture that I was
showing you all along.
But what if it's
not 2, it's 3?
That was part of the reason why
we wanted to look at this
hyperedge generalization, that
each job may have more than
two options on which
we can do this job.
So it could be 3, 4.
You might have, for example,
some data that this
job needs to run.
This data is replicated
four times.
Or it is on four different
machines.
In some data center, you might
say, well, I can assign this
job to any of those
four machines.
But the number may not be 2.
So that was one of the reason
why we were looking at the
case where the job can
be done on some
constant number of machines.
It's not an unbounded number.
But it's not 2,
And as we saw, it's actually
pretty hard to deal with a
case that's not 2, but
a bigger number.
But that's how it lead us to
trying studying this question
of how do assign these edges to
nodes in this hypergraph.
So this is another quote that
we read from a paper written
by researchers at Intel, which
had to do with Batch Computing
Algorithms on machines.
So this batch computing concept
which I'll talk about
next is actually quite closely
related to some of the things
that you've seen so far.
So you've seen this machine
activation problem that I
talked about.
You've seen this Vertex Cover
and Hypergraph and Hard
Capacities problem.
And then, the third thing
I want to talk
about is batch computing.
And the interesting thing
about batch computing is
that-- so what is it you do
with batch computing.
Batch computing says, there's
a whole bunch of jobs that I
want to do.
Jobs may have some constraints,
release times,
deadlines, and so on, and
some processing time.
And I want to do these
jobs in batches.
Now, traditionally, if you look
at all of the scheduling
literature, literally over the
last forty years, of the
problem of machine scheduling,
almost all of it assumes that
the machines are always on.
The machines are always
available.
And then, they look at trying
to optimize certain metrics
from the job's perspective.
How do I assign these jobs
to these machines in some
intelligent way?
But we assume the machines are
always available to us.
So in even batch computing,
which is the problem that we
looked at, they are trying to
match these jobs to batches on
the machine.
So you can do a bundle of jobs
together on the machine.
But they never care,
for example, how
many batches you get.
So the simple example might be
that my wife tells me, hey, I
want a cup of tea.
And I would say, well, I want
some coffee in 15 minutes.
If you are willing to wait, I'll
go to the kitchen once.
And I'll make tea and
coffee together.
So that's exactly like a batch
computing thing, that I can do
two tasks at the same time,
without them doing two tasks
separately.
If I did two tasks separately,
it's two batches.
If I delayed one job and I did
it with the other job, that's
two things that happen
simultaneously.
But I suppose I can't do an
unbounded number of things.
So that's the batch-size
constraint.
So you want to somehow cluster
jobs, if you will.
And do them in a way that
you don't do too many
things at one time.
But you also try to optimize how
many batches you picked.
And any questions?
This is a good time to stop and
take questions, because I
want to change gears to a
different scheduling problem.
So this is a good time to ask
any question that you have.
Yeah.
AUDIENCE: What's the typical
power usage for a machine
running two jobs at once,
compared to one?
SAMIR KHULLER: You guys should
tell me the answer to these
questions, [INAUDIBLE].
So I'll give you one other
example in a second, which
doesn't have to do necessarily
with machine minimization.
But the idea is that-- let's
say, for example, you have a
multi-core processor.
You say, well, if this processor
is powered, I can
run up to a certain number of
threads simultaneously.
And my energy usage might not
change too much, whether I'm
running three cores,
or I'm running 15.
But I can't exceed
the batch size.
So this is a simple model.
Our model is extremely simple.
I don't know whether it models
the energy or not.
But as you see, there are some
other compelling reasons to
study this kind of model
as well, that
I'll get to in a second.
So for the last one to start,
I'm going to assume a very
simple model, where
we have jobs.
And in fact, here, there is one
thing I use, which is also
very, very simple.
So I'll actually go with it,
and I'll use some detail.
So jobs have release times
and deadlines.
And in the general version, the
jobs have some processing
requirement with length.
But we'll also be interested
the unit job case.
And then, most of it is answered
right now for the
unit-job case.
And we've got to assume that
this batching machine has the
property that time is slotted.
So in each slot, the machine
is either active.
It's awake.
You're paying some
energy cost.
Or it's inactive.
But if the machine is active,
you can schedule up to B jobs
on this machine.
That's the capacity
of that batch.
And the goal is simply to
minimize my energy cost, which
is the number active slots.
Now, people have looked at
non-batching models, where
they'll try to find--
so for example, the paper I can
think of is a classical
paper by Baptiste, where they
tried to find a way to
schedule jobs, where if you
schedule two jobs next to each
other, then sort of pay in some
intuitive way to turn on
the machine once.
Then you'd run these two jobs
or three jobs in a cluster.
Then you turn the machine off.
But if you separate these three
jobs out into three
separate slots, which are not
contiguous, then you may pay
these different on-off cycles.
But there's no batching
in all of this.
They're just unit jobs that
you want to schedule.
And then, there are some dynamic
programming algorithms
to find an optimal solution
in those cases.
But I don't think they have a
model where any parameter is
huge. you're talking about
n to the 5, n to the 6.
But the problem is
not NP-hard.
It can be solved optimally
or [INAUDIBLE].
AUDIENCE: So one thing that
happens in practices--
machines are already
doing other things.
So say, they are running
services or something or doing
some storage work.
And batch things are
done in the gaps.
SAMIR KHULLER: I understand.
AUDIENCE: So we may not
always turn it off.
SAMIR KHULLER: So what you're
saying is that with every time
slot, somebody might prescribe
a non-uniform batch size.
So if you go back-- and I'll
show you another picture--
so for example, you see
the next picture.
So again, you can think of doing
these jobs as things on
the outside that you assign.
Except that now, I hear
I don't have machines.
But each time slot,
in some sense,
intuitively is a machine.
And what you're saying is that
this capacity function can be
non-uniform.
But each time slot is a machine
now, which if I pay
for that machine, intuitively,
then I put some processing
capability in that time slot.
If I don't pay, it's zero, which
is exactly the way in
which I've been looking at all
of these problems I'm doing.
But this particular algorithm
that I'm going to talk about
is a very simple algorithm.
But there the batch size
has to be uniform.
It's not unbounded.
But people have looked at the
problem that you asked about.
And I'm going to do
you a pointer to a
reference in a second.
But the running debt
is much higher.
It's a dynamic programming
algorithm [INAUDIBLE].
So here's a very simple instance
of what is the kind
of problem we're trying
to solve.
I have, let's say, unit jobs.
I have a batch size of 3, which
is uniform in this case.
This is time, on the x-axis.
And I have to find
some schedule for
all of these jobs.
And again, I want to minimize
the active time.
And you can also think of the
active time as, once you find
a schedule, I want to make sure
that, at every time, at
most B jobs are being done.
And the cost of the schedule is
about the projection of the
schedule of the x-axis.
Because that's the time for
which the machine is active,
whether it's doing one
job or B jobs.
It doesn't matter.
So for example, this four-job
schedule would have four
active slots, as shown.
So now, going back to some
of the motivation in the
scheduling literature,
as to why people
looked at batching problems.
It had to do with these
two applications.
One has to do with ovens.
And one has to do with trucks.
And the applications themselves
are slightly different.
And I'll explain why.
So the trucking model is like
saying, I have some packages
that I want to send
out on this truck.
The truck has a carry
capacity.
And every trip is
like a batch.
So the packages have arrived
at some point.
They have been delivered
by some deadline.
And then, I'm trying
to minimize the
number of truck trips.
Or if you're somebody who's
organizing a conference, and
you have to pick up these
300 people who are
arriving at the airport.
They're all arriving
at different times.
And you have a certain
collection of vehicles.
And you have a capacity
of 20 each.
And you're just trying to
minimize the number of trips
you have to make
to the airport.
But you want to make sure that
you pick up everybody and so
on and so forth.
So that's exactly the
trucking model.
And the other model is
slightly different.
With the other model, so it
let's you put these pizzas or
whatever the guy is baking in
and out of the oven, even
thought the oven
has a capacity.
So for example, I might
start this job here.
And this time, as soon
as it's released, the
job is halfway done.
And then, I can start another
job, because my
batch capacity is 2.
And then, this job ends.
This pizza comes out.
Another pizza can go in.
So this model is a little
bit different.
Once a truck has left, you
can't try to push another
package through.
All right, so these models
are slightly different.
But they're both batching
problems.
So again, you can use Wolsey's
greedy algorithm for the
problem I just described and get
a log n approximation for
this problem, where, in some
sense, the sets that we are
choosing at every step are the
time slots with some budget of
a capacity.
And Wolsey's greedy framework
gives you a log n
approximation.
So there's an exact algorithm
based on dynamic programming
by Even, Levi, Rawitz,
Scheiber, Shahar,
Sviridenko from 2008.
This algorithm [INAUDIBLE] can
handle non-uniform batch size
at every step.
They actually converted to
a computational geometry
problem, where they have
these intervals,
like in this picture.
They have these intervals.
And they want to stab
every interval by
this vertical stabber.
And these stabbers have some
capacity of how many intervals
they can cover.
But this is basically
the same problem.
But as you see, the running
time is quite high.
Here's the running time
with that algorithm.
It's a Dynamic Programming
algorithm.
But that one, I believe,
handles the case of
non-uniform batches.
So one of the things that we
were thinking about is that,
why isn't there a much
faster algorithm.
And this is joint work with
Jessica Chang and Hal Gabow,
who I'll give references on
at the end of the talk.
So the algorithm that we came
up with which I'm going to
describe to you next is called
Lazy Activation.
Now, this algorithm sort
of comes in two phases.

Since I'm a little bit short
on time, let me not go with
the motivation behind
stage one.
Maybe if we have time
at the end, I can
explain that to you.
But phase one is something
which says,
something very simple.
It says that, suppose I
have a collection of
jobs I need to do.
So this is the window in which
every job has to be done, and
so the release time and a
deadline for the job.
We have a batch size of 3.
And now, I'm going to look
at all of the jobs
with a common deadline.
So notice here that
I have five jobs
with this common deadline.
The goal is that, somehow we
want the property that we
never have more than B jobs
with a common deadline.
So that would be a useful
property to us later.
So we want to somehow take any
important steps, which has
arbitrary release times
and deadlines.
And then we somehow want to
convert it to the property
where, at any deadline value, I
don't have more than B jobs.
And that conversion process
is very simple.
Because if you think about it,
in this last time slot, I'm
never going to be able to do
more than B jobs anyway.
So what we do is we have this
very simple rule, which scans
the slots from right
to left and then
decrements the deadlines.
And what we find is overloaded
slots where there are more
than B jobs for a deadline.
And then, I'm going to favor
decrementing deadlines for the
jobs that are also
released earlier.
So in this particular example,
I have five jobs.
And I want only three of them
to have that deadline.
So these two jobs, notice,
were released earlier.
So we're going to adjust their
deadlines like that.
And in some sense, we haven't
really made any mistakes,
because the optimal
solution can't be
more than B jobs anyway.
We can argue with an exchange
argument, that this is the
right thing to do.
It doesn't cost you anything
to do that.
And we do it recursively.
So now, what may happen is
that you decrement the
deadlines of these jobs.
Now, it may be that there
are more than B
jobs in this deadline.
And so we've got to apply this
rule recursively and then,
again, do the same 3B choice
to decrease the
deadline for some.
And now, you might wonder
what if some jobs window
collapses to empty.
So in that case, we can prove
that the job has no way to be
done in any feasible schedule,
basically, more or less.
And even if we do that and it
just means there's no feasible
schedule that does
all the jobs.
And in fact, we could also show
that the number of jobs
then is maximum possible.
So it's not as if there would be
another schedule that would
do more jobs.
So everything is hunky-dory
so far.
Now, the algorithm, after this
pre-processing, becomes
extremely simple.
So now, I'm going to go to step
two and order jobs in
decreasing deadline order.
And you will see in a second
why that property from step
one will be useful.
And let me explain the algorithm
by an example.
So I want to focus on
what is the job with
the earliest deadlines.
And what do you want
with that job?
So then I go to this example.
So imagine that I've already
done the pre-processing.
So that was B jobs with
any common deadline.
Now, look at the job with
the first deadline.
That's this job.
This job has to be
done somehow, at
some point in time.
And there's no reason
to do it too early.
I will sort of intuitively
maximize the overlap of the
other jobs, if I delay it.
So we will do it as
late as possible.
So we'll schedule
that job there.
So this slot will be active.
And in some sense, we can prove
that the optimum also
has to mete out losses in order
to activate this cost
for the job.
And there's an optimum accurate
[INAUDIBLE], even on
this [INAUDIBLE].
But now I have some spare
capacity here.
Because I can do B minus 1
other jobs, and for those
jobs, out of all the overlapping
jobs, we'll simply
pick the one with the
earliest ending.
It's a very simple proof.
So we'll keep doing this, so
I can pick those jobs.
Then I remove them all
from the instance.
Look at a job with the next
deadline, schedule that.
Look at all the overlapping
jobs, remove
those, and keep going.
And this algorithm, if you are
very careful, you can actually
implement it like basically
linear time and get the
optimal schedule.
So for this example, the
optimal schedule looks
something like this.
And let me skip the proof.
So there are a few other things
that I want to say.
So this is the point.
So let me say a few other words,
which is, we're also
interested in a slightly more
general model, which is that
what if a job doesn't have a
single window of time in which
it has to be done, like a
release-time deadline.
Or maybe there's some disjoined
collection of
windows in which this
job can be done.
So that's another interesting
problem.
But it turns out, very quickly,
you sort of get
NP-hardness there.
So this is a much
harder problem.
And the reason is that, for
example, you can show that his
model can be captured in the
set cover framework.
And all the standard NP-hardness
groups go through.
So this problem is a little
bit more involved.
But going back again to this
bipartite graph or an interval
flow, we are thinking
about things.
Again, what we have here is
a collection of jobs that
somehow we want to assign
to time slots in
the schedule of machines.
And then, when I want to decide
to activate a machine,
I get a certain capacity of
B. So those of you who are
experts in network flow will
notice that there's also a
flow problem, which is very
similar called Minimum-Edge
Cost Flow, which is different
from both max flow and
min-cost flow.
So in Min-Edge Cost flow, you
want to find a flow which
starts with the same
type of value.
And you pay it in the linear
function of every edge.
If you send three times the
amount of flow on some edge,
subject, obviously, of some
capacity, you pay
three times the cost.
In Min-Edge Cost Flow, it's
this binary function.
So if you send any non-zero
amount of flow on this edge,
you pay a fixed cost.
And then, there's some capacity
that you get.
So a Minimum-Edge Cost Flow is
not surprisingly NP-hard.
There's some hardness of
approximation results.
With all of these problems, you
can think of them in this
way of Min-Edge Cost Flow.
But they have all these
common properties.
And we are being able to exploit
certain properties.
And sometimes, this is deriving
better approximation
guarantees.
Because Min-Edge Cost Flow, the
approximation results are
basically almost the
hardest result.
There are no interesting
approximation algorithms for
the most general problem.
But here, sometimes we're able
to exploit the property that
each load here has low
degrees, for example.
It has very few options.
And that's useful.
All this batch size
is uniform.
All the set of time slots you
go to is a convex set.
So you can go because you have a
release time and a deadline.
So all of these special
properties are useful in
trying to understand the special
cases of this problem.
Going back to the problem
where each job does not
necessarily have a single
release time and deadline but
possibly a set of possible
times perhaps
it can be done on.
So we just have arbitrary
bipartite graph, where an edge
means this job can be done on
this time slot and so on.
In this problem, the NP-hardness
proof crucially
used the fact that the B value,
the button with the
batch size for that time slot is
greater than or equal to 3.
So initially, we didn't really
think about what happens with
a batch size 2.
It turns out that a batch size
2 probably isn't NP-hard.
There is actually an optimal
strategy that solves it.
And the optimal strategy has
to do with matching.
And it's somewhat interesting.
So let me talk about
that a little bit.
So this relates to a problem
known as the Maximum Bounded
Degree Subgraph problem.
So the Maximum Bounded Degree
Subgraph problem, you're given
a [INAUDIBLE] graph J. You have
some upper bounds with
the degree constraints
of the nodes.
And the goal is to find the
maximum collection of edges,
but that you don't violate any
of the degree constraints.
And this graph does not even
have to be a single graph.
I can even stick
in set groups.
So we're going to create and
instance of the Maximum
Bounded Degree Subgraph where
we have a degree bound of 1.
In fact, I wouldn't even
want to consider it
exactly equal to 1.
I'd consider it like
at plus 1.
Equal to 1 makes things
a little bit easier.
And maybe a bound of at most 2,
for the batch card nodes.
And the main property that I
want to show you is something
like this which is that, in
some sense, every time our
time slot is inactive you
gain one set group.
So for example, if you look at
this schedule, the first two
jobs are assigned to that common
machine, which has a
capacity of 2.
The third job is sitting
all on its own.
It goes there with
these two jobs.
So if you split your jobs
and spread them out.
And you don't utilize this batch
capacity of 2, then none
of the set groups
can be chosen.
If I somehow have n jobs and n
slots, and I spread them out,
then I cannot choose any
of the self-loops.
Because of the degree constraint
of my set group.
I assign one job here.
And there's a self-loop that
can't be chosen, because of
the degree.
So if I choose a self-loop it
ought to be divisible by 2.
So in some intuitive way, every
time you pair up a set
of jobs under one machine,
it lets you pick
one more set group.
So that's called the intuitive
connection.
The next step is all details.
But the formal property that
we have to argue is
something like this.
So I've already mentioned
this problem,
Maximum Degree Subgraph.
This problem itself, in turn,
is solved by reducing to the
bipartite matching problem.
And then, I'll add the
deductions that you're going
to use for the bipartite
matching.
But like I said, it's a little
bit more involved.
So you have to first start out
by finding a standard maximum
matching without
the self-loops.
And then, you have to do the
augmenting part, tricks which
are standard for matching.
But after that, in the
self-loop, it asks the
question, how many more edges
can I add to this
degree-constrained subgraph.
And in the end, the property
you get is something like
this, that the size of the
number of edges in the
degree-constrained subgraph,
you can prove, is exactly
equal to the number of jobs.
Because in the initial step, you
matched to all the jobs.
We started off with a perfect
matching for the jobs plus the
number self-loops.
Now, the number of jobs
is a fixed amount.
So if I maximize this.
I'm maximizing the number of
self-loops, which is exactly
like maximizing the number
of inactive slots.
Every time you have a self-loop,
that node is
inactive, because there's no job
assigned to it, basically.
So that was one connection that
I just wanted to talk
about a little bit.
We also looked at one other
problem, which has to do with
the idea of having partially
active slots.
So let me just explain
what the problem is.
So here, we have a
batch size of 2.
But notice that we don't
necessarily-- so we have three
jobs here, the yellow job--
I'm not sure what that color
is-- and this maroonish job.
So we have three jobs.
And as our batch has a two in
the normal model, I need at
least two active slots.
In one slot, I can
do two jobs.
And then, in the second slot,
I can do one job.
But now, time slots can
be partially active.
You can start doing
strange things.
Like I do this job sometimes,
0 to time 1.
This next job starts a time
1/2 and goes to 3/2.
And then, that first
job is split up.
And it's done a little bit here
and a little bit there.
And not only is it, at no point
of time, am I doing more
than two jobs.
But I look at the projection
of this schedule on the
x-axis, solely using times 1
and 1/2 slots [INAUDIBLE].
So this is not a thing that we
understand very well, other
than the fact that it's clear
that there are cases where you
might have an instance where,
if you're not allowed to
preempt, it costs more than if
you're allowed to preempt.
So here, I can reduce these
two slots to 1.5 slots.
And we were only able to
analyze this for the
batch-size 2 case.
In the batch-size 2 case,
basically, the main thing that
we can show is, that example I
showed you is the worst thing
that could happen.
So in that example, the ratio of
the non-preemptive schedule
to the preemptive schedule
is something like 4/3.
And basically, that's
the worst example.
So the ratio's upper
bound would be 4/3.
And this process is really
quite involved.
So I don't even know how to get
started on the case when
the batch size isn't 2.
But our projection is that this
bound here will be some
function of the batch size B.
But it'll be something like 2
minus 1 over 3 plus 1, or
something like that, is what
we were anticipating.
But [INAUDIBLE].
So our projection is that this
idea of preemption, even for
large B, is not going to save
you more than a factor of two.
It'll give you some savings.
But not a huge savings.
But we don't have to
go back in here.

So I've already, I think, made
sort of through the dark
talking about this idea of this
Min-Edge Cost Flow problem.
And for the most general
problem, we don't really know
how to solve it.
And all the lazy activation and
all this stuff I showed
you is only really for the
case of unit jobs.
So if the jobs are not unit
length, then I don't really
know how to solve
this problem.
So this is, again, a picture
of what happens.
So this is exactly like saying,
I want to maybe choose
these three nodes and pay the
cost of these edges in the
Min-Edge Cost Flow framework.
And it gives me some budget of
how much flow I want to send.
And the goal is to route all of
the flow, subject to some
constraint on the cost.
So let me conclude here by
saying a few things about some
open problems.
So the first interesting
question in my mind is whether
or not there is a simple
combinatorial approximation
algorithm for the Vertex Cover
with Hard Capacity.
So everything that we did so far
involved solving a single
linear program with the
rounding process.
And the analysis is extremely
messy and complicated.
And I don't know whether
or not it'd be a simple
approximation algorithm.
Because with a standard Vertex
Cover, there's literally a
three-line algorithm that gives
you dual approximation.
But of course, that's
an unfounded device.
So whether or not it's a simple
combinatorial algorithm
that will deal with
the capacity case,
we don't know that.
And this active-time scheduling
problem that I
talked about, we can only
prove optimality for the
unit-job case.
If jobs have arbitrary lengths,
and you want a
non-preemptive schedule, then
there's an easy proof that
it's NP-hard.
But if you're allowed to preempt
jobs-- so I say that
this job has some length of 5, I
just think that I have to do
these five hours
of processing.
But I can do one hour today,
one hour tomorrow.
It doesn't have to
just the length.
It could be just times cost.
Then, we actually don't
know what the problem
is going to be like.
So it's an interesting
question.
And more recently, we just were
looking at this min-edge
cost flow framework and also
developed some alternate
heuristics to the greedy
approach, based on eliminating
slots, rather than incrementally
adding slots.
And those heuristics actually
behave quite well.
And a lot of best cases give you
extremely good solutions,
like with M plus 1 of
[INAUDIBLE] and so on.
And more recently, there was a
sort of paper just this month,
not even last month, which looks
at some online variants
of the kinds of question
that I talked about
as well as I have.
Let me quickly conclude by just
giving you a bunch of
references.
So the first two papers are
research that my students and
I have been involved in.
So the scheduling problem I just
talked about was a paper
published e-style with Jessica
and Hal Gabow.
I also mentioned this
experimental work that we did
at the ALENEX conference
just a couple of
weeks ago with Jessica.
And then, I mentioned some other
results, which would be
hypergraph problem and
hard capacities.
This was one with PhD Barna
Saha, who graduated and went
to AT&amp;amp;T Labs, and so one.
So the first six or seven papers
are papers that I've
been working on.
This paper is one with a high
school student I've been
working with, Frederick.
He's been looking at batch
scheduling on parallel machines.
And that work is not
published yet.
And one of the things that we
did there was, there's several
direct programming-based
algorithms for
many of these problems.
But all of them have very,
very high running times,
something like n to the 5,
n to the 6, and so on.
In fact, with one popular
version we are running that
with like n to the 8.
And so Frederick and I were
looking the problem of batch
scheduling with multiple
machines,
not even one machine.
So the exact problem of this
model is something--
like I mentioned this
application earlier-- if you
have an airport, people that
are arriving, you have a
certain number of cars.
Each car has a capacity, and
you want to schedule the
pickups for all these people.
And you also have to try to
minimize the number of
batches, the number of trips
that you make to the airport.
But now, you don't have
one car, like in
the trucking model.
But you have a set of five
cars, ten cars, whatever.
That's the parameter.
So that's the number
of machines.
And it turns out, we actually
do not need that in
programming.
So we have algorithms that are
pretty efficient running time,
like roughly at [INAUDIBLE], and
will find you an optimal
schedule, without using
any type of program.
And the other papers here are
most of the papers that I
mentioned as being closely
related work that, if you're
interested in this
topic, would be a
good starting point.
And I gave you of the slides
also, if you need this.
So let me quickly conclude by
thanking my collaborators.
So Jian Li was a PhD student
at Maryland, who graduated
2011 is now an assistant
professor at Tshinghua
University.
Barna Saha graduated about
the same time in 2011,
is now in AT&amp;amp;T Labs.
Jessica is currently earning
her PhD formally at the
University of Washington.
But she's actually been at
Maryland for the last couple
of years, before
she graduates.
And also a lot of the scheduling
work was joint work
with Hal Gabow, who's Professor
Emeritus at the
University of Colorado,
but still amazingly
active in his research.
So I've been collaborating
with him, mostly
long-distance.
OK, so let me stop here.
Any questions?

AUDIENCE: Question sir.
SAMIR KHULLER: Yes.
AUDIENCE: So with the
batch scheduling--

so the way you formulated it,
it's like a single-machine
version, right?
SAMIR KHULLER: Yes, that was a
single-machine problem, not
micro-machine.
AUDIENCE: So do you have a
sense about if you have
multiple machines and you add
some of the constraints, like
in the gap problem or something
like that, where not
all jobs can be with
their own machine.
But then, you still want to--

you find a machine to be active
if you're executing at
least one job.
SAMIR KHULLER: So the work with
Frederick is the closest
to what you're asking about.
So there we have M machines.
The batch size is uniform
for every machine.
Jobs have release times
and deadlines.
And in fact, there, we
don't want to assume
that time is slotted.
So release time can be 73.5.
And deadline can be 79.
But we do assume that all the
jobs have the same duration.
So you can think of it as,
either my release times and
deadlines are integers.
And all jobs have processing
time B, which is uniform.
Or you can scale things
so that B is 1.
And then, you lose the
release-time deadline being on
integer boundaries.
So that's why the techniques
change.
In our proof, the slottedness
of time is crucial.
That's what gives a very
fast algorithm.
If time isn't slotted,
then things become
a little bit messier.
And that happens even
in classical
work by Barbara Simons.
And I don't know how much you're
familiar with the Unit
Job Model, which was very
popular in the early '80s.
There's a bunch of papers, and
the same issues you run into
when the release time is not
slotted or non-integer.
So in that model we cannot
impose a constraint that this
individual doesn't want to
go on this van at all.
So the assumption is that if
anybody can go on any van we
can have a release
and a deadline.
And we have a fixed
number of vans.
And we tend to ask the
question-- can we schedule a
pick up for everybody,
minimizing
the number of trips?
So they just want the
vans getting there.
They don't care how many
trips were made.
But we really want to look at
this energy cost or the cost
to schedule.
We want to minimize the number
of trips. and so that N-cube
algorithm sort of solves that.
But the batch size is uniform
across all machines.
And jobs just have a release
time and a deadline.
And then, they can be scheduled
on any machine
within that window.
They can't specify
which machine.
But it's a very interesting
question.
How can we handle it?
I have no idea right now.
AUDIENCE: So what are your
thoughts on when the job
length is not unit?
Is it practical?
SAMIR KHULLER: So Jessica and I
have started working on that
problem for six months.
And at this point, we're
not sure whether
it's NP-hard or not.
So we don't have any
NP-hardness proof.
Whatever algorithms we have seem
to end up giving you an
approximation guarantee.
So I don't want to claim
any result, because we
don't have it yet.
But we have an algorithm that,
currently, we believe the
right answer for this
algorithm is a
2-approximation.
So we have no examples where it
could do worse than 2 times
the outgoing busy time.
We think we can prove
like factor 4.
But nothing is written down.
So it's not confirmed yet.
We just have some limits to
prove that look right.
But I think for the same
algorithm, the
right answer is 2.
We don't have the
proof for 2 yet.
I think we have it for 4.
But again, I don't even know
the problem is NP-hard.
So the form of problem is
simply, every job has some
integer-processing
requirement.
And so it has to be scheduled,
say, in 3 slots or 5 slots or
some window of time.
And then, whenever a slot is
active, it can be going up to
B different jobs.
You can't be doing 15 units
of the same job.
But you can be doing up to 15
units or up to B units and up
to B different jobs.
And you can do one
piece of them.
And then, the final cost to
schedule is a projection of
the x-axis.
But now, jobs are
not unit then.
And every extension that we
tried with the lazy activation
seems to fail.
So we've tried many different
variants of rules for dealing
with non-unit length jobs.
And they don't seem to work.
And looking at the structure
of the optimal solution it
seems highly non-trivial.
For example, in the unit-job
case, when we opened a slot,
we were able to just use some
simple greedy rule to fill in
more jobs, just based
on the deadline.
And now, the jobs that overlap
this slot, the problem is that
they have deadlines.
And they also have different
processing times. and we
cannot come up with one rule.
Because the future determines
what is the right thing to do
here, and there are different
future scenarios that seem to
show that different rules will
suggest what needs to be done.
So at this, our way
of left-to-right
processing won't work.
So that has to be something
clearly much more intelligent
besides what jobs to do.
So it's a problem that
we've sort of been
stuck on for a while.
AUDIENCE: Any idea about
preemption depth?
SAMIR KHULLER: Yes.
So you don't have to do things
in a contiguous way.
If you are not allowed
preemption, then you can prove
what's NP-hard.
If you always sort of put
things contiguous
and then set it.
So the preemptive model, you
can think of it as a
chain of unit jobs.
That's really the right
way to think about it.
I have some precedence
constraints.
So a job is like, A1,
A2, A3, is job A,
three units to a job.
Job B is B1, B2, B3, B4, B5,
five units to the job.
But it has to be scheduled
in a chain.
With that problem, we
don't know what the
right complex is.
Han?
HAN: So yes.
For the batch-scheduling
job, just to clarify--
right now, the optimal
solution is
for unit length jobs.
And they have to be release
at integral time slots?
SAMIR KHULLER: So if the release
times are arbitrary
then the algorithm is
significantly slower.
It's n-cube.
It's not like this fast n
log n-type algorithm.
HAN: But it still--
SAMIR KHULLER: It's polynomial.
Yes.
AUDIENCE: I see.
SAMIR KHULLER: But the algorithm
is completely
different and, in fact, solves
a much more general problem
that algorithm, where have to do
multiple machines, not even
just one machine.
AUDIENCE: In the extension that
you mentioned with your
student Frederick,
does it schedule
arbitrary release times?
Or it's also integer
release times?
SAMIR KHULLER: No, arbitrary
release times.
But the jobs have
the same length.
But the release times
and deadlines don't
have to have any--
you can choose it a 73
or 9 or whatever.
It doesn't matter.
AUDIENCE: I think with
preemptions, what would be a
better model is you
allow preemption.
But there is some cost
of preemption.
SAMIR KHULLER: Yes.
So I wouldn't be surprised if
people have looked at models
where preemption has a cost.
I'm not that familiar with
that literature.
So we were looking at this as
scheduling unit chains of jobs
really, as opposed to a job.
But then, people will
[INAUDIBLE] more or less.
But you're absolutely right.
If there's a cost of preemption,
then things could
be a little more complicated.
So with two other students,
I've been working more
recently on a different
scheduling model, which has
more to do with, as the number
of jobs that the processor is
working on increase-- so you're
allowed to add jobs to
a processor.
But they're sort of doing equal
sharing of the processor.
So if I'm running one job, that
job is running at 100%
rate, in some sense.
If I can rig two jobs, then
each on is getting 50%.
If I'm doing three jobs, each
one is getting 1/3.
So you can just keep adding
more and more jobs.
And there's some interesting
applications for that.
So one application that we were
looking at had to do with
filling boats.
So I don't really go sailing.
But there's some deck on which
there's a pump, which is
giving gas to different boats or
ships and a certain number
of docking stations where
the boats arrive.
And this pump is providing
fuel at a certain rate.
And when this boat docks, they
assume that the fuel is going
to be equally split.
So if they're filling up three
boats, then each boat is
filling up at this
rate of 1/3.
But if the docking station
allows for four more boats to
show up, they can join.
Then each one is
filling at 1/7.
So that's an interesting model.
and then, there is some
overhead for switching.
So if a boat has to leave and
another boat has to come,
there's a fixed delay.
There's some very interesting
scheduling models
that you run into.
You want to do things like
minimize weighted completion
time and things like that.
And so there's a lot of
interesting things that happen
in that particular setting.
So there's no exclusive
cost for preemption.
But in some sense, you can
add remove things.
But everybody else
get slowed down.

More questions?

HAN: OK, cool.
Thank you, Samir.
SAMIR KHULLER: OK, thanks.
[APPLAUSE]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>