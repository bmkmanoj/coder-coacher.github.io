<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>An Overview of the MySQL Engine and its Latest Features | Coder Coacher - Coaching Coders</title><meta content="An Overview of the MySQL Engine and its Latest Features - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>An Overview of the MySQL Engine and its Latest Features</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_nfUCpywMJc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so it's my pleasure to
introduce Timur here timorous started
with my professor in in Sweden he did
that after after I was left that group
and joined IBM I worked on databases
where but then he came and visited at
IBM so we got to know each other quite
well where his self description says
that he started doing databases when he
was like three years old or something or
later on maybe so I think that's the
treat of people doing databases we do a
lot of databases one way or another and
I think everything is the database but
not many people get as far as looking
into a query engine and actually
modifying that and trying to improve it
so this is what Seymour is going to talk
about the architecture of my sequel
query engine and how things work inside
under the covers and I think this is
gonna be quite interesting so the floor
is yours
Thank You Yanis
so so my talk consists of three parts
the last one being very short one the
first part in the first part I will talk
about the basic principles in our MySQL
query engine and I'll cover two main
issues the first one is how query plants
look like and how they are being
executed and how and the second part is
called
actually we get to execute how do we
look for good execution plants and in
the second part depending on time I will
try to cover six of the six most
important features that we developed our
team developed since and introduced in
five version 500 and one of these
features is in version 5 1 and finally I
will talk about the latest stuff we are
working on currently which is either in
design phase or already partly
implemented so we begin with query
plants and their execution and first
I'll go through the architecture of the
query engine excuse me
well the way client application talks to
to the query server is similar to any
other database system we can go you can
send your sequel via either ODBC JDBC or
native interface or anything you like
then it goes to the parser the parser
analyzes the syntax it produces a parse
tree and then number of the subsequent
phases operate on this parse tree the
next phase is the preprocessor and it
does semantic checking name resolution
and access rights checks and this is
basically where we we check whether they
the query semantically correct according
to the sequel standard I will not be
talking any more about these two
components next the parse tree gets into
the optimizer and it does some logical
transformations does some cost based
optimization and
finally some plan refinement I call this
plan refinement because we do not
generate P code so normally it would be
called here
plunk generation but I use this small
general term once we get query execution
plan optimized code execution plan we
send it to the twenty second engine
which can use a number of data access
methods like tables can index can range
and ref or eqf of the one spoon of the
mysql terminology and it's also here
where we actually compute different
joint methods one of them is already
here of all kinds of variants of nested
loop joint and currently I am working on
here John and the query execution engine
through the handler API basically this
is our term for Storage Manager API it
goes and accesses the database through
through different storage engines so
here I've put three of them but there
are many others already existing for our
system now how do query execution plans
look like in my scale for the for those
familiar with database textbooks they
would know the typical representation of
query plants as trees and the general
representation of a query plan is as a
bushy plant that means a tree where the
input of a joint can be an intermediate
result like this one an intermediate
result of a previous joint on both
operands can be intermediate results and
this is called the Busha plan well we
don't have that in mice QL what we have
in - QL are only left deep linear twist
that this is our internal representation
of query plants so always the the right
input of a joint is a base table
eventually access through some index now
given that this is tea on abstract level
this is the representation of query
plants I will explain more about talk
more about how they in
suddenly represented yeah thank you
it's a limitation
well both now the question the question
was is there is this a limitation or is
this a feature and was this was there a
good reason for designing it this way
now my first answer is that actually it
wasn't me who designed it that way so
for me this is a fact and that was
designed it was designed this way from
from the very beginning now the second
the other answer is that generally from
database theory it is known that if one
does not explore parallelism which we
don't do currently Busha plans Coverity
to use otherwise so most of the good
plans so to say can be found in the
space of left deep linear plants bushy
plants are most useful if you can
explore if you can explore explore
parallelism and compute let me go back
and if you could compute each of these
two joints in parallel then it makes
sense really to explore the space but if
it if you don't if you cannot do that
then it doesn't make sense really
because the space for all possible
plants becomes much bigger and the
potential to get a better plan is much
it's not very big I mean in simple terms
right so in order to explain a little
bit more how query plants look like and
fall in the rest of the presentation I
will use a small world database example
so suppose we have three tables with a
number of indexes on each of them each
of these tables guess each of these
tables has a primary key like ID code
and country I think no they are one of
these two country language is the
primary key here and let's consider this
query on the tables languages a join
between languages country and city how
would this query actually be represented
as a plan so given that our plants are
left deep linear trees we don't need to
represent the most trees what we do is
we represent them as a race of operators
and
the reason I shown I've shown here the
operators top down and not bottom up us
in the tree is because I would like to
make a relationship between how a plan
is subtly internally represented and
explained output of the query engine so
whenever you actually do explain on this
on the previous query you will get three
rows each of them describing in some way
each of these nodes in the plan so I
call these nodes plan operators and for
example let's consider the first one
this is not all the information in the
plan operator but this is the most
important so each plan operator contains
at the table being exist the condition
used to access the table and the
condition used actually to filter rows
at this level of the plan the access
method being used to access data from
that table and eventually if there is an
index the index used and the condition
used to access the index right so and
this the same for for the remaining
operators so these conditions here are
called pushdown conditions so if this is
the first operator this the pushdown
condition is still select condition but
any other condition below in the plan
after the first one can be a joint
condition as well and another important
thing to notice is that each of these
plan operators has one variable attached
to it which describes the state of the
current row and the collection of the of
these variables all together collected
constitute the current state of the
pipeline so our execution is pipeline we
do not materialize intermediate results
in general except for certain cases like
block nested loop join or here join then
we would materialize only some parts but
on a general level we do have pipeline
execution and again on the general level
for the simplest case when you have just
nested of join or index nested loop
the way it works is you would basically
execute this plan via three nested loops
of course we do not unroll them as such
but just for simplicity I showed the
three nested loops so basically in this
case for each variable here roll city we
would do an index index range access
method we would retrieve while we would
make perform a look up with this
predicate for the first key in the index
that that fulfills this condition and we
will load the first key in or in in
variable city here then we would inside
the outer loop we will move on to the
next loop we would do we would use
another access method key risk index ref
that is directly woke up into into the
other index we will load the second
record here we do the same for the next
operator and once we are complete with
all operators in the contents of each of
the three rows essentially constitutes
one result row and we are ready to
return it to the client and then we move
back to the first operator and we move
to the to the next iteration of the
outermost loop and so on until we
retrieve all tuples in the result set of
this joint so that's how generally Masco
works now just as a hint there is one
data structure that corresponds to a
plan note but there are also many other
data structures that interrelated with
this one this one is called okay I'll
say that later on so there are a number
of other structures related linked to
due to this structure that all together
constitute the complete execution plan
so it is not very easy to find the plan
in the source code the representation of
a plan in the source code now how do we
the next couple of slides I will talk
how do we actually get to to optimal
plans with this representation
now the first thing is when one talks
about query optimization is what is the
cost model it is slightly more
complicated than what is on this slide
but generally as in all other databases
cost is per for is proportional to to
disk to disk access and the cost unit
currently is equal to one random rate of
a data page which is just a constant
which is not necessarily realistic the
only thing that we care about is to be
able to order plans properly and the
main cost factors in MySQL are data
statistics are we taking to account the
number of pages per table or per index
we take into account cardinality of
tables and indices we know the length of
frozen case even though quite
imprecisely there is no special
statistics on that we just do if we have
variable length columns we could just
have to do some guesswork there there is
no special statistics on that we also
have key distribution so because I've
discussed that with Yanis and he said
like in some cases you get really bad
plans sometimes the problem is if you
don't have index on a table and we
happen to use non index columns there is
no statistics on those so the only thing
we know is the number of rows and data
pages if there is no index so there we
have to do quite a lot of guesswork
unfortunately but we are going to fix
that next year sometime and another
information set of information we use is
the schema information so that whether a
column is unique or not whether it's a
primary key or not and whether the
column is a given column is nullable or
not and the simplified cost model this
is pretty much by the text book so the
cost of one axis is proportional to the
number of pages that the cost of join is
proportional to the number of pages of
the left table plus times the rows of
the left table the number of pages in
the right one and in general we also do
include CPU cost which is the disk
access cost basically the cost of one of
computing one simple predicate as the
equality or inequality comparison
is some constant defined in the in the
code is the one disk axis divided by
some constant in the code so now first
we go of MySQL for many years guest
exhaustive search and the search is
performed bottom-up so we start we with
one table plus and then we gradually
expand these plants with with bigger and
big we add more and more tables and this
is performed in depth-first search with
a depth-first search procedure and of
course well we do search the space of
all possible plans we use the principle
of optimality to prune those
intermediate plans which are more
expensive than the current mock best
plan and we also there is also some
rather convoluted heuristics which is
possible currently to switch off a noun
that uses some heuristics on key access
it's kind of hard to explain in in few
words but it's there so it also prunes
some of the plans as well so the the
overall the general faces of the
optimizer are there are four general
phases we begin with a number of logical
transformations where we do unlike
traditional things like negation
elimination conversion of outer to inner
joins equality and constraint
propagation propagation we also have a
couple of tricks to evaluate tables that
happen to be single row tables so if we
can determine that certain table access
will give us one single row then we can
retrieve that row already to
optimization time bind the the constants
of that row to the corresponding
variables and then use a constant
propagation and eliminate eventually
some predicates so this is a very neat
trick I haven't seen actually that in
other databases may be honest laws
then once we do all these logical
transformations we we have a face that
prepares for cost based optimization and
basically this face extracts yeah sorry
I didn't see you
yeah the question was if we have a moat
if we have a table that contains more
than one row are we able to predict the
weather a table actually weather the
query will actually get exactly one row
yes for example if there is a primary
key and we happen to have quality
conditions on the primary key we are
sure that there will be exactly one row
in this case we prefetch during a query
optimization and then we bind all the
constants we use constant propagation so
we may actually deduce already it could
query compilation time that this query
will return nothing right other cases
are also ability when we know that some
column is null
cannot be null and if we use the
predicate is null
right so we know there is nothing there
already so no need yeah
so sorry this yes it does the single row
replacement execute exist in 4-0 the
answer is yes it is quite old I don't
know maybe running three something I'm
not sure
though the question is is the table
locked before query optimization right
yeah it is locked I think so actually
J do you know when tables are locked
you have the the answer is the tables
are locked upon access but the thing is
perhaps when you know what I have to
check this when it comes to single table
axes I'm not sure what like this
constant table replacement I'm not sure
what happens perhaps logically one
should think that the table should be
locked and and then released all only in
the end but I have to check this out yes
the question was is does the table
handler yeah does the table yeah the
question is does the day at the table
handlers the selectivity estimate
already tell us whether there is one row
in some cases depending on the table
handler yes for example my eyes on it
may return it returns exact row count
but if it's in ODB we don't know
well we're use it we we have a we trust
my son that this is there is one role
and we will use this algorithm now there
is a special flag internally that tells
us whether well the implementer of the
table handler has to provide there is a
special flag that it should set so the
optimizer checks whether a particular
table handlers returns exact arrow
counts or not and then it will
accordingly apply this for example if
the table handler supports MVCC like you
know DB or the upcoming Falcon engine
actually don't know about welcome but
you know DB for sure does not return
exact row counts so we cannot trust it
okay I'll go ahead
so this preparation for cost based
optimization phase actually I'm also not
aware of if other systems are doing it
what we do here is that from all the
world clause we extract a sub condition
whenever possible which is a conjunction
of all equality predicates and it is
internally represent it it's it's a
graph of all these predicates and
basically on all of the arcs of these
graphs we assign some the fan out of of
each of the conditions and this loss
later on basically this is the input to
the optimizer to to estimate the
selectivity of conditions so that's one
of the things that is probably both
disadvantage and advantage during query
optimization we do not use all
conditions of the of the word clause to
estimate exactly to estimate the number
of rows in the intermediate results what
we do use we use the result of this
preparation phase and the pre estimation
of the of the Equality condition fan out
only so the next phase is cost based
optimization which uses a one very
important component the range optimizer
unfortunately I don't have time to
describe it's worth the whole talk on
its own it's a very interesting
component that we use in many cases by
this is the component that takes some a
boolean formula of range conditions and
figures out the minimum sequence of
intervals that represents this where
clause then we have the plan condition
push down face so once we have a
complete plan and we know the order of
the of the tables in the plan we can
decide what condition can be executed
where and then we have so-called like
code generation phase it essentially
sets function pointers because we do not
generate pick up now the second part I
will in the second part I will speak
about the new features from five-o so as
a logical continuation from the previous
part the next thing I'll talk about is
the greedy joint optimizer so the cost
of optimization can be pretty high as
probably some of you know if you happen
to need to optimize more than I queries
with more than ten tables it may easily
take hours or even weeks in some cases
right so in exhaustive search is not
feasible for such cases and all modern
database system do have further search
algorithms expect except for the for the
exhaustive form and that was my first
arts task at my skew L I implemented a
greedy search algorithm and the idea of
this algorithm is that one it allows you
it is not fixed or to say it allows you
to control the depth of the exhaustive
mess which we call search depth and the
general ideas are that at each search
depth search step whenever we consider
how to continue a partial plan we
estimate the potential extension only to
certain depth so basically we do not run
a full exhaustive search we pick the
best extension based on this prediction
step I call it and that's why we call it
a greedy algorithm because it picks the
best current possible extension we
forget all of the extensions and then we
continue we extend the plan continue
with the next one
so the most important thing to know is
that there is a trade-off we can get
worse plans
we get them faster much faster I will
show one graph in the end so I did some
measurements of this now this is a
visual representation of the algorithm
so suppose we start with an empty plan
and on the next step we want to pick the
first table in the plan so the way we do
it we go through each of the tables and
in this particular case I have to make
the picture image smaller I used the
minimum possible depth so suppose we
look just one step further
basically this we extend each table
which plan of one table with all
possible other tables and we estimate
the cost suppose that of all these
extensions the best the plan with the
minimum cost is T 3 and T 2 and then we
say okay our first table is T 3 because
it has the best possible extension with
we depth only 1 so we we select our plan
initial plan to be T 3 and then for T 3
we do the same for T 3 we continue and
perform exactly the same step so instead
of having a very wide tree here going
through almost every branch basically in
this case we will consider very few
combinations in this case the complexity
of this search procedure is is quadratic
so it's it's very quick but this steps
here it can be controlled so you can
make it is possible to control how many
prediction steps we do the optimizer
does there is a user variable for this
system variable for this and so
generally there are several ways you can
control the optimizer you can control
what indexes are selected like use index
what this hint would tell that please
use only this index if possible force
index says use this index even if table
scan is faster and ignore index means in
exclude those particular indices yes
these are actually the question was
whether these are index index control
features are available for one yes they
are available in for one and but for for
completeness I just made a slight cow
all possible ways to control the
optimizer now these are new features
from five-o and the first one is degree
of exhaustive nest this is basically a
way to control what I just described the
greedy optimizer zero means automatic so
the variables called optimizer search
depth zero means automatic so there is a
small procedure currently rudder dump
that decides what would be the sub steps
one is minimal as on the previous slide
62 is maximal and anything in between is
okay of course if the depth is bigger
than the number of tables it would be
exhaustive anyway because the search it
cannot search more the depth is not
bigger than this the number of tables
right and there is another variable
which is called optimizer prune level
and this is the the magic thing that I
will not describe here probably because
I don't understand it very well actually
I admit it I mean it's a very strange
thing I can if somebody's interested I
can show it where it is it's just one if
condition and there is a the third thing
also available in the older versions is
the straight joint hint and with this
Kent you can force a particular join
order and then you just directly bypass
the join optimizer this is a small graph
showing like what is the trade-off
between between between what the depths
the search depth and the the time it
takes to optimize the query of course
this is on a synthetic set of queries so
other queries may show different results
but it is quite representative so no
surprise I use here exponential axis
because otherwise it won't fit so as one
may expect the cursor generally the
dependency is exponential but I
this on a number of different machines
just to see on modern architecture
approximately how much it takes to
optimize on our with Moscow's optimizer
and basically it seems that not a good
cutoff value it is seven and that's what
automatic means currently it just checks
whether your query has more than seven
tables if it has more than seven tables
it sets the depth to seven and no more
because we know that even if the user
didn't do it manually probably is going
to take a very long time for example 10
tables up to 100 seconds so probably
nobody wants to wait 15 tables that's
going to be a very long time so that's
why I decided to set it to 7 now I'm
switching to other new things in the
query engine so these are mostly a yes
sorry
do do we have clownfish
- yes there is something called query
cache and it just compares the question
was do we have plan cache the answer is
yes we do and it called the collocation
it only contains no plan cache oh sorry
I got the question no we don't so what
happens is when you do prepare you will
run the optimize that every time even if
when you do execute you run this thing
the optimizer every time yeah of course
we have the query cache and the query
cache actually skips that entire step
clear cache stores the original query
just in plain text and the result set so
for an identical query you get your
result and generally do that for smaller
queries but you can do it for bigger
queries it is provided you have memory
for it and provided you tuned it
properly way faster than just tuning
than just storing a plan storing a plan
still means you need to do the retrieval
and this is just a step beyond it's
simpler but it's highly effective okay
there's one thing about that just a
slight thing when when Orion said that
it's the plain text of the query
something a little bit different it
actually md5 the query so even one space
difference in the Select statement one
little space that's a different query
okay so it empty files it you know to
store it in the query cache so make sure
that you're not like adding different
comments every time you put in a select
statement cuz it's gonna it's gonna be a
different query to the query cache so
yeah that's that's why I initially
understood the question that it was
about the query cache but in terms of
punishment now we partly there are some
parts which we do not redo every time
when it will execute but particularly
the journal optimizer is called every
time for a number of reasons which I can
explain later on so the the next thing I
will talk about is a new index access
method called index merge it was
implemented by one of my colleagues and
the index mark uses the the it's very
much based on the range optimizer so
this is the component that takes a where
clause and generates a minimum minimal
set of sequence of intervals that
represents this this world clause and
this is also used in the current range
for generating the currently existing
range access methods already existing
for one so let me first remind you how
does it work arrange how arranging
ex-cons working up to and including for
that one suppose we have this query with
range predicates it will go through the
range optimizer among other things we
get a set of sequence of intervals these
intervals are shipped to the query
execution and then the handler handlers
can accept in some internal
representation sequences of intervals
and then basically the handler will
retrieve all the rows in each of the
intervals given that we have a let's say
b3 index this is quite efficient way to
scan an index now the problem comes when
you Duke
disjunction of two conditions but each
of the conditions can use different
index there is no index for both
together so what we did for this case is
three different algorithms that can
efficiently use both of the indexes and
compute the result of the the overall
result so in this case we have an
equality conditional on one index on the
index country and arranged conditional
another index so what we can do in in
this case is the trick we can just use
two range scans on both indexes and
correct the row IDs of each of the two
results result in keys and put them in a
intermediate storage that computes the
only the unique values of the row IDs
once we are ready with both index scans
performed in sequence we can continue
and either and retrieve the actual rows
but there is there are two other cases
when we can do even better than this we
can there are two cases when we can even
avoid this intermediate storage if it
happens so that the raw IDs on the two
indexes we know that they are ordered we
can do something better for example if
you have a disjunction and this method
is called Union index mulch if you have
a disjunction we can just start from the
index containing the smaller row IDs
basically we check the first row IDs of
both indexes and we continue with the
one with the smaller ones we continue
until we reach like on step one until we
reach our already that is just smaller
smaller or recalled and row ID in the in
the right index if we get a bigger one
we switch to the right index and so on
so basically this is the same algorithm
as the merge step of sort merge exactly
the same and this allows us to produce
the
result of - or by using two indexes in a
streamed manner without any intermediate
materialization with those we perform a
similar algorithm for conjunction so
basically we just do the opposite we
collect or row IDs from both indexes
until they are the same then we skip all
the others then we collect the other all
with this until they are the same so we
do this in parallel on both indexes and
we're in the end once we finish with
both indexes we can just we read
actually nil on each step we can return
already one complete row of in the
result in this way so this is fully a
pipeline and there is no need for any
intermediate storage and finally we can
also the optimizer also automatically
does index superposition so we can use
these methods we can combine these
methods to to compute to merge indexes
on more complex conditions and there are
some internal limitations of the range
optimizer but generally speaking this
multiple index mode superposition is
applicable to almost any boolean
expression of range conditions such that
all range conditions are covered by some
index and then we we can compose them in
in such a way because if we know
especially if we know that the output of
of an intersection is also ordered by
row IDs we can apply also the the row ID
algorithm to do the Union and we can
superimpose in such a way the previous
algorithms so this is the basic idea
behind index modes yeah
no because debate the question was if it
is possible to do index merge on joint
predicates the answer is no at least I
already got this question yesterday I
can't think of an easy way to do that
because the whole idea here is that we
have ride this and roll it is happened
to be unique only / table so I just
don't you see that's does the whole
point to index lookup see
well but still the whole trick of the
whole trick of merging to see I mean
we're basically merging royalties over
the same table and the thing is that we
know that if we pick or skip one
particular ID in this way we identified
the row I don't see how can this be done
for two different tables you see because
if we have row ID 35 let's say they're
integers if you have row ID 35 in one
table and row it is 35 in another table
they are actually and they identifying
different tuples so we can't do anything
with that
well maybe we give us a good idea now
the next thing this is something I
implement it for five Oh is an algorithm
that we call lose index con it's a
special access method for group by and
distinct queries with minimum and
maximum this is for the ones familiar
with Oracle this is a subset of Oracle
cause index skips come so basically this
is a method that allows you to use an
index even if you don't have constants
forming a prefix on the index and here
is how it generally works suppose we
have this simple group by query that has
a maximum aggregate function in addition
to that and suppose we have an index in
this on the fields country name
population in this order and we have
group by on the first column we have an
equality predicate on the second column
and we have our minimum or maximum on
the third column so in version 4 one
what we could do it the only thing we do
is actually we do just a simple index
can basically we jump and we check each
in every single key in the index but in
500 we do something smarter so what we
do is we first jump and we identity
I suppose I started here somewhere in
the middle of the index but we do this
step for every group in the index so
what we do is we first find the the the
first row in a group for example we
start with the first key in the index
which we don't know what's inside it but
we can retrieve it so we get the first
one
then we look ah what is the value in in
the column country the group column
well it's USA and since we know that
this is an ordered index we know that
all subsequent tuples all subsequent
keys contain exactly the same value in
this column we use this fact later on so
given that we now have an equality
condition we want some constant we can
add it to the constant USA and we form a
new search tuple and now we can
immediately jump and find the tuple USA
sorento for example in this case right
so we don't need to do a scan to find
that people anymore and since this is
the first tuple with this value and the
index is ordered we know that 290 is
actually the minimum value here but the
query needs maximum so we have to do one
more one extra step we actually go and
here it did not display that but we look
for there is an a the handler API allows
one to look for the next tuple after the
tuple beginning with some value with
some constant so we jump to the first
key of the next group and then we know
that exactly the tuple before it which
is the LA is the last people in our
group so this is the tuple USA is rental
1,260 in this way we can get the maximum
value in just three lookups instead of
doing an index can and if it was if we
were looking for the minimum value we
would need to look up spur group if we
just need to find the group we do one
lookup only right so so this since group
distinct is equivalent to to group by we
apply the same trick for distinct query
and I heard from customers some
customers got up 200 times performance
improvement it was really nice and there
is some rules of thumb the logic of this
access method is exactly the opposite so
if you're designing your indexes the
logic is the opposite as you would do
for range access methods there you need
the most selective index here you need
the least selective index because the
bigger the groups the fewer the jumps
right if you have only one big group you
will do three jumps per whole table if
you have smaller groups you have more
jumps if your groups are just one if you
have an unique index this this basically
doesn't give you anything because it is
the same as doing an index come and the
general rules to apply this method is
that the index must cover all fields in
this order first you have the group by
fields equality condition fields range
access methods fields and then in the
end minimum and maximum fields and all
these fields have to be covered by the
index so five minutes then ten more to
go
now the next thing is partition pruning
that's a new feature from 5.1 the
problem is that how many of you know
that my skill has partitioning in
general table partitioning except Yanis
and okay so my school does have table
partitioning since version 5 one yeah
five one seven six okay anyways five one
something we do have table partitioning
and it is pretty much done by the
standard so for the ones familiar with
table partitioning on syntactical level
it is it is the same now what we did in
the optimizer group is we added a
feature that allows one to two to
logically at already at compilation
at compilation time to figure out that
it is possible not to to access certain
partitions and we call that partition
pruning so the problem we're solving is
given a query over some partition table
and give all the DDL that describes that
the partitioning for example we have
arranged partitioning and we we use we
match the DDL and they were close and we
find a subset of all partitions that
that actually need to be accessed and we
and we throw away the partitions that do
not need to be accessed so there is a
small example here suppose we have a
hash partitioning on column Department
number and therefore partitions that
basically in this case we would use the
modular function to do to compute the
hash and if you have the query select
star from EMP where Department number is
1 or 2 one would one can see that
essentially we don't need to access two
of the partitions because we already
know that this query will need only two
partitions to test it is not possible to
find the result in any of the other
partitions and if you don't explain
partitions on this statement you will
see the result is that actually only two
partitions remain p1 and p2 and how this
happens we first that's it this is
actually a bit tricky to explain ok I'll
give it a try so now what we do is we
create a virtual table definition from
the partitioning description so
basically we pick all the fields
described specified in the partition and
internally we create a virtual like
another virtual table description and
then we use the range optimizer which
can analyze range conditions over any
table so we pick the world clause with
its range conditions and we pick this
virtual table and we feed them to the
range optimizer as a result we get the
sequence of ranges and so this is a very
good case of reuse of one of our
smartest components and once we get this
interval sequence we walk over the
interval sequence and over the
partitions disk over the partitions
which are essentially ranges as well and
we perform logically on intersection of
those and we throw away the partitions
that have empty intersection with the
range with the range with the range
sequence with the ranges in the range
sequence so visually described suppose
we have one range of all of many ranges
suppose it's one from one to five well
let's it's strange a B there can be many
ranges this is what couples for one
range we pick this range and and we run
all all the partitions and compare each
one the boundaries of each one with the
range and if the range does not overlap
with the partition we throw we throw
that partition away
so this is the general idea of partition
pruning now there is some some more
about the internals which I will skip
for lack of time I will just mention
that we do have equality propagation as
most other databases do I suppose like
database people are quite familiar with
this so I will skip this the benefits of
this is that we can generate
automatically more more conditions to
use for index access we also we also use
this for equi-join transitive closure so
we can deduce more join equality joint
conditions we can generate more
predicates to use for index range index
access and we also have some new
features related to two nested outer
join so basically we can we have a very
unique thing unlike Firebird and
Postgres in MySQL if you nest if you
have nested outer joins the intermediate
results of the nested joints are not are
not materialized we have fully pipeline
to execution so nothing is materialized
and I think that's quite quite unique
probably commercial databases have that
as well I'll ask
Yanis again later on and we also do
transform outer joints into inner joins
whenever there are no rejecting
predicates on the inner table so if we
know that there cannot be a no we just
transform the out the outer join to an
inner join and this allows us to - this
expands the search space for the
optimizer so it can actually explore
more different join orders and finally
what's coming soon some of these things
some of these features will make it in
5.2 some will not I do not take I do not
make any promises specifically so the
the major feature that has been asked
for a long time from the optimizer team
is sub query optimizations and what
we'll do is we will apply sub query
optimization
two queries in sub-queries in the where
clause typically in exists and similar
sub queries and will flatten down
through semi joints and in the cases
when it cannot this cannot be done for
example if the sub query contains group
by has a group by then we will use hash
semi joints to to efficiently compute in
predicates and equality predicates we
have another access method new axis
method which is called batch multi read
range which I will not describe now but
basically it will it is able to process
many many range many range predicates in
a batch and it reduces for example for
many big cluster it reduces network
access time and another big feature this
is something I am working on is here
join currently it will be implemented
through so called one pass algorithm
basically will if if if a table being
cash doesn't fit in main memory we will
not partition it we will just not apply
her join the next step would be to also
partition tables bigger than joint
memory available and finally we will
implement also loose index can which is
similar circles index skips can we will
implement it for more general cases with
equality conditions and that concludes
my talk I hope it was interesting
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>