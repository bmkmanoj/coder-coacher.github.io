<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2016: Need for Speed - Accelerate Automation Tests From 3 Hours to 3 Minutes | Coder Coacher - Coaching Coders</title><meta content="GTAC 2016: Need for Speed - Accelerate Automation Tests From 3 Hours to 3 Minutes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2016: Need for Speed - Accelerate Automation Tests From 3 Hours to 3 Minutes</b></h2><h5 class="post__date">2016-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/khSsjjg2eSQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">keeping on schedule with lightning talks
next we have a manual yep hi so I'm the
last one standing between you and the
lunch so as you know in Google is value
food a lot so I'm gonna try to be as
quick as possible
so 3 hours to 3 minutes I usually give
this presentation for about 40 minutes
and now I have to eat my own dog food
and do it for about 10-12 so let's start
so what is this all about
we had a bunch of API tests and we used
to run them every night it took three
hours to run them and now they run for
three minutes so this is the actual
scale 60 times faster this white dot for
you guys at the back this is just three
again so that's the actual scale so how
did did we do that so now is it comes
before and after slight like those
infomercials so this is the actual
screenshot of the Jenkins as you can see
three hours three hours the last one
here is four hours and 40 minutes
and we used to run them every night and
what would happen in the morning we
usually we're gonna have to play
whack-a-mole find the developers
responsible for the breakage and it was
hard cause you go to the first developer
and what did you do yesterday he would
say I don't remember I got drunk last
night
don't ask me I was the other guy go to
the other guy he'd say I fell in love
with this girl I don't know I don't
remember so so and it was really tough
and so we said okay this enough of this
 so we set a goal let's run all
of that for 43 minutes so whitey man is
basically the idea was with
comprehensive feedback after every code
change to the developers and after every
code change you run all the tests that
you have no more smoke tests no more
nightly runs and so forth so this is the
situation now so two minutes two minutes
this a little bit more than three
minutes and
you want to see some numbers today and
the numbers are not gonna be meaningful
to you they mean to us but not to you
and I'm gonna see some techniques that
are not brand new there is no magic
fairy dust but it's about continuous
improvement this is what we all have in
common all of us are unique snowflakes
unique challenges in our companies but
if you set up a goal and take small
incremental steps to achieve this goal
put the time in the effort you will
achieve some amazing results so this is
how our journey so the first thing that
we did is dedicated environment all the
tests were running on environment that
was shared with the developers that we
used to put their code together before
pushing it on production we didn't have
a dedicated environment so the first
thing that we did run everything in
dedicated environment only for automated
tests so this is how we start 180
minutes by having a dedicated
environment we were down to about an
hour of the execution time so the next
thing that we had to do is databases we
were still using the shared database
because it contains a lot of data useful
for those tests but this database was
heavily used by bad jobs and some other
and stuff that happened during the night
so figure out if we want to really fast
this we need to have a separate database
but this was a problem because we had to
rework most of our tests to insert the
data that they need so we had to
reengineer the framework it might sound
a little bit crazy but this is what
happens right now before every test case
so we call a bunch of api's and we call
and we manually if we don't have access
to the API to manually change something
in the databases or add some rows just
to just to setup the playing field just
to do a setup and then and then in the
real real test case starts before that
it's only set up but having able being
able to do that means that now we can
run the tests on empty database on phone
dirty on the integration on stage we
don't run those in production
risky but you get idea basically we are
free to use every database
so we've saved about 30 minutes with
that so next thing external dependencies
we what we do is software as a service
we collect a lot of data from the
biggest social networks and even sorry
Google+ also there okay so so the idea
was that all the test cases had in one
way or another connecting to external
service and if this service was down say
Twitter was down and the test case were
failing and this didn't had anything to
do with with developer pushing the new
code so we wanted to basically stop fake
simulate and no pick pick whatever you
like those with something that we we
control so that now I believe Google
calls this chromatic service like you
don't have access to the outside world
you it's everything is wrapped up and
doesn't leave your network so this is
the I don't know if you see the font
this is the landscape as of Sprint of
this year a bunch of open source tools
why mocks OPI mock server still be 4G
that we evaluated but we had some
specific requirements like being able to
act as a transparent SL proxy basically
we wanted to monitor all the connections
and figure out which you want to fake
and which we want to pass to the real
service I want to be able to return some
real binary data have some local storage
nine responses etc and none of those
would fit the bill so we didn't want to
do it but we had to develop our own to
to RSP scratch our own each
it's called project Negro is gonna be
open source really soon we're waiting
for some external stuff to happen it's
really cool and you'll be able to check
it out
I think in a month or so so by having by
have by stepping all the external
dependencies faking HTTP traffic
we didn't gain much in terms of speed
but we gave a lot in terms of stable
tests for non flaky tests for us the
biggest reason for flaky tests was the
external services and sometimes doing
the Internet is down or slow but now we
control everything so moving to
containers we didn't want to do that
initially but we had this big monolithic
application and developers decided that
it's a wise idea to start splitting this
how big money to a smaller services and
we basically had to follow suit but we
didn't have enough servers to mimic what
the producing environment looks like so
we had to use containers and we use
docker so what we did was created one
service one container and when we run
all the tests we were actually in a big
surprise the execution time glue yeah
and we started scratching our heads and
you see I did a lot of scratching and we
figure out what was the problem was we
use docker so as you know it uses
layered file system and it's really bad
if you have high disk i/o operations so
we measure that for our case it was
about three times as low die over
performance than using real file system
so our biggest user of file system IO
was most databases and so figure out ok
let's run the databases in memory so to
see how how did you go because we don't
need the data it's only fake data we run
everything in memory now so we're back
to where we were started basically but
at least during Edwards so so next
don't remove this test data what we used
to have a logic in the framework back
when we were using the shared testing
environment each test case would create
the data and it didn't at the end it
would delete the test data so that we
don't pollute
the whole environment when we do manual
tests but now we did not need that
anymore because we have dedicated
environment and the thing is when you
stop the container just data evaporate
so there is no need for this heavy logic
to do that kind of search and deletion
so we are down to about four six minutes
and it's not much but every little bit
helps when you're up at that point um so
the next thing that we did was the
obvious thing that usually everyone
starts with so run run stuff in parallel
but according to our experience this
should be one of the last things that
that you do our own stuff in parallel
when you run them in parallel and you
get variation in execution time so you
don't know if it's a normal variation or
it's something caused by you know you
trying to improve the system so try to
explain all you other options and then
try to run in parallel that's the actual
graph of you know the trace that we run
tests in and the execution time each one
took as you can see it's easy to see the
optimum value is 10 threads for this
this is Hardware specific so if we add
more memory and CPU it may shift to the
right but we run those in 10 threads and
so the execution time was about 5 5
minutes still a little bit of our
initial go of 3 minutes so we had to do
something else and it turned out
equalizing workload it turned out that
the software that we used to run all the
tests the driver it did not do a good
job to separate the test cases equally
into into each thread so as you can see
thread number 3 had about 130 test cases
in it so we everyone had to wait for a
thread number 3 while the other were
idle so we had to manually who'd reduce
the workload so this is the picture
after and it's not perfect but it did
the job so we were down to yes so we're
down to about 3 minutes so this is how
the whole picture looks like the actual
results
or even a little bit better and so
that's that's a screenshot and they took
- two minutes in 15 seconds but this
doesn't make for a good presentation
later it's month three hours to two
minutes in 15 seconds so stick to three
hours to three minutes and at this point
we have exhausted our options to
optimize video software so only option
was now to go after the hardware so a
couple of months ago we upgraded
basically wants to drive in the CPU the
machine that we run and now the test run
for one minute and three eight seconds
without doing anything else
this one's on single machine one single
server if we want to go below the
one-minute mark and this is what right
now what we are doing it's super easy
right now to scale horizontally and so
this is our current option this is what
we are gonna do aiming for one minute
execution time just for comparison our
unit tests run for about 45 seconds
about 8,000 of them but they don't touch
the database the network and system
those guys do so we so this we have high
level tests these are the most of the
complaints that you get you know
compared to two unit tests everything
else is slow so we accidentally
addressed all the issues that we have
with high API and UI level tests so
there there are there are no slow now
they run for three minutes and we aiming
for less than one minute they're not
unreliable like I said we control all
the dependencies and no one else uses
those machines so they only dedicated
for for for fast tests the entropy is
you know lower this is a little bit
harder cannot pinpoint the problem
exactly
sometimes when when your tests fail high
level test you get internal server error
and you don't know you look if you get a
stack trace most of the times you just
get internal error or something happens
somewhere so it's really hard to figure
out exactly what but
having so much faster tests actually
change the behavior of the developers
now they start to getting used to this
fast feedback and before they used to
push their code once a week from once
with two to three days a week to once a
day to every hour and when they push
three lines of code and and the tests
fail then it's pretty obvious which of
the three lines is is a problematic one
or so by having super cheap and fast
reliable test run so we actually run
everything on every commit like we run
static code analysis from winters when
I'm custom tools unit testing and in
those days so there there is nothing
else to run after each commit we are
able to push on production or a continue
to manually test if we like so and those
are no longer problems for us yeah so
what we think is that in the future this
is what we should aim for to be able to
run all of your tests know not only the
unit tests but everything for no more
than three minutes we think that this is
going to become a standard development
practice the way right now we put our
code in version control system and we
use CI so this a couple of years ago
this was I consider the crazy practice
but what you want to do is keep
developers in the flow what happens if
you have a pipeline that it takes 15 20
minutes to run all your test so
developer pushes their code and what
happens is they go they while they're
waiting they go to check their Google+
said no one ever
Facebook page they go to make a coffee
make a sandwich they check their you
know tinder app Grindr where everything
is and when they come back you know 15
20 minutes later they have forgotten all
that stuff no built failed what did I do
you know the idea is to keep everyone in
the flow okay so keep keep them in the
flow that's my last slide two books to
help you with your journey
the first is the goal by le Hao Goldratt
introduces the Theory of Constraints so
basically we have one big bottleneck he
worked on this bottleneck and then you
have additional bottlenecks but they are
not visible until we fix the bigger
bottleneck
it's basically reads as a business novel
the next one is Toyota kata by Mike
rudder introduces continuous improvement
kata so we set up a goal and take the
small PDCA cycles by dr. Deming want to
check act and you take every day you
take small steps and small steps and
when you you know hit the problem take a
step back and the way we did with in all
the containers so that's all I had to
say
excellent thanks Emmanuel and I thought
my comments on JavaScript were harsh
Google+ burn ouch hope they didn't do a
parking lot LEP therapy at Google I
think you're okay probably have time for
one question how do you ensure your API
stub stay current if and when the stub
API changes yeah that's very common
question so so three things the first
thing is that we do say Facebook or
Twitter API send they announced a new
version and we initially don't switch
for us to this version we switch only
internally to using the save version 2.8
and if we detect some problems with the
stubs we adjust the stubs we have time
to do that and when we are all set up
and and we think we're ready in testing
then we push the production then we use
two other things we also do extensive
monitoring so if we monitor some
problems we were just disturbs we also
30 thing that we do is we do a lot of
contract tests we basically pink
Facebook or Twitter or Google+ for do we
is this a JSON that we expect to be so
every 10 minutes or less we do those
kind of stakes tests and we did not use
to do those kind of tests but what
happens is that Facebook at one point
changed a little bit in their JSON
responses they added additional field
just for one day and it caused us
thousands of dollars of losses for us
because we didn't handle this case so we
learned our lesson and we use comfort
tests to check if what they return is
really what they said they would return
all right very good another round of
applause
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>