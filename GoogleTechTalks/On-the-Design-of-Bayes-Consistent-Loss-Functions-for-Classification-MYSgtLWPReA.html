<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>On the Design of Bayes Consistent Loss Functions for Classification | Coder Coacher - Coaching Coders</title><meta content="On the Design of Bayes Consistent Loss Functions for Classification - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>On the Design of Bayes Consistent Loss Functions for Classification</b></h2><h5 class="post__date">2010-09-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MYSgtLWPReA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I met him actually last month's at cvpr
where he gave a very interesting talk
and I thought it would be great if you
share some of his work with us here
google and just to spell his full name
so it's hamed maserati Shirazi and he
works that was in the middle of his PhD
at UC San Diego with sonu know and the
Department for a statistical visual
computing and jamboree I'm pleased to
hear about on the design of Bayes
consistent loss functions for
classification so thank you for the
introduction yeah so the title is on the
design of Bayes consistent loss
functions for classification and this is
work that I've done at the statistical
visual computing lab at the University
of California San Diego so during this
talk we're going to basically be talking
about classification many classification
algorithms some of the most successful
ones such as SVM's and boosting are
basically based on the minimizing an
expected value of a loss function fee
which is excuse me can you focus on the
sourcing and Bayes consistent so hello
such loss functions assign a large
penalty points with negative margin
thank you and they generally assign a
small penalty two points of small
positive margin and zero or close to
zero penalty two points of large
positive margin so we also need to talk
about Bayes consistent loss functions
and like I said Bayes consistent loss
functions are very special and important
for classification they and the reason
for that is because they produce the
Bayes optimal decision rule without
having to know anything about the
distribution of the data so an example
for a general Bayes consistent loss
function fee we would take the it we
would take the expected loss to find the
risk and we would minimize the risk to
find our predictor f star and given that
our loss function was Bayes consistent
were guaranteed that f star our
predictor is Bayes optimal it implements
the bays decision rule and you can see
that through this progression we didn't
make any assumptions about the model of
the data we didn't know the distribution
of the data nothing like that but yet we
were still able to arrive at the Bayes
optimal decision rule so on the other
hand unfortunately only a handful of
Bayes consistent loss functions were
previously known in the literature some
examples include the well-known least
squares loss that's used all over the
place another example is the hinge loss
that's used in SVM classifiers or the
exponential loss that's used in boosting
another example is the log loss that's
used in a logistic regression and also
in boosting and you can see that each
one of these previously known Bayes
consistent loss functions has led to a
highly successful classification
algorithm we've plotted
actually these uh based consistent loss
functions and just by looking at them
you can see that they all generally have
a few properties in common one is that
they're all convex loss functions and
the ones that are considered here are
are all unbounded for large negative
margins and number three is that they
all have zero or close to zero positive
margins of course this excludes the
least squares that increases on both
sides and the last property is that they
they are all costs and sensitive meaning
that they assign the same amount of loss
to both the positive class for and the
negative class so they don't prefer one
class over the other so the natural
question that we could ask at this point
is are we stuck with just these four or
five loss functions with those common
shapes and can we actually derive other
Bayes consistent loss functions that are
custom tailored for our application can
we derive Bayes consistent loss
functions that have other shapes and
other properties for example can we have
a Bayes consistent loss function that's
non convex like the plotted loss
function can we have a loss function
that's based consistent and yet has
bounded negative margins as you can see
this loss function flattens out can we
have a loss function that has nonzero
bounded positive margins unlike previous
loss functions give me have a loss
function that's based consistent and is
cost sensitive so here we have two loss
functions one is assigned to the
positive class and then and the other is
assigned to the negative class and in
general can we have a base consistent
loss function that's just any shape we
want it to be given our application
so these problems and questions were
previously known and researchers
generally would try to start off with a
known based consistent loss function and
in order to alter it to their
application they would either make
alterations to the loss directly or
alter the classification algorithm
derived from the loss and the problem
with that is that once you actually
alter the Bayes consistent loss function
in any way it ceases to be based
consistent so during this talk what
we're going to show is how to actually
derive novel based consistent loss
functions and this is work that has been
covered in our nips 08 paper we're also
going to talk about designing novel loss
functions based consistent loss
functions for specific applications and
two of the applications that will cover
during this talk the first one is going
to be the designing loss functions that
are robust or outlier resistant for
robust classification and this is work
that's been covered in our cvpr 2010
paper and we're also going to talk about
designing loss functions that are base
consistent and cost sensitive so this is
work that's been covered in our ICML
2010 and I Triple E pammy 2010 paper so
now that we know the importance of
having Bayes consistent loss functions
for classification we can talk about
some specific problems and applications
an interesting problem is the computer
vision problem here we have large margin
loss functions generally don't overcome
the unique challenges posed by computer
vision problems and one major difficulty
is the prevalence of noise and outliers
or class ambiguity that exists in loss
in computer vision data sets so here for
example we have an image of a famous
street in 10
francisco and this is clearly an image
of a street but if we were to do patch
based image classification on this we
would get many patches that include
trees and cars and building even though
this is supposed to be classified as an
image of a street so in order to be able
to do robust classification we're going
to try to design a robust loss function
one limitation of current loss functions
is there unbounded negative margin so
for this simple classification problem
we have at the top we clearly have three
outliers those are the three X's the
green axis to the left are obviously
outliers and if we were to use the
well-known exponential loss to classify
this problem the exponential loss has
been plotted in light blue the light
blue dotted line and because of its
exponential shape it basically will
emphasize and concentrate on these three
outliers and assigned to them and
exponentially high loss whereas what
we'd really like it to do is to
disregard to these three outliers so
recent improvements have been made in
terms of dealing with this type of
outliers one is the linearly growing
logit loss used in logic boost
introduced by friedman in two thousand
and that's the green line you can see
that it grows linearly instead of
exponentially so that's an improvement
the other is the bounded savage loss
used in savage boost that we introduced
in 08 and that's the dotted blue line in
the bottom you can see that it's
actually bounded it flattens out after a
while and so these help with these type
of outliers but we argue that the
negative margins are not the entire
problem and that it's also
important to penalize large positive
margins so in order to explain this
further we can consider this simple
classification algorithm I mean this
simple classification problem we have
where the data is distributed uniformly
in the vertical direction and is
Gaussian with equal variance and
different meanings of me of plus 3 and
negative 3 in the horizontal direction
so for these two classes it's easy to
see that the base decision rule would be
a line at x equals 0 and in fact if any
of the previous Bayes consistent loss
functions are used on this exact data
set they would all place their decision
boundaries where the red line is which
is very close to the black based
decision rule so that all the previous
Baeza this is base consistent loss
functions will approximate the base
decision rule almost exactly I didn't
get the question was that come to this
it yet took the fact that I know that
the optimal basis region rule is the X
line is because I know that their
Gaussian but if all I had were these few
points and i used the Bayes consistent
loss function any one of these loss
functions that I have on the right they
would in actually for this data set give
you the red line which is almost on top
of the black line which is the optimum
so even if you didn't know anything
about this distribution and all I gave
you were these few points and you were
to follow the procedure of start off
with a Bayes consistent loss function
compute the risk minimize the risk you
would get the red line without any
information about the distribution so
that's the power of Bayes consistent
loss functions
I'm not assuming anything about the data
whatsoever all i have is just the data i
know that the blue lines is one class
and the blue circles is one class and
the red x's are another class given
their coordinates I can but these points
were actually sampled from a Gaussian
which I do not know what so ever
sure sure sure four and I'm gonna
actually consider what happens one when
you have a you know a rogue point or
something like that it could actually
change immediately so what I what I'm
going to do next is actually introduce
an outlier here so let's say I had one
sample point at that point so the impact
of adding a single X at that point which
could still happen as part of the data
right because it's Gaussian in the
horizontal so that could be one of the
exes that come out the impact but in
essence it is kind of an outlier for
this data because it's highly unlikely
but in any case if you do have a point
that negative 2 and 0 what would happen
is that all of these previous loss
functions would drastically shift their
decision boundary to wear the purple
line is and they would place their
decision boundary at about x equals
negative 2 point 3 and the reason for
that is because all of these loss
functions assigning zero or close to
zero loss for being correct right this
includes both the savage loss which is
the green one or the sorry the log is
the green one and the savage is the blue
one so you can see that they assigned 0
penalty for being correct and since by
placing the line at negative two point
three you can classify everything
correctly that's just where they're
going to put their boundary which is
very far away from the Bayes optimal had
we known the true distribution on the
other hand the tangent loss which is a
loss function that we're going to derive
during this talk is also based
consistent but since it pen Eliza's
being correct it penalized is both the
positive side and the negative side it
discourages solutions that are to
correct so it'll actually place the
decision boundary where the red line is
given this exact data which is still
very close to the base decision
yeah now I'm not assuming anything other
than just the data I mean if
yeah yeah that's true if there was
another let's say another you know I'm
let's say it was like two gaussians all
of them consist made up the red class
that's true that red point could be one
sample from that but the problem is that
right now given the fact that it's just
one point it's still considered kind of
an outlier I will actually deal with
this problem later on because once you
derive the tangent loss you can you can
actually change the amount that that it
pen Eliza's being to correct so you
could even actually you know flatten
this to make it look like something like
the savage loss which wouldn't penalize
it and actually place the decision
boundary very close to where the purple
one is so you can have a control over
this it's just a matter of given the
data it's right now I mean just looking
at it it would seem better to place the
line somewhere close to the black one
rather than drastically move it all the
way to the left even if that were a
distribution and what actually when I
was building this you know toy example
to get the point across of why it's
important to sometimes penalize being
correct if you actually add another X to
wear that far left one is the tangent
loss will also place its boundary close
to negative two point three because now
it's you know there's too much penalty
for classifying two points wrong and you
would expect that if that was a true
another mode you would have more
examples from it and once you actually
add another one it'll shift it'll be
just too costly it doesn't want to make
a mistake on two points so yeah yeah but
I mean this toy example is just to get
across the importance of actually
penalizing you know outliers that that
are of this type
so you have to in order to deal with
these you have to kind of penalize being
to correct sometimes as well so I don't
know if I yeah yeah
yeah that's true I mean that's why you
yeah a true but that's usually what we
have to I mean that's what we deal with
in the real world we always have only a
few samples we never have the true
distribution of the data and so I mean
the purpose of this talk is to say that
it's important to have Bayes consistent
loss function that gives you that
flexibility and that control so when you
are dealing with situations where you
have small amounts of data and the
possibility of having outliers is
increased in that case you want to be
able to you know have loss functions
that are not only based consistent but
also have you know these different types
of shapes that can deal with these
potential problems
so I actually if you were to use the
hinge loss which is what the SVM does I
didn't I didn't run an SVM because this
is so simple you can solve it directly
you can just like do a exhaustive search
over the space of lines here but if you
were to use the hinge loss which is what
the SVM uses it does still place you
know because there's no penalty for
being right so why not place it where
the purple one
mmm yeah I didn't do a pass like that on
this yeah uh-huh
the square loss the square loss the
least-squares loss is actually almost
never used in classification because let
me go back to its shape that purple line
or give you a better example so the red
line is a square loss right and you can
see that it penalize is being correct
almost as much as I penalize is being
wrong because of its symmetry it's just
x squared so it's never used for
classification if I were to use I
actually that's an interesting question
I maybe I should have used it here and
see what would have happened if I had
actually used the square loss because
then it would penalize this as well and
let's see if it pen Eliza's that point
so actually the square loss wouldn't
work it would it would place the line at
where the purple is I think I'm kind of
guessing because there's going to be
such a huge penalty for getting this
wrong that it would prefer to get it
right rather than skip over it so the
tangent loss here assigns a bounded
penalty for being right that's why it
can disregard to this outlier you see
that it's kind of you see that it tapers
off for any incorrectly classified point
of that sense
yet so exactly that's the whole point of
a loss like this because it wants to you
know keep once actually in the paper I
actually have an example where I've
plotted like a histogram of the distance
this is kind of the margin so it
actually encourages small margins that
are placed at that plate you know at I
think it's that dip that minimum is like
0.5 but the interesting thing is that
when you see that example even though
most of the points have a small margin
of about point 5 meaning that that
minimum is actually enforced it does
better in classifying data set that has
lots of outliers so we were I think in
that in that example we had some digit
classification problem and the margin is
smaller but the classification output is
better than all of these other loss
functions so that's a that's a problem
that we actually considered in the paper
so did I that it's just because the
margin is smaller doesn't necessarily
mean you're doing worse especially when
you have outliers so that's true it's
just them how much for example when
you're using the exponential loss it
tries so hard to increase the margin
that it you know screws up the
classification which is the goal so
for the I wanted to give these I know
what you're saying but that's that's
regression it's not classification
because in regression in classification
you actually have the two classes and
you're trying to minimize the risk and
in regression yeah if you allowed it to
you know plot a line through here that
that would happen it would place a
horizontal line so
what I think I did for this data set was
I see I think I limited the I didn't
yeah I didn't maybe I didn't want that
to happen so I limited to vertical lines
it's might have been what I did
so if i remember correctly that's
probably what I did to avoid that in any
case I haven't even considered the least
squares so yeah
I i think what i considered was like
from minus ten to plus ten something
like that
I wanted to put a why dimension here
just so it with the look nice when I
plot it because if I only had one that's
why their uniform in the Y direction so
I already know that the Y Direction is
you know useless yeah because if I had
only plotted these points in the X
direction you would get these circles on
top of each other you couldn't really
see anything but that's why i said it's
uniform in the vertical so you can
disregard it completely yeah but the
point of this toy problem is something
completely different you guys should let
me go further what I wanted to just get
across was the idea of the fact that
some first of all we have problems where
we have outliers and second of all we
want to deal with outliers and there's
two two types of outliers in a sense one
is when you're wrong and so one is these
type of outliers that I considered here
right there's no and the other is that
when you can be actually right
completely right and still have you know
an outlier so the the savage and the log
kind of deal with the first type but you
also want a loss step looks like this
that can you know penalize being too
correct right when you have outliers
that's kind of that's just to get across
the point of having a loss function that
has a shape like this now of course
we'll go into more you know advanced in
practical example so don't worry but in
order to derive something like the
tangent loss that I just talked about we
need to talk a bit about risk
minimization so we define a feature X
and the class label Y and and we call
our classifier H and our predictor f so
our classifier is just going to be the
sign of our predictor f and we also have
the loss function
well and so what we're going to be doing
is minimizing the risk or the expected
loss which is equivalent to minimizing
the conditional risk and here we're
going to define etta to be the
probability of one given X so with these
in mind we will consider the traditional
approach of dealing with a Bayes
consistent loss functions work and
actually talk about what what does base
consistency mean in more detail so a
good example is the exponential loss
that we have here and the traditional
approach is to of course take the expect
take the expectation to find the risk
see and then you want to minimize the
risk and find your predictor f star and
for the exponential you can actually
show that F star will be one half log of
the probability ratio so the the
question now is to see whether our
predictor is based consistent and we've
plotted this f star and you can see that
it is indeed Bayes consistent because it
assigns positive values to probabilities
that are higher than a half and negative
values two probabilities etta that are
less than a half so it's implementing
the base decision rule which is good so
we're good there we take our f star and
we pipe and we plug it back into our
risk to find our minimum risk which we
call c star and then what we have to do
here is check to see whether c star is
convex or not and the reason for that is
because it can be shown that in order to
have a convex optimization problem you
need to have a minimum conditional risk
c star that's convex so for the
exponential loss where everything works
out fine but you can immediately see
from this example that this traditional
approach doesn't actually let you design
Bayes consistent loss functions it only
lets you check to see whether a loss
function is based consistent turn
we can kind of say that we got lucky
with the exponential because all these
things you know fall into place so in
our next section we'd like to talk about
how can we actually we're going to
propose a new path for designing based
consistent loss functions and the
general approach is going to be that we
can choose any strictly concave function
C star and any invertible function f
star such that they have the simple
symmetric constraints and we plug these
two functions into this formula and we
get a fee a loss that is guaranteed to
be based consistent so this gives us a
principled way of deriving designing
novel based consistent loss functions
now i'm not going to actually go over
the proof of that formula so if the
proof can be found in our nips of white
paper and it basically deals with
probability elicitation and Bregman
divergences and so I'm going to kind of
skip over these but we're going to go
over an example of actually deriving
using the formula and deriving a novel
based consistent loss function here
we're going to derive the savage loss so
let's say we start off with the logistic
link function that we actually saw in
the exponential loss as well and we pair
it with the least squares minimum risk
and these two satisfy the symmetric
constraints all we have to do is take
these and put them into the formula and
we derive a novel loss that we called
the savage Los and we've plotted it and
you can see that this loss function
unlike all previous Bayes consistent
loss functions is first of all not
convex and also
is bounded so it it flattens out it has
a bounded penalty and we use this loss
function to do some robust
classification but for a truly robust
loss function we saw in our previous
discussion that for a hypothetical loss
we it should have a certain number of
properties we would first of all like it
to be bounded for large negative margins
and we'd also like it to be have a
smaller bounded penalty assign a smaller
bounded penalty for large positive
margins and we'd also like it to be
margin enforcing of course and it can be
shown that under base consistency those
three properties are satisfied if and
only if these three conditions or
requirements hold for our choice of F
star and C star and unfortunately
existing link functions f star don't
comply with those requirements so we
introduced the tangent link which does
so this is the plot of the tangent link
and we pair it up with the least squares
of minimum conditional risk again if I
get into a formula and we get what we
call the tangent loss which actually has
the desired shape of course so now that
we have our loss function we can easily
use it to do classification and derive
like for example of a boosting algorithm
to minimize the risk associated with
this loss I'm not going to go over
deriving the boosting and algorithm and
stuff you the details can be found in
the paper I'll just go over some
examples and experiments that we did
you've done experiments on image
classification and also multiple
instance learning data sets right here
I'm going to talk about the scene
classification the 15 class scene
classification problem
we used the semantic space
representation from razi vasya and
vasconcelos cvpr 09 and the only thing
that we changed was replace the svm
classifier with the tangent lost
classifier and we got for now they're
seen classification method was already
state-of-the-art but we got four percent
improvement with just simply a you know
an outlier resistant or a robust
classifier instead of the SVM so and
actually the interesting thing about
this data set is that most of the
improvements were seen between classes
that were to lose my voice what was that
yeah we're doing one versus all the most
improvement was seen in between classes
that are easily confusable for example
when you're trying to distinguish
between the highway class or the street
class or the tall building class or when
you're trying to distinguish between the
mountain and forest and open country
class these are actually one of them
like these are some of the 15 classes
out there there's also living room and
bedroom and kitchen and these are
similar but they actually produce
produce the most improvements which is
expected when you're doing robust
classification you've also done
experiments on tracking and we use the
discriminant salience II tracker of
Mahadeva and Wes cansado cvpr 09 and we
use tangent boost to combine the
salience II maps in a discriminant
manner here in these two examples
tracking examples the red box is for
when the tangent loss is used to do
tracking and the dotted blue is when the
exponential loss which is in
robust is used this one let's see if it
works nope still doesn't work on my
computer that's weird but okay this one
should work so you can see that the
tangent loss keeps track of the object
whereas the exponential loss drifts off
and loses track and this is because the
person keeps changing shape and there
are plenty of outliers so it's much more
robust now a natural question that was
already kind of asked was can we control
the shape of the tangent loss so the
there is actually a parameter a that
controls the ratio between the negative
margin and positive margin so the
general formula is includes the a and
we've plotted the tangent loss for
different values of a and you can see
that it you can actually either actually
best results are found through cross
validation for a it or you can kind of
like guess a depending on how much
outliers you expect to have in your data
but in general you can kind of you can
control the amount of penalty that you
assign to them are the different margins
so you have that flexibility another
question is the question of convexity so
here we have examples of a convex
function a quasi convex function and a
function that's neither convex nor quasi
convex and so for quasi a quasi convex
function isn't convex but it still has a
unique minimum on the other hand all
convex functions are also quasi convex
and we can actually prove that base
consistent loss functions from this
formula are all quasi convex now the
question the main question is is our
optimization problem still convex even
when
the loss is not convex so if so the
thing to keep in mind here is that we're
not minimizing the loss we're minimizing
the expected loss or the risk so with
that in mind we can prove that the risk
is actually still convex in probability
space even though the loss is not you
can also prove that the risk is quasi
convex in functional space and this is
an important proof because when you're
doing classification you're usually not
working in probability space you're
working directly in the functional space
trying to directly find the function
that classifies that minimizes your risk
so it still quasi convex even though the
loss is not and also more importantly
for practice the empirical risk is also
quasi convex and functional space so in
conclusion we can say that the
optimization problem is quasi convex and
has a unique minimum that can be found
with functional gradient descent such as
boosting even though the loss function
is not necessarily convex so the the
optimization is not by definition convex
but it's quasi convex meaning that it
still has a unique minimum so you can
use quasi convex methods to find the
minimum the easiest thing that works is
just do functional gradient descent
which is what boosting does that's why
the boosting algorithms work even though
these loss functions have all these you
know quasi convex shapes so the
important thing is that it still has a
unique minimum
no no no that's what i mean by unique
minimum it doesn't have a local minima
even the empirical risk won't have local
minima not that you that's what's well I
can actually prove this I mean if you
want later initially that I mean that's
the question that people have they they
think that Oh since you have a bunch of
these little dips for these different
points but think of it this way let's
say you only had one point and you're
trying to do classification beaches just
have like one or two points you can
still show that the sum of two quasi
convex functions that have this formula
are themselves quasi convex meaning that
the sum of even to like the simplest
classification problem in the world is
just one point here in one point there
right the sum of these two would still
be another quasi convex function that's
the empirical risk write the sum of two
of these losses let's say you some two
of those yep you can saw many too and
they'll still be quasi convex as long as
they come from this formula maybe I
should have but the thing is that before
giving I i got this question during my
cvpr talk and i had gotten that the
first question when I gave the nips
paper so I knew I was good in terms of
probability space but that's not the
space we really worked are dealing with
because when you do it like you do a
transition from the probability space a
transform doesn't necessarily preserve
convexity so when you transform from the
probability space into functional space
there's a chance that you'll lose
contacts city and so I have to actually
go and prove two and three as well which
that's why I wanted two weeks before I
came to give this talk because
I spent the first week actually proving
two and three it's actually quite simple
to do if you if you take the derivative
of this function down here you can prove
that it's quite like convex by the
classic way of proving closet convexity
which is to show that the derivative is
positive up to up to a certain point and
then negative from then on meaning that
there's only one one change that's the
that's the general one of the ways to
prove quasi cognac city so yeah so the
sum you know adds a little bit more but
but it's still nothing more than adding
one function fee of V Plus let's say
alpha time fee of negative V which is
the positive class plus the negative
class so and in fact once you let's see
once you prove 23 which is the case that
you're talking about is is not that bad
afterwards so we can go over it I'm yeah
maybe that's what I should have
concentrated on on more later yeah this
I this I haven't actually included in
any of the papers part one is in the
nips paper theorem one but two and three
but we're we're added to the paper that
i like the journal version of the paper
that i'm writing almost done this is
kind of like the missing link so
no just any I just gave the simple to
example but the point is that you can
add alpha number of the positive numbers
and better number of negatives and still
have some so the thing about convexity
is that when they're talking about
empirical risk we all know that the sum
of a bunch of convex functions is still
going to be convex and that's why when
you're doing empirical risk minimization
using convex loss functions the thing
works well because you know that when
you sum up a bunch of convex functions
you're still going to have convexity the
same theorem unfortunately doesn't exist
for quasi convex functions in general
you can't say just summing up a bunch of
quasi convex functions will ensure
contacts but fortunately if they come
from this formula they do so
the sigmoid is almost like savage but
it's not exactly it's not um one of
these I'm trying to find the formula for
savage here it is it looks almost like
the savage but it's not exactly the
savage does too it's quasi convex it's
derived from that formula it'll still
work but yeah it seems more remarkable
that you would have a loss function
looking like that I would still work
with
so are we so I can bond hopefully okay
so so in conclusion for this section we
showed how to derive novel based
consistent loss functions for
classification and we derive the set of
requirements that robust Bayes
consistent loss functions should have
and we also showed that Bayes consistent
loss functions can be non convex and we
gave some examples of using the tangent
loss function for doing classification I
don't know if we have enough time but we
can I also wanted to talk about deriving
Bayes consistent loss functions that are
cost sensitive so I've been talking for
about 50 minutes already do you want me
to continue okay so many classification
problems are actually naturally cost
sensitive for example when we're doing
detection or fraud detection or medical
diagnosis or business decision making
the cost of one misclassifying one
classes usually much higher than the
other so for medical diagnosis for
example missing a tumor might be
considered as having an infinite amount
of loss because you you know the might
result in the death of a patient whereas
on the other hand a false alarm or false
positive would just cost a bit more
money because you have to do for other
experiments so there's obviously a
demand for cost-sensitive extensions of
state-of-the-art techniques and be here
we propose an alternative strategy for
the design of cost sensitive algorithms
rather than directly manipulating the
algorithm and risk compromising Bay's
consistency or Bay's optimality we
extended the underlying loss function
and derive a cost sensitive learning
algorithm from that class that
implements the bays this
room and also approximates the base risk
so here this is just the same slide that
we had before to remind us of the
general path that we took before for
deriving base consistent loss functions
we would choose any strictly concave
function f star and C star sorry
strictly concave function c star an
invertible function f star such that
that you've satisfied these constraints
we'd plug it into the formula and we
were guaranteed to have a Bayes
consistent loss function and the
question now is can we have a general
procedure for deriving cost-sensitive
purchase cost sensitive loss functions
as well so here we're going to define c
1 as the cost of a false negative and
see negative 1 is the cost of a false
positive and we'd like to ask what
conditions need our choice of F star
deceased are now satisfy such that we
have a cost sensitive loss that's also
based consistent so generally we would
have a loss v1 assigned to the positive
class and the loss fee negative one
assigned to the negative class but we'd
still want to conserve base consistency
so the gent so the generic procedure is
to start from an invertible predictor f
star that has this cementery and the
concave function c star that has this
symmetry in the symmetry is basically
the same symmetry in that the bays risk
have so we're ideally trying to find a
risk that has the same behavior as the
bays risk and then we define J to be a
negative of our C star and we derive
these auxiliary functions I 1 and I
negative 1 from these two formulas and
our loss functions can be shown to
basically be the negative of I one
Sophie one is going to be negative i won
and fee negative one is going to be
negative I negative one and these come
from probability elicitation the proof
comes from there but in general the
theorem is that the loss derived through
this procedure is cost sensitive and
Bayes consistent so here we're going to
go over an example which would be the
exponential we're going to try to derive
the exponential cost-sensitive loss we
choose F star to be this formula which
is the cost-sensitive version of the log
link and you can see that it is cost
sensitive it implements the
cost-sensitive base decision rule
because now the boundary has shifted
from a half to in this case point 3
because C 1 was chosen to be too so all
points that have probabilities higher
than point 3 are now assigned positive
values in all points that have
probability less than point 3 our sign
negative value so we've shifted the
boundary to make it to prefer one class
and we also have the cost-sensitive
version of the risk and you can see that
in this the it's still concave but it
has an asymmetric shape and the point
here is that the maximum of this risk is
again at point three so the maximum risk
is being assigned to the boundary which
is what we want going through all the
steps we can find v1 to be like this
which which is now the cost-sensitive
version and there's there are now two
loss functions one is the red one that's
that's used for the negative class and
the blue one is the loss that's used for
the positive class and it's still a
Bayes consistent so we've actually used
the
you could I haven't in order to get a
loss function that looks different you
would have to choose different f stars
and see stars so if you could go back
and derive the cost-sensitive asymmetric
version of the well you already have it
here who combine it with the the
cost-sensitive tangent link you could
probably get an analogous version for
the cost-sensitive what I haven't done
it um but you know maybe that's up for
grabs yeah so the least-squares is the
same c star that's used in tangent the
difference here is that the tangent uses
a different f star so if you derive the
cost-sensitive version of the of that
one you could get something similar but
here we use just the exponential and the
reason for that was because the the
history of this is because i wanted to
derive the cost-sensitive version of
adaboost and that uses the exponential
loss but here using the cost-sensitive
version and and still based consistent
boosting we did something we've done a
lot of experiments for pedestrian car
and face detection we've even used it
for brain EEG the surprise signal
detection I'm going to move on to
deriving the cost-sensitive SVM
so there there have been lots of papers
on making adaboost cost-sensitive um in
am i actually in the Pammy paper that's
basically what it's all about is going
through the history of this and finally
you know deriving this which is the cost
and the interesting about this is that
it still based consistent all of the
previous methods usually dealt with
altering the adaboost algorithm directly
with what they would generally do is
change the weight so the adipose has its
way to update formula they would go at
introduced costs directly to the weights
and depending on how you kind of change
the weights you would get different
algorithms but no one when you go and
alter the algorithm like that you have
to still kind of show that it's based
consistent because otherwise you don't
know what you're doing anymore are you
really minimizing you know what you
should be and given infinite amount of
data will you still converge to the bays
decision rule which is what you should
be doing so here when you show that it's
based consistent you answer those
problems and you don't need to go
directly minute like manipulate the
algorithm once you have the correct loss
you can just derive the you know
classifier from it and that's what we
did um so
so for that we're going to try to do the
same thing for the SVM because they're
there have been many papers on
cost-sensitive svms but again what they
would do is you know go into the
algorithm and you know tweak this or
change that portion or add the cost here
or there but the principled way to do it
is to actually derive the cost-sensitive
loss function show that it's based
consistent and then once you have that
you know the rest is just you know go
through the steps of finding the
algorithm so here we're going to we're
going to start with F we're going to
choose our f star to be this which can
be shown to be based consistent and it
implements the base decision rule and in
fact it's just the cost-sensitive
version of the link used in svm so if
you set c1 and c2 a few sets are you c 1
and c negative 1 equal to 1 it will just
a you know the simplified to sign of to
etta minus 1 which is the exact same
thing used in svms so for cut for cost
sensitive so this suggests the
cost-sensitive minimal conditional risk
and this has four parameters a B D and E
that can be shown to satisfy the
symmetric conditions for base
consistency if they satisfy these
conditions but in any case if you go
through the steps you can find the
cost-sensitive svn loss function which
is what I have there so this losses four
degrees of freedom a B and D and for the
positive samples we've class of
basically they're assigned a margin of a
over D and the hinge slope of D so if
the red line is the positive hinge loss
they get a slope of the inner margin of
Ed but the negative you have a margin of
V over a and hinge slow
of a so the blue line is the hinge loss
that's used for the negative samples and
after some parameter normalization you
can find C star to be like this but you
go through the steps and now that you
have the loss the risk basically leads
to the cost-sensitive SVM primal
formulation and this is assuming that
you you're preferring the positive class
you can derive a similar primal
formulation for equal to one which is a
good check right because if it doesn't
return to the standard SVM when i set
the cost equal to 1 i've probably done
something wrong but it does actually
simplify to the standard SVM when you
set the costs equal but here we have
some experiments that we've done using
the constancy of svm and we've compared
it to the previous algorithms the
boundary move the boundary movement SVM
and the bias penalty SVM so the biased
penalty SVM injured would introduce
costs on the slack variables directly
and the boundary movement SVM would
basically just change the bias of your
final SVM so so compared to these two
competitors that we we did experience on
10 UCI data sets and causative
cost-sensitive SVM has smaller error on
seven out of the 10 data sets and does
equal to bias penalty on two other ones
we also did experiments on the German
credit data set and here the goal is to
identify the bad credit customers to be
denied credit by the bank and each
person each gets 24 attributes such as
24 features such as the person's age and
income and we're trying to decide
whether the bank should you know give
this person credit or not interesting
thing is that here the cost matrix is
actually defined in the problem because
if a good credit customer is denied
credit the bay
Inc has a loss of one meaning that the
bank loses a good customer whereas if
you give credit to a bad customer the
bank suffers a loss of five meaning that
the so the bank loses money the guy so
this suggests the following constraint
on our choice of C 1 and C negative 1
the ratio should be 5 and this is
defined in the problem so it's part of
the data set but the final results is
that if you use cost-sensitive SVM you
get a total reduction in cost of
thirty-seven percent so in conclusion we
extended the problem
so here is know when you're there are
different ways to compute the error so
here what we did was just yeah just
count the number of errors that you have
over the data there isn't any cost
sensitivity incorporated within the data
set itself because I don't know how much
a Miss would cost when I'm doing let's
say heart disease or I'm trying to do
pima I wouldn't know I mean you couldn't
define these to be whatever you want it
would it wouldn't change the result it
would just be multiplying these by a
number
yeah so what I ok so to go into yeah so
go into details of how I actually did
this the the way we did it was we all of
these algorithms have their different
parameters right like the C 1 C negative
one so you tried all the different
parameters such that all of these
algorithms attained a certain detection
rate on a validation set so for example
we said we want to have all of these
such that they have ninety percent
detection rate once they're equal on
that sense I would go to the test set
and test it and see how many false
positives they gave me and of course the
one with the smallest false positive
would be the best because I've ensured
that their detection rates are fixed
given the amount you know like the
amount of flexibility these have the
cost sensitive and biased penalty SVM's
they have the same number of parameters
that you can tweak but the boundary
movement svn the only thing you can
change there is just a bias so but with
that in mind that's the fair way of
comparing them so for this section we
extended the probability elicitation of
you've lost function design for
cost-sensitive problems and we specify
the generic procedure for driving cost
sensitive algorithms and also derive the
cost-sensitive bayes consistent boosting
and svm loss functions and also went
over some experimental results and thank
you oh I'm open to any more questions
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>