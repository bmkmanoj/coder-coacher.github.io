<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Supporting Casual Data-Centric Interactions on the Web | Coder Coacher - Coaching Coders</title><meta content="Supporting Casual Data-Centric Interactions on the Web - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Supporting Casual Data-Centric Interactions on the Web</b></h2><h5 class="post__date">2008-08-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s5VLj9T_pyI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and so I'm gonna started this is uh
David Young MIT and we're talking today
about a bunch of workers from doing at
MIT involving extraction even
interaction faceted search and I guess
I'll let him kind of give a better
reduction than that so thank you Andrew
hi everyone I'm very excited to be here
today to talk to you I'm constantly
amazed at the ideas that keep coming
down Google and in the past app or one
or two good ideas from you so you know
after looking at Google Maps it's kind
of obvious that somebody should do a
time language and I happen to be the
first one and so here's my little time
language it and a lot of people have
used it to do timeline mashups so back
here you can see that this is a website
that shows earthquakes and at the top
and is the timeline showing 20
earthquakes happen and at the bottom to
the occasion so talking about mashups do
you remember the first few mashups that
came out the the more popular ones for
housing maps com that took data from
craigslist and then put it on google
maps and then this Chicago Crime org
which has been part of every well so I
was really excited about this mashup so
I told my friends about it and
explaining to them how you know
ingenious this was right so you scrape
the data out of the web page in this
case from craigslist and you then they
put the data into a database and then
they serve it off on and the same thing
happens with Chicago Chicago Crime
goerge so my friends say well this is so
great I'm moving to Chicago now I want
to rent apartments that are far away
from crimes can I put a tube map
together and it's kind of very apparent
right away that you know you can't do
that right without access to the
database behind the website you're on
some user land over here and that's so
Berlin and you can't reach across get
that data to do the mash up yourself so
if my friends were actually moving to
Chicago well he needs to know more than
just you know where apartments are and
where the crimes are
he probably wants to find where the
transportation the subway stations are
where the schools for its kids know what
I shop for his apartment and so on now
he would have to wait for some you know
vc back mashup site to offer him off its
data right you know the more unique the
combination of his needs is the less
likely that there will be some site that
does this for him and you know there's
also data that he might not want to
share with any mashup site maybe he is
doing his budget for his move and he
doesn't want his bank account
information to to be put into that
measure well maybe he has some local
data where his friends and relatives
live in the city and you know maybe he
doesn't want that to be shared with the
mashup so this is called the suicide
mashup paradigm where the mash-up
actually happened on the server laying
instead of the user Lane an alternative
would be to swap that and do the mash up
on the client side and in order to do
that where you gotta get all the data
from those websites over and then once
it's over here the user can use whatever
tools that they have a debt disposal to
do deal with that data maybe they
generate a map where maybe they actually
want to use a spreadsheet interface to
calculate the budget for whatever so my
research goals for the last three and a
half years been pretty simple just to
get the data from server laying over the
user land and then let the user do
something useful with it um that breaks
down into three topics that serves as my
tops outline here data extraction they
are publishing and data integration so
actually this message of getting the
data to flow it's always doing one of
the messages of the Semantic Web
community and that's sort of how it got
started for me um so in 2004 in late
2004 I was really intrigued by the
vision of the Semantic Web and so I went
to the some people in the w3c and I
asked them so you know show me some
tools
help people to get some value out of the
semantic web data that they encounter on
the web and the answer is that there's
no such tool that's kind of a surprise
since you know the first thing that Tim
berners-lee did in trying to launch the
web is to get on the web browser built
so that people can actually look at web
pages and see well you know this is
useful to me so so since there's no tool
um I started my research by building a
tool called TD bem it's a web browser
extension for Firefox that let people
collect this data that's kind of like
bookmarking but instead instead of
bookmarking the whole page you bookmark
little tidbits of information on the
page one routine is structured the
festivities so if the web page you're
looking at has a link to some audio data
some semantic web data then the
extension would be able to scrape out
that data and show you the those tickets
are over on the other side bar here and
then this this little details button
over here that when you click it and it
would flip the whole content view of the
web page and shows you that data in more
details let's zoom in for each update
the record that it's extracted there's a
save button so that you can save that
logo so you can accumulate more and more
of the data a semantic web data that you
encounter on the web and then you can
later on browse through all the data
that you have collected so that's why
the the name piggy bank kind of
collecting that information this is good
at all but then you know in late 2004 we
look around and there wasn't that much
semantic web data on the web anyway so
and the next thing we try is to support
screen scraping to convert data sorry
HTML into data so we built the version
to a piggy bank and given a website like
this this website showing movie shows
we had a show and the time of each of
the show so if you're looking at this
website and there's a button down here
in the the status bar of the browser you
can click it and it would run a screen
scraper code on that page and it shows
you the the data that it has extracted
out in this view so the result over here
and then on the right side are several
browsing controls that you can use to
narrow the results down to just the
items that you want to save and save
them from so these are filters or we
call them facets so for example if I
click on PG I will get the 16 16 g g
rated movie shows and then if there's
any address information on that you can
just flip over to the map view and
you've got that on a man so this is user
mashup and you can scrape data for
multiple websites and put them together
in the same air so I can show
restaurants and movies shows on the same
man so i can plan the evening that's the
usual scenario right so Big Bang is
about the screen scraping plus a very
flexible data store that can hold
whatever data that those screen scrapers
generated and this is the wall this we
are semantic web technology starting to
get useful you know simple traditional
scheme eccentric database can't do that
because for dimensional database you
have to design the schema up front and
you can't change it on the fly so we
have to use something called triple
store and in addition to that flexible
data store we also need to support
generate browsing on top of that data
and we don't know ahead of time what
that data is when you look like we
publish our word to the international
semantic web conference in fall 2005 and
the pitch was experienced the Semantic
Web inside your web browser now and it's
pretty well received and I think it's
the first application to demonstrate
inside user control data mashups so so
the insight that we want to get across
to the Semantic Web community was to
show usefulness of the semantic web
technologies as early as possible and
that's that's actually possible and that
they have to work force with the user
interface and the data together rather
than just focusing primarily on the data
now d there's a problem with P bang is
that like grease monkey a user scripts
the screen scrapers are very hard to
right then in JavaScript so you have to
know JavaScript you have to know how to
walk through the dorms and so on and
they are hard even for us so the next
thing we try was to do automatic screen
scraping now I know there's a lot of
work with automatic screen scraping here
and i think the would be the the three
general challenges would be to first to
locate the items of interest on the web
page and the locator fields within each
item and then finally label the fields
so you would say this is a publishing
date of a book this is the author's name
and so on now the labeling of the few
seem to be the most difficult part for
computer and let's take a quick look at
why that is so these are some search
results from amazon and i think you
would guess that they are about books
and then we would know right away that
March second is most probably the
publishing data book but update is
actually closer to the author's name so
maybe it's the author's birthday who
knows way that's nothing on the web page
that actually say publishing date or
publication date and publish date away
so it's very hot for the computer to be
able to label that view let's take a
look at another example here so at the
top we have two numbers and I think we
can guess that the first number is the
original price
the second one is the sale price in a
corset there's nothing in the web page
that you later but there's some visual
cues and contextual information that we
all know that no make it super easy for
us humans do to know what those numbers
mean so so since labeling the fields are
very difficult for the computer but it's
easy for humans so a lot of applications
require the human to you to do the
labeling and in case that you actually
need a few labels that's that's fine the
humans can the user can understand the
needs of those few labels and go to lab
shores but in some applications and the
need for labeling the fields might not
be apparent to the user right away so if
you require a user to do all that over
the head would front they might not get
through it and then you might not get
the result they value out of the
transcript so we're going to cheat well
we're going to not label the fields at
all and what we're going to do is
actually let the people interact
directly with the information on the
page so if they want to sort by the
publishing date the computer doesn't
have to understand that's a publishing
the human can just point at that and say
computer sort by a dot via whatever it
is so so what you see is what you
manipulate in a sense and in order to do
that we have to extract out the
presentation basically the HTML fragment
from the page and preserve them let the
user interact with the fact so I built a
web browser extension called sifter that
basically does that it extracts out the
HTML fragments from the page and then on
top of that I'm ad filtering and sorting
features on to the original web page as
if to the web page itself implemented
those features and all that is done
without burdening the user with labeling
the fields first so let me show you a
screencast of that
so so here we are doing a search on
amazon we've got a bunch of results
there are three pages and then there's a
lot of structured data within that web
page on the right side is the the sifter
of paying and there's a single button on
it let's say filter or two items and
when you click it a plus to detect the
items of interest on the way occasion
and highlights them so you can verify
that it has done that correctly it also
indicates that it has fam three pages
and then all you need to do next is
click continue positive doesn't do it
well then you can go back and correct so
it goes ahead and fetch the two extra
pages and then extract all of the data
and it shows you the web page as if it
were before the extraction happen so
everything looked the same so far right
with a little exception so in the web
page next to each of the few value
you'll notice that this little asterisk
there basically direct manipulation
handle so that you can grab on and do
something with that feud so here when
you click on the asterisk next to the
hardcover you get a browsing control box
on the right that you can use the filter
the items on the web page by that field
so I click on audio cassette and you I
get only two audio casa type so what
it's doing is is actually rewiring the
web page on the fly taking out the
original item each name and fragments
and then do the query see which one fits
and then put that back into the damage
and then it also understands the date
user groups it in two months and years
so we publish that at which 2006 and the
insides are that this human design
presentations that you see on the web
pages they carry implicit semantics in
the in their visual information and then
all you need to do is add this direct
manipulation handles to and then you can
offer extra value on top of that data
without ever requiring the user to to do
the labeling of the views and these
presentations when you expect them and
use them again after you scrape then it
helps preserve the visual context so
that the user isn't thrown into a
totally different user interface and get
confused also these presentations that
you see in web pages were designed by
humans so they generally beat any
machine generated presentations they are
visually appealing and concise within
the context so if you're searching for
book and Amazon well you know that's
probably one of the best way to show
that the information about books are you
merging multiple we've scrapings
together into a single mystical thing so
somebody asks similar for multiple times
are you going to use the label ins to
use their previous one in the you know
if you get to the Amazon twice you are
you going to keep that information and
we haven't got to that part yet but yeah
that would be a good idea to to do
that's a revisit that topic later so the
sitter works but it mostly work for the
web 1.5 well what do I mean by 1.5 it's
it's the web where and the content is
generated from templates on server ASP
PHP whatever it doesn't work for a
handwritten HTML at one point oh and it
doesn't work for web two point oh that's
too dynamic dynamics now we are moving
to a word of where to
find that the numbers become meaningless
here but you know just so on web 245 you
know the website start offering ad is
for you to access data so Google Data
API this Facebook API lots of API is
coming out but they are all intended for
programmers so the users still have the
short end of the stick and this API is
are all different so it's kind of hard
to build you know a single application
government I know there's a map word in
unifying the api's but it's clear comic
right now in well three we know
hopefully there will be a unified way
for the user to actually get at the data
to access the data and this is moving
toward the Semantic Web so if we look at
the infamous long tail at the head of
the curves are the types of information
that are very popular or that are
present in large quantities they are you
know products events news photos movies
whatever you name it the all the common
ones and then tow it the tails are very
exotic esoteric collections of data is
that people you know happiest are
interested in maybe you're interesting
LEGO sets something like that people
like the bunch of these and you've got a
collections and you want to publish
about it so that your your friends who
are also interested in with see I think
that there's a trend that as you move
further and further along into the tail
you start encountering you know earlier
and earlier web technologies it's simply
because that at the head of the curve
there's a lot of information a lot of
people aren't interested in these types
of data so then there's more money more
financial resources to build better
website more modern web sites and then
when you move into the tail fewer people
are interested in them so you know this
party a business model for supporting
website with those types of information
now permission that you have accepted
was to organize the world's information
right so you want to extract out data
from the whole
curve and it's kind of easy to do it for
that the head they already have all the
data databases as long as you show them
some benefits of exposing some of that
data maybe to get along and you know
Yahoo search monkey is sort of trying to
get at that in the tail it's kind of
harder because these people actually
don't have both money or programming
skills to be able to publish the data in
machine accessible wait so so okay so
the tail is hard to describe but you
should scrape because you want to build
an inclusive data way if you want on
this web 3.0 or whatever to represent
both information that a lot of people
are interested about and also you know
more esoteric kind of data and we know
that the tail is actually when you
accumulate the information in the tail
it can rival in the amount of
information in the head right so that's
that's a hard problem but at the same
time the authors in the tail actually
have a need to publish that data they
want to show their friends their
collection of LEGO sets and they want to
do it in modern way is not in you know
handwritten HTML so can we kill those
two birds with the same stone someone
approached do that is to make to build a
structure data publishing framework how
exhibit that basically let these people
with just passing knowledge of HTML be
able to publish their data in British
user interfaces for the price of making
that structure data easily accessible so
let me quickly demo that
so pretend you're a history teacher in
high school who wants to teach your chip
your students about the US presidents
and you have 42 presidents to tell them
about and let's say that you want to
show them that most presidents were born
on the east side of the country so then
you want to put map the precedence by
the Blessed place maybe you want to show
them when each president came into
office so you want a timeline here when
I synchronize the two views maybe yours
you also want to implement search
functionality so that your student can
quickly find Kennedy maybe you want to
show the the distribution of the
president's by their religion so here is
so you can see that the number in front
of the counts of precedence of that
religion and actually clicking on the
choices in that list would filter the
map in the timeline to precedence of
that process of that religions and then
you can also combine this various filter
together so I can filter that down by
political parties as well now in order
to build such a web page using
conventional technologies or what you
have to start with installing a database
designing a schema loading the data base
with a schemer and then designing your
tweets your web applications with you
know the server side script and the
client side square can connect them and
then trying to make them work together
and all the browser software right so
that's a little bit hard but by using
the exhibit framework that i'm going to
show you it's actually pretty easy all
you need to do is provide 251 is the
data file and one is the html5 so let's
take a look at the source code of this
web page here
so at the top and there are two links to
the data and that's the data files are
in JSON format and I'll show you in a
bit and then the references to the
exhibit API here and then this also
reference to the timeline extension in a
map extension and then down here in the
body of the HTML what you see is
actually pretty much regular HTML except
for extra attributes here that specify
how this ego witches should be
configured so so for that time line at
the top this is what you need to
configure you say that here in the place
of this div render the timeline so the
new class is an eyeline the for each
event on the timeline retrieved the end
a property of the records and then
plotted use that as the starting date of
the event so these are little
expressions here that would bind the
database to the view grade and then the
same thing goes with the the maps here
and in order to show the precedence
portraits in each of the icon all you
need to do is say X colon icon equal dot
image you are so using this very
declarative means user can quickly build
that interactive web pages and which is
basically passing knowledge of HTML and
if the user happened to look at one of
these web pages done by someone someone
else they can look at the source code
and then just copy and paste just like
how HTML was usually author originally
but you can't be somebody else
and now the this is the data file and
it's just JSON and it's basically a list
of writers and each of the right if it
has properly value so it's pretty simple
this is an another example that we have
as exhibits of mushrooms and you can
filter the mushrooms by the cap shave by
how edible it is and so on so just to
show you the breadth of the kind of
which is the we support so what have
people done with exhibit since we
release it well there's a quite a
variety of types of data that people
have published so here is test
operations done in the James Watt
telescope collected by somebody NASA and
basically he just want to plot that
those events on a timeline and also be
able to filter the events by I guess
some sort of classifications by the I
don't know this data that well but you
can get an idea of how you want seem to
interact with the data and maybe it's
not useful just for himself maybe for
for his colleagues as well this is a 305
ongoing projects at the Pakistan
development portal you can filter these
projects by the country that finance the
project by the sector that the projects
in so lots of structure data here and oh
by the way when you look at an exhibit
like this that's this little scissor
that you can click and get the data out
in various formats well actually if you
look at the source code you already see
the data to top you so that's actually
no scraping you just grab the data and
this is I believe a class project that
try to plot the contamination level at
various cities in Spain and a music
composer database done by someone
with very little knowledge of HTML this
is an example if you exhibit integrated
into a news website and it's pretty well
integrated that you can hardly see that
it's exhibit except when you actually
click on any of these filters and it
start to behave like exhibit this is a
scandal for those double dippers this
exhibit is done by the Mozilla Firefox
team why they were completing five Fox
three they want to track their design
documents and so they want a filter by
you know the priority of each design
document whether it's a twist and so on
and so forth what's interesting about
this one is that when you view the
source code the data is actually coming
from a Google spreadsheet and we support
this natively so you can collaborate
entering that editing the data in your
google spreadsheets and then just view
view the data someone you know on an
exhibit this is a clinical trial events
with data on people I guess the patients
that drops the adverse events and so on
this is somebody's ancestral timeline
people are various lineage in his family
and when they were born in and where we
were born and where they died and so on
this is exhibit showing 56 pictures of
collections of rocks I'm telling you we
are getting you no further and further
into their long tail here this is the
gesture project so you can learn how to
these various kinds of gestures
so this is how to do duh in Columbia I
don't know what it plays but anyway now
you so the features for exhibit is that
you don't have to deal with database you
don't have to install you know set it up
ski-doo the schema design don't want to
design is tweet your web application
what you get are rich views Maps
timelines I forgot to show you this
scatter plots and whatever else right
and you get advanced faceted browsing
filtering search and with very little
work how do you use it you just provide
data in the files or in a Google
spreadsheet feet or any other kinds of
feet and you use an extended version of
HTML to configure this big widgets and
how they how they are linked together
this is done all in JavaScript it's
basically a tweet to your web
application framework in JavaScript and
the presentation templates oh I've got
to show you that so back here when you
click on a precedence you see this
nicely format formatted rendering that
basically is done by specifying what's
called a lens template so normally you
would do this on the server side in JSP
asp whatever now we just do it on the
client side it's very simple
so just want to show you that does run
on handheld device nokia n800 on the
little oil pc and then we also have a
version on the iphone but this is not
running on the iphone but just to get
you an idea so why that's good basically
you have the same data file is the data
file data feeds and then you can create
different HTML files do for different
devices it's good for ad-hoc mobile data
browsing apps let's say you have a
little conference with 100 events maybe
you want people to be able to carry out
its carry the schedules on their iPhones
or something would be trivial to
generate one of this so so exhibit is is
about letting people publish data I want
to read take a minute to relate that to
the Semantic Web community's vision so
for a long time and the committee
semantic web community has been telling
people to publish data so that you know
someday someone might get some random
use out of the data that you publish and
we have a better work it's very hard to
motivate people that way people tend to
optimize for you know short-term
personal benefits over a long-term
prospect to someone else so with exhibit
there's a clear purpose to why you
publish the data where you want to share
that data with your friends it's the
same motivation for you to publish a web
page as to use exhibits way so it's the
human to human communication that's the
end of the data is just a means for that
and this is this actually helps dry the
designs of presentation and also the
design of the data so let's say that you
want to show your students that you know
most presidents were born on the east
side of the country well the best way to
show that is to plot them on a map to do
that you need to lie latitude and
longitude of the data so then you go
about and enter off that
this is different from symantec where
vision because for the Semantic Web
vision if you publish data without an
audience how do you know what data to
include how much data to boot and how do
you verify the quality of the data
because there's no purpose it's like
giving the top without knowing what
you're talking to you so if data is just
a means to an end then we can try to
ease the pains of handling data as much
as possible to get people to which their
goal we do that by avoiding and the need
to design a schema and yeah to design a
schema so if you look at the data file
of the exhibits that I've shown you
let's go back to the second year you can
see that let's pretty much know schema
you just start typing in data in the
database in exhibit we just take it of
course you can add more data and that
would make your exhibits look of behave
better so if I go to the schema file
here you can add things like four things
of type residents when there are many of
them at an STD air you can add more
schema to make things looking nicer but
you don't have to so you can start out
easy way and then you know iterate until
you you get to what you want so we
publicly exhibit to the world wide web
conference in spring 2007 and so far we
have released version to the code is on
google code we got a lot of adoption in
the garmin yeah government agencies in
the nonprofit organizations in the use
asian students even in the industry like
mozilla that i've shown you there's a
lot of experimentation with exhibits by
the scientists and research labs what
they want to do is for scientists to be
able to quickly generate this
visualization of experiment data without
the need to call
programmers and in conferences are using
exhibit to show their schedules and
events even developers are using it and
drupal is trying to go to Drupal
developers are trying to see whether
exhibit is good for the front end of
their web applications but most
importantly individual people are using
exhibit so the active as happy as
historians teachers students and so on
that scenario that i show you about
using this google spreadsheets as at the
back end of exhibits that's really
popular for teachers to let the students
collaborate on entering the data so we
have about one and a half years of
silver loss so every time somebody uses
exhibit they include the exhibit a b ijs
and our server gets a referral URL so we
got around 800,000 servings of the API
and we distill that down to around
16,000 unique refers 6,000 or HTTP
retrievable the other ones are inside
internet or local files actually a lot
of people are using exhibit internally
in internet and they they want to you
know know how to download the code so
the three and a half thousand refers are
indexed by Google Yahoo so we consider
them to be public those exhibits have
been linked off from some other web
pages so the authors actually intended
them to be visible to the world not
accidentally visible to the world and so
we prado's and we got out 2800 that
contains data and cuz there's a lot of
duplicated and since I'm not so good at
with crawling so I need to resolve the
URLs and so on but with all that data I
put off the data together into a single
store and I call that the exhibition is
today I'm trying to do a search engine
on top of all of these public exhibits
so here's a very very early prototype of
that you can see that the data structure
of the data is all retain and over here
is a filter by type and you can start to
see that this is getting into the long
tail so there's like two thousand dams
some authors trajectory I guess that's I
don't know what that's well counties
dogs there's an exhibit on dogs for
adoption dispersant want to keep track
of the dot this treatments clip book
mutant genes destroyer so a lot of very
esoteric types anyway this does still a
lot of work to be done in this area such
as supporting text search supporting
faster browsing on this heterogeneous
collection and we got off that
structured data but we don't know what
the experience would be like to do
faster to foraging on that because all
the fields are different this what
you're getting into the long till you
get all sort of things in there we want
to support Matthews and timeline views
automatically and we know that because
we can see how these views are
configured in the HTML of three exhibits
so it's easy just to grab those
expressions out and we also want to
spread the original lens templates that
is now you know very visible in the HTML
code and we also want to extract the CSS
rules and with those two we can then
render each of the search results as if
they were as if how they appear in the
original rings events because we want to
automate this scraping process month
there is also questions to be asked
about how is actually being used like
how long tail is the data how large is
the community and number of visitors
around each exhibit is it just for one
person it's the the person using exhibit
just two raspberries on data or is he
sharing it with his friends we want to
track the copy and paste behavior we
want to see whether one exhibit looks
kind of like another with just some
changes and we want to know whether
authors are capable well they whether
they can configure that the filters and
abuse and needs in the exhibits wisely
to best show that data and of course we
want to have in certain search engines
to index this acceptance so few of them
right now in order to author an exhibit
you need to edit the HTML code by hand
now HTML is good to get you know the
original early adopters of the web
started but it's not until block engines
that get on board so we also want to
provide wiziwig authoring for exhibits
and that shouldn't be too hard because
the way you can figure exactly despite
this declarative syntax so that's easy
to provide wiziwig tools for drag and
dropping the switches and we actually
working on something like that and we
have talked to some people at Adobe
maybe they would be interested in
putting it in Dreamweaver now one thing
about it is that right now it's
implemented in Java squirrel JavaScript
so it doesn't scale beyond a few hundred
items and but javascript engines are
getting faster and browsers getting
faster and all that so it's not too bad
it's already useful as is but we do want
to explore one possible solution for
scaling up exhibit so right now when
you're looking at an exhibit and what
happens is that the browser loads the
HTML
usual it sees that this is square plane
to the exhibit API I load staff and not
kind of unfolds itself into a web
application framework and that goes back
and read the DOM and see that this link
to the data file and then love data file
into a database so in order to scale
this off we want to move the database
over to the server side and then we have
to split the code so that some of the
code would sit closer to the database we
started to do this work and we call the
combination of those two components
backstage so how do you use backstage it
would be the same as an exhibit so you
would provide the data file data feed
and then use extended HTML for
configuring that and then you set a flat
to supersize you exhibit and it would
just basically rent you a web app on the
fly tell the web app where the data is
that the web app which is load that data
file into a database on the fly and then
serve the soup DUI to JSON p now
actually you don't have to rent web app
the the user computer is powerful enough
to do all of this calculation anyway so
then it would be interesting if we can
get something like backstage to be
shipped with the browser so the browser
would put the data and the presentation
together in the last minute so instead
up to the web app on the web server
serving HTML you would basically feed
the browser HTML and the data file and
then the browser itself has something
like package or exhibit and then it
would just construct the application on
the client side so we would be dealing
with more than just the document object
model we would also be doing with the
data model as for the HTML configuration
part right now you need to include
exhibit API the top and then you use
this diff you overload the div to
configure the big big widgets but a
better way to do that would be to
use our own kind of elements exhibit
view and exhibit facet for instance in
you know a future version of HTML and
then you don't have to include exhibit
API so so hopefully if we get a lot of
traction and all of the web point 1
point 0 web pages in the tail start to
turn into web 3.0 because the data is so
easily accessible and a lot of web pages
that never get published because it's
hard to publish right might actually get
published so they got onto the web now
the next problem is that you have a lot
of these exhibits with different schemas
what do you do with all of that can you
integrate the data from several exhibits
together well so that's that's the
second problem that I'm trying to
address in addition to get the data to
the user where we got to connect the
data together and my solution would be
to connect the data one usage snare at a
time so when the user encounter two
different exhibits that they want to
integrate together they can do that at
that time rather than trying to go for
some sort of global data integration
this is do with yourself data intubation
personal data match eyes now there's
been a lot of research on data
integration tools in the database
community in the Semantic Web community
and a lot of them focus on large data
sets we have hundreds of classes
hundreds of properties and so on so they
need very specialized tools for experts
to be able to do the data integration
and this usually scares the users so you
have to know things like thing sorry
some sort of class hierarchy that a
person is an agent is a thing you know
that might not make sense to and we're
using so I believe that a lot of data
sets that you encounter on the web tend
to be simple enough that you can
we just direct manipulation in order to
integrate them so I built a tool called
hard life that start to explore how to
use direct manipulation for personal
data integration so let me down with
that so here's the scenario these are
two exhibits about people this one is
about the principal investigators at MIT
CCL lab and it's built bikes with
exhibit and there's a lot of data like
people's position the where the offices
are and this is another exhibit about
people in the Columbia University Center
new media teaching and learning and
let's say that there's some sort of
collaboration between these two labs and
our task is to create a single directory
people's contact information so is a
tool called potluck that can do that and
what you need to do to use it is to copy
and paste the URL of these exhibits over
here and then click mixed data so it
goes and it grabs those data files of
the exhibits and kind of put them
together and show you in somewhat raw
data form here and the the properties
are color-coded by their origins where
they came from so the blue ones came
from the MIT CSL website and the pink
one here come from the Columbia website
and I can start right away with
constructing a table of people's contact
information and let's say I start with
the deposition so I can draft position
here over job position now that works
with the the first record but it doesn't
work with the second one because since
the second one come from the other data
source and there they call it title
instead
position so I can just wrap that in and
that merge the two fuels so this is
somewhat different from conventional
data integration in that in the
convention of data integration you
integrate the data finish all that and
then do the visualization or whatever
you do with that debate data here we are
going to be building the visualization
and integrating the data on the fly
together iterating and until we get what
we want and so i can actually undo this
and now the two fuels are you know
separate and i can we do it in the two
fields are together so let's do let's
put people's phone number into economy
now you notice that for the CCL people
they don't have area code but they are
all in boston's we know they are their
area code is 617 so i'm going to enter
the area code for ninety two of them so
i'm going to click this edit button here
and what that does is it clusters the
phone numbers into columns based on
similarity so everything in the first
column start with 253 in the second
column to 12 and then the third columns
everything else anyway I'm not so good
at machine learning but you get the idea
and then this make it easy to see the
outliers so here's somebody's extension
without the for phone numbers now in
each of the column you get several
cursor tech cursor so my cursor is on
the first row here but you can see in
this the other roles they have this fake
cursors when I'm moving the primary
cursors the other one moves with and
basically you can use this to edit all
the rows in the same column at the same
time
and then you can get the key with this
okay I'm not going to do it for the
other ones but you get the idea now this
is useful for extracting our sub views
within a field as well so if you look at
the office field and there's actually a
building number and the room number here
so what we want to do let's say is to
extract out the building number so what
we can do is drag this over and make a
copy of that field and then use the
simultaneous editing features to delete
the room number and then typing in
building in front
and it's safe
you can also use drag and drop to create
filters so I can practice over here to
make a filter and then I can click on
that to get only two people who work in
building 46 and we also have a map view
where you can drag addresses on to these
targets to plot records on the map but I
don't have time to show you that so so
it would be interesting if we can
integrate potluck with something like
scepter notice that simple already have
these handles where you can click on to
interact with that view now you can do
data into creation from several websites
by dragging those little asterisks
together into the same column or into
the same filter so I think that's sort
of getting to your question now if
enough people actually taking the same
data integration action as I merging
together position and title maybe that
would give heuristic for the computer to
infer that these two fields are
equivalent but maybe it's not maybe they
just you know want to see this in the
same missing column but that's sort of
how I'm seeing this approach getting to
the problem of addressing you know the
millions of schemas out there so
hopefully you know like focus on amis
where people just tack for their own
purpose but then you get this on amis
arising from there tagging actions
hopefully we can get toward a unified
schemer by people doing data integration
one scenario at a time so to wrap up and
I want to go over the list of things
I've shown you in the data extraction
topic we have piggy bank which is screen
scraping tools that has a flexible data
store to store whatever kind of data
that the screen scrapers produced and
then we also have a generic browsing you
I that let you
filter and browse to this results that
you have collected platinum maps and so
on it's the first one that demonstrate
client-side user control data mashups
then we have sifter which is a tool that
does automatic screen scraping by
extracting out of the presentation
rather than labeling the field and then
it act in this direct manipulation
handles next to each of the few value so
that the user can grab on and directly
and directly manipulate the fields in
the data publishing on the date of
publishing topic I show you exhibit
which is a framework that like people
published rich interactive web pages
which is passing knowledge of HTML for
the price of making their data easily
accessible so this is hopefully able to
capture the data sets in the long tail
I've scraped F crop a lot of those
exhibits and put them together and
hopefully building a search engines on
top of all of these public exhibits I
also talked about backstage which is an
early prototype of how to scale up
exhibit the concept day is that you can
rent a web on the floor I to do the to
do all the computations and all you need
to do is provide the data file and the
HTML configuration and finally I show
you potluck a do-it-yourself mashup tool
that probably good enough for simple
data sets that you encounter on the way
and it uses direct manipulation
techniques like drag and drop and
simultaneous editing to that the user do
the data integration why constructing
the final visualizations this works was
done over the last two and a half years
in three research groups the haystack
group and the user interface design
group in a similarly project and for
more information you can visit
this website some fishes
for sifter did you have difficulty with
deliberately broken HTML I think we it
doesn't have to be deliberate doing
stuff years yahoo is to the plate breaks
their HTML in a number of ways and it
breaks it differently each time you in
several different ways there are some
companies that have they will make your
data easily available so try me almost
impossible to try to make me hard as
possible to scrape I was wondering how
much of that how much of that was an
issue in your sifter work I actually was
not noticing that I think i actually had
it now you know trouble trying to make
it work in on regular website i didn't
notice the aspect of them trying to
break the yeah but the in some of the
cases be so instead that we try to
detect next pages links like in the
search result you have several pages
there's a lot of trouble in that since
you know the website can try to append
like your relevant parameters and
confuse so besides when the scalability
issues other implementation of this that
i see is that most of the data is
basically static so if you want to build
an application people are editing the
data on the fly yeah shared from caesars
then um you need a different way to
destroy the back end 7i guess backstage
also maybe could start to get towards
that everything that you can do they
work toward that yeah so well another
approaches is google spreadsheets where
the defeats would get updated there in
five minutes so we have heard that our
own users asking there's a student
started working on that right now but
nothing to show yet
how do the text next pages so we can
well there are a few heuristics one is
to look at all the links with the
numbers right each one two three you and
so on that the label of the links we can
look that you can parse out the
parameters in the the URLs up those
links and see if there's a sequence if
that doesn't work then you can let the
user click on the next page link so it's
a compromise between yeah
started out talking about the Semantic
Web very graciously that all the ways
there's anymore are we almost done yeah
you're talking to
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>