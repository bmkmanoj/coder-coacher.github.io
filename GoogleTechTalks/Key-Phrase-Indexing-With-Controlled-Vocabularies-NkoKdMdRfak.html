<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Key Phrase Indexing With Controlled Vocabularies | Coder Coacher - Coaching Coders</title><meta content="Key Phrase Indexing With Controlled Vocabularies - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Key Phrase Indexing With Controlled Vocabularies</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NkoKdMdRfak" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so the topic of my talk is give his
indexing is controlled vocabularies and
and this is the what I'm doing my PhD in
which is funded by google and i started
the university of waikato in new zealand
and this is where creek naval mining did
his PhD and we have the same supervisor
and the system that i'm going to
describe you in today was is the
successor of the system that break and
has been working on code keyphrase
extraction algorithm indexing means
assigning terms to documents so that
they describe these documents main
topics i would like to show you an
experiment we have conducted with the
food and agriculture organization of the
united nations they have a very large
document repository and professional
indexer who index documents by using
terms from a domain-specific Cesaro
Scott ugly walk and in our experiment we
had ten documents or related to
agriculture domain and six professional
indexers from the FAO have assigned
terms from the Agora to these documents
and the other book has 17,000
descriptors the version that we have
used and which terms that the indexer
are allowed to assign and their 11,000
non descriptor standard that link to
descriptors which helps the indexers to
keep the indexing consistent over the
document collection for example if there
is a term obvious
that appears frequently in the document
the indexer is encouraged to to use the
term overweight instead so that
documents on overweight and obesity are
grouped under this the same location
here is a typical entry from the agar
boxes hours the descriptor epidermis is
followed by a short scope note
describing its usage and there are a few
related terms semantically related
descriptors broader and narrower terms
and you can see that there is a line in
a kind of hierarchy this is ours has a
hierarchical structure and for some
descriptors have related terms which
represent general Railey relatedness
without specifying what kind of relation
so here is a sample document an example
document on the global obesity problem
um in my first two days at Google I
experience your healthy lunches and I
know that you don't have this problem
our six indexer have assigned 33 terms
from the agri back to this document and
each of them between 5 and 11 terms and
all of the six in boxers have agreed
only on one term overweight and these
four terms are terms it at least three
indexer half of them as agreed on um
there's a 12 term that at least two
indexer have selected and the majority
of the terms were selected by just one
in itself they were idiosyncratic to
each of the indexers to compare it to of
the inductor I marked the choices of one
of them with blue and the second one
with red circles as you can see the
prayer and the one in turn they agreed
on is overweight index around selected
Dietary Guidelines diet food nutrition
policies price policies in the 30 words
with more prolific and selected more
terms generally when we evaluate
automatic indexing system we compare
phrases that were extracted
automatically to the one that were
assigned to manually so if you would
imagine that the first in the second
inductor would be an automatic algorithm
and evaluate the system would be a
complete disaster for the system because
only one term would be identified as
correct in this case all terms are
correct and you can see here the
semantic relations between the germs as
they exist and they agarwal and actually
the terms are not that different they
all relate to the domain of the
agriculture on this slide I am showing
you the choices of all six indexers it
was very interesting to look at such
diagrams because here we can see effects
that are not obvious when we just see
the words them separately it is quite
obvious that terms that were selected by
most indexers overweight nutrition
policies rice policies are terms that
are more important for this document
also some terms are related to move
other terms for example nutrition
requirements is related
24 terms developed countries to just one
German public health the regulation are
not related to any terms the more terms
phrases related to the more significant
is its meaning for this document so if
we would evaluate the quality of the
indexers the one who performed the best
would be the one who selected terms that
were assigned by most other indexers of
terms that are related to most other
indexes and this is yes context I'm not
30 is why like a who are bitch audience
why are you your question is what is
what who needs the indexing well index
terms are important to access document
collections of any sorts on the Internet
in libraries when when you see published
papers and they're always somewhere
underneath in the first part before
introduction and abstract keywords this
is the same thing for phrase it authors
eposide and when key phrases are
selected from a controlled vocabulary
and as I have shown you with this
example with overweight and obesity it
helps to access the document quickly and
they are all grouped together under the
same topics also if you would have
search results and each of the document
would follow a set of index term the
describe its topic it would be easy for
the user to see what's there and what
they are about and in the library's
digital libraries especially they of
even greater importance the library
subject heading is a standard way of
classifying library entries and hope is
the answer to a question so how do the
varian evaluate indexing they use the
term code inter indexer consistency and
it is measured with some pure form of
the remind you and some familiar went
from the area of information retrieval
basically what they do they compare the
number of terms to indexer have in
common to the number of terms each of
them assigned to the same document there
were a few studies on indexing
consistency and and they all had in
common that indexer sent seldom agree on
terms that describe documents topic they
rather disagree on concepts yes
by Allah queen the question is what is
about meaning of all values and one what
when 0 into also conserved until my
trousers I'm attracted and actually when
I analyzed these formula I discovered
that the m2 is actually always lower
than m1 and m one is very similar to the
F measure I'm not sure what is your
question ok these are the standard
formula that my brain have been used and
some of them computed the number of
matching terms by and taking into
account semantic relatedness between
terms and in that case and consistency
was higher however there was no standard
way of computing consistency by taking
into account semantic relatedness in our
data we we had also noticed that index
or a degree and concept that in terms
and in a new measure that we have been
working on we tried to include the
semantic relations into mathematically
way that makes sense
for a new measure and in the index of
consistency we decided to adapt the
vector model and to represent the set of
index terms assigned by single indexer
as a vector with number of elements in
this vector present and corresponding to
the size of the controlled vocabulary
and it is 0-1 if the terms was selected
by the indexer or not then consistency
between two indexers is the cosine of
the angle between two vectors
representing two indexers index sets
index term sets and the measure is the
standard cosine correlation now how do
we include semantic relations we can
represent relations from that are
encoded in our control to Calvary with
two matrices one wedges represent the
symmetrical relations RT which are
general relatedness and the generality
matrix represent a symmetrical relations
broader and narrower terms
for each of the relation we want to
measure its significance exact match how
important is it went to match exactly
when they generally related or when
they're in hierarchical relations to
each other and to keep the measure
balance the some of this weight should
be one the matter is and the weights i
included into the new measure in this
way and this is the formula that
represents the overall consistency
overall indexer in all documents in our
example collection because we're dealing
with professional indexers who think
that they actually can be seen as gold
standard and we can choose the weights
for exact relations general relatedness
and hierarchical relations to maximize
the to maximize the result of this
measure and we did that and the results
were 0.24 general relatedness and 0.15
for hierarchical relations
our index earth have achieved
thirty-eight percent with the first
standard measure used in the library the
vector model without semantic relations
gained forty-nine percent and when we
included semantic relations and in this
way the measure improved by 2% we were
yes from the auger boxes horas y is
pretty file this is ours the relations
were pre defined by professionals who
created us is ours yes
Oh
only about half
yes
um
the question is how this number was
computed drug
so the in in this in this measure
thirty-eight percent we did it is
computed with the measure showed you on
this slide so if index are agreed on two
terms it's for here and depending on the
number the terms in total is divided yes
okay
I was trying to make my flights more
pretty and I found this picture and my
supervisor told me to I should better
remove it it's a kind of sexist picture
of an older lady who is not very pretty
our stereotype of a librarian I didn't
remove it because it's actually not a
woman it's Benjamin Franklin and I
discovered that he was a librarian as
well and he actually founded the first
lending library in the United States
obviously professional indexing is
expensive and time-consuming and I hope
I could show you that the consistency is
low under fifty percent and even for
professionals who do this job every day
and that they know that we actually
measure them consistency even if the
same person there is a job on different
days it is not consistent with itself
with her itself however consistency is
important there have been study that
confirmed that the higher is consistency
in a document collection the higher is
the retrieval efficiency so where do we
employ humans although their performance
is so bad because give raises are such a
valuable resource I mentioned earlier
that they are you very useful to
organize documents to provide quick
access to them and they can be used
all these different sources to improve
in interaction of the user with the
document collection and of course there
are different ways there have been a few
approaches to do this task automatic
name
I have classified the existing
approaches for automatic indexing into
two groups the keyphrase extraction
approaches are quite straightforward the
document is analyzed and engrams or noun
phrases are selected that appear in this
document and then the characteristics of
this candidate phrases are analyzed and
the most significant of them are
selected into the set of key phrases
these approaches are easy and fast to
implement and they do not require much
training data if these are machine
learning approaches however they are
restricted to mostly restricted to some
tactical properties of phrases and do
not take into account the meaning of
phrases the quality of extracted phrases
is often low because of some mistakes
and Engram selection on phrase
extraction approaches and most
importantly we phase extraction
approaches on not using control terms so
once they are extracted there is no
consistency over a document collection
and which is quite bad the key phrase
assignment approach is also known text
classification text categorization
approaches is where it documents are
clustered into groups according to words
that appear in this document this
cluster may overlap and once a new
document needs to be classified it is
compared to the clusters and assigned to
the most similar ones we label if we
assign the label to each of the class we
have the same result a document with
automatically assigned
and these approaches are more accurate
because they do take into account a
meaning of turn of words however they
need large training corpus we need to
create a cluster for for each key phrase
basically and if we have a very large
controlled vocabulary with 17,000 for
example like in the case of agri walk or
library of subject heading has about a
million of terms these approaches are
not practical greg has been working on
keyphrase extraction algorithm that can
be classified into this group of
approaches and when I came to work
harder I try to improve it and I decided
to combine it with the controlled
vocabulary and I called the system gear
plus plus
I don't know if any of you ever been to
New Zealand Kia is actually a native
parrot bird of New Zealand and these are
birds that make traveling in the zoo and
dangerous they especially common on the
South Island and when tourists rent a
car and leave it in the parkings place
and go and visit some fewer or go for a
book they might come back and discover
their car this destroys they like to
pick on this rubber part they pick up
stones and throw them on on the heads of
the tourists and steal their food
they're very clever Burton I think it's
a good thing for an algorithm and now
how does it work um we have a set of
document and in the first stage as I
know noted before we need to extract
candidates and previously we did not
have a control to acai berry in this
case each end room that isn't extracted
I met one to control of acai berry check
that isn't actually a valid term I'm to
ensure a bit imagine i'm using a student
phrase technique where in the phrase
each and all stop words are removed
praise and then we make this the
remaining words stamped and order it in
the of abeta co order so that phrase is
like predatory birds predated birds
bird predation are all mapped onto the
same phrase also which is not shown in
this diagram if I identify a nondescript
ER in one of these terms I include the
corresponding descriptor instead so the
candidates that may contain phrases it
actually do not appear in the document
in the next step I need to identify
which of the candidates are the most
significant one and to do this i compute
their characteristics and so code
features this is the one one of the
important step and I'll tell you on the
next slide what features have been using
k plus class is the machine learning
approach so we need to train it to do
this we use a document collection with
manually assigned key phrases and for
each of the candidate we can then see
disappear was it selected by the humans
is it a positive or negative example and
depending on how the teacher that is
distributed among the positive and
negative example we compute our model
with naive Bayes and the small token is
then used to compute probabilities for
phrases where for candidates and where
it is unknown which are the significant
ones
the one with the highest probabilities
are selected into the final key Fred set
the rest of
the first to feature is that I've been
using the same as in the previous
version tf-idf and the first occurrence
the later mean that if you phrase 10
turkey were in the beginning or in the
end of the document it is more
significant I added new two new features
when I compared the distribution of
phrase length in the controlled
vocabularies and among the phrases that
when selected manually I could discover
that the distribution differs
professional indexer tend to select
longer phrases so I use this feature and
no degree is kind of semantically
enhanced feature when you remember the
diagram with overweight obesity and
other terms that are shown how are they
related to each other and how indexer
have selected them we have said that the
ones that are related to most other
terms are more significant the same in
the documents if a phrase is related to
most other phrases in the document then
it is more significant for this document
content here is a sample document on
bird predation terms that are valid
index phrases are marked and here are
they in the same diagram form with
semantic relations between them
representative black lines here is it a
bird predation was mapped into predatory
birds predation abuse here and various
occurs here so the no degree of this
term is equal to and in this example
this is a potential key phrase
I used three different ways to evaluate
campus plus the first the standard way
of evaluating indexing approaches where
we compare it to present were assigned
manually is 10-fold cross-validation and
have computed to precision Rico and F
measure and compare it how the various
different from the previous version care
and the results were twice as better as
scarce results when I try to take into
account semantic relatedness in the same
evaluation I counted as a hit or correct
phrase cases where phrases do not match
exactly but they are related and here +
+ F measure was fifty percent and the
difference takia was even higher
indexing consistency in my opinion is a
better way to evaluate automatic systems
our goal to have an automatic system
which is at least as good as human
indexer and that could be the seventh
indexer in our experiment so ideally the
algorithm should achieve as high
consistency with humans as a day among
each other I've shown you that our
humans have achieved thirty-eight
percent with standard measure and 51
with semantic enhanced vector model
measure unfortunately campus plus is not
consistent with humans as they among
each other has eleven percent less with
one measure and thirteen percent with
the second measure that they're
differently room for improvement yes
yes I've done that when I was
experimenting with features to include I
evaluated the importance of of the
features and no degree did contribute
much more to do the results whereas
phrase length just a few percent just
but I still left it's a burger yes this
is correct the sample is very small and
which we need data to evaluate it on on
larger collection but it's just so
difficult to acquire it is very hard to
convince librarians or other people to
index the same documents over and over
again and we are working at the moment
and ideas how to overcome this problem
for example we are thinking about using
students in a class on digital libraries
and give them computer science articles
so they can index them so we can have a
large asset if you have any connections
to low risk wit you know
just behind most
always
I can't hear you so bad if it's the
cloud you know it's it's not true
because I mentioned we do this
nondescript er to descriptive anything
so if a phrase does not appear in the
document but it's semantically related
or equivalent to a face that occur in
document we replace it by that phrase
but
yes where I i met phrases num engrams to
the controller cavalry to extract valid
phrases excuse me
now
yes
the efficient
I cannot speak louder
expensive resource password is did
possibilities
actually it is it is not very hard to
get the question was where to get the
data and that it could be easy to get it
on the web and I agree it is not a
problem to get manually index document
to train the algorithm with it is hard
to get multiple multiple index documents
documentid are indexed by many people I
couldn't find any any of those on the
internet but what I was thinking is
using for example delicious they index
web pages with phrases that are
controlled phrases and there are many
other examples of later
I have here a few phrases few examples
that a phrase that k + + actually
assigned to the document and the global
visited for example here are the phrases
that were assigned by at least two
indexers and here care + + phrases it
does assign the most important term
overweight and food consumption was
selected by three indexer three phrases
did not match exactly index of choices
but are quite similar there are a few
areas that were not matched by care that
we're not extracted by clear plus plus
and one of the gems did not exist in
indexers phrases but still kind of
related to net to the nutrition area and
here is this the same diagram again with
phrases that k + + is selected and you
can see that they are related through a
grove of links or the same that the
index area beside
here is another example to introduce you
one of the problems with campus plus
apart from a few things that went wrong
while making it is a particular bad
example the problem here is that k + +
failed to match some important areas in
the document that indexer were able to
identify because when phrases are
selected into the final phrase give
preset we do not check what topic areas
they actually belong to and in this case
they all belong to the one with the most
important topic but other topics were
not covered and this is what I'm trying
to improve at the moment and probably
you would know from the text
summarization that lexical change the
squid which yes would didn't you say
that you where we can text summarization
it's a technique to identify sequences
of related words in a document like how
is related to sheep and sheep is related
to go and so far and this sequences can
help us to identify cohesive structure
with the text and how the discourse is
flowing throughout the text and we can
identify lexical chains manually or with
the desires automatically and in the
natural language processing lexical
chains were and use were identified with
web net or Rosetta Doris and they tend
to produce quite useful results for
problems like
deck segmentation question answering and
what they haven't been used yet for
kyra's indexing i want to show you how
such a lexical chains created on this
more example first manually I start with
the term natural disasters and this is
the first element in my chain and after
that i'm looking for whether the similar
to natural disaster hazard is related
flash flood is a kind of a certain kind
of natural disaster washed away or
related flood and flag is a kind of
Hazzard and flash flood as a kind of
flood so this would be the appropriate
place for this term so included here and
there flood occurs again story very
strong currents and so far the the
longer is a chain the less is the
strength of the chain and if we compute
if the only restriction is the
relatedness to the previous member
in this case I asked their motors that
can be included into the chain but there
is no space in this case for example
strong currents is not really related to
natural disaster we went stronger chains
and there is another way of computing
the chains is by including them only
they are related to all elements in the
chain so that the kind of lexical net
evolves I've implemented a lexical chain
lexa kinetics tractor with the Yeager
boxes hours and these are the three
chains that were identified in the
stacks automatically the first chain was
much longer on the first slide this
words could be included into this chain
as well but they do not exist in the
younger weapon so I think at this stage
would it would be useful to have more
information some knowledge base sorts
and lexical resources once the chains
are computed we need to wait them to
identify the most significant ones here
is the 11 of the measures proposed 97
that takes into account the number of
occurrences of chain members in this
case for this chain its flight secure
three times natural disaster 1 and
landslides 1 so this would be five and
number of distinct occurrences which is
in this time in this case three I don't
want to say about the quality of this
measure is just one example there are
other weighting schemes they take into
account the nature of semantic relations
between the terms and the graph
structure and I thought it it is
actually possible to learn the best
scoring function by analyzing multiple
indexer data for example if I take these
are the six top lexical nets that were
identified automatically from the full
text of natural disasters in Vietnam and
these are the terms the index terms that
indexer professional indexer have
assigned to this document so the more
of professional indexers terms are
covered in the top lexical Nets the
better is our scoring function in this
case quite a few were mapped with the
scoring function from the previous slide
there Marcus boat and some terms could
not be identified for example emergency
relief however this is the term this is
another advantage of lexical yep that we
can use for this problem even if a term
does not appear in the document but
according to our control vocabulary or
our lexical knowledge base it is related
to all members of a chain we could
include it into the number of our
candidates which would be helpful to to
increase the term coverage of the
document I've found here do pictures of
useful Mexico peaceful change in our
life this is a toilet chain and
lexical chains are particularly
practical and useful as I shown you and
I'm planning to include them in PFS
class for different purposes like
identification of better candidates
calculating the features and and most
important measuring the topic coverage
into the final in the final index term
set I have a few decision to make should
I rather use lexical change the lexical
nets and what is the scoring function so
far and I think multiple industry data
could help to find the answer to this
question and this is my final slide I've
been talking about manual indexing and I
hope I could show you that consistency
although important very difficult to
achieve even for professional indexers
for an algorithm it actually very easy
to be consistent because for each
document in the document collection the
same strategy is used the problem is the
quality so an algorithm can be
consistently bad but Kayla's class
consistency so far as is quite it's kind
of satisfying and not perfect yet the
other problem with manual indexing is
that humans decisions are subjective if
we would ask professional indexer why
did they actually select this particular
phrases they
wouldn't it would be hard for them to
explain an algorithm can always track
back why the phrases were selected and
in machine learning and kind of learns
the subjective scheme from several
humans in my PhD or I just started it's
my first year and I'm trying to improve
kpaz past performance by using new
techniques such as lexical chains and
I'm very interested to see how it
performs in other domains and languages
and hopefully there will be a cube
that'sthat's blast at the end and the
question the main question that bothers
me is whether we could create an
automatic system that could perform
better than humans be more consistent
and produce higher quality phrases thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>