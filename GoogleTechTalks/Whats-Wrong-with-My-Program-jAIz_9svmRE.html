<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What's Wrong with My Program | Coder Coacher - Coaching Coders</title><meta content="What's Wrong with My Program - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What's Wrong with My Program</b></h2><h5 class="post__date">2008-06-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/jAIz_9svmRE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so next we have Jaime Andrews and
I'm excited that he's going to tell me
what's wrong with my program hi I'm
Jaime Andrews University of Western
Ontario and I have a program here to
show you it's called movie database and
it's a very simple program with a little
scrolling interface it just lets me add
movies and what ratings i give them and
remove them from the database show them
what the rating is that I've given them
so let's let's test it out here let's
see how it works I'm going to add let's
say alien with a rating of 5 show alien
that seems to have worked okay add Juno
with the rating of let's say four stars
out of five I I accessed a list of
movies with one-word titles so that I
could I could figure out what so that I
wouldn't have to type very much okay so
that seemed to work add let's say Rambo
let's say we give it one too generous I
don't know so show Juno that seemed to
work show Rambo yeah that works so let
me do some removing here remove Rambo
whoops remove oh that didn't work remove
Rambo ok movie Rambo deleted remove Juno
movie movie Juno deleted move alien
movie alien does not exist in database
that's strange i wonder what happened
there show alien no it's not there let's
try that again add alien with rating 5
ad juno with rating for is that still in
the database their alien yup it's still
there show Juno okay that's still there
remove Juno show Juno no it's not their
show alien okay something's going wrong
here this is I I added these two movies
alien and then judo and I only deleted
one of them and the other one is gone
now too that's that shouldn't be if you
have a database you sort of expect that
when you put something in the database
is going to stay in there and until you
explicitly take it out so there's
something wrong with my program here
what's wrong with my program so what's
what's your theory what's what do you
think is wrong with my program here any
theories list I heard the word list what
about the list
it's inadvertently deleting everything
that's in list okay so let's see if I
but i deleted Rambo first and it seemed
like Juno was still in the database
right so couldn't be exactly that any
other theories here I asked this
question at my university at my
department and somebody said I know
what's wrong with your program you
didn't use formal specification you
showed you should have formally
specified it and so different different
perspectives but I think you're close
you're you're very close so yeah there
are various different interpretations of
this question what's wrong with my
program that we that we ask sometimes we
could be saying what if anything is
wrong with my program so like is there
anything wrong with my program you know
you give it to a tester and say okay
what's wrong with this you know assuming
that they're going to be bugs where what
are the bugs so another way of saying
that as does my program do what it's
supposed to do so I used to work in do
research in semantics of programming
languages and I could never explain to
anybody what I what my research was
about now our work in software testing
so when someone asks me what I do
research on I say well software testing
so testing programs to see whether they
do what they're supposed to do and
everybody understands I never get any
any questions after that so a more
technical way of expressing this might
be does my program conform to its
specification and that specification
could be some formal thing or just a
very informal thing so a related
question to that would be what are the
best ways of finding out if anything is
wrong with my program and ninety-nine
percent of the time in industry what
that means is testing the software so
what are the best ways of testing
software how can we compare them to each
other we could be asking when we say
what's wrong with my program not just to
get one particular test case that fails
but what are the sort of patterns of
failure of my program can i get several
different failures
that expose what generally is wrong with
it and we could be asking actually what
are the faults in my source code that
have caused the failure of the software
so let me go over some terminology that
we use in testing just so that we're on
the same page and saying using the same
terms for things when I say test input i
mean the input to the program that we
use to run the program to run a test
case when I say test output I mean the
output that the program gives me in
response to that input so a test suite
then is sweet and sometimes you hear
test set or test bucket or things like
that it's a set of test cases where we
give the test inputs and usually we want
to give the expected outputs for the
tests as well so a test Oracle is a
program that's used to check the output
of another program on tests and usually
we might program that or we might just
have a human evaluating test outputs so
a success is an instance of correct test
output a failure is incorrect test
output where it's done something wrong
and when I say failure pattern I'm
talking about like groups of failures
that look related to each other so a
fault then is the incorrect code that
leads to a failure so in testing we want
to make a distinction between those two
things so we sometimes use the term bug
like this guy or whatever informally to
mean either a failure or fault so we
might have we might look at what I just
showed you on the screen and say that's
a bug or we might look at some code on
the screen and point to a line of code
and say that's a bug right so we use
failure and fault to make a distinction
between those things so the research
areas that are relevant to this question
I'm going to be talking about today are
well the standard approaches to testing
and comparing them in my research group
recently we've been working on something
called randomized unit testing where you
use an element of randomization in the
testing and we would like to be able to
measure the effectiveness and compare
the effectiveness of testing techniques
so I'll talk a little bit about that and
I'm going to talk about some research on
fault localization that is finding what
the fault is given a failure so I'm
going to be talking about work that I've
done in collaboration with these folks
that are listed up there and also about
research that other people have done
that I'm just interested in and that
I've started to do some work on and
those are some of the people who've
worked on that so standard approaches to
testing are things that have mostly been
developed in industry rather than in the
ivory tower black box testing or
functional testing is where we go
through the requirements we analyze the
requirements and try to figure out test
cases from that white box testing which
is I suppose supposed to be the opposite
of black box testing is also called
structural testing and it's where we use
tools that show what statements have
been covered by a set of test cases and
then try to find new test cases that
cover those statements so that's one way
of dividing the testing task up and
there's another dimension of
classification of testing tasks where
you talk about system testing versus
unit testing and unit testing is where
we're testing methods and groups of
methods and classes so this is usually
something that's done by a programmer
and a unit test case is a piece of code
that contains calls to the methods that
we're interested in testing so a
standard tool for for that for java is j
unit how many people have written j unit
test cases here okay so fair fair number
of people in both both people that I
recognize from our program committee and
also people who are attending from
google so basically you're writing
sequences of method calls possibly
preceded by a setup of the arguments and
possibly followed by checking the result
and that's basically all at j unit does
but it does it in a very night with a
very nice interface and with tools that
help you to do that so what I would like
to know is is there anything better for
unit testing than just doing the
standard white and black box testing
with j unit or something like that like
SI unit or some of the other ex unit
things so what we've been looking at is
randomized unit testing and what that
means is you write a driver program that
randomly selects a method to call among
the methods that you're interested in
randomly selects arguments calls the
method reports the results and then just
keeps on doing that like hundreds of
times thousands of times hundreds of
thousands of times whatever so it
definitely needs a test Oracle of some
kind because you're making basically
thousands of test cases of running
thousands of new test cases so you can't
have a human evaluating that all time
you need something which is doing some
kind of evaluation of those those
results but the potential benefits you
get are that you're able to generate
lots of inputs quickly and you might
generate unexpected inputs inputs that
you didn't think we're going to cause a
failure but that do and that you're
evaluating those results automatically
seal the analogy here with the dart
board this is supposed to be a dart
board I didn't bother to color it in all
the way but with our standard techniques
we're sort of aiming very carefully
darts at this dart board and trying to
hit all of the little bits of it that
we're supposed to hit on you know using
our intelligence to think cross off all
of the requirements and all of the lines
of code with randomized testing it's
like we're just taking a machine gun and
going you know just shooting up the dart
board spring it with bullets this is the
last time I can use this joke because
it's on YouTube now so I can't that's
the end of that of the dart board joke
anyway so the idea is we're just
covering that that dartboard with with
bullets and it might hit sum of the
squares twice or ten times or what
might blow the dart board off the wall
whatever doesn't matter because we're
generating so many of them so we did
some experiments on this on doing
randomized unit testing of some data
structures and we found that when we
compared this randomized unit testing to
just kind of standard black box testing
techniques what we got was that they
randomized unit testing was about 23%
more effective and only took about two
and a half long times longer in CPU time
than these black box test cases and when
we augmented that test suite the black
box test suite with test cases that
achieved a hundred percent condition
decision coverage what we got was the
randomized unit testing was still four
percent more effective and only took
about one and a half times longer in CPU
time in the amount of time the
development time we didn't measure that
so accurately but we think that it was
comparable so when when we saw that we
thought this is weird because this isn't
something that they tell you about in
textbooks it's this isn't supposed to to
work very well randomized testing gets
gets a really bad rap so we downloaded
some data structures packages from
sourceforge that were well tested they
were considered to be stable releases
and had test Suites along with them and
tested them and we found lots of bugs we
didn't find that it took a lot of
development time and some of the results
of that are reported on in a paper in
ASC the automated software engineering
conference in 2004 we developed
something called randomized unit testing
engine for Java and this was inspired by
j unit and we found some bugs in an
example unit that is with the day unit
distribution money and money bag example
so something that no one had noticed
before in the j unit distribution and we
also found a bug in Java util Java dot
util dot bit set of version 1.4 it
turned out that that bug we looked on a
Sun database and it had already been
reported on that database but it's
interesting that we found you know our
little research group in Ontario found a
bug in Sons production code so you guys
have been very quiet and you haven't
called me on what on earth I mean by
more effective but I'll get to that in a
minute let me just tell you what the
fault was in my program and you're very
close calling it was a hash table it was
one of these hash tables that's that was
among those sourceforge data structures
and what it was doing is the hash
buckets were linked lists and it wasn't
updating a trailing pointer as it was
removing things and so when you remove
something in the middle of the hash
bucket it removes everything before that
in the hash bucket so this was something
that we found with the randomized unit
testing so in particular what I
cunningly devised this program so that
it had it was a hash table of size three
and alien and Juno are I was using the
first character modulo 3 as a hash code
and so those are the same thing modulo 3
and they go into the same hash bucket
and so when I deleted Juno it deleted
alien as well I don't really blame the
guy for not finding this bug because
it's quite difficult to find that what
you have to do is you have to add two
things to the same hash bucket you have
to delete the second thing that you put
in and then you have to check that the
first thing is still there would that
really occur to you to to write that
test case not necessarily okay so let's
look at what i mean by measuring test
effectiveness intuitively we would say
that a test technique a is more
effective than a test technique be if a
forces more failures than be so where
it's some way of selecting test input so
that we got more failures so the ideal
experiment to do on that to see if a is
more
effective than B would be to get
programmers to apply those to their
programs so you sort of locate 30
programmers somewhere and get them to
apply a test technique a for a week to
their or two weeks or whatever to their
programs and then same thing for test
technique be and when you've expended
all those you know thousands of person
hours on the experiment in a company
then maybe you can find that you have
enough statistical significance to make
a claim unfortunately it's extremely
labor-intensive to do that kind of work
and it's generally considered to be
infeasible to do those kinds of
experiments so the usual simplification
of this that you see in in research
papers is an experimental technique that
was developed by mostly by these folks
phils Frankel Monica Hutchins Tom
Ostrand and some of their co-workers in
that period where you simulate a and B
by selecting test cases from a big test
pool and you do that on many faulty
variants of a program now acquiring
those faulty variants there are
basically three techniques for doing
that we could develop a program and
identify the faults that are in that
program while we develop it and then
reseed those faults back into the
program in order to get these faulty
variants that we're trying the testing
techniques on that's also very
labor-intensive and there's a lot of
bookkeeping that you have to do to
remember what the bugs were and where
they were and to take them out in a very
careful way so you can reintroduce them
you can go instead to programmers and
get them to hand seed realistic faults
go to a programmer and say okay put in
some some faults in here and make them
realistic okay so that's a little less
labor-intensive because you just have to
spend a few hours doing that but it's
very subjective it's very difficult to
reproduce that kind of experiment in in
a rigorous way so another way is to use
some kind of program called a mutant
generator to generate so-called mutants
of the program and the question there is
whether the mutant generator is
realistic we don't know
problem with labor-intensive notes you
just press a button and you get all
these mutants and it we don't have a
problem with subjective pneus they are
either it's totally objective you take
this program you give it to another
researcher that's what they use on their
on their programs so let me go into a
little bit more detail about this mutant
generation so here's a mutant he's
basically the same as as any of us
except he has forearms and pink skin and
all that and so a program a mutant of a
program is basically the same as any
other program except it's possibly has a
little fault put into it so it's
something that's generated from a
program by applying a mutation operator
so a typical mutation operator would be
replacing a less than x less than or
equals that might cause a fault in the
program so we say that a test input
kills a mutant em of a program P if M is
if the test output of P is different
from the test output of em on the same
inputs so we're just sort of assuming
that that means that that's something
that triggered a fault so the the main
point then of testing when we evaluate
it this way is to kill a mutants kill
all mutants all mutants must be
destroyed you're not laughing but I can
tell you're you're smiling so that's
that's good that's all I can expect so
yeah we judge a testing technique a then
to be more effective than B if a kills
more mutants than B and that is the what
we used in that random randomized unit
testing evaluation so again the question
is is this realistic are we are these
faults that we've we've seeded really
realistic so we did it a little
experiment on this I and two colleagues
at Carleton University lee and i'll be
on yvonne Labiche and then my PhD
student doc Garcia Mina Mina joined us
on that later we accessed ate very well
studied subject programs seven of them
have these hand seated faults where they
went to developers and said look put in
some
realistic faults and one of them was one
of these programs one of the very very
few programs that was developed in a
very careful way with where they were
keeping track of all the faults and then
reheated eating them back in and they
all came with very large pools of test
cases which is what we need for
simulating different testing techniques
so we generated mutants from all those
programs and generated lots of test
Suites from the test pools and then for
each one of them we compared the ratio
of the mutants killed with the ratio of
the faults killed so this is what we
found when we compared the mutants that
we generate it to these hand seeded
faults that programmers put in so these
each of these dots represents a typical
test suite of a particular size so this
dot is saying that a test suite of size
10 10 test cases kills about something
like fifteen percent of the hand seeded
faults in these programs but a typical
test suite of size 10 kills about
something like seventy five percent of
the mutants that we generated and so on
across all of their so we found that the
mutants were easier to kill in the hand
seated faults the mutants that we
generated seemed to be more simple or
that these these test Suites seemed to
be able to detect them more easily than
the hand seated Falls so what did what
what about when we looked at the real
faults in the program that had real
faults well we found that they were
almost identical in fact the mutants
were slightly harder to kill than the
real faults but that it was almost the
same thing so as far as evaluating or
comparing two testing techniques it
seems that if there if we're talking
about real faults that these are it is
possible to use mutants as stand-ins for
real faults and we also the implication
is that when the programmers seeded
these faults into these programs they
were putting in unrealistically hard
faults they were putting in faults that
were more
more difficult to detect than typical
faults that you get from real programs
so this paper came out in international
conference on software engineering in
2005 it has already had more citations
than all of the other papers that I've
ever written put together and I think
it's because this is giving research or
something that they can use in their
research and researchers are the people
who write papers right then so I may
have done all kinds of things that are
used by people in industry I don't know
but they don't tend to write papers as
much so this is something you can find
on google scholar actually okay now I
want to also talk about a little bit
about some stuff that is coming along
that I'm very excited about and
interested in I haven't personally done
that much work on it yet but I want to
get into that so let's say that we have
some failing test cases can the computer
help us to find the fault that
corresponds to the failure and I hope
that no one who's listening here or
watching will be too offended if I say
that the current state of the art or at
least very close to it is the tarantula
system that was developed at Georgia
Tech by Jim Jones who's coming to to
work at UC Irvine in a few months and
Mary Jean Harold and some of their
colleagues so there there are there are
other approaches and not completely
comparable but but this is very close to
the state of the art at least so let me
tell you about the tarantula algorithm I
got this from wpclipart that site I
don't think this really is a tarantula
but it's called a tarantula on on
wpclipart the so the tarantula algorithm
is the the input is a program and a set
of successful test cases in a set of
failing test cases and the output is a
ranked list of program lines where the
most suspicious is at the top and then
the least suspicious is at the bottom so
the algorithm basically as you run the
program on all of the
those test cases you store the data on
which lines were executed by the
successful on failing test cases and
then you in put all that into a big
formula went to get a ranking between
zero and one for each line so let me
I'll show you something for the movie
database which is not really it's not
tarantula but I'll give you a flavor of
what happens with it i will show you
some let's see success input.text this
is a successful test case this is input
that succeeds for my little program
there's some input that fails right so
this actually I designed these to be as
close as possible to each other but this
is the one that manifests the bug
because we removed Juno first we remove
the second one first and this one works
fine let's whisper it's perfectly so I
have a little script here what is it
called compared up bash so that just
what it does is it runs the hash table
application there sorry the movie
database application on the successful
input and then it runs a program called
G cough which is something which
measures the lines of code that were
that were executed by a test case and
puts a report on something with the
extension G cove so I do that with a
successful and with a failing test input
and then I compare those using diff so I
get the diffs between those in this file
diff text ok so details aren't that
important but let's run that and see
what we get compare dot dash I have to
say dot slash because I haven't set up
my path ok right so we did all that and
we get from what we get from g cub is
this is a public domain thing that comes
with GCC by the way if you have GCC then
you have g cove it tells us how many
lines have been execute
and let me just show you like this
success dot G Cove file this is what we
get we get the source code on the right
hand side the line numbers here and here
in this column we have minus if there's
no executable code we have a number if
it's been executed if that line of code
has been executed that number of times
and we have a line of hashes if it
hasn't been executed at all so here we
see that this line has been executed
three times and four times two times
these lines haven't been executed and so
on so let's look at that diff stott text
and see what we get so what this is
telling us is that on the successful run
line 71 and 72 were run for x + 2 x
respectively and they were on the
failing run they were run five times and
three times respectively and all the
other lines of code will run an
identical number of times so let's this
is all of the differences here 71 72 75
let's look at that source code go to
line 71 I'm a VI guy yeah so this is
this is it basically this there's a
trailing pointer prior p that isn't
updated and this is precisely where the
bug is you can see that I have a fix
that I put in and I've commented it out
so as to retrieve that failing behavior
so it was able to something some
analysis like this was able to find
basically where the fault was now
tarantula doesn't do that it's more
sophisticated than that it has this big
formula but it works pretty well how do
we evaluate how well it does and tools
like it well there's now a sort of
standard way of evaluating and comparing
these tools which was developed mostly
by some people in Germany clava hoga
Cleveland and andreas teller so what
we're doing is we're assuming that the
programmer is basically looking at this
ranked lines we have this model of
programming programmer behavior where
they're looking at these Rick the ranked
lines and they're going down through
each line and seeing whether is that the
fault is that the fault is that the
fault and so when they actually get to
the actual faulty line we're assuming
that that's the end of the search
process and they have found it and all
that so the saving that we get through
using one of these tools is how much of
the program does not need to be examined
by the programmer when they're aided by
this tool so we can say that that's the
accuracy of tarantula or tools like
tarantula is that the percentage of the
program does not need to be examined so
what we can do is we can run tarantula
on different faults with different test
Suites and various things and compute
how often the accuracy score is greater
than a given value because it does
depend on the kind of fault and the kind
of test suite that we have and this is
the same with all tools like this so
here for instance is a something that I
took directly from the Jones and Harold
automated software engineering 2005
conference paper it shows that for
instance here this this point at the
right beside the 40 means that in forty
percent of the test runs for this
particular program it was the programmer
do not need to examine 99 percent of the
code so it localized the fault to within
one percent of the lines of code this
one means that in not in yeah something
like eighty seven percent of the test
runs the ninety percent of the program
did not need to be examined so we want
this to be up in this corner as much as
possible and up at the top as much as
possible we want as much space
underneath the curve as we possibly can
so it was able to fairly accurately
determine the locations of these faults
now this program this space program is
one of these famous subject programs and
this is the one with the real faults
that I talked about earlier that where
they developed a ton kept track of the
real faults and then receded them so we
did that same process in our research
group with another subject program
which we just developed it's called
concordance and this is what we and we
we've been sort of repeating all sorts
of classic experiments with this one
with this new subject program and this
is what we found for tarantula so what
we found is that it did very poorly on
one fault that's why we have this big
gap at the top there was one fault that
tarantula just couldn't handle at all
and it gave the worst possible score for
the actual faulty line but if we ignore
that one it actually did fairly well
fairly similarly so on about 20 what is
that twenty three percent of the test
runs it was able to localize it within a
hundred percent so if we go backwards
and forwards between these two things
you can see it is pretty much the same
except for the part right at the top
there so this is sort of an independent
corroboration or a repetition of an
experiment the kind of thing that we
want to do in experimental studies so I
as I said I haven't done that much
research on tarantula yet but we're
starting to do that in our research
group the kind of thing that I want to
know is how does it perform on another
program with real faults well I just I
just showed you something about that
does the composition of the test suite
affect the accuracy of tarantula and
actually Jim Jones @ @ XE this year at
the international conference on software
engineering this year gave some results
on that does the nature of the fault
affect the accuracy something about the
the fault that could be that could cause
differences how does it perform on
mutants versus the real faults so this
is the research that one of my students
wantagh juan is doing for his MSC
studies and i also have a PhD student
named China alley who's interested in
how how you could use machine learning
techniques to learn about how faults
correspond to failures from all of the
test data and maybe come up with some
something better than tarantula
basically we don't know how tarantula
works or why it works it's this formula
are there other formulas that might work
better and other people have have
proposed different formulas I also want
to see how randomized unit testing fits
into all of this because the potential
would be that you could take your
software run your randomized unit
testing to get a lot of failures out of
that maybe some successful test cases
feed all of that into a fault
localization process and get some idea
of where the fault is so a lot of
automation right from the when you've
got a clean compile all the way to maybe
guessing at where there might be some
faulty lines in the unit so I haven't
been able to talk about a lot of
research that the we've been doing
especially in randomized unit testing
there are some people at NASA including
Alex gross who have been working on this
for testing UNIX implementations that
are going to be on spacecraft and
comparing them to unix implementations
on other platforms there's this idea of
differential testing where you have a
reference implementation and also a new
implementation is one place where you
can use randomization because you have
an Oracle already you have the the other
reference implementation as an Oracle so
I have another master student Melissa
lesson is working on doing randomized
testing for device emulators like cell
phone emulators or game console
emulators to see how reliable they are
so lots of lots of research in this area
I hope I've given you a flavor of some
of it I usually have had in the past as
my last slide picture of a light bulb
which is the universal symbol for bright
ideas and then thank you and then any
any questions unfortunately the slide
that I had used an incandescent light
bulb and that's just not you can't do
that in California these days so thank
you very much are there any questions
yes thank you
the random test cases yeah
so when you guys ran a random test cases
what method did you guys used to verify
that the correct out the output for each
of the random inputs is right okay yeah
I'll I actually have something on my on
my demo directory for that as well so I
have a the thing that we had was I'll
show you what the driver does so H table
driver so this is just oh yeah have to
give it a number of calls so what this
is is something where I can run it and I
can give the number of calls to these
hash table functions that I want to make
and it'll call them and I've decided
that I'm going to use like integers as
the keys and integers as the data as
well so a che will put 13 14 6 13 that
means key 13 and 14 6 13 as the data and
it's inserting that so what we get out
of the driver is something like that i
can say i want a hundred calls i can say
i want a thousand calls and you know it
takes not very long tube to do that so
what does the oracle look like lips the
Oracle is this is actually what sort of
led us to this this whole area of
randomized unit testing we were using
state machine based test Oracle's where
we're assuming that where we have a log
file like from one of these random
drivers as input to the Oracle and then
we're describing what it would mean for
that output to be correct and we're
using the same assumptions about the
input so that it's integers and that
kind of thing so we found that this
isn't that difficult to write but
another sort of piece of evidence that
these Oracle's might not be difficult to
write as J unit and the fact that people
are doing that in j unit quite a lot now
so i don't really this is some this work
on log file
analysis with stuff that I was doing a
few years ago and if you're doing it in
Java I wouldn't necessarily recommend
doing it this way there's somebody
called Harry Robinson who used to work
at Microsoft I think now he works at
Google actually who's worked on
model-based testing and what he means by
that is you write a model maybe with the
very simplified data structure which is
something which is supposed to be a
model of the actual unit or the actual
program that you're testing and that's
what serves as your Oracle so you're
just you're just using some kind of
simplified version of it does that
answer your question is that does that
get to what you're asking ok any other
questions ok thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>