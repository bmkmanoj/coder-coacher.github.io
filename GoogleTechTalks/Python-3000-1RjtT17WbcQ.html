<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Python 3000 | Coder Coacher - Coaching Coders</title><meta content="Python 3000 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Python 3000</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1RjtT17WbcQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I know you've all come here to see Guido
of course but I wanted to first say that
this is the latest in our series of
talks about programming languages topics
at Google the goal of the series of
talks is to have everybody who knows
something about programming languages
that Googlers in general don't know come
up here and give a series of talks
obviously we're very lucky here today to
have we don't and Rossum the benevolent
dictator for life of Python but you
don't have to be Guido van Rossum to
give this talk if to give a talk at you
to give this talk you do but to give it
time but but to give a talk at the
series you don't have to be so pleased
if you have ideas for talks if you want
to give a talk come up and see me my
name is or email me my name is Jeremy
Manson and again my okay so now I should
move on to the actual meat of the talk
here again we know van Rossum is the
benevolent dictator for life of Python
he is the creator and father of Python
and we're very very lucky to have him
able to give a talk to us to here today
and here he is Thank You Jeremy thanks
for giving me the opportunity to do a
preview of my talk which is going to be
a keynote at Python by the Python
conference next week reminder for the
Googlers we're going to put this up on
Google Video so please don't ask any
Google sensitive questions quick
overview of what I hope to be talking
about and I make I'll make sure that we
will not skip the last two bullets what
you can do today and questions what
happened since last summer they're mean
I mean I started giving Python 3000
talks well I really started giving
Python 3000 talks about seven years ago
in 2000 for a very long time it was
purely a daydream it was purely
conceptual was going to be the next big
thing last early last year we decided to
really go make an effort fix a set of
features and actually start implementing
and so
gradually over last year the plans
became more solid and we had various
revisions of a schedule I'll give a
little bit of a timeline I'll give some
highlights if you have to leave in ten
minutes
stay until the highlight slide is
finished then I'm gonna have a long
laundry list of various things that will
definitely or most likely or in one or
two cases potentially make it into this
new release and the things that are sort
of most interesting from the developers
from the from the end users perspective
I'll try to say a bit on how you turn
your Python 2 code into Python 3 code
which is not completely trivial but also
doesn't have to be tedious completely
manual process and I'll I'll start
giving some hints and over the the next
six to nine months those hints will will
probably improve in quality on what you
can do to your code today to be ready
for Python 3000 basically to make the
final transition as easy as possible so
we started with having lots of
discussions at some point I actually had
to say we've had enough discussions
let's get down to implementation work
and I had to say it at several times and
I think the last time I said it was
around Christmas 2006 and since then
it's it's really been very much nose to
the grindstone work out the details on
features that we know we're going to
have and work on the implementation and
sometimes the implementation actually
informs the specification as things go
we did write quite a few peps still not
enough in my view and I think we were
pretty much on schedule in terms of
writing code but it's certainly going to
be a big effort between now and June
which takes us to the timeline slide I
hope that by April this year we'll
really be done with the sort of the
feature proposal process in the feature
selection process should be done soon
after that because that typically goes
hand in hand
we don't collect all the proposals and
then there is a long pause where
somebody selects them we discussed them
as they are being proposed and as they
are sort of finalized that also means
they're accepted so I hope to be to be
able to complete that by April then by
June I hope to actually have a first
alpha release then I'm giving myself and
the developers another year to sort of
work through the feedback shape of the
the sharp bits improve performance
because at the moment we're really
feature driven and performance sometimes
goes by the wayside increasingly get
users to actually try the new Python
with their source code with their
applications and then hopefully in 2008
in June were somewhere in the middle of
the of next year will actually have a
release that we can be happy with that
doesn't mean that at that point everyone
who is using to point X will be forced
to upgrade there's going to be a Python
2.6 release actually somewhat earlier
than the planned Python 2 3.0 release
although you never know releases tend to
sort of fluctuate a bit 2.6 is the first
release that is going to make an active
effort to also incorporate things that
will help you transition to Python 3000
it will in some cases have options that
turn on warnings for things that are
going to disappear and in some cases
port features from Python 3000 will
actually be back ported into 2.6 and
unless the transition goes really smooth
for everyone immediately it's very
likely that there will also be a 2.7
release at the usual schedule for the
two point X releases so the highlights
and I have I have more slides on each of
these but print is going to be a
function that is sort of I just
implemented that last week and I really
have to get used to it still but it is
the right thing to do
dictionary views are even fresher oh by
the way a single star means that there
is some working code but it's not
complete and two stars means that it's
currently completely vaporware but we
know we're going to do this question
marks means that it's not just paper
right we're also not sure that we're
going to do that but there aren't any
question marks on this slide
dictionary views is another thing that
will impact many people's code basically
dict of keys and items and values will
return something that smells like a set
rather than like list comparing objects
is also changed at least a default
comparison will sort of be more typesafe
and less lenient probably one of the
biggest things certainly in terms of
implementation is Unicode we're going to
move to a more Java like model where all
strings or Unicode and we have a
separate bytes data type which is more
like an array of small integers than
like a string now that means that we
also have to implement a new i/o library
which I'm actually pretty optimistic
about some things that have already been
done into integer unification which
means that there's only one integer type
there's no more long no more long
literals and you can be pretty you can
get pretty close to that even today in
Python 2.4 you you in 2.4 you almost
never have to cast things too long
anymore and you don't have to cast them
back from long to int because most of
those conversions are taken care of by
the system in 3000 the long type will
completely disappear integer division
will return a float that's been a long
standing way
mine actually you can turn that on in
Python too since I think probably since
Python 2 1 or 2 0 even you can turn 2 1
Thomas knows everything but not too many
people use it and then of course there's
lots of other cleaner like string
exceptions no longer exist classic
classes no longer exist we're changing
the race statement and so on and so
forth so a little bit more on many of
those items and a bunch that didn't make
it to the highlights page prints print
is a function we had a discussion and
there were a couple of competing
proposals one of the proposals was that
they were going to make it a function we
should also drastically change what it
does maybe not insert spaces between
items maybe have printf functionality in
the end we decided to actually go with
very simple transformation where we have
a print function that is just as
convenient as the print statement is
currently so in most cases all you have
to do is put parentheses around it by
the way you won't have to edit your code
yourself we have a conversion tool and
while the conversion tool is far from
perfect this is one of the things it can
do really well there's this funny
business with a trailing comma that
suppresses the trailing newline you can
simulate that by I mean print function
will have three different keyword
arguments it will have end which is the
character that is printed at the output
at the end of the list of arguments
which defaults to a newline there is sap
which is the thing that gets input
output in between items which defaults
to a space and there is file which is
the file where it's going to print we
printed too which defaults to whatever
cystal stood out is at the moment so
these three forms of print syntax all
translate to very straightforward calls
to the print function and there's some
functionality that you can't easily do
with the print statement at the moment
that you can do by setting for example
the SEP keyword to an empty
dream we can automatically translate
this the only place where that fails is
in it turns out the print statement has
a couple of bits of cleverness where it
works with an attribute on the output
file named soft space which is mostly
hidden but it's actually accessible to
end-users if you really want to and the
soft space is attribute is used to delay
the outputting of the space between
items until you actually know that you
have a next item that is pretty murky
semantics and it means that everybody
who implements a file like object at
some point finds that they also have to
support the soft space feature so I
decided to just get rid of that it does
mean that there are a few corner cases
like if you print a string that ends in
either newline or a tab character and
then comma and another item the current
print is cleverly suppressing the space
between the two items the print function
will intentionally be slightly Dumber
about that
so I actually when when converting the
standard library and the standard unit
tests I had maybe I think on maybe five
cases where I had to fix this manually
in the code usually it's very
straightforward
so dictionary views this has a star
because the dictionary views currently
while implemented don't quite behave
like set objects yet they can be
compared to set objects but they can't
quite implement they don't quite
implement all the operations that you
expect on set objects like Union and
intersection they do sort of have the
basic functionality you can iterate over
them you can do a membership check and
you can compare them to another set for
equality which is actually a relatively
big deal in the past if
wanted to see if two dictionaries had
the same set of keys you would have to
make a copy of each dictionaries keys
into a list and then sort the lists or
make copies into sets if you're sort of
using a more recent version of Python
like 2/3 which has a sets module or 2/4
which has sets as a built set as a
built-in type and then you had to you
could compare those those two sets or
there's two sorted lists problem with
that is that if you have a large
dictionary you end up making a large
copy of all the keys what you can do
with the keys view is actually you can
just compare the two keys and because
they act as sets it will automatically
and efficiently compare whether the two
sets have the same same elements whether
one is a subset of the other and vice
versa just a mathematical definition of
set equality we do the same thing for
items items also returns a set view the
values of course cannot quite return a
set because you can't have duplicates
and you'd like those duplicates to show
up when you iterate over it
we continue to maintain that the
invariant that if you iterate in
parallel over the keys and the values
that you get matching keys and values at
the say at the same position in the
sequence as long as you don't of course
modify the dictionary while you're
iterating over it this of course is all
being borrowed from the Javas collection
framework I'm not afraid to borrow stuff
from other languages never have been I
don't think I would have gotten anywhere
if I try to invent everything myself so
the important part of the keys our
dictionary views in general I expect
that keys and items are going to be the
the most important ones and values are
going to be only rarely used in practice
mostly probably in unit tests that's
that's where I found most of the uses
these these few objects are very
lightweight because they're basically a
structure containing one pointer which
points to the original dictionary so
it's rating over a keys view or it's
rating over
of those three views was actually
trivially implemented because I even
though I removed the in Turkey's inner
items and area of values methods I
didn't remove their implementations and
their implementations are still useful
as the it as the iterator is over the
view objects so because I actually did
some of the work on this over the
weekend there are two unimplemented
parts of it one is as I mentioned the
set semantics are not complete you
cannot check whether your keys object is
a subset of some others a keys object or
another set you can only check compare
them for equality the other thing is
that we currently have about fifteen or
twenty failing unit tests still I expect
that most of those unit tests are
failing for very trivial reasons like
they're assumed I mean what a lot of
code does is it assumes that Keys
returns a list and then it compares that
the unit test especially often do things
like they create a little dictionary
they mess around with it a little bit
and then they test at the list after
sort the keys after sorting have a
certain value when they usually just
compare the keys object with a list of
constants that doesn't work anymore you
could fix that in two ways you can
explicitly cast the view to a list
object and that sort of fixes it solidly
but you can also replace your list
constant that you compare it with a set
constant which I haven't mentioned yet
but which is one of the later slides we
have set literals now so much for
dictionary views the default comparison
I already mentioned that in the
highlights equality and not equal of
course compare whether I mean you have a
default comparison and you can overload
your comparison you can implement your
own comparison any way you want it I'm
not touching any of that but the default
comparison that you get when you derive
from object and you don't overload any
comparison operators is changing quite a
bit in Python too even in Python one
even if I from zero I think if you
compare two objects of different types
for with an ordering relationship we
just compare the address of each object
and say the one with the lower address
comes before the one with a higher
address that turns out to be mostly a
useless comparison it can give you a a
sort of a false sense of security that
if you sort or compare something and you
don't know what the types of the objects
are it's not gonna throw a type error
but actually you want to throw a type
error because most of the time if there
are objects of different types that
aren't really comparable that have an
explicitly programmed how they should be
compared with each other
the default comparison is it's just
giving you random results and maybe in
one run this object always shows up
before that object but another run
because you have slightly different
input data there allocation on the heap
is different and the object that was
smaller first is now certainly larger
and you can have all sorts of bizarre
situations where you have flaky unit
tests so in particular this means that
you can no longer compare or sort
integers and strings just like you can't
concatenate them or do anything else
with them before converting that's
pretty much it in practice I have not
found that this this affects much code I
mean I have found very little code in
the standard library that actually
relied on this default comparison
existing except again in unit tests that
were specifically checking this behavior
which always feels good to rip out code
so then we get to the scary thing and
it's scary for me because I haven't
started implementing it yet
I think it's also scary for application
developers because it can potentially
affect application for
Foreman's application semantics it's
it's going to be one of the bigger
things for converting code to Python
3000 now if you're not using Unicode
today in your application you're
probably pretty safe but if you're using
Unicode today sort of everything you
know about keeping track of encoding x'
and which strings are unicode and which
strings are not unicode will probably
have to be changed somewhat so again
we're borrowing heavily from java
there's going to be one string type
named stir but again it's implementation
will be most likely that of the two
point x unicode implementation i will
have a separate bytes type which is new
brand new although its implementation
resembles resembles most closely the
array module that has been around since
probably since python 1/5 or so you can
only ever go between these using and
encoding if you compare them or
concatenate them if you compare bytes
object to a string object it'll just
throw a type error so this is yet
another place where that the change to
the default comparison is actually
helpful because it just points out that
you're doing nonsensical operations
quicker
what will completely disappear and this
is actually a big improvement and the
main motivation is the endless problems
you have in current Python applications
that use a mix of 8-bit and unicode
strings and occasionally encoded unicode
ends up in 8-bit strings so you have
characters with a high bit set and then
suddenly they will not interoperate
happily with actual unicode strings the
thing is if you have an 8-bit string
that only contains ASCII characters you
can concatenate it or compare it to a
Unicode string just fine and it will
have this sort of the proper semantics
but if you have an 8-bit string that
actually uses bit number eight of at
least one of the characters in that
string you certainly cannot compare it
or concatenate it to a Unicode object
and unfortunately this often happens
after your application has been deployed
especially web applications the
developers live in the u.s. they do a
lot of testing they type in their name
and there's never an accented character
around then their first French customer
enters their login name and everything
blows up painful so we hope that by
forcing you to sort of do all the
conversion between bytes and Unicode at
a much more specified point slightly
earlier in the life of the strings you
won't in I mean you basically if you
make a mistake and you do not explicitly
convert your bytes to Unicode typing a
name without accented characters will
also not work so you'll you're much more
likely to actually have effectively
tested your application for all use
cases this has caused a lot of
discussion and I think that's still an
understatement there are lots of
different implementation choices my
personal choice would be we'll go with
the basically the Unicode data type that
we currently have in Python to point
well since python 2.0 it hasn't changed
a whole lot
it uses an eternal representation that
is either two bytes per character or
four bytes per character when it's two
bytes per character technically it's
utf-16 because you can have surrogates
in there if you care about that but this
york the surrogates for most practical
purposes look like characters to the
application unless you really go dive
deep into unicode that is one possible
implementation another possible
implementation would be to keep a
similar thing but actually have three
internal representations one that is a
single character why single byte white
one is two bytes wide and one that's
four bytes wide this means it's less
easy to use some of the c standard
library
that might exist or extensions of the
standard library that might exist for
working with Unicode characters on a
particular platform on the other hand it
means that you never have to worry about
surrogates because the surrogates would
always be converted into four byte
characters it means that if you have a
string that contains one character that
doesn't fit into bytes the entire string
is four bytes per character that's a
compromise currently you can compile
Python in such a way that all Unicode
characters are four bytes wide it's sort
of a cultural choice whether whether you
it's it's worth to you having the bytes
I haven't having a character to be wider
and not having to worry about surrogates
the capi issues frankly are mess
generally my my approach to Python 3000
is first I want to get the sort of the
Python programmers api's cleaned up and
well it's too bad if extension writers
will temporarily have to deal with a
sort of a slightly messy set of api's I
mean it's C code you're used to things
being messy there is a different faction
in the Python developer community or at
least in the people who are quite vocal
in the Python 3000 place which is not
necessarily the same who would like to
see things like well the most extreme
view is actually support variable length
and codings as the internal
representation for example if you have a
large file containing unicode data you
might want to read that into something
that calls itself a string object but
actually still contains unicode the
utf-8 bytes internally the problem with
that is now I have ten megabytes of
utf-8 and I have a program that sort of
tries to walk through that code from the
end or just random read
the excesses bite 7,000,000 there's no
way to find out we're sorry character
7,000,000 there's no way to find out
where character 7,000,000 is without
parsing the first 7 million characters
you can try to optimize that keep a
cache of a couple of pointers but it
gets Messier and Messier and more and
more complicated I'm not sure that
that's that's at all a viable idea maybe
someone can prove me wrong by actually
coming up with an implementation that
I'm skeptical a slightly less ambitious
but still very controversial idea is to
optimize things like slicing operations
and potentially also concatenations so
that if you for example if you have a
slice you have a string of 10 megabytes
and you take a slice of 4 megabytes out
of that string currently Python always
copies you could say well let's just
share that array that already contains
those bytes I mean after all they're
immutable objects they can't change once
once they've been read into memory there
they are they're that they're the
objects not going to move
unfortunately it's most of most of the
implementations of that idea are very
easily lured into a worst-case behavior
where you do something like you read
repeatedly you read a megabyte string in
and you slice 30 bytes out of it or
something and so now you have a 30 byte
object as a 30 character string object
that references a slice of a megabyte
long string object and you can't D
allocate that megabyte until you D
allocate the 30 by 30 character string
and you can try to work around that with
your wrist excite is really small you
you copy anyway or if it's small or
small relative to the size of the
original or you can try to use weak
references to sort of dynamically copy
and
the only effect of that is that you have
more and more code that could go wrong
and less and less actual performance
benefit so I think that in the end the
approach of very straightforward simple
algorithms that always copy is still
going to be a winner but I'm I'm trying
to keep an open mind about this so the
bytes type the best way to think of it
is a mutable sequence of small integers
so it behaves a little bit like a list
but the values you can you can store
into it are limited to being integers
they have to be positive and they have
to fit in a byte it also behaves a
little bit like a string there is a
bunch of string methods that make total
sense for byte arrays like find on the
other hand certain string methods that
are locale dependent or character
encoding dependent will definitely not
be allowed like you will not be able to
lowercase or uppercase a byte string a
byte array to go from a bytes array to
string use its encode method to go from
string back to byte array you could use
its decode method and those always
require an encoding parameter if you
want some kind of default encoding
you're gonna have to dig it out of the
environment yourself bytes type is
actually being implemented some of the
string behavior probably still needs to
be added but in general I'm pretty happy
with it you can actually already use it
for IO in limited situations so that's
nice segue to the new i/o library which
is yet another idea inspired by Java and
you could also say it's it's part a
little bit by Perl which also has
stackable components in its newer i/o
library so at the very low level
you can read bytes from a well from a
file descriptor a file handle on UNIX is
going to be a tiny object that wraps a
UNIX file descriptor on Windows it's
going to be a tiny object that wraps the
windows
file handle it provides read write close
seek and tell methods
there's no buffering going on and it
always talks in terms of bytes doesn't
do any carriage return line feed
conversion either if you start on a
brand-new platform that is not at all
like UNIX or Linux or Windows or Mac
you're going to have to provide your own
low-level byte IO implementation most
likely there's actually UNIX emulation
library that you can probably use as
long as it you can turn off any
character translation features it might
have I mean that that's that's a
possibility for Windows 2 but on Windows
that are actually slightly lower level
things that are more efficient and more
flexible but that's the only thing you
have to do for a platform
I mean buffering unicode encoding
decoding character and line-feed
translation all those things can then be
built on top of that without any
platform specific stuff using this I
expect that in most applications unless
you're doing very messy stuff where
you're sort of not sure whether you're
reading a reading binary data or text
which of course happens you will not
have to change your program you the open
function will continue to return a file
object you can tell it to open a binary
file or a text file for reading or for
writing all those things will still work
however if you open a text file read and
write will use strings if you open it as
binary region right will use byte arrays
so that's probably if you if you're
doing binary i/o you're more likely to
have to change your code than if you're
doing text i/o now how does it decide
only encoding when you're doing text i/o
and you don't specify the encoding
in the as an extra open parameter open
will have a keyword parameter that will
let you specify it and then coding but
if you don't specify that it's going to
pick a default and I can imagine a
number of different ways of picking a
default you can say well we'll pick
ASCII or we'll pick utf-8 or will sniff
the file and actually see whether it
looks like utf-8 or utf-16 little and
you know big endian encoding you may try
to see what the users environment says
about file formats there are a couple of
different ways I mean if you're if
you're dealing with the TTY device in a
Windows environment I think the tty
device actually knows what encoding to
use so that would be another way to get
your encoding by default I expect that
when you're using an opening a file for
binary IO you will not be able to use
the read lines or read line methods
unless it turns out that a lot of code
breaks I mean I don't actually honestly
know if there is much code around that
has a legitimate reason for calling red
line on binary files but there might be
so we'll see an interesting thing is
also how you're going to tie these
things to sockets but I think all the
socket has to do is provide little
wrapper that implements the same
read/write operations that their lowest
level or binary i/o object does and you
have to somehow decide on what your
encoding is latin-1 or ASCII or
something else and then you will be able
to read and write from sockets by the
way we're completely weaning ourselves
off the CIE standard i/o library for a
number of reasons mostly having to do
with the CIE standard i/o library not
actually always providing a
functionality that we need like it
provides
buffering but it doesn't provide an API
to see how many bytes have been buffered
if there's anything buffered it doesn't
have a way of peaking in the buffer we
need those things there's also this
thing that the CIE standard i/o library
says that basically you could expect the
sec fault or World War 3 when you read
and then suddenly you start writing the
same file descriptor even if the thing
is suitable for reading and writing you
still have to seek when you switch from
reading to writing since the standard
i/o like the CIE standard i/o library
doesn't promise you get a neat error
message when you forget to seek in
between that's really unpleasant thing
for pythons we have so Python has to
keep track of are you reading or are you
writing so we end up sort of redoing too
much of the CIE standard i/o library
functionality anyway so we'll just throw
it out and hopefully have a bigger and
better implementation so int and long
unification is a really simple thing
currently python has small ends named
int and large in named long the large in
SAR actually arbitrary precision so you
can represent numbers as long as they
fit in memory the small integers are
actually mapped to C long so there are
32 or 64 bits depending on what kind of
platform you have that was really a
mistake and I made that mistake sort of
very very very early on in pythons
design and over the years we've made
more and more compromises where you can
use in then it will actually behave as
if it were long if it doesn't fit like
in older versions of Python if you kept
multiplying numbers together and the
results got bigger and bigger at some
point you get an overflow error in
modern pythons I think it had started in
Python 2 3 or so certainly in Python 2 4
when the result doesn't fit in 32 bits
or in 64 bits on some platforms you'll
just get a long integer
and more and more places if it doesn't
fit in a small integer we'll just give
you a long integer even to the point or
if you if you if you called int function
and somehow the int function can do a
couple of things you can convert a float
or a string or another integer to an int
in most of those cases is through if the
result is a valid integer but doesn't
fit in 32-bit so it's a valid
mathematical integer nowadays int will
just return a long object and so the
only place where you're still aware of
the difference between ins and Long's is
if you're explicitly checking the type
of your your objects if you say if is
instance X comma int then do this
otherwise do that then your code won't
work when someone passes you along even
if it's long containing a very small
value so that long thing becomes less
and less useful and in Python 3000 were
just throwing the type out we looked at
the number of different implementations
what we chose was actually taking the
long implementation and renaming it to
int at least at the Python level in the
sea level the distinction between long
and int is still very much visible we
did have to optimize it a little bit
because the end of the implementation
was traditionally very optimized like it
has a cache office of small integers and
a couple of other allocation tricks the
long type was completely uh knocked my
scythe Inc the new long int type is
somewhat optimized at least it has a
cache for small values we're probably
going to try to get that performance
back up to speed
comparable to the best performance in
Python - point X during the year after
the 3.0 alpha-1 release you know how
close we'll get but I'm hopeful that
some smart people will be able to do
magic there and it makes life for the
programmer much easier because you know
you can actually write if instance x
call my end and it will do the right
thing unfortunately I have no idea what
time it is
I'm worried that that might actually 15
minutes Oh excellent except the tape
runs out
doesn't matter I'll try to be done in 15
minutes I think that's okay so we have
integer division and again that was
there was a very early mistake where I
sort of mindlessly borrowed behavior
from see if you divide 3 by 4 it gives
you 0 turns out that certain algorithms
really sort of find that the booby-trap
waiting to explode when you least expect
it so we're going to make three but
divided by four returned 3/4 doing some
kind of float representation and you can
use double slash if you really wanted
that zero now the double slash operation
has been in Python 2 point X probably
since 2.1 again 2.2 okay I believe you
so you've had plenty of warning and
there's also an option you can pass to
Python to point X that will tell you
when you're using the single slash
operator and it is used on integer
operands so changes to exceptions we're
getting rid of string exceptions
we're also enforcing that all exceptions
derived from a single root exception
type which is called base exception in
practice you should derive all your
exceptions from exception which is
slightly lower in hierarchy than base
exception but you can if you know what
you're doing derived from base exception
also we're going to move the trace back
into the exception objects again I
should mention this is an area where
Java has been leading-- we're cleaning
up the race statement there are two
different ways of raising an exception
with arguments you can say raise e
parenthesis arguments close parenthesis
or you can say raise e comma argument or
arguments in parentheses even that
second syntax was only necessary back in
the day of string exceptions so we're
getting rid of that if you want to pass
a trace back you call a method on the
exception object that you already
created it sort of adds a trace back
object
which also changing the except clause
when you're catching exceptions there's
a pretty common mistake where you wanted
to catch two exceptions but you forgot
to put parentheses around them and now
you're catching the first exception and
when you catch one a local variable is
created with the name of the second
exception in order to prevent that we're
going to use instead of a comma between
the exception and the variable we're
going to use the keyword s also new is
and this has to do with the exceptions
now sort of containing that the trace
back as an attribute we're going to
delete that variable if it still exists
at least at the end of the except block
I'm basically going to put a try finally
in that block that you don't you won't
see but will be there that deletes that
value if it exists which means that if
you want that value if you want that
exception value to survive beyond the
except block you have to just assign it
to a different local variable so we're
not going to do optional type checking
but we are going to add some syntax that
will allow other people to implement
frameworks that do something like type
checking or whatever they would like to
do basically currently every parameter
of a function has a default value it can
have a default value we can now also
associate an annotation with every
parameter the annotation is introduced
by : the default value is of course
introduced by an equal sign you can
combine those the colon annotation equal
signs expression notation you can also
annotate the function return value with
an arrow all those things are evaluated
when the function is defined so at the
same time the function object is created
both the default and the annotation
which are just generic expressions I
have no constraints on that but they
must if they you if they reference
variables those
variables must exist at that point in
time and then you can pull those
annotations out of the function object
by asking for the func annotations
attribute of the function and that's
just a dictionary indexed with variable
names and the keyword return if you want
to do with something with this you have
to do it yourself I can imagine all
sorts of decorators or metaclasses that
make good use of this to enforce all
sorts of things from from actual type
checking to automatic adaptation and a
number of other interesting things I'm
not going to put anything like that in
the language at least not in 3.0 another
small change to function signatures
completely independent from the previous
one both of these have been implemented
by the way sometimes it's really helpful
to have a parameter that is required to
be used as a keyword in your column in
your call syntax if you really want to
enforce that in Python 2 you can use
star star keywords and sort of pull it
out of the stars to our keywords
dictionary but it's kind of messy and
you have to sort of check for each of
the key words that you might expect and
check that there isn't anything else in
there in order to be sort of robust and
and and user-friendly now you can just
use this strange notation where there is
a star without I mean the star of course
normally means star you can use it
already a star arcs which means we have
a variable number of positional
arguments here that gets returned as a
tuple now if you leave the name out from
that syntax you just have a star without
the star arcs and then you cannot
specify arbitrary positional arguments
but you can after that specify more
arguments that will then be required to
be keywords and they don't even have to
be have to have defaults so after that
star you could have C is 42 so that's an
optional keyword parameter but D
doesn't have a default value so that's a
required keyword parameter so every call
to Phu in that case must specify a value
for D and it must specify it using the
keyword notation set literals very
simple you put a number of expressions
in curly braces and it creates a set
object except if there are it's nothing
between the curly braces it still
creates a dictionary at some point I try
to propose to unify the dictionary and
the set object that didn't get a lot of
support from the developer community if
you really want frozen sets it turns out
frozen sets are only very very rarely
used you have to cast that thing
explicitly to a frozen set or of course
you can use frozen set with a list
argument we're also going to implement
set comprehensions those are not yet in
the code base it works the same way as a
list comprehension except it returns a
set absolute import you can already do
that in Python 2 5 from under future
import absolute underscore import that
means that if you import a module using
import foo or something like that
inside the package normally in Python
2.4 and before it first sees if there
tries to find that foo in the package if
it's not in the package it looks in
cystal path in 3.0 or in 2.5 if you have
that future statement in your module
it's not going to look in the package
that solves a particular ambiguity where
you might have a module in your package
that has the same name as a module in
the standard library a top-level module
in the standard library currently
without this future import there's no
way to reach out and actually import
that the standard library module because
the one in your current package will
always be seen for first well you could
dig it out of system modules but only if
it's already been imported by someone
else
if you want to say I definitely want the
food that's in my package rather than
potentially the one in cystal path you
can say from dot import food that's also
already in 2.5 the only difference
really is that in 3.0 you always have
that future statement automatically
implied in your code exact very early
Python versions it actually was a
function it takes an object which is
either a string or a code object or a
file and then optionally Global's and
locals at some point I thought that the
compiler could make good use of the fact
that you were using exact somewhere in a
function and I decided that in order for
the compiler to know about it it would
have to be a statement well compiler
technology has advanced a little bit and
you can actually tell fairly reliably
whether you're using a function like
this so there's no need for it to be a
statement and it's actually easier to
have it as a statement necessary has a
function so it's back to being a
function the interesting thing is this
is very easy to to do in Python 2.0 so
because since it once was a function
that same syntax with a tuple of three
value of up to three values is also
still supported in two point acts so
range just like we have keys and a
Turkey's we have range in X range
because range was there first the rage
creates a list of many integers
potentially many X range produces only
the integers that you asked for so we're
going to change that so that there's
only going to be a function in range but
it will behave most like mostly like X
range the difference is the current X
range is optimized so that it actually
only works for integers that are less
than system accent and Neil Norris has a
patch to fix that but I'm still waiting
for him to upload the patch or something
zip this is actually a pretty minor
issue
zip is something that would be a very
good candidate for returning an iterator
in Python to when it was except it was
introduced before iterators existed so
there's an introduced of izip thing that
does return an iterator but makes much
more sense for zip to be in a Prius
Raider in the language so string
formatting has a couple of problems and
there is a PAP which I hope will be
implemented I'm certainly in favor of
the proposal to give strings a dot
format method and to use curly braces
instead of percent something as the
indicator for replacement in the format
string in here quickly are a couple of
examples you can specify format
arguments by position 0 and 1 or by name
foo if you want to include literal curly
braces you can double them you can even
access attributes or use get item
dictionary notation in simple cases on
the formatting object you can also
specify parameters after a colon I think
that is actually borrowed from dotnet
although I'm not sure that we're taking
exactly the same notation read the PAP
if you're interested so this is
something that's actually probably not
going to make it but I'm mentioning it
anyway because it is potentially an
interesting feature it's just there
there are a couple of difficult
decisions to be made I mean it's very
easy to come up with a decent switch
switch style syntax you can say switch
expression case expression lalala
question is when you evaluate the case
expressions in order to actually benefit
from a potential speed of the like you
could do with this patch based on a
dictionary you would like to pre compile
those case expressions for example
compile them at the time the function is
defined rather than each time the
function is invoked but that limits you
to actually constants and it's not a
concept we currently have anywhere else
in the language which makes it somewhat
problematic sort of conceptually
which is why we haven't implemented it
yet and it's marked with both stars and
question marks another thing that is
likely to be to make it in even though
it's slightly ugly if you have a
function an inner function that
references a variable defined in an
outer function you can use it but
currently you cannot assign to it you
can modify it if it's a mutable object
like if you have a list object in the
outer function you can append to that
list or even index it and change an
element of that list which you cannot
replace it with a different list object
using plain assignment turns out that
there are enough places where people
would like to have that functionality
and we had a long discussion we're
capping ye did a brilliant job of
summarizing the discussion and sort of
guiding it towards perhaps not final
completion but at least closure so that
everybody could agree with what was
written down in the pep we're pretty
much settled on the syntax and on the
semantics the only thing is there are
different flavors of keyword that sort
of each have their own advantage and
disadvantage non-local is the current
favorite it's sort of ugly because it's
a long word and it is has sort of a
negative meaning unfortunately the only
real contenders were global and outer
where the problem with global is the
global for most people's minds has
fairly set semantics which really
doesn't mean just go search outward
scope by scope by scope but really go
all the way to the outermost scope the
global scope so that's why I mean even
though global was my favorite nobody
else seemed to like it very much and I
have to respect my users outer was a
nice candidate until we found how often
that that word is already used as a
keyword as a variable name or a function
name and that
much less attractive so it's probably
going to be non-local which is not
something people tend to use laufes
variable names so another very
speculative thing is abstract base
classes we had long discussions about
interfaces generic functions abstract
base classes actually if the more I
think about it the more attractive they
look from the perspective of a somewhat
voluntary declaration of AI implement a
particular protocol our protocol is a
very informal concept we've had constant
the concept like protocol in Python for
a long time we've been talking about
sequences and mappings as sort of
implementing implementing certain
operations and not others the problem is
if you have an actual object and you
don't know whether it's a sequence of or
mapping there's not really a good way to
decide which one it is you can check
whether it has a keys method but there
are actually some cases where you have
something that really behaves like a
mapping but it Maps an infinite number
of keys and you really don't want to
have a implemented Keys method that
tries to enumerate all of them so if
there was a abstract based base type
that didn't provide any semantics or
implementation that just serves as a
marker class I am implementing the
sequence protocol or I am implementing
the mapping protocol or I am
implementing the file protocol and
there's probably going to be a couple of
dares there is going to be more fine
grained distinctions like you have
readable files and writable files and
readable and writable files and you
probably have mutable sequences and
immutable sequences and very basic
mappings that only implement the the map
operation and sort of very complete
mappings that implement lots of other
functionality like update and keys
but if I get time between now and April
I'll write a PAP about this and then
implementing it is going to be simple
please this is this is something that
just adds some stuff it's going to be
easy to make all the standard types
declare what stuff they they implement
and then it's just up to user code to to
voluntarily follow this mean we won't
stop you from implementing sequence
protocol methods without declaring that
you're a sequence but the sort of the
the carrot in this case is that if you
want to interface with a large framework
like soap or twist it or something like
that it might be that eventually future
versions of those frameworks that work
under Python 3 point acts will actually
instead of sniffing which methods are
implemented actually just look at the
base classes that's the hope anyway so
I'm going to skip the miscellaneous
changes you can get the slides from the
web eventually this is mostly cleanup
very small stuff
library reform is not my own idea of fun
I like to focus on the language language
is big enough that other people are
interested in reforming the library
there's currently not a lot of activity
going on it's certainly something that I
think is a fine project to do after
we've released the alpha 1 release of
the language so again the C API I'm
currently not too worried I'm just
randomly changing the C API as object
types change of course if you're writing
a third-party extension that's not
already part of the Python source tree
you would like to know what's going to
happen at this point the only thing I
can promise is I'm not going to change
functions to have a different signature
but the same name or different semantics
even with the same signature I'm going
to add api's I'm going to delete API is
that
no longer relevant or impossible to
implement I'm not going to change API is
in an incompatible way that would break
your code I am going to require everyone
to recompile the code that's the meaning
minimum I can expect so if your
compilation passes you're somewhat
likely to actually have a working
extension best-case scenario if you're
using API is that no longer exists
you'll get a clearer compile time error
about something that doesn't exist or
maybe a linked time error so now you
have a bunch of Python to point X code
and you want to turn it in Python 3.0
code well you could just try to run it
with 3.0 and fix all the syntax errors
and then fix all the the runtime errors
hopefully you have unit tests that's
going to be pretty tedious because there
even though the general flavor of the
language doesn't change much there are
clearly a lot of small changes that
really add up classic classes except as
different races in tax no comparisons
keys the dictionary views is going to
affect a lot of people print statements
of course is going to affect a lot of
people Unicode is going to be a major
deal for at least some people so there
is a conversion tool now we cannot do a
perfect conversion because in in some
cases it's inevitable that you sort of
have to do with symbolic execution of
the application in order to find out
what the types of a particular variable
are before you know how to convert a
particular call mean if I say X dot keys
there's no guarantee that X is actually
a built-in dictionary it could be a
completely unrelated object that has a
keys method however there's a good
chance that it is a dictionary if you
have something that hasn't its
quis method there is an even bigger
chance that it's a dictionary so what
we're doing is we have a a tool that
parses your code and looks purely at the
parse tree and it's able to transform
that parse tree in place and then write
it back out and we annotate this parse
tree with exactly where the whitespace
is and where your comments are so in
theory certainly if the I know I have
tested that if you don't make any
transformations it's always the output
is exactly the same as the input every
single whitespace character it's that's
that conversion is perfect now if you
make transformations sometimes it's
possible that you would lose a comment
if that comment sort of is in the middle
of an expression that gets completely
discombobulated and transformed into
something completely different that's
not very likely to happen because how
often do you have significant comments
between the parameters of a function but
right after a binary operator not so
common so if you if you're interested in
looking at this code currently you have
to go to s viendo python.org and find
the sandbox code and go to the
two-to-three subdirectory it's
relatively easy to add new conversions I
mean if I've had a couple of Python
developers who started contributing
conversions actually that that's that's
been really great
the idea is you write a pattern that
decides I want to match certain nodes in
the parse tree that look like like like
that match the pattern and the pattern
completely ignores what the comments say
it purely looks at what what the parts
are actually see so there are really two
section two parts to the parse tree
there's the annotation for whitespace
and comments and there is the syntactic
tokenization and parse so the matching
is purely concerned with
matching nodes and leaves in the tree
and the path I'll show the patterns in
the syntax in a minute so you write your
pattern and then you write a
transformation function that sort of
picks the the node you find a part and
puts it back together in a different
order and returns that that new node and
with some caveats then there's a
framework that does all that all the
rest of the work like traversing the
entire tree looking for all the nodes
that match the pattern and calling your
transformation on each of those a sort
of a separate strategy that is also
going to help is Python 2.6
by default it will just be Python 2.6
but it will have an option where it will
warn about things that will go out of
style in Python 3000 it will probably
also backport certain python 3000
features so you can start using those I
don't want to give examples because not
much of that has actually been
implemented maybe Thomas can talk about
that next week
so here a couple of things that the
transformer is really good at you can't
it can take in a call to apply and turn
it into the more modern notation using
star arcs and start star keywords and as
long as you don't have a local variable
name to apply this is going to do the
right thing and it will put put extra
parentheses around the function or the
arguments if necessary to make sure that
it doesn't sort of get affected by
nearby operators slightly less perfect
but still pretty close it turns
everything that says a turkeys into keys
and it arite UM's into items it can also
do a really good job with exact it can
do a really good job with print can we
go do a really good job with accept
clauses it also recognizes Heskey
assuming that you don't have
again a user object that happens to
implement ASCII well I found one example
in this in the standard library whether
the bsd wrapper library actually has a
two argument hash key where the second
second argument I think passes in
transaction state so not quite sure what
to do with that so we just don't convert
that one but otherwise turning indeed of
hash key K into K and D again making
sure to parenthesize sub-expressions or
the whole thing as necessary based on
the context so it doesn't add
parenthesis unless they are necessary to
disambiguate stuff on the other one and
if you have redundant parentheses in
your input you will have the same
redundant parentheses in the output it
is very simple to turn the less than
equal then sorry less than greater
notation for unequal into exclamation
point equal sign I could turn back ticks
I can even turn into too long I found
that actually these things were not
quite enough to get most of the unit
test suite were to pass the problem is
that a very popular testing framework in
python is called doc test and it works
by having documentation strings so
they're just string literals to the
parser containing fragments of Python
sessions interactive Python sessions
that you in in theory you can just cut
and paste them out of your shell window
into your Python source code and then
there's a framework that automatically
tests this sort of a regression
framework that checks that those
examples still have the same output as
they had when you paste them in since
all this stuff is inside string string
literals it's not so easy to to see how
we could convert those because we can't
we can't just go scan all the string
literals and assume that they contain
Python code and turn everything that
looks like a print statement into a
print function call
however what you can do is it turns out
that at least for the doc test stuff doc
tests are pretty recognizable because
they have to start with a Python problem
three greater than signs and if there
are continuation lines they have to
start with three dots and they all have
to be sort of indented the same way so
with very great reliability I parse the
dark tests out of the source file you
have to actually run the tool a second
time maybe eventually I'll combine that
currently you have to run the tool the
second time and it'll just scan the
source code looking for dark tests and
this was a great relief I mean at some
point I was a little panicky because I
realized how much unit testing code I
would have to menu convert manually and
then I realized I just have to do this
the only place where it broke down
tremendously was that the doc tests for
the doc test module itself which applies
this trick recursively there I just ran
the tests and and sort of fixed the
things manually until it worked there's
nothing else I could do now there are
also a whole bunch of things that this
conversion unfortunately cannot do if it
sees D dealt in Turkey's it has no way
of knowing whether D is actually a
dictionary if it sees deed of keys it
has no way of knowing whether you're
going to expect that thing to be a list
or not it has no way if it sees x / y
whether you meant that to be whether
when you execute that code x and y are
integers or not so it's not able to sort
of turn that single slash into a double
slash it can't find code that somehow
depends on being able to order objects
of different types
it certainly doesn't clean up your code
or remove redundant definitions if you
write your own code that emulates the
dictionary or implements a mapping
protocol it's not going to touch that
is also not going to fix your string
exceptions basically all it can do is
match on a parse tree stuff that you can
reliably or mostly reliably fix by
looking at the parse tree only as a good
candidate for this tool I don't know if
that's going to be enough maybe at some
point we'll have to add understanding of
variable scope and things like that
so it can actually tell whether a
particular occurrence of a variable
named apply is in fact the built-in
function apply or not
I'm currently hoping that we won't need
to do that otherwise we would somehow
probably have to merge this tool with PI
checker which would be quite the
refactoring so if you're interested I'm
actually probably going to skip this
this is what the matching notation looks
you basically you use the names that are
also used in the grammar file Python has
its own grammar file here's a couple of
examples power it's a token a power is
an atom followed by zero or more
trailers and then optionally followed by
a double star and of something called a
factor and there's a couple of
alternatives for what Anatomy is in a
definition of what a trailer is and
there is like several hundred lines like
this that make up the entire Python
syntax so our conversion tool actually
reads that file with a Python syntax at
the start of a run and builds a parser
customized to that syntax so it's very
easy actually to change the syntax that
the conversion tool uses but you just
have to edit one text file the trick I
use in the patterns is I use the same
notation as in the grammar I actually
use regular expression notation you can
so you can you can match here a pattern
power and then the angular brackets are
actually
sort of they specify and inside this
node name labeled power
I must match the following thing so this
is we want to match a power that starts
with well one or more nodes of any type
but they must be exactly at that level
and then a node of type trailer with a
particular substructure namely the
trailer alternative that has a dot
followed by a name in the name in this
case must be it our items and then it
can have more trailers that's an example
of a matching rule that's close to
actually the rule I use for fixing
either items so if you have that
expression a square record 0 square
bracket dot other items / n / n the
parser sees that as an atom containing a
and then a node that's a trailer it's
the square brackets another no that's a
trailer the dot a terrariums and
northern oh that's a trailer that's the
parentheses and that happens to match
this this pattern as follows the first
two together actually match to any plus
then follows the trailer which happens
to match a trailer with that particular
sub structure and then the final trailer
it matches the trailer star and you can
you can nest these things as much as you
want and it's relatively efficient in
just traversing the tree and finding
matches what your transformation
function gets is it gets the node that
matches match the top level of the
pattern it also gets a dictionary
containing elements sort of sub nodes of
that node and what I didn't show when
not showing here is you can add names to
any particular section of the pattern
you can say oh this sub pattern call
that foo or just call this other sub
pattern bar and then you can sort of
pull those up the thing the sub nodes
that match those name sub sections out
and you can rearrange those in a
different order that's for example how
you do the apply thing
so here's the slide that you you're all
waiting for what can you do today
well my first recommendation is don't
worry about the changes that the
transformation tool can actually take
care of I mean my first version of this
slide actually started out with okay so
use star arcs instead of apply and use
raise exception parentheses parentheses
instead of arrays exception comma value
and then I realized no you shouldn't
have to worry about all the stuff that
we can transform through syntactically I
mean it's unlikely that you'll be able
to write code that is both valid Python
2.6 source code and valid Python 3.0
source code so you're going to have to
run a transformation tool anyway what
you can do is make things easier so that
after you've run the transformation you
actually end up with working code Python
using Python 2.6 means that you can use
Python 2.6 as warnings to find certain
things that the transformation tool
cannot handle it's always a good idea to
have unit tests so you can sort of see
if the semantics of your your new codes
is still what you expect it to be and
then there is a couple of things that
the transformation tool does not handle
like if you extract the keys from a
dictionary and then you sort the
resulting list the transformation tool
is not smart enough to correlate that
the variable you assigned on line 1 is
being sorted in line 27 or on line 2
even but you can write today you can use
the built in sorted function which is
available in Python 2.4 and up and then
you have code that can be easily
transformed correctly similarly if if
you really have good reason to want to
reach the return value of keys as a list
call list and pass it the inner keys
function the inner keys will be
transformed by the transformation tool
and so it will still be
and it will be just as efficient in 2.6
as in 3.0 another thing you could very
easily do is make sure that all your
exceptions are actually using classes
derived from exception you can also make
all your class your classes that aren't
exceptions that don't have a base class
and derived them from objects of their
new style there are certain semantic
differences between classic classes and
new style classes by converting them to
new style classes now you catch those
semantics while you're sort of thinking
about it and then with print don't worry
about the print syntax and I recommend
recommend that you just use the print
statement and reliable transformer to
turn them into function calls when the
time comes but be aware of the two cases
where the transformation tool doesn't do
the right thing
which has to do right I think I showed
that on the slide about print if you
have a string ending ending in a new
line or a tab and don't another thing
you can do now is make sure that your
code uses a double slash where you
expect an integer division so now we
have one theory we have five more
minutes for questions if anybody has the
energy yes
well yeah so the question is why do I
not want strings to use internal utf-8
or 16 representation and why do I think
that order when other one indexing of
strings is important I think because
it's a tradition in Python unlike some
other languages that we actually write a
lot of code that sort of traverses a
string and keeps track of a particular
index there's just lots of code that
that indexes a string I mean it's very
common to say that if if s dot ends with
dot py return as sliced from 0 through
land as minus 3 that's all I can say
it's it's sort of common idioms in
Python code are are using slicing which
uses numerical indices quite a bit and
pattern matching is used much less
okay so the question is can the
transformation tool potentially be
abused for other purposes I think it
definitely can there's nothing that says
you have to use it to transform into to
pop it you don't have to use it to
transform Python to point X to
three-point Xcode I mean you can you can
make the input syntax whatever you want
it and you can slightly alter the driver
so that instead of transformations you
just get error messages if you match
certain patterns yes that's an excellent
idea actually
is
there
I didn't get the last few words but your
question is did I consider some other
string abstraction that would not make
it necessary to to rely on indexing so
much Oh as an IC your question is
specifically could we have an additional
string class that has sort of different
a different model that's a reasonable
question I hadn't really considered that
I see it as a library issue I I think I
would encourage people to sort of to
write custom string classes that might
be more efficient for certain situations
and you can you can probably write them
by the MOOC you can implement them in
Python by using a byte array and and a
thin layer on top of that or if you're
really interested in super performance
you can of course do it all and see but
I mean that's the beauty of an
extensible language it doesn't all have
to be in the standard library in the
back
sorry could you speak up it's been it's
getting noisy
okay so yeah so the the question is
there's going to be a long period where
library developers third-party library
developers especially will sort of be
required to maintain a 2.6 and a 3.0
version of the same library or maybe
even going back to earlier versions than
2.6 is the expectation that they limit
themselves to code that can can be
automatically transformed to 3.0
expectation is a strong word
I would I would recommend that because I
expect that that is the sort of least
painful way for library developers to go
of course if you have an existing
library that has backward compatibility
requirements going back to python 2.2 or
sometimes even before it becomes
gradually harder to to sort of maintain
your source that code in a form that can
still be transformed I mean if you're in
the lucky situation that you can
actually say 2.6 is the oldest version
of Python I support then at least you
can use some of the 3.0 features that
will be back ported to 2.6 but I think
the syntactic conversion approach will
work I mean there's no reason that the
transformer couldn't convert python 2.2
goto 3.0 it would just sort of the
subset of python 2.2 that actually is is
validly transformable into 3.0 is
slightly smaller but that's i would
recommend that i mean that the bigger
nightmare is for developers who have
extension modules because the C API is
going to be it's it's going to be a
rougher ride unfortunately
well if you all aren't exhausted I
certainly am so I thank you for staying
all the way until the end</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>