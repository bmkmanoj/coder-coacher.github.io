<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Move Fast &amp; Don't Break Things | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Move Fast &amp; Don't Break Things - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Move Fast &amp; Don't Break Things</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j_JviA5nvS0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Ankit Mehta: Good morning.
&amp;gt;&amp;gt;Audience: Good morning.
&amp;gt;&amp;gt;Ankit Mehta: Good morning.
&amp;gt;&amp;gt;Audience: Good morning.
&amp;gt;&amp;gt;Ankit Mehta: Seems like everyone's had a
late night.
Welcome.
I'm getting old.
I'm going to hold on to this, mostly to stick
to the script so you don't make this keynote
a three-hour one.
And, yeah, what I was going to be talking
about today is a topic which is very near
and dear to my heart --- how do you move fast
and don't break things.
Because I think each one of these things are
interesting to do in isolation, but to do
them together poses some interesting challenges
and opportunities.
And we have had a fun ride for about the last
decade where we have done this.
And we think we have it down to the point
where we would like to share this with all
of you, so you guys can get a perspective
of how we try to do this at Google, but, more
importantly, you can help us make this even
better going ahead.
Quick thing.
GTAC, last time we did this, this was in New
York, it was wildly successful.
The New York one was based on a theme.
I don't know how many of you were there.
And we somewhat felt constrained with the
theme.
So this year, we decided to keep it open-ended.
We roughly had about 1500 speaker applications
this time, which was roughly about 3X of last
year, which to us, gave us a signal that the
community is still very much engaged with
GTAC, which is a great thing.
That really enables us to keep doing this.
Out of the 1500, about 200 were Googlers'
applications, with about 1300 were external.
And of those, you -- 250 of you are the unlucky
ones or lucky ones.
You'll find out in a couple of days, who get
to spend two great days in this room.
And the speakers.
We have about 15 speakers.
What do I like about GTAC?
The booze is great.
[ Laughter ]
We can't put it on the slide.
Oops.
This is recorded.
[ Laughter ]
But on a more serious note, I think what has
amazed me about is GTAC to really get a chance
to talk with most of you guys from the industry,
and it's an interesting variance in how testing
is approached differently in different industries
and different segments of the tech industry.
And I think we value at Google at sharing
what we have done within Google, but I think
we have gained more from learning from the
experiences some of you guys have shared at
GTAC.
So again, thank you in advance to all of you
who are going to be talking about -- the other
speakers who are going to be talking about
stuff.
So why am I here?
Yeah, I've become one of those guys who spends
a decade at a company, but Google is a pretty
damn good company to spend that decade at.
Over the years, due to the kind of work I've
done at Google, I'm super passionate about,
again, the balance between velocity and quality,
because it's almost like a two-edged sword,
and you need to balance that.
And I think I just want to be able to share
with you guys what we have done.
A quick history of my career at Google.
I started about ten years back leading testing
efforts for Google desktop, there used to
be this thing.
There was a time people used to build software
for desktops.
So did Google.
And then I moved on to leading efforts for
everything Google.
Anything Google that you install on any operating
system, toolbar, desktop, Picasa, so on, and
so forth.
After that, I moved on to Gmail which was
great.
For the first month I literally tried to figure
out how to find a dong on Gmail, but then
I realized I couldn't.
And then after about three years on Gmail,
I led testing efforts for -- we bucket these
projects under an umbrella called social at
Google, but projects like Google+, news, blogger,
hangouts, voice, et cetera.
Things like that.
So very quickly, My first automation project
at Google, I call it MAD (Millions of Automated
Documents), because it sounded like it was
mad, but we had to create content, a lot of
content which we could stress as the Google
search appliance and Google desktop, and so
that's UI automating, Word, Excel, AIM.
I don't know how many of you remember what
AIM is, but yes AIM.
Back then, Google used to use a variant of
Bugzilla as their internal bug database.
It was running under someone's desk.
That was Google ten years back.
It started getting really slow because it
turns out one-fifth of the bugs on the bug
database were from my team, so she's like,
yeah, you guys need to take care of this.
Then it kept getting slower, and I essentially
(indiscernible) just made it 10x faster.
What did I do?
I just moved it to a faster machine in a data
center.
That helps.
It's amazing what technology can do, I guess.
I wrote down a 20% project called surveytool.
The reason I talk about this is this was my
first real-life example of how not to do things
in terms of how this grew organically much
to initial joy, later to dismay.
Gint action.
In that day and age, there was no AppEngine,
no Google forms, no Google+ Polls , and so
it grew to the point that at one point, Google
H.R. wanted to use it as the tool for our
annual survey.
Within literally I believe it was less than
three minutes, the tool melted after we sent
out this email to Googlers.
I spent the next two weeks pretty much recovering
data and then I happily retired the tool after
a month.
But that was a good learning experience.
And one of the reasons I am doing what I am
doing and I like test engineering is when
I was doing my bachelor's, I worked with a
buddy and we worked on a microprocessor control
pick-and-place manupulator, which is a mouthful
way to say Robot but that really gave me a
perspective that while software is great,
hardware is a lot more difficult to mold.
really -- for those of you who work in hardware,
I salute you.
I almost remember spending 10x more time on
hardware
So what is test engineering?
It's -- We think we have a dual purpose at
Google.
We think we want to build infrastructure and
tooling that is going to enable us to test
things at a scale that no one else tests things
at.
And the other focus we have is we want to
test and automate projects right from things
like the autonomous cars to things like Glass
to things like, you know, one of my teams
test these, I like to use like how blue is
your blue in terms of photo editing software.
Which is a very tricky problem to automate,
if you come to think of it.
But that's kind of how we think test engineering
at Google is all about.
This is how Google looks from the outside,
you know?
Everyone is like, yeah, this is the world
of Gmail, Chrome, YouTube, free food, shuttles,
massages, paddleboats.
Wow, it's awesome.
I'm sure it's probably a fun place to sit
and code, too.
And it is, depend ongoing what your definition
of fun is.
[ Laughter ]
And what part of the code base you are in.
So but this is more like how it is.
[ Laughter ]
It surely does feel at least three of the
five days of the week.
I guess great things come with great pains.
I don't know if that's a saying, or I just
made it up.
But with rich products -- I mean, we have
great products which have rich integrations
but that leads to pretty and complexing tasks.
We have legacy codebases, and the moment that
resonated with me was probably a few years
back, when I was on Gmail, I wanted to add
a feature in Gmail to add this thing called
Google feedback into Gmail.
If you go to Gmail right now you'll see send
feedback.
And I thought that should be a two-day thing.
I'm just adding a module, every feature in
Gmail was a module, and I'll have it done
easily.
Turns out I added a module, but turns out,
of course, there was a lot of refactoring
which had to be done in another module.
And then I did that there was race conditions
between the module.
And then turns out you have to handle the
JavaScript.
You have to lazily load it, so you also have
to ensure the JavaScript is not too much because
you're causing every user to download this
every day.
And then you have to write a unit test.
And then you have to write a UI automated
test and then you have to write an integration
test.
By the time I was done with all of this it
was like two months.
I was like, wow, that is shockingly sad.
I did get that done but I very rightly went
back to all the guidelines and principles
we had in terms of how to write a feature
and what are the things you need to do, and
revised that.
It was good to get that perspective.
So that's kind of really, I think, a lot of
what Google is.
Why this slide?
I wanted to take an opportunity to highlight
-- the code throughput at Google continues
to grow linearly.
Ever since, I think, Ari spoke about this
in the last keynote, and I've been tracking
this metric.
I like graphs.
And I've been hoping or assuming that this
number would actually slow down as we add
more people.
Bigger companies, things get harder to do.
That's kind of what you hear.
Except at Google, that doesn't seem to be
the case.
And another interesting tidbit I found, and
this is probably true for every tech company
-- I guess, but it's interesting to see the
peaks.
You know, there's a peak right before the
holidays, after the holidays, and right before
Google I/O.
So kind of shows -- yeah, it's working as
intended.
That's what engineers do.
[ Laughter ]
I got my slides together 5:00 a.m. yesterday,
so that's what happens.
And there's the trend down correlates, too.
I mean, it's the holidays.
But I have no clue who is still checking in
those 10,000 CLs?
Who are those people and why -- Yeah.
So that's something we've got to fix.
That's way too much code going in when you
shouldn't be getting code in.
Larry strongly believes in velocity and we've
had a very sustained push over the years on
velocity and we've made a bunch of things
around that, that in turn has made us really
efficient in how we do our releases.
And we've had a bunch of other new product
offerings.
Google Fiber, things like balloons and whatnot.
It's still shock to go see we're doing almost
twice as many releases.
I think this is another testament to -- I
mean, I want to give a shout out to the release
engineers we have at Google.
I think they have done an incredible job in
automating a large part of our release tooling
and work flow to make it really easy for the
teams, individual teams, to really sustain
themselves and a lot of their tasks needed
for release.
Of course, you know, our SREs rock.
I mean, while we continue to push out all
these releases, they ensure Google and all
our infrastructures stay up always and have
a very high availability.
Google actually publishes these number on
our apps -- dashboard for our apps, and it's
kind of insane, the reliability we have.
As I've been talking to teams within Google
and to folks in the industry, I think theme
emerged to me.
A theme about when I started hearing or I
heard Googlers who leave Google and join startups,
and they're like, wait, we kind of feel stuck.
We're a startup.
We should be able to release quickly; right?
A lot of people end up having long release
cycles, and the definition of long can vary
from a few weeks to a few months before but
as a result of that, everyone wants to get
stuff in because they're like wait, the next
release is a ways out so let's get this out
with this release.
I call it a lack of discipline.
It leads to further problems.
It leads to further iterations because you're
trying to put in stuff which isn't totally
baked.
It leads to delays.
It's kind of a hole you keep digging yourself
into.
Once you're in there and if you encounter
problems, you're so deep in the hole that
you -- there's no way to isolate it.
The only way to get out of that is to fix
that problem and -- fix it and work around
it.
The context, this slide is important because
this slide is maybe the reason why you guys
are going to be sitting and listening to me
for an hour, about a bunch of years back on
Gmail, we suddenly realized we need to iterate
faster.
While we thought we had for a release cycle
which was weekly, we realized even though
we have weekly cycles, by the time our code
makes it to prod, from the time it's cut,
it's checked in, it's three weeks.
So while we were cutting weekly, that code
took three weeks to get to prod.
And that had a bunch of problems.
That model had a bunch of problems because
it hindered our ability to react quickly when
we needed to.
It also meant that there were three parallel
tracks, which means if you find issues somewhere,
you have to ensure you down integrate them
on all the branches, there are conflicts.
One specific example I can remember is we
pushed a certain Gmail release to prod, and
there was -- the initial page load, aka the
inbox when you load the page, it slowed down
by 20% on IE.
We had a laser focus on latency and we still
do at Google, and so that wasn't acceptable
to us.
We did have a state of the art latency regression
system, which could help us detect regressions
of even 25 milliseconds, but that thing didn't
show a blip, but on production, we saw a big
spike.
And then that led to an interesting wild goose
chase for two months, because then we were
iterating on production, it further accentuated
the three-week delta problem we had.
And not to mention we were literally pushing
code to our prod users and continuously updating
it.
After almost a two-month goose chase, we narrowed
it down to a changelist which introduced a
pseudo CSS rule which in IE causes all the
rules to be reevaluated every time you move
the cursor from one element to another.
And the reason our system didn't catch it
is we didn't have hover based automated test
back then because they can be tricky to automate.
We did get it added after this.
But that's kind of what really was the aha
moment for us.
And we said this isn't great.
We need to iterate faster.
We need to ship daily.
And that's how we set about this mission.
.
Okay.
We call this a bio break for the deck.
Okay.
Moving fast is good; right?
Everyone likes to move fast.
But I wanted to add a bit more color to that.
Innovation is no longer just the thing to
do.
I think you need to do it to survive.
We all know what happens to companies who
don't innovate, especially in the tech industry.
It also really is a great way to retain and
hire talent because people like to work for
companies that innovate and move fast.
And I think moving fast really enables you
to do that.
The ability to react quickly to critical bugs
is huge, and I think moving fast gets you
there.
This is a chicken and egg problem in my opinion,
where you can't react quickly until you move
fast, but you can't move fast till you know
if you can react quickly.
And at some point you really just have to
take a leap of faith and hope for these two
to come together, and they do.
We have tried this umpteen times and it's
come through in some or the other shape or
form, in some places painfully, in some places
less painfully.
Another side effect of moving faster is productivity
is, automation is a key in getting to make
this happen, if you want to move fast, you
have the automation in place which means if
you are making changes in your work space,
if you actually write code that doesn't compile,
you're immediately going to find out about
it.
If you're writing a code where you break a
test, you will find out because the tests
are continuously running in the background.
And I talk a bit more about how we do that.
Or if you try to check in a changelist without
tests in it, we have tools at Google which
actually will yell at you, though our human
reviewers do a pretty good job of doing that,
too.
And now, I think -- I think this come up in
the last couple of years, we have tools at
Google which show incremental code coverage
for every change list, which I think is extremely
useful.
Lastly, I think why moving fast really helps
is it helps get the focus on code health.
In my opinion; Every software product has
a 15-year cycle.
It's zero to two years to build it depending
on how complex it is, about a seven-year shelf
live, and then three years it's in maintenance
mode.
And then there are the last three years where
it's in deprecation and migration mode.
Having the focus on code health right from
the start really makes this journey on the
later part a lot easier.
We know by experience in Google, like, we've
had to deal with this on projects which didn't
have code health and had code health and it's
a day and night difference.
So again, that's why moving fast is good.
But breaking things isn't.
Obviously.
The most important part is --, we owe it to
our users to ship high-quality products.
And all companies and businesses do.
You know, unbaked features really lead to
a crappy user experience, or low quality products
just build up bloat, feel clunky all the time,
and again that doesn't result in successful
products in the long term.
But also, really a lot of big tech companies
are getting into this phase where they are
having a hard time shipping -- shipping, their
launches slip into years, and I think if you
have the focus, really, the one way to not
get in that state is really ensuring that
you have the focus on code health and we move
fast but you don't ignore quality.
So Alister is here somewhere.
Yes.
So this -- I'm borrowing this slide, this
image from him, because over the years, I
think I found this is a great way to communicate
to people what I think about my testing philosophy
or automation philosophy.
I believe a lot of teams in companies -- when
they talk about automation, there's a lot
more emphasis on UI-based automation and that
is unfortunate because in my opinion, UI automation
is actually the hardest thing to do with all
the moving pieces.
Very often the people don't just stop at the
UI automation.
They want to test the whole stack.
My litmus test is I know a team is doing too
much UI automation when they are catching
the null pointer exceptions via the UI test,
essentially.
And so I believe this cone needs to be inverted
in more teams than not.
Manual session-based testing is, in my opinion,
a fancy way of saying exploratory testing,
but I believe exploratory testing is great
because -- and I wish my teams can do a lot
more of it because UI automation is hitting
the same code paths over and over again and
exploratory testing is the way you are going
to be hitting these other code paths which
don't get exercise otherwise.
And I strongly believe that if the testers
are running the same set of test cases over
and over again, those need to be automated
unless in rare circumstances the return on
investment is not there.
API tests are, I think, great bang for the
buck.
These are relatively simple tests to write
if the APIs are defined cleanly.
They are almost always stable and almost always
have a binary signal.
And everyone likes integration tests.
Everyone is like oh, more integration tests.
But again, I've seen beyond the point integration
tests stop being an asset and become a liability
because the signal-to-noise ratio flips.
You just have too much noise.
So you want to have integration tests but
you want to have a healthy dose of it and
not too much.
The component tests are essentially if you
have a stack, you start isolating, and I talk
a bit about it later.
You test each component in isolation and fake
all the other things, which helps you really
not -- run the real world in terms of tests.
And, yes, -- unit tests of course are essential.
Google has always had a high bar on developer
testing, and I think we continue to do that.
So the rest of this slide is going to be talking
about this flowchart.
I tried to take a step back and summarize
what are the things we did to take our journey
of moving faster and not breaking things.
I could think of ten things.
So let's talk about them.
So the first one is what I call push on amber.
But before I talk about that I want to take
a detour and talk about push on green.
This should be a concept most of you would
have heard.
Kent, I think in 2003, came up with this concept
and I think it's a great one.
If you or your company and team is not doing
this, by all means, please start with push
on green.
At Google what we started realizing about
half a decade back is push on green was in
some cases starting to not enable but limit
us in what we wanted to do.
There were some teams which had tests which
were dependent on servers which were changing
under the hood.
And the test would fail.
The test was failing, but the product wasn't
and should we really slow down the product
for that.
Or we would find a bug, but the bug was in
the back end, in the stack, it was two levels
down the stack.
Should you be blocking the entire stack from
going into production for a bug at the third
layer of the stack?
So we thought it was time to take more risks
and do what I call push on amber.
The expectation we set and we mostly meet
is we say if you submit a changelist at 4:00
p.m. today, it will be in live production
at 4:00 p.m. tomorrow.
I want to take some time and explain a bit
more how we got there.
Because most people are either shocked or
fascinated by this.
We'll see which camp you guys are in.
We have early snapshots of our stable top
of head.
And we call this fishfood.
I'll talk a bit more about this later.
At around 6:00 p.m. every day, we take the
latest snapshot and graduate it to the next
day's Prod-bound release.
We have a team of calibrated testers who have
worked hard on calibrating the product who
immediately jump on this candidate, if we
can call it that, and over the next four to
six hours, run a combination of regression
tests and exploratory tests.
And I think on an average, even with all the
automation we have, at an average, find three
or four regressions.
We have a simple rule, every regression is
a blocking bug and has to be fixed.
Otherwise, we go into these debates of what
should be fixed and what should not be fixed.
We have another set of folks who once they
know these bugs, they start culling through
the list of changes and actually try to isolate
what caused these problems.
And this was another aha moment for us when
we did it.
This is a lot easier to do with daily snapshots,
hourly snapshots, because we have the data,
than to do with the weekly one, where at times
we would have excess of 10,000 changelists
in a weekly snapshot.
We know what those changes are.
We essentially removed buggy code or what
we internally call rollback.
We integrate those fixes, then do what we
call smarter regression testing, where we
don't rerun the entire thing all over again.
We just regression test the affected parts
of the codebase.
We, of course, do not bypass critical tests:
ACL, data, stuff like that.
And, of course, all of our automation reruns
all the time, because automation is cheap.
You just need to throw machines at it.
All the automation tests run at every step.
We have also done some experiments where we
have instrumented our servers to help us assess
the coverage of a manual exploratory and regression
testing.
Because while it's great to have numbers of
automation and your coverage, that gave us
insight of we are testing way too much in
this area and not much -- and we compare it
with where the codebase is changing and actually
use that as a way to further refine how we
spend our time doing exploratory testing.
So that's what I mean by push on amber.
I've been telling the teams I work with, you
know, don't catch bugs, but prevent bugs.
Because I think that's the only way to stay
ahead of the curve.
The ideal situation is the checked-in change
list produces a perfect code.
While you can't get there, you want to get
to a point, essentially, where bugs are prevented
from being checked in.
This is the point where I go to my notes,
because, as I told you, I'm getting old.
We invested in what we call hermetic automation,
and I talked a bit about that.
We essentially say we are going to test our
system on the test and fake out the other
servers that talks to the stack.
So that way, essentially, the way to think
about it is, if a change list is going to
cause a null pointer exception, my example
from the old days, it will actually -- and
if you have a hermetic test, which brings
up the server, it will not let the change
list to get checked in the first place, because
servers are not going to come up.
So it doesn't even get in.
And so you're not, dealing with your staging
servers not coming up or stuff like that.
Over the last couple of years, we have taken
it to the next level, where we've actually
started recording our responses on these fake
servers, which also helps us get our test
even more faster.
And I'll talk a bit more about why faster
is good.
What is a hermetic test?
If you Google it, it says complete and airtight.
I think this is a concept on which I think
I have made my living at Google.
On some of the projects, I was frustrated
at seeing tests randomly switched from green
to red and back to green.
I don't know how many of you have experienced
that.
It's like watching Christmas lights on a dashboard.
[ Laughter ]
And it didn't help with respect to motivation,
telling people, why you should write automation,
not to mention when you talk to software engineers
that why they should,-- why automation is
good and why they should also write UI tests
and integration tests.
So for a good year also on the initial project,
again, on Gmail, I didn't really have a great
solution.
And, essentially, we just didn't write any
more automation, because we needed to find
a better way to deal with our automation.
So there was temporary pain.
My definition of a hermetic test has been,
if I'm on a flight, I should be able to run
my test and it should pass.
Except now, in the last few years, there's
Wi-Fi, so you can write nonhermetic tests
in airplane.
But that's been -- that used to be my litmus
test for years.
And I would actually do that, in the initial
days when someone told me, wait, we have a
hermetic test of our project, the next time
I would be on a business trip, I would try
running that on a plane.
It would mostly work.
So that was awesome as a litmus test.
Because I think when you're not talking no
network connection, it really isolates a lot
of the flakiness you see in a lot of these
end-to-end tests.
A bit more about hermetic tests.
A complex Web app has a lot of servers under
the hood.
There's a front-end server in it, a bunch
of other servers.
An end-to-end request makes a request to the
server, gets the other responses from the
back-ends, and gives you the result.
But the problem with this is that a lot of
these back-ends introduce variants of flakiness.
There's network.
So this is what leads to a lot of -- a lot
of the front-end debugging and flakiness a
lot of us have.
Okay. what we did, essentially, is, we designed
hermetic servers.
And, again, we said it's a server in a box.
We're essentially have entire test come up
on a box.
It can be on different boxes, too.
But if it's in a sealed network.
We start the system on the test.
The client -- it talks to the test client.
It makes the request while the test client,
we essentially fake out -- we just use the
in-memory data store.
That way, you reduce all the complexities
out.
And this leads to a test which is a lot more
stable, the logging module we have helps us
pinpoint where the issue is.
We have a somewhat similar hermetic test for
the back-end and so on and so forth, if that
makes sense.
So pushing testing upstream -- we can't do
it all, and I'm sure this is true for others
-- a lot of you.
There's a lot going on and you don't try to
do it yourself.
I like to say, spread the love.
Spread the love of testing.
So when I started talking to a lot of these
software engineers and asked them why they
don't write automation, they -- they essentially
said, the framework is just too complicated,
and I am writing my code in JavaScript and
why are you making me write my test in Java?
I want to write the test in the same language
as the application.
So we invested what felt like almost a couple
of years, maybe it was a bit less, in building
-- an automation framework within our servers.
The automation compiles -- it's a feature,
essentially, of the product.
It compiles with the product, which means
your automation -- no one can break your automation
framework; right?
Because it regresses the product.
We, of course, ensure mostly that that doesn't
ship to the end user.
We ended up doing that a few times.
But we have learned not to do that.
So that's really helped us getting the software
engineers to get involved in writing the tests
and running the automation.
I think most good things in life are accidents.
So this was an accident.
This might be one of the most important things
I've done at Google from a prioritization
perspective.
We just had too much going on on Google+;
It's like we launched 100 features in the
first 100 days.
And there was no way we could test everything.
And the onslaught just kept coming.
So I was just trying to figure out how do
we scale our testing efforts.
what do we do?
And I literally was just thinking stuff aloud
on a slide.
But then a few folks stumbled on it and they're,
like, this is great.
We'll just do this.
And this helps us figure out how we should
test and what -- we should just make the PMs
and SWEs test.
What this led to is it helped us define where
we want our integration tests.
This also helped us as a by-product throttle
the number of integration tests.
In an ideal world, it would be great if every
feature had an integration test, but they
come with a cost.
So this helped us focus on having integration
test for the right features.
It helped us get everyone involved in testing.
We got a lot of product managers and software
engineers who were actually excited and helped
-- collaborating on test plans, doing bug
bashes.
We really made it an effort to make it self-service.
To date, this is how we continue to scale,
which allows us to really focus on fewer things,
but do it better.
Okay.
I'm going to speed up a bit.
This is important, delineating product releases
and features, because this disengages the
releases from features, because they keep
changing.
This helps ensure that your features don't
have a high dependency on each other.
If one feature is not ready to launch, you
just don't flip on the experiment and it doesn't
launch.
what is a product release? we launched -- last
week, we launched Inbox by Gmail.
Some of you might have invites.
Others have invites, grab the Googlers here
for invites.
It's cool.
And, a feature would be, like, Google+ launched;
the Polls feature in Google+.
That's what the difference between these two
things are.
It also helps us launch a lot of things silently.
Because Google does things at unprecedented
scale.
And this lets us actually figure out how a
new back-end or a new feature is going to
fit into the current stack, which in some
cases is barely hanging there by the thread.
So this helps us do a lot of, to say achieve
invisible testing, under the hood.
More importantly, if we find we launch a feature
and it has a critical flaw, we just turn it
off, because it's an experiment or a flag.
We can turn it off and not have to do an expensive
binary release roll back or roll forward.
So this has been something which has really
helped us -- enabled us to take more risks,
too.
And I just realized I spoke to this slide.
So productivity.
Why is this important?
You know, yes, users come first.
But, you know, you need to take care of yourself
and your product team.
If the team is not happy, they won't do good
work.
They will -- and eventually lead to not so
great products and unhappy users.
So we -- I have this dream -- sound like Martin
Luther King.
But it's invisible tests.
If stuff works, you just don't notice it.
It's -- basically, it works.
It's like it's your car, it works.
Like, you don't know under the hood how many
things get switched or whatever, it should
just -- and we are getting there in a lot
of places.
We are actually at a point where we have now
under five-minute UI automated tests on some
of our projects.
That includes the build, test, set up tear
down, -- and we are working on figuring out
how to scale this at a Google level.
I remember someone senior at Google once telling
me, you need to ensure the tests stay an asset
and don't become a liability I think that's
really important.
And then flakiness, I have a personal believe
that flaky tests are worse than no tests.
So -- and I know we have a few talks around
flakiness.
So I'm interested in seeing what other people
have to say.
But when I joined a particular team, they
told me they have thousands of automated tests,
and I was excited.
I was, like, where are they running?
They're like, oh, no, we have them, like 70
of them are running.
I'm, like, what's wrong with the others?
They're like, yeah, they are like this Christmas
tree thing.
So I'm, like, yeah.
So I think you -- that is what really is a
big productivity killer.
So, yeah, I just don't -- attack, kill.
You know, &quot;kill&quot; is a hard word.
But bad tests are terrible, like plague.
In my opinion, there are two kinds of bad
tests, slow and flaky.
When you want to move fast, ship to production
daily, you need to have fast tests because
you want to run the automation at various
steps of the thing.
If I have slow tests, then that doesn't help.
Even one flaky test, if you have 1,000 tests,
one flaky test out of that is going to cause
a submission to fail.
It doesn't matter if that is just one or that
is ten.
So that's kind of why flaky tests are horrible.
So we really have a laser focus on this.
A recent example on one of my teams, we had
-- we knew over a few quarters, we knew we
had a bunch of flakiness that creeped into
a particular area and we actually had a bunch
of fixes lined up.
Going back on the philosophy of tying everything
behind an experiment.
We made a bunch of changes and actually put
it behind an experiment.
The other thing is, I think some of this work
at times doesn't get to as appreciated as
it should.
One way to do it is if you actually quantify
its impact.
Actually, other than just making incremental
changes, we did in our own ecosystem, but
then we turned it on via flag switch, and
we -- so the data really shows that the flakiness
went from, like -- like the downtowns to the
suburbs, I guess, whatever you call it.
Another thing which one of my teams has done,
and we call it -- we wanted to name it Robocop,
but there are many of those, so we called
it Robosheriff.
The goal with this is to inflict the pain
on the person who made a test slow or flaky,
and not the team.
We have this thing background keeps running
and collecting data.
I believe we have the threshold set at about
seven -- three flakes in 100 runs or something.
But it automatically pulls a flaky or slow
test out of the critical presubmit or continuous
build, and it logs a bug.
So you're not just ignoring the problem.
We are super close to having culprit finding
in there.
So, again, we'll be actually able to find
out what the change list is which introduced
this problem.
And then when things actually get stabler
fast, they automatically get put back in.
we sync it with everything.
Of course, we have a blacklist of critical
tests which -- white listed, like, you don't
take out the ACL test no matter what.
If it's flaky, you fix it.
You don't ship without the ACL test.
So that's something exciting I think we are
doing.
So 
you don't know where you are going until you
know where you have been.
I don't know if that makes sense.
It's a fancy way to say metrics.
I like metrics which can be automatically
generated.
I hate metrics which you have to put on a
spreadsheet and calculate every week manually,
because you just don't do that after, like,
a few weeks.
I think, fortunately, Google has good tooling,
and it's improving exponentially right now
in terms of metrics.
So I'm not going to read out each and every
thing on this slide.
But kind of tells you guys, like, the areas
we believe are important areas for us to keep
an eye on.
Again, the goal is to move fast and, don't
break things.
The two things which are red -- and I want
to explain why they are red.
Why is it bad to have more automation?
So, by the way, that's 114,000 days.
Yeah.
So -- and think that equates to, I don't know
-- yeah, it is basically -- if you calculate
the amount of time our automation ran, that's
what it equates to in this year so far.
And I actually got this data probably three
weeks back.
That's great.
In a way, you have more automation.
That's bad, because we believe as we keep
adding more automation, we ought to make it
more and more efficient.
Our tests need to get more and more faster.
but, really, it's code.
It should compile and your test should be
ten seconds, if needed.
So as we add more tests, we want to add more
automation, but we want to reduce the amount
of time automation runs.
We want to get smarter at how we run it, how
long it takes.
Similarly, with the presubmit time -- and,
again, that -- the reason that number is really
falling off the cliff is, again, a lot of
the work we have done, we have had a big focus
on speeding up presubmits.
We took a lot of our flaky tests, slow tests,
they get pulled out of presubmit.
So that's great.
But that's nowhere close to where we would
want it.
We want -- we have the dream about invisible
tests.
We wanted to be there.
Yeah, that's kind of one of the key things
I wanted to highlight on this.
Fishfood.
Dogfood is a common industry term.
We wanted to take it to the next level, fishfood.
Doesn't taste great.
It's yucky, hardly edible, but I makes the
immune system better.
That's kind of what we mean by using it in
terms of our product.
You share the pain, expose the pain.
If you have bug where you can't send an email
or you can't make a post, there's a good probability
whoever introduced that bug will also see
it and they'll fix it.
So this also enables rapid iterations.
. And it's -- these are the early snapshots
I spoke about.
I don't actually sign up my teams to manually
test this crap.
I hope I can say &quot;crap&quot; in a public conference.
Because there's a lot of moving pieces here.
Again, the concept of hermeticism -- the moment
of feature is implemented, someone might have
implemented the button but not the action
handler on the button.
Which means you see the button, but that will
just do nothing.
If you expose testers to it, they log the
button, yeah, I clicked the button, but nothing
happened.
But that's because it's being implemented.
But this really helps us have rapid iterations,
rapid turn around time.
We've had more than often, we have, like,
four hours from, like, the time, like, a feature
is thought about and you actually see it somewhat
working.
So I think it's pretty good.
Prioritize releases.
The grass is greener where you put more water
essentially
If you don't prioritize releases, at the end
the releases is the way you get everything
out.
Postmortems need to be part of the culture.
Postmortems are a great way to learn from
mistakes and actually improve tooling and
process.
At Google, we actually have a bad postmortem
of the month award.
I actually didn't know that until two months
back.
We did something really bad and they said
but wait, you probably qualify for the award,
and next week -- So, yeah, Google is great.
All sorts of perks, I tell you.
And it's also good, having on duty rotation
helps because everyone knows how the stack
works.
Every software engineer works in isolation
and each are part of the puzzle.
And going through, understanding how the releases
work give them a perspective how the product
works, which in turn, we have got most feedback
from them really helps them build better features
down the line.
They think about implications up and down
the stack when they go back to do this.
So 
treat regressions as build blockers.
And the best way I can -- the analogy for
those of how have done this is knitting.
I did this back when I was in school.
If you make a mistake knitting what do you
do?
You undo what you have done and then you redo
it.
When I used to do -- try to do knitting, I
think I undid more things than did, which
isn't possible although it felt like it.
This is a cultural shift and I think it's
important.
Again, and one way to enforce this is I just
started rolling code back and I stopped doing
it after I crossed about a thousand changes
because by then, people got the idea that
you should do this.
And now we are tooling our continuous build
system, actually, auto rollbacks code that
doesn't compile or code that breaks more than
thousand test targets.
And we are actually evolving that into more
finer thresholds.
But the key is this is a guaranteed fix, whereas
if you try to make a speculative fix it, might
fix one thing but cause another thing.
We actually, at times we favor -- like the
changes you want to roll back is maybe four
changes deep in our list of changes.
We actually roll back all four of them, and
then we resubmit the top three.
So that still means we have removed the buggy
code and then we have essentially, back to
where we were.
And this gives people more time to react.
You're not like -- someone's not on the -- especially
if you ship daily, no one has a gun in their
hand and says you have to fix this and you
better fix this before you go home and stuff
like that.
Murphy's law.
Anything that can go wrong, will go wrong,
and it does more than often; right?
So this is the slide where I'm telling, as
optimistic guy I am, we need to be a pessimist.
Hope for the best but prepare for the worst.
This is where you put on your tester hat or
your how to break things hat.
Imagine all the possible things that can go
wrong.
Some of the things which we have done here
are things like we call it missing back-ends.
We actually take a perfectly functional stack
and randomly start killing servers.
I mean not in production.
Mostly.
[ Laughter ]
And see how the system behaves, has it degraded
gracefully, do the actions that don't depend
on that part of the server actually work.
So we actually have this and in some cases
we actually it automated so this continuously
runs and helps us identify problems.
Earlier this year Google had an outage, and
this is on the Google blog, a bunch of services,
Google+, Gmail, you couldn't, like, for about
25 minutes, users couldn't log on; right?
Because the configuration at Google was pushed
which tells other services at Google what
to do with user requests.
And the software bug caused the bad configuration
to get pushed, and that caused the user request
to kind of black hole and stuff.
So we've taken that very seriously and since
then we've put in significant efforts around
making sure like automated configuration folks,
like machine generated code and we test it
to the extent we can, because it turns out
you can't even trust that.
Lastly, yeah, so this is like sangria.
You can keep making the best sangria ever
but at some point you just want to drink the
damn sangria; right?
So I don't believe in being a gatekeeper.
I believe the test engineering organization
is in a great position to provide a good risk
assessment.
We tend to get our power users involved in
giving feedback, especially as we get closer
to launch.
And we, in fact, have -- if you want to get
under Google trusted tester program, we're
try to circulate a form if you guys want to
know something.
That also gives us very valuable feedback
on the product on what things we should do
and shouldn't do.
And this is really where you have to balance
how much you want to go velocity.
We of course donât compromise on any things,
ACLs, security, data, stuff like that, the
things which you can't compromise on.
So if you are sleeping so far, wake up, and
this is essentially what I was talking about
all this while; right?
[ Laughter ]
So for those of you who don't hate boring
talks and don't like slide decks, this is
it really.
And I wanted to, like, just end like you know,
of course, everyone's -- the world is moving
to mobile, and while I knew this graph would
be upward, I was surprised how upward this
graph is.
We clearly -- This poses some interesting
-- interesting challenges and opportunities
for us, for our move fast and don't break
things model.
I think we need to make some adjustments.
We need to rethink some of our tooling.
For starters, you can't push daily to users.
You can, but it would be kind of lame.
There's battery data, there's network limitations.
So I think also the state of tooling has to
catch up and evolve in the state of, like,
Web and server tooling.
And at Google we have a big push for it, but
I think we need to -- it's a challenge to
have fast, blazing fast tests, especially
when you're using emulators.
We have come a long in speeding up our emulators,
but a lot more needs to be done.
It's interesting to test battery -- automate
battery testing.
The moment you connect your phone to a machine,
it's charging.
How do you test a battery when it's charging?
So we actually -- we do it in a weird way,
but it's -- so those are the kinds of challenges,
how did you do this in the way that scales?
Integration testing has to be rethought; right?
So this is actually an area I'm hoping over
the two days you guys are here talking, I
would love to hear thoughts, ideas, you know,
like we are -- we want your opinion.
We want your thoughts on how you think things
should be done in the mobile first world.
We have a few things in flight but again,
I didn't want to make this a three-hour talk.
Also, I think we want to polish what we have
a bit more before we share it with you all.
I created this community.
We'll see how far this goes, but again, I
hope this discussion doesn't end here today.
So I would love to hear thoughts and comments
from most of you guys on how you do things.
And that is it.
This is my last slide.
Yea.
But another way to look at this, in my opinion,
it's not about tooling or -- A lot of this
is there's a cultural element involved here.
And in my opinion, you have to bucket these
things into these three areas.
There are things which fall into the cultural
part which doesn't require a single line of
code or a single tool to be written.
There's tooling.
Some areas require tooling and some areas,
a combination of both.
That's it.
Thank you for not throwing any rotten tomatoes.
Open for questions.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you, Ankit.
So do we have any live questions?
Anybody want to come up?
&amp;gt;&amp;gt;&amp;gt; Hi there.
Good talk.
Thank you.
Can you clarify when you do your release cycles,
you said you start at 4:00 p.m. each day,
and typically the team finds three to four
defects.
You back those out before you push to deployment
or do you fix those in place the next day?
&amp;gt;&amp;gt;Ankit Mehta: So if I understand your question
right, we don't start.
There's a cut happening every hour.
We just happen to take the one around 6:00
p.m., the latest one and graduate that.
We find regressions.
We fix them on the branch.
And then we revalidate, but we do it in a
way, like we don't validate everything.
Actually, I missed a step when I was talking.
We push it to staging too.
Especially for servers which are state, because
there are some corner cases which you are
hit with when you expose things to millions
of RPCs and related to down staging for a
bit and then we push it to prod.
But in staging there's a bunch of other metrics
we go to but that's a topic for another talk
all together.
But, yeah, I hope that answers the question.
&amp;gt;&amp;gt;&amp;gt; Thank you.
&amp;gt;&amp;gt;&amp;gt; I have a question about your hermetic
tests.
And I wonder, given if you isolate the dependencies
by faking them out, that itself creates risks.
How do you manage the risk of making sure
that everything will still run when things
are not stubbed out?
&amp;gt;&amp;gt;Ankit Mehta: That's a good question.
What we try to do is simplistic, let's say
a stack is three servers and you're taking
one and two out.
What we essentially do is in the old world
we have one test where all three would be
running.
In the new world we would have three tests,
which essentially have one real and two of
the other fake.
So you are writing three tests, but we parallelize
all these tests.
So in essence, you are doing the same testing
which you are doing with the one test, but
you are parallelizing that.
One of my teams has also come up with this
interesting concept called Cutover (phonetic),
and we have a prototype ready where we are
working on a fake that doesn't go stale.
It's actually a pretty good idea.
The fake actually inherits a lot from the
real server works and it sort of keeps itself
auto updated.
But that's -- All said and done, there have
been some instances where we had these three
tests, and some stuff is still broken, but
then that one test was still -- you know,
like still didn't catch it, because it's just
something.
Depending on how deep of a stack you want
to go to.
Google stack is really -- you can go ten levels
deep.
But hopefully that answers your question.
&amp;gt;&amp;gt;&amp;gt; Do you have any examples of enterprise-level
applications where what your application is
doing is transactions kind of things?
And then if you release in production, there
could be a customer out there who is so unique
in the kind of configuration that the customer
has that you may not have covered in the possible
scenarios in your testing.
And then there is no way to roll back.
You have to roll forward for the transactions.
&amp;gt;&amp;gt;Ankit Mehta: No, and that's a good question.
So how does this map to the enterprise world
where there is a different risk tolerance
level.
The example -- again, on Gmail, we have paying
customers; right?
In the enterprise version of Gmail.
And for some of our big customers, we essentially
created -- we run all these tests in all the
enterprise -- like not for a specific company,
but we run it in education mode, premier mode,
so we actually have all this replicated across
the different modes, and that's how we kind
of -- but, yeah, there are these special use
cases.
But in general, I think Google has done pretty
well not trying to get too rattled into very
customized enterprise hookups.
In terms of transaction stuff, they still
have hermetic tests, but I think there we
actually -- at times, some teams cut daily.
They actually test for longer and they push
-- like they still have daily cuts but instead
of pushing daily, they push, say, like every
three days.
So it's a risk tolerance.
Okay.
I'm getting the placard.
Thank you, guys.
I would love to hear more from you guys over
drinks later today.
But thanks again.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>