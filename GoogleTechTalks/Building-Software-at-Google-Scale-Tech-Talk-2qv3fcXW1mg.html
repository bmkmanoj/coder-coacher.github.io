<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Software at Google Scale Tech Talk | Coder Coacher - Coaching Coders</title><meta content="Building Software at Google Scale Tech Talk - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Software at Google Scale Tech Talk</b></h2><h5 class="post__date">2012-03-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2qv3fcXW1mg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I want to welcome everybody to the this
next chapter of the Google NYC tech talk
meetups
a little more about the office you've
seen some of these pictures it was fun
talking about that particular picture
it's a real authentic water tower um you
know the one company that still makes
these things in New York City came here
and put one together it's over there for
those of you that remember the old menu
this is a Ben &amp;amp; Jerry's truck there was
aa the the this is the old Port
Authority building some of the elevators
this building can actually hold the
truck right we can move a truck up what
they used to do is they used to bring
the truck into the building move the
truck up fill it up loads of stuff the
truck used to go down used to go um I uh
need on't know why uh and it's one of
those things where I'm not going to ask
why you there because the question the
answer might be a little scarier than I
do it we walked into that large
conference room we saw Ben and Jerry's
truck in there serving ice cream they
served ice cream for a couple hours and
then the truck uh you've seen some of
these pictures also in the video this is
what some of the some of the stuff in
the office looks like there's also a
Google rule that you can't be more than
150 feet away from food if you consider
a horizontal distance I think that goes
down to about seven five ah you know
that's the micro kitchens are staffed 24
a staff filled up 24 by 7 we also have
four or five full-time cafeterias our
natural competitors cafeterias that are
staffed three meals a day with all sorts
of food there's also a lot of as you've
seen in the video all sorts of
interesting great time opportunities
within the office there's also lots of
great people um I started at Google
about two years ago I was sitting in my
cubicle at 8 o'clock in morning turned
around to say hello to the person next
he introduced himself as Brian half an
hour later as we're chatting back and
forth II let me know that his last name
was um you know for somebody who grew up
in the UNIX world carrying K&amp;amp;R this
programming language as the Bible during
my college days I almost me so he's the
the K and Ock he's the
hey in knr he sits right here Peter
Weinberger that W is also in New York as
well um another thing that habit I was
having lunch one day I sat down next
thing I know Vint Cerf sitting next to
me having lunch with me started up a
conversation for those of you that don't
know we've been served go to Wikipedia
he is one of two people that are
credited with inventing the Internet um
so pretty cool guy and you know on the
relative to these other three
individuals Guido embodied Eva we only
invented Bible so other public figures
who have visited Google we also get lots
of lots of people to come um just a few
weeks ago I came into the office early
in the morning and there were lots of
police lots of security I couldn't get
into the building because Bloomberg was
here having is one of the many things
that happen to work so um so we're
hiring there are if any of you are
interested or just quit being exploring
there are several members from our HR
and reading organizations as well
floating around feel free to the ask all
sorts of questions so today's agenda I
walk through it before let me uh without
further ado let me turn it over to
Michael Greg and pepper to talk about
building software at Google scale
Michael's gonna start getting the AV
stuff worked out um probably won't take
very long but I just wanted to introduce
myself
um I'm pepper Lee Beck Joe worked on the
build tools team here in New York one
thing that we're gonna try to do in this
tech talk is we want you not to feel
that you absolutely have to hold
questions to the end of the talk if
something just isn't making sense or if
you need just a touch more detail please
like throw up a hand and we'll see if we
can make things a little bit more
clearer but do know that at least the
part while I'm talking is going to be
very high level and then these two are
going to step in and dig down a little
deeper so if you feel like I haven't
gone through much detail please don't
stop me because I think that they'll
probably cover in more
to tell what's going on and now kind of
makes sense as the structure of talk
continues let's do some quick show of
hands stuff while we're waiting for the
presentation to show up um who's ever
been frustrated that you had to run make
clean to make something work yeah
awesome
uh who's ever waited on a compile
awesome okay what are some other good
ones who has ever been frustrated that
some library you depended on updated but
it didn't work with your code in awesome
all right cool these are all kind of
thematically things were going to be
talking about today
here it comes the presentation all right
thanks Michael all right so we're
talking about building software at
Google Michael and Greg also work on the
build tools team here in New York and
let's dive into what we came here to
talk about you know what I'm going to
actually also bring up the speaker notes
hold on just a second
okay so why do we need build systems um
everything from make to PI build
maven s cons all these build systems
that are out there all to make all the
things that you kind of know about why
why do we need them how do they help us
um one thing is that they encapsulate a
recipe for making your software the
ingredients are kind of the other things
your software depends on the
instructions in the recipe are the
commands you actually have to run the
the different executables that have to
be invoked in order to make you know
your outlets show up another thing that
they do is that they allow developers to
express their relationships between like
on other their dependencies on other
binaries and other libraries and then
thirdly they also help us get really
organized and help us audit things so if
we download some tar file from somewhere
and bring it down to our Linux machine
we open it and it's probably got
instructions including run config or
configure and make and then we have a
binary that's up and running so it helps
make the tedious boring things that we
have to do over and over and over again
um really easy to kind of pass around
among other developers so the systems
there's just a couple of like snippets
of a make file and a maven pom here but
um let's start picking on some of these
guys um let's first talk about how they
work ok so let's say that we want to
make some pizza so we're going to use
make and it's going to make pizza the
first phase is you have to load your
make file parse it and make sure that
everything's right so it's like okay you
want a pizza dough and you're going to
need the ingredients so if it's a dot C
we're gonna run GCC to make it happen
and it all parses syntactically and we
know that the inputs are there so we're
happy right so we can load it it's
syntactically correct and it looks like
we see our inputs the next phase is
analysis we need to know how all the
different pieces fit together
what depends on what um so after we know
that syntactically correct let's make
sure that it semantically is going to
result in a pizza getting created so
this is kind of just like connecting all
the pieces and making sure that we know
the order in which things have to happen
the third phase is execution where we
say the build system says hey first I
need to take flour and yeast which are
down here on the bottom and I need to
mix them together and let it rise then
I'll have crust
once I have crust I can also add in
cheese and sauce and then I've got pizza
after I bake it so execution is kind of
the last thing so loading analysis
execution looks like people are working
so far so what's wrong with make
primarily it doesn't scale and I'm going
to talk in the next slide about why
scaling is so important that Google um
the couple of things I'm not going to
read you all the kind of bullet points
on the slide but kind of continues to
use the pizza analogy one of the main
things covered by the first two points
is that there's this limited concept of
incrementality
inmate it does have some sort of
incrementality you know based on
timestamp so look the files already
there I don't have to build it again but
it's limit it's limited because it
doesn't consider for example the
incrementality between two different
developers that built the exact same
targets from the exact same sources
right wouldn't be cool if you had a
system that you know wrote the whole
company you only had to have one
developer built those sources and and
that's kind of one of the things that we
shoot for um it doesn't uh it's not
really awesome about guaranteeing
correctness of incrementality so there
are certain things that can happen and I
won't go into the details but they
essentially make make sometimes thinks
that it should be doing an incremental
build when it actually should have
started back a little bit further and so
many of us have had to run make clean to
kind of clear up that crud um so that's
about the correctness of increment ality
the other thing is poor parallelism so
what happens in make all happens on one
machine
and though you might have a deep
dependency graph with lots of different
things need to get built and you know
theoretically you could just start at
the bottom and work your way up in
parallel but it's hard to make it run in
parallel there are things like just CC
but it's also got some some drawbacks
and language assumptions and things like
that
and then you know kind of lastly each of
these different build tools for the most
part um or kind of built with one
language in mind or one mode of building
in mind and don't do a great job of
extracting out the the pieces connecting
the execution model so if we had to wait
at Google for make to build it wouldn't
be good we would never get anything done
but to understand why you have to
understand development at Google just a
little bit more so if you remember
nothing else from this slide
remember if we build all of our sources
at head now what I mean by that is we
have you know thousands of Google
engineers and they're developing
everything from our most low-level core
libraries that everybody uses and all
the different products all the way up to
the user facing UI that's very specific
to that project now every time the
developer builds they build against the
sources at whatever change lists they're
working at so they don't have library
dependencies we don't have the
equivalent of like a maven repository
sitting there with all of our core
libraries built that we can just build
against we compile everything from
sources but we do it really really
quickly and we take a lot of caching and
stuff so that it kind of looks like we
have them already built for the most
part but what's nice about this is you
know earlier I was asking about people
have been bitten by library skew well
internally Google at least we don't have
that because everyone's always
developing and building against pretty
much the latest version of everything
because it's the latest version that's
checked into the giant monolithic
repository that holds all of our code um
so the main reason I want you to
remember that is that that's where our
main requirement for
doing super fast super efficient builds
comes from is because we build
everything from the sources at the same
change list so one of the things that's
difficult about this is imagine that
you're on your brand new workstation and
you need to get all the sources for your
project well what if you had to check
out the whole repository to do that well
that wouldn't work so one of the things
that we do even if you only had to check
out the sources that your project
specifically was dependent on you'd
still end up pulling in millions of
files for something it does anything
interesting so one of the things that we
do to solve this problem is that we have
a file system in userspace
that we in this diagram is it's called a
source FS and it's backed by BigTable
and essentially every time somebody
checks in a change to the repository
this big table in the background gets
the same information and then our file
system essentially makes the sources
available five version number or like
index by version number so if you you
know check out the code at a certain
change list you're essentially just C
being into a directory to get the
picture of what it looks like at that
change list but you also are going to
make local changes that are specific to
your project so our build system takes
that into account and allows you to make
local changes and then we kind of
overlay that on top of what you were
developing against that a particular
change list and we'll work on the
details of how this all works a little
bit later but I wanted you to be able to
see this diagram on your workstation
you've essentially got all of the source
and all the versions of all of the
source in a fuse mounted files okay
alright so another thing about
developing development at Google is that
our build system is huge um it supports
tons of different languages um it's
extensible uh the way that we are able
to make all these builds happen is that
we make it massively distributed so I
have this big gray cloud here and it's
essentially like you're thinking you
know there's like
thousands of machines out there that we
can throw at the problem especially the
execution phase of building also as you
can see like on Joe's dev box he's got
all the sources but when he says I need
you to build food at CC he just tells
the worker nodes hey I need you to take
the source code that's got this digest
and we'll talk a little bit more about
how the digests are actually tracked but
it's got this content like hashed I just
think like you hash the file right and
it just says hey I need you to have
these files so that you can make the
outputs and here's the command line
you're going to run to get the outfit
outputs fires off the work on the build
workers it does the work and then it
uses an object file system in the
background kind of analogous to the
source file system which allows us to
keep track of all the outputs that have
been created during a bill so if Joe
builds a target at a certain change list
on one of these build workers and then
sue turns around and builds that same
target essentially everything's going to
be cached throughout the pipeline Sue's
going to hand the same hash digests same
command line it's all going to look
exactly the same and the build workers
are going to say oh I already have that
output here you go question
you
this thing on perfect so the we actually
special case inclusion of C++ files
that's exactly right if you need to
handle we need to handle include
scanning rather especially so we
actually do include that information in
the transitive closure that we build up
which I'll be talking about with these
lights yeah we're going to
correct we get some processing of the
file - yep no you're exactly right we're
gonna we're gonna kind of get into that
a little bit closer to that level of
detail anyway but yeah that does have to
happen okay so um one thing to notice
that's kind of cool about this is we
have a continuous build system and it
uses the exact it's a continuous build
and test system I should say so it uses
the same build infrastructure to do all
the builds which means that every time a
developer checks in a change the
continuous build system fires off one of
those builds at that change list which
really means that for the most part
everything's been built at that change
list at some time in the future I mean
it not immediately not immediately upon
check-in but that immediately upon
check-in the request to build it is cute
so if anyone later in the day goes and
builds up that change list it's a large
amount of what needs to be built has
already been built and we'll be able to
get quickly from caches so it keeps our
caches warm kind of throughout the
developers day all right
that's by the way this is not a super
detailed diagram there's lots of little
pieces in here that are being kind of
hand waved over and don't worry about it
this is kind of conceptually what I
think you need to know at this point
we'll go into more details soon okay
so what are we talking about with Google
scale why why is why is it that we have
to scale so largely um so here's some
numbers to kind of get your brain around
um every year we've got sixty million
bills that means 60 million executions
of our build system that's not to talk
about the individual breakdown of the
actions that have to be executed uh and
it's growing like crazy
there's over a million a hundred million
lines of code um and even a small build
what you would think of as a small build
like hey I just you know made a web
server and imported two files and set
run um
can have greater than 10,000 build
actions it can create generate hundreds
of thousands of artifacts um and there
are millions of edges between the
actions so if we build up this action
graph as you were kind of imagining in
the pizza example and path light you
know five edges or something like that
well here we're talking about millions
of edges to get your entire graph built
so one thing that we do here with our
continuous build system it's a little
bit different than other continuous
build systems is every change gets all
the tests run that that changed modified
some continuous build systems uh will
essentially build as soon as they have
some CPU available and then any change
lists that get submitted in between are
potentially what broke the build so that
when the next view becomes available you
don't know exactly what change list it
was it broke our continuous build system
as you may have heard from previous Tech
Talks we'll give us a green or red for
every single changeless that gets
checked in for those tests that it did
it affected um yes so we have to we have
to be super quick one thing I'll quickly
describe about this graph um essentially
this is our average um build times over
time so we're able to kind of bring down
the average time that's this blue line
going down even as the entire graph gets
more complicated so the yellow line is
the number of edges between actions the
dependencies of kind of all the system
so even as development becomes more
crazy we're kind of keeping up and
making sure that bill times skew down
let me see is that it for me all right
Michael why don't you tell us how this
all works so we had a bit of a
high-level overview of the build system
and how development Google happens
generally and what I won't be talking
about will be primarily how we ensure
that all of this is correct and then
we'll get later on into some of the
performance and the testing framework as
well to make each analyze your list so
we have a build language a similar to pi
build or same in concept web make file
or a pub which essentially describes the
various types of rules that we have so
let me define what a rule is a rule is
something that you can build essentially
and so we have a CC binary rule here
that means we want to build a C++ binary
rules belong to packages these are the
directories that the rules sit in and
the way that we identify them as a
package is very Python s we have a name
followed by a file News built so any any
directory in our source tree that has a
build file is considered to be a package
and the build file defines what rules
exist with the package so these rules
have different types and when we set our
build system is multilingual we have
support for many different languages
inside of it and we also have the
ability to extend it to custom role
types as well C++ is one of the most
common ones Python as well so what these
rules are doing is they are identifying
what we might want to build so each of
these rules has a name that allows us to
identify on the command line what target
or what package we may wish to to
assemble and they also have attributes
such as sources and these are the files
that need to be directly pulled in and
compiled and eventually linked into the
final binary they also have dependencies
on other rules so the syntax here we
typically use two slashes at the
beginning of the directory name to
identify the package a colon to identify
the rule and if there is no package name
then we assume that rules are within the
same package so here we have a package
name Death Star we have a C++ rule also
named F star and it depends on within
the Deathstar packages clones laser and
that exhaust part that blew up in the
end of the first movie we also have a
Python test this is so this source of
this sum of this role is the test itself
need dependencies include the thing
under tests and then any other
information that it may need for example
maybe if we wanted to test the death
star we need to pull in the older end
package so we can test its planet
blowing up capabilities so on the
command line a developer can then say
test slash last death star death star
test
so that will give us a result fully
armed and operational excellent the
emperor approves so there's a lot of
work happening behind the scenes in
order to give the developer that
capability to test software to build
different rules the first thing we need
to do is we need to identify the
transitive closure of the dependencies
of that rule so when we say the death
start test depends on the death star
that's not just pulling that one rollin
that's pulling the clones the laser the
exhaust port all the transitive
dependencies of the Death Star roll need
to be pulled in and that's once again
because we're building from source and
we need a two-tire transitive closure of
the dependent source tree to be included
in the compilation it also may depend on
other rules so we do the same there the
interesting thing about this graph about
the way that we build it is we perform
all of this in parallel so all of these
siblings the clones the laser the
exhaust port will begin running at once
then let's say that Senate and jar-jar
finish will start building all the round
as well so we take full advantage of the
parallelism that we can get for this
graph the only constraint is that these
have to be satisfied before we can start
building that and that will be a
recurring theme throughout the various
things that we do we have more than one
graph that we're using here so we'll
attempt to maximize the parallelism in
any graph structure that we build in a
similar way but before we can start
breaking these rules down we need to
talk about what an action is so the
actions are sort of the atomic elements
of the build these are things like run
the compiler on a dot CC file compile it
to a dot o or link the dot o files into
a final binary so these are essentially
functions we treat actions as things
that you inputs they get executed and
their execution produces outputs and we
expect these outputs to be there after
the action execute such that will
actually fail the bill
we're produce so we have a strong
functional definition of an action and
with this we can do things like farm
them out to remote workers or build up a
graph of them without having to worry
about the details of exactly what is
happening inside of that executor it's a
very nice sort of object-oriented
breakdown in the other way you can sort
of override behavior of one action when
you derive anyone so I'm going to show
you the sort of naive way of turning
this these actions into sets or sorry
these rules into sets of actions and
then I'm going to talk about how we
actually do it so each one of these
actually these rules produces a graph of
actions we call it the action graph of
that rule so if you're building
something like a CC binary ellipsis is a
binary flow then it will expand to
compile all of its sources and then once
they're compiled so this is a bipartite
graph it's a graph of actions and their
outputs so once they're all compiled
they'll produce the outputs Fudo and
Bardot they will be expanded into a link
action which takes them all and squashes
them together since the through the
linker and finally gives you the Foo
output binary and then there might be
further customization to these actions
based on their action types and also
based on the Flex that usually specified
for example compiling and debug mode
versus optimizing so if we have two such
rules one conflict that we may run into
is we may have the same output blinking
to two different things so for example
we have food oh here but then we need to
follow it up with link food Oh Tovar
dodo and link for food at OU des dot o
so the naive way to approach this is to
attempt to run these both in parallel
you know let's just start off a compile
start off a compile start off a compile
start off with a file or maybe we even
know that we need to do this well fine
but then we get here and now suddenly
we're repeating work or we need to pull
through oh and twice and it becomes very
messy to try and merge these and get
good increment Eleanore
Bill's remember that's a critical
property that we want we want to be able
to reuse the results of existing bills
as the Tomica Li is possible so this is
just sitting out there and we know that
it's not invalid that's digest is still
up to date then you know maybe we don't
want to execute this compile action and
if we attempt to run these both in
parallel and probably we're going to end
up with getting work so what we do is we
take this drift and we turn it around so
we take the output so we actually route
them as part of a new graph and one nice
side effect of this is that we can
actually start dropping the rules the
notion of a roll completely from the
build at this point so we call this
structure of the forward graph and this
is basically the ultimate blueprint for
the build so again we're building this
in parallel and these siblings are all
being processed at once we have notions
of how many unbuilt inputs we have
depending on each one of these actions
so for example we will know that we
can't do in VAR CC until this this and
this are found you will notice that
certain nodes here are shared by many
different actions for example GCC is
required by pretty much any CC act this
is also a bipartite graph so we have
outputs linking two actions linking to
their inputs linking to the actions that
generate them so this is actually kind
of nice because we can go and construct
this graph and immediately once we hit
the leaf nodes we can start building so
we say we need foo well we know we need
to link it to bar and in Fargo and photo
we need to compile these to get those
files and oh hey here's bar dot CC let's
start compiling it you know hey here's
food us to see let's compile it let's
start compiling it so this is happening
in parallel and certainly within level
sometimes across level so we're taking
full advantage of this graph then once
we build all of the dependencies of each
action we can then step up through the
graph so we have bar dots you see we
have food see see we execute these
compilations they succeed they produce
Fargo and photo and then we can start
linking so what we're doing is we're
just
running a bunch of things and as soon as
the number of dependent inputs differ
immense to zero we are launching a new
thread that is executing this action
gradually it's not even a thread you're
just sending it off to the remote worker
and waiting for it to finish it produces
the outputs we know how to talk to the
output file system so we know how to
check them and then finally when we
finish building we will reach the top
the outputs that the user expects will
all be built and we'll know that we're
finished so Gregg will be explaining now
how we make that performance and how we
take advantage of the incrementality and
catch high ok so Michael is covered then
over the past few minutes how we can
sort of define these roles ie the
binaries that we're trying to build we
can define the structure them and how
they relate to each other in ecology
turn that into an execution strategy
forward action graph and we can traverse
through that graph in order to produce
final outputs with your clients basic
structure works we wanted to be fast
I guess that sets a given that you
always want your bill to be fast but we
really want it to be fast
we need it to be fast as has been
covered earlier and put some numbers
that you saw earlier few slides earlier
these are large graphs what you see on
the slides here are only tiny tiny tiny
subset tiny representations of the
graphs that we have to work with
mentioned in the slide a long time ago
what we considered to be I think I guess
would call it a medium sized growth
market step for not a large one can have
tens of thousands of actions in tens of
thousands of files that are inputs and
outputs of those functions and literally
millions of edges these are large graphs
we need to traverse if we quick to the
extent that can we have this we have
this integrated code base everybody
builds from Quetta more built into the
same branch everyone is building
everything all the time that we have
it's very heavy systems for continuous
building and testing feedback on whether
code works or not
it's really really critical to our
development process that we can traverse
through these graphs as quickly as
possible really not too many more words
so one thing that we measure this will
not come as a surprise to anybody I
think but we do measure this we do
notice fairly well is that most bills
are implemented right most people when
they're doing builds usually that the
standard cycle of the build involves you
are in your IDE or coding away probably
done that build on that binary before
you make one a couple of changes
probably a single file and then you
build again and see how it works and
then you tested it for changes work or
not
it couldn't serve repeat that process so
even though you're building a rule which
may be part of a very very very large
graph you're actually making changes to
very very very small part of the graph
right most of the time the structure the
graph doesn't change what it looks like
doesn't change what actually the
contents the files involve they have
into each other which means we really
need to optimize for this case
assistances but the vast vastly greatest
percentage of fulfills that deal we need
that to work well so we spent a lot of
time making sure that we traverse to
these graphs as efficiently as we
possibly can and then we don't do any
work so one thing is again like these
builds tend to be incremental we need to
support the incremental value space and
as has been mentioned a few times not
every build system supports that very
well right there's this idea which a lot
of there's people in still sort of get
used to this idea that incremental
builds are not as reliable as right that
it's possible that if you're Bill if
you're like your software doesn't work
for some of these maybe just kind of
some weird snake or something we'd have
to clean out your build and and start
again and there's a pretty good chance
that you'll fit whatever was wrong I
plan to that money plans in the past
before here um we don't want that at all
and it did that's a complete completely
inefficient use of resources and
completely unnecessary so we have strong
guarantees for correctness even for
internet builds and our build system
incremental builds are as reliable as
correct build and we consider above in
our system if you ever have that if
someone has
do that for some reason we want to file
a bug report and so really the key thing
that we're aligned in order to feel to
making criminal felt work reliably is we
do digest based validation of all our
files right so we have a craft and we
need to know what parts of the graph is
change probably most of it has so which
part empty so we examine md5 digests of
all the files in the graphical container
so this is distinguished something like
like make where we look at the change
time right which is a less reliable way
of telling whether or not a file has
affected or whether or not that
translates into necessary work done
there's really nothing more reliable
than coming to digest which is really
just a standard for the actual
conference has a team condom look at the
content so that is the be way in which
we figure out whether or not files are
changing how we look at their digest one
problem with that in the abstract is
that you know computing a digest is free
the way you can get a digest the
contents of file is you read the file
you read over the entire continent and
then you you do some sort of function
there and that that has a performance
edge that as well that's that's real
issue so we strongly strongly strongly
mitigate that through support in our
various systems our source our source
tree system which was discussed earlier
alpha tree system these are these
systems basically pre compute and store
the digest of every piles that are fall
so if you check something in to the
source tree and you build at a certain
at a certain point in that in that cert
in that source Creek you basicly have an
immutable state of the source which
means that you know that that follows
compute it and we do that intercept when
we want to find digest files we just
look at those pre compute values we
don't we almost never have to actually
run to the contents of an outsider so we
can do that quickly so given that
ability to reliably figure out which
files have changed we have our craft and
then we go through this process that
Michael is described earlier of
traversing of the graph and
you actually produce helpless but again
we want this to happen quickly and we
never want to do work we party of course
we aggressively cash everything we can
one of the consequences of highly
incremental bilities it's much more
likely that you change the contents like
a CC file then that you've actually
changed the structure of the graph right
how often we go into like a bill file or
make file and change the dependencies of
various holes it happens but that's not
the common so more often than not this
graph will look exactly the same who
builds your building the same the same
role you have the same structure that
guy beside the chain so we very much
catch all that information of all those
stages floating analysis and execution
and everything takes to build this graph
we cache that as much as we can whenever
we can and for the most part when we do
build especially final build this crap
we already have this fits for grub we
don't need to be do that from scratch so
we can work with the graphic then from
there on top of that we have what we
call an action cache which tells us
whether or not we actually need to
execute in action so we're traversing in
this case in this a subset of my cloud
before we have five actions right we
have a number of files source files a
few intermediate files need final
outputs and we apply back to produce
scale so as we so what I'm trying to
represent in this graph here is anything
that's white is essentially
the source files here being like means
that that they haven't really changed
but the user who's doing this build they
they made some small change to part on
CC kind of dirty spots they see anything
that relies in part of CC that's gonna
have to be pre value but nothing that
nothing else does only whatever relies
directly or indirectly apart on CCB's to
be evaluated so we use an action cache
combined with our ability to find digest
in order to traverse up the stream
figure out what we actually be so when
we look at this compilation action here
filing bash TC we tell we submit to the
action cache okay we have this action
with the signature we have the digest of
these inputs and we have the digest at
this output is that in the cache did we
perform that action that specific action
on those inputs produce that output
before and if the action hash has a hit
then you know that nothing is changed we
don't actually need to exit we skip and
we don't next
get a support safe so that way we can
move up the graph and these actions
never need to be executed this will not
have to be executed because we'll see
like this the signature of this input is
different from what we have from
previous runs won't have a hidden down
so that saves us a lot of work we don't
actually have to execute most of the
time a lot of the action to produce
outputs um
this alone actually isn't quite good
enough though to get a ideal performance
again these graphs are huge and we've
actually come to a point where the
graphs are so large with these millions
of edges that just traversing over the
graph in order to even consult the
action cache has a noticeable impact on
converters that you can notice how much
time it takes just to diverse background
so we have another optimization that we
apply which is essentially called other
construction you live the dirty graph
based basically when we start qualifying
incremental builds we look at which
files a change
and in this case part ICC a and then
only of we figure out which part of the
graph is grayed out which part of the
graph might be invalid in this result
set and then we only Traverse up that
part of the graph we don't see any
people subscribe because we just
automatically know nothing and the
consequence of that is we don't need to
pull each person suit graphs our goal
which we achieve fairly well in this
context is that the the time of a build
should be proportional to the size of
the dirty graph to the extent that the
dirty graph will you change the very
very small subset of you Huayra graph
that should be much lower than the time
to evaluate entire so um so this gives
us some really important performance
benefits makes our builds a complete a
much much faster than they otherwise
what if we didn't have something but
there still I'm sorry did you have a
comment ok so there's still one big huge
optimization that we can make that I
haven't covered in this Lydell that has
been mentioned time in earlier slides is
another major major bottleneck in in
your average abilities is
resource extensively do this on a single
machine with one for two or three or
four eight community course that's going
to be a bottleneck if you if you're
doing the river barge so so the next
sort of logical step especially in the
Google world because we have a very very
large number computer displaying around
that's better to do with them is to is
to use the cloud and to extend possible
form out the work that we actually need
to do out into the cloud and get
dedicated machines many as we can get in
order to actually do that work so now
we're lucky enough to have thousands of
machines available just for the purpose
of executing build actions you know
running in a GCC compiler GCC Weinberger
or whatever and as it's been covered
earlier the structure of our build
system the way it works is is carefully
designed such that we have portability
branches of have this ability to move
our actions put them over to whatever
machine and we always know that our
actions will produce outputs which are
purely a function of their inputs of
regards to what machine to run on the
GAR so when you run them etc etc so this
is again going back to the functional
model in action as we view them in
action is essentially just a function
which takes inputs like the digestive
input like on this CC file which has
this complete contents in it
its environment which may be environment
variables or that sort of thing applies
the function which essentially it
essentially what that means is the
function is a command line by you
command line is GC C - C my file dot C C
whatever and it produces an output and
we know that given that commands can
mental invocation given those inputs and
given the environment we always know
exactly what the output will be there no
side effects whatsoever we don't have to
worry about what Bashir on or anything
else like that they makes these actions
portable we can then for farm them out
as we need to onto any arbitrary shape
we have a few restrictions in place in
order to make that work right like you
can't have a reference to a local path
like a path that's that's like slash
users as local something like that right
we had we have very strict hardening
work um
I'm sorry uh we have a cross target
compilation we can compile for various
targets target architectures from from
whatever source architecture and we do
support that in fairly large number
varieties
then what's also neat here and they're
going to be mentioned before is you know
these functions like as far as the build
system is concerned when in these these
machines which are executing these
actions it only knows is you have a
command line vacation and you have
inputs it doesn't know anything beyond
that they're black feel like what's
actually happening is a black box built
system doesn't care which makes very
very easy and natural for us to have a
heterogeneous system so you can have a
single build which can involve C++
compilations it can fold Java
compilations call running shell scripts
and whatever you want everything can be
mixed together whenever lines and any
particular language of technology so we
get a lot of interesting educational
integration leverage and then aside from
having this infinity view that we get by
having the zillion machines are working
on this we get this a natural benefit of
because we essentially have a service
here at built servers right like
everyone who's doing their bills they
submit actually need to execute this
build service and then it it computes
the the actions and then produces the
outflow which buts in fact they build
because everybody's using this common
built service we have a very natural
opportunity for an additional layer of
fashion right cause user caching because
everyone is began building ahead built
in very large graphs generally what
you're modifying is a very small part of
that graph with very many common
dependencies right often what we build
and what we change is like multiple
layers of abstraction level above the
lowest level libraries that we have like
our code our volume code or network's
ability string process code that sort of
thing which means that and because
Indian people are building all the time
and tests running all the time it's very
very likely that for many of the actions
that you run on the build someone else
has probably run that action on those
conflicting some time passed and ours
very much aware of that and if it finds
out that you asked you to execute an
action that someone else at some point
has executed a pole just sub store in a
cache return the cache file which means
even if you think you're doing a clean
bill even if you never run
on your system ever before it's actually
probably highly incremental is probably
very little vortex that in this again
this is critical this is really
important it's cool but it's also really
important we didn't have that feature we
could not build a iterate in the way
that we do we would have qualitatively
different sort of it so so this is great
this is wonderful
um we can now do with infinite machines
and and fairly a fairly optimized
pipeline for running throat graphs
efficiently we can get our outputs uh
really really quickly as a result of
having this we did run into a few maybe
unintended consequences one of them was
be at some point we realized that by
doing this we sort of created a
world-class denial of service attack we
have outputs right we have large outputs
again everything we do is large
everything the bill graphs are huge the
binaries are so it's very normal that we
producing and we like you know we put
everything from source we pretend to
statically link everything we can so
it's very normal that you could build
like a C++ binary which will be
gigabytes in size and the object files
that produce a finer they're also
gigabytes in size right so what happens
if you're in some office maybe like
limited network damage if you want to
run your build you want to build a multi
gigabyte file farm out all these
compilation you know has like a thousand
CC file feed acknowledge send them out
to the farm instantly they come back
gigabytes of data coming back to you and
you're building all the time everyone
else is building all the time as well
you've got the denial of service and
that's a problem right so we really
needed to think about carefully what
we're going to do with these uh these
output artifacts what we want to do with
them fortunately most of the time people
who run builds don't care about many or
most of the of of me of the Outlands
right again if you want to be able to
see the c++ finer probably just care
about the Barney I don't care about the
intermediate object files it that they
can produce intermediate steps may be
ideal you probably if you're running a
test which often involves building code
and then and then and then executing a
chess program on top of that you don't
care about any of all you one knows to
the test for you
you can meet any outfit you'll need the
L provider so basically we're executing
our actions producing this output out
there in the cloud we really don't need
much of the time those outputs back
under local machines so there's really
no point in and sending them back and
forth in a knife sort of way so
essentially what we do is we have
systems which support keeping our output
files in the cloud all the time close to
where everything is happening and only
delivering it back to the end-user
really when they need so there's two
notable systems that support that one is
simply a cache for our execution system
British execution system again you do a
C++ ability to a simple one compile OCC
file produces an object file that gets
followed up by a link which respond a
binary right which means that when you
produce that object file is an output of
the compiled action there's a very good
chance you're going to need that again
nerds right so our execution service has
a very fast a very very low latency
access to recent recent outputs it's not
art it's not so it gets it gets there
gets infinite because a little bit you
answer because it's not a totally
stateful system a sense of whenever you
run an action there's no guarantee that
you're going to run to action on the
same machine that ran the last action so
again we're producing this object file
and that we use that object files an
input in order to produce the final
binary those may run on separate
machines so we also have in addition to
local in memory storage on these
machines of a cache we also have a
certification kilometer so essentially
when you um choose to execute an action
and remote machine you you send it a
command line and you send it the digest
of the files which are necessary for it
to work what it'll look into the local
cache if it doesn't have information is
hopeful cache then it'll ask its
neighbors we're also very close to a new
can access quickly do you guys have it
many of them have it
it'll retrieve it from them quickly if
nobody has it in the cache then it will
send a request back to the end user and
say ok I don't have this
only then will it actually transfer a
file from your local user machine into
the file so that's how we can allow a
intermediate object files and output
files to be accessed quickly and
efficiently but ultimately in the end
after the build is done you do need a
reference to the outputs and we do allow
people if they want to access whatever
they want even though most of the time
people don't care about video files they
can access everything if they want to
allow them to do that the way that we do
that is we have another alpha storage
system more persistent it's a can store
much larger amounts of data for much
longer kind of you can almost think of
it as like like a like a level one
versus public and that's where build
results go to to finish out their life
or to die or whatever happens with them
at the end of the build so what happens
is when you're done running your build
no output data actually no applicant
actually gets sent back to your local
workstation why ends up happening is the
system sends back metadata it sends back
a list of paths of the other output
files that were created and a list of
digests right the digestif of the
contents of those files and then it gets
stored locally and then on your local
user machine you have we have a a daemon
the runs which provides use access to
making place a fuse essentially means
file system access so it so what looks
like a file system to use end user you
can you can navigate to the file system
you can find the name and the attributes
of both the output to be produced but
that's all metadata not the actual
contents are available unless you
actually request it so what happens if
you kind actually run a binary that
you're accessing this file system then
this demon will talk to back and then it
will retrieve it to your local machine
and then basically it's on demand X to
convict one as you want in no form and
that's critical to allowing our built
you can fork quickly and not bring down
our network and I guess there's a
reference there about I don't member the
exact speed right so apparently they
make it makes a about a 2x improvement
on the speeds for builds versus not
having which is great right it's another
great option
so it's a really quick summary of a leg
up in practice or the results that we
get from everything I've been talking
about I think probably the most
interesting thing is this it kind of
looks like a like like a flat reading or
something like that but basically what
this is saying here is I was talking
about caching in the backend in the
distributed execution service cloud what
that is saying is about 90% of actions
are castles means even if you think
you're doing a clean build probably 90%
of the actions you need to execute will
never ever actually get X they're stored
and available and and that work doesn't
have to actually be done which is really
again a big deal if we can cut down the
amount of work that we need to be by 90
percent and we have thousands of
machines available to do work we can do
a lot you can do it quit and I guess you
know so this graph is little bit
interesting though obviously showing
what you would hope the incremental veil
of 10 which are represented down here
are faster than them then please write
server measurement the time it takes
over over connect is different periods
are 20 bills and you'll see that bills
during her mental fur right this
representing of 15,000 action role which
size the incremental bills are basically
between 50 and 0 seconds right down
there and the non-incremental once you
know they're a lot larger they're still
measure in the seconds displayed
concequences all right and again these
tend to represent bills which if you
didn't have these optimizations would be
talking like hours or maybe the longer
so you know we can do things like we can
build our entire entire source tree and
we do that we can build it a very large
scale and be heavily rely on these
optimizations in order to be able to
make that work
and to allow for the Belkin posts so I
think Michael is up again now to us or
leave this into how some per our testing
right so this this scale makes available
some very interesting opportunities to
us and one of those is to run tests
alongside Bilt's and not only to just
run tests manually but also have to have
a test service which runs tests on a
continual basis so the way that we
integrate tests into our build system is
to treat them as actions so we already
have this nice functional mapping
between inputs and outputs
well the inputs here are going to be the
targets and its dependencies the output
will be a test log basically and oh by
the way along the long net result will
generate a status of tests or fail so we
do some interesting things here we do
what's called sharding tests so you may
have one very large monolithic test that
has I don't know 100 test cases or
something and we can actually break that
up and we have all of these remote
executors just sort of waiting to run a
whole bunch of work so we break it up
and we distribute it so we'll send one
executors one test method we'll send
another one another test method the
important guarantee and it allows us to
do this is that one test does not
influence the results of the other so a
tests have a sort of Hermeticism to them
where they're they're guaranteed not to
sort of play around with each other or
execute in a certain order that's
another guarantee that we make so in the
end each one of these test charts gets a
status of pass or fail so you can see
here we have a example of our death star
test where we want to do things like
test the laser make sure it works test
that we can blow all the run-up that's
important test the TIE fighters launch
and then you know maybe test that the
reactor is not some sort of fatal
weakness that someone can just fly in
and turn their targeting if we were off
and blow it up so these three shards
might pass and those might that one
might fail so we have an aggregation
routine that runs at the end actually it
now runs incrementally but at the end of
the build we
aggregate all of these results and you
know if we have one chart that fails
then we consider the test to have failed
so we have a bunch of different things
that we detect no one tests are sort of
flaky when they'll sometimes no
sometimes fail we obviously know when
they consistently pass or fail and we
have a few other error codes as well as
certain you know external errors to test
which blow up and we can't recover from
so protip make sure that all of your
tests pass before release and that's
very important in the case of this all
right so this the entire build system as
well as its support for testing enables
some very ambitious sorts of goals so
the typical the typical routine is as
Greg was saying to edit compile tests
repeat so you're essentially stuck in a
loop where you're just iterating over
and over again over the same process and
I think this was mentioned in Joel of
software the fact that Joel and software
the faster you can do this
the faster you can code so we want to
eliminate this sort of thing where you
know people are sitting on chairs
playing swords with each other because
they're waiting for their builds to
profile we don't want that that's not
acceptable at Google you can play with
stories at other times but not not while
you're building ok so this allows us to
do things like test the entire code base
at every revision we do it we have a
service that does it we might actually
be giving a teacher tech talk on that
service at some point it allows us to do
things like pre-release testing of the
entire code base so let's say that we're
releasing some piece of software that's
going to deeply affect everything like
the builds to itself and we want to make
sure that you know everything still
builds with it we do that
on a routine basis it will build
everything and get the results back in
hours two days so that's pretty cool
because Google's code base is very large
obviously we can catch results across
machines across users Greg Williams bet
this provides transparency tips to
everyone especially when you combine it
with these full full code based builds
that we're doing is if we're running
everything
we're almost never going to have a cold
catch we can troubleshoot our releases
we have a nice little search thing going
on where we can actually search through
something like 10 million changes to
find the one that broke in a release and
that takes know about 10 minutes so
that's pretty cool and that's leveraging
the power of remote execution on a very
large scale and you know we can spend
more time doing things like hanging
pterodactyls on the walls like this one
which is on the fifth floor instead of
having to worry about finding bugs and
you know doing all of that quality stuff
that we sort of want our software to
handle for us so ultimately this lets us
enter it quickly over our bills to
produce high quality software which is
the end goal before so at this point
that is our presentation we'll be happy
to take questions open up the floor
oh yeah send them we have a dev blog
which is open to the public this has
some of the information in this
presentation some of it is a little bit
older there's a bit more detail on some
other things so if you're interested in
this topic definitely check it out all
right so yeah just come up to the mic it
was just wondering is the version of GCC
part of the build system or just another
command within a description of them
it's it's a file which is checked into
our source Depot you can change that or
as you eat itself also what they don't
care about what that is or how that
works has everything broken have you
what when it's when you have downtime in
this sort of system all work stops
across the entire company right so we
try to make that not happen okay that's
very rare it's not it's not like this is
breaking every day it's a quite a stable
piece of software there definitely
challenges keeping it running so that
that's something we dedicate resources
hi my question is with these Action
workers that you have living in the
cloud how do you load balance across
those I mean you might have different
different action
is that you know what some that take you
know minutes at a time others that you
can complete in a few seconds how do you
like determine that and balance
appropriately we have a scheduler
component to the workers it sort of sits
behind them and distributes the load
accordingly it knows the state of all of
the worker machines so it can figure out
you know this is a really large job it
has a lot of you look sorry large so
let's send it to something that is free
it will be free for a while I might also
come in we do have timeouts on actions
they can't run forever and take up a
machine forever um and this is another
case again where this caching this
aggressive caching have really really
comes in handy like expensive actions
can be free if they've already been done
before in our cache and I can't stress
enough how critically we rely on
aggressive use of caching caches but is
there is there no way for like I know
you can you can determine the size or
the expected length of an action based
on how many dependencies there are test
cases things like that but what about
like single actions that rely on like
you know a few core actions I mean like
for instance a build of a file that's
you know millions of lines long like how
do you can you determine that or are you
tracking like how long it took last time
or well so another thing that really
helps with this is the the worker
machines themselves also output copious
state so I mean it's like you know I'm
this busy right now and so if it's
memory spiking or it's disk is busy we
we get all kinds of signals like that no
not sent more work that way
well thanks my question is is there a
strong coupling between your version
control system and your build system is
so they're kind of is there kind of
isn't um we the the build system is
designed to not really care where the
sources come from but some of the
optimizations on top of the build system
like for example being able to get the
digests correctly from an extended app
or directly from an extended attribute
um is a feature of the extension of our
version control system so there there is
a ultimately for some of the
optimizations there's a tight coupling
although you could in absence of the SMI
pack we do
in absence of the source file system
still built so it can be correct it's
just not as efficient so it kind of
degrades nicely in absence of its super
smart source control system um I guess I
just didn't pick this up from the last
section on testing but if you have a
component level test does that test get
run every time you build things or just
when that particular component gets
built so the test system that builds
everything only builds new tests that
are altered by the component so it
actually knows somewhat of the
dependency graph and it can determine
you know something changed which updates
the test and you can even run okay so if
that's the case what about the scenario
where there is some element of
probability in this test say it's going
to fail 90% of the time but you know it
just so happens that it got built and
then no one else is ever going to try
building it again right so we have some
detection for that scenario it isn't
perfect sometimes developers just see a
whole bunch of failures we decided you
know this is flaking we're going to
ignore we've gotten a lot smarter about
that sometimes we've run tests multiple
times a trunk if it passes the first
time it fails the second we might say
okay something crazy is going on one
other thing to note is that to some
extent failing tests continue to be
rerun through later changes it's it to
some extent that there's there's a way
to say that you know this test even
though it failed and even though none of
its sources have changed keep writing
because it might pass later
yeah but the more troubling situation is
when something succeeds and that's the
misleading result right we tell
developers to fix their tests except by
breaking it to come two questions one is
there's an implicit presumption here
that the repository is actively you've
created the output cache is a repository
at the beginning of the talk you said we
don't use repositories all contraire you
do you just create them no that's that's
very true yeah our one thing that's kind
of tricky though is the
the permanence of our output cash it's
not very permanent so where as in like a
maven repository you can go back and
look at version 900 stood yeah
these NIC leaned out Shakespeare said
about roses the second comment into a
question really is before you embarked
on this did you ever investigate why
clean why people think incrementals
don't work is it really because for
example they have dependencies on deep
down H files that don't somehow in
effect is the make bug that make doesn't
detect a change and then that breaks
something downstream right we actually
had a comment as when we had test this
around to the teams that effect and it's
true that is usually the cause but
ultimately make needs to be detecting
that and alerting the user hey you've
come yes but if my point is it's clearly
a problem with makes way of keeping
bookkeeping it's not a generic problem
of incremental bill it's just make you
sloppy right right and in theory
incremental bill should be as reliable
as clean bills but it's complicated to
make them work reliably because things
like you're suggesting figuring out
exactly where all the dependencies are
and picking on sex what has it hasn't
changed and making sure that there
aren't urine side effects requirement
variables or in requirement effects
which which could have in fact it's not
we invested in a decent amount of effort
into making sure that we handle that
well
before we get the boot so if I didn't
catch us earlier I apologize but did you
guys ever come up with any weird bugs
out of the fact that you were sending it
to the cloud and having all these guys
go at it at the same time we sorry we
have had in the very early stages of
development some concurrency related
issues but those were pretty simple to
sort out so we haven't really seen
anything fundamental that couldn't be
fixed relatively yes I got the
incremental nature of the make and the
build and they're sort of tracing the
code tree on the on the testing side are
the testing sweets in the sharding also
incremental so do they help you isolate
out exactly what change so if you make a
if you make several changes our whole
suite of changes
does that help isolate which change
caused with which tests failure and can
that somehow recurse our partition
excellent observation yes we actually do
treat tests incrementally as well we
have a cache of test results
we only rerun tests when we know that
something has dirtied it the same
actually reffering working to be used to
detect builds to you so I guess now
we'll needle and BA all right we I'm
really really quick when I'm just the
size of the bill form of how often you
you you know add machines to it Tetris
its thousands of machines this question
is like how often we have to expand it
oh I see well yeah the corn size in the
whole thing is fine just out of
curiosity to put some kind of you know I
I think the the most specific we can be
is thousands of worker nodes and as far
as expansion I mean it's kind of gradual
like yeah we just occasionally might
need some more it can happen but unless
we come to one we're hitting the limits
of what it can handle then it needs to
so any plant with this open source OH -
open source it yeah
yeah and if you're on Twitter</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>