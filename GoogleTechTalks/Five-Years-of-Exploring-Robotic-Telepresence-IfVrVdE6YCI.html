<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Five Years of Exploring Robotic Telepresence | Coder Coacher - Coaching Coders</title><meta content="Five Years of Exploring Robotic Telepresence - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Five Years of Exploring Robotic Telepresence</b></h2><h5 class="post__date">2008-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/IfVrVdE6YCI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">well thank you everyone for coming today
we welcome Fred new car the founder and
CEO role dynamics he's got background in
real estate finance consumer electronics
and web applications his latest venture
is in the field of Robo telepresence and
today he'll be giving us a presentation
of the last five years of their work and
the demonstration of his robot so
everyone please welcome redknee car hi
good afternoon it's great to be here
with you today my name is Fred Nagoya
and today we're going to be talking
about robots so every year I give this
talk and the talk is called five years
of exploring robotic telepresence I just
increment the year every year and I
guess the lesson is that making robots
are not that simple so today I'm going
to tell you a bit about who we are and
what we decided to do and how we ended
up doing it in where we want to take it
and we're very excited to show it to you
because this is a 2008 is the first time
that we're actually putting robots into
customers as office and so it's sort of
our coming out here and I'm very happy
to be here at Google to be doing this so
who are we it all started a long long
long time ago when I watch Star Wars way
too many times and mike most of you i
assume here this was just something very
exhilarating for a little boy and most
of you who were extremely popular in
junior high school like myself probably
attempted to build r2d2 a few hundred
times and not succeeded very well well
neither did I but it taught me a lot of
valuable lessons that taught me how to
take things apart and to modify things
and put them back together and they
never really worked as intended ever
again that said the experience was
phenomenal and so back in 2001 I started
traveling the world talking to people
who were making robots as a hobby or as
a business or for academia and research
and I wanted to understand what it is
inside all of us that makes us
interested in these non biological
beings that seem to behave intelligently
and what I found was that whether you
could write software or not whether you
were a techie or not everybody is
interested everybody is drawn into two
robots everybody is a can relate to
something that behaves intelligently but
it's not biological and so this was very
interesting for me and as I explored it
for a couple of years by 2003 three I
decided that I was going to make a
company that made robots so we have dr.
saleh me here so high so i met dr.
solomon at USC when he was finishing his
PhD and we talked a great deal and in
2003 we eventually ended up in my
parents as garage and we started
building robots now prior to that we had
to decide a framework for making these
robots and we came up with four basic
things we decided that robots that we
made should be simple most of the robots
that we knew of were things that were
extremely complicated made by NASA or
the military it wasn't going to be a
two-man company making those so we
wanted to make things very simple the
other thing was that we wanted to make
it expandable so we understood from
early on that whatever we did was going
to be great but that the community at
large can really contribute and make
something even greater the third thing
was affordability and back in those days
most of the existing robots were not
really affordable there was a little dog
by Sony named I Bo that was a few
thousand dollars and and there was some
military stuff and some medical stuff
but really everything was very expensive
compared to the value they provided so
we set out to make something that was
affordable particularly given the
application that it was going to have so
we wanted to make something that was
also useful that was also something that
was lacking in the robotics field at
large so this useful thing was actually
a little challenging
so how do you make a useful robot and I
was very much interested in this idea of
teller robotics which is I can put a
robot in a remote location I can log
into it remotely and I can make it do
something in that remote location
without me being there so this has a
tendency to provide certain things for
me so for example if it's a dangerous
job if I'm looking inside a nuclear
centrifuge it's probably a better idea
to send the machine than to send the
human or if we're going to Mars we can't
even send the human we don't have that
kind of technology what we can send the
machine so this notion of teller abadox
was really really interested to me and I
was specifically interested in something
called robotic telepresence so robotic
telepresence can we get this thing to
work there we go so robotic telepresence
was this idea that I would embody a
robot would embody me in a remote
location and I can interact with that
remote location in person so to say it
differently imagine a video conferencing
system on wheels it's got a screen it's
got a camera and I have a laptop at home
and I can log into this robot remotely
and you would see my face on the robot
and I could see yours via the robots
camera and we could interact but more
importantly I could move the robot
around and move inside of that remote
location as if I was actually there in
person and so if you look at what this
really means it's it's basically the
power to be in two places at the same
time so let's think about that for a
second you don't have to clone yourself
anymore to be in two places at the same
time you can just have this robot
inhabit inhabit your body and
immediately transport yourself to a
remote location and this was a very
powerful concept to me so as we looked
around this was not really a novel idea
right so NASA had been sending up our
robots and probes to different places
the military and law enforcement was
using EOD robots for ordnance disposal
and more recently we were putting
telepresence robot central hospitals I
want to take one
minute and talk about the telepresence
robot in a hospital so imagine urine a
remote hospital and you need a
specialist a brain surgeon of some kind
normally what would happen is your
distance would not allow you to see a
specialist unless you actually got in a
car or got medevac'd into the bigger
sister city hospital and had access to
that medical professional but not what
we're doing is we're putting these
robots into the remote hospital and now
and instead of moving the patient we can
move the doctor without physically
creating downtime for the doctor and so
these types of applications were
starting to come out and showing value
so now that we've decided we want to
make telepresence robot we have to
decide how we're going to do it and we
wanted to use as much ubiquity as
possible so for example you wanted to
you to be able to use your own laptop
that you're familiar with we wanted you
to use the internet we didn't want to
create a new network we didn't want to
create a new control station you want it
whatever it is that you already own to
be able to log into this in a very
ubiquitous frictionless way and be able
to hold a one on one or one too many
conversation in a very nuanced way that
you and I are having right now now what
happens if we can accomplish this for
things happen we can eliminate travel if
I don't need to travel to China because
i have this robot I don't need to travel
to China I've cut that out and by doing
that I'm eliminating downtime or at
least decreasing a great deal and so if
I can do that then I can increase my
throughput and overall as an
organization I am increasing efficiency
because we're maximizing collaboration
this essentially means cost savings to
the company and this was the promise of
what this technology held for me and why
I got so interested in it so I'm going
to show you a brief video that sort of
brings all of this together and then we
can continue more about it
can we get a sound do we have sound okay
you know an executive for overseas his
company's website but he constant
interaction with his dedicated in cisco
and his superiors in new york in
addition to conventional collaboration
tools and software those company
basically insulting several telepresence
robots which enabled build a hold on the
spot face-to-face meetings and or San
Francisco use for the robot Phil can
walk through the hallways and approach
anyone or purchase a water-cooler
conversation to the scholars this
catchment of communication the robot is
just like only faster and upfront work
bill can just log in and start driving
and talking to they can see the states
on the robot and he can see that one was
last rock screen sometimes bill just
loves it or no reason at all and
interactions have been intuitively and
naturally if your job requires you to be
in all corners of the world at the same
time perhaps Tyler's the robot
you can see more at Virgo dynamics com
so that's bill and he wanted me to tell
you that he didn't spend a lot of money
on that haircut so this is the idea that
we can log into a robot and project our
presence into a remote location move
around that remote location and interact
with the people in the environment as if
we're actually there so this so this
idea is basically presumes the fact that
talent is distributed so a company might
be located in santa monica but talent
can be in northern california it can be
in ohio it can be in bangalore it can be
anywhere in the world and the companies
that understand this are the companies
that are going to thrive going forward
so they understand that they have to do
can we get this sorry guys so they
understand that I have to do three
things really really well they have to
be able to recruit in a distributed
manner they have to be able to evaluate
these recruits in a distributed manner
and they have to be able to empower them
in a distributed manner and this has
many many many facets and applications
here but I'll just go over this one
particular one here this is a company
called sybase how many people know of
sideways the database company so sybase
this is their ontario office in canada
and they had an employee who had to move
away about 1,200 miles but they didn't
want to lose the employee and so the
employee is ivan and this is ian his
coworker so they did not want ivan to
quit working and so they started they
offered him a remote position he can
write code just as good anywhere else as
he would at the home office so they
started working together using
conventional tools like email and IM and
phone and as they did this they realize
that there was some communication gaps
because small teams
especially those who use agile require a
lot of ad hoc conversation that happens
in a day-to-day basis and so as they
realize that they have communication
gaps they started investing more and
more money into video conferencing tools
so they bought speaker phones and
cameras and whatnot but the problem was
not so much that they couldn't talk or
they couldn't get a threat of
information from one place to another
but that Ivan did not feel part of the
team because he was physically removed
from the day-to-day grind of what it
meant to be at that company so in
sitting at the in the chair decided one
day that he is going to put a tablet pc
on wheels and use skype to transport
video and audio and so what you see is
this robot that they call Ivan anywhere
this robot became so popular that it now
has its own website over at sybase and
people and there's been numerous
articles and and and and TV shows and
whatnot but what's important about Ivan
anywhere is that it allowed Ivan to have
a presence back at the home office so
now somebody can just turn around and
say hey Ivan by the way what did you do
about that blah blah blah and have a
conversation that easily nuanced
frictionless ubiquitous and that's the
power of telepresence a robotic
telepresence so now that we know we want
to make a robotic telepresence we have
to define some characteristics and the
first one of those characteristics is
height we decided to make the robot four
feet tall well if the robot is too low
to the ground you can't really hold the
natural conversation with it so that's
problematic but if you make it too tall
most people might get scared having
something five foot six walking around
the office right so we had to make this
compromise between being tall enough and
not being and not being too short and so
four feet tall seemed seems to be after
our research seems to be right around
the height that people can relate to
it's the size of a small baby the other
thing that we're interested in was to
make sure that it fits into the existing
IT infrastructure so
were very concerned that while people
tend to be a everybody likes to
experiment with these types of things we
wanted it to be applicable to the
largest pool of people possible and that
we didn't have to create any extra
infrastructure to support it so we used
a standard PC we use standard Windows
operating system how many people know of
Windows operating system anyone yeah a
couple you so it's my company up in
Seattle called Microsoft and they used
to be pretty popular with operating
systems and and and a lot of businesses
use them and there is IT departments
that understand this and so they can
just plug this robot into their IT
system because all it is is just a PC
just like the one you have in front of
you all it is it's a PC on wheels and if
you can get that pc on your network you
can get this robot on your network and
that's about all the IT work you need
and and of course Wi-Fi which is pretty
ubiquitous across organizations so if
you want to have bi-directional audio
and video you need a camera and a
display pretty self-explanatory but but
the quality of the camera and the
display are extremely key and I'll get
more into that in a bit you also need
some sensors you need to be able to make
some you need yourself in the robot to
need to be able to make some sense out
of the reality if they're about to
collide with something or if they do
collect with something how fast are they
going and in various different types of
sensors ports were important because we
wanted to make it open and we wanted
people to add things to it and of course
runtime this is another one of those
afterthoughts when it comes to robotics
it's really easy to make a robot it's
really hard to make a robot that's
useful now imagine that if you have a
telepresence robot that only works for
20 minutes that's not going to be very
useful it wouldn't even get you through
a meeting our robot works for about
eight to ten hours and if you think
about your laptop I don't know that many
mobile computing platforms that work for
that long and ours has wheels to and it
moves on its own so we have now some
physical characteristics of a robot now
what are the key features that a robotic
telepresence platform needs to have well
it is to have bi-directional audio/video
that's a given I need to be able to see
you and talk to you and vice versa the
other thing is I need to have a
bi-directional control loop so not only
do I need to be able to control the
robot in the remote location but I also
need to in real time know whether I'm
about to hit somebody because if there
is some delay it's not really useful to
me if I knew that oh I hit Joe two
seconds ago I need to be able to know in
real time and or be able to avoid it by
putting some algorithmic intelligence on
the robot the actual implementation is a
combination of both obviously the other
thing you need is mobility that's sort
of obvious you need to be able to move
it around but the other thing about
mobility is that it's not that is just
on wheels and you can move the robot but
it's also moving your point of view so
being able to pan tilt and zoom just as
if a human would and feel comfortable in
that remote location last one being
manipulation and manipulation can have a
lot of meaning but essentially will be
useful if we can do something in that in
that world that involves physical touch
so back in 2003 when we started this
company then I man I after a couple of
months this is the robot we came up with
his name was I oh done and about a year
later we came up with Milo Milo was
again the same concept pc on wheels we
learned quite a bit with that and then
we rubbed them up and this is still Milo
about a year after that in early two
thousand five which became the basis of
our platform now we played with this
platform for quite some time and then an
end of two thousand seven we now have
Tyler I'm going to show you a brief demo
are we ready for the demo of how Tyler
works
so this is a this is our robot his name
is Tyler and so as you can see there are
two interfaces there is there's audio
and video on the robot and there's audio
and video on the client side and and so
now people can now talk to each other if
I go over here I can say hello to all
you hello and so the idea being that we
can use the internet and a laptop and
move this robot in a remote location now
how do we do that there are several ways
to do it but i'll show you the quickest
way i like using a mouse just drag the
ball moves on its own axis goes forward
and all other points are basically
curves now you'll notice on on the
screen we have those green colored bars
those green color bars are colorful
representations of the proximity sensor
which are these sonar is here so if i
move closer to it in the middle you
should see it get dark which it doesn't
but it usually does and you also get
visual representations down here which
obviously it's not working but at any
rate okay this works so if you hit
something you'll notice that it turns
red and tells you that you bumped into
something and takes away your driving
ability so then when you get stuck we
normally don't want you to go backward
so we make you hold down the control key
you get the arrow and you can move it
back but imagine this would be the
equivalent of me doing this which is not
a very good idea I don't often walk
backwards because I don't have any
sensors behind me so that is the basics
of the robot i'm going to give you
another demo at
at the end kept coming with us thank you
so that's Tyler now drilling into the
system architecture we have some sensors
and actuators that connect to a
proprietary microcontroller board we get
day we get sensor information fed into
the microcontroller into the firmware
and up all the way through windows and
into the application and we get receive
commands from the application down into
the hardware and down into the actuators
the from the application standpoint
they're two main tasks that the
application has on the one hand it has
to provide bi-directional ID on video so
we capture we capture from the camera
and the microphone we encode it and we
shoot it into the network we get signal
from the network decode it and show it
render it on the display on the hardware
side we get commands from the network
send it down to the actuators receive
the sensor information back up to the
firmware and back into the network now
the actual audio video component we use
like I said we use Windows so we're
using the.net framework to do this
camera and microphone signals go into
DirectX get encoded using windows media
codecs and shoot out into the network
and the exact reverse happens so we get
the signal from the network decoded and
get Direct X to render it locally on the
client or end or the robot the two
pieces of the two pieces of the software
are virtually identical and if it
detects the camera and the
microcontroller board it assumes that
it's on the robot as opposed to the
client now one of the things that always
comes up in fact the third question that
always comes up is it safe and we've
been thinking about safety pretty much
since day one because if you think about
it you're moving something in a remote
location and so you there are many many
many things that can go wrong so you can
lose network connection windows as it's
sometimes no
to happen will crash and so what you
don't want to do is to have a 65 pound
robot run over a little baby and so
thinking about safety we had to address
this from multiple angles and we came up
basically with a four prong framework of
handling it at the most rudimentary
level we use the mechanical design of
the robot itself so it can tilt to about
45 degrees before it falls you can run
into this robot and I want tip over
because we kept the center of gravity
very very low we don't make a go too
fast we make sure that it goes kind of
slow so even if it does hit you it's not
really going to injure you and a button
and a number of other mechanical design
features that we put into it we also
have safety sensors so we can detect
collisions we can detect bumps and
essentially what this means is that
algorithmically we can use the sensor
information to avoid really bad things
from happening we also have a network
timeout feature this is a simple
heartbeat I expect a heartbeat from the
other side every so often if I don't
hear it the robot will shut itself down
we'll try to establish connection back
to the client and this also happens
internally with the hardware itself so
if for example let's say Windows crashes
the microcontroller board and even the
motor driver boards below that will shut
everything down and try to re-establish
communication with Windows and with the
application layer so the combination of
this has given us enough confidence to
put this into people's office and we're
actually getting some very interesting
feedback and everyone seems to be pretty
happy with the level of safety that with
that we've provided now where are we
going with all this first and foremost
were not like I said at a stage where
we're putting these into people's
offices and getting feedback we're very
interested to see how people are using
this where we're really creating value
for people where we can improve and and
one of the interesting things that's
come out of this is that we always
imagined that this was about the audio
and the video communication
and it turns out that that's not really
where the value is the value is presence
so meaning that if i happen to be sick
or or for whatever reason not at this
office i can still log into this robot
talk to each and every single one of you
as if i was actually here walking up to
your desk and having a very simple
nuanced conversation with you and so it
becomes about the presence not about the
video conferencing component it's the
ability to turn around and say hey Fred
how's it going by the way what did you
decide end this thing that's simple
without creating email threads and
without creating video conferencing
agendas and whatnot so going forward
there are three specific things that
we're going to put our effort into one
of those things is autonomy and semi
autonomy and let me tell you what that
means i'll give you some examples so
imagine that instead of me driving this
robot to Joe's office I can give it a
high level command and say hey go to
Joe's office and the next thing I know
Joe pops up on my screen and the robot
is there so it's not quite kids not
completely autonomous I give it I still
give it commands but I don't need to get
that much resolution into you know
actually driving the robot for example
we're docking I know that the battery is
low i can click a button in or go and
find its own docking station dock on the
autonomous level will be more like a
security bot so i know that every night
after 6pm these doors shut down the
robot has a predetermined path it would
start going through that path and if it
sees something that looks suspicious it
would start recording and reporting it
to somebody manipulation it's kind of an
obvious one suffice to say that I don't
know anyone including anybody in this
room who wouldn't love to have a robot
to get them a beer so manipulation is a
fantastic thing and the last thing is
community and openness this is a bit of
a tricky one for us because we have to
strike a fine balance between what is
proprietary and sort of company secret
sauce versus having this policy of being
open in the past we have always provided
SDKs and api's we will do so at the very
least
with Tyler as well but what is important
is that we want to go even further and
we want to invite people to come and
give us feedback and contribute to this
because we think that besides this
particular platform the technology
itself has tremendous value and people
can really contribute a great deal too
okay excellent question so the question
is how is the usability let me let me
say also add that the human field of
vision is about 120 degrees and the
camera is about 47 degrees so you have
tunnel vision to begin with but the idea
is that given that you have the camera
and that you have the sensors you get a
sense of the outside world so we're
about to drive this remotely in our
office so you can see that you kind of
have tunnel vision it's it's it's not as
good as actually being there but you can
also see that if you have good enough
bandwidth you can see pretty good video
beta testing 82
so how much would one cost the raw cost
is actually not that much it's on par
with let's say an expensive PC but you
have to understand that you know Dell
makes a few million of that pc and we're
going to make a few hundred so the end
user cost and our costs are obviously
two different things but we're trying to
be somewhere in the five to ten thousand
dollar range yes Scott yes so one thing
is that we have a huge battery and so we
try to get a lot of power out of that
but the other thing is the way we turn
things on and off and do some tiny
little clever things that gets us a lot
more power down the line also we can
generate power using the motors itself
without getting too technical about it
but you do have two electric motors that
you can get power back from so we get
about we get about eight to ten hours
depending on the configuration and that
means let's say in an eight hour setting
that means about two hours of full-on
drive time and six hours of standby time
go ahead
I'm sorry can you repeat that but Bionic
robots pioneer robot okay so so another
excellent question that this this brings
up a bigger topic most Japanese firms
have invested in various types of
robotics technology and from the
mechanical and electronic side they're
far ahead of the u.s. in this area now
they their approach to robotics is
mostly humanoid robotics so they're
interested in things I have two legs two
arms and we'll go inside to somebody's
home because of the specific elderly
population that they have there are
robots that are wheeled like this toyota
has one pioneer like you said has one
and a few others but none of those are
commercial robots we're actually one of
the very first companies that is doing
this on a commercial basis so you had a
question yes
sure so the question is are we confined
to just open spaces do we need special
doors to be able to go through those so
obviously if the door is closed the
robot is not able to open doors right
now but at the same time if you know
that you're going to talk to somebody
you're going to make certain
accommodations so you know if I know
that I'm going to be video conferencing
with my guys at work I generally don't
close my doors or it's understood that
they'll leave the door open because I'm
going to be coming in at some point kind
of like the same courtesy I would extend
to you if I if you were here in person
so if I didn't want to speak to you my
door would be closed and you would have
to send me an email and set up an
appointment but if my door is open that
means come in my office and talk to me
so we see that as a very similar thing
now that being said a lot of companies
have electronically controlled doors and
a robot with the proper security
credentials could just as easily get
into a electronically controlled or or
elevator for that matter okay anybody
else
sure go ahead
another excellent question so can we do
remote diagnosis there's two ways that
we do that one is just using a basic
desktop sharing application so if
there's something that we need to know
we can just log and remotely and do that
now that works as long as you're giving
it to your friend because they don't
mind you you know logging into their
network but if I gave one to google I
probably wouldn't be able to do that so
the way we do that is we are there is
something running in the background that
that if you choose to will send us some
information that would help us diagnose
things go ahead okay so the question is
what kind of hardware I rewinding can
you be more specific with regards to
hardware
okay so so so okay so this is just a
general discussion about the hardware as
far as lenses are concerned you can put
whatever kind of lens you want you can
put a lens that sees 360 degrees it's to
the extent that the distortion of the
image is acceptable to you so what we
find is that once you go beyond that 50
degree angle then your ability to judge
distance to objects becomes very much
distorted and it takes some training of
your brain to sort of get a feel for you
know where exactly things are not that
it's that much easier with this but it's
definitely easier it's more it's more
natural and intuitive with this kind of
angle the particular camera that we use
is a 1200 camera twelve hundred dollar
camera by canon it's got a 26 time
optical zoom it's extremely powerful and
you know it can pan and tilt and zoom
with three yards to the PC hardware we
run an intel core duo at two gigahertz
yeah 2.1 gigahertz intel core duo and
really the trade-off here is how much
battery are you burning versus how much
processing do you need and so we have a
lighter version where we run a cpu by
via it's a 2 gigahertz CPU but it's I
believe based on the pentium 4 code base
so it's not really as advanced and you
can't run all the different instruction
sets but it works fine we're able to get
upwards 24 frames a second over a nice
pipe with that so for the eight for this
adult presence application it works now
if you decide someday to put a facial
recognition application running
alongside or on top of our app you might
need some more processing you know we're
really we try to create it in a way that
we for things like computing to try to
stick to off-the-shelf components as
much as possible and that would then
allow people to customize this as the
need arises does that answer your
question
uh-huh yes yeah the balance of trade off
vs
sure so for fried we do have the
question is the trade-off of you know
doing the actual encoding and decoding
on board as opposed to offloading it to
a machine on the network so the first
issue that you have to remember is that
we have a real time component that we
can't really get around so putting a
proxy somewhere along the line is not
really going to work for us because if
we're about to fall down in a ditch we
need to know that as soon as possible so
that wasn't quite possible for us but
what is valid is how much of this can
you do in hardware and how much you need
do in software so for example our camera
is an analog output camera so the signal
comes out we go into a USB frame grabber
and we go in now at some point we're
going to go into hardware encoding as
opposed to software encoding that will
decrease our battery time that are
decreased our battery usage and probably
give us better encoding the trade-off is
that it will probably add a couple of
thousand dollars to the cost so as
things become cheaper and fall within
our range then it will start to make
sense for us to do some of those things
I want to address one other thing though
we do use windows media codecs it just
happened to be the one we're familiar
with and met our criteria there are many
great codecs out there and at some point
we will migrate away from any windows
specific codec and into something open
source or at least open so that then we
can move towards platform independence
the client should be and in the next two
or three years the client should be any
machine that you want it to be so it
should be it could be as light as a cell
phone it could be a macro could be a pc
or linux box we're trying to make it as
platform independent as possible
yes so those particular so NARS they
have I believe it's a 35 degree field of
view and they work for about two meters
they're blind for the first ten
centimeters I don't know if you know how
sonar is work but basically they have a
minimum distance where they're blind but
they do tell you that there's something
that I just can't tell you the distance
the reading seems to be accurate
somewhere between 20 centimeters to 1
meter that's where you get the most
accurate reading but you also have to
understand that we're not we're not
really that concerned with how many
centimeters is an object in front of me
I just want to know if I'm really close
to hitting it close to hitting it or I'm
okay to go you know so that's why the
colorful representations work there
needs to we're improving the algorithm
of how we take all this data and
represent it as a color and that keeps
improving but the idea being that
imagine that you have a 47 degree field
of view and you're about to go through a
door you're not going to be able to see
the door stills using your vision so
you're so nars have to be able to tell
you that you're clear to go and you
could you should be able to go through
this by just looking at the colors and
that can be accomplished now yes
how much
sure so so so the question is are you
tied to just one way of interacting with
it arise in many ways so the short
answer is that we are committed to
making this as easy for anybody to use
as possible so I like mouse most of my
guys play first-person shooter games and
so they're used to a certain keyboard
movement that's what they like you know
the point is that we're really agnostic
as to what the input method is as long
as you're concurrent with the data sets
that we need to move the robot we can
move it regardless of the input device
what I will tell you though is that
we're going to add Xbox joysticks and
not only are we going to add it on the
client and but we're going to add it on
the local robot and the reason for that
is that let's say that I wanted to give
you a tour of my office now currently
the way I would have to do that is I
would have to make you drive the robot
through my office problem is you've
never really been there you don't know
where you're going right but whereas
with this I can just grab the joystick I
would move the robot you can still
pan-tilt-zoom take pictures take video
as it normally does but I would be the
one giving you the tour of my office so
now remotely I can do that and so
obviously the next extension of that is
that I can do that one person too many
persons so shooting it out into some
kind of a proxy and then and then going
multi-point from there that's one of the
other areas that we're looking at but
the local joystick will be available in
our next version of the software and and
just to add something to what you asked
the where this this this this next
version of software will add some more
functionality but the main thing that I
wanted you guys to see is that the UI is
extremely crude we're just demonstrating
the very basic functionalities of the
robot and and as we get better at the UI
element it becomes a lot more useful
for you for people to to to use huh sure
so the sort of question is if you want
to make it autonomous what else would
you need to add so personally you can
never have enough sensors right so there
there are a few common sense things that
we even thought about adding now before
making it autonomous so for example one
thing was putting a second camera low so
and use that instead of sonar to detect
obstacles because not only can you see
with the human eye way you can run some
algorithms and see things that way to
sew along side with that obviously
stereo vision is always good because it
gives you a lot more information about
your world and plus you can extend your
field of view and one main safety thing
that we don't have yet is being able to
detect ground underneath you so if you
get to a cliff like stairs the robot
itself has to be able to detect that
there's nothing left for me I'm going to
fall down and stop we have we had that
implemented in Milo we haven't
implemented in Tyler we know how to do
it it's just it's just been a matter of
time so that's something that's going to
be added to now beyond that you know
autonomy is a very loaded word right you
know exactly what do we mean but in my
personal view at the very least before
you can make something really useful you
have to have stereo vision and and
enough processing either on board or
have an ability to do it off board where
you can do some things with vision
sure uh-huh okay so if we compare Tyler
to to rumba sure so rumba without the
camera runs for about 45 minutes right
and it's very low to the ground and it's
made out of plastic this guy can be
thrown can be pushed can be kicked it's
it's a pretty sturdy platform it's made
out of the same material that they make
airplanes with now that's not to say
that it can't be improved it can be but
we ship the sky we abuse them we've been
playing with them for upwards of three
years and they last inherently a Roomba
is a consumer product so they're not
really designed to last beyond 18 months
whereas this is an industrial product
and it's designed to you know last a
while and really Roomba is not a
platform you know that's made for this
application can you hack one and make a
do telepresence of course but was it
specifically made for robotic
telepresence which is what we're doing
and the answer is no yes
excellent question so localization and
mapping how do you do it on a robot this
question predates even computers right
and and so there are companies in fact
one out here called evolution robotics
that does just that they're out of
Pasadena if you guys are familiar with
ideal Abba there an idea lab company and
our friend Paula progeny and runs that
company we don't intend to do we don't
intend to create breakthrough technology
in an instant slam however there are
many different ways of doing it and some
of those are commercial which we would
take advantage of or perhaps you might
come up with one the point is that I
think every application would be
different so for example outdoors you
would do it differently than you would
do it indoors and and so depending on
the application we will implement the
best possible way I don't have any
specific methodology right now but one
thing that I can tell you is that it has
to come to what we will bring to the
table is not the slam technology but
making it easy for people to use so how
do you program it how do you tell it
that this is Joe's office and that you
this is where when I say go to Joe's
office this is where I want you to go
the ability to interact with it in a
very simple easy way for people to
understand that's the thing that we will
be bringing and not the actual
breakthrough technology in navigation
you're welcome any other questions ok
well thank you very much for your time I
really appreciate it</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>