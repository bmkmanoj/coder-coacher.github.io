<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning Workshop - Algorithms, Systems, &amp; Tools for Learning at Scale: NeuFlow... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning Workshop - Algorithms, Systems, &amp; Tools for Learning at Scale: NeuFlow... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning Workshop - Algorithms, Systems, &amp; Tools for Learning at Scale: NeuFlow...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KaJtT1K3GtI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright cool okay so the first part is
going to try to keep it short probably
because you have already heard this kind
of stuff and this is just kind of an
introductory segment about control nets
and the Clements going to talk about are
going fermentations and some
applications of that so so I think that
one of the big challenges for you know
AI machine learning our science etc is
too strong representations so it's the
whole you know the topic on several
workshops here of deep learning and one
of the things that we've been working on
really actively in my lab so rather than
just you know learning classifiers
longing the set of features
simultaneously with a classifier and you
know if we had good representations we
could use machine no need to do good
representations we would be able to you
know learn recognition task with just a
few examples and you know so it breaks
the sort of traditional view of pattern
recognition as kind of a succession of
sort of handcrafted people are seeing a
feature extraction followed by some sort
of 20 volt classifier which is the
standard model for the last 50 years
when I kind of replace this by something
that looks more like with the visual
cortex does which is sort of hierarchy
call you know couples or multiple stages
all of which are trainable these are
slides extracted from you know papers in
neuroscience computational neuroscience
and it's no evidence that those circuits
athletes for SIBO recognition are
essentially feed forward and so we can
emulate this with sort of a sequence of
modules that transform the low-level
features into slightly higher level
features or pixels into you know
low-level features into middle of
insurance high level features etc so for
image is going from pixels to edges to
text on parts of Jack scenes or you know
other types of modalities it could be a
move order or example the
you know Steve actors and cones and
ponies and things like that of course
we're going to make all of those things
troll which is you know the the topic of
when working on keep running so if you
look inside the busted what what
transform should be doing if the the
nature of the data that you are
processing is is that you can arrange it
in multi-dimensional arrays and you can
arrange it in multi-dimensional arrays
in such a virtual way that neighboring
components within the array are
somewhere highly correlated so things
are like pixels in an image you know you
know samples of the audio signal or or
or in time frequency representation or
you know where the coefficients to this
signal you want no frames with video so
in a video you have you know every pixel
is is highly dependent upon its
neighboring pixels in time and space and
so that means you have a local structure
that means you can represent any little
patch in your array by a feature vector
that will sort of capture the sort of
low dimensional structure of this patch
so if you can organize your data in a
raise yeah correlated arrays then the
the good thing to do probably is to do
local filtering to extract appropriate
teachers you apply linear filter let's
say to a neighborhood and you know
multiple linear filter is to kind of
represent the neighborhood with the
feature vector and then you go through
sort of various nonlinear
transformations of this and perhaps a
pooling of the responses of the filter
is over over area to to allow for
variation of the position of the other
features and building newsies of indians
so that's the basic idea of cultural
Nets it didn't work for a long time
going back to the 70s with the work of
fukushima and the ladies with my work at
bell labs and then you know more
recently a lot of people in working with
these kinds of architectures with papers
here they're not always call them
accomplish all night for this
idea so this sort of an interesting
aspect of this so solid let's go a
little bit about that let's go if you
can find here we go so Nora scientists
call this simple cells and complex cells
so there are sort of the simple sale to
things like the tag features and a
complex a blue things like aggregate the
response to the feature detectors were
small areas and the way you implement
this is by using a filter back so calm
additional filter is three-dimensional
in the case of images to be emotionally
kids video perhaps multiscale there is
some sort of nominal transformation here
which may be fought wise or not to be
pretty bold something like competition
winner take all or something that type
shrinking specification whatever a
cooling operation which is an average or
max or its simular symmetric operation
and okay i had some sort of contrast
normalization operation here on top and
that constitute a stage of a teacher
transform and can stack multiple stages
of those and learn the filter quick
Titian said even though you're using
various algorithms combination of cats
unsupervised running and supervised
learning so so this is based on the old
idea of essentially neuroscience work
from the 60s google and weasel were
prized when you work on the cash visual
cortex you know where each each each
unit responds to a small area in the
into the visual field and ESO replicated
units over the visual field that look at
multiple locations and and then the
response of those things i can pool and
integrate it to build illiterate
invariance yes you might have heard a
bit about this nicorette stuff that i
wasn't here so i can take they can tell
it's trying to find in space up so are
the commercial net architecture is sort
of a particular instance of this where
we sort of make the entire system
differentiable so we can use gradient
based on e train all the filters using
back prop and or other methods for that
matter and as well as has pretrial
assistance in the answer
learning lots of applications of this to
object recognition and object detection
and handwriting recognition in fact on
the show you fetal animation if I go to
the right side okay so this is kind of a
sort of old vintage neural net
commercial net the recognition and what
you can see is that the the high
dimensional representation expected
functional net from the input image is
sort of equivalent or robust to
irrelevant or affirmation of you put
security stability so here goes from
white to black to white as the digit
translate which means the many fold of
translated version of this key is very
highly nonlinear you know it's kind of a
is very very curly and there will be
other pro resurface falsified another
carissa surface for the eight and all
those curvy surf is a very sort of you
know entangled he put space but what
this thing does is disentangle the
surfaces so if you look at a
representation here you hardly see any
valuable going from black to white to
black or vice versa this maybe this guy
who's drinking around but also the other
guys just go in one direction and what
that means is that the the manifold of
translated versions of this three in the
output space is flatter and if you have
fat manifolds it's easy to separate them
using linear classifier so that's kind
of the advantage of those things once I
so that was it and so you know there's a
whole bunch of applications that we
built over the years in addition to
handwriting the things like face
detection where you know this works
pretty much as well as everything else
is actually quite a bit better and you
know road sign recognition so this was a
competition that was organized by a
German institution this past year and
the interesting thing is that there were
two 200 entries in this competition to
recognize what signs and in the
pre-final take off the 14 of 13 of the
top 14 entries will complete all Knights
607 from Kiev sorry ETA in Switzerland
juergen schmidhuber slab you know which
has also been working with commercial
Mets and the the other entry either the
top 14 was a human performance actually
number six so there's five commercial
Network better than human commercial
Nets so that was the pre-baked off and
then there was the kind of the final
result where the ETF people came up very
very closely just above us and because
they use or tricks which is great other
applications are commercial an edge to
image segmentation this is one by varun
Jane Sebastian song at MIT and this is
for segmenting kind of 3d volumetric
images of brains of brain slices
essentially and it works really well
congrats you sort of other techniques
for so it's most timely techniques for
for segmentation this you're going to
see some more work on segmentation by a
Clemont later you can also apply
commercial meds for sort of tempo signal
so this is an application to build
couple years ago a few years ago for
detecting epilepsy seizures from each
you know cranial EG so that uses what's
called a time-delayed neural net which
is really a temple commercial net and
there are sort of commercial
applications you know AT&amp;amp;T which I built
in the 90s together william go to
patrick have me on your show banjo and
bunch of other people Chris Burgess you
know people you see around here at nips
necs build a whole bunch of applications
that are pretty cool some of them are so
many wishes that recognize the user and
advertising posters also that look at
the user and figure out the gender and
age of the user and offer them different
products you know it rest any different
ways depending on their age and this is
prequel way all runs on the little box
which with a DSP circuit in it is very
cheap and convened in any machine Google
uses a son common chalmette for favor
suspect removal from studio images they
have kind of a first pass system that is
where I come from at energy use comes
from Athens more accurate detector
survives other applications in various
other companies so the interesting thing
about functional Nets is that the
mystery of approaches in computer vision
for object recognition that sort of
converge toward the similar architecture
as Commission let alone Young Voices are
very different it is very sour when you
think about safes or hog the first stage
of sit to hog is a basically photo/mike
on the RIT and future pooling and some
organization on top and the second stage
is it o.k be so sparse coding which you
can think of as helper manga
non-linearity and teach you putting on
cell and so you get this sort of basic
architecture that use absolutely
everywhere including that so mainstream
vision systems and so if you can do fast
convolutions you can basically do fast
vision because all the food systems use
that as the main computational element
and one thing will be working on as well
as sort of incorporate new
nonlinearities in the in the system and
that II cooperating unsupervised running
and not going to tell you exactly it's
based on kind of sparse 02 encoders turn
on essentially
about this detail and you know you get
nice filters when you do useless
unsupervised learning algorithms to keep
retrying a commercial net and you can
refine it using supervise the running
and that that seems to be what works
best for many applications in particular
i can get too late so this is the
convolutional version of it and it works
really well for things like pedestrian
detection so we had some result of the
last nips paper the last tips on using
this sort of correctional sparse coding
to do to preach right accomplished all
night to do pedestrian detection since
then we've we've got much we've gotten
much better results that basically
because the state of the art on the Erie
a data set and the service media let you
take a more show the video if it has
time and and well we also ate right now
oops I'm just going to skip ahead a
little bit and we're winning the so this
is the parish can detect I built by
Pierre 7a and and it pretty much detects
everybody who is saying it's not trying
on seated people so it doesn't detect
those but but it works quite well it's
not real time it's a kind of batch but
it works really well and with this I'm
gonna let clemontic over yeah can you
hear me ok so based on the end the
presentation of conventional networks
and these this range of models I'm gonna
jump right I'm going to change our
perspective and look at his mother's
from the hardware point of view so when
you look at conversational networks and
seal our models interesting thing but
acknowledges that the entrance
intrinsically power and the parallel
isn't that purple levels of the red the
architecture is extremely homogeneous
meaning that the only need a head full
of busy
we're also able to completely implement
and computer models the Internet's of
those matters the mother to speed power
and it must get extremely efficient in
time I one other thing that's really
interesting about convolutional net much
is the fact that they can accommodate
extremely no numeric precision because
you can take this into account during
training so the looking at the plane
again from the dog specular computations
something interesting reddit is the fact
that you know sort of setup district
distributed set of extremely simple
compute nodes that don't really need
synchronization and that don't really
need to do flow confluence
microcontroller I mean instruction
decoding instruction fetching and
instruction caching which current beauty
car architecture McGee to use and recent
Intel Architecture billion so we
developed this hardware architecture
this is completely custom a couple of
barrio couple of years ago so it's a
true dataflow processor by del apto I
mean that you don't you basically don't
need any flow control in of this period
set of processing elements so we so
that's the main characteristic so we
have this tree developer position units
each unit is completely independent and
doesn't require any any explicit
instructions one second characteristic
that will make the system is the fact
that the greed can be very efficiently
we configure on at one time to
accumulate a large set of digital
algorithms so here's a pictorial view
that that shows the idea this
architecture there you have degrees
compute elements here you get a set of
centralized CPU that we use to basically
we configure the grid at one time so
again compared to a regular classical
beauty processor system each grid is
configured want to compute a particular
operation what it's configured it is
going to be carrying the same operation
over I don't know depending on the
incoming data see the interesting thing
with that sort of true data flow
paradise is that one of the greatest
want to just to do something you get an
option of table to put at each level of
the data for free basically so now the
tricky part about this is that it is
extremely natural to program because you
don't have a sort of classical you don't
have classical flow control so you can't
write you know typical C code program
this system and so the way we program
this is tell if I basically describing
all our algorithms in terms of
high-level graphs and and we work on
these graphs that you need anything so
we developed this compiler that we map
arbitrary graphs describing those
algorithms into byte code that
reprogramming our grid so I'm going to
be scrubbing this algorithm in type
steps it studies trade for one we have
an input description of the algorithm
which can really be any language right
now for 10 is a 12-7 so there is that
you know this description is pretty much
a graph it can have an arbitrary
structure was where this graph we're
basically gonna extract all the data
nodes and compute nodes in the entire
algorithm and an open a description of
that kind and that description we get a
parakeet divide the complete algorithm
into sub algorithms and sub graphs with
a simple constraint that each of these
sub class
has to basically fits the other the
rotor speed we have right now is this
very sizable handout 135 accumulate
units that are sort of compositional and
typically right now you have four
conditions with em sighs calentar el so
you're going to have to basically happy
device programmed to subtract that
exactly in the grid what's you have
division so we're really good map delgo
a turn into a set of tires that are
available and include all the back
pasture are necessary to beta keys
throughout the data each note to each
other node and a little bit of physics
are external ddr so once we have this
part of the other medical inc that we
provide input data to the grid the grid
is going to automatically compute the
exact subgraph sub brothers will not do
it let's see and so and so once we have
each of these sub graph not to the grid
the second part is to be sequence the
reconfiguration I guess then we got a
string so degree so that's surely a very
simple way then then what were the
sequence of reconfigurations we can do
all sorts of optimization by trying to
pack the paralyzed the eb configuration
of the breed we the actual stripping of
data and do something like that the end
goal being that you want to have beta
constantly streaming on the grid and
lexical cook with it so right now the
system is not general purpose that is
not that far so of course it next flow
control issue these on the beach and you
can evaluate and we can be
Shelley's can evacuate them that
delicate passive and and opt that
data-driven we have 20 conversation to
the combinations outsourced put in town
by town ition division so the son of
classical but you do need in your
networks accomplish your networks things
like fine why is not another thing to do
RP to I despise than you know transforms
in terms of performance those two
currents so i think that's that's
similar to what nigga was showing this
morning the fact that you can get about
10 so right now we have an
interpretation that gives us about 12 PP
operations per second or these kinds of
completions a regular Intel Folker
system now in terms of GPU so our GPA
penetration is not optimized to death
and that's the meta generated so it's
sort of expert in the food capacity of
the GPU but it's still very decent and
here we are basically showing what we're
getting with our guaranteed imitation
that works another pgs and here was
showing an extrapolation of what we
would get we had a money in the capacity
to educate public at this architecture
in a in an actual for 45 nanometer
technology so amazing edison i think the
number that's interesting for us here is
the ratio of media operations per second
so what function so we are never going
to really be able to pick for nvidia is
doing right now in terms of puppy power
but i think we have a great advantages
in terms of power consumption and
especially for that kind of AC basically
where we get a an advantage of you know
to all of the negativity
okay so i'm going to jump into two
applications that are can exploit that
architecture a very efficient way the
first thing is cheap passing that's kept
seeing transcripts imparting is
basically involves neighboring each
piece set in a given image in one of n
levels it's a challenging task instead
with files settings develop segmentation
telianet the nation and division at the
same time we sent the paper recently
where we proposing inverters that that
basically does that it's a teacher right
nughty scared convolutional network the
basic presents each each location in the
input image player each of the script
now the feature vector physically
describes a local texture as well as the
context of the local password yet so
it's an extreme it's a very robust
representation of the fifty cents this
is the new seascape position network and
then para we compute a hierarchical
segmentation which basically contains
multiple level of levels of observation
for each pixel in the input image and
then we have a mechanism to basically
automatically retrieve the best
observation level for each pixel in the
input based on the UT scale d jobs so
what gives us is a is actually a parable
of free segmentation method that's
driven by the classification test
say so ginger some interesting parts
about this method is that we're
basically looking at each each the
components that we are playing in the
end don't need to be disjoint that's a
relative if you have hierarchical levels
like you know a window within a building
and we'd with your car you'll be able to
retrieve all these levels hierarchical
manner so we specified much cheaper
better sets the start for micrometer
says one data set that has been quite
used recently it's a night class problem
and we're basically getting stead of the
arts and both pixel wide at USC and for
class accuracy so people usually only
way past the pics LOL accuracy and we
think that the tough test at us is more
meaningful because you usually get many
more pixels of know yeah its buildings
and basically some classes up let me
touching the scenes and what we're
really interested in is the actual
object we commissioned were you know
whatever their size is so now of course
the interesting thing about our mother
is that ever neat software the implants
time is extremely efficient it's about a
second on a regular laughter so we're
also getting set up your results in both
those metrics and more challenging
datasets 33 class problem which is going
to sit flow data set and 170 class
problem which is called the faster than
that dataset so i'm going to show you so
here's a couple of examples of how it
looks and now one thing that's
interesting is that we both don't recall
it some arbitrary videos in the streets
of new york and the mother generalized
is quite well so this is the the network
that was trained on the 8th class
problem the classes were objects and
then trees buildings guy and world so
now because the bulk of the computations
of that model is like a dense population
of network with a naturally map and
compile is network for hardware and
we're basically getting a speed-up of
about ten times and
so we used yeah so the actual speed up
we obtained on the condition it's much
larger than that it's about it's close
to 150 times but the overall speed up is
less because you know the other head of
communications and so on those are set
of more examples and different streets
and different conditions so just
complete with another interesting
application that the model is Kirk here
our lab has been working on it sort of
showed the approach we have for digital
problems here we basically China
wearability said congressional network
to approximate a ground truth that you
go pro predicted IC use method so it's
better be eyes and hierarchical NRF and
used to scare em away so his method is
basically given us extremely nice and
most results but the problem is a
different name is is non-negligible when
we're trying to do here is to basically
use the office for rhythm and to predict
the same thing dance convolutional net
worth and basically allows us to run the
same model and obtain see our wizards in
a long time so one strain the network is
able to predict this same optical flow
with the accuracies of course not as
good but it's something that you can
embed in in you know constraint systems
and environments yes I think I come up
pretty much all I qualities
now the results so the entire entire
hockey texture is completely dependent
from fpgas right now we emulate the
architecture in apj because it's on the
end but the entire up tomm where that
allows us to reconfigure agree this out
the reason we can't use what's provided
by apts is that we configurations in
fpgas or sequential so we configure
reconfiguring even a subset of the brief
text in the order of milliseconds that
seconds in our case we reconfigure the
grill with the network and ship it
broadcast because integration packets to
the whole bit and that basically allows
us to be configured a complete with in
in almost nanoseconds so really like we
conf you in the complete with texting
the other 200 to 500 milliseconds
there yes yes exactly so all the sort of
GSP times that so I forget to mention
that the sides are not homogeneous we
had different types of times and the
type that we use to do GSP so
combination operations use local fibers
to basically be able to star lines and
pieces of energy study and we have
another version as we're not really
using that uses much more distributed
memory in each tile and that you can use
in sort of more which way memory but yet
the idea is that we don't want to do
with champ we don't have the capability
to really use these memory as arbitrary
this is a memory so the way we want to
using these memories is by reprogramming
the time it's basically right thing
that's a certain location and we did
like primarily on location so that it is
purely director was the greatest of the
earth
Sudanese politically
very much more easier so the CPU is
basically useless if not the computation
the only thing the CPU does is
broadcasting configuration packets
agreed we configure the grid to do
multiple things so basically if you had
an algorithm that was Daniel have to
completely eat in and agreed you could
throw that away exactly the sequence
they can keep the whole idea of this
estate is the interest rate again we
really buy an external memory and then
maybe we have inside we have a dollar
mega that I yesterday the audience I
with another very Republican our own
ship will already be able to power and
two or formerly lights and so there is
to really be the system in the velvet
Maximizer something that we do optimally
intensity is given a certain amount of
bandwidth external ddr3 are comparable
inner edge of the whole sequence is a
form of data stream so that the velvet
is for that we used to the maximum
yeah that's very important
so what we have an attitude and a gtx
party we have to it's pure food a co-ed
that's what it is young embedded
efficient
to do the web
Oh
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>