<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Learning Semantics Workshop: How to Recognize Everything | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Learning Semantics Workshop: How to Recognize Everything - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Learning Semantics Workshop: How to Recognize Everything</b></h2><h5 class="post__date">2012-02-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5yqhE-NZlV4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">nice purse okay thanks Victor thanks
everybody for coming um so I start
actually yeah so um so I'm gonna start
by talking about that mission is usually
defined to division it's usually nothing
about recognition we think about
assigning some image to some object
category so for example if we wanted to
build a dog attack there is something
that can detect on the way that we would
do it is we would gather set of training
examples of set of examples of dogs and
a set of examples of things that are not
dogs we choose some visual features and
then train a classifier that can be
French it between the dog examples and
the NOC dog examples based on a visual
representation and so and so from this
the object representation that the
computer system has is that there are
few things in the world that are dogs
and there's a whole lot of things in the
world
not dogs then if we want to take this
representation and find dog's natural
images the way that we would do it is we
searched over different positions and
scales and extract features at each
window and then cook each one of these
touches the template model would say is
this a dollar that's not a dog's if not
better none of it in like ways if we
want to be able to recognize more
different kinds of objects we would
pretend in the same process just
training new detectors and running them
all on this image and then um so this
this basic framework started with face
detection on around 1995's that sort of
better when a statistical template
measures started to work early about and
then eventually people wanted to draw in
this this object recognition capacity
and the way that we brought that in
general is by making more categories I'm
saying went from frontal faces to things
like cars or pedestrians multiple views
and then the Pascal challenge came where
you started with four object categories
in 2005 then expanded at ten object
categories in 2006 720 categories and
then if you look at the task of image
categorization we have some picture of
an object and you want to assign it to
one of Queen categories we started out
with something like Caltech for where
you trying to differentiate between some
objects like your
plain and I ran a background image then
10 12 56 and finally 1000 and so that
the implication of this trend of
progress is that if we could only get
enough categories then maybe we would
finally have a complete recognition
system and maybe if we had the 30,000
categories that they demand says the
complete set of recognizable categories
then we would be able to recognize
everything but see but when we go about
our daily tasks we don't really do
recognition that way most of the time
we're not searching for a particular
object that we have in mind we're not
searching through collections of images
or clutching the visual data for objects
or for classifying into one of two
categories instead we have to deal with
the world as it comes to us and that's
important for a lot of applications
where you put a machine out into the
world so that's one example imagine if
you attach one of these category based
recognition systems to a weighted
vehicle so let's say we've got some
vehicle that has some detection gruesome
in it it's got all kinds of different
detectors the chapters for cars and
bicycles motorbikes cats dogs sheep so
the things driving down the road and
running all of these detectors and then
all of a sudden this thing walks out in
front of it so it's running it it's cat
and dog and sheep and bicycle and carb
detectors but the designers forgot
llano county sector who so the things
going down the road and then the last
thing that passes through its electronic
mind before the vehicle troubles into
this very large non content is not a
sheep a very satisfactory solution they
feel appreciative and we all know even
if you have never seen this thing before
in your life you would be able to
respond appropriately and if time it
doesn't really matter if he can
recognize it as a cow what you would
want to know that it it's going to do
major damage to the vehicle unless you
hit the brakes or swear it or something
and you want to know that if it's some
kind of animal that that's moving
towards the left so you know how to
respond appropriately so we want we want
recognition systems that are not just
searching to an image collection for
some target but that can deal with the
world if the world comes to it and it's
very hard to predict what the computer
is going to encounter so so it might not
be as how it might be a flock of sheep
or a private lines where a bunch of
other man-made objects there's a whole
ton of different things that you could
experience and you just have to be able
to deal with it so there were example
amending of a automated security system
and so these guys grace into the stores
carry jacana you're hoping I'm going to
think to go in a controller
a system and yet hopefully you would
realize that this is not a commonplace
event and it's something that you should
you should sound an alarm or notify
security or something so um so the
question that I'm interested in is the
question of how we can recognize
anything and that's a different question
than honey recognized one hundred or a
thousand or ten thousand categories
because it requires that we think about
objects in a different way that we think
about objects in a way that allows us to
generalize to two new categories the new
things that are not in our current
vocabulary or training set and so for
another way to express this is just how
do we get rid of the non mob yet how do
we think about recognition in a way so
that nothing is completely unknown so
that so that there's no voy nothing is
nothing causes a complete failure of the
recognition system so so one of the key
challenges is one of his one of
representation the question is how does
a new object relate to new Naja so this
is sort of the semantics problem how do
you create a relations between the
different objects that you've observed
in between the objects that you've
observed and the new kinds of objects
what kind of relations do you have so
the categorization way of doing this is
that you would sort these different
images into different categories you'd
have like cats and dogs of courses and
so on and that tells you something if
you see a cat or dog or horse at least
you know it's similar to each other
these other objects and if you have some
information in touch the categories then
you can see something about those
objects but if you get some new object
it doesn't fit into those kind
then you have a complete loss so for my
view like this categorization is kind of
a lossy quantization of a continuous
space of natural objects and the big
problem is that if you have a London
number of categories then you have some
complete holes in that representation
there's gaps we just have no ability to
deal with those objects at all so
another challenge which is relating the
related but just framed in a Moore
machine learning a view it's how we
learn about many related objects so
intuitively if we're trying to learn
about Palace and we already know a lot
about horses and dogs it should be a lot
easier and but the key question is what
do we share among these different kinds
of object categories what is it that I
dog because in common with the cow that
should allow us to more easily learn
about cows given our knowledge of dogs
and for this I think the key is really
to think about the shared representation
not so much the transfer learning or
multitask via which which sort of
implies that that the answer is in the
regularization the commutes figuring out
the right way to kill these different
kinds of effects so I'm going to talk
about a couple of our efforts in this in
this area and I will say that these are
Morgan next
attempt to understand the problem than a
final solution and then we're going to
talk about just a couple of what i see
is open challenges so the first is this
is that attributes RFA's representation
where we want to try to describe an
object in terms of its properties
instead of just categorizing it and i
want to date them just one more example
so imagine that you're on vacation
somewhere you eat a burrito you couple
up the rapper you only don't short out
so you walk over there a garbage can and
then this thing spells us so my guess is
that most of us probably wouldn't
identify this very precisely at least
not immediately and yet most of us would
probably not proceed to go into Gotham
throughout the rappers the reason is
because we even though we have no idea
what it is we can tell that it's not
something that we that we want to put
our hand near it's got a huge claws it's
got a hard body looks really scared and
so so this motivated the attributes
worked in a couple of ways I wanted that
it's really important to deal with the
unfamiliar objects so even if you can't
identify it you still want to respond to
it you don't want to just ignore its
presence it's not just somebody else's
problem you have to you have to know
something about it and also often what
we care about is not so much the
category but the properties of the
object and so the idea of the attributes
is just to try to get those companies
directly it's a very straightforward and
simple idea so I'm so we so we did that
so the so the idea is that we're going
to describe these different kinds of
objects according to their shape and
parts and materials and we're going to
try to learn to predict those properties
in a way that will do
life's new kinds of projects so for
example this this airplane is described
as having the shape of a horizontal
cylinder has wind color window wheel is
made of metal of blush the car has a
window wield your headlights I here and
it's made of metal and it's shiny so
there's these different properties that
describe the objects and some of the
properties are shared among objects in
different categories so this allows you
to relate something new to something
that you've seen before so the winning
problem that we have now is that we have
a set of these objects images that are
annotated with these binary attributes
so something has a shape of a horizontal
cylinder or it doesn't say in a metal or
it's not and for these part descriptions
they're they're indicating the parts
that you can actually see so we're not
necessarily trying to infer but
invisible here just slips what's
physically present I'm let me want to
learn some model that will allow us to
take some new image like this one and
predict the attributes for that vodka so
we want to say that this thing as a
foreigner headed and I and so on in that
Spurs gonna be able to provide some
description of these these novel objects
and so the first thing that we tried is
just the most straightforward
classification task so we we take all
the positive for HIV separately we take
all the positive examples of things that
have that attribute all the negative
examples things that don't have to
attribute create some standard features
color texture can scramble gradient
kenny edges and then train a classifier
that differentiates between the objects
has a have the attribute and the ones
that don't know this has this has a
problem actually the problem is that
these attributes are strongly correlated
to the training categories so if we take
the head wheel attribute for example and
we've learned it from bicycles and
airplanes and cars and animals say which
don't have wheels then there's no way
for the system to figure out what a real
actually is it's going to correlate
instead of learning this has wheel it
might learn features that have to do
with made of metal or other things that
these categories have that most of the
other categories that don't have wheels
don't have so there's this problem of
correlation to the categories and then
this causes if you're a problem when you
try to make predictions for new objects
because if you see something else that
doesn't have a lot of these same
properties of the categories that you
learn from but still has this attribute
has wheeled and it's going to
incorrectly assignment attribute so on
so the way that we do with this is I
instead of just learning a classifier to
differentiate between all things that
have been attributed the things that
don't we train classifiers at
differentiate within each category so
fur has wheel we would say train select
features that can differentiate between
airplanes where you can see the wheel in
airplane where you can't see the wheel
or cars where you can see the wheel in
cars where you can't see the wheel and
then pull those features together and
use only those features to to classify a
particular attribute and so that leads
to much more robust prediction of
attributes without getting tangled up as
much in the correlations with in the
training categories so um so for this
approach our experiment our setup is
that we retrain about 60 attribute
classifiers describing the parts the
shape material on 20 object classes and
then we want to test to see how we can
better predict those attributes for
those same classes and familiar objects
but Morgan
way to see it hello we can predict those
attributes for new kinds of categories
that might seem during training and
those new categories are images of
objects that were downloaded through a
different process and they're different
categories and then there's there's a
bunch of question so then we want to see
what kind of ability is this attribute
based representation gives us so this
year is showing some examples of the
attribute predictions for objects from
novel categories it's never during
training I didn't see any buildings or
carriages or zebras I mean but it's all
things that have some attributes in
common so um so it thinks that this
building is a has a pretty bossy shape
it's also things as a vertical
cylindrical shape has parts window
mobile windows things as a headlight
sees this carriage and it thinks it has
a kind of a Trudy boxy shape and also a
rounded shape that it has a window a
wheel and a torso sees this the zebra on
it and says it has a tail us now or legs
it also thinks as text which is probably
because it's black and white and it
thinks it's made of plastic which is
probably because it's just kind of shiny
so so it's certainly not a perfect
description of these objects but it's
able to provide some description work
with a with a purely category based view
you'd be able to say nothing or you'd be
able to say something wrong and so just
quantitatively we looked at their compel
where we can predict these attributes on
average and for different kinds of
boots and how it compares for the
familiar objects which are the
categories used for training and the
unfamiliar objects and so chance here
would be about point five so we're able
to predict the attributes much rather
than chance parts are easier than
materials in shape and there's some drop
off as you'd expect going from the
familiar objects to the unfamiliar
objects but it's still still well above
chance the drop off is not is not that
great so another thing we're interested
in and you can tell what's unusual about
some objects so we can learn from the
training set what attributes you would
expect from a particular category and
then try to see and testing if we detect
some attribute that was unexpected given
the category label so these are these
are some examples the computer says that
this is the sheets that doesn't have any
wool this is a boat that doesn't have
any sale this is a sofa that as a wheel
this is a bike that hasn't worn so some
of them are not actually some time are
correct like these examples some of them
are not actually correct but they still
tell you something additional about what
I've seen so this sofa of course doesn't
have a wheel but its colors kind of look
real like this bike doesn't have a horn
but it has a kind of corn shaped can't
handle bar so they're telling me
something so this description is telling
you something about the visual companies
of those objects and then we're also
interested in whether we could recognize
things based on a description um so in
this case in that this is a qualitative
example
up here so this is the query it's the
somebody's trying to give a description
of a monkey something that has an arm
has faced but it's free has a bunch of
other parts and then these are these are
the top retrieved examples broken down
into the true positives and false
positives and join this in this case the
data set just has 12 different
categories monkeys are wearing the
categories and then you can see so
there's so far as much of monkeys the
false positives oh also mostly fit that
description there's a pin next to a
donkey like a monkey looking statue a
couple of centaurs which which also sort
of fit the description so so people to
be able to at least like find some
things that fit that description and
then this is just showing that the
feature selection that I talked about
when you try to 2d correlate within the
categories next big difference for this
kind of test and it also makes a really
good difference for predicting the
unusual properties of objects so a
chance here would be at eight percent
there's call different categories
without the feature selection it's about
25 year cited in women is about
thirty-three percent accuracy in terms
of the top retrieved images so um so
from this work a couple of conclusions
is that mainly the naturally prediction
gives us more flexible recognition
systems it gives us the building to deal
with unfamiliar objects to some extent
it gives us the ability to provide more
information about the familiar objects
and to potentially learn from
description and I when you mentioned
that there's a lot of other related
works and attribute some of my
having at the same time and some which
was subsequent one thing to highlight is
that there's some recent work by perky
and grumman were they instead of just
stealing the finite attributes they
described objects in terms of relative
attributes like a tennis ball is more
creative than a baseball but less free
than a cup and these relative magicmy
lead to much more accurate descriptions
they show and they also be to better 0
shot learning so there's a lot of
interesting stuff in this in this domain
so for our own part like one of the
things that we were that we weren't too
satisfied with is that we're just
labeling everything at the image level
we have these binary attributes at the
image levels we're not taking into
account the correlational structures
among the different attributes what are
the interesting spatial relations for
things like parts and shape and so on
the other thing is that with the work
that i just presented you're given some
picture of an object and you're trying
to assign the attributes to that picture
of the object button natural images you
first need to be able to find the object
unless you can find the object you can't
really assigned any hot foods to it and
so the next thing that I'll tell you
about tries to deal with those two
problems we try to learn sharing models
of the appearance and spatial layouts of
Park of objects so that we can find and
describe novel objects in in images so
um so when we started work on this we
realize that there is kind of that we
need to pluck some new data to adjust
this problem most of the data at mission
has been focused on a
sation test and in part we thought we
felt like with the attitudes were having
just these image level labels made it
really hard to learn the correct
meanings of attributes to correctly map
between the visual features and the true
and and the true labels in every bus way
and so we decide just to provide a lot
of annotation so we created this data
set where we have a whole bunch of
animals and vehicles and we have
segmentations for the objects into the
parts and category labels and a bunch of
different attitude labels that describe
the pose and context and other
properties of these particular objects
and in this data set is available online
and we're continually growing it so
right now the version that's available
has has a lot of animals and vehicles
but we're also collecting furniture and
hand tools and all kinds of other
objects so the trust for this data set
is that you get to see some of these
object categories during training but
then there's other object categories
that you'll only see during testing so
you'll see something some some so you'll
get to learn about some animals and
vehicles and the training set when
interesting you'll experience all the
different animals and vehicles that are
in the data set and you need to make
predictions about all of them so they're
the problem the problem set up here is
that we have some training data where we
have
share some objects and we have like
bounding boxes or segmentation of the
object locations and the part locations
and then there's some kind of horrible
attributes that describe the pose and
other and other properties and then the
goal is if we take an input image like
this and that will have some model that
will be able to describe the objects
within the known domains which in this
case are the animals and the vehicles
and if it's some object that we're
seeing if it's a particular category
that seemed during training like cars
and the training set then we want to
know that it's a car but if it's
something that's not in their training
set like cows are not in the training
set then it's not going to be able to
say that it's a cow but we wanted to
know that it's some kind of four legged
animal and find the parts and predict
that it's walking towards left which
would be useful information for taking
action and so the way that our
representation is organized in this case
is that we have groups of categories
that we see related like animals and the
idea is that within these related groups
of categories it's meaningful to share
attributes to share parts actions and
other things within those objects but
you're not there examples going to
benefit much from your knowledge of
tables if you try to learn about cups so
across the cross these domains there's
there's no interaction isn't learning
I'm so in this case are a representation
of the office is we have some set of
detectors for a basic category
elk or pack or dog four broad categories
like for like an animal or water animal
or or land vehicle and in parts like for
animals the left torso and head so these
broader categories like four leg broader
detectives like four legged animal are
supposed to are trained to be able to
detect any of the four-legged animals
and the data set the parts are trained
to be able to attack the heads or legs
of any animal that's in the training set
and then the basic detectors are
supposed to be more tuned towards that
particular object category so um so we
learn these detectors and then we also
learn spatial models that are shared
among the different kinds of objects on
to encode the relative positions of the
objects in their parts so in this
representation we have we have shared
representations within a domain like
animal or vehicle and in particular
we're sharing the appearance models for
the parts and the broad categories and
sharing the spatial layout models the
idea is that if you have like a dog
facing the right and our couch facing
the right you can share quite a lot
among the appearance models of their
parks and their spatial layoffs so the
initial approach we talk is just a
training separate detectors we trained a
separate leg and head
and we're like an animal detector and
then we want we run those detectors over
the images we also train a spatial model
that allows those detectors to predict
the whole object location so so in this
case we get a bunch of detections this
is simplified they really be a lot of a
lot of potential detections at this
point when we get a bunch of intentions
for possible locations of the part and
parts and objects and then these vote on
the location of the lp objects as a
whole so it's a I think there's some
animal here and then we're giving the
different part detections in their
locations and given the object detection
all these different detections vote on
what the attributes of this particular
object is no harm so unsettled so we
want to say if this is a dog more like
an animal it hasn't always predict the
hearts say that it flying down facing
the camera and it'll jump and so on so
on so this is so so that's actually a
pretty simple approach but but it
provides much more detail descriptive
ability to the recognition system so the
animal did get to see elephants during
training so I can identify some of these
as elephants this is these solid boxes
are the object locations the text so
that puts here or indicating what
detectors made it think that there was
some animal here so it can find the
trunk the legs some of the parts of
these objects and then it predicts the
attributes it's just shown for this
elephant on the left um so I can project
that this is
vigorous and it's facing towards the
camera and that's based on a combination
of the partly oh that's detective and
the detective categories but more
importantly it could also make I can
also then say something about unfamiliar
objects so the computer didn't see any
characters or horses in training but
it's able to say a lot about them so it
can find this carriage it doesn't know
what a carriage is but it can say it's
some kind of vehicle and multiplies the
wheel and say that it's something that
moves on the road it doesn't know in a
horse's but it says it's some kind of
four-legged it before like a mammal it's
got a head it's got Frank the head if
I'm the leg and it predicts that it's
something that can run and jump and that
it's facing the right certain people to
provide a lot of descriptive ability
despite not having this these particular
categories within its representation and
then this quantity we show that these
having me shared models where you learn
about the parts and the broad categories
and you learn across these different
across these different object categories
and training leads to better detection
performance and it also leads to better
prediction of the attributes so this is
Rick Salinas so this suicide this is if
we just train basic category detectors
and we pull those basic category
detectors like dog and sheep and
whatever seen during training in order
to try to test all the animals in your
data set this is for the million objects
and this for the unfamiliar objects and
this is for vehicles so if your blood
familiar
objects learning needs more detailed
representations helps if you just trying
to detect the objects and it also helps
if you're trying to predict the
attributes so the the green green boys
are you just trying to infer the
attributes from detective basic
categories and the red bars are if you
try to infer it from all the different
part in category detections and
especially with pose as you would expect
the things like detecting the parts are
quite important we can't really infer
the pose from from a category level
detection so when one downside of mr.
cook so far that we just independently
train these different detectors in one
problem with the is a lot of things are
difficult to detect in isolation like a
lot of the parts this is showing for
example that in order to correctly
detect the head here which is the fifth
most confident detection in the image
you have to get a whole bunch of false
positives and other things that mainly
what kind of round like different spots
on the dog or its nose and so these part
of detectors but in cells are very weak
in one consequence of that is that the
prior detectors don't play much of a
role in the end up in the task of look
like
the objects it's mainly the whole value
object detectors which are more reliable
which help you which provide some
context for the parts so the idea though
is that is that if we train if we
jointly train the different appearance
detectors in the spatial models then we
can then during the learning process we
can take advantage of the context
provide by the other detectors so if
you're trying to learn to recognize
heads or less it's actually a pretty bad
idea just to try to differentiate
between a peasant legs and all different
patches in any image because there's
lots of different leg like structures
and images like like branches in a tree
might look like a leg but they're not
around other things that look like an
animal um so so when we learn these
detectors we want to learn them all
learn the detectors that work well in
the context of the other detectors and
then we also are jointly learning a
spatial model across all the different
related objects and we close this as a
late instruction us again so I don't
want to go into the details of the model
but it's actually turns out to be a
fairly complicated thing that optimized
so um so just one more thing that most
silly so the way that
is that we will provide a set of objects
some different categories with the
locations of the parts and then we
represent a spatial model as a mixture
of different spatial layout so you might
have one facial layout that sort of
represents the side view of standing
animals another one that represents
animals that are lying down and facing
the camera they all have these labels
attached to them but they're based on
lust and they're based on clustering and
then optimizing the layouts with the
learning of the appearance parameters
and then each of the appearance models
also has a mixture is a mixture model so
you might end up if different animals
might have sort of different looking
parts or as you change the pose of the
animal or the objects the appearance of
the park can change and so so there's a
mixture of appearance models and
mixtures spatial layout models and then
all these are trained during so i'll
show you a couple of results in this
this is showing um just one component of
learning station models so this is a
this is a model of spatial layout of an
animal facing left the enemies are a
couple of their detection so the yellow
circles or head blue circles are tourism
and the red lines indicate the positions
of detective likes and so the awkward in
the train there were other elk in the
training set so I was able to correctly
localize the parts of the elephant
detective and the cows are not in the
training set and yet it's still able to
localize the head in the torso and
predict some of the leg positions and
then there's a couple more examples here
so these are objects categories that are
seen during training so it's able
predict their more precise category
labels and in most cases localize their
parts fairly well although there's some
mistakes and then these are examples of
objects that are not seeing during the
training process they're not it doesn't
see any cows or attacks but it can
detect this path and sort of mine down
in the patient gray and it can it can
mobilize the head this yellow circle and
the torso confused things that this cow
is none of it but it's still still able
to find the parts and then this cow
doesn't know what it doesn't try to
project what the basic categories it
just says some four-legged animal and it
sort of does assess their job in
localizing its parts so so one thing
that we found from the slider experiment
is that that the joint training of a few
captain and spatial models is actually
really crucial these blue bars are
showing the results that you get if you
just individually trained the detectors
and run them the red is if you add some
spatial model on top of that and then
the green is if you join me train
everything together and it's kind of
unfortunate because it would be really
nice if we could take the recognition
problem and just split it into these
different modules of things that we want
to predict and then separately train
those modules and stick them together
but but you won't actually take a pretty
big performance if you do that so at
least
to some extent you really need to learn
all of these different related
prediction tests together so so the last
thing that I want to do is just to
highlight a couple of what I consider to
be open and cussing challenges and these
are more geared towards the machine
learning side of things since some
improvement holding will be some people
in the audience that are more savvy
about machine learning methods than me
so I'm one of them which is more was the
main mission thing is it's how we
supervise um so that we have lots of
different choices for this we could just
provide images for which we have
surrounding metadata we could provide
images where we have some basic object
annotations we could provide images like
our data set where we have like a large
number of imitated parts and other
attributes my breasts gusts is that the
final solution will be that you won't
hold this you basically want to be able
to deal with many different kinds of
supervision and so we want to be
opportunistic we really want to have
recognition we want to have learning
methods and recognition systems that
just take all different kinds of
supervision it could be captions with
images
it could be videos that are unsupervised
it could be images from the web or
images with some partial annotation and
we want to be able to learn from all of
those different annotations so that when
we r during the wedding process you just
basically trying to figure out what does
this image tell me about objects what
can I use from this but at this point
it's sort of like a it's this is sort of
like a batch processing way of thinking
about it like you think about you have
to think about what does the computer
need to know ahead of time to learn
about objects so you need to anticipate
what's going to be difficult um that's
not really a good long-term solution
instead I think we need to find better
ways of having a more interactive
learning process where you might provide
some data with annotations that you hope
will give a good start but then you but
I think we need better ways of trying to
fix the mistakes instead of having to
anticipate them I think rinsing the
mistakes isn't just going to involve
adding more of the same kinds of labels
we need to kind of ways of exposing the
weight in structures that are learned
within the recognition tasks and having
trying to have people fix the underlying
conceptual errors that the recognition
system is making and that's a really
hard problem in a lot in a lot of levels
it's hard to expose the internal
representations of the system and it's
it's it's hard to then incorporate that
into the learning framework and then
another thing that that another thing
that I think is a big need is
have more lightweight strategies for
continual learning so the recognition
cross problem is really hard it's really
a huge it takes people many many years
to really be able to recognize most of
the objects around us like if you if
you're on the two-year-old a
two-year-old will point to all kinds of
things and give their own names for many
of them will say they'll point to a deer
and say dog or they'll point to luggage
and say room groans to think it's a car
so there's a huge number of prediction
tests so we really need very flexible
machine learning frameworks that can
continually build on what it already
knows and we need good representations
that enable that kind of learning okay
so um the problem recognizing everything
I think is not really just a simple
extension of the form of recognizing a
single object category not really just
about recognizing more subject
categories we need to think about
objects in a different way to think
about up to have more flexible
representations and objects that allow
us to relate any new object that we
might experience in some way to the
objects that we've seen before and so I
talked about a truer efforts in this
direction one the direct prediction of
attributes or properties and the other
trying to jointly train the appearance
and spatial model so that we can detect
the objects and then provide
description about them now I really
characterize these as as us just trying
to understand the problem trying to
understand what are the difficulties
what are the cutter effective kinds of
representations and but really we've
only scratched the surface and I think
that some of the major open challenges
there's there's the basic problems of
visual recognition of these different
attributes but I think that as beginning
to these more open-ended challenges is
more open-ended form of recognition we
need more more flexible learning methods
and ways to learn about objects we can
take in any kind of annotation and learn
about it and when you can continually
build on what your every know so thank
you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>