<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Evaluating MapReduce for Multi-core and Multiprocessor... | Coder Coacher - Coaching Coders</title><meta content="Evaluating MapReduce for Multi-core and Multiprocessor... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Evaluating MapReduce for Multi-core and Multiprocessor...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UWUbRXO3T5I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">ok good afternoon everybody my name is
chris is cozy right
faculty involved with this work and i'm
going to introduce colby kind of give
you a little bit of a background for
this work so about three two and a half
years ago you know one of the students
in my master's level architecture class
brought me the Muppet newspaper and said
it's a great people we should probably
talk about in a class and I read it and
I thought that you know those lucky
people at Google they have the easiest
problem of the face of the earth it's
read only good for them why should I
care and then we spend quite a lot of
time talking about maybe some other
applications map into this model and you
know investors are turning around so now
MapReduce ends up being part of what we
do in this class a 282 class and then
eventually in another class 382 a
managed to find three bright students to
give it a try to see you can take
exactly the same model and use it to
program multi-core machines so Colby
Rahman and Arun or three master level
students at Stanford who did exactly
that within a quarter they did a certain
memory runtime and implementation of
MapReduce which is not necessarily you
know the most robust thing on the face
of earth it was still what they were
able to doing in three months but it
showed some very promising results so
cold we will give a talk who you know is
finished with his master's degree in a
few months and he's supposed to start
here in April so feel free to abuse him
I don't know about the abuse so I'm
going to be presenting an evaluation of
MapReduce for multi-core and
multiprocessor systems and i worked with
ramadan and Arun on this with some help
from Gary and professor costarakis so
MapReduce was developed by google as a
general-purpose parallel programming
model is designed to be used on very
large data sets and maps parallel tasks
to thousands of computers in a cluster
it's nice because it takes a functional
representation of an algorithm and
allows for the automatic expects
abstraction of data parallel tasks it's
proven to be very successful within
Google since it's easy to use and maps
really well to web indexing applications
distributed weapon sword document
clustering among other applications it
sees abuse comes from the fact
that the parallelization synchronization
and communication issues are abstracted
away from the programmer in addition it
provides some level of fault tolerance
through task monitoring and test
replication so let's see how an example
application would map to this model
here's the original board can example
from the original MapReduce paper it
counts the total frequency of occurrence
of all the words in a document it's
broken into two functions the map
function and the reduce function so for
the map function its applied to each
element of the input in this case we
take in a document and emit a word and
occurrence of one for each word in the
document and this is our key value pair
that's stored in the intermediate queues
for the reduced function it's applied to
all pairs from the intermediate Q's with
the same value in this case we take in a
word and a set of occurrences we sum the
set of occurrences to get the total
frequency of occurrence and omit the
word and the total frequency of
occurrence as the final key value pair
the output is a set of lists of sorted
by keys so even though we have the
simple functional representation of map
and reduce this can be run in parallel
across different sections of the input
and intermediate pairs given this
function Google use this to run it down
thousands of computers but we want to
explore if this can be applied to see
MPs as we all know all the major
processor vendors are turning to see MPs
and the critical question is how do we
simplify the programming of these
devices given that MapReduce abstract
sawey the parallelization
synchronization and communication issues
from the programmer it makes it a good
candidate model so to show that
MapReduce is applicable to multi-core
program we need to answer a few
questions what is the performance is a
performance scalable and portable does
it help with locality in fault
management and how does it compare to
other parallel programming approaches to
explore these questions we develop
Phoenix a shared memory implementation
of MapReduce it takes parallel tasks and
maps from the shared memory
reds instead of computers it runs on a
single shared memory machine and the
current version works with C and C++ and
uses pthreads as a threading library
however this should be easy to port to
other programming languages and
threading environments as with MapReduce
our model can be broken into two parts
the API and the runtime so let's take a
look at the API this is a sea-based API
and here's an overview of it it's
divided into two parts the system
defined functions and the user defined
functions the system defined functions
are the ones provided for you it
includes a call to the Phoenix scheduler
which initializes and controls the
environment emit intermediate and amid
which are called from map and reduce to
store key value pairs into the Phoenix
manage queues the user defined functions
are arguments to the Phoenix scheduler
these are must be provided by the
programmer that includes Map Reduce and
the key comparison function the key
comparison function is used when omit
intermediate and emit is called to sort
the keys in addition the programmer can
specify the splitter function which
splits up the input data for the map
test the process this is optional since
by default we have an array splitter
function as you can see this is a simple
and narrow API where you only need to
specify a few functions there are some
more arguments that the programmer can
specify that aren't discussed here but
are discussed in some more detail in the
paper so the call to the phoenix
scheduler starts the Phoenix runtime
this is what orchestrates the program
execution across multiple threads this
includes starting and stopping of worker
threads assigning map and reduce task to
workers and handling buffer integration
the key features of a runtime which help
improve the performance include dynamic
scheduling of tasks for load balancing
trying to reduce communication by using
pointer exchange instead of data copying
whenever possible optimizing for
locality by breaking up at the data and
algorithm into smaller chunks and
providing some support from fault
recovery
as with MapReduce to details of parallel
execution are hidden from the programmer
including low-level threading
communication and scheduling that way
the programmer can focus on the
functional aspects of their algorithm so
here's a pictorial overview of the
Phoenix runtime starts with a call to
the Phoenix scheduler which passes the
input data to the splitter the splitter
divides up this input data into chunks
for the map test the process the map
test process this data and emit it into
intermediate queues after all the map
test finish the reduced stage starts the
reduced stage processes the data from
the intermediate queues and amidst the
resulting key value pairs into the
reduced queues the merge stage combines
all the reduced queues into a single
output queue the next few slides I'm
going to discuss the Map Reduce and
merge stage in some more detail and I'll
provide some insight into how phoenix
manages the execution also I'll give
some trade-offs for the programmer to
keep in mind when using the model so the
splintered function divides the input
data into chunks for the map tasa
process it does pointer manipulation and
it has to deal with some trade-offs when
it choosing a chunk size if the chunk
size is too small there could be
increased overhead from starting and
stopping up map task the chunk size is
too large you could have some locality
problems or possibly some work and
balance issues Phoenix attempted to
explore these trade-offs by choosing a
chunk size determined by the cache size
to optimize your locality again the
program can provide a splitter function
however by default we have an array
splitter function provided for you so
let's see how this data is processed the
map stage is composed of workers where
each worker runs on a separate thread
each worker runs multiple map tasks and
typically an application has hundreds of
map tasks dynamically scheduled across
all the workers so as a map test process
the chunks of data from the splitter
they call amid and store the
intermediate key value pairs into the
intermediate queues
partition function make sure all the
keys are the same value get assigned to
the same queue since we need to combine
all these keys of the same value
partitioning can introduce significant
overhead after all the map task complete
the reduced stage starts the reduced
stage again is composed of worker
threads where multiple reduced tasks run
on each worker the reduced test will
process the data from the intermedia
accuse the partition function if the
partition function unevenly distributes
the keys across the queues there could
be some work in balance which is why we
use dynamic scheduling to help relieve
alleviate this problem when we run
reduce tasks so as a reduced tasks
process this data they omit it to the
final key value pairs to the reduced
queues now the partition function by
default is just a key hashing function
however the program can provide a custom
partition function if they know more
about the distribution of the keys so
after all the reduced task finish the
merge stage starts in the original
MapReduce model it simply ended at the
reduced stage and the final output was
the set of reduced queues we added the
merge stage which combines all the
reduced queues into a single sorted
output queue this may be unnecessary for
some applications but the merge time
tends to be small one compared to the
map and reduce stage times in addition
it's convenient for many shared memory
applications to have this how it works
is we do a binary merging of reduced
queues until there is a single sorted
output queue and that's the overall
execution of our runtime so I've told
you how we can paralyze all these
applications with all the workers and
tasks running on them and running on
them however some things can take away
from the speed up some of these things
are significant while others are either
infrequent or easy to fix I already said
how partitioning can be a major overhead
since we need to combine all the keys of
the same value together in addition
repeated calls till we met an
intermediate can introduce overhead
since we need to do is
data copying some problems are related
to how the programmer uses the API
rather than how the runtime manages it
this includes key management since some
applications do not naturally associate
keys with data what this means is in the
original implementation of the algorithm
these keys did not exist so the
programming had to introduce them which
made it more difficult on the programmer
and it was pure overhead for the runtime
another issue is some algorithms are
iterative and require multiple
indications of the MapReduce scheduler
which is an expensive operation some of
the more insignificant issues include
the final emerging and sorting of
reduced queues into a single output
queue but when compared to the map and
reduce stage time this tends to be small
in addition buffer management tends to
be a smaller issue since we aggressively
reuse pre-allocated buffers we deal with
reducing balance by dynamically
scheduling reduced tasks so this tends
not to be it's much of an issue and one
can imagine the input splitting function
being a serialization point however most
map task contain enough work so this
doesn't actually happen I just want to
bring up a few high-level issues on this
slide since they're going to affect the
performance later and there are good
things to keep in mind when using this
model but before we get to the
performance results let's talk about the
fault tolerance model we used we focused
on transient and permanent errors in the
worker and our air detection scheme was
based on worker timeouts so if a test
tended to be running a little bit longer
than the other task we assume it had an
error and start the error recovery
process it included restarting the task
and if that failed after multiple times
we assume the worker had a permanent air
and we assigned that task to a different
worker the scheduler handles input and
output buffer integration for you when
these errors occur in the future we'd
like to add fault tolerance for the
scheduler and do some air detection and
isolation through worker sandboxing so
to evaluate Phoenix we use an smt system
and a cmp system the cmp system had 88
CPUs on a single day
I and hardware support for four threads
per CPU the SMP system had 24 CPUs we
found the SMP results to be very similar
to the CMP results which means we got
portable performance what this means is
you can write one version of the
application and it works well on both
machines I'm not going to discuss the
SMP results today but they are discussed
in the paper we wrote applications from
three different domains the enterprise
scientific and consumer domains in
addition for each application we have
three code versions the sequential
MapReduce and P threads version each of
which were optimized independently today
I'll talk about the power model
performed in scale and how the model
perform versus P threads in the paper we
discussed dependency to data set size
dependency to input test granularity and
how soft and hard default injections
affected our model so here are the 8
applications we wrote I'm not going to
say how we actually wrote them in the
MapReduce model here but it's disgusting
the paper I just want to bring up a few
high-level issues associated with these
programs here these programs on the top
like reverse index matched well to the
MapReduce model and seemed to perform
well and we're easier to write these
applications down at the bottom doesn't
necessarily perform poorly but they had
other applicants oshi ated with them for
instance k-means is an iterative
algorithm and required multiple
indications of the MapReduce scheduler
which is an expensive operation in
addition pca and histogram required the
programmer to introduce keys that
weren't in the original algorithm which
made it more difficult on the programmer
and introduce some overhead in the
runtime but just because you need to
introduce keys into the model doesn't
mean your your program has to perform
poorly in the case of matrix multiply we
introduced a coarse-grained coordinate
as a key but we associated this with the
large enough chunk of data such that the
overhead was amortized in the runtime so
here's a graph of the CMP speed up as
the Phoenix version of the application
versus the sequential version of the
application as we scale the number of
cores higher is better we ran it down to
four and eight cores with four worker
threads per core since we had hardware
support for it so as you can see we get
pretty good speed ups with the worst
speed up being about a a tech speed up
on eight cores for k-means what this
shows is we get pretty good performance
and it scales for some applications we
get a super linear speed-up as we
increase the number of cores this is
because we're breaking up the data in
the smaller chunks and the algorithm the
smaller chunks so we get great caching
effects and there's reduced work per
task for other applications you see a
flattening as we increase the number of
cores this is because we're running 32
worker threads and all in parallel with
workers and some bends which saturation
takes place when this happens in
addition loading bounds can take place
at hike or counts so Phoenix seems to
perform well and scales but how does it
compare to other parallel programming
approaches pthreads is a low-level API
that is much more flexible than Phoenix
since it doesn't impose a structure on
the programmer in P threads you could do
exactly what Phoenix does or you could
do some other parallelization technique
what this means is the performance
support pthread should be at least as
good as Phoenix in this graph we have
the Phoenix version of the application
versus the P threads version of the
application as scaled against the
sequential version of the application on
a course again higher is better so for
some applications we see that Phoenix
performs comparable to P threads and
that's because the these applications
met the match the MapReduce model well
used to notice that Phoenix actually
performs a little better than P threads
in some cases and that's because we
dynamically scheduled tasks in Phoenix
whereas we statically scheduled them in
P threads
an interesting anecdote is when we were
writing some of the code for P threads
we actually looked at the behavior of
Phoenix to tune the code for our P
threads for other applications we we see
that P threads actually performed better
than phoenix that's because these
applications do not match the MapReduce
model as well either because we had to
introduce keys or was an iterative
algorithm so we seem to get a pretty
comparable performance with P threads
and we're pretty pleased with that even
though we may not get the best
performance all the time this tool
simple really does simplify parallel
programming and it is a useful tool in
conclusion yep so are you talking about
why the numbers are so large okay well
maybe I'll wrap up the conclusion and
come back and answer that right way so
it will take a few minutes okay so
Phoenix is a shared memory
implementation of MapReduce where we
take parallel tasks and map them the
shared memory threads instead of a
computer we run it on a single shared
memory machine and we provide some key
runtime features that help improve the
performance such as dynamic scheduling
locality management and fault recovery
among other things I've shown we get
scalable and portable performance that
compares fairly well to P threads in
this research project in the short
amount of time we had to do it I think
we've shown that it works and it's an
interesting thing to do as with many
research projects I'd like to improve
the key structures automatically detect
some system configuration at better
fault detection an experiment with more
systems and applications so let me do
this is where we'll make a copy of the
source code available and we'd
appreciate any comments you may have on
that but let me get back to that
question you asked so the question was
why does why is this performance on
reverse index ridiculously high
and the answer is the algorithm we chose
for reverse index in the sequential case
is doesn't scale well as you increase
the size it's an N squared algorithm so
that's why you see the much larger
increases so when you break up the
algorithm into smaller chunks you get
much better speed ups does that answer
your question so why is the speed up for
Phoenix so much better than P threads
that's because when we do the Phoenix
version of the application again it's
not just that we statically scheduled at
well it's mainly because we statically
scheduled task and P threads so this
shows up much larger in this case
because the data set is very large it's
in the gigabytes range so and these
documents that were running it across
you can't tell how much actual links
you're going to have in a document just
based on the size of a document so some
even though you break it up into equal
chunks these tasks have different amount
of works different amount of work that
don't get balanced as it does in the
Phoenix case what would the speed-up be
if we use the same algorithm for this
cereal and if we broke it up into
smaller chunks in the serial case and
right right right I'm not sure what the
speed-up would be exactly but it is the
same awkward just a few awesome face
problems and then split outwards so if
the end is lower because every task is
blue ice water
what the sequential program down there
you get square thanks again sighs the
same a group it really is if you're not
ready to say no so your question should
be I guess you know what if we use their
note n square algorithm
so it is the same algorithm just like
you want this one favor support
implementation much more okay are there
any other lower sum of squares so you
can basically take this results in
custom whichever way you want and one
thing that you say is oh you didn't do
then I'm scheduling interface which case
that's it's unfair and what it comes to
ultimate performance in software and I
think that you know if you give me an
infinite amount of time and the lowest
level API on difference I wouldn't be
anything you can do with anything higher
level like our ma producing television I
think the question is within some amount
of time with some original programming
rodents what can you do so in this case
we're the same for in one case there
aren't that pin that I'm sitting for you
the other case yet for deployment and
you know get all the Cardinal program is
out there enough time base to figure out
which is the best algorithms for this
specific you saying where is a scale
when we're safer for just be arguing and
one of the points here is it's not the
cable more than where you're at it once
it does originally where they were
compromised all the time I think that
you know it could just focus on either
what's the big performance and what do
you do
this assembly which is looking visible
do we do another topic at how how about
are these data sets and what is the
thing that's getting your performance it
really depends on the application what
is first thought the question was what
are the data set sizes we're using and
how are we gauge ongaro formance okay
well as far as data set size goes it's
not it varies in sizes it's mainly based
on runtime so for the reverse index we
could do gigabytes of data in a very
fast amount of time whereas for these
other algorithms because you only have a
linear pass really through one of those
whereas if you're doing like a matrix
multiply you use the same data a lot
more and keep on passing it around so we
use smaller data sets for that in the
paper we discuss the actual sizes we use
I don't have that figure with me right
now now for what's actually limiting
performance it really depends on the
application and I mean it varies because
you know you got key means you know
which is an iterative algorithm and then
you have other problems where you need
to introduce keys and for some things I
think that flattening effect as you saw
in the higher sorry the flattening
effect that's where you in those
applications you tend to get that IO
congestion so it there's really no one
answered to what what slows down a
program it's really on a case-by-case
basis and how we how it actually breaks
up the data and how it uses the data
it says that sometimes you just saturate
the machine little saturate a new
connect for a reason suppose I have some
none of the vizio Bob this thing's all
fading memory because that's what it's
suppose I have some amount of money to
throw away the trouble whatever to use
the cash side who they want to that park
or whatever well so can I get that
bastard don't give the figures there but
the pass another big effect because was
you sanket it look all the work pretty
world in some cases the interconnect the
off-chip public becomes engaged to my 23
bday assume they are output out and none
of the problem we're almost big enough
to exercise the ecosystem
yes
um
oh I see you're saying so did we compare
our implementation versus the original
MapReduce implementation running on
separate machines and no we didn't do
that sorry
John pool and Mel
now
so how does our programming model
compared to other higher level parallel
programming approaches well there are
functional models which makes it easier
on the programmer in both cases and as
far as performance we don't have any
statistics on that but for ease of
programming you know I think I'm not
really sure but they're similar
approaches it's just our MapReduce
implement some structure on the
programmer with a map and reduce stage
where as in those other models you have
a little bit more flexibility ok or
something like know it kind of fits the
moment you guys develop this is a very
nice high level programming part that
something amazingly simple then the rant
on console college the challenging the
cultural everything it doesn't fit the
more than that's where the weird stuff
happened that's where the gold version
with other subjects English so for
example you know the sub programs you
know remember at the linear equations
brother ACA he's so he have to come up
with is and he forget whether it was
faster growth when he thought it was an
exercise which was natural
so in that case maybe using openmp of
your favorite chocolate red packets or
someone it would be a more natural
experience so I think that you know if
you look at many things that are
happening in many places including start
nobody's trying to say here's what
programming more than with speeches open
it be you know professional heavily what
we you know this version of master keys
and so let's put for everything it's
more like well many who can care submit
which is good for a class of algorithms
you look up based on this kind of
algorithms which is the basic body when
i call it there quickly I suppose it you
know the last thing about this one is if
you express this way everything is
making hero why with features for
example you know you still have water
conservation value choice things so you
better get right don't
yes the particular in a little more
detail sure let me bring up the overall
picture so this partition step you see
at the end of the map stage this is what
make sure all these keys get assigned to
the same cues and I think what you're
trying to get at is how do we reduce
contention when you're doing something
like this right trying to get at the
other question is maybe on the other
side in that other great where you see
the Alt alt side how do you mean so that
all values associated with the same key
or continue our present and continuously
to the reducer right so how this works
is for the partition function we
actually keep for each worker thread a
set of cues for each reduced task that's
separate for each thread so there's no
communication when you insert into these
cues you only need communication when
you combine which happens when you start
your reduced tax and since that reduced
task is only going to be using that
values there's no contention for those
reduced cues that it needs so you can
just combine them it just goes through
compares the front of them and combines
all the keys of the same value and
starts a reduced task yeah Peter strata
k cfp this is yeah since we're grabbing
data from every thread there can be
there is a lot of communication here and
it's definitely where we should look
into improving either the queue
structure or the partition how we
actually partition here yes yeah
well oh you mean oh right right because
we do it that way because then you can
run them in parallel on different
sections and there's no guarantee that
the the keys and the lower if it's just
a basic hash function and we're sorting
it as we going down we want to do it all
in parallel because you can get more
bandwidth that way if you would did it
as an N way merge you'd have to do it
all on one processor there and we yeah
right that's a that's a linear pass
through all the data at the last node
notice officers of the state didn't
really bothers me but I mean the merge
time was very minuscule I mean compared
to these map and reduce stage times so
we didn't optimize the merge stage maybe
as much and there's more we can do to
optimize that in the future
yeah it could be more significant in
that case but it depends what what
exactly the application is you know if
you're just doing something like running
through all the data in your map test
just admitting it and then reduced
testers just admitting it and then
you're merged aegis is sorting it it
could be more significant in that case
yes yeah yeah so assuming you have a
good partition function this merge stage
should just be combining near cues that
are already sorted actually modern
reviews even the sweeter the way to
implement example yeah this equation
speaking you know what about me well
everybody's assess me nothing nothing to
an application is a wonderful process of
aerial make concurrent splitting yes
yeah
any other questions we've what they
should say where the paper is absolutely
the use of in rivers times okay there's
one more question yeah did you try this
on architectures I know we did the model
we haven't tried it on that but the
question was did we tried on any new
architectures by the way
yes and reduce work yes sure so for
these worker threads we like I said we
had 32 in the case of this one machine
but it we usually have one per processor
and the number of reduced Hass exceeds
that so you can run multiple reduced has
on a given worker if it completes so
what has reduced cast complete you start
the next one that's how we dynamically
scheduled them and the reduced cast
includes the merging of all the keys and
then running it for reduced task or each
key I mean it depends on the data set
size really because we wanted to break
it up these reduced has to work on
smaller chunks of data oh yeah
yeah it can be
well I would say there is an optimum
range and we definitely play one of
these values to get the best performance
we could and that took some time to
figure out what that would be so varied
from application to application since
the distribution of keys is not
necessarily the same for each
application making the number of reduced
test that's optimal different but
generally I really don't have the answer
for that question but it we can do more
research into what exactly would be a
good methodology to figure out what the
optimal number of reduced test is
several hundred you stop and that seemed
to work well one of the problems that we
actually ran into is as we increase the
number of reduced as we increase the
runtime of the word stage because now
the word state has to reduce more for
kids so we did run into those problems
but we haven't explored it from vs kuya
generally order of several hundred
reduce tasks where we had like 30 in
addition as you increase the number of
reduced tasks as you had with splitting
there can be some contention as you're
starting a map test they can also be
contention for figuring out what the
next reduced task is the schedule at
these reduced casts have nothing to do
if there's no queues and no keys in the
queue then you're going to be waiting on
this lock to get the next chunk of data
if there's no data for you
emerge because here you of your touching
and they are bringing from the men from
the main memory Logan cries while with a
new emerging would keep the inside cash
and cash much faster operation of the
meaning behind them
memory problem if I will get from the
main memory cache forget so that I could
from main memory caching Maine State
cash around petrol engine go to main
memory so the number of my may never Act
smaller so this is why ask us why
so are there any other questions yes
sort of a generic one those applications
of the model look how much tuning did
you have to do to get a reasonable speed
up over the mental case Oh to get a
speed up over this quenching case once I
mean once we played around with the
scheduler to figure out what the optimal
chunk size would be for these
applications it didn't take that much
tuning just that was mainly the picking
at chunk size that fit into the cache
and that was pretty pretty quick yes you
can tune it to the application you can
give it some idea well by default the
scheduler world picks data a size based
on the cache size however the user can
provide some hints based on how much
work you actually do on this data if you
how good was the Eagle defaults pretty
good and we didn't change it for a lot
of applications after we figured out
after we played around with it and it
turned out to be pretty good
the results stages as opposed to see
Bernardo Alaric and initial remarks
better but there was always a mystery
then the basically had to play with his
people certainly changing the skate
providers can projectors so there was a
little bit of going back and forth a
positive that the absent and the overall
transaction the possible proper input
the natural environment you have a
minute
yeah so after we got through the first
batch of applications the other
applications after that we didn't really
have to play it right with anything I
think you know the interesting thing is
prior obligation receiver out of this
case where Mark is trivial thing is a
Saturday of the game
in much worse invitation any other
questions lessons which will be
applicable to do some of the others of a
13 dwarves and so what was the question
no I'm not asking i got i think i think
it's worth what somebody going down the
eastern sea thinking involved but reduce
what one way or another i don't remember
the photographer i won't recover the
rate of the world but if you think about
the most of them maybe episode stuff is
exceptional remember the movies are
preferably a top bottom i can look to
remember whether they actually here
tears or they want one but a bunch of
you to do something planned but let's
face it if you think about scalable part
of it you're thinking about the thousand
personal check it has to be an apartment
because there's no way that anything
else will be easy to write it cannot be
a thousand different tasks who write the
program to the positive software center
point source in the parliament well as
there will be functional style is a
little power
or SP MP did you open in P star or
something like that I don't know what to
listen see if it works but it's a great
apartment but once I think you're right
yes it's the secularization very thick
inter communication requirement right
what they used to think about this 13
words it is that if you look at that was
classified worries again specific
limitation for the solitude or no
they're not surprisingly different from
what scientific ability has been apart
since is when I picked up the scientific
there's a lot of experience or so it
shouldn't be too difficult tell whether
they marked with this model or any other
model so without side of the problems
are not important maybe you know Dave is
right these are the only things aleve to
worry about if he is right it's a good
thing cause we have lots of experience
program and its new episodes which I
think it's currently supports on program
unless I missed something too graphic
reason to know how can you limit
nicely to them functionally whatever
their own problems
any other questions disagree with a
difference
our visual dreams of this one
again the paper there's a link to the
paper section there where you can
download the paper so great
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>