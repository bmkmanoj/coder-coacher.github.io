<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast Route Planning | Coder Coacher - Coaching Coders</title><meta content="Fast Route Planning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast Route Planning</b></h2><h5 class="post__date">2009-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-0ErpE8tQbw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this morning our speaker is Peter
Sanders reversal counsel he's here to
tell us about lots of different
interesting work that his group is when
we're doing and that's drought planning
so thanks to army I'm very happy that I
can be here today this nice environment
so what I explained you as in done
together there's lots of cool office the
names you see here are my current group
except I put Dominic soldiers in
brackets because unfortunately he left
to industry unrelated to route planning
but he is one of the people who was on
this research award proposal which is
one of the reasons I'm here today so I
wanted to mention him and he did a lot
of the initial work yeah so what is
route planning I probably don't have to
tell you we want to find shortest paths
in large open roads may be also other
kinds of networks least the want them
time dependence meaning that the travel
time along an edge of the network
depends on the time of day what we want
of course clear should be fast they're
actually point-to-point trees and many
to many queries I will tell you a little
bit more about that you can do some
pre-processing but it should be fast and
it shouldn't take too much space and
maybe pre-processing as a problem
because networks change so we want to be
able to do fast update those
applications are obtaining systems of
the internet which probably of the
shorter basis is what's most interesting
for Google but also car navigation
systems you all know that and other
applications like logistic optimizations
traffic simulation if you have a
particular application on right sharing
that I would talk about it deal so this
is most difficult slide it explains our
most recent technique in a single slide
don't worry there are a few more slides
coming but maybe it's interesting to
spend a few minutes on it because it's
actually very simple so suppose we have
some own products and notes of our
network by important these important
first one to N in a light years to build
a hierarchy that has one level for each
node and building a new level basically
means getting rid of one node so the
least important remaining node so here
you see an example we want to get rid of
node week so what do we have to do I
mean we can't simply delete it because
then the remaining graph doesn't have
all the shortest paths it had before but
what we can do is we add a shortcut edge
once it's a red one to the red edges
with way to here where this red edge
basically encodes this shortcut this
path of length two let's go sous-vide
and this red edge encodes
I don't know what that works is which we
called this shortest path and you can
may also notice that from the yellow
node to the green node at the bottom we
do not need a red edge because there's
another shortest path that doesn't need
V alright and that's important because
it one who keeps this graph sparse and
what we get from this is what we call a
contraction hi oh and he said the
picture on the bottom there are edges
are all only followed upwards when you
do a cube so it works really from s to T
you just look at the edges upwards and
you find this red path which now
contains a shortcut from node 6 to node
4 which you can unpack but they're just
some simple recursive data structures so
that's a basic idea about contraction
and so in a nutshell by this contraction
process yet transforming the graph to
graph that contains shortcuts and we are
directing these edges always upwards so
that both and now the query is a
bi-directional search this directed
acyclic graph and you only follow the
edges to the more important notes and
then as usual and by directed a
bi-directional search that the search
for tears will eventually need and the
best meeting point is the Schwarz
so how do we order some notes what we
use in the basic case is a priority
queue where the priority of a node is
rated with a couple of terms and in
front of each time we have a tuning
parameter that helps us getting good
ordering and what are these terms the
most important one is what we call the
edge difference it counts as shortcuts
we have to insert for the contraction
minus the number of edges it was there
before so in this example we get rid of
three edges and introduce two shortcuts
so the edge distance is minus 1 and we
want to have small edge differences
because that means the graph is sparse
and then another important ingredient is
what we call uniformity terms that make
sure that the graph is contracted
uniformly everywhere on the graph and
that will mean that you get a reasonably
flat hierarchy and there are very easy
ways to get this for example by just
counting the number of neighbors of a
node that have already been deleted then
the construction process simply takes a
smallest priority node puts it into the
next level of the hierarchy contracts
and updates climatic terms of the
neighbors entities
so here's an example that's a wood from
the Putin to house for another like that
because she was working here the Max
Planck Institute for computer science as
I did for seven years and you want to go
to cards well that's a medium-range
thing like 140 kilometers and we see we
have a path of 299 edges into a literal
Network which is compressed to just
thirteen shortcuts and if you have woods
on a continental scale in Europe you
will have something like 20 shortcuts so
it's a very effective way of compressing
tasks and what do your single is the
path given by growth so that's this one
which is a bit strange I and also even
about it and she said yeah it strangely
will change the objective function but
it didn't have a power so the point was
this powerful it takes a motorway all
the time and it's like forty kilometres
longer distance and takes about the same
time into Google evaluation function in
practice it takes longer because it
anyway so it's an interesting example
where you don't do to have hierarchies
that just follows a street cutter
because this one is not a motorway okay
so this is a search space so the direct
edges has a search space from zabolotin
the blue edges as a edges from cuts
where it looks pretty big fights is a
serve the channeler to britain so we go
very far away like it's not a problem
because if you look at the dot see on
the picture that's a note see expand and
for example this complicated thing is
just a single edge which can be relaxed
relaxed in a couple of nanoseconds we
have all in orders we ended 16 several
notes and 951 relaxed edges and then
ordinary dice a search would have looked
at all the immediate answers so to speed
up it's like 100 microsecond clearly
time or probably less because it's a
short distance maybe 50 microseconds on
an ordinary person
by the way sir questions don't hesitate
to us so what what do we have of
contractors has a piece it's the
foundation of all our other methods
right now they are conceptually very
simple it just is contracting vultures
right in previous works of higher
highway hierarchies we have two
different things one of them was
basically simplified form of contraction
and the other one was a more
sophisticated thing that gets little
extras you don't do that anymore
apparently coordinates moment we process
in time for our standard benchmark of
about 19 million nodes is seven point
five minutes pre-processing that
includes actually the node ordering in
many cases you don't have to do it that
case like less than two minutes be
possessed this says 200 microseconds to
determine the path length but they are
also very it's at the only one I don't
know and about half a million to all
unpacks the path which by the way is at
the same time you need to just reverse
the path and space consumption is
interesting test 23 bytes per node but
it actually less than what you need to
just draw the graph if you want to
implement a class basic a group because
the thing is for but Dijkstra's
algorithm directly address in both
directions because you don't know from
which side you will enter them whereas
an hour technique you only store them in
the direction unimportant no too
important
so you have additional space because you
need shortcuts but then you saved
because you store a dress on once and
the bottom line is actually positive but
that may be not sort of one for Google
it's not a lot of space any hour faster
technique is yet another two orders of
magnitude faster we are basically able
to reduce planning to just a lots of
table lookups on edge like 100 table
lookups the idea you see in this picture
basically for every node we store excess
node something if you want to go from
the Google office in Seattle to anywhere
in the u.s. that's sufficiently far away
there's probably only the I guess here
there's only one thing like you go to
interstate 5 and then either you go to
the 95 - good to go east or you go to
south on the interstate 5 and that's it
right so there's only one or two places
that you have to remember and if you
want to go to Canada they make enlasa
access to a boat away that's it and it's
similar almost everywhere else so you
just remember these places where you
access a long distance Network
you remember how far they are away and
then you have just try out combinations
and the shortest one is your shortest
part so then interestingly got us a
publication in science which usually
doesn't publish computer science results
and even the press was interested like
Scientific American said oh it's one of
the 50 most interesting results of the
year 2007
another thing that's yet an order of
magnitude faster is
Jew concluding distance tables which you
need in logistics optimizations
what's Avila 10,000 times 10,000 table
fiddly hit things like 10 seconds zero
point 1 microsecond for each distance
computation and that's interesting for
quite a number of applications and
actually we have one that might even be
interesting google also write sharing in
germany it's actually a very popular
thing for people who don't earn a lot of
money likes to use this one but I think
it gets also more interesting for people
who want to to share cars and they go to
work because they have long commuting
distance isn't want to say I'm sure the
websites I know
currently basically work like an
ordinary database right you tell start
and destination and then it looks in the
database if says an offer in the
database that has the same start at
destination and some of them have the
ability to to search in a circle but
suppose you go from Seattle to San Diego
right and somebody else wants to go from
San Francisco to those angels then you
can give them a ride and it's on the way
honest and you wouldn't find that in a
circular so anything like that so this
doesn't explore all the possibilities
and actually not so important between
the big cities but if you have something
in the countryside and Germany year for
example is the countries where there are
people living almost every day abide and
the ads sometimes not easy to find this
right share opportunities because they
only between the big city
and what you have to do for this a
database is commandeered something like
100,000 entries if you want to do
something more clever like looking at
how much of the detour works the driver
have to take to give the other one right
that's a good measure of how expensive
it is to some right then you have to
basically compute to shortest path
distances figure that out for each entry
in the database so one search of the
database is 200,000 shortest path
computations
let's prohibit it right no not for us it
was a 25 milliseconds basically using
this thing okay and of course once we
have a million offers or so we might do
something geometric pre-processing to
further speed it up but can't it's
really not you just that kind of nice
demonstration of the power and I think
it's useful application yeah usually our
measurements are without taking
penalties for tip for taking turns into
account if done experiments of this yard
considerably increases memory and
clearly time and so on but it's still in
good traction algorithms it's forded 20
microseconds which is still more than
fast enough for most applications and we
even have done some experiments how to
do it with very little space and so on
which would be needed for mobile
navigation systems now let's look at
dynamic synonymous which is maybe also
interesting for google
the currently what you usually what's an
important application is in the radio or
some digital format that's available
like TMZ or tips Pro with the list you
get information about traffic jams not a
very big number right now but important
because it's usually on highways and
that changes your cost function and then
for the first glance destroys all the
pre-processing efforts but we have
developed techniques where you can
handle at least the number of edge razor
changes that are currently through there
are actually two scenarios one in a
server what you would probably do you
would update your people's data
structure but only in those places that
something changes and for a mobile
implementation you do something else you
keep the pre-processed data as it is let
you change the Camellia room so that you
still get optimal paths so what he tell
me half is is actually two papers on
this dynamic scenario but I will tell
you the mobile implementation which is
under no PRN 804 hundreds are processor
we are able to compress a graphical
presentation by about a factor of three
compared to the straightforward internal
memory compression communities take
something like fifty six milliseconds
and what's even more important perhaps
as than you miss the connect miss a turn
or something like that then you have to
be comfortable to your path then things
are still already into cache that are
very useful then it takes only 40
milliseconds so what we have is
instantaneous how changing on a mobile
you is and for this we have also
implemented this that amazing techniques
and with 1000 x-ray chain which is
already pretty big number it's still
rarely lower second so when you s
desired it was presented of the olynyk's
conference in January is a extension of
contraction hierarchies to
time-dependent group planning and what
we do they are concretely is we assume
that we are optimizing travel time we
assume that the travel time function of
an edge is piecewise linear then you can
approximate almost anything to this file
and we need some clutch rifle property
which means that the function cannot go
down with a slope of more than one of
time which means basically that it never
pays to wait at the beginning of an
excellent use and time which is
reasonable
you do need these assumption I mean you
don't necessarily want to optimize
travel time right but if you want to
have optimal paths and launches use
things like Dijkstra's algorithm then
you better have this thing because
otherwise it becomes np-hard sources
problems doesn't mean that you can't do
it in practice and of course that will
be one topic for future work but we
wanted to start with keeping what we are
solving is so-called earliest arrival
problem where a clearly means we have
given the starting knowns destination
node and the departure time and we are
looking for a path that minimizes
arrival um
so that's two layers of building blocks
that you need you first lay eyes what do
we do with these a trade function
basically three operations we have to
evaluate it and that's important
operation you and clearly that takes
basically constant time so it's
interesting when you implement time
dependent Dijkstra's algorithm oast as
fast as a static one because evaluating
this edge rate function is not be is a
limiting factor here you have priority
two operation two of cash Falls for
accessing the graph etcetera etcetera it
maybe is effective and then enjoying
pre-processing you have to do more thing
you have to compute a minimum of two
functions like here all you have to
chain to fund so if you have to address
G and F described by function G and F
then you may want to compute a function
that describes the travel time that you
get when you first go this actually then
that you can training both obligations
take linear time in the complexity of
the input functions it both have sir not
summarized properties at the output and
complexity in the worst case as a sum of
the input complexities so when you do
this you get more and more complex
functions which is kind of bad news
so now from this we have three basic
techniques that you can use for all
kinds of time-dependent planning
techniques one is just time to the
time-dependent dykes clavicle which with
these five of property assumptions works
basically in the same way as Dijkstra's
algorithm except that when you are a
known you were relaxed and XUV the two
other time you are used for this edge
has to be evaluated as a shortest path
distance time of you but you know is the
case of taxes ago so it's not important
then you have something else which we
call profile search here you do
basically Dijkstra's I go over not for a
particular starting time but for all
possible starting times and then the
edge labels this was a tentative
distance and Dijkstra's algorithm when
all these edge labels became function
functions again and an edge relaxation
basically means that you have to take
the minimum of the old label and the
label you get when you take the shortest
path description to you which is a layer
current label of U which it's not
necessarily a shortest path
unfortunately and chained it with the
function describing the edge now
unfortunately
you cannot guarantee anymore that here
you answer shortest path distance which
means you may have to relax an edge
several times then you get what's called
a labour correcting it but in practice
you don't have to do these leave
annexations very often so it's not a big
perform what is a big problem is that
usually this F old thing I mean that's
or this thing fu also they describe an
entire path which can contain hundreds
of edges right so it will be a very
complex function if you evaluate
everything exactly and then it's not so
easy it's a very expensive operation yes
so in normal dice rate you choose which
know to settle next by the minutes
minimum cost yeah but here you have
these vector-valued costs yeah how do
you which might not have a strict
minimum yeah so how do you choose which
noticeable next so more or less
arbitrarily to take just the minimum of
this function you could do anything you
could take the average value of this
funds that the function describing the
label whatever it just of heuristics in
development by being willing to do these
realizations who will remain correct and
the number of relaxation you make may
depend on the choice of cysts autistics
but the minimum works
okay so this is very expensive operation
so you have to use it very sparingly and
that's a reason for a third component
they'll do something like a compromise
which we call min/max label search it
can be viewed as an approximate version
of the profile search we only compute
upper and lower bounds for the travel
time and now a distance label isn't here
in upper bound lower bound upper bound
an edge relaxations means that you do
component-wise mini Mar and things like
that so it's basically a Dijkstra search
working on vectors of size two again we
don't really know what kind of priority
function you should use so consistently
with the previous investigations the
lower bound cousin Santiago bomb for the
priority queue you again get a label
acting algorithm and then the minimum
operation what you see here kind of
narrows down six
okay now how to use this for making
contraction hierarchies time dependent
there are two major challenges here
first we have to do contraction using
the pre-computation right and I said
when you are removing a node and
reintroducing shortcuts and you don't
want to use all conceivable shortcuts we
want to pause and certain shortcuts are
not necessary for this you need
something you need an alternative path
which we call a witness okay and in
order to find these witnesses exactly we
would have to do profile search which is
very expensive and if you implement this
in the straightforward men are getting
credit system for large networks so we
have to do something more clever and
then there's another thing with which
the scientists involved were always
telling us where you are you poor guys
you have these very fast static
techniques but they are all using
bi-directional search so this won't work
for time dependent you lost
there was a lot of work on getting fast
unidirectional planning techniques but
we said no we stick with bi-directional
search and a verb will explain unit
second
so what's the problem here the point is
that in order to translate
bi-directional search to a
time-dependent setting you would have to
do a backward search from a node you
want to do it backwards time dependent
Dijkstra's search right which is no
problem if you know arrival time
otherwise you can't use time to pay the
dykes time but our idle time is the
thing you want to compute it's not
something you know so you're in trouble
that's a challenge
okay so what are we doing first about
the profile search what you basically do
is you don't do it a name officer so say
VF you're considering whether you need a
shortcut from you 2w u RV then we
contract V and you would have to use a
pretty big search space for a forward
profile search from you instead what we
do is we do a min max label search from
you and use the the bound and the data
we get there to identify something that
we call a corridor that defines kind of
interested then our interest in
candidates for businesses and onions is
covered or in dual profile search
they're just basically unpacking a path
which is still not very cheap because we
have to uncheck it and compute sort
whether time function and then compare
the results of the travel time function
for the shortcut but it's really much
faster than this
and what about
bidirectional see again it's this
minimax thing them so for the forward
search there's no problem you can just
do the time-dependent Dijkstra for the
backward search we could even do it as
an exploration of this directed acyclic
graph even that works reasonably well we
do it a little bit more cleverly by
doing a min/max labels for edge and then
it's a meeting point we have information
that tells us which of the retail points
are obviously because if you know what
path if if see for example we know that
this path Plus this upper bound is less
then this path plus that lower bound
then we could drop this edge this
meeting online and again we get
something like the Commodore which is
interesting for which Kidman contains in
a guaranteed way all the shortest path
from s to T and then basically we
continue the forward search in this
Commodore starting from the meeting
points and using only the edges that
have been marked by the backward search
which funnels it into the target and
that works telling me
so here's some experiments so there's a
time-dependent network of Germany with
quite realistic travel time functions
that are basically a 15-minute
resolution and of course in midweek
scenarios much more complicated than
some days in Maui where it's almost said
so what the f is something like 20
minutes pre-processing time 1
millisecond time which is like 1200
times faster than typing Dykstra in for
the sunday scenario interesting needs of
repossessing time is lower but the fully
time it doesn't pause it very much about
its problem we do have is that currently
takes a lot of space is so about 12% of
the edges are time dependent but as I
said this is probably quite realistic
because there's a dental streets and so
on don't have a lot of traffic anyway
yeah so the problem will I now have a
sense a memory consumption would be too
big to extends is to say all of Europe
because the memory goes up and up these
that address get more and more
complicated to travel time functions but
there are lots of opportunities of
low-level compression which would give
us effect of 2 or 3 and you can also use
approximation of the travel time
functions before I the first experiments
about that the telecenter should also be
is the factor of 2 to 4 that you can
and if you do it in the right where you
still get exact rules edge which is
probably not so important for patience
but the input is approximate anyway
we also have a very computation
implemented which is relatively easy for
the contraction I mean I guess note
ordering you don't have to do again and
again every day because the basic
hierarchy of the network remains the
same and then basically you can contract
and pedal at any independent set of
nodes you have to be careful about even
the known ordering you can paralyze by
going away from this priority keyword
basically computing is a priority queue
priority values in the benched manner
and then just picking a set of nodes
that have some sufficiently independent
from each other they should have a
reasonably big distance on each other
and have float biology and then you get
something that's very similar to the
sequential approach and we see something
like 6.5 speed-up on an eight-core
machine and I guess you can paralyze
that are distributed memory machines
with more implementation effort but all
of these are basically local searches in
a huge graph I don't see a fundamental
reason why you couldn't scale it even
more with your bigger networks I mean
this is all for Germany what we want to
do is do it for Eurasia or so so to
summarize I hope that I make clear that
static routing by now is an easy part in
order to do it very efficiently there
are nice applications that
require massive amount of routing and
there's still interesting otherwise we
are too fast anyway I mean or just
giving directions in Google I guess you
need something like 20 to 50
milliseconds anyway to display things
and woods and engine agencies it's one
so whether it takes 100 microseconds or
200 microseconds to computer route is
not really interesting but the
applications like these logistics
applications wide sharing and so on
ray would likes it this is important and
time-dependent about and you have also
demonstrated I think quite clearly that
you can do it also other machines which
is basically the Google application I
guess the algorithmically interesting
seeing is that we can handle by directed
by directional type difficult search the
very fast clearly is reasonably fast big
computations our only remaining problem
space consumption but as I said they are
relatively easy waste so maybe more
interesting for us what do we want to do
in the future
where we are currently looking at is
various waste work with multiple
objective functions so there is two
important interpretations of system -
that you want to have the user choose
certain preferences like oh I am driving
the slow track and then maybe the boots
would be different than if you have a
sports car or you say oh I would like to
have an energy-efficient hood and then
things change right and the other thing
is multi criteria where you say whether
I mean I want to be fast I want to be
energy efficient but also it should be
an easy route and I don't really know
which one is more important for me then
maybe it's good to have several choices
given that are reasonable and that's
classical multi criteria optimization
actually for a time dependent travels
what we want to have as our objective
functions and just travel time and
mathematically you get something very
similar to the multi criteria thing
because now you still have to take
travel time into account because that's
what defines where you have to evaluate
your travel time function and subjective
which is then in your knee already a
multi material problem and probably
np-hard so that should also be addressed
and then the other things like a
integrate individual transportation and
public transportation I mean when you
say I want to go from cars were but the
street where I live to the Google office
and CFE then this thing should tell you
well walk to the railway station take a
train - comfortable - take a direct
Lufthansa connection to SeaTac Airport
then take the bus blah blah blah right
and in particularly Europe when you want
to have
Travel there are dozens of airports
which has strange connections but then
you can travel cheap other than always
going to Frankfurt and currently listen
before for a vacation of Laguna I spend
three or four hours researching all the
possibilities it would really be great
to have said in an integrated fashion
that they tell you well fly to
Montpellier rent a car later then drive
to your place something like that
another very challenging thing we want
to do this a traffic simulation which on
the first glance is something more
scientific you want to find out how
traffic spreads over streets but we
won't actually want to use that for
really realistic
travel planning information when you
have traffic jam I mean the problem is
currently if you have a traffic jam then
there are certain heuristics which
decide is a true avoid it goes somewhere
else or or just stick to it sticks to
the motorway right and if you decide to
avoid it you you know one was for sure
there will be another traffic jam on the
detour route and it's not in the
database and you somehow have to
estimate how bad it with be and what we
want to do years use game theoretic
approaches you want to conclude
something that's called a Nash
equilibrium
maybe simulate what all the agents is a
street system do and say
whenever anybody can pick a better route
and he'll do that assuming complete
information which is unrealistic but
maybe a good approximation and then what
will happen is basically that if you
have a bad traffic jam on a motorway
then the traffic will spread around it
there will be several alternative routes
which always have a mild traffic tram
some people will stick to the motorway
where the traffic jam also gets less
severe because get away from it
so basically of several other
alternative routes which are all about
the same
that will be the national continuum
assumption situation and then the good
plan I can just tell you oh look there's
these words they are all probably
equally good
so that pick one for you all you want to
pick it yourself do you mean assuming
that everybody has you know a GPS device
or something in their car so they all
know about all these routes yeah so
that's a modeling assumptions with your
buzzer lots of traffic sciences tell us
that even if you don't have a GPS I mean
a big part of the travel as I experience
I they know there's a traffic jam their
feet weeks and if I take the primary
detours and there will be an in Reverse
traffic jams so I take a different and
therefore this is an approximately
Quebec such so it doesn't this model say
it's just as good reload on everyone
else's detours stay on the original
route in to be as good as everything
else I mean
I mean statistically that's what you
should do with in the absent of
different information right but I mean
this system can tell you whether these
are the reasonable detours and then you
can make holistic decisions like okay
which one of those are the people most
likely to prove no then I pick that one
and then made you see as a traffic limit
any less things like that and also it
depends on your personal preference
aside some people say it's so
frustrating to stick in a traffic jam
I'd rather make a big detour better than
I can tonight which is sometimes too but
this is traffic simulation you at least
have a chance to model this in a
realistic way in contrast to just saying
oh we have a traffic jam here around it
everything is fine right so at least it
will more often tell you stages or stick
to the motorway because secondary
interventions let's something we'll want
to look at and they're just
computationally really challenging
because now you have time-dependent
level you have to simulate maybe
hundreds of millions of individual
routes and you also have to model this
stuff which is not our path but actually
the traffic scientist says they have
data sources and models that allow them
to get reasonably realistic sense of
trips and basically what you then do is
you have measurements also telling you
traffic frequencies on various hot
streets floating car data etc and then
they can calibrate the parameters they
use for their write generators to get
reasonably
traffic patterns yeah I don't want to go
into much more detail so that's maybe a
good place to stop
I have two questions can you hear me if
yes so can you go back to slides to the
paralyzation yeah so you said you can
paralyze it but there's I mean in
contraction hierarchies you are changing
the graph so you need a global graph
somehow so if you paralyze it more
that's not so easy right I mean the
current mobilization is shred memory
like if you have a single graph yeah
exactly
yeah if you have shared memory is that
you're updating an independent set so
you don't have a problem but is it this
and then when you go to distributed
memory there two stages first stage is
to say well it simply replicates it
evolve and as long as the graph fits in
each individual node that's fine when
you'll be on set and you can say oh
whether maybe let's petition the graph
into a couple of chunks and work on each
chunk and parallel and and only at the
borders of the chunks you have to do
some communication but now we went late
how can you how can you just replicate
the graph I mean then one note might
make changes to the graph which might
affect local searches of another node
isn't that so
yeah of course I mean you have something
like a community communication around
yeah right so in nature in the beginning
of each detonation you have a replicated
version of the graph then you determine
an independent set of nodes this
computed over the distributed memory
machine inch process of contractions
independently
and produces shortcuts new edges and now
these new edges are communicated in a
also all broadcast of evasion or
gossiping affirmation okay but but it's
not a huge communication for you because
of just the new shortcuts so that
shouldn't be a big problem then you
synchronize nice again insert the
received edges into your local copy of
the graph and then the invariant that
you have a replicated graph is to
wouldn't complain about that we now that
requires fine-grained communication
right that's always a problem you don't
wanna really I mean you have single
gossiping operation in each of these
stages and in this implementation it's
about for the contraction I think
something like fifty iterations right so
you have 50s and colonization's here
that's not a because I'm able to do our
cloud that's distributed over the whole
glue to probably be on a cluster of
machines that have reasonably fast
communication and for the two
distributed memory implementation of
course you want to contract the nodes
that are in your local part of the graph
and then you only have to catch some
part of the graph for the local searches
that's as a boundary of your piece of
the graph and that will work fine as
long as the graph still not to not
contract too much then you contracted it
say by ninety percent you have only ten
percent left and then maybe it doesn't
work anymore but then the graph is so
small that you can go to the replication
version that such things as pattern you
should do of course it's more
complicated to implement than just our
share memory
implementation but there shouldn't be
fundamental problem
okay but you haven't done you haven't
done a distributed implementation yet
you're just thinking about it
okay yes oh I have another question if I
may
yes so if and you mentioned this min/max
label thing for a time independent that
was very sensitive now as far as I can
see this works well as long as the range
is relatively small when the range
becomes huge it doesn't work anymore
right and this is what will happen in
dimension public transportation there
you will just get huge ranges sometimes
you can get there in two hours sometimes
takes you 12 hours if you go overnight
or something and as soon as the ranges
are big this doesn't work anymore you
are probably right I mean we did the
whole track networks and it worked
surprisingly well I wouldn't have
expected that when you go to product
transportation you probably have to go
for a different compromise between
profile search and min/max label search
namely something like you subdivide your
time into I don't know 24 intervals or
so and then for each of these intervals
you do a min Max and then maybe you
things work better but of course I don't
know and I mean over this now all over
guys Paragon is now implementing
contraction hierarchies in a time
dependent gray area it's not just switch
on our code and it works you have to do
a lot of stuff and it's unclear yet but
at the end it works my feeling is that
maybe it will not work all the ways will
not contract all the way through but you
should at least be able to get rid of
any fairer pieces of your network right
you have residential areas with just one
bus as that with 20 stops or so that is
contracted away similar for railway
network pieces and are not well
connected anywhere so ever get
contracted and then maybe you get rid of
90 percent of your network then you have
something where contraction it
eventually doesn't work very well
then you do combination with
goal-directed techniques something like
that you may know more about this and
implemented wonderful it may be some
leverage
oh yeah yeah yeah I'm writing up the
paper sending soon I'm looking forward
here
no more questions thank our speaker</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>