<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Hashing Searching Sketching. | Coder Coacher - Coaching Coders</title><meta content="Hashing Searching Sketching. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Hashing Searching Sketching.</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/EdsyTzUk1A8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everyone it's a great pleasure
to introduce veena I know Lena's since
my undergrad days from iit bombay he
then went on to MIT to do his PhD but
decided to come to the area after his
master's so he worked in cisco for some
time and he's been interacting with
rajiv group when he was at cisco at one
point he formally decided to go on
Andros HD and he just recently
completing his PhD from rajeev or 20 at
stanford in theory area and he will be
talking today about hashing searching
sketching I guess we are all interested
in all of these techniques yes you know
thanks for giving me this option to talk
here at Google I'm going to be talking
about some recent results on hashing
sketching and searching so as we know
search has become a huge industry in the
last few decades and there is a wide
variety of data with data type that we
want to search on there is there could
be a simple search in a database where
you have an input key and you're looking
for something it exactly matches that
key an exact search but in other cases
the search can be more complex for
example if you are searching for similar
documents or similar images or blogs or
even geometric shapes or news articles
if you have an image search then the
input key which is the image is not
expected to exactly match an entry in
the database so there the the search is
more fuzzy the similarity is defined at
a more intuitive level in in more
complex structures the data could be
hierarchical so it's not just a flat
structure the fuzzy search we're looking
for something similar is often captured
as a nearest-neighbor search by mapping
the object into a mathematical
abstraction the most common underlying
techniques for search are hashing and
trees now when you have a search which
is not an exact match let's say looking
for
image search then one common method is
to take the data object and map it
represented in a mathematical format for
example as a vector in high dimensions
so every image will become a point in
say the Euclidean space the way you do
that is by let's say dividing them image
into quadrants and then looking at the
average color / quadrant and that's how
you get a feature vector and often the
number of dimensions that you get is
very large when you take such an object
and map it into a into a mathematical
format and then when you get a query
point in that again Maps becomes a point
in that high dimension liquid in space
and then the similarity search ends up
becoming a nearest-neighbor search so
you're looking for the point in this
database of points which is closest to
the query point now such a problem of
high-dimensional search suffers from
what is called as the curse of
dimensionality which means that the
problem is hard as the number of
dimensions increases which means either
the search time is going to be
exponential or the space requirement is
going to be exponential so finding the
exact nearest neighbor in high
dimensions is a very hot very hard
problem and that is why algorithms
researchers have settled for an
approximate solution to the nearest
neighbor search problem an approximate
in the sense that you are you are happy
to find a neighbor which is within
constant factor of the distance to the
nearest neighbor say for example at most
five times or two times a distance to
the nearest neighbor and this is often
okay in practice because the the near is
the similar the most similar image is
likely to be much closer than the other
images or the matching similar documents
is likely to be much closer than the
other documents probably closed by a by
a constant factor
now a point in a high dimensional space
is only one of the mathematical
abstractions that can be used to
represent such objects but there are
many other if you take if you take a
vector you can either take the Euclidean
distance or you can take other distances
for example you can take the l1 norm or
an LP norm for different values of P and
depending on which norm you take you
will get a different notion of what is
similar and what is dissimilar a
slightly different notion so you'll be
careful in picking up picking your norm
or you could use bitmaps that is another
common representation and then use the
Hamming distance between them or you
could use strings which is a natural
representation for documents and then
use something like the edit distance or
you can have sets you can look at a
document as a set of words and look at
distances on sets for example the set is
the symmetric difference or you can use
the earthmovers distance the earthmovers
distance is is a distance measure on
assets between sets of points and it is
given by the smallest amount of motion
that you have to perform to transform
one set into another and trees are a
natural representation for hierarchical
data now often there are there are ways
of transforming one metric space into
another so all such mathematical
abstractions with a distance measure can
be thought of as a metric space and
there are ways to transform points from
one metric space to another and they
tend to be distance preserving that
means the distances are almost preserved
so if you have a nearest neighbor search
algorithm on one metric space let's say
the Euclidean norm then it it translates
into algorithms for the other norms as
well so just an algorithm are four
nearest neighbors on one of these metric
spaces seems to help in finding
similarity search among all kinds of
data objects by transforming them to
these other metric spaces
and then there are also ways of of
converting any of these abstractions
into a bit vector into a very short bit
vector which again tends to preserve the
distances and this is called as
sketching where you can take a big huge
object such as an image or a document
and convert into into a few number of
bytes with small number of bits so the
Hamming distance between this final bit
representations indicates how close the
original documents or images are so I'm
sure you are all familiar with this in
when you have we are looking for similar
documents you probably know that it is
it is possible to compress a document
into a few bites and still tell by
looking at the few bytes which documents
are similar to this this document and
that is called as sketching so as we saw
sketching is the art of taking large
complex objects and compressing
compressing it into a few bites or a few
bits so that by looking at these
sketches you can tell how similar or how
far apart the original objects are a
bloom filter is a type of a sketch of a
set so it takes a huge set and then
compress it is compresses it down to a
small representation and can be used for
approximate membership queries so you
can tell by looking at that short
representation with high confidence
whether or not something is a member of
the set and it's very useful in storing
a large set on chip or sorry all in
memory and and answering with high
confidence if if an element is present
and if you want to perform some action
if the element is present then only if
if the the on-chip computation says yes
then you go go off to the disk and you
do the full computation a related notion
is locality sensitive hashing so you
probably all know that hashing can be
used for an exact match where you take
ki and you use a hash function to map it
onto a bucket and then if an element
from the database is present in the
bucket you will exactly find it in the
same bucket but there is a method to use
hashing that can be used for approximate
search for nearest neighbor search and
that is called as locality sensitive
hashing and the idea there is is that it
had tends to hash nearby points to the
same bucket so it's likely to hash
nearby points to the same bucket and
it's unlikely to hash far off points to
the same bucket so slightly different
from sketching sketching Maps similar
objects to the same bitmap sorry it make
maps similar objects to similar bitmaps
bitmaps which have very small distance
between them whereas locality sensitive
hashing tends to hash similar object to
the same bucket or to the same value so
they are there are similar notions but
not exactly the same and then the
techniques of hashing and sketching are
also commonly used in what is called as
streaming algorithms a streaming
computation involves in streaming
computations you have an incoming stream
of objects large number of objects and
you want to perform some computation say
for example finding the most frequent
element in the stream and the constraint
is that you don't have a huge amount of
space to remember all the elements and
that is why you need to you need to
store a summary of what you have seen so
far concise summary of what you have
seen so far it's just just enough to
make that computation say for example
finding the most frequent element and
that is why techniques of sketching and
hashing are commonly applicable here so
as we all know even for exact match
hashing is one of the most classical
techniques it's very easy to implement
and the most common problem you run into
is collisions when many items hash to
the same key so the object the hope of
course is that your hash function is is
random enough so that different items
will hash two different buckets or the
distribution of items into buckets is is
about uniform but then if you run into
these collisions then you have to
resolve them say by using linked lists
and that increases your search time
because you have to traverse over the
linked list so you can reduce the number
of collisions by increasing the size of
the hash table but then that takes space
so there is a natural space-time
trade-off here now the most common way
to analyze hashing is to analyze them in
terms of balls and bins where you think
of the bins as the buckets and the balls
as the items and the hash function
essentially takes each item and maps
around to a bucket but if the hash
function is random then you can think of
it as you're taking an item and throwing
it or into a random bucket if you use if
you use a pseudo random hash function so
what you're interested in here instead
interested in here is how even is the
distribution of balls into bins or how
even is this distribution of of items
into buckets now this framework has been
analyzed in detail in algorithms
research and it is well known that the
maximum bin if you take n balls and if
you randomly throw them into n bins you
take each ball and throw it into a
random bin then the maximum load is
given by login over log log n that means
this is the length of the longest list
the maximum number of collisions
now it turns out that you can make a
huge improvement in the distribution of
bins by making a small change to your
hashing process small change to this
balls and bins process the way of
throwing the balls and that is as
follows instead of instead of taking a
ball and throwing it into a random bin
you pick two bins at random interrupting
one bin you pick two bins at random and
throw your ball into the smaller of the
two bins so just that small change gives
rise to a lot more even distribution of
the balls into the bins in fact the
improvement is exponential the maximum
load drops exponentially from about log
in to about log log in or another way to
understand this is look at the
probability that a bin has a certain
load what is probably that have been has
a certain load I in the earlier case
when you're when you're throwing a ball
into a random bin the probability is ill
exponentially small in I it's about 1 /
2 to the power 5 but now if you use two
bins and put in the smaller of the two
it becomes doubly exponential it becomes
1 over 2 to the part of the bar I so the
probability of finding exceeding a
certain load drops exponentially drops
by huge amount so the the distribution
is lot more even now let's try to
understand why you get this exponential
improvement why is the why is it that
just by taking two bins and putting the
smaller of the two you get an
exponential one would think that you
would get a quadratic improvement or
something like that why is there
exponential and the reason is as follows
let's try to recursively compute
the fraction of bins with a certain load
I let's call that pea I now how can you
ever get a bin with load I plus 1 when
you are using when you're doing these
two balls and putting it a smaller when
you're just using these two bins and
putting the ball into the smaller of the
two the only way you can get a bin with
load I plus 1 is if both the bins that
you chose have load at least I if even
one of them had a load of i minus 1 or
smaller you would put it into the
smaller one and so you will not create a
bin with load i plus 1 so the only way
you create a bin with load i plus 1 is
if both those bins that you chose have
load I which means the probability of
that happening is about VI square and so
the fraction of bins with load I plus 1
which is P I plus 1 is about p i square
so this gives you a rough recurrence on
on how the POW p I grows and if you
start with VI P of 0 is P of one or some
constant and if you keep chasing this
recurrence then you will get that p1 is
is half squared and then p 2 is half
squared squared and p3 is half squared
squared square and so on so forth so by
the time I becomes about log log n you
will find almost no bin with load of
load of log log n so that is why you get
this exponential improvement any
questions here because you really need
to understand why this is you know why
this is so much better than log in does
anyone have any any doubts about this I
mean it's it's a lot better when you
look at the numbers if you look at if
you if you let's say you want to find
what is the probability that a bin
contains five balls and if you plug it
into the old scheme you will get that
the probability is about 1 over 2 to the
power 5 which is one over 32 but if you
try the new scheme
the probability is 1 over to the bar
will barf I which is 1 over 2 to the
power 32 which is less than one in a
billion yes exactly so that has been
well studied and it does not become log
of log of M so it stops at two logs but
you get a denominator you get a log log
n divided by a log D so you'll get
slightly in decreasing denominator so if
the probability here probability here is
about one over 30 then here it is about
one over a billion let's look at some of
the results will see in this
presentation we will see a new result on
hashing with exact match using a variant
of this balls infants processes will
show how to get about eighty-five
percent space utilization of the hash
table so we will show how you can get a
maximum load of actually a constant
maximum load not even log log in a
constant maximum load and we'll show how
you can you can arrange you can perform
inserts and deletes and you don't even
have to resort to linked lists for for
resolving collisions and you can still
maintain about a very high space
utilization will also mention result on
bloom filters which improves upon the
classical bloom filters by a small
constant factor in terms of space
complexity and then we look at hashing
for nearest neighbor search which is
locality sensitive hashing and we will
see a variant of locality sensitive
hashing that uses very small amount of
space and we will see how ideas from
from there can also be used in in KD
trees which is very commonly used data
structure for similarity search and
we'll also might also look at some lower
bounds or four nearest neighbor search
using locality sensitive hashing and if
time permits we will
look at some algorithms for sketching of
hierarchical data and some results on
finding frequent elements in a stream so
first let's look at the result on exact
match using hashing so as we saw instead
of instead of hashing an item to a
single bucket if you hash it to two
buckets and put it in the smaller of the
two then the distribution of items into
buckets is a lot more uniform the
maximum the bucket containing them with
the maximum number of items will have
about log log n items and you can
actually and then every every search
will look in both the buckets that an
item ashes to so every search will
involve to reason and you can actually
paralyze your memory operations by
dividing the hash table into two parts
and then parallely reading or writing
into both the paths now note that since
your search is looking at both the
buckets that an item hashes to each item
could could reside in any of the two
buckets that it has to so even if you
initially placed an item into this
bucket because that bucket was of the
smaller size in future if the size of
that bucket increases you always have
the option of moving it to the alternate
bucket and things still work because the
search is always going to look at in
both the buckets so if you can exploit
this fat and perform such moves then
maybe that can result in a better
distribution of items into buckets so
the ID yes just make sure I'm clear it
you would have to rehash that object
right because the two buckets are linked
by themselves yes so you you can find
both publicans by just hashing that that
object here you're right but the basic
concept is that just because you place
an item in in one bucket you always have
the option option of moving it into its
alternate work and what we show is that
indeed it's possible to get a huge
improvement by performing such moves
during inserts by performing such moves
you can actually maintain a maximum load
of two items per bucket that means the
distribution is so even that most of the
buckets will have exactly two items some
of them will have one item and some of
them will have none so what this means
in terms of hashing is that you don't
even have to implement any linked lists
because you can pre-allocate two items
per bucket and with high probability
you'll never experience an overflow yes
so we'll show that it's possible to pack
more than an items into n buckets I just
curious how you're getting more than how
much to the bands would have two objects
with still same number and some keys or
other items and buckets Oh actually it's
closer to n items it's actually go it's
me show that we can pack 1.6 are n items
into n buckets so if you pre allocate to
/ yes how much work you have to do each
time you see would be a nice thing yeah
so let me get that so every time the
number of moves that you perform is
constant in expectation and with high
probability the number of moves is no
more than log log n and then we also
have risen will also see result where
what if you are only what we only want
to do one move what if you don't want to
do more than one move then you will see
that you'll get a slightly less lesser
improvement not as not as good as this
so you by performing such
moves during inserts you can get a very
high space utilization or the hash table
by pre allocating to location for two
items per bucket and the space
utilization is about eighty-five percent
and of course every read involves to
every lookup involves two memory
accesses and so so far inserts and you
don't need any dynamic allocation as I
said with high probability log log n
moves and constant and expectation and
this idea has been used before button
but not with slightly weaker results in
cuckoo hashing by fog and roller yes but
is that with high probability one as a
two items per bucket with high
probability you will never experience an
overflow so if you try to answer at one
point one point seven items 1 point 7 n
items into these 2n location for 2n
items with high probability you no
experience at all so this was actually
inspired from work on on routers where
you are searching for IP addresses for
forwarding so you have a hash table of
next up next up IP address and your
during an exact match and there it's
very critical that that your memory
bandwidth is is low and your parallelism
is high and also you don't want to do
much allocation you know dynamic
allocation that's where this this this
work came from and then as you said the
overflows there are ways to take care of
those overflow in that context by just
keeping a small on-chip cam for a few
entries it turns out that's not more
than 20 or 30 entries
so let's try to understand let's go to
analyze this this arrangement of balls
and bins and understand how these moves
occur pictorially as a graph so the way
you view it as a graph is you think of
the bins or the buckets as nodes as what
it sees and then you can think of a ball
a ball is picking two bins at random so
that becomes an edge because you can
just put an edge between those two bins
that it shows so each ball can be
represented by an edge and then you have
to take the ball and put it into one of
those two bins and that choice can be
shown by an edge so the balls and the
bins and the bench oils for every ball
can be represented by a directed graph
and whenever there is an arrow pointing
into into a node that means that
contributes our load onto that node so
the total load on a bin is the N degree
of that bin and what is the move let's
say you're trying to insert a let's say
you're trying to insert a new ball into
the top vertex there then you have the
option of moving this this ball from
that vertex to this vertex because
you've chosen either of those two
vertices so you always have the option
of moving it and what what that
translates into is flipping the
direction of that edge so the vertex you
the load of the vertex you decrease by
one and the load of the vertex V
increased by one and you can take the
new insert and insert it into e into you
and you can actually cascade such a
sequence of moves let's say try to
insert a new item here you can take
this you can take this ball move it from
this been to this bin and then take this
separate ball move from this been to
this man and this wall from this pen to
the spin and then perform the new insert
here so that leaves the load of all
these three what is is unchanged and
only the load of this vertex increases
by one that means even though you
perform the insert into this node its
effect was felt only by this node yes we
analyze it for a graph I believe it
should not be is it should not be hard
to extend it for my divorce so in
general you can perform such a sequence
of moves along directed edges while
going backwards so if we're trying to
insert a new ball into this node you
perform a backward search by following
directed edges and find the least loaded
bin I'm all among all the bins that you
can reach in this fashion and then
perform a sequence of moves let's say
for example you find that the least
loaded bin was this node then you can
perform a sequence of moves and then
perform the final insert in you and what
that does is essentially puts the load
of the new insert into this least loaded
been you into this least load bin V so
it suddenly increases the the space of
bins where you can essentially transfer
the new load so earlier you were just
putting in one of two bins now you can
actually choose any of these bins to to
take the effect of the new load yes
easily will you was
and you don't have any edges yet
well yes for example if this bin is
actually empty if you look at this bin
although there is an edge here the load
of the bin is zero because the pointer
is finding this way that means the this
ball could have gone either here or here
and was chosen to be placed here so the
empty bins are the ones which have zero
in degree so that's the insert algorithm
you you hash your yes either search
every detecting every edge takes two
hashes yes yes so just to look at this
graph you need something like 30 hashes
you need yes you're right it turns out
that it's about login hashes what do you
mean so we the depth of the tree that we
look at is about log log in in with high
probability so in expectation it's
constant which means is with high
typically you will find an empty bin or
a bin with load at most 1 by the time
you just search at level 1 you have to
look at children of the top model most
of the time you just have depth 1 and
then once in a while very rarely you
will have to go down further so that's
the search algorithm you hash your item
into two buckets into two bins and then
perform such a backward search by
traversing directed edges and find the
least loaded bin that you find and
perform moves so that the load of that
least loaded bin increases by one and
what we show is that it's possible to
maintain a maximum load of two so you
can keep performing inserts always find
a bin with load at most one so that when
you perform the new insert the lord will
become at most two so the claim is that
you can always find a node with load at
most one by performing such a sequence
of by forming such a breadth first
search along directed edges now let's
see how many how many items can you pack
how many atoms can you keep putting so
that the maximum load is still too
so to analyze that observe the graph and
ignore the directions for a moment now
the graph is a random graph because
every ball is picking two bins at random
so every edge is placed randomly so the
graph is a random graph and let's try to
see when can an insert operation fail
that means under what circumstances will
you fail to find a bin with load at most
one the only way that can happen is
you're performing such a search a
breadth-first search backward search and
you never find a bin width bit load one
or less that means every bin that you
find has load at least to the only way
that can happen is you start with the
top node and that has in degree 2 so you
have two edges pointing in and then both
the nodes that you came to both of them
have in degree too so they also have two
children each and so on so forth so you
always find two children that means you
are finding this this complete binary
tree embedded in this graph while you're
performing this search so the only way
you're your search can fail is if there
is such a complete binary tree embedded
in the random graph so the question
becomes how for what values how many
edges under what conditions is such a
complete binary binary tree present in a
random graph what is the ratio of the
number of edges to vertices so that this
does not happen and we actually compute
the probability recursively the
probability that such a deep binary tree
is ever present in a random graph that
probability can be computed recursively
in terms of the height in terms of the
depth of the tree and we show that as
long as the as long as the number of
edges two vertices is less than
and 1.67 as long as the edge to vertex
ratio is no more than one point six
seven there can never be a binary tree
of depth more than log log in that means
as long as the occupancy is less than
eighty-five percent by the time you
search to a depth of log log in you will
always find some node with load at most
one and that's why your insert will
succeed and that gives us the
eighty-five percent space utilization
any questions and then I also mentioned
that you don't have to you you you don't
have to be willing to perform a large
number of moves yes how tight is the
human in terms of any radiation um can
you reduce that length and still have
mastered of two exactly so that's what
I'm coming to you can you have a
trade-off between the number of the
depth of the search and the maximum load
so if you are willing to perform a
search up to a depth of log log n then
you can maintain maximum load of two but
even if you are willing to perch to a
depth of one you still get an
improvement over nope not performing any
moves at all so you're performing a
depth of H upload depth of H then the
maximum Lord is given by this about log
log and over H and log of log log n over
H so even if h is 1 this is about log
log n / triple login which is
asymptotically better than just picking
two bins and putting it in smaller to do
yes
I think the same question that you
mentioned that there are no movie
villages which means that we assume that
no two balls are hashed into the same
ferry okay Liam sorry sorry I think I
got confused many hearts instead of
Mardi Gras um we we do a lot multi
graphs so you are dual our parallel
edges so that's not an issue at all it's
it's very easy group in fact the
analysis is for allowing such edges at
allegis yes at love could force a bin
load of three with his fuse to binge
five objects bins will sit we can we can
only transfer a ball to another bin if
there's already an answer there yes so
no matter how big our bin space is bad
luck can force us to use a small subset
of that as we're like well that's only
if there are lots of edges in that small
subset so i can give you 100 bins and
then say oops the dice forced everything
into these two well the probability of
that is extremely small as we say yes
it's small but it's not always well what
we show is that the probability if you
have n balls and n bins or you have 1.6
and balls and n bins then the
probability that you would this will
fail is something like 1 over N squared
that means if n is a million or a
thousand there is one over million
squared so of course it can you're not
saying that it can never happen but if
you look at the expected number of
overflows as I said in the example where
you're willing to place an internal cam
the expected number of water flows is
about 20 or so I mean even 20 is with is
extremely unlikely so just by pairs
placing a small on-chip cam you get a
much more efficient search search
component yes oh how i did this day go I
think this oh this big ol in the
analysis is not very tight but the but
just for two balls and bins it's it's
very tight it's actually log log n plus
constant so log log n plus 0 of 1
in the theoretical analysis is not very
tight but I am pretty sure in practices
it's very tight in all these balls and
bins processes very often there is no
multiplicative constant there is only an
additive constant so that means even if
you perform a small number of moves just
performing one move will give you an
asymptotic improvement and one move is
is very easy you just have to take the
two bins and compare it with each of
their children and then as I mentioned
these all these methods of multiple
choice hashing can also be used to get a
better construction of bloom filters
which which beats the classical bloom
filters by a small constant factor I
won't mention in the details so having
looked at hashing with exact for exact
match let's now move on to hashing for
nearest neighbor search which is called
as locality sensitive hashing any
questions before we move on
so as I said before it's possible to
take or object search objects and then
map them into some mathematical space
for example Euclidean space and then you
are performing a nearest-neighbor search
in this Euclidean space and you're not
insisting on finding the exact nearest
neighbor but you are okay getting an
approximate nearest neighbor so we are
let's say we are looking for a see
approximate nearest neighbor where you
are happy to find any point which is
within at most three times the distance
to the nearest neighbor or another way
to look at this is think that your
nearest neighbor there is a huge gap
between the distance to your nearest
neighbor and do the other points let's
say there is a factor of C gap in that
case finding the sea approximate nearest
neighbor is same as finding the actual
nearest neighbor who's much closer than
the other points and let's say you know
what that distance you know the search
radius let's say you have some estimate
on that search radius let's say you know
that your nearest image has to match
this image on at Lisa many pic pixels or
you have some estimate of that sort so
let us assume that you know that have a
handle all that has to on that distance
on that search radius now one of the
most traditional methods for finding for
performing nearest neighbor search is by
using KD trees so it's exactly like the
classical tree except that it partitions
the space along the different dimensions
so first it will partition along the x
axis and choose half the points here and
half the points there and then choose
another plane along the y axis and so on
so forth and gets a tree off about log
in depth now it's if you analyze the KD
tree performance in high dimensions even
even slightly higher than even in
dimensions which is slightly more than
two or three it turns out that the
chance the probability of finding the
nearest neighbor or even an approximate
years never drops a lot as you increase
the number of dimensions so even by that
by the random number of dimensions is
three even then the it's it's quite
possible that you will not end up
finding the nearest neighbor as you
perform the walk down the tree and that
is because since you have so many
dimensions since you are walking along
so many of these cutting hyper cutting
planes it's it so happens that the that
the query point and its neighbor gets
split by this by this cutting plane
especially if the number of dimensions
large so let's see how can hashing be
used for performing nearest neighbor
search is there a way of hashing these
points so that nearby points 10 to map
to the same bucket and one way to do
that was shown by indic motwani which is
which is as follows you take a random
hyperplane and you take periodic shifts
of the hyperplane and you take another
random hyperplane with the random
orientation you take periodic ships of
that one and keep doing that and that
will essentially divide the space into
cubes and if you look at this
partitioning and if you look at two
nearby points nearby points have a much
better chance of falling into the same
cube as compared to far off points so
this is the hash function you think of
each cube as a bucket yes gets
normalized I want to understand in how
many time you mention still do this well
it turns all that you do need about
locket dimensions remember many
occasions when is the number
of coins and at our points the polyline
you do this login times and any
initially mentioning to how many
sections do you drive well these
sections are conceptually there are an
infinite numbers of such hyper planes
but most of them will be empty how many
will be filled well not more than n for
sure is those endpoints and the periodic
the periodicity in the world the
interval length is given by that
estimate off so if you think of our is 1
then this is one you choose this length
in such a way that the nearby points
that means you and your nearest neighbor
are likely to fall in the same in the
same interval and far-off ones are
unlikely to fall in same means if you a
quarter sir 10 so that's what I'm going
into that the problem is actually very
small the probability of nearby points
you and your nearest neighbor falling in
the same bucket or same cell is around n
to the power minus 1 over C where C is
approximation constant so if you're
looking for a to approximate nearest
neighbor then the probability is 1 over
square root n but if you take far-off
points then the probability of falling
in the same bucket is about 1 over N so
there is a huge bias for nearby points
falling into the same cell so it's so
the probability is high but it's not
it's not like constant it's not three
quarters or one off it's it's more much
more than the probability of far-off
points falling into the same bucket so
as I said the probability that the query
point and it's near the nearest neighbor
fall into the same bucket is about n to
the power minus 1 over C so in order to
amplify this probability you can use n
to the power 1 over C such hash tables
independently and then perform your
look up in all of these and then with
high chance you will find your nearest
neighbor so the space complexity is
about n times n to the power 1 over C
and the query time is about ends bar 1
over C now this division of the space
using hyper planes is only one possible
locality sensitive hashing shun but
there could be others the only criteria
that a locality sensitive hashing shun
ship needs to satisfy is that nearby
points are likely to go to this are more
likely to do to go to the same value as
compared to far-off points so formally a
locality sensitive hashing shin is
defined by if you have two points which
are separated by a small distance then
the probability that the hash to the
same value is at least above some
threshold m and the probability that far
off points go to the same hash value is
small it's smaller than some threshold G
and what in dec motwani showed is that
if you have such a localized locality
sensitive hashing then you can construct
a data structure four nearest neighbors
search with space n to the power 1 plus
Rho and search time n to the power rho
where rho is given by a function of
these bounds on the probabilities and
then if you analyze you perform this
analysis you plug it into this framework
the the hyperplane based locality
sensitive fashion you can actually
compute these probabilities and it turns
out that the row is about 1 over C and
that is why you can perform a search in
time n to the power 1 over C and with
space into power 1 plus 1 over C any
questions here
so so basically they have they make n to
the power 1 over C copies independent
copies of such hash tables now then the
new results that we show is that you
don't need to have so many different
hash tables so think about think of C
equal to 2 what their method is saying
that you need space of n root n you need
about root n hash tables if n is large
then this can mean a huge amount of
space what we show is that you don't
need so many hash tables it's sufficient
to just work with one hash table you
work with one hash table but the
difference is that instead of searching
for the query point in the hash table
you look for random points in the
neighborhood of the query point into the
ash table and you do that many times and
what we show is that with a good chance
you will find the nearest neighbor of
the query point so instead of searching
for the query point you take a ball of
radius R around the query point and pick
a random points on this ball and then
search for those that in the hash table
that means hash those points and go to
the bucket where they go and look for
and see the distance to Q among all
those find the nearest the pointy ears
to the query point so instead of
searching for Q you just perturbing q by
your peers perturb ink you buy a small
distance and looking for those points
and yes we have to do it many times so
the waiting time is about n 2 bar to
oversee that means have to do it about n
to the power to oversee times so let's
try to analyze how many how many points
will have to pick in this ball how many
perturbations of Q do you have to search
in this hash tables so that you can find
the nearest neighbor maybe that on the
bar in the ball on the ball so what
happens if your nearest neighbor was
your query regretfully what happens if
your nearest neighbor was in fact we're
you are right well for the analysis we
say on the ball it doesn't matter if
your talk about high dimensions when
you're picking random perturbations it
can be shown that most of the points are
if you if you are talking about a point
at distance R if you have the handle on
the distance R and most of the points
are on the ball exactly on the way if
you know if you have an estimate on the
distance to the nearest neighbor so
let's try to understand how many points
you'll need to pick on the ball so q is
the query point and these are the cells
that you get by your hyper planes and
the nearest neighbor is some point on
that ball because it is that distance R
from the query point and it is
essentially a random point of that ball
why is it random point because these
hyper planes have random orientations so
you can think of the nearest neighbor
has some random point on the on the on
the ball so how many random points you
need to query till you find some point
in the same cell as the nearest neighbor
it turns out that too has to find the
number of points in a to query the
number of points is related to the
entropy of the cell where the nearest
neighbor p lies given the query point
and the hyper planes which is intuitive
so it it's it's intuitive of that that
what the amount of mission information
is the entropy of the cell of the
nearest neighbor and it's ready to read
to this quantity it's actually
exponential in this quantity to
understand this more clearly just look
at this lemma we are essentially asking
that you have a random variable which
can take some of us some values from a
certain distribution and how many times
do you have the sample values from the
distribution till you hit upon the
correct value of the random variable so
for random variable has a you
perform distribution let's say it can
take n values with equal probability
then the number of samples you have to
take till you hit upon its correct value
is about n more precisely is exactly n
log in with high prob n log in samples
with high probability you will get the
correct value by a coupon collection
argument but then if you look at the
entropy of that distribution then the
entropy is log in so the number of
samples that you to perform is seems to
be exponential in the entropy of the
distribution and that holds in general
if you have a non-uniform distribution
on a random variable to guess it's
correct value to guess its value the
number of samples you have to make is
about 2 to the power entropy of the
random variable yes you have a question
so here you are trying to guess which
neighboring cells the nearest neighbor
Eliza and by that lemma that i just
showed the number of samples you to make
is 2 to the power the entropy of that of
the cell value of a random point on that
ball and so you can actually estimate
the entropy you can actually compute the
entropy and it turns out a buck it turns
out to be about twice login oversee and
if you plug it in you get a search time
of about n to the power to oversee so if
you sample so many points you will find
the nearest neighbor with high
probability and you can actually
estimate some the space requirement in
practice if you have million points you
need to store only one hash table and in
you don't even need to store the entire
object in the hash table it is
sufficient to store a sketch of the
object because the sketches are
sufficient to tell you how similar or
dissimilar it is from the query point
and the sketches are very small it can
be done in about eight bytes per object
and so even with with million even for a
million objects the space requirement
comes out about to be about five
megabytes sorry five bytes per object so
tons of comes out to be about five
megabytes which is
a small amount of space for nearest
neighbor search so we also get an
alternate formulation of the locality
sensitive hashing where instead of
instead of stating it as probability
bounds on items hashing to the same
bucket will say we in fact put a bound
on the entropy of the hash value of the
nearest neighbor given a query point and
we get an alternate formulation where
the row is a function of that entropy
and the probability bound on far-off
points and the search time is about n to
the power row and the space is linear
now the same algorithm of that with the
new algorithm that we saw on locality
sensitive hashing can in fact be you
even be used on trees on KD trees so
even if you have a KD tree you get a
huge improvement by by not just by not
searching for the query point in the KD
tree but by by searching for points
random points in the neighborhood of the
query part see if you just take if you
just take a traditional KD tree in D
dimensions and take the query point and
search for the nearest neighbor there's
the probability that you will not find
the nearest neighbor is very high the
probability of finding the nearest
neighbor is drops exponentially in the
number of dimensions it's about e to the
power minus D over C now this is for a
random database of points if you take a
database / points randomly distributed
in a unit cube in D dimensions and if
you plant a nearest neighbor which is at
one over C to the distance to the other
points then the probability that you
will find the nearest neighbor is is
very small as number of dimensions
increases so even if C is 2 and D is for
the probability is about 1 over e square
which is one-tenth I mean
ninety percent of the time the KD tree
search algorithm can fail in finding the
nearest neighbor but if you just make
that small modification of not just
don't just look for the query pond
itself but look for many points in the
neighbor or the neighbor of the query
point and search for those then with
high chance you will find the correct
nearest neighbor and how many points you
have to choose in the neighborhood of
the query point it's about e to the
power t over C so if this probability
was one tenth what we are saying is you
just take ten random points take what we
are saying is take ten random
perturbations of the query point and
search for those in the tree and then
you've got much higher chance of being
successful in finding the nearest
neighbor and that is actually verified
by using simulations even in three
dimensions if you choose C equal to four
thirds then if you take a typical KD
tree search algorithm then in 25 with
twenty-five percent cases you will fail
in finding the nearest neighbor and that
can be amplified all the way to ninety
percent by just making five you know
taking five random points in the
neighborhood and that probability the
failure rate even drops is even higher
with KD trees if you take five
dimensions then the probability of
finding the nearest neighbor is about
fifty percent for this value of of C and
that can be amplified all the way to
ninety percent by choosing more points
and it gets even better for higher
dimensions so it's just a simple small
change to the KD tree search algorithm
but gives a much better result result in
terms of finding the nearest neighbor
and you have to you don't even need to
modify all the code base all you have to
do is call the search routine a few
times with points which are variants of
the query query point any questions
now how am i doing on time okay okay
let's not let me just go over this skip
over this lower bounds this is actually
essentially saying that recently there
was an improvement on locality sensitive
hashing for nearest neighbor search in
Euclidean dimensions where the row value
was improved from one over C to bond one
word C squared and the weighted did that
is by instead of using hyper planes
based partitioning of the space they
used spheres a spherical partition of
space into into buckets and that gives a
much better improvement and the reason
it gives an improvement is because the
spheres are the smallest surface area
for a given volume and so the chance
that nearby points will get separated by
biosphere is small but what we show is
that a lower bounds which match those
upper bounds so what we show is that
these are indeed the best possible
partitions of space for nearest neighbor
search
and besides that let me also mention a
result on sketching algorithms for trees
you some of you might be familiar on
sketching algorithms for sets if you
have a large sets there are ways to
perform a sketch take a small sketch by
using what is called as a min hash you
hash all the items and you check the
value of which has the smallest hash
hash value and what we show is there's a
way to generalize that to two trees
where the objects are not just arranged
in a flat set but where the objects are
arranged as a hierarchy in a
hierarchical fashion as a tree it's
possible to generalize the ménage
algorithm to get sketching algorithms
for four trees so again conclusion we
saw some spacious space efficient
algorithms for exact match using hashing
by using balls and bins and we saw how a
result on bloom filters how balls and
bins variants can be used for getting
bloom filters and then we saw nearest
neighbors search using locality
sensitive hashing and we also mentioned
some algorithm sketching
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>