<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Feature Selection Through Lasso | Coder Coacher - Coaching Coders</title><meta content="Feature Selection Through Lasso - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Feature Selection Through Lasso</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/R159wnY4AQU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you it's nice to be here
so let s see where the UGI talk to this
26 people so can find out who you guys
are are you from machine learning break
one engineering background Oh statistics
some statistics and eng sub machine
learning looks like a base ok so some of
the spirits is many constitutions so
bear with me so I'm Marcia see this is a
the technical part of the talkers join
work with my former student hangzhou who
end up with finance couldn't get him
into neuroscience but i'm hoping he
would donate some day he went to see
today so um this is something from the
view of inside statistics and see where
all the data are nowadays and the first
part I think you guys Mary familiar ways
she's but very young unconventional from
the traditional point of view in
statistics information retrieval and a
lot of things happening here and the
second part which I don't know how much
you guys do here is actually statistical
machine learning also getting into some
weary system stuff like chip design
program debugging and also network
tomography and most of people I think
outside the IT area see in statistics
really seeing the data impact from the
science point of view like technology
motion sensing astronomy in your science
and finance as um said I have my group
it's really a kind of at core doing
machine learning and statistics but
everybody has a different application
area so how five students working on
five different area but the hope is that
statistics is kind of a hub can kind of
trance up and Paul different ideas and
see common mathematical structures so
how better idea would states to be
dealing with in terms of methodology so
this is another inside view from
statistics
this kind of new form of data for
traditional institutions an image video
and sound multimedia and you see a lot
of high-dimensional this and that in the
stitches or community already but one
thing I think the two station haven't
dealt wish I think machine learning so
far as in relatively students high
dimensionality the Allah aspect in the
practical level I think it's coming but
not as much emphasized it's a high data
rate where you basically have to do
streaming data a lot of the machine
learning techniques won't be readily
usable and the plug prostituted here
that I think statistics I see machine
learning as the frontier of statistics I
mean all the time happening in CS
departments but as a part of the
scientific discipline is very much the
statistical dealing with IT data and if
you look at the vision put up by this
and as a blue ribbon panel 9203 they
brought out this comes of cyber
infrastructure and if you do Wikipedia
and they will point you to them say that
is a concept defined but as a panel it's
called a science in Britain and there
they and also the 2020 a science report
by a group of scientists organized by
Microsoft really doing kind of sending
the same message which you said the
technology has come to a tipping point
that the computing technology have been
integrated into science in much greater
detail or more similar than jizz
application and core and for statistics
to be part of the infrastructure I think
we need to worry more about storage
database like communication a lot of
times i'm working on sensor networks as
well now you can now say i don't worry
about communication with data
compression because that's can be all
competing for the same battery power and
computation and now I'm narrowing down
to the technical part of the paper is
that feature selection is kind of
something of statistics and feel like we
can study more relative on the side
because it appears in a lot of the
stupid errors which meeting the IP
challenges if you have good feature
selection then you
kind of do better storage and you can do
feature selection then it's easier to
communicate and the future selection
also has this aspect of giving spark
state space model which is very much
from the scientific point of view you
want to have interpret interpret models
rice machine learning have been doing a
lot of good prediction work but if you
work with newer scientists they really
want interpret models not just good
prediction it's a first step but you
want models which can be interpreted and
made sense of all of them of course need
a fast computation so I being involved
with a project with JPL for acted clock
detection based on multi-angle satellite
images and there is an example that
kudos diffused a very simple measure
from 1921 features quadratic
discriminant analysis end up being our
choice of a model of processing because
it was streaming data we couldn't really
run SVM it is too heavy and the
accuracies didn't really improve too
much with SME i'm either and but there
you basically have in two years and
years administering knowledge that's
where you have this qualitative
knowledge in the community and we
basically had the role of quantifying
this knowledge and we have three very
meaningful features elections which is
very easy to interpret it's not just
something a mission learning mentor
turned out however this is another need
for my automatic feature selection for
newer areas or there's a times constrain
you couldn't have a graduate student
professor work on something for five
years you are forced to look at this
automatic feature selection and feature
selection was kind of studied in the 70s
when people instituted recognized maxima
liquid estimation was kind of the
popular mess at that time had this
problem overfitting if you like the
maximum like Lisa like say a predictive
model that and now is the largest why'd
you only look at inside training Arab so
people recognize that you need to do
some penalty and there are different
ways of dealing with this over feeding a
Kiki a Japanese engineer proposes
people call a key keys information
criteria but he insisted it's our
information theory yeah information
criteria and ideas is that can you
estimate missing date using the data you
have of generalization error and he used
a parametric model and this on Cooper
grabber expansion and then say that you
should penalize your likelihood by two
times the amount of dimension where
suppose you know what model dimension is
which in primary case is well defined
and then 1978 shrouds ass efficient took
a beige in view and so suppose I put
price on my different models and then
look at the posterior probabilities of
these different models and again it's
Mathematica similar expansions then I
will penalize the one dimension by log
in page the sample size again in the
classical domain you know what a is you
know what dimensions and MD I swear I
did a lot of research is from coming
from Shannon theory and reasoning who
used to be at IBM and really uses south
coding idea try to form a line Occam's
razor and BAC can be look at this one
form of MDL by the other form so i think
mvl is more general however all this
different early version model selection
criteria led to a comment or research
and computationally is not feasible so
today i will concentrate on this
automatic feature selection which is
computationally feasible it's a convex
optimization problem and i'll consider
its radical property suppose you want to
replace the model selection using this
lawsuit how does it work in the ideal
situation where you know there's a true
model and does it really get you the
right model and the second part is on
something relating solving lovesu
problem in approximate way and related
to boosting 1302 now the either boost
just to give you a sense how hairy this
commander convert our research can be
through the earlier work with a former
student we're looking at gene selection
yeah man MDM framework we
want you to do simultaneous gene
selection and sample classification and
we started with six thousand genes and
use pre-selection very simple minded
that we'd like sure to get down to 100
or so suppose now you want to fit some
linear model we say possibility hundred
terms and all possible subsets will give
you 10 to the power of 30 possibilities
well just not doable on the other hand
if you really think about it we only
usually the data right now from my
career's like a hundred you know 100 to
200 data points if you think that think
about it you actually you shouldn't be
doing this many even computations are
feasible you shouldn't be doing this
many subset comparison anyway help
others can you have with 108 upon tell
apart 10 to the power 30 possible subset
right so there's six question now we're
trying to hammer a little further is
that there's this computation constraint
but in the end I'll try to push for the
idea that actually sometimes a lot of
time now with slightly has a
computational constraint actually work
to our favor which means brings some
necessary regulation and this is a case
that you don't want to search that many
possible subsets anyways you just don't
have the information there to tell them
apart and let's move on to what people
have been using recently as a
replacement for this comment our
research I mean we did this comment our
search for small p large n right it's
not i was not done classical statistics
we have nine predictors and we had a few
hundred one people do search over this
let me know too many possibility p small
even our consumer two men you have supra
predictors just a few possibilities so
people do that but not become impossible
it's very this so-called large p small
and a scenario and this lasoo which i'll
tell you is a convex optimization can be
rotated as a embedding of this discrete
optimization problem into a continuous
optimization problem and one connection
people made earlier before us was
through boosting and i will call
boosting a third-generation computer
measured in statistics well machine
learning probably shouldn't be there
become machine learning is really sir
generation already so statistics if you
look at the development scenes feature
about like 90 years ago that's where it
is really formed as a discipline then
the first couple decades people working
on closed form solutions because we
didn't have access to computer people
doing tabulations of T values and know
that but very limited and we rely a lot
of nice chromatic families for closed
form solutions are possible and around
the seventies or sixties computer became
available so people now using computer
but didn't touch the optimization
routine which is called routines which
the numerical analysts already set for
convergence criteria to do the
optimization right there's a separation
of statistical procedures say you prove
us important amenity for max my cute but
you don't really worry about how it's
being solved and that was fine for a
while and then now recently for the last
15 20 years people started realizing
that a lot of a large data sets you
couldn't just design a criteria and not
worry about computation because it won't
be implementable and therefore
computation has to be brought into the
design of sisian message right up front
instead of an afterthought and machine
learning that's one of those very active
areas and also mcmc in some sense was
already pushing the Bajan car down
forward by worry about computation now
finding a feasible computation measured
but now of course for like data set mcmc
is also hitting some roadblocks and
boosting against most of people here
probably heard boosting yes I'm actually
so boosting is you know the history and
in the beginning people really puzzled
by the improv the eddies ever see the
statistics there was a lot of margin
like using empirical process result and
try to explain things and from cities
upon you people fell like was not
tightly now is only upper pond so start
by leo breiman very much people
searching for other kind of explanations
and if you look at boosting promise to
use coupon you is kind of unconventional
cetacean have always been doing a
nonparametric statistics by choosing a
smoothing parameter like smoothing sply
things like that but we didn't really
change the data and boosting stands out
because this idea use a very simple
procedure you're not Union procedure but
you're changing your data eugenio data
and non-free the same procedure and now
your linear combined these different
estimators and you achieve better
classification and we experimented with
boosting early on which peter human we
work a few things together then if you
start with a very strong leonard like
with its projection pursuit and then of
course doesn't improved so the empirical
success was very much related with
relative week learning lies cart 0
decision trees and another
characteristic of boosting is that you
use validation data which is available
for the IT data which for a lot of
science data it's kind of hard to
separate thing because it's very small
then you stop iteration for why was kind
of mysterious forcing at least from
station point you maybe not to the
people who proposed them until this
gradient descent view of boosting
started by leo breiman and then
messenger doll and fremantle who turned
boosting into an optimization framework
and suddenly the connection with
classical statistics became very clear
so people can now read arrive either
boost by optimizing exponential loss on
the margin which is upper bound for the
0 1 loss function and the convex and
it's very nicely right you to greeting
at convex loss and friedman which a
professor stanford recognize that what
for regression problem and natural log
function 02 and then what do we do if we
do gradient descent on l2 then you end
up just the waiting disappeared right if
you look at the boost you have to relay
the data points and you modified it here
you do modify our data but you don't
have to wait anymore because the special
property out to
you simply take the residual from the
first fit and I use that as a new
response and then you feed it again so
you do modify our data by you know
example anymore everything's do you have
the same probability and now it's a
familiar domain you can see that
repeating of residuals was proposed
already in 72 by two key in the time
Syria contact he only didn't watch he
didn't keep doing it Oh using
cross-validation but this is connection
the other thing which really intrigued
me was this fact of the coupling of the
numerical parameter number of iterations
with regularization if you look at
boosting there's no smoothing parameter
but the regularization is controlled by
when you stop and that's controlled by
cross-validation so suddenly computation
parameter number iteration usually we
don't even worry about you know
optimization package becomes a smoothing
parameter a regularization parameter and
that got me and Peter Peter beulah
really interested we just also at the
time there was this mysterious or
resistant to overfitting which in the
community people kind of puzzled why you
keep adding more and more terms however
you don't seem to overfit okay i'm sure
you're what you do but you do slowly
fuse this equation but if you use the
sequential version yes so but though
it's a lot of talking about mr. like
resistant to overfitting at a time so
now people have to move down you know
you're not allowing IV to a different
loss function you if you do either boost
you look at the exponent I definitely
over fit but 01 you don't see it as much
and then we did this work calculate it
very simple casing out to case you can
calculate the variance and in that case
you can see that you're hiding more
terms by unlike linear models you're not
the complexity is not linearly in the
number of terms the complexity is
awesome told when you have more more
terms you're adding tiny bit complexity
so in that sense you cannot simply count
the terms complexity is measured by our
parents where there is 0 alpha your
protection okay
so that's what in l2 that you have this
nice bias and variance decomposition so
that's the right measure at least for
the l2 loss of course complex is really
two coupled with the last function but L
2k that's why we like to l2 because a
lot of things easy to calculate it
doesn't give the whole picture bellies
brings about the possibility that they
say that you cannot confirms that just
know the right way and now with the
understanding from the point of greedy
and they're not different versions of
boosting if you soak all the people
again the statistic community I mean if
you take either booze you basically do a
gradient and then you do a line search
you find the direction among other
possible predictors or covariance and
then you do a line search and go there
and there's a revised version that is
very boring idea from a shrinkage in
statistics line search may be too greedy
so it's I think what's Friedman's idea
Jerry Freeman again saying that you
shouldn't do the line search you should
take a big step because you doing
shrinkage and just do more cautious
steps then if you do that you have this
called the forward stage wise feeling
which is a discretized version or
gradient descent with a fixed step so
you're doing component wise gradient
descent but then you don't take a full
greedy line step you do a fixed step and
this way its physical research right you
allow yourself to go forward backward on
the right and on the left in two
dimensional case and then they observed
in this tool for a no statistical efrain
HTTP shawnee and the jury in johnstone
they are observed this property of
boosting with lasoo now i'm all in
regression so moved away from other
books because mathematically it's easier
and no statistics pretends to care a lot
about regression and let's sue which is
I should tell
into l1 panelized 02 minimization okay
la soo has this known sparse property
just similar to hinge loss in support
vector machine so you the formulation of
course if you killed pick the duo form
of these optimizations that minimize
your minimizing our to Los subject to l1
penalty ol1 constraint and most of the
time you end up your solution add up on
the vertices okay and then you have
sparsity so you do recognize asian the
same time you're doing variable
selection because when your solutions on
the vertices then one of them is zero so
you have sparsity and so that's no and
in the orthogonal case this is
equivalent to Selfridge holding and it's
not a connection with Vivid thresholding
work of Donald I'm johnstone so the
sparsity is very much attached to the l1
optimization and then they observe that
this is a data set of ten variables for
some tidy from some diabetes covert
setting some patients and what I'm
plotting here is called a path plot so
what I'm happening is that if you for
the l1 penalization I can change the
size of this diamond ok for each size of
the diamond I'm going to have a solution
right so what I'm being what's being
plotted here it's my change the
constraint because you can show that
constrain the solution happens on the
boundary ok of the constraint then you
can have 10 different covers in the
beginning a lot of a 0 and then
eventually they'll pick up when you have
no constraint at all or you already
include the l2 least square solution
that you end up with the d square
solution ok so this is called the path
depends on the regularization amount in
the beginning very lot of regularization
everything is false 20 when how very
tiny tiny diamond and then things
eventually you include the solution the
noun constraint solution you end up
without energy squares and this plot is
in danville IL to boosting you know
fashion that you write your iteration
for each distribution you can calculate
your l1 Nome of your solution and then
you plot you two are now plotting the
iteration number you're plotting the
hell right now so that these two will be
comparable and then there's a lot of
similarity it's very the sparsity
pattern they seem to be picked up around
the same time in the same order and then
they show that in the paper that under
certain condition these two should be
the same if you have the the boosting
step going to zero okay and I already
told you about that traditionally in the
beginning when the suits proposed people
thought well we have the l1 constraint
out to minimization it's a quadratic
programming problem there is a
well-known currently programming
efficient method and therefore latches
for different lambdas let's get a
solution using that package and then we
do maybe I'll agreed of lambdas and then
we find the lambda which give the best
cross-validation error so the smoothing
parameter is liked it however if you
think about it there's a waste
computation to even this is a lot more
efficient than the convent our research
for different lambdas the solutions are
very close but the way you're doing it
because no opening of the optimization
package you are not taking advantage of
that you let the program solve it again
and again so it's no more expensive and
is now needed it's a means better than
comment served by eaters to waste basted
computation so now let's first look at
the problem I started by saying that la
soo is a nice kind of convex embedded
problem for the commentary search so
let's ask the question suppose I have a
very nice problem the model is actually
true right of course in your life is
never the case but that give you some
sense where things could break down
suppose have a linear model y equal to X
beta that's the most attention notation
and now I have error term and I'm
assuming that I this in statistical
design matrix and put all the relevant
predictors the first q columns and the
rest to the x2
the rest columns okay so the first q
columns are the relevant ones because
the corresponding betas are now 0 and
the rest are the noisy or irrelevant
predictors because the beta is a zero
okay this is where I do set up and later
right now making Q and P fixed and later
I can relax that that p go to infinity
and qo to go to infinity with n and
there was an earlier work by night and
food in two thousand showing that will
enter the standard conditions of unity
approval alternative square have nice
properties than the LA Zoo solution is
actually in the altitude sense
consistent to find you the two
parameters ok that's a traditional way
of statistic understanding different
message to see suppose the data as I get
large do you find the right answer okay
that's kind of a desirable probably you
can argue that but that's one thing we
can measure how good a master these well
is you want to pass a benchmark if with
lots of data you don't find the right
one seems to be worrisome however if you
think about our to distance you can have
tiny tiny estimates but if you look at a
sparse pattern then it won't be matching
so we went further with its work asking
the question of model selection
consistency we said we want the selected
la sua selected nonzero ones to match
the true nonzero ones and we want that
to have high probability and this is
different technical definition that
worried about it so the the take home
message here that actually is almost
necessary and sufficient condition for
you to be model selection consistent so
let's look at what I have there so x2 is
a matrix if x2 is only one column and if
you remember what alternative square
solution is that will be the
coefficients of regression if you now
use x2 to regress on the true one so it
measures their dependence and then x2
you reach the matrix therefore the first
part this part is a matrix and now you
multiply by the sign which is a vector
so that you end up
vector and this lesson wise observe a
Leo component wise okay so it's you
can't guess if you think about
everything notes linear model you know
that if the two if the predictors are
two correlated in the worst case if they
are totally determined linearly
dependent there's no way you can tell
them apart especially with noise however
without doing that the derivation is not
that hard it's just I think most
thinking about doing it that to know
that why the sign shall working you
really have to get the details and y is
less than what not less than half that
you really have to get a detail
everything is now alive so you have to
look at it d detecting the derivation
but the idea is now where is pretty
furious tick in the sense that you don't
want the irrelevant ones to be very
correlated with the relevant ones and
this is like a trace transition why it's
kind of the boundary went below that you
ok but when you go about that you lost
that what the selection consistency ok
and they know this can be generalized
this same thing derivation can be done
if you assume that p also goes to
infinity with n and it's a sample size
and your sparsity now means your
possible to true model dimension grow
slower than the sample size otherwise
not really sparse but the possible
number of prediction you can look at can
be explained n te n so you can take a
huge amount of of predictors and you you
won't be confused this is also
asymptotic right but it just give you
some sense that actually if you believe
the word it's kind of sparse and then we
actually can have get a lot out and
there's a lie of several research we
started earlier than we did in the
deterministic standing by Donahoe and
who started very early they were coming
from the signal processing kind of angle
to ask a question without noise while
you can tell too sparse systems
part in some kind of wavelet stuff but
without noise and then more recent
version but donohoe's work dealt with
noise by deterministic so these people
pursue the lion research assuming that
you have like 2x and had determinative
if snow and asking the question one the
l1 the LA Zoo solution will be close to
the l0 solution and see our two cents so
they don't directly address the question
of when the nonzero ones will match but
it's a nice line of work and the circus
exciting my house and then bumin did
they follow a bulging graphical model
case and then we started working like
that from the linear model so they came
first they actually had that condition
missing in the first version and that's
why we start working on becoming shot
there's something not right there and
then they discovered that themselves to
that the me said condition so so now
they have it in the published version
imagine me right made this oh this
condition written Q and P like mole and
the most special light conditions up to
the constant here I'm just only older
I'm not hiring by the constant so that
was more refined after our work and the
my house and human work because the sign
you don't know right what's the true
model so it's an easy just say let's
forget about that look at this quantity
okay and that's a sufficient condition
and then you can show that if your
correlation is a constant between
different variables or decay
exponentially like in time series
usually you think that yesterday and
today are more correlated than today and
they are before yesterday okay you have
some decay or you have a compounded
correlation all of these can be verified
that the condition actually satisfied so
a lot of the typical simulations
independent case actually come from this
class is actually that you won't see it
you wouldn't see the condition violated
however if you simulate from this ID you
know golf just to see that this actually
happen but in high dimensions the
probability of this condition satisfies
you know this is the sparsity gets worse
and worse but even started reality good
when the sample size and PR similar you
only have twenty percent chance this
condition being said this is just to
show that
this violation does happen it's now
something we assuming something actually
always happens you mean this very
trivial case however right now we have a
new more recent work relax this
condition saying that when this is
violated then you won't get exactly the
right once you have a few more but still
a suit there's a very helpful reduction
of possible number of prediction from
exponential to something like edgy break
so you include a few more but it's still
useful but you need something because in
the worst case one thing is linearly
dependent there's no way you can tell
them apart ok so it's and respect this
is saying that for why we saw out la sua
it's not so good but now I think the
suits ok it's like this up and down this
low coverage information well the things
for prediction point of view you could
do that but for four principal component
analysis you have a bigger problem is or
the random matrices theory for large
data set principal component vectors
cannot be trusted so you can do some
other dimensionality reduction but then
you might do some interpret bility
because you will be mixing up different
predictors the goal is to the petition
then how do we care about we hope you
don't get interpretation you don't hear
right the two things which are kind of
first even you don't care about
interpretation principal component might
not be the right way to go because in
large dimension kv so the random matrix
is real your principal components might
be messed up it's very unstable so
there's compensation or is like if you
want a model where when you're actually
using model live you don't have to keep
drawing out a lot of features from
whatever your day yeah it's useful don't
yeah that's the cost issue right do you
want this is selection stuff if you each
features would cost you money and then
there's a definite interpretation assume
they're so now coming back to this
similarity so in we are basically having
in mind the p going to infinity this
engagement is very large there's a lot
of dependence and so lets you might not
get the right things but definitely will
help so now what we do is for the same
data set as the last paper we added the
eleventh artificial predictor which is
highly correlated with three of them we
did a linear combination of the first
three and then we we added some
perturbation so they are not completely
dependent and then you can see that
boosting a lot subaccount very different
even the sparse patterns right for
boosting you won't see sparsity and you
very late and here you see sparsity much
earlier and of course is coming back on
it but there's a very stable range where
you're going to say that well this one i
won't want it because possibly the
Adamites already included question this
is a pure lessors it with an l2 penalty
term also at it because the death of
people do right no la soo never have out
to penalty knows who is always our wine
without to loss but I i think more
recent work ball the young can say more
I guess you know they have to LT because
of precisely because again you have
correlated predictors you'll have these
instabilities right well this is just
pure this is the the tiburon e96 version
of not sure yeah we will move talking
about something with some revision later
but right now it's just a classic that's
sooo with l1 pile now to and out
reducing so the point I'm trying to make
here is now releasing out to LA suits
better than now to boost you eyes first
so i think that drew is due out we don't
depend on the situation what i'm trying
to say that they can be different that's
the point i try to make here okay
if you think about our to boosting your
taking that out too large and then you
keep taking gradient steps and assume
it's a well-defined optimization problem
with a penalty and not all the gradient
descent step for the first term will be
beneficial when look at them together
okay so you can see that they can be
different while you're taking the
gradient descent but look at the
combined loss the compilers actually
increase even the first term will
decrease when we secrete the great
conversion of stage wide version of
gradient descent it's very easy to see
that yo I'll one penalty will only go
out by epsilon or go down by epsilon
depends on you know the next one is a
positive step or negative step okay and
so you really see that well when this
decrease is not going to observe based
then you might want you might not want
to take this that if you want the
combined loss function to be the
objective function and that brings to
the backward step and the way we came
out with the backward step actually was
not sur la soo was from x your context
where paul my former student was working
as an intern with a hedge for small
hedge fund in San Francisco and they
were doing some a are basically squared
prediction and we wanted to try some the
shrinkage method and as a boosting seems
the computation because in stop anywhere
instead of knowing the fixed amount and
problem is think that before they are
making money already ordinary square
remember that's the end of the pass full
of soup then you shouldn't you can also
show that imagine for boosting if you
keep going you and that with the
ugliness square too because just keep
going to the global minimum that you
shouldn't start from zero
computationally it's not very efficient
when I start from zero and boost your
way up you should just go back because
it's a pretty good point that's where
the company is you should just take a
few steps shrink a little bit and those
who from the times here contacts the
predictions should be updated every time
so if you only do forward step you've
stuck with a lot of the bad predictor
which you know the star might keep
changing so backward step
it's kind of needed for that reason to
also if you remember if you know
classical statistics forward models like
share and backward model selection right
so it's kind of backward needs to be
there and then in the end pwned
discovered that actually can derive the
backward steps through the loss function
of lasoon okay but we kind of really
like the backward step and and punk
really discovered that if you do the
loss function combined minimization of
l2 and l1 to agreed in descent you're
forced into a backward step okay so
that's you know where we started and
then you can show that you when lambda
it's very very big then you're forcing
the solution has to be 0 right because
you putting a lot of penalty on that so
I falls everything to zero because every
a little bit of fitting would cost you
too much so you can show that you can
start with lamb that is big basically in
terms of solutions start from zero you
won't lose anything because beyond that
the 0 is the solution and then remember
what I said the the quadratic
programming way of solving la soo is
kind of in fish in the sense of wasting
computation because you're not taking
advantage of them the neighborhood
solution for different lambdas so this
algorithm attempts to solve the LA Zoo
problem for different lamina you might
sweep in a fashion very similar to the
barrier mansard interior point
optimization right there you also have a
tuning parameter so you take advantage
of the previous solution and that's the
starting point for the next lambda and
we're also updating lambda along the way
and the details are things I'm running
on time I won't get into it's not very
hard at all we only need differences
because we all agreed we don't really
need a gradient actually and the reason
we don't need it is also because it was
doing statistical optimization I think
it's different from numerical
optimization I think people realize it's
to buy the tackling different way is
that when you look at the classical
optimization people take optimization
function as a deterministic function
very seriously right they want that
optimal
and that's what they want they want
convergence Newton everything because
they think that's something exact and if
you think of stitches optimization
everybody agree that if I lose two data
points my solution shouldn't change too
much right they'll be horrible if that's
so sensitive so you need stability and
you to chin chin to boost tragar data
you know surface will change and you
want to find the point where actually
when the surface change the point
doesn't change very much so you don't
want to go exact to the Optima of that
particular version of your random
function and if you go that in class
because the tens it doesn't hurt you but
it's wasteful computation and now it's
that massive amounts of data were busy
saying are we going to cure ourselves
together the exact optimal use the
computation for something else look at
more data right that's that seems to be
better than try to really get the exact
optimization and that's what all this
discretization really doesn't hurt us
especially we can tune the eve soon if
the data has high resolution then we can
just choose a small one otherwise if
Shawn can be pretty big and in terms of
prediction performance it doesn't hurt
okay so basically now you solve the
lambdas as a sweep and then you can do
cross validation to stop at lambda so
this become a liberal I this iteration
step this is there's a iteration step
two just like previous messages in the
loop when you fix lambda you try to
solve find the right beta and then you
move to the next lambda and this way you
have been taking advantage of the
previous solution and you're not even
doing second first I thought Barry
message was always Newton but somebody
pointed out to me does it have to be
Newton step and if you do that take a
very small staff what we call pls ooo
actually now the papers that's a little
story that we we send it to journalist
royal statistical society the first
version was like beginning of 205 and
all sperm who divides you know if not
exactly is in very similar agron as
large was the referee and we didn't
reference him and he just hit the roof
he was a full page report and we thought
with the last battle
so we sat down there for another year to
paper and use pones words all the people
I want to read the paper already read it
so she didn't really care and but he had
to graduate so you need to send this out
so we just sent to journal machine on
your research and took a while and I
came back and the rebbe fast what what I
was expecting the paper no that's not
this paper let's other no well you did a
good job no I have been sending quite a
few paper true right now that's not
that's the one no not this paper that's
fine which P turbulent sparks brunette
was passed this one that the I detail
will mention the name apologize the
first entering his lighter for the Latin
see review but I was okay no response at
every thief should read it forwarded
ready but up but the problems and now
the referee doesn't like the name wants
us to change the name it isn't like
boost is Ella sue and so that's kind of
sticky now because people already using
this and people and we think we're going
to try to say for historical reason you
just have to bear with us we can put the
disclaimer saying it's probably not the
fitting name but if you've already using
it so next time I tell me well you see
that might now be can't beat us to it
anymore nothing be that's one way to do
it I'm sinking just like stick it out
and say that well would you please just
people have been using it and I just I
was in Washington yesterday and
insightful it's very interested who does
as plots very interesting having this
being part of the package so I think if
they got in there may be some what are
they already calling this anyway that we
have to resolve and once you want to
point out this week live the Eggman is
very simple right you pick difference of
functions and it's very easily you can
define your function in a different
module that's what the inside was
sinking pal they're going to put a
friend and two will write some core are
codes and they were let people user use
a formula for my to putting their own
loss function and just call the program
because a program only needs a
definition loss function you don't need
derivative you don't need a spare
UT and so it will also works for hinge
loss other convex loss even the theorem
doesn't cover this case but in practice
run to prove the theorem we need some a
nice second-order quadratic bounding
from below and now exactly what somebody
was asking the question so if you do l1
you can do out like L 1.1 right and
things get a little smoother you lose a
little sparsity and you can do out to
that's rich and go to l4 and then this
is the more interesting case this is our
infinity so what happens I will fill the
pc max right so now you're asking that
you force your solution to be on this
line it's not in the heart it's a soft
thing right it just encourage your
solution to be on this line this line
means the beta 1 beta 2 are the same
okay and geometrically this is very
similar so this our infinity to do rl1
so these are fast algorithm to just like
large so homo topi for solving that
problem it's very similar so here it's
like you learn to be zero and then I'm
to depart here you put them on the same
equal line and 11 separate so just the
reference line so it is a different okay
it's the angle but it's you have far
second for that case and we've been
following our own days trying to pull
the group structures and they will
already work on elastic net and also
your ending for special cases we
basically we knew you liked the net but
we did on the other work we're doing a
more general friend so the paper is
still being written we had the patent
report but we try to risk our experience
with GS we now try to get everything
covered so let me just comment a little
bit about the difference between large
so homotopy method of oz per with
boosting Ella sue so it's very efficient
to get exact solution for these squares
problems and with a fixed number of
predictors events more than the size of
the sample and there are some extension
to analog expansion but you have to work
case by case because very efficient are
going to really take into account
special properties
your loss function so if you have
infinite number of predictors like you
want to search over a possible cut point
like in stumps then there's no large
okay the other thing is when you have
like what we started with times your
contacts it's very hard to revise the
large solution for the next batch
because now it is it's looking for the
pink points of the path you can show
that l 1 l l two parts will be piecewise
linear and boosting with some fig small
steps can deal with different loss
functions and now can do with the
infinite number of rain because you're
putting your toe for each step you don't
have the search of fixed number
directions you can search for an
infinite number of them you have in your
loop like a bearish method but the point
here is that is not always a good
approximation it's very good measured it
just not always give me you'll assume ok
it's made mainly like these are this is
like a local of suitors or paper by
Stanford I think the names a few people
that you basic and interpret boosting as
doing a local assume by now the
globalist and adaptivity also propagated
keep adding ones you've got you don't
realize how to revise and below sue has
the boosting pros and we do have
conversion to the zoo that's just you
can say good or bad but it just you know
it's what she's doing and for the time
series context I think it's very that's
where we started basically you need a
backward step to revise to adapt to non
stationarity and here's my general still
on stituted computation is different
from and it's the traditional and
optimization even the parametric case
there is a result in the pickled of some
text was a homework showing that if you
start with a root and consistent
estimator I mean the classic case you
can get out by mess of the moments in
closed form in many cases now you only
need a one newton step to get asymptotic
efficient estimator so you can do
modules in step but doesn't really help
you in terms of us ontology performance
it's for the same reason because next
time I change your data your surface or
change you don't need this
second order that's the most expensive
it's only needed by am stinky of surfer
is so perfect you want exact solution
and the number magic case it's really
the Wild West now there are so many
different models we can give you the
same prediction performance right if you
only care about prediction and then
other consideration shall come in to
choose which one to use computation
interpret ility and you don't to be you
don't have to be very precise again
there's so many different paths right
now that has to do this way and if you
do a discrete version of something
usually doesn't hurt if you saves
computation go for it it's a lot of time
helpful and you guys at Google so you've
probably all heard the story about the
machine translation right Peter
knowledge was in berkeley like two
months ago and first time i heard about
the google you i think this is a tag
computation last year google was ahead
with every other team right so first
heard it from your enemies microsoft but
they're pretty happy to have you guys it
seems like they said well we have
greater salary now so now the you know
there's obviously to the random people
the natural language processing they
seem to be taking a stride so the story
I heard from my friend I worked with him
we have some work out tell you about
later that it was at Google one they
speak contacts and he's totally like
search base which is the guys notice
tell but when Peter came over two months
ago I spotted are these two things he
did its basic statistical regularization
I don't know if one thing he said was
that in terms of try to save memory they
tried eight bits storage for the number
of for the probabilities of the
different words and they try six seven
bit 6 feet and forbids actually give
better prediction performance for me
it's regularization it's false on them
by memory space and another things they
only use the first five letters of
character five characters of award to
again for storage again give them better
result so so histogram pinning that's
against the computer
asian rich to helping you instead of
lighting if you have the computer they
would have got worse results so it's not
always bad to have computation
constraints Lester that's the thing and
that's my last so that's the summer you
know it so one thing I wanna emphasize
that the speedos who can be dealt with
different context loss and cons
penalties and the computation Mary
communication open help get better
prediction in the large datasets is that
with the computing power sometime we go
overboard because we have the computing
power and if we don't get a complex
structure right you know sometimes
really overfitting and the computation
concentrate on communication country
sometimes falls you to the better so
it's not always bad to have fewer
computers but i think that the
university people per saw that with the
Google approach in a losing them as many
computers as Google computer with
natural process well I saw Michael
Collins recently he still strongly
believe the syntax will help I think
that this this version Google did he
said the google we didn't win this year
right with it right I wasn't very clear
on that last year there yeah sometimes
among 15 of those icy but not as it like
like sweeping us last year so he Lieber
cirrostratus Brown pity first I thought
gotcha this really makes us this is a
totally relevant you just search but if
you think about it the search probably
has some stitches guide you in there how
to search right that you're computing a
lot of modeling into the search step
anyway so we're things how we are now
i'll try to use our infinity to building
group structures and in the high very
high level i would say this approach is
very much like semi-supervised learning
the sense that p is so large an
interactive small that you need to have
some side information coming either
through the formal semi soup who are
learning with class ring or through this
group
so this give you some knob to Putin
hierarchical structure and group
structure to make the problem facing my
well post and imagine database man now
oops joking this time I post out with me
he was a student peer bumin and we
basically relaxed the condition and this
is where we think it would be very
interesting try out feelers sue is we
sensor networks but data collection with
Intel tend not to be very difficult
because they have this policy of two
years for a director and called
disrupting research so when the new
director come on board they told you
change the direction so we will always
the direct Joe finds in time oh um sent
an hour then he stepped down and now
it's totally this kind of supporting
infrastructure rural areas in India and
it took the sensor network into a
startup and so my student was left with
collecting data himself and took a long
time but now we're trying to work with
the citrus group David color and then
Jim demo they do now have Golden Gate
Bridge wired with vibration sensors and
they have been aust putting all the data
to a centralized station analyze this
way but with they now wanted to disputed
analysis and communication so boosting
all below sue for that matter because
the sparsity you have fewer to
communicate and it's easy to adapt to
the non station energy so we're kind of
cured to see whether this one will work
an intern the beetle soup I've been
working with a neuroscience live on in
berkeley I've in telling Jacqueline who
is the boss they're like didja try
boosting they try everything SVM neural
networks what they try to do is they try
to relate a image natural stimuli which
is very know of in that they don't do
this kind of psychological analysis was
different a very active features team
and I they do nature stimuli and have a
single neuron you know I'm a cake to try
to relate what the macaque sees and what
in firing rate is and I've been turning
Jack that he'll try boosting but of
course he didn't listen he's have tried
everything
everything's the same and then his
postdoc this car were boosting by
himself he was just reading residuals
and then they liked it because give them
sparse models so I said why'd you try to
be to assume so they tried but then they
don't work because it's a very high
noise to signal case because you imagine
you have one new wrong and so much so
much other things going on if you see
anything you like it right now is that
neuron they do try to find a good spot
to see the neuro of fire but student the
noise is very hot so what's happened
there is that oh the forward steps are
the same and the danger doesn't support
the refining step of backward so you
gave the same exact it didn't hurt them
but didn't improve and then we tried
with skull at Redmond this project of
color input measured editor so you try
to convert a Japanese phonetic string
into characters I mean this guy used to
be microsoft research Asia I wasted
every summer and they are very much
especially asian languages and there
this is a reranking we took a michael
collins framework and i did the panel
tl1 panel so this is not really la soo
anymore it's the exponential loss with
l1 and in that case we do see
improvements for the for revision
because huge data set so the you support
this backward steps and in terms of
computation popular su in one sense is
expensive in a batch data compared with
large you have a faster acronym on the
other hand in the time series domain
actually large would be more expensive
so this was fast and was slow depends on
the context and where you start with
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>