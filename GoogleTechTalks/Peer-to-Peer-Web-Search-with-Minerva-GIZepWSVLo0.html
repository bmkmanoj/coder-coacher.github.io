<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Peer to Peer Web Search with Minerva | Coder Coacher - Coaching Coders</title><meta content="Peer to Peer Web Search with Minerva - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Peer to Peer Web Search with Minerva</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/GIZepWSVLo0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's our big pleasure to have gerhard
weikum here with us dear heart from the
Max Planck Institute in Germany and
zombie Gerhard is a big mover and shaker
in the Davis community and beyond and
I'm gonna start telling of everything
that he did my understanding of the
german university system is that when
you're a professor you have this little
kingdom and for your part of a team in
the university but then there is one
step beyond that's the max planck
institute so if you are keen at the
university I have no idea what you are
and
Mike URL for the introduction thank you
for your interest in this store now let
me elaborate on a lot of introduction so
there is a comparison what once has
another us and the answer is it's being
a professor in Germany but nds and
outside hotel on the earth is having
colleagues in your department or
Institute don't take notes here so this
is about getting a web search and an
ongoing project that we're working on
Nava at work where the whole bunch of
flood students and also some
Mediterranean collaborators so that's a
fun part of being involved in European
Union projects in at least these project
meetings and interesting locations like
release our role for example
so get furious is charming paradigm that
promises to be out from scalable and
self-organizing comes in different
flavors it has applications some of them
are for 20 applications what others are
commercially successful life by sharing
or enough for me and so the question
that drove us was is there any
applicator less useful than three good
at the same time actually because I'm in
academia the more important thing for us
was also is there something that is
scientifically challenging as opposed to
just doing engineering for some in order
to discover in the
so and I believe that the intopia wife
search could be such an epic tradition
this is why we're working on this with
the Managua project and we have a little
prototype system body so the vision that
drivers is that each and every same
power user like a graduate student or
scholar or journalist market analyst
they could have a power search engine
that's just once on their own computer
PC at home in the office or your
notebook or someone to PA and that's
good enough to have a little corpus
locally available say a few million
pages and that's probably enough in
order to cover most of your information
amounts so that could be compiled by
using technology like focus groaning
losing classification and so on and then
most of your queries would be answered
by your own personalized power search
engine most of the time but occasionally
the 10 million pages that you have our
it's not good enough or you're missing
some fresh result and then you should
talk to other search engines and this is
why peer-to-peer search is actually a
network of search engines millions of
them in the ideal case so this is the
vision why is it interesting to work on
this well it's also it's not a end by
itself but it's it's it's a test ground
for more foundational work on scalable
and self-organizing data structures and
algorithms you see a few examples but I
do believe it has a potential to boost
also search result quality and there's
three arguments along this route one is
that the lower per computer in this
invasion system is much lower than the
load per computer in a big centralized
server form like you guys have here so
that allows us to to run much more
powerful fancier techniques
mathematically more advanced techniques
that are computationally more expensive
or for example run deep natural language
parsing over
our text pages and represent the text
and a much richer way you would do want
to do this with billions of web pages on
your big server farm but with
publications in this network scholarly
network for example you might want to
invest these resources number two is
leveraging user input and community
input that could be explicit assessment
feedback or implicit behavior given in
the form of query logs Blake streams and
so on and this can be done at the
individual level or the community level
and then we're talking about the popular
term wisdom of crowds now this can be
done with a centralized search engine to
and probably it's being done but that's
some people in the world are scared by
this thought so and might have privacy
concerns and so in some sense a
peer-to-peer paradigm is a natural
habitat for doing these things because
at least in theory user can directly
control the extent to which this
information is monitored to which it
extended chair to which it extended
needs to be anonymised and they can
revoke whatever they allow the system to
do as I said in principle whether users
mass users are able to handle this is a
different story and number three is if
your local search engine is not smart
enough or doesn't have have enough data
you can collaborate with other peers so
you forward your query your information
demand to other PS that helped you along
these lines now each of these and tells
lots of research issues this is a
long-term vision so I'm not saying that
we have this running in a convincing
manner within a year or two there's some
sociological and political dimensions as
I said in my idealistic view of the
world I think the world would be better
off with lots of different search
engines smaller or medium scale search
engines maybe in addition to some big
ones that satisfy most of the mass user
demands but oligopolistic situations are
not considered healthy in the view of
some European minds so at this point you
should stop me actually and say well
this is interesting but it has nothing
to do with reality so is it
the solution that that solves some non
existing problems and indeed I don't see
a killer app with rich business well you
here but I think it's nevertheless an
interesting research direction one thing
is the technology may be interesting in
other settings like if we have
algorithms of the kind that I talked
about they might be applicable to server
farms as well maybe in some different
flavor and then there's some intriguing
applications that may not have business
values by the things they have society
well you and here are two of them so one
is a scholarly a search engine now
google scholar does a great job on
bibliographic data but it doesn't
address the content of the publications
and then actually we would like to go
one step further and represent the
content not just in text keyword form
but in more explicit knowledge form
extract entities relations and so on
make this searchable in the ritual
manner this is one direction and xml
could be a syntactic format for this but
there might be other approaches to and
you need then information retrieval on
that richer knowledge representation
here's just an example query which
should tell you this is not the kind of
query that google.com would would expect
to see the other application you would
be web archiving so I am NOT up to date
on the numbers at the of the page
versions that the Internet Archive has
that's a laudable efforts to archive the
history of the web but I'm afraid it
can't just keep up with a high pace that
the web is evolving at and on the other
hand the history of the web the older
versions that pages go through is very
interesting it reflects the born digital
content of the internet it reflects
culture and subculture and near-term
history so sociologists pollito lodges
historians are all interested in these
kinds of things even people like lawyers
in intellectual property rights are
interested in this like patent offices
are highly interested in the history of
when things were published on the web in
which versions and so on so of course we
would not just want to archive we would
want to provide also richer querying and
data mining capability is covering the
time aspects of this and here are again
two possible examples of where this
might lead to now this talk is not among
about any of these we're working on this
but this would be different talks so if
you're interested talk to me offline
okay so let's talk is about the
peer-to-peer aspect and one of the main
aspects here is a query routing issue so
when I have a query and my power search
engine is not good enough to answer it
to which other peer should i forward it
and this is the first part of the talk
then link analysis page rank style link
analysis makes sense in the setting to
but then it incurs extra complexities so
that's part 2 and if time permits I can
talk about some ongoing work that we've
been doing on personalization and
community aware ranking that's not
really specific to peer-to-peer but it
fits very naturally into this setting by
the way if you have questions interrupt
me anytime so the computational model
that we pursue is like this peers are
connected by some overlay Network might
be a distributed hash table a court or
pastry but it could be other topologies
as well and in addition we can talk
directly to pierce if we know their IP
address each peer has a local search
engine so it comes complete with a
crawler may be focused crawler index a
query processor and so on the peers are
autonomous this is a point I would like
to emphasize at least in this relation
where every professor every graduate
student in the world would have a power
search engine they want to compile their
own content maybe they produce a lot of
content too so we don't have control
over how the content distributed we just
assume the pier
have whatever they have that entails
that the peers might overlap in their
content and to some extent this is not
bad some redundancy might actually help
queries are executed locally first and
then we're out them if necessary to a
carefully selected set of other peers
we're thinking of routing it to maybe
five or ten peers not thousand or ten
thousand no massive flooding of messages
here so we want to be cost beneficial
and finally to achieve this routing we
assume that piers post summary
information about the content and maybe
also quality of service properties that
they can offer to the entire network and
this is posted conceptually to a
network-wide directory but that network
white directory can itself be
implemented by a distributed hash table
for example so it's perfectly
decentralized scalable failure resilient
and so on an important point to make
here is that this is different from just
using the complete text and index that a
full-fledged centralized search engine
would have and making it that
distributed the granularity here is
coarser than you would have in the
actual index the actual index your index
documents or pages web pages here we
index Pierce so that's several orders of
magnitude less the actual page or text
indexes would set on the individual
peers so that's important for
scalability this is the illustration of
this with our concrete system in lava so
we have the peers connected they have
the local indexes contents and indexes
they post information now here it's
organized in terms of searchable
features or terms in I our terminology
keywords in typical web search jargon so
for one term we have a list of peers
that have information good pages about
that contain these terms and this is not
just the the peerless but every entry
comes with some quality measures
statistical measures that tell you how
strong is at pier about this term how
good is its the content that it can
offer for this search term the and
that's distributed using the hashtag
right distributed hash table we can post
information about bookmarks so every
peer or the peers that want to post some
some subset of their bookmarks or social
tags whatever they have to offer can be
posted same way so here in one of these
directory peers which is just a roll all
the peers of also as directory peers so
here for you our LZ we know which peers
have that URL as a bookmark and then
when a query comes and it's issue its
issued here executed locally first and
then we need to solve that query routing
problem so that should aim at optimizing
some benefit cost ratio of measuring
taking into consideration p a content
quality content overlap freshness
authority and so on and i will give one
specific approach for dealing with this
the issue is much broader I cannot cover
all this and I don't have a good answer
for this or comprehensive issue anyway
so it turns out this is a decision we
make a query runtime when we see the
query and after having it executed
locally we think about which other five
PS we want to involve in that query
execution so it's already part of the
user perceived response time and that
puts stress on this what we can do at
this point so another approach would be
to pre-compute frequent good dis
frequently made good decisions so when
we realize so most of the time it makes
sense to send this query to the
following five peers so if we can come
up with some pre-computation of that
kind we can rewire the network or wire
an additional overlay Network that is
sometimes called semantic overlay
Network or maybe social overlay Network
and it would just make the best peers
that help me typically in my queries my
neighbors so then the query runtime
decision is sped up a lot because I only
need to talk to like a handful of
neighbors
okay so now the principal form for
making these routing decisions is we
should be driven by your benefit model
and by a cost model the cost model is
pretty standard it's not trivial it's
not solved but we can build on like 30
years of research and distributed
performance modeling and so on so
there's nothing new here so I will focus
on the benefit part so benefit for of a
peer pi/4 given query should be the
quality of that pier or the piers
results for the given query and we can
decompose this into two parts one would
be so how good is that Pierce data
content I call this x i4 index I with
regard to the query q and how good is
that pier with regard to the the query
originators overall interest profile so
as measured for example by the
originators own content called X 0 here
but it could also be measured in terms
of the query lock of that guy so if I
have a query query log is nothing else
and the local corpus you can sit in you
can build statistics over that similar
to the kind of statistics you build over
an actual text document corpus and
there's to know now there's two possible
routes here for the query part we use
probabilistic on our models like there's
old work and distributed ir before the
days of peer-to-peer so call corey model
which builds on probabilistic ir maybe
you heard about or some of you should
know about being the okapi be m25
scoring model this is one particular
instance n she a shin of these kinds of
models so it's pretty standard given a
query and given a corpus or not here the
entire corpus of the pier serves as a
document we compute some some goodness
score and that's typically precomputed
per term and then when the query has
multiple terms we aggregate over the
goodness course of these per term scores
for the part where we compare anti
appears like how good is this pier with
respect to
my peers interest profile statistic
language models make a lot of sense in
fact I think we could argue this is a
marriage made in heaven because
statistical language models is like the
most principled approach in I are these
days are widely viewed this way but it
suffers from some problems like often
you need to build these over very sports
spaces so a parameter smoothing becomes
a big issue now here we don't do it on a
per document or per query basis but pure
content basis so it's a much richer
corpus so it comes with its own
background model already so parameter
smoothing should be much less critical
here and it leads to measures like this
here this is one particular
instantiation of one kind of statistical
language model that leads to the notion
of relative entropy or kullback leibler
divergence measuring the similarity of
two peers data contents so if I'm a
sports peer the other sports peers with
lots of good sports pages should become
similar to me and should score high with
regard to this metric so I already
mentioned so all that it's also the KL
part is pre computed per term and then
as I know the query or well for the KL
part because I'm comparing entire
corporal I actually don't depend on the
query but for the other part I aggregate
over the terms and the query at runtime
now there's one caveat which I want to
address so far I'm I don't elaborate on
this because it's pretty much standard I
are state-of-the-art standard I are
applied to the peer-to-peer setting and
maybe some adjustments but they are not
very earth shattering now the
peer-to-peer similarity both the query
part and the pier corpus part kind of
over Fitz to content quality so if I'm a
sports pier and I have create web pages
i just talked to our lawn about the
soccer championship of crates our web
pages about the soccer championship and
now but I want to find something else I
want to find some additional commands or
analysis of the matches and so on so and
and maybe my query gets routed to a
one spear because he also has a lot of
good information about the soccer
championship but what if he has exactly
the same pages because he did the same
kinds of focus crawls over the same
kinds of parts of the web so that's
redundant I get great results but I had
them already the whole point of my local
search engine of the peer-to-peer system
is that a benefit from my local
resources so what I want is a one not
just good rip the results I want good
additional results new results that I
didn't know before so we need to
consider the overlap of the contents of
these peers so this is work we did like
in the last one or two years adding some
overlap awareness so we need to estimate
the overlap between two corpora
essentially between two sets of Units
web pages in this case here and we
employ a technique that has been
pioneered by entry until a broader under
the name invoice independent
permutations that's an efficient
synopsis for estimating overlaps between
large sets he actually invented it in a
different context namely for finding
near duplicates of of web pages and
applied it to like shingles and crams if
you wish taken from the content of web
pages so and I have a slight explaining
this on the one slide later just a film
now we have this so we have some way of
estimating the overlap between two piers
and we can pre-compute this because this
is not query dependent and we post it
also to the directory can be posted on a
pure basis or on a per pier and term
basis so for each term which documents
does at pier have so let's a set and
conceptually that set is posted to the
directory but in the implementation we
post this MIPS structure which is a
compact vector say think of 100 integers
no bigger than this and then at at one
time we can actually consider the
quality of a pier and the overlap with
regard to the query originator so
overlap p JP is 0
means so when I ask Allah to help me
with my query does he give me your
results or does he give me the same good
results that are already had and as
always so there's no miracle here so
combining these different things
typically leads to linear combinations
as one approach right and with parameter
tuning but we can use feedback and
regression and whatever else in order to
tune the parameters so that part so far
takes into account the overlap between
some candidate target pierre pj and
myself but if i asked multiple peers not
just alone but maybe you'll see who must
see here in the first row so then then i
want to consider also the mutual overlap
between these multiple target peers so i
want an incremental notion of novelty
and that's defined here and can be
derived from this pre computed overlap
measures and then actually i proceed in
a query planning time in some
incremental Creedy manner and pick peers
for for query execution and an order of
integrated quality novelty this measure
that's defined above so here's one slide
on men wise independent permutations so
which I really think is a cool technique
by Andre so we're given a set of ID say
URLs and and we want to represent them
in a compact manner now conceptually we
compute a set of random permutations of
this set of ID's or a list of ID's
viewed as a list and for each
permutation will remember the minimum
this is where the name comes from now
the implementation just uses the hash
functions from some family and instead
of remembering the first in the
permutation first element in the
permutation we can equivalently remember
the minimum hash value this is what we
do we this is a the representation then
of this set and it can be a huge set and
we can do this with all kinds of sets in
particular with different peers and now
we can compare this maps representation
this map synopsis of one peer against
that of another peer
and the number of identical elements
compared to the entire length of this
vector gives us an unbiased estimator of
overlap so you can also think of this as
some sampling procedure over the space
of pairs taken from the two sets so
that's a very efficient technique and we
work with like 128 integer vector
something or 256 no bigger than this so
here's some experimental results so we
work with well you guys would say toy
corpora but we work with what's
available so that would be a request of
mine if you guys want to help academic
research make resources available but
corpora is not the real bottleneck
queries query locks click streams as a
bottleneck so the academic research
community would certainly appreciate if
Google could make something along these
lines available so here we work with the
data from Drake benchmarks old crawls of
the dot gov domain and also using their
queries because it comes with relevance
assessments from the track benchmarks so
we compare here to techniques the quarry
baseline is like the best some of the
best known techniques for quality only
query routing and this is the queen
curve and the blue curve is our
technique which adds overlap awareness
on the horizontal axis you see the
number of peers that we sent a query to
so let's drop the doubt with as a
parameter here and on the vertical axis
you see the Equality measuring our
success measure the relative recall
which is the recall relative to a
centralized search engine so in a search
engine what what are the results best
results for the query and how many of
these do we get in the peer-to-peer
setting as we ask more Pierce and the
data is distributed across peers and
some random manner but with a bias so
that we construct specific overlaps
where we can vary the parameters of the
overlap in a systematic manner
and you see that for example Cory with
five peers gets like thirty percent
relative repo we get 55 and if we give
ourselves a target like we want to
achieve let's say eighty percent of the
recall that a centralized setting would
give us then we could do this with say
seven or eight peers and you see that
Cory needs almost 20 peers or 20 or even
more peers so I'm going to skip this one
here so this overlap awareness is an
example of distributed statistics
management this is the actual overriding
theme here I think really peer-to-peer
web search is about distributed
statistics if you understand this you're
pretty close to to understanding
peer-to-peer web search so here's
another example this is estimating a
measure called global document frequency
how many documents overall does the
network have with a certain term so
we're interested in this for each and
every term and we're interested in this
for two reasons one is for something
which we've coined discriminative
posting was explained in more detail on
the slide that escaped so sometimes it's
better for a peer not to post summaries
on each and every term but if I'm the
sports term I have a lot to say about
soccer matches penalty goal whatever
terms of this kind and I have little to
say about technical terms like I don't
know lsi SVD or whatever or spectral
analysis so that happened that is is
used rarely and soccer so I would want
to post only when I think my local
document frequency is high compared to
the global document frequency so I need
to have an estimate for global document
frequency in order to do this but if I
have that estimate I can do it and that
saves a lot of network bandwidth and
makes the the direct network directory
slimmer and and and simplifies the query
routing decisions another
for being interested in gdf is result
merging when i get back results from
five different remote peers they come in
a nun normalized manner so they come
with scores but they are computed based
on local statistics and maybe in very
different local models so the best way
the most principled way of merging them
in an intelligent manner is to recompute
this course but then I need global
statistics on it global document
frequency for that so that's used GF is
used in tf-idf but also in probabilistic
iron it's everywhere it's you need it
for everything so that wouldn't would be
an interesting topic but maybe not too
hard if the peers had the world nicely
partitioned among themselves so so no
overlap between peers in terms of their
content but this is what our scenario is
about like we emphasized autonomy of
peers this is why we're saying we're a
peer-to-peer system otherwise we are a
top-down pre-planned centrally
administered distributed system this is
not what I understand by peer-to-peer so
overlap is part of the game and turns
out we have a very elegant solution
which is very simple because we we need
two ingredients for this the ingredients
are not so simple what they are not ours
and we just put them together and count
one and one is two and it works very
elegantly so there's nothing deep here
but it's very elegant and extremely
useful so one of the ingredients is hash
sketches and the yossi has worked on
that too but then we refer to some older
paper and the other ingredient is
distributed hash tables so let me
quickly go over the hash sketches this
is a probabilistic counting scheme
originally invented by flash early and
Martin and and then forgotten for 10
years and then Josie was one of the
pioneers who rediscover it and extended
it and came up with new results and it's
a it's an estimate cardinality estimator
for the distinct elements in the multi
set so this is exactly what we're
talking about because of the overlap
situation the way it works is it hashes
and there's
variations of this this is one of them
and maybe it's a bit oversimplified also
for the ease of understanding so we hash
each much is that element onto a bit
vector and we remember the least
significant one bit so it's comes in
different variants and then the maximum
of these least significant one bit is a
biased estimator of the cardinality
after duplicate elimination so the bias
is of course the tricky part but flood
surely and analyze this so once we know
that bias we can compensate for it it
has also it has relative a relatively
good standard deviation but for
practical purposes that's not good
enough so it's still can be off quite a
bit so what we do here is some some
smoothing or statistical averaging we
recompute this multiple times using
different hash functions and then we
average so that works very well and
leads to very accurate estimates so just
to give you one intuition about this
technique so after the hash half of the
the well you thought and half is even so
the probability of the the least
significant one between being bit 0 is 1
half the probability that this least
significant one bit is bit 1 is one
force that is but two is 18 and so on so
if the max of these least significant
one bit is less than K this is good this
kind of looks intuitive now hopefully
this should indicate that the set is
small right and and actually it holds in
both directions otherwise we cannot use
that so and here's how we combine it
with distributed hash tables and the
observation here is very easy but
elegant as I think so distributive ax t
is free so the union of hash sketches is
the hash sketch of a union because of
the way they are designed so and then we
can run global document frequency
estimation I will not go through the
text i will animate it in the following
way so suppose here is one peer this guy
and all the peers also serve the role of
directory pierce so they have data their
standard peers but they also help the
network with maintaining the directory
and there can be replication for these
directory entries anyway so this guy now
has hash sketches for terms a CT and
needs to post them to the directory so
they are just sent to where they belong
using the distributed hash table we use
court or pastry but you can use other
techniques then this guy happens to be
one of the directory peers for this term
team so this is the hash catch this guy
got from this pier but then the other
peers have this term to they post it to
and then what happens is that a simple
bitwise or computes the right hash catch
because of the distributive 80 so then
this is the correct hash catch actually
there might be much there are multiple
hash sketches for the statistical
averaging and and and this is a fairly
accurate estimator so if any peer wants
to know the global document frequency
all you need to do is check out using
the distributed hash table which is the
responsible directory peers one of those
ask the guy and the guy can give you the
estimate using these hash sketches and
this can be maintained dynamically as
data you were gonna skip this because
this is only shows that it's accurate
but this is also in papers okay any
questions so far okay so as for the
further plan i will talk about now the
link analysis and i'm in in the interest
of time I will skip the third part I
already thought this might be tight here
so okay so it again interrupt me anytime
if needed so page rank of course if
there's one place in the world where you
don't need to explain the PageRank
equation it must be here by definition
right any questions on this can I take a
random exam alone why don't you explain
me the role of the dampening factor
epsilon
okay no I guess I mean we don't get much
into technicalities the key point is
that we have this recursive equation pi
is the the stationary visiting
probability of a web page and
specifically designed random walk or the
PageRank authority measure and we
completed by power iteration right so we
use values for the right hand side
recompute left hand side and iterate
until sufficient convergence so now we
want to do it in a distributed manner
and this is not completely new people
have looked at this so any no okay so
something should be highlighted here
this year Bobby so and and the work can
be classified roughly into two camps so
one is where we go is a isn't as a
direction with coarse grained block
oriented computations or per site
computations if you wish so that does
the following it assumes there is high
locality in this graph structures and
for example because lots of links are
within a site or within a domain and
there's very few that cost domains and
then we can if that's the case we can
compute page rank values within a block
and we also compute block rank well use
using looking at the coarse grained
craft that consists of the blocks as
nodes and we combine the two in some
fancy manner so this is and this works
well if if the assumption that we have
this high locality and block structure
it holds true and if we can make it
explicit for example if we matrix of the
craft if we really see this block
diagonal structure now this is stolen
from an from old Stanford papers not so
old maybe 34 years old and this is
actually the only two domains and big
surprise look at
t holz here I'm not convinced we look at
arbitrary web data arbitrary pluck data
that we can identify these kinds of
blocks so cliff there there is this
block structure it's not clear if even
if it hits there if we can find it if we
can make it explicit because
conceptually the so this is one
direction it also makes a strong
assumption that these blocks up disjoint
which would be a burn in our setting now
the other approach is a fine-grained
message passing paradigm so it really
conceptually takes the the polar
iteration steps to the finest
granularity in Emily per page so when
this is a page and now in this poet
eration it propagates score mass
pagerank mass to its neighbors so and
this is done here and if the neighbors
are local this is an efficient local
computation of the neighbor page to
which it points sets somewhere else it's
a message that needs to be sent now this
is very elegant but it's very
inefficient so maybe asymptotically it
looks good on paper I don't believe in
this being practically viable because of
the fine-grained message passing
algorithm so what we would like to do is
have the block oriented cross grain
structure and the efficiency of that and
combine it with the generality of the
message passing paradigm and in addition
we would like to allow overlap between
plox so we think of blocks block equals
pier but now we have overlap which was
not at rest in this old work so the
overlap can be Illustrated once again
this way so if this is a global graph
and now suppose we have three piers that
have done their own personalized focus
crawls so they end up with these craft
fragments that overlap the same color
should denote the same page so some
pages set in multiple peers
now the approach that we've come up with
and that some preliminary version was in
last year in a workshop last year and no
better version is in this year's really
be with coined SJ XP for juxtaposed
approximate page rank it also happens to
be the initials of my student giorgianni
savvier pehle but i'm the one who came
up with a cool name and and it is based
on markov chain aggregation theory so
built on some existing theory which has
been used in other contexts to for
example for incremental updates of
PageRank measures so Steve Chen and
languid and Miami language and Carl
Meyer had some papers along these lines
also building on that theory for very
different purpose so the idea is that we
from one peers viewpoint we collapse
everything outside our own local craft
fragment into OneNote this is the state
lumping technique we call this the world
node and we compute a local page rank
based on this this abstract abstract
craft so our own craft fragment plus one
note the world note and we increasingly
refine knowledge about that structure
that abstracted structure with a
super-state by talking to other peers so
we run random peer meetings and whenever
we meet some other peer we exchange
different various data structures one of
them is I compare my page rank vector
for the pages that I know which might be
a small a very tiny fraction of the
whole web and I combine this with the
page rank vector that the other peer
knows I also exchange the craft
fragments or some information about this
in particular if I have my local craft
fragment I'm interested in in neighbors
of my craft fragment my I know my out
neighbors anyway because it must be
links in my pages but I don't know my in
neighbors so that other peer has some
notes that point two notes that I have
that's interesting information and we
want to exploit this so that's being
exchanged we build an enhanced local
craft but still using this abstraction
technique with
super state and recompute page rank and
come up with a better PageRank estimate
so it's important to emphasize we don't
we keep this lightweight so every pier
always has data structures and performs
computations both space wise and time
wise that are in the order of that
Pierce general storage and computational
capabilities so if I'm a peer that can
handle 10 million web pages I will not
be forced to build a jason c matrix or
with billions of pages and i will not be
forced to run computations over a bigger
matrix it's always like essentially my
data plus some other peer so it's in the
extreme case if we didn't optimize this
in a better way it would be a factor of
2 in all regards space and time voice
now it turns out this converges to
global page rank so we had experiments
first came up with the proof later so
the proof is in the vldb paper and i
will give some intuition about this now
it turns out the convergent speed can be
improved by biasing the random peer
meetings and so we have a little
strategy your Essex strategy that we
called peer to peer dating f1 slide
later on this so the key idea is we
prefer it appears that actually give us
valuable information namely appears that
have pages at point two pages that we
have now that leads a gang to an overlap
estimation problem but not my pages
against your pages but my pages against
the pages that you points to so again
turns out the mips synapses are very
handy here too and give us a very
efficient technique for estimating these
and we prefer these other peers as our
meeting partners now this is the most
complicated slide trust three slides are
kind of illustrate the computations
don't get lost in the math we will focus
on the pictures and maybe on one of them
are will explain one equation so this is
now a situation where we have three
piers gfh there might be more but we
don't see them here from the viewpoint
of this guy gee this is just the world
one note nothing else
now initially we don't know anything
about the world all we know is that
there are some notes there we assume we
have an estimate of the total number of
pages overall we know how many we have
and we can estimate how many are here so
we know how much random jump mass would
be transferred here remember here you
should or everybody should know
everything about page rank right some
random jumps is a common thing and and
that's that leaves some transition
probability is unused here in the south
of the world state so we are not this is
no longer a web graph this is a Markov
chain model just but build after this
special situation where we have a craft
fragment and abstract everything else
into one super state now we meet one
other peer for the first time for
example this guy so this is what we have
and we're interested in the end
neighbors so we realize that too in
neighbors from the other guy we figure
out the out decrease so we know
essentially a transition probability
from these blue guys to the node in
purjee to which they point to so if we
know that and if you also knew this is
actually know this part so we we
collapse again because we all want to
blow up the computation we want to work
with the world node so both of these
edges or there could be more right so
they all collapsed into one super axe
from the world goes to whoever they
points too and so we need to offer the
computation we now need a transition
probability from will know to the target
q that's a random drug part and that's a
part how much mass in transferred there
now if you'll be the patron the exact
page rank or a markov chain terminology
this is a stationary visiting
probability of the state if we do this
i'll be exactly we could have we could
do an exact aggregation disaggregation
techniques of state landing would lead
to exact values but that requires
already solving the problem so doesn't
work so it's that we use estimates of my
star means whatever estimate we have
the page rank of that HP and that comes
from the other pew so we just take what
the other offers traditional ability
ability of being outside of G times
probability that we followed a time /
article because every length is equally
problem now we remember actually this
information so we remember information
about in neighbors because we can reuse
this later on now if we meet the next
guy H there's one note pointing to us
and now all that together is subtracted
into the world node and multiple
transitions so all the three blue nodes
so and that leads to transitions from
the world to different targets again
using this transition probabilities and
according to this so that is the theorem
we have in a fair sequence of meetings
the scores actually converge and they
converge to the true global page ranks
if the network didn't change or the web
didn't change of course if the web
changes all the time or the network
changes all the time there is nothing to
prove it would still work reasonably
well but there's no theorem now what is
a fair sequence that means if in an
infinite sequence every period is Matt
infinitely often so there's no
negligence of some peers which would
give us a bias that can no longer be
corrected so as I said
yeah conversions is infinite always
there's a little bit right so ID good
question so we don't have theorems about
convergence speed like convergence rate
idly this is desirable but the math gets
hairy here so and there's simpler
settings of PageRank variations we're
world-class mathematicians haven't come
up with rate analysis so will we are
back to experiments and I have some on
the next slide the the proof is based on
partly based on that existing theory but
it's not theirs we took us a long time
because it was there gaps first we
thought well there's such an elegant
existing theory sitting around for 20 or
more years and it should should apply
here but it does not because our setting
is pretty different because we have this
partial knowledge of the underlying
markov chain and so we need to needed
some additional things here so the key
observations that are part of the proof
are that the for the world node the jxp
score always overestimate the true page
rank the page rank of the super-state is
the sum of the page ranks of its notes
right that's always holds here and the
gxp scores are non decreasing and at
some point non increasing sorry and that
and there's an additional theorem that
at some point they must decrease and
finally converge to the right value and
for all the other nodes that set in our
craft g it's the other way round here we
underestimate the true page rank and the
intuition is easy here because it we
lack the information that they have that
we have in neighbors so these in
neighbors give us additional authority
and and and we like this and gradually
learn it right and this is the intuition
but the hard part is that we don't
overshoot or fluctuate that we actually
converge so let's come back to this
dating strategy I learned that in the
u.s. there is a new yeah
as well
okay yes so the question is about what
if some notes cheat that's in my outlook
right so we don't have a solution for
this we're working on this right so like
if the other if the meeting partner lies
about the the page ranks of of in
neighbors right so it is subject to gain
here right so I believe in good society
we have some we have some initial
thoughts on how to get this under
control not surprisingly it's highly
related to the recent work that others
have been doing on link spam because
it's a it's a related problem so coming
back to the dating or meeting strategy
so I heard in the US there's something
called a power date so you meet as a
male and I think you can reverse it you
meet 10 women in five minutes each right
and then you make up your mind which of
them you really wanted me to actually
both make up their minds I think and it
can be reversed sorry if this gets the
wrong message here so the women hour to
decide which of these guys they want to
meet for real and so I heard this is
this exists in the US in areas like this
one here so we we make this so we do
this short power date by only exchanging
very compact synopsis namely the
synopsis about overlap between my notes
and your your your outgoing link targets
and for that purpose we need to
pre-compute maps on like appears pages
and the piers outgoing link targets and
then we compared to peers I compare my
pages against the synopsis of your
outgoing link targets and this way I
build build up some candidate lists this
way so I can do this very efficiently
with many potential dating park
and and and rank them and then using
some query aristocats um point make up
my mind which of them do I really want
to meet and only then I exchange craft
data and PageRank vectors and really go
through some less lightweight
computation so this is some experiments
will performing experiments on small
scale data we here we're using Amazon
data is not really wet data I realize
this using recommended similar products
as links it appears this way in the web
pages we also did experiments with
small-scale web cross that we performed
ourselves and they pretty much coincide
with the trend so there's no big big
difference here so you see the
convergence is recently good both into F
this is the number of meetings in terms
of rank distances this is virtual
distance and in terms of scoreless
and you also see Lou the clean curve is
the one that uses this biased power
dating strategy so it also we also did
experimental query results ranking in
the setting so guess what the jxp scores
or authorities course page rank style
authorities course can improve the
result ranking and it can also be used
for the query routing itself because you
want to route not only to be appears
with high quality with high quality
content but also highly authoritative
pages right so that can be added also
into that part and ongoing work is peer
trust measures so if we can compute some
trust measure we can factorize it into
the computations by essentially biasing
when we when we recompute something we
we take we essentially run them down the
value is that some slightly mistrusted
guy gives us right but of course we need
to find also a way of computing these
trust measures and the idea just very
quickly for those of you who are
familiar with this link spam detection
techniques so one recent technique is
comparing distributions of course
because it's it's it's you can you can
fake one score but it's difficult to
consistently fake an entire distribution
for example your PageRank scores should
also be a power should typically follow
a power law distribution or at least the
ones for the pages that that for the
that share a common target or a common
source and so you can run essentially
statistical techniques and that gives
you indications of whether this other
guy is likely to tell the truth or not
right at least it's or he must be very
smart in line I'm not saying this is a
perfect solution
so unless there are any questions now so
summary so emphasized I think
peer-to-peer web search is an
interesting research topic regardless of
whether it is a business killer
application or not thing leads to lots
of insights and findings that will
sooner or later find their way into some
setting and and one of the technical key
areas is distributed statistics
management so it's key to query routing
two estimators for quality overlap and
other measures and also essentially the
peer-to-peer a pagerank computation is
also distributed statistics if you wish
with some specific flavor right and so
and therefore specific results like the
convergence theorem the the challenges
and making this lightweight and
efficient and scalable and that has two
sites so capturing global statistics is
one thing and then disseminate it to
semin aiding this to the places where
it's needed is it might be a different
thing of course we've used the same
mechanism for both the distributed hash
tables is essentially the mechanism that
we've employed in combination with
different other careful considerations
like the combination with hash two
catches for example now in terms of big
issues further down the road robustness
to churn and cheating is a big issue
cheating is harder than churn and it's
not clear to me if it's well defined it
might be inherently able find even in in
current work on spam detection I see
this phenomenon it's treated like an
arms race so you try to detect some
anomalous patterns but then the the
other party also adapts and changes this
so maybe this needs a real paradigm
shift taught some adversarial theory is
here and that hasn't been in the
literature at all so far right so right
now it seems that yes we're trying to
use heavy calibers but if the enemy is
really has the incentive to also
boost its machinery and its cannons they
might do it right and then it goes on
and on so overlay networks is a charming
paradigm because it would kind of
pre-compute frequent decisions as
opposed to making decisions and
fine-grained manners per query or we can
pre-compute but of course these networks
must be dynamically maintained so as the
the network changes as the web changes
as interest profiles change this needs
to be maintained so you can call this
they come in under different name
semantic networks social networks and my
view of the world this is all about
statistical characterisations again
because true semantics is not in the
computer and social things are among
humans also by definition right so what
we do in the computer is mimic the only
shallow aspects of this and and
capturing the statistics about this is
the key thing and finally experimental
evaluation is some metal topic that I
found to be difficult to there's no
established benchmarking methodology for
these distributed things in the pit
appear systems community the way they
experiment as they they run it when
there is no no no notion of reproducible
they would give these rocket science
papers where they say we ran it on
planet lap on 500 computers and then
half of them died by some accident and
we lost only three percent of the data
there's nothing repetition rule there is
no general insight is just an exotic
evidence of something and it's difficult
to interpret so the benchmarking
methodology is a big issue here because
we're combining I are worse distributed
systems so even in IR benchmarking has a
has a culture with track and I necks and
other things but it has a hard time to
keep up with the pace of the world
evolving towards more challenging
settings now a large-scale test bed is
one piece of that so we've been thinking
of running this and planetlab but the
difficulty is that planet lab doesn't
have users
real users so we would have to deploy
our load drivers and then planetlab
doesn't give us any real additional
benefit so we might as well run on one
of our clusters we have a big cluster on
which we can run this experiment so if
we have load generators then then why
use the real system we're not in ish
into issues like to the firework well
there's no firewalls and planners lab
anyway to do certain idiosyncrasies of
networking protocols affect our our
approaches so we're not into this kind
of game anyway so planetlab I mixed
feelings about this so and and instead
of using load drivers ideally we would
have real users and capture their
behavior and that brings me back to my
to an remark amid earlier that's a good
concluding remark so of course there is
such information available like query
logs and click streams so if the but
it's it's it's in the possession of a
few players in the world like three to
five players maybe so if that some some
sanitized versions of this of course
right could be made available to the
wider research community this would be
highly appreciated thanks for your
attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>