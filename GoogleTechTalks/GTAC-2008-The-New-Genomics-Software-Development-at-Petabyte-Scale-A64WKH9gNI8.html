<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2008: The New Genomics - Software Development at Petabyte Scale | Coder Coacher - Coaching Coders</title><meta content="GTAC 2008: The New Genomics - Software Development at Petabyte Scale - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2008: The New Genomics - Software Development at Petabyte Scale</b></h2><h5 class="post__date">2008-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/A64WKH9gNI8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello everybody let's get started here
okay to hello a video says my name is
Matt woods I work at the Wellcome Trust
Sanger Institute where I head up
production software development and
we're really interested in using genomic
information to further our understanding
of human and mammalian diseases and so
it's just three things I wanted to talk
about Oh 10 0 &amp;amp; 1 ok let's binary I
guess so I three things I want to talk
about today I want to talk about a bit
about better work a bit about our
background some information about the
state of current state of play in the
new genomic landscape and just finishing
up a little bit about our process so we
can get started here so previously on
the human genome projects as a picture
missing there so the human genome
project was a worldwide consortium it
took 15 years to decode and the basic
idea was that we tried to find all three
billion letters of the human genome and
the idea behind this was that once you
know these three billion letters and
funnily enough those letters are g g a
and C its fate obviously I'm here so
it's just a linear string the three
billion letters right and once you
understand the sequence of these three
billion letters you can start to look at
the relevance in terms of disease and
what should be there on the left-hand
side is a printout we did a printout of
the full three billion letters at eight
point text and it's absolutely massive
but you'll just have to take my word for
that so it was a very large project as I
say cost around three billion dollars we
think although to be honest were not
really cuz sure what it cost it was such
a large projects such on a worldwide
scale that it prolly cost a lot lot more
Matt it's interesting partly in the
information that it provided and in
providing a foundation for modern
biological research but also because it
was a race for a prize there was two
consortiums actually going head-to-head
on sequencing the genome there was a
private Institute based in the US and a
worldwide consortium of open advocates
and had the private Institute sequence
the gene
first the data wouldn't have been made
available luckily for us the Wellcome
Trust who fund our research basically
wrote a blank check and we were able to
get there first and so we were able to
do the full human genome in around 15
years because we're a charity we're
actually able to just give away this
data so it's completely open data you
can connect up at various different
others you can get it off our ftp site
you can just connect up and download it
we have de normalized databases you can
just connect up to our MySQL databases
and query the data right there it's all
also all open source and so all the all
the source code that was used to
generate this data used to analyze it
further downstream is all available and
open and the primarily written in perl
so pearl is really the language of
bioinformatics so who here uses pearl
hey yeah my peeps so we've actually got
a lot of pearl right so it's it's
unusual to see projects this this size
in Perl most pearl project CGI scripts
that sort of thing so I checked our
source code repository before I left we
have around for half thousand modules
and I started to do a line count of how
many lines were but it didn't finish
before my plane had to leave so I have
no idea how many lines but for now a
thousand perl modules that's a
reasonable a reasonable size so once
we've done the human genome we started
to move onwards and we started to look
at other species so specifically and
some examples these little Wrigley guys
these are nematode worms interesting
because we know the full genome and
they're not in genomic turns that
dissimilar from us this little guy are
there chimpanzee obviously and my
personal favorite the doc bill passed so
this guy's all kinds of freaky so from a
genetic point of view he's occupies a
very unique position in evolution shall
we say it's got this weird platypus face
as a mammal that lays eggs humans have
to sex chromosomes x and y if you have
2x you're one if you have XY you're a
man double platypus guy here has ten sex
chromosomes and nobody knows how they
determine their sex so I guess they know
right but so so we just finished
publishing the the draft genome of the
duck-billed platypus and you might ask
what like why do we want to do this of
all the 40 species we've done including
the double Baptist and the mouse and the
rats gorilla all these other species why
are we doing it and the the role really
is to map evolutionary space we want to
be able to see going backwards through
time through the millennia how we've
evolved and the idea is that we want to
be able to compare genomes so compare a
mouse with the rats a mouse with a human
genome see where the similarities are
because highly conserved regions of
genomes are likely to have biological
importance so it's kind of interesting
when you start comparing species and
that's a good thing to do but when you
start comparing individuals it starts to
get really really interesting now up
till about six 12 months ago this was
prohibitively expensive it took 15 years
just to do one individual we couldn't do
another individual could take another 15
years and we certainly can compare large
numbers of individuals but there's
recently been a quantum leap in the
actual chemistry associated with doing
with doing this process and it's meant
that what we can do is we can start to
look at individual human genomes for the
first time so this you may have heard of
companies like 23andme nav igenix doing
personal genomics for uh you know five
hundred dollars that sort of thing so
this is a step beyond that what they're
doing for doing is looking for known
genes what we're doing is looking for
novel genes we recently ran a very
large-scale study with 10,000
individuals and we identified some novel
genes which could be used for
therapeutics for such diseases as high
blood pressure so hypertension we look
at diabetes coronary heart disease
bipolar disorder and we're doing a lot
of work with with malaria partially
funded by a this guy Bill Gates so don't
know if you've heard of him so that's
really what we're about and we've
already been able to make curse and some
reasonable inroads into some of these
diseases and find novel targets for
therapy using Perl all my poor guys
the counts or something Perl 6 yeah okay
so moving on I can talk about a talk a
bit about the sort of process that we go
through so in the new genomics and for
the first time modern molecular biology
and modern biological research has moved
from being basically a lab process into
a software process the amount of data
being generated for the first time
requires very very very high quality
software this is orders of magnitude
beyond these CGI Perl scripts and we're
now starting to have to get a lot better
at very developing very very high
quality software and maintaining that
quality throughout this is particularly
true in the in the region that I work in
the in the sequencing informatics team
at the Sanger Institute where we're
responsible for taking the samples from
individual people and running them
through our sequencing pipeline
generating the data and they're
generating the genome at the end as a
couple of reasons for this and a couple
of key challenges which although our
novel and interesting from from our
perspective I think have a broader
appeal so quick look at our little
process here pretty simple deceptively
so but pretty linear right so we first
of all we register our projects it's
pretty easy we register the samples so
this would be a bit of good ruler or an
individual or something like that we
prepare the sample this is actually a
lab process that we need to go through
we do the sequencing and then we do some
analysis on the data that comes off the
sequencer so although its linear we run
it very very high throughput system and
that means that any failure any of these
points can have a dramatic effect and
because sequencing is so expensive we
want to optimize for this bit here so
any decrease in our software quality
remember this is all just data these
days any decrease in our software
quality can have very very large very
very expensive push on effects it's
particularly important when we're
dealing with what we call golden samples
these are samples that we couldn't get
again even if we wanted to so if we have
a bug which stops sequencing at some
points or with some low quality software
which causes problems down here we've
got a sample which is effectively going
off at the top of the pipeline that we
can't get in we can't get that sample
again and we'd lose the data from it so
quality is
extremely important to us and becoming
more and more so and so I'm just going
to go over some of the challenges and
some of the tools we've come up with to
try and maintain quality as we go
through first of all some more pretty
bitches so this is actually a sequencing
machine in our R&amp;amp;D lab the sample sort
of goes in here that's a username and
password on the post-it note you want to
see it again so this amp is actually get
loaded onto this glass cell that's a
laser and a 1 megapixel camera we can
load eight samples at a time and
basically we shine the laser at the
sample it for arrest is at different
wavelengths take a photo 1 megapixel
photo and then we use image analysis to
derive sequence from it here's another
piece of laboratory equipment or but
Heath Robinson you can see lots of
fluids and cables and stuff and again
there's another photo of the where the
actual samples get loaded and the laser
huge amount of Robotics very very
difficult to manage so the first problem
that we have is as I mentioned one of
throughput we're very very high
throughput facility we've got the
largest deployment of these machines I
think in the world it's just about to
get a lot bigger and just to give you an
idea of the rate of change here's a
graph of the number of Giga bases this
is the number of bases that we're
sequencing per month so with the
previous facility we were doing slightly
under five Giga bases a month with our
new alumina production facility you can
see the difference and so this is why
it's now turned from being just a
question of handling samples into a lab
in terms of actually handling data this
works out at a huge amount of data and
this is our sort of throughput so we
have our sequencing machines in blue
over here on the Left these we mirror
the information from them onto a 320
terabyte storage array that's connected
to a thousand core sequencing farm
there's a quad cause machines and
eventually that's moved off or onto an
archive so the the sequencing farm here
basically takes care of a number of
different things and the sort of key
number today is seventy five terabytes
which is the amount of data that we
generate every single week so at full
capacity
we're running 75 terabytes so you pearl
guys you're still with me right 75
termites okay so we have to use a
mixture of pearl and Ruby now so if
anybody tells you Ruby and rails don't
scale all right okay I'll see you
outside if anyone tells you Ruben rails
doesn't scale you send them to me
Twitter have it easy believe me like
this is this these are pretty big
numbers so the sequencing farm basically
takes care of a number of different
things the the pcs that control the
actual sequencing instruments haven't
got enough disk space on them to store
all the data for a single run so we need
to mirror that information off in real
time we need to check some it at either
end and there's a lot of a lot of
infrastructure there a lot of blades
basically in the sequence performance
just take care of that they also
generate a huge number of interim files
that we need to be able to read in
analyze and basically look to see if
there's any problems with the sequencing
process as it's in process because we
won't be able to stop it because it's
expensive if something's going wrong so
we're talking in the region of 100,000
different files which we need to load
into a database in order to analyze
there's a lot of work goes into
maintaining that we're starting to look
at image reduction technologies because
these images very large we can't stall
them for very long but if we reduce them
down maybe we can and then there's a
sort of more informatics site things
assembly in alignment which has a lot of
high throughput and a lot of
infrastructure associated with it so
here as an example of the throughput is
a one of our one of our applications
this is a rails application and I can
just zoom in on some interesting
features here so you can see at the top
here we've got a readout of how many
bases that we've sequenced for this
particular project so that's just under
a terror base of sequence and to give
you an idea that's more than had ever
been sequenced in the whole history of
mankind before this project was run so
we've just done on the whomp in this one
project and if you look at the top here
this is a pilot this is what they're
running to make sure that the the
project will work eventually so a
terabytes the sequence in a pilot
project just to give you some some idea
of the right of throughput so there's a
lot of infrastructure and a lot of work
goes on in maintaining quality
particularly because any network
problems any hardware problems can have
this huge backlog effect so we had a
network outage lasted a day or so and
that caused a two-week backlog for us to
push through so anything we can do to
maintain quality there is important and
tied into that is the phenomenal rate of
change so we heard this morning from
people talking about auto generating
their test cases and things like this
because their domain wasn't changing at
a very high rate they were able to do
that it's completely the opposite in
scientific software and a lot of other
domains the rate of change is absolutely
phenomenal it's basically always in flux
so much so that when we were receiving
these these instruments from the from
the vendor when they first arrived they
had to have hand drilled holes in the
side of them because they were
overheating when they left the factory
so when the instruments aren't even sets
out even stable and the pipelines and
the process in the laboratory aren't
even stable we need to come up with good
ways of handling this very this
environment of change and we can
actually use it to our advantage so long
as we have high quality tools around it
and so one thing I want talk about in a
bit more detail is an approach of
flexible data capture that we use so
traditionally if you have a database
driven application you would have you
know Rose which will contain the actual
information and columns which would
contain your field your attributes right
that's fine except there's a reasonable
amount of overhead in maintaining the
schema of your database and because
we're running high so such high
throughput environments and we can't
often take the time to read develop our
schema to make a chain for something as
simple as adding a new field to a
project specification or sample
specification or particularly as the
workflow in the board see changes we
can't stop what we're doing and stop
production just because they've
introduced a new machine or they want to
collect a new piece of QC information so
we call this helix it's basically a
system of virtual fields so rather than
being primary database columns primary
attributes these are our virtual
basically they're still stored in a
database but they're a little bit more
flexible and they basically allow us to
make changes at runtime one problem we
have is that
that things happen so quickly that we
often don't know they've happened until
they've already happened but we still
need to collect the provenance
information of these samples as they run
through the process as they run through
the pipeline so that the analysis can be
complete at the other end so what
happens when that process changes we
need to be able to basically model it
almost effectively in real time because
it's changed if we don't collect that
information there and then we've lost it
forever and the analysis can't continue
that sampled effectively been wasted
which is very expensive but again forgot
these golden samples it can be a real
problem so how does this work so very
very simple so we have a sample and it
would have a couple of fields right so
named organism concentration that the
samples taken from for rails aficionados
any rails people where's my rails peeps
yeah put them up so this is basically
how it works so we have a sample and we
basically say that in this sort of
modeling language that it has many
descriptors and then has many descriptor
values and we store the descriptors with
some additional metadata but as the
actual definition of the specification
and we store the values in another table
with some denormalized information
against it it basically boils down to
just cavalli pairs so for example if
we've got off our family of our sample
here our record would have the same
things stored in key value pairs yeah
it's good so they see a lot of fast and
you think right so with some with some
with some clever indexing and some good
denormalization we build a lot of the
normalization automatically
normalization tools ng normalization
dsl's which available on github open
source just give me a chat forum access
to it it's a lot faster new thing so
this is some of the sort of information
that we collect we have a UI for
specifying these things again we don't
have to take the app down or the
pipeline down if we want to want to make
a very very simple change you can see
the sort of information we want to
collect a lot of is relating to funding
the sponsor which platform is going to
run on additional comments all that sort
of thing each one of these is a virtual
field and I can zoom in on this a little
bit so you can see it we also
automatically generate all are you is so
as a data model changes we don't have to
go back through the application retest
it make sure it's all in the right place
that the flow is still in place just
because we've got in another field and
you can see that we can add things like
drop downs and required fields and we've
got uploads and various other bits and
pieces like that and initially we had
hoped that this would allow us to just
keep on trucking when we wanted to
change our data model it's turned out to
be a lot more a lot more useful than
that because we can map change very very
quickly but the application is automated
basically so the UI builds itself old
values know how to represent themselves
and we can migrate between values as we
go through but we don't have to every
time we make a change to the data model
go back through the application update
all are you is and they make sure that
everything works so it makes testing and
updating and maintenance very very easy
so what happens when when change occurs
right so we've got our we've got our
original sample this is version 1 and
then we want to update our data model so
you can see down here we've added two
additional fields an origin and some
sort of quality metric all we do is jump
into our editor but run time again we
make sure that everything is working we
have the additional fields and then
automatically oops automatically the UI
updates so we don't have state the app
down we don't have to rerun all our
automated tests although there are a lot
in there to make sure that all this runs
smoothly the application just keeps on
running and to certain trusted users we
can allow them to do this right there in
the lab to start with we made it open to
everybody that was a really bad idea
they went crazy here and it was like
Facebook or something it was it was not
what we had in mind at all anyway so
yeah access control very important so so
it allows us basically to maintain
quality because we've already got our
tests in place for this data abstraction
effectively and because we've got that
in place at a foundation level the rest
application can just keep on truckin so
it's important in terms of actual
actually creating this sort of thing but
all our search engines we can build
queries right there in the UI and they
all automatically update because we know
the the keys and the values makes it
really easy for people just to jump in
and get the information they need we can
update all our reports so we have a lot
of things which automatically look
related failures and because we've got
this data abstraction we can we can dig
into the data and use the metadata
associated with that to build you is and
to identify failures early in the
process and you can see where these are
the sort of versions we're talking about
so in only six months we're already sort
of at version 20 so although it took a
little bit of time to get this sort of
thing up and running it certainly paid
off we're not completely crazy we do
also rationalize so as the data model
starts to settle down for example all
samples now have a name we will
basically refactor that out into into
the database for even faster queries as
I mentioned we've got some migration
tools as we can move forwards and
backwards between different families
we've got a UI to do that as well so we
can just select the different versions
of the families and the UI will detect
differences between them and again this
just allows us to keep the application
up as we make changes which happen
really really very frequently we also
use this for our pipeline applications
so we have some kind of workflow in the
laboratory for example that we want to
collect the provenance information the
the basic process information for a
particular sample in terms of the name
or the operator the serial number of
particular chemicals that are being used
and again we can specify this at runtime
without any problems when we use that
for our lab and QA pipelines I'm here
some of them here you can just jump in
and here are the sort of steps that you
can see and we can add new steps remove
old ones reorder them all this sort of
thing again allows just keep on trucking
without any problems okay so it
basically allows us to change a maneuver
at speed and in mon biology and in any
sort of environment which is in flux
this sort of this sort of technique can
be really really helpful so the third
problem we have is one of scale so i
don't know how many of you read the
Technium Kevin Kelly's Technium blog so
he has this idea of what it calls truly
onyx right so this is the idea that once
you start getting into orders of
magnitude changes in scale the whole
game changes so the example he uses is
with a penny a penny a single penny is
very very easy to manage right you can
put it in your pocket I can pass
around it's easy if you have a trillion
pennies they take up more than the
volume of a sports stadium and then you
start getting into really big problems
and modern biology in a lot of other
fields are now having this very large
data explosion which take us into this
realm with truly onyx for example if we
only had a two-fold increase in our data
throughputs we'd already be at 150
terabytes per week and that starts
getting really difficult to handle and
obviously it's no real surprise that we
already have machines in our R&amp;amp;D lab
that have the exactly this capacity and
can do it in half the time that we used
to so we're already looking at 300
terabytes per week that we have to now
manage and process and this again is
we're having high quality software in
place and having these sort of tools
which make an advantage of the fact that
change is going to happen rather than
baked in rigid functional specs re a big
advantage so for example over the next
three months we're gonna be adding more
hardware so we're going to add probably
another four hundred additional nodes on
to our farm we're going to have another
200 terabytes of storage probably handle
these things and this is in addition to
the Institute's already large data
center so we've got one of the largest 8
centers in Europe that I know of and we
spend around 17 and a half thousand
discs and we have about 5 petabytes of
online storage at any one time so it's a
reasonably large endeavor and it's it's
a good thing to be a part of so as we
start to collect up this additional data
and we have these petabytes and
petabytes of stuff we need to start
looking forwards and this is something
that we call a virtual Institute so we
have lots of data as I said and because
we're open and we provide the data for
free to only anybody that wants it we
have a lot of people that want to get at
that data and you can see it starts to
multiply up because lots of people with
lots of data require lots of compute and
they've all got lots of uses that we
haven't yet even thought about and
there's lots of awesome lots of other
things that we need to start thinking
about not least of which is the amount
of money that this sort of thing costs
we are as I say a charity we're
reasonably large well-funded charity but
the whole team that puts all this
together is only 15 people so
really very hard resources and hard
limits on what we can do there so we
need to make the most of every single
little thing so all of you the work in
800 strong engineering departments you
want to make a change and come work
somebody bit smaller do something good
we're hiring so just to finish up then I
just want to talk a little bit about our
process and with such a small team how
do we how do we make this work basically
and how do we do it in a reasonable way
well as I've sort of hinted at we don't
have we don't put a lot of sway in
functional specifications so this is the
sort of typical like software 101like
timeline so you have a concepts you
built and you generate some requirements
by chatting to your users whatever you
do some developments and then eventually
you end up as a product and it's that
easy right obviously sorry easier than
that right okay you can take out our
requirements most of the time or in some
cases a concept I don't know if it so we
use the process of scrum basically so
anybody hear you scrum yeah good this is
easy so with scrum for those of you
don't know basically it's much more
iterative than that the problem that you
have is that as you're developing your
requirements are changing all the time
and so we have a central idea we start
to prepare for our iteration we go
through a development sprint we plan we
review and then we just keep cycling
round and round and for those of you who
who don't do this provides a very very
focused method of development keeps you
keep your eye on the particular problems
and for us allows us to to iterate a
very rapid rate with a small team that
don't have resources that they need this
allows you to always be on top of what
you need to do so we have a lot of
interaction with our project owners so
they come to our scrum meetings we meet
with them virtually every day to make
sure that they're in the loop and these
will either be internal people principal
investigators other scientists people
like that and we also have a weekly
releases so we don't miss around here as
soon as the products ready to go and we
iterate at a very very high rate this
house us to just basically be more
flexible in terms of what we need to do
allows us to break down and get a very
good
understanding of the domain and I think
that's a real advantage to scrum for
those who don't do it takes less time
obviously because we've always got a
working build basically we can just push
it out the door and there's better
transparency as well this is a common
problem in scientific software people
don't know the process and they don't
have a high confidence in it most
importantly it leads to less software so
of four and a half thousand perl modules
we've gone from that to the new platform
which is only about five thousand lines
powering all this stuff so we really
really try and pare down as much as
possible and that's running this full
multi petabyte storage system so that's
pretty much all i wanted to talk about
obviously i don't do all this on my own
as i say we've got a good team in
sequencing informatics back in in
cambridge back in the UK so thank you to
those and thank you to you for listening
to me
we've got time for some questions if
anybody has any yep okay so the question
is what was the source of the change in
terms of data modeling so primarily it's
a change in scientific process so
science is always innovating it never
ever stand still so a process which is
cutting-edge you know one month can be
complete user apply something that comes
along in the literature of the next
month and particularly biologists are
very very keen to put the best practices
in as soon as possible so if a protocol
in laboratory changes or there's a new
instrument for example that comes along
they want to get that and implement it
very very quickly and as I say that will
affect the overall analysis further
downstream so it's if you like in stream
changes that we have to be able to
monitor collect and then filter in
further downstream and because we have
this data modeling layer effectively we
can do that and maintain our metadata
and the system's downstream we have
these loosely coupled analysis systems
and QC systems they know how to read and
write basically with the state modeling
layer so although we make a change it
can still figure out further downstream
have to use the new information in the
analysis with very very minor tweaks we
have dsl's and things to make that even
easier and this all comes again because
we're relatively small team we have a
lot of data to deal with the back
right sofa loser you didn't hear it the
question was with such a large amount of
data how do we identify failures and so
there's a couple of different failure
States if you like the first is a
process failure so somebody drops a
slide or sample gets contaminated and
that sort of thing and that we again we
handle in software and we basically have
message systems which alert the
necessary bits and pieces to a failed a
failed run the second state if you like
is an analysis failure which happens
once we've collected all this data and
this is why it's so important to be able
to model on the fly because analysis
valleys are very very expensive these
machines are expensive a run is
expensive and people's time is expensive
so in that sense we have a lot of tools
which go back over these this model data
and again we spend a lot of time getting
this right so that we can identify
similar problems in a failed run so we
can look back through the through the
pipeline information see similarities
between multiple fails and identify what
that is most often it's a piece of
laboratory equipment which hasn't been
calibrated correctly or as I say
somebody who's we often identify people
who aren't following a protocol that
sort of thing on top of that as I say
there's this analysis failures which we
have a lot of automated tools for
identifying so we generate as I say a
huge amount of metadata around the
sequence quality scores intensity scores
of the laser temperature graphs all
these different things and we put all
that into a database and mine it
basically and there's a whole group of
people that just spend their time
looking at that on top of that we have a
group of project managers who go through
every single run and eyeball all these
different information and information
derived from it to identify potential
problems and they often make a call as
to whether it's a pass or fail based on
that so so it's it's an interesting
interesting problem too involved with
yeah joe
right the first question was have we
looked at document-based databases so
schema-less space CouchDB that sort of
thing we have a little bit the reason
it's done in this slightly bizarre
relational way is because we already had
infrastructure for relational databases
and we need to be able to get up and
running very very quickly so this whole
system was put together in three months
I think so we had to get up and running
quickly I mean we know relational
databases but we knew that we had to
make change our key part of our process
going forward it's github com / mza yeah
sure so there's a this there's a couple
of different lab teams involved and
overall the whole Institute is around a
thousand people around 125 work in
informatics and as I say around around
15 or 20 work in sequencing informatics
actually doing this stuff in terms of
the actual laboratory parts the rest of
it is lab people basically so there's a
large number of people into are involved
in pushing these samples actually
through the lab getting them done
getting them finished there's a lot of a
lot of experimental design that goes
into making sure that we get high
quality sequence and we're finishing
these genomes particularly the species
genomes that I've talked about so as a
large group of people that are just
involved in looking at and assembling
these genomes into a sensible way yes
exactly yes so any basically any genomic
project that within the Institute we
would we would provide software for so
that includes the sequencing platform
but there's also other things like
genotyping and various other bits and
pieces that we're involved with as well
yeah
right right right so the question is how
do we know if there's if there's a bug
so sequencing isn't perfect and
sequences often make mistakes and so a
lot of a lot of what we do and a lot of
the time that we spend addressing
quality issues like that is in reviewing
this metadata from the sequencing run
because we know of what intensity and at
what point a particular base was called
so it's one part of it also we we
sequence the same base many many
millions of times and that's why it's so
expensive so then we just take a
consensus at a particular point and the
sequences votes basically so but they do
make mistakes and we use that
information further downstream again
that's part of the information that we
collect up as we go through yet on the
end so we have a lot of custom tools
that are based around sequel queries
basically so nothing particularly fancy
we have a lot denormalized data and do
normalize databases so we take it and
basically reprocess it and repackage it
and then we have some queries which will
run very very quickly against the dean
on rise data so it's not as bad as you
think actually it's manageable it's
manageable compared to the enormous sort
of hump of data that you get initially
yeah in the back
sure so the question is how do we go
about testing some of this information
so because the domain moves so quickly
will often just take a most recent
backup copy of the database and then run
our that's the best we can do with
weekly releases and such a dynamic area
we can just take what we've seen in the
past couple of weeks and then run a full
integration tests across all the glue
see coupled items basically so there's a
couple of million rows in those
databases I think plus the actual
production information which is stored
in a slightly different way but that's
the best we can do yeah in terms of
language own terms of hardware so we
have we have blades basically so they're
just IBM blades we have us about
thousand cause and then the file system
is a lustre HP lustre file system
clustered file system which seems to
work relatively well with these large
data sets but as I say going forward
that probably won't be good enough
because we need to be able to put the
data with the analysis and that's sort
of what we're talking about with this
sort of virtualized to institute anymore
for anymore
sure so we're primarily focused on
production so just getting the sequence
out there but a lot of that moves into
the analysis steps so in terms of the
the numerical analysis and the
statistical analysis and particularly
the statistical genomic comparisons
these are basically large population
studies and say oh there's a lot of both
sort of input and output from a wide
range of fields but that's part of what
makes it a fundamental work in because
there's a lot of overlap with with
various different types of medicine yeah
do we have a process of performance
testing not really is the answer we know
what slow and we know that that's what
we have to concentrate on and we have a
bunch of metrics about you know the
average length of time for a job to take
and what should be happening and a whole
bunch of stuff like that but as I say
with small and number of resources we
need to focus on the production so we
need to make sure that those samples are
moving through and if the analysis at
the the other end takes you know an
extra day it doesn't really matter too
much it will matter in time we've done a
lot of good work in paralyzing and
distributing our computes across very
very large clusters but it's an order
primary focus where we're much more
interested in making sure we get the
right information at the right point to
provide value further downstream if it
was worth 30 if it doubles yeah we're in
serious serious trouble we'll need more
storage we'll need more computes and
we're very lucky that the Wellcome Trust
consider informatics at a primary
primary source of investment basically
so we're often able to get the compute
and the funding that we need to do what
we need to do and as a large-scale
sequencing institutes we're very lucky
to that extent but we are definitely
need to get going to need to get smarter
in how we handled our data and our
compute because it's very easy and we're
very used to dealing with
with a single genome so it's 15
gigabytes maybe and I say you can you
can pass that around it goes on to you
know almost fits on and memory to pick
these days but when you've got a couple
of petabytes of this stuff we need to be
a lot smarter and how we handle it
particularly as you know that the sort
of the workflow in the analysis excuse
me in the workflow in the analysis is
you know you want to you want to get
some data you want to filter it then you
want to do work against it and work is
the interesting bit and then you can
generate a subset of that date for you
and run a normalization on it some
statistical analysis and you're going to
you're going to end up with two copies
of the data set basically so now you've
got two petabytes that you need to deal
with and as I say we've got a lot of
people that want access to our data and
a lot of people that don't have the
large scale resources to deal with these
very large data sets but who are doing
very valuable science so part of our
remit is to provide resources for people
who don't have the resources to do what
they need to do so we are going to need
to think more about it and
virtualization is certainly something
that we're looking at very very
seriously and how to put the data
together playskool anymore yep
it's the it's the it depends full
capacity it runs quite nicely but the
bottom makes it is always the number of
sequences that we have so we have I
think 28 sequences at the moment and if
we had twice as many we could probably
put twice as many through but we need
then twice as much compute and twice as
much storage and that then starts to
become the bottleneck as it is we can
only keep our raw data for around three
to four weeks we only have enough
storage for that amount and after that
we have to have analyzed it and then we
have to delete it otherwise then we get
a bottleneck further upstream so that's
a big challenge as well 15 gigabytes so
there's additional quality and other
information that's associated with it
and sample information and things like
that so it works out at about yeah 10 15
gigabytes for for a human genome and
that's what you can download from our
ftp site yeah exactly yeah yeah they so
malaria is is a very obviously a very
important area so pathogens are
notoriously difficult sequence and
malaria is one of the most important
pathogens and so again with our we have
a very broad remit to handle these sort
of things and so we go after the we tend
to go after the big problems first of
all so malaria is obviously a big
problem because you get different
strains and they have different variants
and they have different susceptibility
to different types of drugs and in the
third world where drugs are very
expensive you want to make sure that
you're getting the right drug which will
treat the right strain and so what we're
trying to do and what we're ramping up
and scaling up to do is to identify
these different strains so that we can
deliver the right medicine at the right
point in the third world and make that
more more cost effective in terms of
hypertension we know there's a genetic
link and we know a lot of people suffer
from it so that was a natural thing same
with diabetes yeah
one percent of the population is not
truly immune to HIV right done any
genetic mapping to try to find what the
genes are to be uh we haven't no I I
don't know I don't know it it's more
than likely we'll look at it we're doing
more and more virology but as I say
viruses are very very different very
very difficult sequence same with other
pathogens yeah again I think it's
something we'll get to not this week
yeah good question so in a miraculous
feat of English planning our data center
our data center is at the end of a world
war two runway so there's a raf doxford
if any of you been there just outside
cambridge they fly world war two
Spitfires and tiger mauls and all other
things at the end of the runway our data
center tell me if you think that if
that's all right so what we're most
concerned is yeah that the thing will
burn down or that a tiger moth will
crash into the side of it but seriously
backup is a really big problem for us we
have so much data online all the time
and we don't archive anything off it's
always spinning and it's always
available to ever anybody wants it so
backing it up is a big big problem and
we have these big tape robots so that if
you guys have seen these these big swing
cool things with your hands are going
awesome to watch and we store those
types of sites but if disaster did
happen say we had a fire datacenter
burnt down it will take a year just to
recover the data from the tapes so we'd
be out of action for a year so it's a
really big problem and again why we need
to look more distributed approaches yeah
yeah yeah we've got a beautiful like
stainless steel and glass building looks
awesome but I'm not sure it's going to
withstand the Spitfire to the face yeah
yeah so we have we have a lot of key
collaborators around the world either
with good partnerships with vendors and
good partnerships with other
universities particularly those working
with cutting-edge large data sets and
primarily there aren't that many fields
in science of tradition you had to deal
with these very very large data sets so
it's traditionally the high-energy
physics so the Large Hadron colliders
and the astronomers who do the Sky
Survey sleep they're basically looking
for moving were moving items in the sky
every night particularly things that are
getting bigger and and they have similar
data requirements and so we work very
closely with them to to make sure that
we're doing the right thing and what I
would say is it we're not sure we're
doing this right but it's just what we
found has worked in a relatively short
space of time any more questions couple
more yep our algorithms I don't think
are but there is this I think was this
Shaymin or something they used something
something of ours to generate a whole
tune so it's on one of their one of
their buying is on one of their organic
albums but that was generated using
using some piece of DNA or DNA algorithm
but I don't interview thing you like in
terms of Pandora and a classification
yeah so I don't think any of our
algorithms use further which is a pretty
yeah
sure so so the question is is it all
software or are there physical
interfaces that we have to implement
with robotics and the answer is it yeah
we use a lot of robots because we
searched high-throughput we searched
high-throughput system a human basically
couldn't handle it all it's much more
efficient for a robotic for a robot to
do it and so yeah so we work a lot in
automation physical automation and it
can all get a bit Heath Robinson but it
kind of works you know this we do a lot
of bar coding and so as these plates
with samples they have 96 samples loaded
as they whiz around the lab barcode
readers read them at various places that
we know where they are they get loaded
into the right place on these large
flatbed robots and then picking handles
will come up and select bacterial
Pickers when we're doing the original
human genome so we grow up recombinant
DNA in bacteria and then pick the best
colonies automatically all that sort of
thing as you go to go to the Wellcome
Trust Museum at near and just how
Houston in London there's one of our
robots working in there so you can take
a look at that also Henry welcome who
started the Wellcome Trust had a
slightly unusual English trait in that
used to collect many many thousands of
identical things and so if you go to his
museum there's like a thousand glass
jars all exactly the same or a thousand
fake limbs or identical is bizarre
anyway yeah we've got time for more I
have absolutely no idea okay thank you
very much choice
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>