<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Learning Semantics Workshop: Towards More Human-like Machine Learning of Word Meanings | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Learning Semantics Workshop: Towards More Human-like Machine Learning of Word Meanings - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Learning Semantics Workshop: Towards More Human-like Machine Learning of Word Meanings</b></h2><h5 class="post__date">2012-02-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CQzQJBpvyts" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">sorry another level of time same stuff
no I'm just I'm curious of how saw one
of the other jobs that i gave more words
all right it's all go very quickly over
a little bit a little bit of overlap and
i won't say much about so the reason why
i'm interested in doing bird meetings
and semantics because i think another
time absolute sending a tuna commission
but i'm super sitting in any interested
in studying it in the context of nips
and machine learning be collecting when
you study how we learn semantics it
forces us to to change in the large and
rich or idea of learning there's this
paradigm that you know this is built on
basically it's this idea that learning
is statistics on a grand scale taking
data high dimensions binding structure
supervisors about learning is all about
finding structured in high-dimensional
data and we know how to capture this in
the math of authorization we know how to
implement some of this in nothing you
know it's neuron lighting that we know
how to relate to neural networks and the
rain bath what gives is about and this
kind of math taking pic the outside of
neuroscience context has been really
useful in buildings real-world useful
systems that you start to realize it and
it looks kind of like the promise of a I
whether it's facing the death detection
or Google you know could become
commonplace but it's it's worth still
recognizing particular talking about the
reading semantics that that Google you
might not have thought of it originally
as an AI system but it's it's you know
basically the world's best AI semantic
system that a maybe cuz imagine if you
so if you went back
and impress that achievement or you know
maybe most recently on the IBM Watson
system for playing jeopardy which beat
some the world's champions at this game
these are systems that take the real
world language data and and definitely
get sometimes meaning out but I want to
highlight what some of the limitations
of these systems are because they sort
of you know point the way towards the
need to actually understand semantics
and I think it's going to require
enlarging our toolkit statistical
learning so it's just a couple of of my
favorite examples here google spelling
correction again might not think of this
as a semantic exercise it's a
statistical pattern recognizers mean
it's good see notes this is basically
n-gram technology which is a hard also a
lot of other applied MLP constitutes it
works amazingly well you can type in a
slightly garbled version of that curry
you can all read it lougle not only
reads if it gives you a useful answer
but not take something like this anyone
want to read this it's kind of hard to
read but anybody sees you something
something yet let me rearrange the the
order of the words
okay all I did between here and here was
rearrange the order of the words and of
course you might have seen people send
these things around on email these
little cute example as it turns out you
can read from the rearranged the letters
pretty much massively with in words and
people can still read it if you preserve
two things in to preserve this this
sequential structure of the sentence and
ask to be a meaningful sentence so I
would say this shows that your intuitive
abilities our natural cog abilities
force or finding the signal denoise of
language you might not have thought of
spelling correction as a semantic
exercise but i think that's that's a
very concrete way to see it of course
google has nothing to say Ivan Watson
again this is impressive as anybody
watch this this thing on on TV yeah um
the the of to me that the neatest thing
about you can watch these on you on
YouTube here i'm just going to routine
the four th DS number uh thing okay
meets up with some article a row it was
vegas at the benjamins 40kg not know
you'd be rude tens how much had you can
count but pretty good an idea for okay
so the key thing to look at here is look
at the second inverter shows it's neat
that they show you their first choice
and then the second and third one place
um didn't quite get that one but there
were the most striking to me is how
often it is the case that we're
repulsive whether i apologize to mark
our regardless of whether yes voici gets
the right answer this the second of
their choices are often make no sense at
all it looks completely semantically
anomalous the best in korea
and oreo cookies are introduced and for
the 20s no Watson what is 1920s it has
no speech recognition switches is nobody
around or something oh that may be to
establish business right what is gambler
Walter I maintain hundred started
shelling on the scores you know all
graduated in the same here what's up
what is no sorry class okay you get that
you can you combine says I just kicked
around the plan it's particularly good
or bad but you can see cases where where
it's where it's definitely saying things
that are not there not just like sort of
wrong answers but they're just like the
answers that no human whatever whatever
think so what's the gap here well you
know without saying too much about how
you built it these systems which are
really impressive cheese I don't want to
diminish them I am many cognitive
scientists are trying to understand in
computational terms how children are
able to learn the abstract meanings of
words and what I thought I'd do here is
just give a survey of some of the work
that we have with people in our field
doing trying to get some of the really
interesting things about how humans
learn works and I guess I titled the top
towards more human-like machine learning
most of the things I'm going to time
that you aren't really very useful
machine learning techniques they're more
like using the language machine learning
and stretching it to Tabitha the human
learning of semantic concepts in ways
that hopefully will be somewhat
inspiring for machine learning people to
try to move more in this direction in
the kinds of applications that we care
about Matthew so some of the things that
we're interested in a how children can
do one shot learning how we can learn
the meanings of words from very few
examples and how that's supported by
what we could call it n learning to
learn so this is something I talked
about
and one of the morning workshops and
i'll just go very very quickly over that
but then focus more on abstraction like
how we can learn abstract concepts or
types of concepts concepts that have of
course with the ring to it don't
actually have a direct perceptual coral
that doesn't mean they are grounded what
we'll see is that these concepts are
grounded but they're kind of cognitively
graphic the ground lead and asbestos for
the role they play in an intuitive
theory and then that theory grounds out
an obsession we're very interested in
learning context sensitive language like
will show you can just very simple
adjectives for example that you might
take for granted you think you know what
they mean it turns out to be quaint
Souls subscriber they mean you need to
use interesting kinds of programs to
describe how they work or function words
words given like the or every or three
or four of words that don't seem to have
any direct reference but they're lelu of
compositional semantics that helped give
language at the level of a sentence its
meaning I think that's probably enough
what I hear and I was sort of trying to
give a tour and emphasize a couple of
themes ideas ideas that that that we
think a photography system can explain
how human shoulder able to learn these
things the idea of Bayesian learning
over probabilistic models with certain
kinds of forms that sort of go beyond
the simplest kind of machine learning we
need to look at they all have rich
hierarchical structure they often
capture the generative processes or the
causal process of the world in richer
ways then we currently are used to
tapping machine learning and then lets
you know really the fire of semantics is
they have this compositional language of
thought character to that and I think
that's the theme that's oh I didn't see
a lot of the toxin workshop so far it's
certainly a theme that that I think
people have been emphasizing certainly
the last pot and it's really what brings
us here just one other kind of
motivating example and I'll come back to
this at the end I'm very interested in
the origins of the ultimate organs of
knowledge and I think it it highlights
some of the issues in some
position that people in developmental
psychology focus i'm looking at
analogies between children's learning
and the origins of scientific theories
so if you ask yourself you know how to
Newton learn or come to this knowledge
like the universal law of gravitation or
how the men will come his theory of
genetics you know they observed data and
arguably particularly in in Mendel's
case we're all very familiar with that
but even in Newton's case where the
astronomical data was pretty noisy
there's a kind of statistical inference
right you can't just deductive Lee read
these things off of the data but you
also can't just crunch them through some
learning algorithm there's no as far as
we know you're so convex optimization
problem that leads to this being the
output of you know the best description
for orbits of planets right we're or
this theory you know specifying the idea
of genes and alleles and dominant and
recessive and the whole sort of
combinatorial structure that that men
will came up with their rather it's
something more like kind of programming
duction and this is a concept that is
you know it's it's sort of scary one but
I think many of us were interested
learning semantics sooner or later come
come down to it or come back to it right
so that somehow you could imagine that
there's some kind of program that
describes that bad semantic content of
one of these theories like the other
theory of natural selection that leads
to the origin of species or something
you can you can express new laws the
kind of program that's maybe a
cabalistic program in order to explain
the noisy data and certainly Mendel's
genetics is a classic and probabilistic
program where you you have abstract
functions and the probabilities on the
things you don't know and like little
late variables the original genetic
state of the population you start out
before you cross several generations and
then you compare different hypotheses
which correspond to different different
models in the space of public programs
you compare their belief you motivated
and some broadly speaking bayesian
approach where you have a prior the
tends to favor simpler
programs and you have a life which is
how well these stochastic programs can
capture the distribution of your
observations that at least gives you a
kind of a scoring function and then you
face this terribly difficult and
terribly scary search problem but well
you know it's our best understanding
that that's smoking a way to think about
the origins of scientific knowledge I
think that when we start we start to
think about semantics we're going to be
coming back to things like that so I'll
go very quickly through so these were
learning things but that probably just
skip it except i want to just highlight
how working on these problems started to
get us thinking in terms of program
induction we want to understand how
given just a few examples like that you
can learn this word against you know if
it's some of you probably saw the slide
before but again it's not very hard to
see with those several tufas you can say
well that went there on upper left is it
too but the one below it isn't the one
below that third one down isn't but the
one that the right probably is and we
can think about what's going on here is
performing some hypothesis space that
captures the structure of these objects
something like an evolutionary program
doesn't have to have natural selection
in it but something like know if you saw
that seems some of those sort of
coalescing based clustering model some
kind of hierarchical tree structure that
explains how these objects might might
be derived and rid of it this some kind
of branching process that captures these
super categories like you could say all
of these are some kind of funny plans in
that corresponds to a high-level branch
these tufas down here or maybe some
corresponding to this ranch and by
taking this they taking this sort of on
you unlabeled set of objects and
deriving something like this kind of a
hierarchical causal generative model you
can generate a hypothesis space forward
learning where the words would now map
onto branches of this tree and that
turns out to be a reasonable way to
capture how children learn and
generalize one or a few examples in this
sort of word working set but I don't
want to dwell on this I want to focus on
the questions of where the hypothesis
space come from like how do you know
what what are the relevant features of
objects to explain with something like a
tree-structured model well this is the
kind of
that again I talked about this in the
earlier morning tuck in one of the other
workshops and I won't go into it so much
but this is where the Russell khuda na
man I've been doing also with Antonio
trava we've been trying to understand
something about how you can learn what
features count forward learning and it's
again this idea of a tree-structured
kind of model that captures classes and
super classes to be able to say
something there so there's some sense in
which similar categories have similar
similarity metric seen using this idea
we can help you know learn from multiple
related examples of different classes we
think this is it this is a basic thing
that human children are able to do it's
it's been really interesting work and
developmental psychology explaining
something about how how trying to
capture how children's I'm realizing
this is I'm trying to depress a whole
talking to a couple of slides probably
no not going to make sense anybody but
anyway the really interesting work with
developmental psychologists have done in
trying to trying to explain how
Children's Ward learning accelerates
very dramatically around around the
between one and a half and two years of
age and this kind of learning to learn
in a hierarchical Bayesian model is is
it is a compelling way we found the
description I think I'll just say which
I got this that this paper that Russ and
I wrote if you're interested in that and
come to lift come to our first sort of
program induction and a piece which is
to say well it's one thing to be
learning to learn in a hierarchical
Bayesian model but how do you get the
idea that you should be building some
kind of a tree-structured representation
at all and then this is worth it Charles
Kemp and I did where we try to describe
very simple kinds of programs that could
capture the abstract structural form of
different kinds of representations to go
to be able to say well you see something
like objects which are you most animals
or plants they have some features and to
recognize that something like a tree
structure this is the tree that's
learned from it a data set of animals
and their features is its it is the best
way to capture the structure there
as I'd say just going in with a higher
clustering algorithm in and just sorting
think hierarchically assuming that's
what you're looking for but actually
trying to learn that for this data set
something like tree structure is the
right way to capture what's going on and
what Charles came up with a clever way
to do that is likely to basically giving
simple kinds of graph grammars which are
very simple kinds of programs for
growing out structures and then letting
me infants in a hierarchical Bayesian
model where the top level is some kind
of a very simple rule for growing grass
and then that that basically generates a
prior on models which with that at low
level below that which then using some
of the tools of basically gaussian
graphical models the standard kinds of
ideas where you define some kind of
smooth distribution of features over
these objects with a sort of a
covariance structure of a gaussian
process that corresponds in versus the
graph laplacian sore fancy language for
basically saying things that are nearby
mcgrath tend to have the same features
that's the way if you do inference in
this high of the model to learn not only
the best save trees reprographic to
learn the very idea of the tree by
scoring at the top level these
qualitative ly different programs for
growing different kinds of structural
forms representation and you apply that
same kind of framework to say booming
data in this case this is how the US
Supreme Court judges voted on a theory
bunch of cases and there you discover a
qualitatively different structure this
linear left right structure with more
liberal judges are on the left in the
more conservative ones over on the right
you can take cross products of simple
structural forms and get for example you
take the distances between cities on the
globe and you get this cross of a chain
in a ring but sort of a cross product of
two simple graph grammars which
corresponds to latitude and longitude or
you take for example data on faces where
we generated these with the face of the
sizing program which varied a--they the
race and the masculine the gender
dimension of faces and their this
algorithm discovered that a cross of two
chain structure rules
this is the best way to describe what's
going on that data the reason why I want
to highlight this from a semantic point
of view it's disabled if we can if we
want to learn representations we could
do unsupervised learning in any kind of
compressive system need to use a deep
belief that we could use a Molson
machine whatever but having a model like
this extracts representations with this
abstract structural for that is
semantically meaningful right in biology
these internal nodes of the tree those
have names to like fish or mental right
in this case we talked about liberal or
conservative we talked about that right
what we're talking about it parts of
destruction those are more abstract
concepts that in order to understand
what those words mean you have to know
that you're talking about this
one-dimensional spectrum or latitude and
longitude what are those words mean they
refer to the abstract parts of this for
they don't correspond to individual
concepts or nodes we started off with
but it's something the possible meanings
of those words are discovered by this
kind of an album or you know these
dimensions here where we talk about
masculinity or we talk about black or
white racial dimensions or ethnicity
again those those words matter and their
semantics are in a sense you know this
is in a world learning model but it's a
model that's able to discover the kinds
of concepts that could be the semantics
of those abstractions so this is worth
it you know sort of summarizing Suffolk
that we did up until a few years ago and
then what we've been doing more recently
is really getting more at language and
so I just want to tell you about work a
couple of students from our group one
comes from a thesis by Lauren Schmidt
who is studying learning context
sensitive meanings in the particular
case of Gradle advocates since I wasn't
here this morning i'm not sure if people
argue talk to us anybody talk about
adjectives like tall and i want to
reverse a familiar example it is but
it's pretty interesting right these are
very simple words like halt or heavy
light strong wood and we might think
that their meanings would be pretty
simple to write what the meat all is
basically although Paul we see it ready
oh yeah
the higher the tall for that sort of
thing but think about the context
sensitivity right the sense of tall and
of a sense of height that corresponds to
being tall if you're a tree is different
than if you're a boy right a tall boy
would be very tall for a tree or it's
all building right it's all treat
wouldn't be very tall for a goalie so
that so what tall means has some kind of
inherent context sensitivity something
like this wait you're tall tree tall the
concert reading something like greater
than the mean value for trees on the
dimension of height and you can write
this in some kind of a simple program
and you know i'm not an expert in lambda
calculus i just hack this up quickly so
excuse me if there's a mistake here but
basically what we're saying is tall as
some function which will take in an
instance in a class and say is it true
that this thing x is tall relatives of
that class and then we're describing
formal content there is something like
check if the height of of x is greater
than the mean of a height of some sample
so we want to assume we have some
function which you can sample instances
of the class say we do that 10 times you
take a little statistical sample of the
high you take the mean and we track x is
greater than me now i go the trouble to
write it out like this because i think
many of you might look at this
definition and say well is that the
right definition of height America tall
what do you think much greater yeah my
must really like we might say greater
than one standard deviation above the me
right it's not just enough to be in the
top 50% right so then we can write that
down with a slightly more complicated or
we know mean it's not one maybe at point
five we don't know right maybe there's
just a free parameter here that has to
be estimated one with the structure of
the program where it turns out when we
started looking at this we actually did
experiments in psychophysics it seemed
like some kind of Morris or ordinal
statistics like want
more like greater than the 65th quantile
up trees on the dimension of time you
can write that down too and we did
experiments which I won't really go
through the details of the experiment
but basically we gave people various
some distributions of things this is
showing you some of the stimuli and they
had to pick out the tall ones right and
then we tested various models of what
tall convene is it defined by some
number of standard deviations above the
mean is it defined by some sort of not
nonparametric quantum statistic we also
found a lot of success with cluster
models in which it seemed like what tall
might mean is cluster the objects along
the dimension of height and then find
the highest cluster and that's the tall
ones and we were able to not really
distinguish very clearly between those
models on through these one of these
experiments that later experiments were
able to kind of tease apart from it's a
somewhat messy story like a lot of
semantics but the point I want to draw
your attention to is if need be learning
with the hypothesis that looks like
those things now suppose you want to you
want to go go beyond as well the one of
the interesting things we see when we
look at gradable additives is it well
there's a whole class of words that have
this form and we might want to be able
to capture that semantic abstraction
right so a whole class of words which
have a meaning like this relative to
some classy it's greater than some
quantify on some dimension and we even
write down that abstraction right as a
higher level kind of lambda and I think
there's evidence that kids actually
learn that in the sense that once they
should get the idea of how these
attitudes work they can now learn new
great left is for new dimensions very
quickly because they understand how they
work or take something like good or
strong which are particularly
interesting ones where unlike hall like
you're tall the dimension is fixed part
of the word meaning right like tall
refers to the dimension height but good
it so it sort of has a category specific
dimension right like the mood man is
different from a good cheeseburger he's
different from a good conference right
in each case you the class doesn't just
specify the reference set that you're
going to compute if they're also
specifies what's relevant variables so
for four men you know a good man is
something they do like ethical whereas
for cheeseburger at some
dimension of taste and for conference at
some dimensional of your intellect
flawed social stimulation or whatever
right and so good is is a similar kind
of word as strong as kind of like that
but even more abstract so these are hard
but you know approachable challenges I
think um here's some work that Steve
counted OSI has been doing he wrote a
very nice thesis looking at a bunch of
different sort of aspects of learning
functional language 11 case study that
looked at was the case of learning to
use basic number words like 1 2 3 and so
on currently because there's been a lot
of empirical work recently in the last
decade or two in cognitive science
looking at how kids learn these words so
the kind of task if you do with kids
here is you might say you know can you
count the balloons and pretty early on
no bye-bye each two and a half for smart
kids they're able to do that smart kids
in our society logo they'll learn
accounting routine one two three four
five six right and if there were ten
they might know of ten they could go on
your question every nine ten but there's
evidence that that's really just an
alert routine not a real semantic
abstraction and one way to test where
the kids really understand what number
where it's mean is the so called you the
end task so you ask the kid can give me
three balloons so if you take a kid who
has no trouble counting up to three or
certainly 26 and you say can you give me
three balloons well at the age of which
kids are first able to do the counting
thing they're not very good at that they
might not they might give you a random
number we're in typical age of say two
years 10 months kids are out of stage
what's called being a one knower that
means if you say can you give me one
balloon they'll give you one balloon but
if you say can you give me two or three
or four balloons they'll just give you
some arbitrary number more than what
that's not it means it's a little bit
more or more cut more subtle than that
but that's four or less what happens and
then a few months later kids become at
to know ur right around the age of 3
typically these are just hypothetical
ages so they'll if you say can you give
me one balloon and you want to be
seeking you be too to give you two
v3 they give you some number more than
two sometimes three sometimes four words
long and then there's a three-member
stage which takes another few months and
you could imagine going on and on like
that for a no or five knower and sooner
or later they'll get to you whatever but
doesn't work that way there's this
really interesting leap of abstraction
that happens typically after the three
dollar stage occasionally there's a 34
in our state where kids sudden to get
all the other word meanings not this
isn't the same as kind of discovering
the screen infinity the idea that
there's no largest number it's more than
all the numbers which are either
counting 14 so all the numbers that they
can access when they go on to provide
between I 10 so on now they understand
how to fix those cardinalities in this
event s so somehow we want to explain
this learning curve why it takes
actually relatively long time while you
get this characteristic sequence of
Notre stages and enterings there's an
interesting kind of beep of abstraction
to the cup words what's called the CP or
the cardinal principle of this
literature so the way Steve tried to
tackle this is he said even starting to
give something that looks like a real
language of thought here give various
kinds of primitive functions that could
be useful for expressing numeira the
knowledge and you know right all of
these at lambda calculus which allow you
to express various kinds of of of states
of knowledge that you can write down a
three dollar as a sort of higher-order
function you can write down am so the
cardinal principle loader up there which
uses recursion to basically count down
the count lifts and mount that
correspondingly to the to the set sizes
you can now also weird things like a two
not one number or the two end number or
even how other other kinds of languages
have account systems which are more like
singular plural just want you know
anything more than two is or anything
what we wanted is to and the reason why
this is interesting from a cognitive
science point of view is because people
have emphasized there's this sort of you
know it's kind of poverty the stimulus
you like but there's this um you know
that the possible made of map words
under cardinalities is not really
determined by the data and we want to
how the hypothesis space was able to
express not only the actual lexicons net
children go through but many other
possibilities and so then done Steve did
various simulations where imagined the
other therapy simulates a child who's
getting mostly true but sometimes noisy
data of adult or competent speaker you
know referring to the cardinality of a
set and the they basically have those
are bayesian program learning set up
here where you have a prior over those
expressions which is basically a minimum
description length kind of prior and
then a likelihood which is how well is
that explaining observation taking to
account the possibility of noise which
could come from the fact that you're
getting data possible someone is using
the words incorrectly but it's also it's
more often the case that you might have
if you don't really understand the
pragmatics of how these words are used
you might have say these four glasses
here but somebody can say well I'm going
to take two glasses like you could use a
number that isn't necessarily just
referring to the obvious cardinality of
the set and we wanted to build in that
kind of her buses now the natural thing
that any machine learning person should
be asking here is you know if you're
going to define a prior over all these
programs and learning that like could
this possibly work or how can you make
it respectable it's not some simple
convex optimization and it's we don't
have easy solutions for that what we're
doing is the kind of thing that probably
most people in machine learning with
Blanche averages we're doing kind of
mcmc in defining probabilistic grammar
over these expressions so that won't
really go through the details are
basically this is this mcmc ottoman was
worked out originally by Noah Goodman
and it's the same inference algorithm
basically as in the church calls a
programming language which involves
making proposed basically having up
having a program derivation trace and
then making proposals where you take
some you cut off that program face at
some point in the tree and sort of
resample a new possibility and accept
the rejected metropolis tasting style
it's amazing that it works at all it's
not very efficient we obviously need to
do better but it's enough that on these
four
tasks you're able to effectively search
a very large space of lexicons there's
some pinot the number of possible number
languages here that the salary searches
in churches you know tens of thousands
of hypotheses and then we look at the
learning curve that is look at as a
function of the amount of data that the
child receives what what concerts they
learn in what order well you a lot of a
hold actually do the things that's a
post learner doesn't think the children
do mainly the highest-scoring sort of
number lexicon initially is the one
knower and then there's the stage of two
now in over four three know where a min
fairly quickly you get that CP knower
and the others for the four or five and
six nodes and so on just at this point
basically are too complex relative to
the fit of the data whereas that CP node
with the lead to that recursive function
ones up being the best community so if
you like that the kind of days off razor
meets probabilistic program induction
and it was this is really the first
model in the cognitive science that was
able to explain this characteristic
number learning curve Steve apply the
same kind of approach to learning
quantifiers and there won't be to go for
the details but basically to find a
similar kind of lambda calculus or
presentation for a substandard set
relational quantifiers like non every
summon most basically did the same kind
of Bayes e-learning set up and was able
to show that these um quantifiers could
be learned and to get similar kinds of
learning curves now the data on the
order in which children learn these
these up these star beings which are
these uh these quantifiers is not as
clear it hasn't been the study of such
extensive case studies but now that we
have models of making these predictions
it's motivating language development
researchers to go out and check on this
um how much time do I have events okay
um so the last sort of set of things I
want to come back to is what I was
trying to motivate at the beginning it
was harder for me the motivation to
think about program induction but also
it's it's this idea that that is one of
the most deepest important ideas in
developmental psychology that sits
between language acquisition and
conceptual development more generally
it's sometimes called the theory theory
this idea that children's knowledge is
organ
is interviews to the fairies that are
abstract systems and concepts that are
kind of like scientific theories in some
way and what words mean obviously in
particular where does it refer to
abstract concepts don't have a direct
perceptual grounding but are really
important to how children think about
the world because they label the
concepts total thing and how they learn
to think how we learn to understand
things that we don't directly observe
but we learn about through a combination
of observation and ristic interaction
with others so if this idea that we need
to be able to capture a hypothetical
candidate word meanings as some kind of
abstractions in an intuitive theory and
we got that originally by starting to
look at in a learning column all
business the sort of stuff that we're
all familiar with Lee you've directed
directed acyclic graphs that can capture
calls relations this is a sort of like
um are like network if you're familiar
with the classic disease symptom ones
that there's an extra set of variables
which capture these risk factors and the
standard and bayesian calls a learning
approach would be to put down a prior on
davits which just possible directed
acyclic graphs on these 12 variables and
observe patients who are samples from
this causal model and you could try to
do call the learning from sample data
it's a really hard problem to learn to
do structural in like this and it
completely bottom-up way with a generic
prior and work that we and others were
doing on not just purely on color you
didn't think that we were studying
semantics but I'll show you why me you
came to that we said well maybe you
could do call learning much better if
you had a high level hypothesis some
kind of abstract intuitive theory of
this domain which divides the variables
into these several classes we call them
behaviors diseases and symptoms and if
you know that let's say these variables
working factory smokings on our map
category behaviors and flu bronchitis
lung cancer and so no the disease
category and headache fever send copping
our symptoms and you have this xmas
seema that says all the tall the links
go
thesis to symptoms and so that you have
you're only going to be learning a bad
which fits that high-level schema that's
a hugely useful constraint for called
learning it cuts down the hypothesis
space from from hypothesis the space of
all possible bags onto all variables
which is super exponential reading the
number notes are very present something
like 521 gazillion really quite big but
if you have that high level knowledge it
cuts it down to only 131,000 and
learning is much more efficient now the
reason why this is interesting from a
machine learning point of view is that
we don't just have to wire that in as
some kind of hand coded knowledge but we
came up with with the way to learn that
actually this was work that Tom
Griffiths and Charles Kemp when I
started doing a Bachelor machine
learning side was done by the COS man
singha as part of his his undergrad and
masters thesis so he worked out was
basically a way to sort of reasons of
time kind of left off most of the math
but basically he worked out a way to go
into this one he worked out a way to
define a nonparametric Bayes and Chinese
restaurant process prior over groupings
of variables which subject to the
groupings one of which is shown up on
top then that gives you a prior on grass
here it's basically just a kind of a
probabilistic version of what you can
see up there and then you do inference
at all of these levels we're at the top
level your make your sort of trying to
decide which class the different
variables go in and then subject to that
that clustering up there that puts a
prior on grass and so then you're doing
infants at this level basically looking
and looking at possible arrows which way
the causal arrows go so the inference
here consists of MC MC it have these two
levels just for the two discrete
variables of class membership of the
variables and which edges exists in the
graph and it's it's quite remarkable how
well this is this is able to work it
doesn't you know we didn't figure out
how to scale up to very large problems
but what we show was how to take you rub
these relatively small browse if you're
still
very hard to learn from which here's a
22 layer sort of disease symptom
graphical model which requires something
like thousand samples to learn high
accuracy effusion if you're just
learning the generic prior but if you
allow yourself to learn at this higher
level where you consider the possibility
that the nodes can be divided into these
groups and then that puts a prior on
grass then you can learn from you know
order of magnitude less data maybe only
80 samples and even just 20 samples from
that graph you can identify those
abstract classes you can figure out what
the first six variables are kind of the
causes and the next ten variables of the
effects and that's get that kind of
getting the big picture first it's a
very human-like kind of learning but
it's also about semantics right because
words like diseases and symptoms those
are words that are important for him for
our intuitive concepts of to ative
theories in medicine and what are they
they're not nodes in the graphical model
or rather their labels for these more
abstract concepts and if you want to
understand how those words get learned
we need ways of generating their word
meanings which this kind of hierarchical
Bayesian learning is able to do sort of
an even greater extension this with some
words that Noah Goodman and tomorrow man
did where they said well suppose we
describe that the abstract models that
shared across different called networks
with some kind of first-order logic
theory that basically ones of capturing
the sort of pearl intervention semantics
of cop so in a sense we call this
learning to be called because this is
basically a lunar who knows about
directed aces with graphical novels but
doesn't understand the Pearl
intervention semantics of what it really
means to be a cause of sort of sort of
aerobraking semantics and I won't go
through the details but basically by
searching over a space of first order
laws that characterize what it means to
be an intervention this system is able
to learn how interventions work and use
that to learn better causal earnings
with in a sense it's kind of learning
what the word cause means the last
example I want to give is is again sort
of took or theory theory stuff it's
there's there's turn from from
philosophy philosophy of science
language called conceptual role
semantics which I want to go through
this quote because time is short but
basically there's this idea that words
take their meaning by referring to
concepts which have some kind of
abstract relational role you can see
this very clearly in the classic
scientific theories like what is force
and mass need Newtonian mechanics or
energy or momentum those words cannot be
defined if as philosophers of science
and stay in observation language there's
no way to sort of write a definition in
terms of axes and x dots for what forces
you can define acceleration that you
can't define force of maps another hand
the way to define force is mass as you
say F equals MA and then you observe
some objects in motion and you can you
can parse that you can beat off what the
forces and masses are likely to eat in
other words the words are defined by
their their meaning in that conceptual
system so this this is this idea of an
intuitive theory that that specifies
these laws these concepts when I was
giving been dealing genetics I mean I
what I went through this rough you know
quickly but it's that that's the kind of
thing we're talking about like these
concepts like gene allele recessive
dominant phenotype genotype those are
technical words in biology which you
know what do they mean that you can't
define them in terms of peapods right
there is abstract concepts which when
you understand the role they play in the
theory then you know what they mean and
you know what they mean is you know how
to use them to make sense of the data
you observe so places where this kind of
business comes up a lot in human
learning human semantic learning is for
example an intuitive psychology so if
you saw I showed some of these demos
yesterday in the language edition
workshop but you know just to show for
those of you haven't seemed to remind
you you
show infants these two ball rolling
around on a green surface but but adults
and even young infants see this as
something kind of psychological or
intentional like the blue ones chasing
the red one or in this classic Haider
and symbol business here you know you
can see this as two triangles moving in
a circle moving but people actually see
it as more like you know a big guy can
maybe bullying or reading up on this
little guy staring your way that guy's
hiding you know we see you see
psychology in even fairly simple motion
like this so what is this what's the
what's the intuitive theory here well
this is a few examples and work from
Chris Baker in our group or any try to
say all right here's a basic theory of a
intentional agent you have actions that
you can observe and those are caused by
goals and beliefs and irrational agent
basically tries to to choose actions or
plans sequence of actions which lead to
light likely to leave it to the
achievement of the goal set to their
beliefs and you can add in other things
like the goals are drawn from some prior
which is the notion preference beliefs
are third by perceiving the world with
some kind of general world knowledge and
again these are the meanings of words
that are absolutely critical in you know
in the kind of abstract concepts that
children use what they're learning about
how to understand people's paying your
words like gold or belief or action or
agent or a preference you know what do
you know who do you prefer well the
meanings of those words the claim is are
basically these structures in this kind
of abstract colors to quality and going
probably out of time years um so what
we've done this is the point where I
also stuff Skip's yesterday's sorry
for me but anyway so so we haven't build
models here of how we can learn this
kind of theory and we haven't really
tried to do Riddler what we have done is
try to build models of how this works
how people can use a probabilistic model
to find it this way to make sense of
agents goals or make sense of their
joint preferences and beliefs like to
read to watch somebody moving
environment and I prefer what they want
and what they know and we think this
provides that the hypothesis space they
basically for it these abstract word
means it's not perceptual grounding it's
i call it cognitive grounding because
the words correspond me but if it plays
the role they not they want perception
around we're only allow me to deal with
these abstractions if the theory is a
whole is what's grounded in perceptual
experience and the words refer to pieces
of theory based so just to wrap up then
I told you about a program of research
that very much in progress and the
language aspects of it are particular
very much in progress people are
interested in working on this you know
please come and talk to me but these
these aspects of one-shot learning
learning to learn learning abstraction
learning content sensitivity learning
function words are absolutely important
and we think that this toolkit of
learning with the sufficiently rich
representations things that basically
come down to calls that program we're
learning as a kind of program induction
is at least so the way forward for us
there's really hard a difference
problems which again we we need to we
need your help on that but we think that
at least this is the stove's in the door
how the most interesting parts of
semantics might be required</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>