<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Is your RDBMS letting you down? Applications of TV... | Coder Coacher - Coaching Coders</title><meta content="Is your RDBMS letting you down? Applications of TV... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Is your RDBMS letting you down? Applications of TV...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Q6_xwrkli9I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright thanks everybody for coming so
today our guest speaker in the TV Tech
Talk series is Aaron harsh from Rentrak
he's been with rent track for 12 years
rent tracks a company in Portland Oregon
that has a variety of businesses that
seemed to center around gathering and
processing and making useful large
varieties of data so they have a product
that works on box office rentals a box
office visitors they have a product that
works on movie rentals and Aaron's gonna
be telling us a little bit more about
that so he's a senior product architect
at Rentrak he's been there for 12 years
word is he's responsible for every
successful product I've ever shipped he
is also a victorious poet he won a
competition in fifth grade for poetry so
at the end question answer maybe he'll
share some of his some of his verses
with us so without further ado Aaron har
she's going to tell us about data
processing how to use it thanks Jeff
Jeff thanks for that magnificent
introduction before I get started
talking about the technical side of what
it is we do I'd like to give you a
little background about rent tracks
history just so you can kind of
understand our motivation for some of
the things we do and some of the
problems that some of the problems that
we've seen in how we approach them so
Rentrak started out 20 years ago as a
home video distribution company that's
the wrong slide first let me say red
truck started at 20 years ago public
company traded on the nasdaq under the
symbol rent in beautiful Portland Oregon
Portland is famous for the Magnificent
Rentrak headquarters building for Mount
Hood and for the best beer in the
country and we started out our original
business model which which is still
active today was back then video tape
distribution nowadays video and DVD and
our niche was that instead of selling
video tapes to video stores for fifty or
sixty dollars however much it was back
then we would ship them to them for a
nominal handling fee and then take a
portion of the revenue for the first
several months so if you wanted to get
500 copies of a movie and so you could
have an in-stock guarantee you weren't
spending ten thousand twenty thousand
dollars getting this end and if it was
you jog on video you didn't lose your
shirt but a side effect of this was that
we had to interact interface with all
the point-of-sale systems to collect the
video rental data so that we could track
it see how much money the video serves
were making how much of it was off tapes
we sent them and how much we needed to
build them for at the end of the billing
cycle so we built a framework in-house
for doing this and then someone on the
board of directors had the brilliant
idea as long as we have this information
let's make it available to the studios
and so we build a system we called it
home video essentials that does this and
here's a here's sample screen for their
we're looking at DVD revenue this year
versus last year and you can see you
know it's about the same maybe we're
down a little bit this year not too much
and like all of our systems home video
essentials is targeted at a small number
of small number of simultaneous users
the end users here are just the general
public these are senior executives at at
movie studios but the kind of queries
that they're running on the system the
kind of reports they're asking for scan
through large amounts of data it becomes
really important for us later on so home
video essentials is the only product
like this we're the only company that
has this view into what's happening at
the video stores and so the studio is
aided up they loved it and the Board of
Directors decided that maybe this was
were in trax real business rent trucks
business was not shipping videotapes
running trax business was supplying
business intelligence to in for the
entertainment industry so we started
looking for other places to this and the
first place we went was the theatrical
box office we found some people that had
had some executives that had a lot of
industry or experience in this industry
that knew all the key players all the
theater chains all the movie studios and
they explained to us how the industry
works and got us some data feeds we
started working on this system in
September 2001 and in the first couple
months of 2003 than we'd signed on eight
of the nine major studios as subscribers
to our system and just like with home
video small number of users a large
amount of data and also interesting here
is none of these reports are pre
generate this is all develop this is all
built as these are requests them and the
business rules are such that you can't
regenerate them if someone from 20th
Century Fox logs in
they'll see a slightly different view
than if someone from winn avista blogs
in or if someone from AMC Theaters logs
in so home so theatrical is a huge
success and we started setting our
sights on television and to get started
we thought we'd go into on demand and if
some of you aren't too familiar with the
television industry right now it's
undergoing a lot of changes there's the
traditional linear television that we
all grew up with you know you turn on
the TV Channel 8 is showing something if
you don't like it you change the channel
but nowadays there's options there's on
demand for instance so your digital
cable or IPTV operator might have a
large library of content available for
you whenever you feel like it choose an
item for this content maybe music video
maybe a movie maybe last night's episode
of The Sopranos click play it's
streaming down to your set-top box
you're watching over there so we built
this system and it's also a huge success
we have 60 networks subscribing to our
services now and we're collecting data
from a dozen cable operators an
on-demand was really interesting for us
it was different the other systems in a
couple ways the first way was that the
data is at the consumer level this isn't
on the box office I'd will get
information from a theater that says we
sold five hundred dollars worth of
tickets for spider-man last night on the
on demand side we'll get information
that says Aaron watched The Sopranos
episode 57 at 813 last night and he fast
forwarded three times rewound twice
paused one time and watched for a total
of forty-eight minutes before he stopped
and so this enables us to do a lot more
analysis than we could do on on the
other systems and our customers really
depend on us to do this analysis we do
for example will give someone a report
that lets them see what's the overlap
between these two titles how many people
that watch spongebob also watch The
Sopranos or how many people that watch
The Sopranos are new to on-demand that
have started using on-demand just
because they want to watch The Sopranos
oh I supposed to being long time on
demand users that just started watching
it when it came out so fantastically
successful system really gave us a
foothold in the television industry and
set us up for the next project so my
current project is linear television
essentials here's the screen shot forum
are from our demo system here we're
collecting information from set-top
boxes from
digital cable digital cable subscribers
from satellite subscribers for IPTV
subscribers system still in development
so this is this is all demonstration
down and none of this is real but but
the back end is there and we've actually
on a small scale built a system that can
they can handle this that can build
these reports for us as actually showing
populated reports the latest track
second-by-second how many people are
watching this program did they change
the channel when the ads come on do they
leave when a gory scene comes on or do
they are they even more likely to say
when a gory scene comes on and this
system is also a large amount of data
and I'll talk about the scale later on
but it's something on the Google scale
will say so this is this is my
explanation of Rentrak but last week one
of the Wall Street analysis sites did a
story on Rentrak and this is what they
had to say for us let's see if the
speakers working here mmm Hollywood
studios all right let's try this again
and in early two thousand three Rentrak
inked a deal to be the data pimp for
almost all the major Hollywood studios
so they called us data pimps and I don't
know that's necessarily inaccurate but
we say that we provide business
intelligence to the entertainment
industry so if you're going to sit down
and build a system like this to collect
data report it back to someone else then
you might be forgiven if you think what
I'm going to do is I'm going to take
this data loaded into relational
database and write some sequel to pull
it out and populate some reports and
there's you know a lot going for that
approach relational databases today are
very mature products they're about as
fast as you could expect something like
that to be they gave you a lot of
confidence that that is going to be
there that you're going to see a
consistent view of the data they do a
lot of fantastic things but there's a
lot of problems with them too and the
two problems that we saw first one is
that databases the API in particular i'm
talking about sequel here this is what
almost everyone uses when they're
working with relational database sequel
is I think to low level to efficiently
develop against for a large system like
ours but on the other hand it's too high
level to get the kind of great
performance that we really need on some
of our systems and I'm not going to go
into great detail on why the relational
databases are like this or look at me
done to change them instead I'm just
going to give you kind of a case study
of problems Rentrak saw and how we went
about addressing them so when I say that
they're too low level one problem is
that these queries are necessarily tied
to a database schema you have to
predefine your tables views stored
procedures and then your queries need to
know about all these things and one of
our systems will have 100-plus reports
on it if every one of these reports
needs to know about the existence of a
table this makes it difficult to
refactor the database makes it difficult
to do or either for performance or for
maintainability if there's not a lot of
ways to abstract this out we have views
and we have stored procedures but that's
it both of those are really useful at
some level but they're they're really
not the end-all in terms of you know
building a truly maintainable system but
on the other end it's just too high
level to give us the kind of performance
and this is a lot of this I think is the
fault of sequel sequel is a fantastic
language that we get a lot of benefit
from it but it has difficulty expressing
a lot of queries a lot of things are
useful to the business community in
particular it's not particularly easy
in sequel to express a query that gives
us kind of customer level analysis that
we're interested in how many of these
people also did this or how many of
these people who are new or this kind of
thing besides sequel relational
databases want full control over how you
get at the data they want to be able to
make the final decision about which
tables are access in which order which
indexes you're using how it's going to
go about doing it and they almost all
stole their data in records which are
not a particularly efficient way to the
store the data so we've we've handled
this in two different ways the first one
is to put an abstraction layer on top of
sequel so we increase maintainability by
putting something on top of relational
databases API to let's this program
directly to what we consider to be the
problem domain and underneath the
relational database we've added code
that lets it do the kind of analysis
that we're interested in this consumer
level analysis much quicker than you
could do if you trusted the relational
databases plans so this layer that we
put on top is essentially a translation
engine when we build a report on one of
our systems will say we would like to
show these pieces of information these
pieces of information were defined by
the developers in response to talks with
the end-users been explained to us we
calculate this metric as follows we'd
like to see this piece of information
after this we build a translation engine
that knows about all this knows where
the data comes from knows how to
calculate it and then takes the
definitions of our reports defined in
terms of the business logic and spits
out sequel or for a system you know that
maybe sequel is inappropriate for so
it's out code directly work with that on
the file system to handle the
performance problem then we've linked in
our code directly in the database to
handle all the real heavy lifting on our
side all our really large projects are
using postgrads as the backend and we're
using postgres because you can write
your own code you can link it in load it
directly into the server's address space
you can work directly with the data on
the records without a lot of overhead
and do a lot of high performance high
performance tasks without too much work
it also lets us handle queries that are
difficult to express with
well but straightforward to express in
C++ for instance so here's an example
report this is from our video on demand
reporting system this is showing this
total number of orders for each network
on demand and it's got a couple other
metrics that you know make a lot of
sense for for the business users in that
community I don't know if they make so
much sense to us so we want to see total
number of orders total number of
different programs number of different
subscribers that watch that and the
average time that they spent watching a
piece of content on that network and you
can see along the type along the top of
this that we've got filters that the end
users can use to customize this and
because we're letting them change what
geographical region to show what what
type of content to show or what date
range to show then we can't be generate
this report this has to be built odd it
has to be built at runtime interactively
while the user right as soon as the user
clicks go so here's the sequel to built
this report and this was automatically
built by our translation engine most
people would I think probably build this
by hand if they're using a relational
database and you know it's not too bad
it's about 15 lines maybe and you know
it's it's mostly understandable if I was
going to build a system that only had a
couple reports on it almost certainly I
would write some sequel like this and
just pull the database or pull the data
directly out of the database and people
do that all the time but there's some
problems with it I think the first one
is that I've really backed myself into a
corner about how the data is arrived at
in the first place i explicitly
specified where the data comes from
every arrow here points to a spot in the
database where I've explicitly specified
this table has the data that I want and
there's two problems here one of them is
it keeps me from keeping me from doing
refactoring without of it without having
to also change all my reports and the
other one is it's a performance problem
for performance reasons I might want to
put my data on several different tables
and dynamically choose based on the
query based on the use of us looking at
it based on the parameters that they
chose when they wanted to look at the
report dynamically choose which of these
tables to pull this from because I'm
explicitly specifying this in the sequel
I can't do this in the business logic is
spread out I mean with
one query like this 50 lines then it's
not a huge problem that it's not a huge
problem that any particular point on the
report is coming out of several
different spots in there but I've got
hundreds of reports of my system I want
to make sure that everything is
maintainable as possible and make sure
that it's obvious too obvious to someone
looking at the code that this builds
exactly the type of data that I'm
interested in so for example this order
rate column third from the right is
actually defined in several different
places I've got a formula in this outer
sub query I've got something in a
sub-query inside that I've got something
in a sub-query inside that I've got this
whole section down below that I'm
joining with which is they're mainly to
process that order rate column and then
I've got to join down there below
explicitly specifying this is how you
should join these tables together this
is this is what you need to do to
calculate that one home and then I
started filling in the rest of these
things and I got about eighty percent
done with it and then dinner was almost
ready and it smelled really good and and
it was I mean it's kind of boring it
about to tell you the truth so I stopped
doing that so here's here's the code
which is actually from from Rentrak
source code base this is the code that
we use to define that report I don't
know how many people can read this in
the back but basically what I'm saying
is I want to see the name of the network
the number of orders the total number of
titles i had transactions transactions
revenue yada yada yada basically one
line for every column that ends up on
that report our translation engine takes
this converts it to the sequel runs it
builds the report for the end users and
if you flip it on its side that might be
cheating i'm not sure but if you flip it
on its side then you can really see that
one line in there actually corresponds
to one column on the output report so
besides making the code a little bit
more obvious then this also gives us a
huge benefit with a hundred-plus report
we need to make sure that everything is
consistently handling the same business
logic and on our systems there's a lot
of interesting business logic at the
very least we've got situations where we
have hollywood video and blockbuster
video and they're not allowed to see
each other's data and if one of them
ever sees any of the other one
data this is a huge problem they're both
unhappy and they're both angry at us and
that makes us unhappy aside from that on
systems like box office we've got a
hundred years of interesting business
rules there's sneak previews which have
really interesting logic on how exactly
the data is calculated and who's allowed
to see it we we don't say double
features actually we say multi features
because there's triple features
quadruple features I think we've seen
quintuple features out there and
nowadays 3d is surprisingly making a
comeback and 35 millimeter versus
digital is also also really interesting
right now aside from that we have been
users that maybe someones in charge of
all the Spanish print distribution for
box office he's not allowed to see the
French data the French guy is not
allowed to seek the Spanish data neither
one of them are allowed to see the the
revenues for the original English
version we have to make sure this is
consistently applied all these rules are
consistently applied across every single
one of our systems and especially on the
new businesses on demand television or
some of the really interesting things
that are happening on digital television
or IPTV we get a lot of new business
logic that comes in people are still
trying to work out you know how to do
this what's going to make what kind of
data can help them give the injuries are
the best experience and help them make
the most possible money so we need to be
able to adapt to that quickly we need to
be able to quickly make these changes
across the entire source code base and
even on an even on a system for a really
robust robust industry with a lot of
history like like the film industry even
though that place has been around for a
hundred years Hollywood is a creative
town they come up with a lot of
interesting business rules all the time
and we and they expect them to be
followed to the letter and implemented
very quickly and if we had to update 100
different queries 100-plus different
queries different versions of the query
for the cable or for the theater chain
different version of the query for this
studio different version for that
theater then this would be a nightmare
maintenance would be would be a complete
disaster
so for example let's say we're going to
add a new business rule in our new
business rule is that HBO wants to be
able to flag some titles as top secret
and then when Showtime logs in or any
competitor logs in they shouldn't be
able to see the rating or the sheriff or
for this program on this particular
query might need to add a where clause
I'll probably need to join to a new
table that keeps this type of
information and the queries
user-specific HBO is going to get a
different view of this and then showtime
is and we can handle that just by you
know passing a different parameter the
report but someone like Comcast for time
warner cable is going to get a wildly
different version that's not just
filling a different parameter to one of
art one of the work losses but with our
translation engine this is just no
longer a problem for us the problem is
solved we make the change in one central
place teach the translation engine about
this new business rule about when it's
appropriate to join in a new table it's
actually all constraint-based we just
described to it we also need this piece
of information this piece of information
is on threes three tables it'll
automatically figure out for us you need
to join here you need to do this this is
how you need to go about calculating
this data it's consistently applied
across 100-plus reports put in some unit
tests to make sure everything's working
fine and we're done so as far as we're
concerned that problem is just
completely solved so you might be
thinking about this and think I should
go I should build a translation engine I
should convince everyone at Google we
need to switch to relational databases
stop writing on file systems or whatever
it is we're doing and then we're going
to use Aaron's brilliant idea start
building a translation engine and I
think that you know like I said if you
just have a couple of different queries
the last thing you want to do is build
one of these translations engine is a
huge amount of work we've build it up
over the last seven or eight years and
we learned a lot doing it right now I
think we've got something which is
unique in the industry which is
specifically designed for the type of
business rules that we've seen coming up
over and over again not just in the
entertainment industry but and some of
the other industries that we've worked
with and it's completely indispensable
now so this solves the problem I think
of sequel being new low level we solved
this other problem with
the database being too high level and
what I mean by too high level this is
this is the thing about relational
databases that I've come to learn over
you know the past 12 years at Rentrak
think about relational databases is that
they're very very slow and we get a lot
of data in a trench rack and so this is
an issue and let me talk about some of
the volumes that we get so on the box
office side we get end of day revenue
figures for each of five thousand
theatres so each theater will say on
spider-man we made two thousand dollars
on shrek we made one thousand dollars
and that ends up being about twenty
thousand records a day and you know
that's really not that much but two
thousand of those theaters actually
report to us every hour and that ends up
being about a quarter million records a
day which is still really not that much
but let's just charitably call that a
fair amount of data on the home video
side we get from several thousand video
stores line item of data for every
rental that takes place of those storms
this kind of summer rented this thing
for this many days for this amount of
money and that ends up being about two
million records a day which is you know
that's starting to get interesting and
this is starting to get to the point
where you need to do something unusual
to get interactive response time on big
queries on the on-demand television side
we get about eighty-five percent of the
North American on demand data which and
this is also at the individual water
level this is aaron watch spongebob
squarepants to date 30 last night for 15
minutes and that turns out to be 10
million transactions a day and that's a
lot of data and if you guys weren't
Google you would be I can guarantee you
be really really impressed if I said 10
million transactions a day but your
google song you have to do more to
impress you than that so my current
project that I'm working on is linear
television tracking recording system and
we have performance targets that are
arrived at as follows there are a
hundred million households in the united
states that we want to collect
information from each one of them is
going to change the channel around 200
times a day on average
every time someone changes the channel
this is going to be a line in the data
file the consents to us so by our
calculations that 20 billion
transactions a day over the process holy
cow that is a lot of data so even begin
addressing this we have to start
thinking about what is it that makes
these relational database is slow what
are we going to do to make them faster
so we can handle 20 billion transactions
a day so the first problem we've got is
that records just aren't space efficient
they're just you know it's very flexible
it's very convenient but as far as
performance go it's not really the best
way to store data and one of the big
problems here is first normal form
defined by dr. Edward cod 30 or 40 years
ago and this is the idea that every
record or any record should not be
allowed to have a variable amount of
data so if I'm going to store channel
changes or program views or something
like that I need to sort one record that
says Aaron what's the office at
eight-thirty another one that says Aaron
watched 30 rock at nine o'clock and a
third one this airs Aaron watch scrubs
at nine thirty and you know when you've
only got five records and that's not
really a problem but you could if you
wanted to you could shrink that down to
two records one for me that keeps a
track of everything that I watched and
14 is that keeps track of what he
watched and that that makes our database
a little bit smaller for a couple
reasons one of them is I've got rid of a
little bit of redundant information I'm
not saying erin erin erin anymore I'm
just saying erin and the other one is
that the relational databases are going
to add some fixed amount of overhead for
every record that they added the
database and relational databases can
actually store data like this modern
relational databases allow for complex
complex fields like arrays of structures
things like that but the quarry support
just really isn't there like it is for
records relational databases want to
work with relational data these are
raised or not relational data and so
they're not quite as performant not
quite as flexible we've got this other
problem they talked about before that
the queries are not as efficient as they
could be when I'm working with the
modern relational database every time I
want data I get it through a sequel
query or a quelle query or actually that
wouldn't be a modern database
and this is great for ad-hoc access it
means that my boss Chris everyone give
Chris a hand please so another stroke
Chrissy versus ego Chris could write a
query run it against the database get
some sort of useful results looks like
English you probably understand exactly
what this what this query does but and
it's in the database has a good
optimizer it's going to take this query
and try and figure out some method some
little scheme for how it's going to
actually go about arriving at the
results there's probably going to be
pretty good so he's going to get his
data back eventually in fact those
things are so good that a lot of people
actually reply or rely on them they
actually build large systems with large
amounts of data and think these query
optimizer is all these advancements that
we made over the last 40 years these are
going to save us these are going to give
us a system that can give us lots of
data you know in a very short amount of
time but there's a lot of things that
they're not great at and there's been a
lot of work on these things try on these
databases over the last you know 40
years trying to speed these things up
and in the industry and academia people
come up with time series databases I'm
not even in a bar waiting all these
things let's just say that there's a lot
of things that people have done but it
seems to me that the big problem is that
if you have a room full of smart
developers and they know how to go about
getting the data then the relational
database is just this big beast that's
in the way it's in between you is in
between the data making it smarter is
not necessarily the best way it's never
going to be as smart is the people in
this room have to sit down figure out
what the best way is to get the data so
let me describe a typical typical
reporting server scenario if we were
going to go the total relational
database route and try and build on
demand or linear on top of relational
database probably would have is an
online transaction processing database
we'd process individual rentals or
television views or whatever on this
thing and then every day every week and
we do a bolt copy of everything that
came in New that day or week over to the
over to our data warehouse and we'd
probably hire some sort of consultant to
come in and he tell us that we need to
put the data into star schema for our
best possible performance or we might
get fancy and say you guys should really
use a snowflake schema
but the real problem here with getting
you know great performance and you know
I mean interactive performance where an
end user can run a report and get
something back before they get bored is
the real problem here is that we've got
this big table in the middle they call
it the fact table and every industry the
facts are different in our cases the
facts are the fact is that someone
rented a video or watched a television
program or change the channel and for
something like on-demand this fact table
would have about five billion rows in it
and that's just too much I mean you can
do if you talk to a database vendor
they'll say absolutely we can handle
five billion row fact table in a
snowflake schema and they can they can
get you reports they can do something
for you in 10 seconds on this 5 million
row database they certainly can't do any
sort of customer level analysis about
this guy did this and this and this and
there's a trend among these sorts of
people that these sorts of people do
this kind of thing so you need to shrink
the data and if we were willing to give
up the information that if we were if
we're willing to lose the fact that it
was errand that watch Spongebob
Squarepants if we're okay saying
something like all we need to know is
that in Mountain View yesterday there
were 500 different people that watch
Spongebob Squarepants then we can shrink
the database down and then on the man's
case that shrinks the database from 5
billion rows to 300 million rows once we
rekey on just a program an area but at
that point you can't analyze the viewer
activity and this is you know really
what our what our customers depend on
this for this is why we're doing it for
them and they're not doing themselves so
first thing we need to do take the data
out of first normal form and so that we
don't lose the data once we've taken it
out of personal more form we make the
records big and fat and we store a lot
of information on them we saw enough
information on it that we can get every
data point that anyone's ever interested
in everything we need to know at the
consumer level we once we've done that
we went from five billion relatively
small records to 300 million and a big
fat records which turns out to be a lot
faster in our case then we extend the
database with with our own custom
analytics that know how to work with the
data that we stored in these big fat
records that can take this and say if
this big fat record here stores a list
of everyone that
this program and this one is a list of
everyone to watch this program how many
of those people are in common what's the
overlap or what's the totally unique
subscriber count between the two of
these things update our sequel to use
these database extensions instead of
using the standard Oracle or postgrads
or sybase or whatever and then we've got
a system that can build these reports
relatively quickly and because we're
using our own custom analytics then the
queries are a little bit more difficult
to write but this translation engine
actually comes in really handy again
because we're not actually writing a
sequel we're not actually writing the
code that uses custom analytics versus
standard sequel analytics then we're in
great shape it takes care of it for us
well we have two is explained this is
not how you get this data anymore this
is the new way to get the data 100
reports are instantly instantly updated
and sped up and results are absolutely
fantastic on this what we've seen on
this is an on-demand in particular is
that the average user runs a report when
the average user runs a report they
aggregate the data over 300 million
different views out of our 5 billion
different view database and every single
one of these reports that they run does
some site of some sort of consumer level
analysis even if it's something as
simple as counting the number of
different people that did something they
all have to touch the individual
consumer level data our typical report
build significantly faster than 10
seconds by significantly faster ending
problem with legs seven seconds on
completely reasonable hardware and you
guys are Google you're probably thinking
to yourself well you know we handle more
data than that and we get our data back
in less than 10 seconds and so you're
confused when I say that this is fast so
i think the disconnect here is how we
view reasonable hardware so this is I
think Google's definition of reasonable
hardware
this is a new data center about 80 miles
east of Portland red tracks headquarters
and this is about the same size as a
town that I grew up in and it was a real
town to head a mayor they had taxes we
had police and is that exactly that size
so here's our definition of reasonable
hardware that's Chad over there on the
left and I think maybe he's not
technically hardware and then he's
pointing to undo man reporting server
number one and number two and those two
servers together with there's two of
them mostly for redundancy those two
servers actually handle all the
on-demand reporting traffic for the
system so things are fast things work
great let me show you what the moonship
with the data flow looks like it looks
basically like what you saw before in
the data warehouse the main difference
is actually two differences one of them
is that we've got some great great logic
in house for integrating the data into
our data warehouse every couple hours
instead of just every day or every week
and also when we move the data over to
the data warehouse we summarize it
preparing it for this analytic code and
stick the data in these enormous fat
records which I'm kind of glossing over
and the QA afterwards you guys can try
and pull some information out of me
about these fat records and once there
the data warehouse then that is
available for the end users they come in
the morning look at last night's data
reports come back quickly everyone's
happy on the linear sigh though we've
got these 20 billion transactions a day
and that's you know a lot more data so
the model changes but it actually
doesn't change that much the database
still maintains these big fat records
and the format of these big fat records
the number of them that you need the
size of the fat records is has
absolutely nothing to do with the input
data it has to do is solely with the
type of information people want to get
out of it if no one cares about if no
one cares about customer one level
analysis you can just completely get rid
of this you can say i want to start one
value the total number of people that
watched it and then you can be perfectly
fine with it it turns out on the linear
side people
the metrics that people are interested
in actually aren't as complicated as the
ones that people are interested in on
the on demand side and so we've seen
that the the end result is actually
slightly faster queries and simpler
analytics on the input side though then
we just completely abandoned the
overland transaction processing database
for for input said we'll get files from
cable operators IPTV operators coming in
process them with with some code that
reads them puts them in some compact
very useful format for for doing the
summarization later on we put them in
the data warehouse in the never process
that does the summarization and and
actually this is another case where the
translation engine concept came in
really handy because we have a lot of
different views on the data for the
linear system just like we do on the
on-demand system then there's a lot of
code which is working in terms of the
problem domain needs to access the data
and so this right here is some code that
you probably can't read either that
builds a view on top of our of our flat
files prepares it to live in the
database and organizes the data by
Network by day and calculates all the
standard metrics the total share the
total rating the number of households
the total amount of time that they spent
watching and and you might be concerned
that this level of abstraction actually
slows things down but one of the nice
things about using this whole
translation engine concept is it means
that you get a tune the translation
engine you don't have to tune 100
different reports or 50 different views
on top of the data and so what we've
seen is that on the linear side one of
those machines that Chad was pointing at
to you know to proc machine dual-core so
for course total actually imports
validates processes and prepares for
quick reporting about 3 billion channel
changes a day which means that we could
comfortably handle the North American
television viewing on the bottom half of
that rack so
I mean this is kind of a high-level
overview I'd be glad to go in more
detail on the QA this thing but really I
guess what I'm trying to show you here
is that I think it's possible to build a
system that that's really the best of
both worlds that uses a database gets
you a lot of the advantages you do or
you get with it out of it you get
consistent data great backup support
ad-hoc queries but lets you still work
with a large amount of data and do
complex analysis five billion rows
available for reporting online right now
on our video-on-demand system and a
system which is capable of handling
trillions of rows trillions of channel
changes online ready for reporting so
that's that
Jeff if we have a local Mike does that
mean that I don't need to repeat the
questions true does not need to
that you don't have great fantastic
alright so once you recite the poem you
want your contest what I don't member my
pal tell me what is the cycle
in terms of terabytes we actually have
it's a great question so we have two
parts so we have the online transaction
processing I'm sorry the question was
what is the size of the database so we
have two parts of this we have the
online transaction part where we
actually store one record per on-demand
television view and then we have the
reporting side once we've actually
summarized the data and have it
available for reporting then we can
purge it from the oltp side and so we
keep that constant about it a terabyte
and the actual the online reporting
system is I think about 400 gigabytes
last time I look at it that's the
question is how much history do we keep
we have yet to purge enemy information
from any of our reporting systems so the
home video system goes back for a decade
box office goes back oh I don't know we
actually 86 we actually got historical
data feeds from from some of the studios
and theater chains so that goes back to
86 and on-demand goes back to the
beginning of on-demand which is 2004 the
architecture you describe doesn't seem
to be specifically tied to the business
that you're in there are presumably a
lot of other businesses that have very
similar problems could you just say what
you're thinking is about that and what
those people are doing who are in those
other businesses right now instead of
using what you're using that's that's a
great question so so the reason that so
architecture absolutely is completely
neutral and we Rentrak has a couple a
couple divisions which handle other
sorts of data not you know not
entertainment consumption data and they
actually use these tools to I focus on
this just because renter X business
focus right now is on the entertainment
industry I mean eventually we love to
collect every piece of information
everywhere just like you would and
report on it but right now we're as we
move forward to trying to focus on our
core competency which is just
entertainment industry data
nielsen is a competitor to us on the box
office side they actually had a product
which is of course what am I allowed to
say on this
okay all right I'll give a
straightforward answer this not pull any
punches so when we got started on the
box office project then one of the
Nielsen subsidiaries actually had a
monopoly on that market and they'd had
it for 15 or 20 years within like I said
within about 15 months 16 months of us
starting the project then we had
actually taken taken eight of the nine
major studios away and now we have a
hundred percent market share on that so
you know in one sense their competition
than the other sense they're just no
competition for us so how many fact
tables do you have and how frequent are
the schema changes on them it's a great
question so so once we actually put a
system in production then the then the
schema changes aren't that frequent and
once you get to a really large database
and there's kind of a vested interest in
you know trying to Matt not to make as
many schema changes as as back when it
was you know a cute little 100 gigabyte
database but see so on our system I
guess of will actually have different
views of the data available for
reporting so we'll have one fact table I
mean a partition fact table so maybe you
know 200 partitions one fact table
keeping the detail either you know the
individual views the individual
videotape rentals or DVD rentals sorry
i've been on track for 12 years back
then there were no dvds and so please
forgive me if I said videotape so in
this case will have on the online
transaction site will have just one
table that stores all this information
as we move the data onto the reporting
system then we'll summarize it in
different ways for you know for
different performance like we'll have
one maybe one overall national view that
our translation engine will prefer to
use if it can and will have you know
more detailed views by area by title by
day that'll better we'll use if it you
know can't use the national version just
because of the inducer is not allowed to
see it or because or because they've
selected a list of filters that don't
allow them to see the over at overall
national view and in this case I think
the the linear system probably has about
50 of those and
the on-demand system probably has a
similar amount around 50
so a little bit beyond ones and zeros
can you describe some of the structure
as you use to destroy this think I can
go in a high-level overview this so so
really the structure of the things
depends on what you want to get out of
it you know if all you want to get out
of total number of orders you store
total number of orders and at that point
you need a big fat record in our case
the interesting metrics on
video-on-demand are the individual
people that are doing things and in some
cases more detailed analysis on it so
it's kind of a its proprietary custom
format to Rentrak list of lists of
people that actually watch these things
designed in such a way that it takes not
too much space on disk and that it's
actually CPU efficient to do this
aggregation later on but well in these
and that actually sounds more specific
than it is we actually do this for all
sorts you use the same format for a lot
of different reasons and we actually
have three or four different formats
that we use on on demand and a couple on
linear for doing things like tracking
gosh I'm trying to think of a good
example here tracking a frequent viewers
versus non frequent viewers on on demand
we can tell you how many people that
watched on-demand content last week
watch five different things versus
ordered you know 50 different music
videos or something like that but we'll
use these will use these same values
there you know sort of specific but
they're also general enough that we can
do things like do demographic analysis
or you know interesting things like that
so that was probably pretty Vega answer
to that but I'm not sure how much detail
I can go in on it
thanks your buddy for coming thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>