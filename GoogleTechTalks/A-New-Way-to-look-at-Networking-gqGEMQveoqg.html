<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A New Way to look at Networking | Coder Coacher - Coaching Coders</title><meta content="A New Way to look at Networking - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A New Way to look at Networking</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gqGEMQveoqg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Bob Feldman and I work in the
platform networking group here and I
have the pleasure today to introduce
this morning speaker van Jacobson for a
lot of you I presume band needs no
introduction but I can't resist going
ahead and saying a few things vans been
working for a lot of years on some of
the most interesting and most difficult
problems in networking and really
beginning with his seminal paper which a
lot of you probably read called
congestion avoidance and control which
was a sitcom paper in 88 he's he's
author dozens of papers and RFC's and
been working in this area for a long
time and in addition to sort of having
big ideas and solving a lot of
theoretical problems van is one of those
rare people who can also really get down
into the code and really solve the
problem at the bits and bytes level he's
contributed to a lot of tools that you
probably use every day if you're in the
networking area things like traceroute
path chart TCP dump and many other
things many of you are no doubt familiar
with the VGA header compression which
was originally used on low speed links
but still exists in lots of networking
technology today and back when van was
at LBL Lawrence Berkeley Laboratory he
was one of the leaders in creating what
was called the N bone or the multicast
backbone and developing a lot of tools
things like some of you may remember Vic
and bat and WB they were audio video and
and shared whiteboard over the Internet
long before people believe this kind of
stuff was even possible in the past few
years vans work has been recognized with
things like the 2001 sigcomm award the
2002 I Triple E kobayashi award and in
2004 he was elected to the National
Academy of Engineering and generally
they were citing major contributions to
the understanding of network congestion
the development of congestion control
mechanisms enabling the successful
scaling of the Internet and he's really
really widely been credited with
preventing the congestion collapse of
the Internet in the late 80s and early
90s so I think Google probably knows
Vanna
a vote of thanks because we might not
even exist today if the internet hadn't
been able to survive that scaling and on
a quick personal note I just wanted to
point out that I had the good fortune to
work closely with van over the last four
or so years prior to I joined Google and
frankly I've never seen anyone who could
learn so much about what's going on in a
network topology or between two hosts
simply using a TCP dump file a NOC
script and the simplest little graphing
program I've ever seen and it just
amazed me I used to take our software
team over it was kind of the pilgrimage
over to Vans office to kind of have him
read the tea leaves for us and we could
look at the graph and he would say see
it's obvious this is a Linux box talking
to a Solaris box and it's dropping
packets here and it's doing this and
retransmitting and you know if we could
only figure out how to do that we'd be
great one note this is being recorded
it's will go up on Google Video so if
you have questions at the end if they
are the least bit confidential or are
talking about internal Google kinds of
things let's hold those questions until
the videotaping is done and we can talk
afterwards over lunch and now without
any further ado fan Jacobson
but for someone who's not going to give
an introduction that's that's way too
much introduction thanks for coming it's
this is a a really pompous title for
what's going to be a really pompous talk
is a hard talk to give the the
motivation behind it is it seems to me
for at least the last decade network
research in the US has been really stuck
in a dead-end it should be just a
wonderful time for networking because
we've got everything connected to
everything else we've got ubiquitous
wireless we've got phones that are
starting to show up on the Internet
we've got this huge wealth of
information that was created by the web
we have things like Google that are
indexing to make it easy to find stuff
but kind of everyday everything we're
doing with the network gets harder
Wireless just barely works you know if
you're on a campus you can make it work
if you're wandering around it doesn't
every one of us has spread over multiple
devices you know I've got two laptops
two desktops home computer phone none of
them are ever insane none of them have
the information that I need on them when
I need it and the tools that are coming
out to help solve those problems are
really ad hoc that kind of point tools
and it seems like there's got to be an
architectural problem because the
problem seemed to be so ubiquitous I
think there was a similar time in the
60s and 70s where there was a view of
what networking was that was basically
networking was 20 because communications
in the 60s and 70s for almost all of the
20th century communications was
telephony and people were trying to make
data work over telephony and they just
failed miserably because data is not
telephony and telephony is not the only
way of communications and I think we're
stuck in that in
similar thing it's not that the solution
we've got that the Internet is a bad
solution it's just the problem has
changed and so we're in the world you
know it's kind of like the Copernican
revolution we would put the earth at the
center of the universe and we're trying
to figure out planetary motion and you
can do it but it's just incredibly
complicated because your point of view
is wrong the earth isn't at the center
of the universe and if you make a little
bit of change and put at least the Sun
at the center of the solar system then
suddenly everything gets a lot simpler
both systems explain stuff but one is
just a lot simpler and a lot easier to
work in and think we're kind of on the
fringes of a Copernican revolution and
communication and I'm going to try to
motivate that and I I'm an old fart they
say when you get too old to thank you
reminisce so I'm going to try and
motivate it by reminiscing about some
history about that 60s and 70s change
try and relate it to what I think is
happening now so starting turn of the
century with the phone system I had
spent 20 years talking to telco people
in the process of trying to work out the
Internet Protocol and it's a completely
different mindset and it took a long
time for me to figure out that the phone
system isn't about making calls that's
what we use it for but that has nothing
to do with the system the system is
about wires and if you think of its
context when it was created back in 1870
there were no wires and the goal of the
system what Bell had to do to make a
telephone system was get wires
everywhere and it's enormous ly costly
to do that you need land you need right
away you got to pull the pair and you're
not making any money while you're doing
that is all sunk costs so
what he had to design was a business
model in the system that would get
ubiquitous wires and as soon as he got
that as soon as there were a lot of
wires to connect to phones and it could
start making revenue by people making
calls with that focus the thing that
people are doing with your system and
what the system does inside are
completely different the calls that you
make are 100% side effect and what the
system is doing is trying to build a
path through his wire matrix and this
causes a little bit of cognitive
dissonance when you're you're talking to
telco people at least it did for me
because I think at the endpoints you
know I pick up my phone and I'm calling
my mom in Phoenix and I think of her
phone number as is the name of her phone
and I want to talk to the phone named
Juan Sloane through telco is not that at
all the phone number is a program that
you're sticking into the system to build
a path through the wires that's how they
view it internally see if I make that
more concrete so when I was growing up
this was how phones work you pick up the
line you say I'd like to talk to one two
three four and some operator goes 1 2 3
4 so that's column 12 row 34 they walk
over in their switchboard and they plug
in a jack and they're connecting your
wire to a particular outgoing line so
the phone number is a 2d coordinate on
this switch matrix there were some human
factors work that said one person can
reach a hundred thousand wires that can
conveniently work in a 10 by 100 grit
and so that's why we got four digit
numbers
this is another way of finding a path
through the system this is from the
first telephone switch Stroger switch
MIT used these up until 1990 I think
they were invented in 1890 so you've got
one wire coming in at the bottom you've
got a 10 by 10 switch matrix every time
you dial a pulse the switch clicks over
one so you dial a line it'll click over
one if there's a long pause it locks and
then when it gets the next pulse it'll
drop down one for each so you're picking
a 2d coordinate new to Jesus in some
picking one of 100 output wires and you
cascade two of these you can do the
four-digit 10,000 lines you had an end
office but your number again it's a
program for this switch it has nothing
whatsoever to do with the phone that
you're calling has to do with connecting
wires going through the system so this
works right completely rebel eyes
commune revolutionized communication it
did it as a side effect but it was the
only way it could have been done given
the economic constraints right you had
to get the wires in place you had to
make this base communication system then
you figure it had to figure out how to
efficiently use those wires perfect
it's got some structural problems if you
try to take it out of the context of
people making calls to other people
one issue that sort of Judge Greene saw
for us in the 70s was if your world is
building paths you have to know all of
the resources and you have to have
control of all the resources because
what you're doing to make a call
is finding unused past segments and
chaining them together and if somebody
else owns those past segments and
doesn't let you grab them and chain them
together or if they're assigned into a
different call your host right and
remember that we started this in the
days of relays and people plugging
Jack's in to patch boards right there
wasn't any electronics there weren't any
databases there weren't any computers
you have to have something that starts
working in this very primitive
technology and then you scale it through
about a century and by the time you get
to the 60s
you've got electronics you could start
to decentralize things but you've got
all of this infrastructure and a big
powerful company set up and it really
wants to be a centralized monopoly
because that's it knows how to work
efficiently in that way and that was
true all over the world even phone
companies had started out fairly
decentralized turned into big
centralized monopolies a big issue for
the military in the 60s and 70s was
we're worried about somebody dropping a
bomb in the US is our phone system going
to go down the answer is yeah almost
certainly lots of things will go down
but this system in particular is really
structurally unreliable because you're
Canton concatenating together a whole
bunch of things and they all have to
work all the time in order for the
system to work that means that the
reliability which is the product of the
failure probability of all of those
elements that's going up exponentially
puttin things in series the probability
of failure is P to the end that's a
problem so back when I was doing control
systems in the 70s
Bell Labs was the world's repository of
information on reliability and how to
build reliable systems it couldn't
actually do reliable systems what they
would do is reliable elements their
switches were all gold-plated they would
work forever
they all had modular redundant
components and voters inside them so if
one thing failed the other two would
take over and work they had to do this
because their system was just incredibly
unreliable and the only way that they
could scale it up to a national size was
to make individual elements that were
incredibly reliable it's kind of the
opposite of the internet philosophy or
the raid philosophy the philosophy you
use in your service which take a bunch
of cheap components replicate them allow
some loose agreement between them and if
one fails you don't give it in right
because there's lots of others so you've
got a system that's structurally
structurally reliable you don't worry
much about component reliability so
they're on the other end of the scale
and then a thing that really killed us
when we started to do data which is
phone calls are this two-phase thing
because the system is all about building
paths right so there's the part where
you dial and connect to the farside
that's you're building a path through
the system and while you're building
that path the things that you know kind
of building a bridge across the river
and you're standing on it to build the
next section right everything behind you
is all committed but nobody can use it
because you haven't made the bridge yet
so that's not an issue for phone calls
where you can build the path in about a
second and a call lasts for at least a
minute right so that's a 1% Costabile
the path but if you're sending data
packets a seconds a long time so typical
transcontinental set up times is
best-case about 100 milliseconds so 100
milliseconds at a gigabit per second is
12 megabytes that means that if you used
a phone network
you would have to fetch 12 megabyte web
pages to use the system at 50%
efficiency right no web page is 12
megabytes big right you can't do what we
do with networks when you have these
huge set-up cost to amortize because as
the speed goes up the amount of
bandwidth a potential data that you're
throwing away waiting for your
connection to get set up is enormous
it's hitting on the order of Giga bytes
right and we don't want to talk in units
that are that big typically we want to
get some information that tells us what
to get next so there was a bunch of work
that was happening in the 60s and 70s
saying well here's here's the core
problem right we're we're either going
to get a data network like ISDN you know
64 kilobits is all God ever intended you
to have and if you make it slow enough
then the holding time really doesn't
matter and you know a bits 10 miles long
at 64 kilobits so you it only takes a
few to go across country but if we ever
wanted to make a faster network we had
to figure out how to do faster setups
and so pretty much the entire u.s.
research establishment was aimed at that
problem networking research was how to
amortize the set-up cost and maybe we
should worry about the reliability and
some of the other structural things but
it went as an unspoken unquestionable
assumption that telephony was the right
model for data networking it was
communication nobody did anything else
but there are these two guys paul baran
don davies baronet rent corporation in
LA but he now lives in the Bay Area
Don Davies in the UK they said what if
telephony is not the only way to
communicate I mean there's where other
things or signal flags there were Morse
code there were you know through history
we've seen people doing lots of
different styles of communication
maybe we can adapt something other than
the telephony conversational paradigm
and so they were free to change their
point of view because Bell had put in a
whole bunch of wires and so they didn't
have to worry about the problem of well
how am I going to get a communication
path between two computers just take it
as given there is a communication path
there's lots the phone systems in place
just use it it's there let's split data
up into little pieces because there's
not an infinite number of wires between
any two places not everything is
connected with its own individual pairs
of wires so we're going to have to share
the path the only way that we could
share the path among among many things
is to have the amount of work that we do
per thing be controlled if somebody can
send a gigabyte into the path and I
can't do anything until that gigabyte
has gone away you know it's like mixing
cars and trains on the city rail roadway
system you're going to spend a lot of
time in your car waiting at an
intersection for a train to go by you
don't want to do that you want to help
women had lent things it would be nice
here to not have big trucks it would you
know just little cars so Devi said hey
yeah chop it up into little pieces will
call them packets and then Paul had this
quote idea he was backgrounds was a
mathematician he said let's use
transitivity to get things together so
if you get one of these chunks of data
and it's got in its header where it's
going and if it's not you then figure
out how to send it on and I don't know
how to figure out how to send it on but
I know I could statically configure that
to get us started and then we could
figure out how to do it more dynamically
so this is a wonderful property right
this you know graph theory if you do a
closure on a graph almost any graph is
completely connected as soon as it gets
more than one edge per node on the
average for your brand of graph says hey
you've got the whole thing connected and
so if you've got transitivity based in
built into their system you don't need
to engineer it you just need to hook it
up and it's going to work so they wrote
about this paulin particularly published
this in 64 and I Triple E transactions
on communications utter heresy right
everybody who did communications the
entire staff of Bell Labs came out and
said this can't possibly work he's
ignored everything that makes a phone
system work that makes a scale that
makes it reliable
fortunately there was this guy James
liquid or director of ARPA at the time
who knew nothing about networking but he
kind of liked the paper and so he tasked
BBN to build the network that Paul Baran
had written about and they turned it on
in September 71 and it worked like a
chimp now it was built on top of the
existing phone system it never could
have been built if it didn't have those
wires available was using those wires it
was using them in a different way than
the phone company was using them it
assumed that the phone company is going
to build as little paths that's what it
does really well and we ignore that
wherever the bits happen to get that's
where they get and then the node will
figure out where they need to get to
next and so it was a network that hooked
herself together as a mesh and figured
out he only had to worry about
connecting adjacent hops I'm just going
to figure out the rest so you couldn't
make the network without bells work you
needed to be able to put bits into it
they didn't really care how fast or how
slow you put the bits in nice thing
about the design was it's completely
speed agnostic unlike the phone system
Network so they took whatever crummy
modems they could get at the time which
were 56 kilobits stick couplers it was
passing packets pretty much from day one
the ideas were so simple they couldn't
fail but if you look at it all the telco
people said with very loud voices that's
not a network that's just a crummy way
to use our network you're taking our
wires you're sending on the paths that
we create and you're putting a lot of
extra gunk on it so that you use it
really and efficiently and you know
that's always going to be the Kay when
Copernicus first wrote his paper on
planetary motions the predictions that
he gave were really crummy compared to
the Tomek predictions right the the
earth-centered universe
astronomers had had three centuries to
work out the math of that and they've
gotten really really good at it and this
new system comes along you know it's one
week all of them hey you know it sort of
tells you where stuff is but you know
don't say your compass buy it is it's
pretty crummy stuff and so you've got
this new packet network is a pure
overlay it is really inefficient
compared to the telephony Network the
twenty Network didn't have anything like
addresses in it they were all implicit
and it took a long time it took like ten
years to most people could look at that
network and see the IP network really it
didn't matter whether it was over a
phone network or over an Ethernet or
over a token ring or over a satellite or
over a radio it was really completely
agnostic about the delivery technology
and he really shouldn't care that it was
an overlay one of its features was it
could be overlaid on damn near anything
and if he got hung up in oh but this is
just an overlay and the real network is
underneath it you missed the whole point
the point was they changed the point of
view they were looking at the end points
and not at the pass and when you do that
you can make this agnostic network but
at the time they didn't convince anybody
the entire research community was still
driving the truck down this road of how
can we do fast setup how can we do fast
connections say almost the entire
research community so there were these
two other guys Stanford professors real
straighten you know all the networking
had been done on the East Coast but
these two guys
the ARPANET was this massive success it
just worked everybody wanted one yeah
everybody wanted to communicate with
that kind of technology and so Dufferin
dozen different military organizations a
dozen different academic groups they
were all building their own networks
they all had to you know sort of piss on
it and make it there so all the
protocols are different all the
encapsulations are different all the
addressing structure is different but
the idea is they're all packet switched
networks and Vint looked at it and said
oh we could kick at innate these
networks if you've got a phone network
where you're building paths the
topological details are all that matters
so you have to have detailed knowledge
of the topology and detail knowledge of
how the links are assigned when you're
building a packet network when you apply
Paul's ideas the topology doesn't matter
nothing could be more irrelevant the
topology is something that a packet sees
at the instant that is going through a
network but externally you don't see
anything but addresses you want to name
the destination node and nothing else
since all the topological details were
irrelevant in these packet networks then
says we can leverage that if we have a
common encapsulation format and a common
addressing structure and it only has to
be common we're two networks touch you
know so the time when borders start to
matter in telcos there are no borders
because there's only marbella there's
only the telco but he said everybody can
go and build their own networks and we
can transit them will get the same
transitivity that you get in a single
network so they wrote the specs for that
in 74 and in 77 they demonstrated it by
sending data from a SSRI van over packet
radio over the ARPANET over satellite
back through a whole bunch of gunk but
they were basically sending bits across
the world and back again over a bunch of
different networks using this
concatenated networks idea but VIN could
never sell the
net name just had too many letters in it
the military likes three-letter acronyms
so it became tcp/ip and you know it won
it it worked nicely if you're doing this
style of packet network you nail all of
the phone system problems now because on
a packet by packet basis you can choose
alternate paths on failure rather than
the system getting less reliable
exponentially as it scales up it gets
more reliable exponentially as it scales
up because you make it bigger there are
more different ways to correct for a
failure so you flip the Conda
combinatoric surround you never setup
anything you just launch a packet if the
network can't deliver it it throws away
okay I don't care I'll launch another
one because you're not doing any setup
you're completely bandwidth agnostic
because there's no setup to amortize
right so you just end your packet you
don't have to make this strict
hierarchical routing system that the
telcos had to make whereas high
bandwidth at the top calls could take a
maximum five hops which means three hops
from the leaf to the center of the
network maximum that you would ever take
they did that for stability in
engineering concerns because it was the
only network they were that they could
control and the core issue there is
scheduling problems all scheduling
problems are NP hard and like the rest
of us they couldn't solve an MP problem
if you restrict it to a sub domain
particular sub domain being hierarchical
allocation in a hierarchical bandwidth
tree you can solve the problem because
you can aggregate things as you go up a
tree and make sure you've got enough
bandwidth for it
IP network arbitrary mesh we don't need
a hierarchy we're not scheduling
anything you launch your packets if
planets align it'll go through it almost
always goes through and if it doesn't
then the end nodes will fix it up you
and we got one freebie so those were the
three known problems in the telco system
that Darren was able to convince the
Glitter at ARPA it was really important
to make this reliable scalable system
for military purposes one thing they
found when they deployed it was it got
this adaptive routing that if there's a
failure you route around it but that
works from day one from the time you
plug something in it's now a new element
of the system it can be used by the
system and it can use it so an IP
network hooks itself up a phone network
does not hook itself up right phone
networks are all about armies of people
spending their lives in manholes
punching down copper wires onto 50
blocks and you don't ever do that with a
packet network because the addresses are
explicit in the data do not implicit in
the particular pattern of wires that are
on the switch matrix so I don't know if
your network is open but if it was I
could plug my laptop into the Ethernet
it would be able to route packets out
faxes would be able to route back to it
and I never punched down anything on a
block right I never had to tell the
network about my connectivity I just
need an address and that makes it really
easy for anybody to deploy the network
this is horrible for telco because it's
a democratization it giving moves a lot
of their value out into commodity
marketplace you can just buy these
devices and need them and you don't need
big company technicians to look them up
but it was great for getting ubiquitous
communication infrastructure
okay so um so like I said tcp/ip one it
was a terrific way of doing doing data
networking it is still a terrific way of
doing networking it nailed his problem
so well that is framed our thinking just
like the phone company framed our
thinking we now think of data
communication as being IP data
communication right it works
the problems that we're seeing today the
fact that our seamless connectivity
isn't really seamless connectivity that
we have firewalls and VPNs ad hoc
synchronization protocols they they're
not the failure of TCP they're a success
disaster that it's created this
ubiquitous communication environment
anything can talk to anything pretty
easily can hook itself up dynamically
can unhook itself the network the
infrastructure all fixes itself up
things like the web were put on top of
it which moved all of the world's
information into digital form and made
it accessible and so now we've got this
wealth of information we have the wires
to access that we've got machines to
access it that was not the world that
tcp/ip was connected in yeah it was
created him when it was created machines
were these huge things that lived in
glass walled machine rooms they had a
thousand people would use the same
machine right there was two machines at
MIT and there were two machines at
Berkeley and there's one machine at
Arizona where I went to school those
machines would get connected together
with wires there wasn't a lot of data
that you were putting data in on card
decks and you were getting it out on
printouts you wanted to be able to
remotely use resources and you
occasionally wanted to be able to pull a
card deck that somebody put on the disk
from their machine to your machine but
you had not a lot of information a lot
of users per machine not a lot of
machines now is the model of
connectivity as this networking was
being rolled out it's kind of a bad fit
to today and these are what I think of
the problem the thing that annoys me
most is that connected is this
completely binary attribute the Internet
is a thing one thing and you're hooked
to it or you're not no middle ground and
if you're hooked to it this wealth of
information is available to you and if
you're not up to it you're hosed and so
if my PDA or my phone or my laptop
aren't hooked to it then I'm hosed and I
figure out some protocol to synchronize
myself or download by mail ahead of time
getting connected to this global
Internet because as a single huge
planet-wide artifact it's a pretty
heavy-duty
operation the only way you can do it is
by getting this IP address that's
globally known you can get local NAT
addresses and there's some boxes your
border that turns that into a globally
known address but you know at the bottom
the particle somewhere want a globally
known address and that address has to be
relatively stable it's not an address
that you can use for a millisecond to
throw away it has to be you the world
has to know where you are and you have
to know where the world is which means
that it has to be a topologically stable
address you have to know what the
connectivity is around you and the
timescales as we've scaled the network
out are pretty gross the addresses have
to be stable on the minutes to hours
timescale otherwise data just loops and
black holes so those two things result
in these three that connecting to the
Nets a real heavyweight operation that
means that you want to carefully control
those connections because of the
topological stability requirement the
net hate things that move you want to
put a net on a train well good luck
there's a whole bunch of ad hoc
infrastructure to do that put a net in
your car well if the cell towers
cooperate maybe you can do it but you're
covering over the movement at layer two
and the neatest technology we've got
today is broadcast we've got all of
these radios that are hooking things
together and we really can't use the
suckers from our networking stack
because the net hates thinks that
broadcasts the protocols weren't
designed for it the protocols were
designed for a conversation between two
applications on two machines the
conversations are point-to-point
conversation so we take all this data
that's being broadcast out on our radios
10,000 radios hear it and 9,999 throw it
away and only one will look at the data
because it's got his address now maybe
something like hey there's an accident
two miles down on 101 that all 10,000 of
those radios really cared about but they
have to ask individually um
you know we got in packet design the
company Bob and I we're at we got a
chance to look at the data on the
routers downstream of NBC servers for
the Olympics at one time their main
router got severely congested swim bori
hit the pole on the slalom in that
router there were 6,000 copies of the
same data right everybody was pulling
down the URL poor router can't do
anything about
you can't optimize it because it's
dynamic content it's all going out as
separate conversations all the router
knows is I've got 6,000 separate TCP
conversations it's the same data all
right if you could broadcast it you
could turn that both the router and the
downstream links from the server reduce
their bandwidth by three orders of
magnitude but a protocol architecture
doesn't support that you know it works
at the conversation level okay so I said
this the main issue is the success of
the protocols change the world and when
you get a new world the protocols may
not fit the new situation there doesn't
mean they're wrong it doesn't mean that
you aim the entire you us research
community at oh let's do a replacement
for tcp/ip it's not the solution that's
broken is a problem that that solution
addresses has changed there's a new
problem
that changes this when the protocols
were rolled out they got rid of the
telco path building model but this thing
that the telcos enabled was
conversations when Bell first wrote out
the phone system he didn't know what the
hell people were going to use it for
right he just wanted to get those out
there but there were no phones you
didn't have conversations with people on
the other side of the planet because
there was no technology for it so you
wanted to put these phones out and
figure out how people were going to use
them and what about Worth's phone calls
so in the 60s people look at it and said
oh you know what we want to do is a
different way of doing calls right we
want this side effect in the phone
system to actually be the centerpiece of
our system so when you design the tcp/ip
protocols you make kazakh conversations
be the central architectural element
right as opposed to the phone system
where there what happens by accident
after you build a path so that was great
it got us going but that's not how we
use the network today people don't want
to have conversations what they want is
their webpage right any of the
measurements that I've seen recently
saying that the high 90% level of
traffic is people trying to get some
name chunk of data they hand in a URL
and they want to get something back
that's not a conversation it's not a
conversational model it's sort of the
computer equivalent of does anybody have
the time does anybody know where we are
right that's that kind of interactions
of dissemination
it's a point-to-multipoint or
multi-point to multi-point it's a very
efficient information transfer it's been
used for much longer than conversations
conversations or a special case where
the multi points collapse to one so it's
a pure superset of the conversational
model but it's a different model
when you're doing a dissemination what
you're interested in is the data not who
it gives who gives it to you now you can
do disseminations over conversational
network that's what we're doing it's
what we've been doing for a decade it
doesn't work very well in at least three
important directions the one that's
biting us most today is you can't make a
viable security model out of that
because if your network works in terms
of conversations in terms of virtual bit
pipes what you can secure in the network
is that bit pipe right so we know how to
armor an SSL connection and I can armor
the SSL connection to my mail server but
it doesn't stop the spam right it's oh
good these are these bits came from my
mail server they're garbage but they
came from my mail server I the network
has no knowledge of the content of what
it means right so can help can do a
security model at the network level
because it's blind to the data like that
NB C server you get a tremendously
inefficient Network because the network
can't distinguish the same content bound
for different destinations and so it can
internally arrange to optimize the
delivery of that content doing something
multi point delivery can't leverage
things like broadcast it can't split
data inside the topology do flooding
style protocols because it it doesn't
know about the data it knows about the
conversations you know the tcp/ip
headers that wrap the data and up at the
user level we've got all of these apps
that are dealing in dissemination style
data you know that want to deal in URLs
mail messages sensor readings individual
voice packets
and our network model is okay to write
this app I have to figure out what
server to open a TCP conversation to and
that has nothing whatsoever to do with
the app right it's nothing whatsoever to
do with the problem that you're trying
to solve and so both the app writers and
the users have to do this horrible
plumbing operation in their head and in
the network
if you've ever set up a VPN to get at
the internal websites or get your email
when you're off site right you're doing
this nasty ugly plumbing to solve a
problem that the network should be able
to solve for you but it doesn't know
your intent and it can't phrase what you
want in terms of conversations so if we
wanted to do a more dissemination based
networking the idea is you change your
point of view to focus on the data not
where the data lives because it doesn't
have to live anywhere right data is
named when you want some chunk of data
you present its name to the network and
it doesn't matter what kind of
networking to technology you got you use
them all asking does anybody know where
we are
it passes data out to the network hope
somebody can give you a reply anything
that's got the data you know view it as
broadcasting your past if something's
got the data it can give it to you if
it's some kind like the equivalent of a
router that kindly forwards your packets
instead it could go get the data for it
for you and then give it to you the big
issue in this is we it's totally bogus
but right now today we decide that data
is authentic if it comes from the server
that originated the data so I'm going to
believe my copy of the New York Times
front page if and only if I get it from
the New York Times server now there's no
I mean those bits got hauled halfway
across the planet they went through a
lot of routers they could easily have
been corrupted they cache reconstituted
I could you know I could be getting fox
news for all I know but
our only model is you've got to secure
the pipes I think I would know if it was
Fox News but if you're going to data
dissemination where you get it from
doesn't matter it can't matter you got
to be able to trust the data based on
the data and that means doing some
crypto things to the data so that you've
got some basis for trusting it and that
actually works in general right that
works in the face of proxies staging
data doing lots of things if you get the
data you look at it and say yep that's
good stuff or no that's bogus so at the
architectural level if you're doing
dissemination you've got to have a
security model you've got to be able to
sign things and make some sense out of
them let me see if I can make that more
concrete so the model is I do a request
for today's New York Times and I usually
don't know what today is so I want to
make this generic request give me
today's New York time and I'm just
sending that off into the ether hoping
that something's going to come back so
if I get back a response it wants to
have the name I request it because
that's used to vector the data to me
that data is moving by a diffusion
there's a request that sort of sets up
the channel and then data flows down
that channel so something is remembering
that my port someone at least someone
that asks for New York Times that's by
the generic name the data's actually got
a specific name today's New York Times
has got the date on it and it's a
particular web pages the front page you
know it's the starting point for getting
the rest of the data and then down here
is just the text to the front page and
there are two things that I've got to
secure when I'm getting this data back
one is the binding of my generic name
today's New York Times to the specific
name of
this really this edition of New York
Times the August 30th 2006 edition and
so I need some sort of methods a Senate
authentication check and probably the
Associated certs that ties together
those two names the New York Times com
has signed the fact that 2006 Oh 8:30
index.html is an instance of New York
Times comm today and when I set up my
contract with the New York Times I got
their search so I can look at this
ignorant say yep
this really is from New York Times it's
not from Rupert Murdoch and the other is
the binding of this specific name to the
data so even if those two names are
bound together and I can say oh this
packet came from the New York Times but
I need to know that Rupert didn't rip
off the content replace it with his own
so I need that name binding to the data
the data itself I mean it's real name is
probably like a sha-1 checksum I said
some cryptographic stringer bits that
represent the data so itself
authenticates or self-identifies but i
need to take that identity that sha-1 of
the data and put it together with this
name and bind them together with a
certainty so that's what I mean by
authenticating the data and both the
bindings in the data itself you know
it's all we ever do to get data is ask
Google and then click on links right so
you're you're following this path
through meta information in order to get
to the real information you got to
verify the path if you're going to have
any trust in the information but when
you're doing that you don't have to
change very much so this was the New
York Times web page and this is some
goop that I added on so that I could
authenticate these bindings but this the
page self identifies I can sha-1 anyway
web page this stuff doesn't have to be
packaged with it
I'll stick this somewhere else and
separately get the credentials and get
the data so I can make an appliance that
I sit at the edge of the network that
handle dissemination for me and would at
least give me the benefits of broadcast
and distribution over enterprise campus
size things without changing any
infrastructure right basically is
collecting data collecting credentials
where it can pushing them together and
sending this out on the wire in response
to requests so it's a nice incremental
deployment a nice nice overlay way of
laying out networks because what we're
doing today is dissemination we just
can't trust it so top level design
philosophy is data is got a name not a
location when I say this really hard
talk to give when I've tried to give it
before the first thing I get asked is
but this is just caching and it's not
alright it's it's a different I mean it
the effect it's very much the same but
your point of view is completely
different in caching you say oh the data
lives there but I've got a copy of it
here and my model is the date doesn't
live anywhere wherever it is it's great
all right I wherever I get it I can
authenticate it I know whether or not I
can trust it I can ask the world anybody
got this data and I don't care where it
is and that's not caching because I'm
not moving it from its one true location
to some other location right it doesn't
have one true location you can only do
that if you do the second integrity and
Trust or properties of the data not of
the way that you obtain it and you have
to be able to develop integrity and
Trust from the data itself so this is by
contrast to say the Microsoft authentic
code model where you download this
update from Microsoft and there's a
little window that comes up and says do
you trust Microsoft and the answer is no
you know I kept cooking milling machine
never got updated
it's I don't write but their model is do
you trust their delivery mechanism
they're basically asserting installing
this update won't cause financial harm
to Microsoft great but that's not what I
want you know I want my machine to work
I want to know what you're going to do
to my machine I want to know something
about the data in the operations you're
going to take but they can't tell me
that all right so I want properties of
the data where I don't have to trust
remote agents that the data itself
lets me figure out what it means and who
sends it you know how it's connected to
the world and there's always been this
tension in the cs community between
different ways of moving bits so that
telco people move bits over these
hardwire connections they don't need any
buffers because they have a master clock
in Kansas that schedules everything on
their network right so things never
interfere they all have their little
dedicated time slots and they move
around and so their religion was the
only true bits or bits and a wire and
when these data networking people came
along and they put in buffers because
they didn't want a schedule so data may
land at the same place in the same time
and that's great you you turn some of
those bits and a wire to bits into
memory put the others on the wire and
then you take the ones out of them
everything the phone people looking
buffers that's not a networking you know
networking is cut through networking is
moving the bits of the wire and those
bits that sit in a memory those those
aren't real bits and then the networking
people would say oh you know those
people do databases you know they got
data on disk we do networking we do
distributed computation so there's like
way too much religious fundamentalism
already in this country there is no
wheantr bits or bits they're all good
anything that moves bits I don't care if
it's a disc a wire memory USB stick you
know running around on your bicycle
great yeah if you moved its time or
space you're part of a network you know
you're doing a good thing okay so you
get some wins if you do things this way
one is the communication win and you can
do way more inside the network on behalf
of users because you're communicating to
the network what you want rather some
other than some meaningless abstraction
you're putting I say I want this data
and it can be something in the name that
tells you even more about the data I
want the contents of my mailbox in a way
that's secure implicit in the fact it's
the contents of my mailbox if I'm not
under fired wall network you know make a
VPN to get it for me if I can say the
intent at the top level I can do things
lower down to concept the right
semantics most of the problems that we
have in congestion today we've got this
high bandwidth core because a fiber
holds terabytes right much more data
than we have available to send we've got
these skinny tales that connect us out
to us and all the congestion that you
tend to see when you're networking is
congestion from the core to you along
this tail all right but the end of the
tale where the data lives is in your
service providers world and you're
sitting in your home PC world right and
you can exactly control the data you
send to him so you never have a
congestion issue you can always send
what's most important first but you have
no way to say what he should send to you
right and because of that I wait for
hours why Suns gaming traffic is piled
up on the network headed down our link
right and my email is way more important
than his gaming traffic at least to me
so I would like to be oh and I pay the
service provider so I would like to be
able to say to the service provider send
my stuff first but there's no protocol
for that
and the people that want to do qos are
the service providers they want to make
it and to end because they can extort
more money out of Google if they say oh
no you know pay us for end-to-end
quality of service otherwise hi deep-six
all your bits or half of them at least
they're not interested in making the
tails any better because that's just
really small I don't have much money
Google's got way more all right so go
after the big target so you get an
architectural focus and sort of the
wrong kind of QoS things that will make
things better if you're doing
dissemination you get data in response
to a question you ask so it's really
request response that doesn't mean stop
and wait you can ask a lot of questions
in parallel right but the order of your
questions is the order of importance so
if I can arrange on my local network
that I always ask from my email before
my son's gaming traffic and I run the
Gateway then I will get good service
from the email right and it's entirely
in my control because I can control the
requests what happens when you do
dissemination because there's this
symmetry it's not a pipe from the
service provider to you down which bits
are blasted and you have absolutely no
control over instead you get a fine
grain control at the data item level
because you're asking for things
individually and we don't have the
Olympic server problem the network is
dealing entirely in content is not
having six thousand separate
conversations about bodis poll incident
right that's just one is it one main
piece of content so you can just
distribute it as efficiently as you need
to no matter how dynamic it is
so this I already said you get rid of
lot of religion bits or bits the bits in
a wire and the bits of memory and the
bits on it just as long as you can name
them uniformly you access them uniformly
and so you don't have to do the moral
equivalent of gnome VFS where you put in
this huge application layer to try and
accomplish the same thing you put it in
at the network architecture layer since
you're not talking two nodes anymore the
reason that you're there is not so you
can have a intimate one-on-one with
google.com right instead you're just
presenting this URL and say help
somebody find it there's much less need
for global topologies global naming
particularly when you're doing something
that's really local like a sensor
network if I wanted to know what the
temperature in this room was or if I
wanted to know directions to the nearest
espresso machine the room could be
broadcasting it to me and it wouldn't
need to IP wrap it right it could just
be information that was wrapped in mac
layer encapsulations and i can use
proximity and diffusion and very simple
local protocols in order to move that
information around because you don't
care about the topology just care about
the data and similarly since we don't
care about data living for time in
buffers or on disk we can still accept
access it by name we don't have a
conversation where there's always a
real-time aspect the fact that you've
got intermittent connections doesn't
matter you can still get at all the data
if anybody remembers it they can give it
to you
so security terms since you're making
trust be architectural it's really easy
at the network level to make farming and
fishing and spam and all of these
impostor attacks or unintended sender
attacks they're just impossible because
you can tell from the data that you've
got who sent it what it is you can never
do that in a connection network right by
its design a conversational network
wants to be completely agnostic to its
data
it wants to be transparent it's built
that way from the ground up you want to
be able to ship anything over a TCP
connection that when the protocol was
designed it was important to make it
completely agnostic but if it's
completely agnostic then you can't do a
trust model anything can be in that pipe
and you don't control the other end
you're putting the trust in the data and
user level objects and the stuff that
you ask for so you don't care whether
you got it over SSL that's not going to
help and if some just purely
hypothetical but if some big greedy
phone company wanted to extort money
from you
they have a lot of trouble doing that if
you can communicate over anything that
moves literally anything that moves you
say hey you're not going to move our
bits no problem there's a plane flying
overhead it's got a disk in it my bits
go up it goes the other side they come
down I don't care right anything that
moves bits I'm flooding the data so if
they don't get there on your wire so
I'll get there on somebody else's wires
it's you know like the original IP
network where you could bomb the hell
out of it and kept working on this
network you can QoS the hell out of the
center and it keeps working right
because the network doesn't matter much
anymore the way that you're moving bits
around is this global diffusion process
so there's a little bit of stuff to
figure out in order to do this there's
some stuff that's probably pretty easy
because it's all been done um one is
being able to check the integrity of
data just based on the data well that's
something that we had to figure out for
privacy enhanced mail it's there's
already a worked example probably not
the greatest example is a much more
limited context but at least a problem
of this nature has been solved in a
fairly convincing way and the trust
structure that has been that you need to
embed it in that's also been worked out
so there was this complete disaster
foisted on us called certificate
hierarchies that have never worked and
they have no trust at all associated
with them you know two years ago
Verisign sold a root certificate to
Microsoft to some unknown party still
floating around you basically can't
trust the people in which you input in a
certificate hierarchy the root you've
got to have ultimate trust but yes these
are people that make their money selling
stuff they have a lot of minimum-wage
workers is really hard to put ultimate
trust in that economic structure you can
get by things like Zimmerman's web of
trust get much more distributed
Community Action and so if you use PGP
models you can get some pretty good
trust in things
you don't want the web to be any harder
than it is today
so we've got a model for creating
content that is you stick data in a
directory somewhere and now you create a
Content keep it that model but I want to
be able to trust that data you know I
want to get some of these signatures but
that's okay
make the repository do it right if
you've got the permissions to shove the
data in the repository that should be
enough permissions to attach a search of
the data that says okay I will attest to
the fact that this is the data that was
put into this repository at this time
I'm not going to tell you that it's good
data but I tell you that yes this is the
legitimate coffee copy that was first
created under this URL so there are
models for how to do that
parks working on a system that they call
usable security kind of an umbrella
project we've got a model for how to do
repositories in that automatic way that
they call instant PKI and ut-austin has
been doing some wonderful things in
distributed networking peer-to-peer
networks very distributed services they
have a an instant BitTorrent model that
they call snake bite it's actually being
funded under Google Summer of Code that
has a similar sort of repository but
also registry models that are in it so I
think you could sort of do a high-speed
collision between these two and get
secure absolutely painless entry and
then lastly you got to be able to find
stuff well Google's nailed this problem
right you nobody remembers URLs nobody
keeps bookmarks anymore what you do is
remember some search terms and you take
them type them into Google and then you
search the page to find out what you
want so start with that model right
maybe you've got a URL maybe you search
if you've got some information that you
want to spread around or some queries
you want to spray around we've got a lot
of ways of doing that in this kind of
model epidemic epidemic algorithms which
you we've used for years on Direct
trees directed diffusions where you set
up channels to constrain the amount of
data that leaks outside the path where
it's wanted you can use small world
graphs so that you can do very efficient
data location for subsets of data
anything where you can define a center
so there's a lot of themes that some
smart people might be able to figure out
not me since you're getting everything
by names you want to spend a lot of time
on exactly what is a name how you want
to do
naming biggest thing is you want all
your content to be immutable once it's
created it goes out in the world you've
lost control over it it's now out in the
world that means that if you want to
update something what you do is
supersede it with a newer version and
that newer version should reference back
to the old version say hi I'm a newer
version of the New York Times the way
that you've got is probably obsolete
that means that wherever the data is you
can use it you may have to do some work
to find out that you've got the most
recent copy or the most recent copy
that's accessible to you but you can use
all the copies of her out there so to do
that you want to be able to put sequence
and time information version number
information into names that's easy
but something that you need to do and
tie in to the verification
authentication machinery you don't ever
want to send huge things across the
network you want to cute things and
split them up into small pieces it's
good for the network for the users of
the network because if you've got a lot
of small pieces and you've got a lot of
pads you can send different pieces over
different paths and get much higher
effective bandwidth potentially much
higher reliability it's good for the
user because the user gets way more
control because you're asking for things
a piece at a time you don't have to wait
for the entire download of this week's
GCC to come down before you can get your
next packet in your VoIP call right
everything's little pieces that are
handled more or less independently and
then we've got lots of memory everywhere
so we put the pieces back together into
a big thing at the end
and as long as you can do the
segmentation and we dealt with it in the
naming then the real-time media I mean I
was co-author on the RTP on the void
protocol that was designed as a
dissemination protocol because we were
worried about going through voice
gateways about recording conferences on
disks and replaying them so every little
voice packet every little video packets
got a timestamp on a sequence number on
it so that anybody can reconstruct the
entire conversation and it doesn't
matter where you get it from and it
doesn't matter what the temporal
characteristics of the medium are
because the time information is all
explicit in the data and I'm sure I've
got a slide that says that over again
that's kind of a general statement if
you look at the big advance that
packet-switching made over circuit
switching was there's a lot of
information that's completely implicit
in a circuit system the time slots that
each piece of data goes through they're
never known by the data that the
destination that data is never known by
the data all that you send across the
wires is the actual data itself but no
addressing no meta information if you're
doing that then you can't possibly
repair from failures because the data is
a tight impotant it doesn't tell you
anything about itself it doesn't tell
you where it's going
it doesn't tell you what path is going
to follow so you can't fix the path work
because you don't know the path word the
big advance in packet switching I want
to pause first incise was put addresses
on the package yeah you use a few bits
but you get this massive increase in
reliability because now every packet
tells you where it's going and where it
came from right and so now wherever you
are in the network you know what to do
with the sucker you know if it's for you
if it's not for you if you know where
it's going so you can figure out a way
to get it there so making implicit
information explicit suddenly enables a
whole bunch of new functions do use
cases it's easy to screw this up
so when ATM the
telcos networking they put identifiers
in their little 54 bytes cells but they
used this local identifier it only had
meaning to the two switches on each end
of a wire it was rewritten on every hop
so you got this thing which got rid of
the timeslot assignment on a trunk but
it had no value at all in terms of the
system characteristics didn't make
anything more reliable didn't let the
destination switch fix anything up
because it still had this number which
had no meaning except for the wire the
data came in on had no larger context so
you give a few milliseconds thought to
this problem because you don't want to
screw it up but you do the analogous
thing in dissemination where you take
information that's implicit like time
information or version information you
say oh if I'm connected to
sourceforge.net then the version of
gnome that I'm pulling over is the most
recent known because this is the master
repository instead you put the version
information explicitly you have
something that tells you what the most
recent version is and you request that
version
hello wake up there's some harder stuff
to do like if you want to do a world
scale Network on this you've got to work
out the incentive structure which is the
equivalent or TCPS congestion control
you got to make sure that people get
everything they need but they don't shut
out other people so they get some kind
of fair share since anything can both
source and sync data you want to really
drive the BitTorrent ideas of you should
redistribute content and it's in your
interest to redistribute content because
you'll get it faster if you agree to get
people to other if you if you're going
to use people riding their bicycle
across Africa as a communication
mechanism to network the third-world you
really want a lot of incentive on data
sharing and you can build that into the
flow control whose torrents done a bit
of it in all the peer-to-peer systems
you have a tremendous problem with
miscreants and freeloaders people you're
dumping in bogus content taking but not
giving back you want to architect that
out at the network level I mean you want
to make that as hard as possible exactly
how you do content routing what the
policies are for redistribution you get
some stuff that you really didn't ask
for but you're going to carry it along
in case somebody else wants it I mean
that's fine my phone's got four gig of
which three gig are empty I'm happy to
carry some extra data spread it out in
the world but exactly what do I carry
right what what's likely to be useful
where I'm going or do I just grab
everything that's in my environment and
then there's liability issues what if
one of the things I'm carrying is a
virus and it comes back to me and say oh
you know I'm going to sue your pants off
because you gave me this virus ago it
was encrypted I didn't even know so when
you're architecting you want to
architect around those issues or look at
the legal ramifications and then you've
got this content that either
deliberately or accidentally
is going to get corrupted and you don't
want to be checking every piece of
content everywhere as its flowing
through the world so you would like a
system where you can mostly distribute
content but if somebody says hey that
copy of GCC that you gave me was
completely bogus you want to be able to
go back up the distribution path and
nuke that copy so that you don't hand it
on to poor unsuspecting people in their
gossip and reputation and epidemic
systems that do that kind of lazy
invalidation so I think we can work that
out when we were working out problems
like this on the original Internet we
had sort of I tried to phrase this in a
way that would be acceptable to
Republicans as well but there's no
intelligent design in the network right
it was just sort of hooked up so we
actually had this evolutionary force
which was the evil empire and they were
going to bomb the daylights out of us
and we wanted to make a network that
would survive that and that really
clarifies your thinking it gives you a
way to test all your design decisions
and say okay well if they drop the bomb
is this going to work do we have state
spread out through the universe or is it
concentrated and that is nicely
distributed do we have routing
information you know what's happened say
you look around and say okay if I were
doing a new network today the evil
empire is gone it moved to Washington
but it
wait what do we have there's an
equivalence and I see there's lots of if
we've got an epidemic agreed if you had
the benefit five generations of
peer-to-peer systems they've all gotten
better they've all gone from very
centralized ideas centralized names
centralized content to very distributed
names distributed content it's all been
basically driven by these guys in their
endless lawsuits so I mean it's great
it's there's a lot of drivers that I
think will help us make the right kind
of content networking so I'm finally
done so it's easy PAP uses huge advance
we no longer had to plumb up our
connections but now you know 30 years on
we've evolved to the point where now we
have to do plumbing at the data level in
order to get the data we want in the
places we need it we have to do a bunch
of low level connection plumbing
if you move your view of networking to
the data level change your point of view
a dissemination architecture says no let
the network do the plumbing like it
should this is being done right I'm not
talking about anything revolutionary the
works going on just seems to me to be
sort of fragmented in that hoc but
there's Akamai there's BitTorrent
someone owes to selling music players
that you just sort of set up and this
spread your music around apples got this
nice rendezvous system for getting at
resources and various kinds of
information so I think there's a
demonstration need you know there's lots
of motion and lastly we're killing so
many brains in academia you know it's
like we wasted 20 years of grad students
having them work on circuit switching
and now we've wasted 20 years of grad
students having them working on
replacing tcp/ip don't replace it use it
do something new and so like to kind of
push the world to a different point of
view and I'm done
questions so I'm supposed to repeat the
questions it what do I think of
BitTorrent as a solution and what
problems does a does it have it's sort
of a long discussion but I the biggest
issue is the temporal binding when
you're doing the torrent distribution so
you you talk client talks to a tracker
it's basically gets a list of other
clients that are busy downloading this
content and can talk to those other
clients and get pieces that they've
already got in addition to getting some
pieces from the tracker because there
isn't any fixed distributed memory in
the system the only people that are
eligible to give you the content are
ones that are getting it themselves and
that means it only works for big content
so what you see going over torrent is
movies and the latest KDE distribution
you know big honking things because the
individual downloads have to last long
enough so that you have a big population
of other downloaders on which to draw
because the protocols that you use to
get them are entirely topology unaware
the initial you know your initial seed
might be anywhere
the initial downloaders might be
anywhere you do some statistics as
you're downloading and preferentially
use ones that are close to you
topologically but there's nothing in the
protocol that can set that up it can't
do any learning it can't use any
location information because basically
doesn't have a way of distributing the
trackers and if it did try to
distributing trackers you'd be in the
Nutella world where you never know when
to forget everything everyone who ever
asked for this content is registered as
a possible source of the content and so
you've got ten billion places to get it
from most of which are unreachable and
say you
when you do a content distribution
network you still have to think about
locality maybe eventually you want to be
able to get stuff from anywhere on the
planet but get it close first and that
requires really architecting how the
requests happen but you can't do it with
something like BitTorrent you don't have
control of that level of the
infrastructure
and oh Christ
ah
so I think the question was I talked
about broadcast what I think of IP
multicast I mean I love IP multicast it
made many things easy when we were doing
the voice and video tools and it's it's
sort of an experience that underlays a
lot of the thoughts about dissemination
if you've got multicast universally
available then you don't think about
conversations anymore you think more
about disseminations with conversations
being a limiting you know just a
two-party case but as you say you can't
scale it up you can't make it global my
feeling is you shouldn't have to that if
you're thinking at that level you're
thinking too far down that you want to
think about the data and where you are
right now what's the good way to ask
your questions and get your answers and
so if you've got room level link layer
broadcasts that's great if you've got
enterprise level IP multicast that's
great and you don't need it in the world
because what you're trying to do is to
get some copies of the data that are
close to you or get the data closer to
you right and so it's kind of I think
like a web proxy auto-discovery
right I I can multicast a URL inside of
Google and that's going to hit the local
the closest proxy right which will
either give me the data or it will go
fetch it and if it goes fetches it then
has got it and so if somebody else is
asked they'll get the data from the
proxy and finding that proxy by a
multicast resolving DNS names via
multicast it's very efficient because
you can use a media efficient we the
topological awareness is buried in the
network so you don't get multiple copies
of the data going over one link and
because the behavior is sort of like
cache replacements semantics the only
thing this unicast are the misses all
right so as you're pulling the data down
yeah so one request went unicast but the
next ninety-nine went multicast and and
I think if you architect the system so
it's got that kind of behaviors as data
diffuses from producers to consumers
then you get more of the cache locality
semantics and that means that you only
need local multicast and we should do a
local multi caster let's thank the
speaker and then we'll have some
questions afterwards if you have to shut
down the video conference within the
videotape</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>