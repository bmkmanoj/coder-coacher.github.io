<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoSketch: A Google SketchUp Plugin for Photo-Centric 3D.. | Coder Coacher - Coaching Coders</title><meta content="PhotoSketch: A Google SketchUp Plugin for Photo-Centric 3D.. - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoSketch: A Google SketchUp Plugin for Photo-Centric 3D..</b></h2><h5 class="post__date">2009-07-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/PIdJRS-Hivw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">was very interesting because at the time
it looked like a really logical
follow-on just for some work I had done
long before that it turned out he'd
never seen at the time so that was
really so we have a lot to talk about
and I'm really very interested to see
what what he's done in the past couple
of years thank you very much thank you
very much pleasure to be here my name is
joy Wahlberg as I've said I'm so
computer science at and City College of
war and this is joint work with see
Ibaka joke I at rain strong technology
which is a software house in New York
City not far from over New York that is
devoted to developing community
photography software ok so uh well I
should say a little bit of history about
this work almost two years ago to the
day I gave a google Tech Talk at now you
got two separate controller sketch what
eccentric 3d modeling and at the time it
was early work in that area and it was a
standalone application and last year at
siggraph in august in 2008 we presented
some of those results to the google
folks and they suggested why not make a
drug enforcer okay so since then we have
and i'm here to tell you about the
developments in that area and and i'll
show you exactly how sketchup will help
to make 3d models likely 3d models that
are forward picture very easy way at the
end of the talk I'll also talk to you
about some of the recent developments
and chosen of lightweight new
construction of buildings from dense
range so most of the top will deal with
the photo centric work the last segment
we'll talk about this away from going to
it okay so let's begin um photo sketches
a photo based Ricky Martin plugin and
what that means is that would take
several photographs a scene and these
little graphs are pretty closely spaced
out there overlapping and they will go
into the photo sketch plugging to
develop the
you see at the bottom is protects your
life waiting for new moms and the
example shown here as well but I
lightweight I'm talking about just a few
hundred polygraphs so this is very
appropriate for web-based applications
in contrast to say the results you get
from radiative can have millions of
members of minutes so the many
applications for this work of course the
photo textured models in the uploading
to google earth it has applications for
3d car navigation systems realistic sets
for movies and video games utilisation
architecture of course urban military
planning cetera to the many applications
now photo sketch offers the following
features first of all camera calibration
easy to use automatic chemicals
recovering from photos which means that
we can actually figure out where the
photographer was standing where what's
the position and orientation of the
camera that took those photos with
respect to the model that we will remove
instruct and of course is it ties in
with Sketchup it has instant
communication interface and intuitive
push-pull exclude exclusion operations
and it allows for constraint going onto
trillian ground plane or convenient free
print Henry photography statute we also
offer our intercept a plane operations
that exceed those that that are
available in stage at knowledge we offer
take money to point arbitrary lines and
curves that's very helpful and the main
element is that we're using photographs
to help guide the sketch it'll fill the
models we also offer advanced texture
mapping so that we can automatically
detect concluded areas and paint them
about me so this is the workflow for
flow stitch there's basically two grand
means that you see here drag down the
left in the Frank in the middle of the
slide the left branch is just devoted to
camera calibration it's very important
that we calibrate the camera before we
on this work and the central branch
basically shows it you have to collect
overlapping scene photos and then
recover the cam approach
program and then votes in put photos as
well as the intrinsic count with
parameters that we found from camera
calibration and the extrinsic camera
parameters that we found from chemicals
recovery all get inputted into photo
sketch plugin which then helps create
the lightweight photo realistic model
okay so let's begin by talking about
camera calibration in camera calibration
we recover the intrinsic hammock
grammars that isn't focal length aspect
ratio principal point which denotes the
village center and the gradient lens
distortion okay so in order to introduce
this basically no started off with a
very simplistic idealized hmmm I'll just
enroll kind of model and there you see a
point cap of X which is a 3d point that
gets projected to the image plane as
little X as the rays converge to the
center of projection scene so the
equation at the bottom basically says
relate 3d point in the world to the 2d
point on the image plane through this
matrix cave which is known as the
intrinsic camera parameter reasons now k
in that yields simplified sense only
consists of information about the
one kept in practice it's a little bit
more involved and basically you see the
matrix K here when she takes into
account theta which is the pixel house
which is the askew angle between
accidents the X&amp;amp;Y axis it takes into
account eps which is Nene pixel aspect
ratio and it takes into account c 0
which is CX and CY gotta coordinate to
the principal point which is a center
the idealized world of course s is equal
to one today is equal to 90 degrees and
c 0 is 0 but in practice it's not the
case i have a softer world another
important element is radio lens
distortion so if i bounce back and forth
between to this slaughter and the next
slide you will see the difference
between the distorted image in the
understory okay the greater amount of
the effect occurs at the periphery of
age and you know basically to highlight
this you'll notice if there is a yellow
line go on on the right side the two
endpoints of the line are on the edge of
the building if you look closely on the
left image you see that the corner the
edge of the building does not let me sit
underneath that line because it may need
a lens distortion so our job here is to
correct for that so that we can work the
image into an undistorted state as shown
other buddy so this is very important as
well and every camera even if she's from
the same model will have a different
radio lens Distortion so we need to
solve for it so how do we use it ok so
that's the calibration procedure you
basically want to mount a checkerboard
pattern something test pattern on a flat
surface and here you see three examples
of different places you can now consider
work we do on the wall floor or table
and then from that mountain checkerboard
image you can take about a dozen or so
pictures from different angles and
distances and this figure here shows you
know various different places you might
wanna consider taking pictures of that
checkerboard pattern and see that they
are scattered around different position
of some business ok so there are some
guidelines to follow and wanted to do a
good job in this because we don't do a
good job of this the rest doesn't work
as well want to make sure do not garbage
in garbage out right so it's important
not to change the zoom settings for
camera resolution once you miscounted
calibration confirm the course attempt
television you have to stick at the same
exact between seven system a resolution
and these settings have to be the same
that what we applied during the scene
jack ok so in order to help with this
process itself it's useful to use the
widest angle for zoom settings so that
the community councils because when we
turn off the camera they turn it back on
you know how do you know about to get to
the same suicide and you have before
well you don't really honest you know
that you're using the why you standing
that's that's one poster you can be sure
ok that's assuming of course if you're
doing this with Rosings camera
with the Oracle things focalin so that's
actually helpful for us it's a little
chance of you changing one also disabled
camera flash to prevent clear and use
the highest shutter speed to avoid
motion so here are some examples of
problematic images when try to avoid
this you don't want flash reflection you
don't want partial views of the
checkerboard image don't want to
underexpose or overexposed images or
images that are out of focus because in
all these cases it's going to have some
effect on the accuracy of extracting
features we're going to have to expect
interest from these illusions so some of
all goes well then this is a window that
you typically find that pops up within
the flow sketch plugin it shows you the
images that you selected those are some
basic examples of checkerboard pattern
to leave firing and them then it prints
out some information about in a camera
its focal and principe point and
regulatory networks also okay so at that
point we've recovered the intrinsic
camera parameters and calibration and we
can proceed with the main drainage so
the first thing we have to do here is to
collect overlapping photos of the suit
again there's some guidelines to follow
there as well so as we said before and
this is critically important you do not
want to change the zoom settings or
panel resolution these are things you
don't want to do want to walk around
with a camera in an arc around the
subject while pointing the camera cat
the same part of the scene so that's an
exam exam you want to do you also want
to have a distance and angle between
consecutive images being small so that
we can have a lot of overlap in Canadian
side images it is ultimately under the
hood what we're gonna do is extract
features from using it is in tractor
anchor drink well you want to avoid
standing in one location and panning the
camera because that limits parallax and
offers little feed me information so by
panning the camera imagine that the
cameras tripod and what you did was you
rotated the camera
pivot around the vertical axis we want
to avoid that and you mentioned how
laksa went to power through referring to
parallax or is it manifests itself as
the apparent displacement of objects
from you along different lines of sight
so here you have an example on the
google image 1 image 2 which are two
images that were taking body is a
snapshot walking into another snapshot
if you simply overlay them you will
notice that the objects in the distance
pretty much heart in one place but the
objects nearby have in a displacement of
a field so mobis at nearby architect
larger parallax emeritus conservatives
and food is between for death by
measuring the image displacements
between the corresponding points and
individuals in general the greater the
displacement too close to the object
however this effect is not applaud when
the images have taken from a single to
center of projection so that's why we
want to avoid this kids and we'll just
storm here again don't want to stand one
place from civil center projection and
the swivel camera around ok that
probably that violates the ability to
get parallel to community parallax is
over ok we also do not want to walk in
straight line towards the sea because
that creates ambiguity between the zoom
in translation so you know the more
chaotic you are in terms of taking
pictures of better you just want to make
sure that Amy don't take them too far
apart so that you can you match features
of one features of yogic if you have
fifty percent overlapping these photos
that is just great here's another
example of guidelines to follow the
reason I'm spending so much time on the
session guidelines is because it's
critically important you don't do this
right you will have problems loading you
not acquire images / concave pads
because they may share single-center
projection images only marginal power
lines so the example on the left is an
example of something bad is it basically
mimics you know I'm standing one place
in this you know sorta long the camera
around that will paralyze what's better
is what's shown on the way move around
the room and collect the photos and
finally you want to avoid motion blur to
improve future natural accuracy and you
want to avoid glass coating is it
possible to eliminate
that's my FATCA now i should say okay
that sounds you know that sounds harsh
but if you look at the image on the
right you will see that there's a
reflection of the building with a lower
right side and the problem is that as
you walk around that building tends to
move greatly into a flexion so you might
be extracting features of something
that's not truly positive but it's not
as bad as it sounds because as long as
there are enough other features on the
building that that's a rigid to that
building then you're okay so you know
I'm just pointing out some of the things
that can come up with yet which one's
your presentation of scanning device
where they described going to an Air
Force Base to scan helicopter when they
got back and look at the data they
realize they couldn't see the airplane
discussed they got the ball from the Air
Force connects day asking them to
destroy the day is still thing materials
a little okay so once we acquire the
overlap in 10 points you have to go
ahead and form camera project over so in
this case that means that we're
recovering the extremes of the camera
parameters particular camera position
and orientation that's associated with
each image simultaneously side of the
same point you also recover a sparse
point cloud in other words we said
before that we extract features from the
images now these features can be
triangulated into three big ones so will
every feature that we find in an image
has a corresponds to 3d point out / in
states and so we are finding both things
it's meaningless to talk about
recovering a camera pose unless its
relative to what development to the
model dodge of your recovered as well so
so we're doing both from getting a count
of position and orientation for each
image and we're getting their 3d
positions of extracted image means and a
purpose of this step is to bring all the
photos in to see the road there is a
feature in the Sketchup called match
photo or match the current one is
mentally and you know there you have to
basically do this by hand every image at
me bring in magnitude specified
vanishing lines in the bank and so
that's about how one
Gordon system relates to the other which
doesn't really scale well when you have
a lot of images this doesn't work that
so it's important virtue of clothes
here's an example where we have five
input interviews and what you see at the
bottom are five recovered chemicals and
skills are denoted by the frustum this
at the bottom left and simultaneously
you see the sports car club on the
bottom line so you get those two things
like I consider them to flip side of the
same point yet hit them as us together
and no that's basically what we
definitely slightly photos now as I said
the purpose of this is so that you can
draw on one image and then properly see
that drawing in the other venues the
residuals that's only possible once you
can upload link up so as you see on the
upper left let's see if you are
sketching out the rectangles which
denotes the rooftop of one building and
then we jump to a different first one
that corresponds to the camera that took
another photo and you'll see that that
same rectangle on top of one photo is
properly displayed on the other focus as
well so we need multiple images to
facilitate total coverage of the scene
in the presence of occlusions general
you can't even hold building one ye you
want to walk around it and furthermore
now you get more coverage that you
basically deal with a clueless so things
that are included in one image are
generally visible in another and we can
use that and we refer to that as the
mixed reality then we'll talk about that
later so we use multi view geometry to
help compute this counter-potion cover
as you can see here they're different
elements to the work that I'll talk
about ultimately geometry is the first
element you also have floral lining in
for me to do than mostly sketch it
footprints push-pull interface and
advanced extra members will begin out
with the building of genomic so in this
case multiple geometry what we do is we
automatically abstract saving features
image and then when we perform preacher
correspondence among image pics and we
will then initialize a camera pros a
structure from the first two new
trainers and we will recover the
chemicals is of all successive so the
first to help us bootstrapping process
and then move on from there we can come
bundled Dustin to distribute to go
better so if you talk about the
extrinsic parameter is no reckoning look
at this slide better understand that on
the right side you see a 3d point
capital then and it's represented in
rural communities but it has another
representation and its attempt the
cameras coordinates so the second
equation we see they are M sub handy
bolts that basically reflects how we get
from 32 point n world coordinate
representation to the counter Portland
system now the next line little m which
is a tuning point to the position for
that cleaning point and you represent it
as K times n sub 10 remember k we saw
that before from the intrinsic camera
intrinsic the camera grounded maybe so k
gives us the intrinsic parameters but
how do you get em sock em while we see
from above that there is a relationship
between a mess of campaign and that's
given in terms of this matrix form you
see here welcome to a good laser pointer
so so we end up with this situation you
see there k times this matrix which is
in terms of rotation and translation
time span which is a community pool so
you can see how we really need all this
information you need the 3d point we can
extend it the extrinsic information the
intrinsic information so that we can
specify how this offending point can
come that turning point on we can
integrate the property we solve for the
intrinsic camera parameters through the
camera calibration on that part is known
what it's unknown is there are in the
teeth notation in the translation so
that's what we have to solve for so we
have to solve for that unknown rnt and
to do so we compute the motion of pixels
across all of the two of you and put
interviews
and bixel motion is computed only on the
salient points so future transform is
used to compute the future descriptors
for the salient points popular future
transform is sift scale and family
future transform and the many variants
on that and our other scale space
variation for show so these descriptors
will be nice to match points from one
image to another so you see how this
computer vision work involved here to
basically refer the only camera
positions about extracting features
here's an example on the left are some
features that we extract on one of those
images it's all the hard to see that
they are no they're a bunch of dots
sprinkled on that photograph from dodge
are the extractor features then in the
next image we track those future
positions and they are connected by
those two yellow one so we see how we
are able to track features from one
image to the next now what you see here
is basically the kind of bird's eye view
of this whole process in frame 0 and
frame 1 we've done the tracking of those
features you can see the three points
shown in frame 0 hunted here ok so the
three-point tion frame 0 and they are
matched into same one that's what those
arcs represent that with that can match
them and we initialize the extrinsic
information is insane 0 with an identity
matrix for the rotation and a 0 vector
for the translation it so that much we
have to assume that's a template or an
abstract process goes through there and
we will then solve for our one key ok
now we can do so by applying a QR
decomposition of this ascension matrix
to recover them that aren t care so what
that doesn't in the end this basically
allows us to have you know these two
chemicals is known and we can take those
2d positions and their camel causes that
are known for those two frames and then
triangulate then back to 3 equals ok so
if the moment counter poses for the
first time it is a no needs to be
position we can triangulate to get the
resulting 3d version of those two eating
implements so that forms of these points
in 15 then we go further and we say okay
for those features that we have already
found can track them again in the next
room ok so we track them again in the
next ring and we use a six-point ransack
algun to build up these unknown location
and translations for all between
successive cameras so Lance a Galvan is
fairly well understood and and so that
helps us get the rotation and
translation for all those successive
report furthermore of course there will
be some teachers at fault away that
can't be tracked anymore so they'll
always be new features that come in that
then attract from that point on so
that's what we call sequential updating
you know build up adventure and denser
warm up as we consider our normal these
images ok so no bundle adjustment is
used to help distribute the error of the
candle hosen structure and in general
what we're doing here is who are
minimizing new projection error so this
equation you see that we have capital m
which is a 3d point in capital community
steep objection matrix which is built up
over that K matrix and the Underland
argentine and we have to find out the
unknown armed team-best minimize this
expression now understand that this
cleaning point and this projection
matrix should fall on the 2d image
feature in from the family using sipping
some variation so we want to pick P and
M to minimize that ok and that's a lot
one is refined and here are some
examples of that imagine we took about
two dozen images around building on the
campus of the city college of new york
so it's a large gothic structure we walk
around it taking multiple pictures and
here are some of the pictures you take
and you can see that those pictures have
been taken we have found for those two
dozen or so images those white guys and
those wipeouts represent the position of
the camera now associate was which I
thought you see red green and blue
accents that
orientation without GPS or any other
assistance like that we are
automatically able to figure out the
position and orientation of the camera
that was held by the photographer as
tables fingers now what you also see is
this sparse 3d point cloud as well so
now they live there the flip side in the
same point newsone to help find the
other and that's this little camera pro
to interpret easy though now the next
step is for one where we have to recover
the ground blind necessary for sketching
the footprint it's important to point
out that because once when we started
this process of cowboys recovery you can
assume one thing assume that we have
going into the identity matrix for the
location and we do i prefer translation
that essentially means that we have a
camera that's that's coincident with the
origin of the world coordinate system
and it's looking straight out at the
horizon but you know that's not
necessarily the kids but for the purpose
of getting this process started me
assuming that as a result we have what's
shown on the left and on the initial
camera is looking straight out the
horizon and everything when we
constructed you know might be rotated
nite nite local off what we know in fact
is that building is jello mats in what
is it generally sitting so that the
plane is no four of their ponies is
unwrapped them and it's the cameras that
can be suitable so we need to be able to
align us and that whole floor room so we
do so by selecting at least three
corresponding points manual including
supply in front and since pose is known
for each of the cameras we can
triangulate these 2d forms to get 3d
position the plane is then fitted to
these 3d models and a normal of that
point can be rotated to the line itself
with the normal before that the normal
that plane can be made to come straight
up okay and the user can select points
that lie on any plane that is parallel
even though all this off on foot prints
and you think that this has committed
for you know eyes that oftentimes we
don't actually see the floor because I
might be occluded by cars or people or
whatever so we can use any other come on
that on that we've instructed what we
have on photograph they have
so a little video said move audio it
must establish three correspondence
points among the two displayed images
these points must lie in a plane that is
parallel to the ground a magnified view
of the pixel neighborhood is shown about
the selected point to help you
accurately pinpoint the corners the
established correspondence among a pair
of points is depicted by yellow line
this process is repeated two more times
when you are finished click the right
mouse button and select done this places
you in the position of the photographer
that took the first picture notice that
the aligned floor plane is now visible
by zooming out using the standard
Sketchup navigation tools we can examine
the results of camera pose recovery
notice the positions and orientations of
the set of cameras that were used to
acquire the input photos each camera is
depicted using a frustum camera pose
recovery occurs simultaneously with the
recovery of a sparse 3d point cloud as
shown here each 3d point in a cloud
corresponds to an extracted image
feature that was matched among multiple
views so once song the floor is alone
then we can proceed with sketching the
footprints and a photographs will serve
here as drawing stencils and the sparse
point cloud that we found earlier and
hemorrhoids recovery will help validate
the footprint place we will be able to
snap those click on the snap to 3d point
icon and select a 3d point to snap the
floor plane to that position then select
the 2d rectangle icon to begin sketching
a rectangular footprint by clicking
three points on the corners of the
building facade notice that this drawing
mode is constrained to the snapped floor
plane a magnified view of the pixel
neighborhood is shown about the selected
point to help you accurately pinpoint
the corners rectangular footprints are
common for many buildings which is why
we use them here
process of snapping the floor plane and
drawing the rectangles is repeated for
all of the footprints that are visible
in this photo you see that we always nap
to the relevant preview point that's
associated in time when you draw the
first edge of the rectangle try to draw
the edge along the vanishing lines in
this example they tend to lie along the
edges of the windows our system allows
you to switch from one viewpoint to
another due to camera pose recovery a
footprint drawn in one viewpoint will
appear registered in the other
viewpoints and will be aligned with the
sparse 3d point cloud notice that the
footprints we each snap to the heights
of the selected 3d points so there you
get a feeling for how sparse about fun
photos but nevertheless it is helpful
for snapping points there that that
allow us to restrain the drawing me to
go home on on the rooftops so the next
step is push-pull interface there's a
familiar with all of Sketchup users with
an exclusion operations click the floor
layer icon to hide the floor click the
point cloud layer icon to hide the 3d
point cloud now click the push-pull icon
and select the face to extrude this
extrusion operation needs to be repeated
to all footprints to create 3d volumes
that are embedded in the scene
you may quickly switch from one
viewpoint to another by clicking on the
next camera and previous camera icons
it's important to note that if you dive
into the scene beyond the texture
frustum you can see the untextured 3d
models in this example now we'll talk
about one old rock where we see the
model from the same vantage point as the
photograph notice that the alignment is
perfect now we jump to a new vantage
point to see the roof we use the taper
to line tool to insert a line so that we
can lift it to taper the roof as shown
here we now repeat the process from a
different vantage point to taper the
tower to a point to verify that the
geometric refinement correctly matches
with the photos we view the refined
models from the different trust ins so
one thing I want to point out there is
that we introduced to taper to line tool
and take it to point tool and in the
paper to lie until you'll notice that
you can orbit anywhere you want to see
the proper view the most convenient view
of the top well online and as soon as
you start pulling it up the table to
that line you get pulled back into one
of these views from the frustum so that
you can use the photograph to help guide
you how far you pull that one and the
same thing applies to the taper to point
to put a point down as soon as you start
pulling up on that point you hop right
back to this view automatically so that
you know let's in this example you know
how far you should pull that apex up and
then we have advanced texture mapping
which allows us to project photographs
onto the facade versus the editing
process we exploit GPU processing for
occlusion detection and there in fusion
so on the left when you see here is you
know something that you would typically
get in
SketchUp clarify you project an image
onto the building the problem is that
the image was taken the camera and the
ground level looking up so the image not
only paints the front facade of the
building but it actually extends beyond
that and intersects with the sidewall of
the next building okay and you know
there's no way of avoiding that unless
of course you detect this occlusion and
we use keeping processing to do that in
real time that's shown on the right so
those areas that were protected to have
been uploaded can be painted with a
neutral gray color instead of having to
deal with this undesirable lung the
texture on the left so now that we have
that we can upload the miles to the way
one main application of our plug-in is
for users who want to add their content
to online mapping systems the user can
geo-referenced their model by aligning
the model footprint with georeference
satellite imagery and then uploading it
on to google earth notice that
translation rotation and scaling is
necessary for proper alignment use the
native Sketchup tools to perform this
alignment
want to make sure that the building is
not underground allowed to live it up a
little bit warm we use GPS information
is stored in the here we see the result
of an uploaded model on Google Earth
this model was completed in
approximately 24 minutes three minutes
for automatic camera calibration one
minute or floral onions and 20 minutes
for modeling on average our models
typically have between 50 to 100
polygons such lightweight 3d models are
highly suitable for transmission and
rendering on web based applications such
as the Google Earth application shown
here there's an whoa examples
you obviously have what you get as much
each other
yes you had a question in the bank yeah
I was wondering if you could position
the models at least roughly on the math
using GPS data from the exit headers
without the original photos well yes you
questions came in position these models
automatically uses GPS information
that's tagged with photo yes there's no
reason I can you do that right now in
this photo sketch plugin no but but
there's oh nothing to stop us from this
most of the consumer cameras they don't
have right now it means the tag is time
informations right yes because that's
correct most of the candidates don't
have that information on consumer grade
level and really are targeting that
market right now because we started out
this whole work focusing on laser
scanners but unfortunately most people
don't have access on the thousand
dollars pieces of equipment like that so
he said no what do what does that mean
well maybe happens consumer grade
cameras and we're trying to work with
that but nothing stops from using GPS
information we've actually cell phone
cameras more and more than 200 that's
what you're good at here are some
examples that show the lightweight
feature you know we have from these
models and probably a polygon to me have
these these renderings a small as
appeared in two polygons for that Golan
is other building added over 500 polygon
tool because in want attack some more
details earn one thing you'll notice on
this example is that you do see you
can't medium the frustum switch to note
the cam position orientation so that's
something else would you lay down on the
lower but not only the models that we're
building but also positions of the
cameras that we need to take the photos
of the Mars that's the thing we don't
see you know use it all now but it's
something that
can be made available it's in a sense of
former club geotag not only can you say
we're going to market like you can see
what what angle you even holding a
camera took those now for those of you
who use Sketchup a lot of course you
know about match photo and in that case
the user must align guidelines to damage
your minds at the scene and so this is a
you know a useful tool force left you
see the initial configuration of a match
photo oh this was called photo matching
sketch of six and its natural sketch of
70 okay so we have the initial
configuration shown on the left you see
is someone confusing array of red lines
there but the user is expected to move
you know the grid lines to coincide with
advanced one so let's see the red ones
they're not a move to coincide with one
potential line the other hair removal
inside is another Spanish line so it
takes a bit of work of manual effort to
set this up on a part of the user now
the disadvantages are welcome to
sentence bangle specification of
vanishing lines I'm asking repeated for
every photo read some photos which we
don't ask the user to set any of that up
its double inadequate because we have to
extract me features from the image and
do some math and figure out the camera
pose ourselves so one asks user to
intervene if this also requires at least
two vanishing points to be visible the
sea and that limits the set of photos
that continues I'll show an example in a
moment where that without applying you
couldn't use that boat
also requires a coordinate axis of each
new photo to manually align with the
existing ass so that means every time
you want to consider another image you
have to go ahead and line corded escrow
it to the old one and so this doesn't
scale efficiently when you have large
number of photos here's an example where
naturally what did it not work the art
song seems like this which is a image of
the new behind use you on any ancient
history here in New York City and you
know they're known like fashion points
and not even planar surfaces here so
there's no 30 agency of you touch it 1 2
3 we will standing across the screen to
guggenheim on the edge of Central Park
into disorder notice also that that the
color on them on the building of the
texture itself is very dull this very
few features you can track yet would get
this no fine because my life like first
of all the output of this I thought this
is a sketch of rendering of a building
then I smell the colors are exactly the
same thing this and give photo texture
with the colored it just happens look
like its a limos stairwell renderings
anyway so you see there you know you can
recover the the model on the bottom you
also see that thrust jumps for the three
images that we took so what stands
between that model in those mushrooms is
50 now when we put that back on Google
Earth unfortunately though there's so
many trees from Central Park that block
you know you won't see normally those
frustum says they are overlaid on the
dark areas a google work so i didn't
bother showing yet but you could in
general take any of these models and
forums and put them back on the work and
see not only the building there but also
aware of these pictures your table
morning patient the camera so these are
additional photos get features that we
have mentioned dynamic texture mapping
with a pollution detection i showed me
an example that earlier we also have the
minish reality
Russian also she learns about the hell
Chris will start out with dynamic text
remember I want you to concentrate on
the top half of the screen the idea here
is that from any baggage review so you
can know orbit to any convenient
position and you start pulling down on
these some on the slip prints that we
made and immediately get the images
properly texture map onto that building
is that that's what we call dynamic
customer an arbitrary for you to
immediately press remember as opposed to
what you see on the bottom which is says
that you're limited to only the frustum
use in order to see the photograph
alongside the model roaring screaming
and in that case it's not really even
texture mapping it's just it's just
overlay because / lenient with with the
model that you're building the bottom
because of dynamic IP mapping you can
orbit to any arbitrary and to this
operation so that's very convenient and
at the same time we have automatic
inclusion detection so as you see there
are clearly marked gray areas which are
on which to know that you do not have
proper texture napkin now you do see
something for some of that texture
underneath of the building laptop that's
only because we didn't finish the model
and we'll just mapping the image onto
local news and in fact there are other
elements on that roof rejected in
modeling me from another important
element of work is diminished reality we
might have heard the term augmented
reality where I can add things out of
the photos here we're trying to take
away so we both the ministry the idea is
that you have 3d objects standing seen
that are appealing rebuilding okay and
we wanted rid of those honors one minute
is honest and how do we then give that
hot welcome those pixels that why I'm
not 30 cougar can be thrown away and
replaced with pixels as seen from a
different angle so here's a good example
then here you have two images slightly
different
I will left you see right on the way you
see the street lights right pretty close
to the center of that image so we don't
want to use those pixels on that screen
violence so you use to paint the
building but of course you know from a
different view you won't have that
problem so you know we can always throw
out the pixels we don't want including
the business we want and we can do so
shown in this case by let's say thinking
out that person in the lower left corner
of the image bike I've known selecting a
boundary around that person and then
replacing the texture from another
community so right now you know it's
kind of an angle to select what part you
don't one and replace it with another
team or just a little smarter about it
for a short in the dunes you know like
some sort of medium filter since you
have corners that like pressing one
image but not present in several others
and those other several images would
inform us that you should be using those
including on right now it's manual
exercise here's another example of the
street lights outline it and replace it
with it was taken from the other new now
in painting is kind of related to the
ministry already in the sense that yes
we are taking out elements of you or
want there are no interviews that one
with the tell us what values they should
be because these elements are so close
to that building I have a gutter right
on that go today your honor all the
possible use it for this told us was
behind that if we wanted to take it out
so you know so this is kind of like some
signals
his career where you can suggest
tomorrow the pixels from neighboring
areas around that I Blanco containment
now I say all this because these
features these four features are listed
here can only be implemented sub-optimal
using heavy file IO to taxpayers or
materials back and forth between
Sketchup in the GPU and now it would be
nice if we had access or important
modifications made to be Sketchup API
that would allow us to do things more
efficiently so this next lot is kind of
Appeal the stretch of development team
and wish list you have for modifications
from sketch of a lie and these are some
of the suggestions you know give us a
way to have a get method rewrite our QB
a material values we want to be able to
write those pics with others directly
without having to know that save it away
as a fire do you know processing outside
them bring it back in that's really
inefficient so we want to avoid that
I'll intensive manipulation of pictures
by external program you also want the
ability to map the texture materials to
the GPU memory to support dynamic
texture map and wall and modifying
geometry and we want to have a method to
set the sketch of that since in roomie
because lily is a programming language
we use all this and won't be able to do
this so that we can improve our
interfaces directly set the axes 110
have enough information in 241 so if
these methods were available with api
that would make it our job a lot easier
and it would allow us to take what is
currently real time operations in our
standalone program and make them real
time operations in our lovely decision
right now they're not real time because
these robeks so something i would take
130 to the second to promote meant to
surrender now might take you know this
is all gonna all these heavy big files
that have to be pushed back and forth
and it's just there's another very
interesting application for dynamic
action
which we call it death from focus at the
bottom of the body on the screen you see
that we are pushing the wall of the
building that can flow and on the top
part of the screen here we see what
happens when we project all the images
onto that wall notice that you go out of
the ninky focus the focus is only good
and sharp once the wall is at a profit
there when you start moving a wall back
and forth you're not quite sure way to
put it then all those images have
informed the right the wrong gap because
it won't learn so you know this is what
the standard in your vision work no that
focus well we can use that to help
inform us what is the right place put
that wall so there you see the game so
right now that's something that can be
done dynamic texture mapping although
it's done manually no the user has to
kind of play with that you can imagine a
case where this could be done
automatically let the program walk
through this and in telephoto focus on
can just take the right depth so that
you have a sharp is so again don't think
would be possible so here we have it so
the disadvantages of existing
programming your solutions with Walter
manual camera calibration where the user
has the magic on to maximum all photos
of fine vanishing points that's
typically the set if you have in
Nashville somewhat difficult to scale up
for large number of intra photos and
modeling was done by inserting a new
primitive objects in space which
sometimes is a little bit awkward and
clumsy and the photo sketch we have
fully automatic camera calibration you
have full automatic chemicals recovery
and automatic texture collision
detection it's a great tool so we
encourage you to apply it I'll give you
more information I dealer to talk to
know where you're going and you can get
copies they have a few minutes
um yeah the questions or yeah I guess
well because some questions now and yeah
it is okay um well thank you very much
those if we have questions your local
they are remotely you mentioned the
regular stores different every camera
even have the same model it is it is
there any chance that for a whole class
and models your iPhone camera that
they're close enough so people would
have to do the manual calibration plus a
little more posts carrion to that
depression was looking to safer classic
cameras nikon cameras you know can we
approximate that with one set of
parameters for a good lens distortion
it's something that too much more closer
every gun we found that just go ahead
and play it safe or ones do a good job
and we act now and take our time to do
camera calibration now me occlusion a
quoting object removal may be missing
some good it seems like you could say
well here's the bounding box of the
object of sketching throw out everything
outside that bounding box it was street
light with vendors it says it's a little
bit we're pretty questionable ok so the
video when you're doing to the ministry
ality here getting rid of the objects
through cluding your pure see so you had
that extreme that that I'm hopes
standing there but you you actually had
an outline and remove it seems like if
you were to say well here's the bounding
box of the building i'm sketching throw
out all the points outside of that take
them out of the scene so the pleasure is
your user interface so the questions to
leave some fun user interface with an
industry-led bug specifying down the box
of the office that we are you care about
in cary go that would seem to be a
reasonable thing to do this with the
leaves and if you are more accurate
about abandoning you use more pics up
from
you miss about losing from the abuse if
you drive bonding works then you're
losing stuff from that varies as much as
possible you missed it from the right
view so no comment was made that you
know basically one of oil using the
consistently as much as possible except
for tend to vary in a low blow and so
one Muslims live yeah it's more head-on
which means still determine that right
still find it and find the camera better
that's at first you know my dreams
tonight if you're trying to restore
detail on choose the best texture
killing but you can choose the best
extra-base information for the model
remote with chemicals in reverse
direction Jenny used in practice some of
the movie something like feathering
where where it's a blended average so I
put up some information on the slide so
no more sketch is the belt that range
from technology you know it's here the
URL the plugin workman launched by the
end of june 2009 I took is enough we may
all beings from Palestine questions and
encourage you to try it out and ignore
the informations bottom if you ask if
you have additional questions you want
to ask to meet with its enemy
incorrectly Warburg xes that cesium like
that you know that you to be happy there
answer more questions about that's best
way to get into it all right now the
question
movie Boulder oh when you looked at all
um actually I really like the idea that
this is lightweight that you have a few
polygons but have you looked at if
someone wanted to make a very high-end
rendering like recovering depth map
Seattle for me building facades for
example using programs yeah nor because
i had another segment of the talking at
a time luckily there we reduce the
visible range data and we were able to
get some flight deck results need some
examples of it so we had dense moist and
we created these lightweight models on
the left you see here or here dissidents
I've some theater no it wins the points
from lasers and on the left and
introduce them to virtually nothing
around so that's subject to some ongoing
work is still very preliminary and it
doesn't deal with the photo schedules
photos kitchen exclusively for eccentric
it's only based on photographs now
please talk more about that someone
asked about mobile mobile phones and
pictures from mobile phones earlier I
think are you looking at that using you
know GPS enabled phones and stuff it
would seem you could get a pretty good
guess of your orientation just from the
accelerometers in there and all that
sure yes that's a good point you know
who look like Excel llamas gets an
estimate for orientation we haven't done
that yet we still focused on these hot
you know using these good cameras that
don't have any of known GPS information
on the mark so longer if just
conventional cameras that we focused on
now but you're right that we should be
moving towards looking at the
photographs taken from cell phones and
uses it so longer that's the next stage
it what kind of I've acting one of the
one can imagine you know having a crowd
simultaneously take pictures of
something with all of their cell phones
upload all their data and give them you
know create a model of what's happening
right there
so it's a lot of in full motion video
perhaps a real time one other thing that
that's of interest is also you know
there's a lot of scooping India out
there right so magnet man you take a
picture of some area that's going Times
Square and pics and victory can't you
use GPS information to know roughly
where you are and then then find from
the street view data that's available
servers what's the best match I just
took a photograph we've got all this
data is you've already gone down the
streets taking photographs and we
obviously can't we match that up and
can't that inform me about my position
and even some other information there we
want a little tie in you know the
photographs are you taking a camera with
speaking information helps place you
more accurately the GPS from the central
it's not very high credit that's like
the GPS myself until I accurate but here
you be able more you know carefully
exactly where you are by matching your
photograph person is insufficient data
Google's enhance their anything else ok
well thank you very much i hear it was
great</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>