<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning Defect Predictors from Static Code Attributes: Lessons from the Tren... | Coder Coacher - Coaching Coders</title><meta content="Learning Defect Predictors from Static Code Attributes: Lessons from the Tren... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning Defect Predictors from Static Code Attributes: Lessons from the Tren...</b></h2><h5 class="post__date">2008-10-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/vrvRsZsoMp8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm very pleased to welcome my ex NASA
colleague and friend good friend Tim
Menzies Tim's done a lot of really cool
work on using machine learning for
different software engineering
applications and he's gonna tell us
everything he knows thanks John a my is
the sound working okay so thank you
thank you John for that introduction
thank you for coming to see me it's a
pleasure to be here at Google I have a
talk that's about 45 minutes and given
the video format what I'd like to do is
do the talk and take questions after
unless you see some gross error that you
cannot accept
in which case please interrupt so we
want to talk about learning defect
predictors this is work funded by the
National Science Foundation of America
and recently we're having a lot of
interaction with the Turkish scientific
research Industrial Research Council to
Batac and we'll talk about that every
talk needs sound bytes I'm sure what
will you remember of this talk in a
year's time well let me give you some
sound bytes maybe you prefer these to
your own I'm gonna say that something
called static code features can be used
to learn software quality predictors
they're surprisingly effective but I'm
going to say that they're kind of stupid
I'm gonna say that there are shallow
well and given our current generation of
machine learners and the Consular data
we're getting we're getting about as
much as we're gonna get out of this
source of information and be better
learners and more data will not increase
what we're learning from these are from
this source unless we change the rules
of the game and the second half of the
talk of some experiments where we change
the rules of the game and we do much
better than seen previously just some
acknowledgments West Virginia University
my colleague boy in Turkish numerous
talented graduate students the soft lab
people at Turkey
Jeremy Greenwald who used to be a
Portland State nurse now at math works
National Science Foundation and the
scientific and Technology Research
Council of Turkey and you have to say
the obvious this is me talking not the
United States government or the country
of Turkey this is an empirical talk so
it's it's worthy to advertise the
promise conference on empirical software
engineering all the datasets I'm
offering here are online at the promis
repository
and if you think you can do better than
what I'm about to show you I invite you
to do so and please come to the promis
conference the motto of promises are
repeatable refutable and improvable
software engineering experiments and if
that's too pompous for you there's a
simpler motto put up or shut up if I
produce a paper promise I try to when
corporate permissions allow to also
produce the data that the paper was
based on and the promise repository now
contains 70 plus datasets for common
software engineering problems and I
encourage everybody I meet to contribute
the promise either as data or as
challenge problems or his papers and our
keynotes for a promise 2009 berry bean
from USC and Brendan Murphy from
Microsoft so hope to see you there this
talk is about defect prediction I need
to describe to you what defect
prediction is and I need to answer some
open challenges on even the base value
of even doing something as crazy as
defect prediction so parts one and two
is stuff I'm sure many of you would have
seen part three is something you may not
have seen before there's a bunch of new
results talking about variants results
and ceiling effects with these static
code defect detectors and they're the
basis of my statement that these static
code measures are a shallow well and
we've gone about as far as we can go
with those unless we change the rules of
the game when I'll talk about one very
simple method for changing the rules of
the game we have some conclusions and
references the slides have all the
references I'll give a copy to John
please email me or John Penix and we'll
send you that so what is the problem
well the problem is blind spots if
you're a diligent driver and you're
driving ahead and you're doing
everything you can to avoid accidents it
doesn't matter how good you are because
of the nature of the car you have a
blind spot you can't see the cyclist in
your blind spot driving a car is it's
like software Quality Assurance infinite
Quality Assurance is infinitely
expensive the mathematics of black box
sampling with replacement on the board
and it shows us the number of tests
required to achieve a certain level of
confidence that we've found something
with a certain error rate and you can
see the log expression at the top and
if I put confidence to one absolute
assurance the log it just it just
explodes because we can't test
everything it's standard practice for
test engineers to skew their limited
Quality Assurance budgets towards the
things they think that matters for
example it's quite standard to restrict
model checking just to the kernel of the
guidance system of a rocket now if we're
only going to look this way at ABCD what
about that huge space of stuff we're not
looking at and what I want is some very
fast method of sampling all the stuff
I'm not looking at the stuff behind and
I want these methods to reach over and
tap me on the shoulder and say look I
know you think that this is important
over here but they're something over
here you really need to look at and one
cheap method one fast method for
generating these blackbox sampling
devices is is to do a little bit of
machine learning and I hesitate to offer
Google the lecture and data mining but
for one slide we have a table of data we
have features we have a class variable
on the right hand side and we're trying
to find combinations of the features
that predict for the class variable and
there's any number of learners to do
this very simple methods naive Bayes
sophistical feature combination Cohen's
Ripper learner the C 4.5 decision tree
learner random forests the motto of
random forests is that if one decision
tree is good why not build 500 and that
many many many more besides and these
are all output combinations of features
that predict for a class so what we're
going to do is we're going to give these
data miners descriptions of the code
we're working with and for this talk
we're going to give them very very
simple descriptions we're going to take
modules and what a module is depends on
your language it's the smallest
functional unit a method a file a class
the smallest thing you deal with we're
going to describe them in terms of some
very quick cheap to collect static code
features lines of code lines a comment
number of symbols inside those modules
something about the core graph inside
that module and we're going to join that
log to defect log saying have we seen
defects in these modules once upon a
time we used to try to do
bug severity and frequency and then be
given the poverty of the data we've been
we've been working with we're force just
a binary I so just say you're defective
yes or no and the output of the defect
predictor is a prediction that says on
future code you I predict you you are
predicted to be defective if now here
are some samples of the static code
features we're dealing with the
legendary McCabe measures on top the
cyclomatic V of G famous or should I say
infamous lines of code measures lines of
code and comment lines of executable
house Ted measures symbols operators
operands are two plus two is one
operator two and two operands but house
it also says we should count unique
symbols so two plus two and two is
really one unique symbol and there's a
whole bunch of other numbers that house
there defines down the bottom and at
this point you're saying these are
pretty stupid we can't learn much from
these I'll half agree with you and
you'll see what I mean as the talk
progresses
why learn static why learn defect
predictors from static code measures
well it's useful easy to use and widely
used if you ask how good at people are
finding defects the I Triple E metrics
conference in 2002 had a panel and asked
that question and the wisdom of the
people of the panelists is that humans
reading code fined about 60% of the
defects this is actually perhaps
slightly on the high side a more careful
study by David Roth Oh talking about a
certain north a Pacific Northwest
company reports that their defect
detection methods found between 35 to 65
percent of the bugs with the mode of 50
and the static code measures I'm going
to show you we rock in at 70 70 percent
eighty percent depending on the data set
and our methods are very fast to apply
it's you know seconds to learn the
defect protector seconds to run it over
tens of thousands of examples and when
we compare that to the time involved in
manual inspection it's much faster if
you're doing a first Scholl kind of
inspection or a Fagin kind of inspection
that they're saying 8 to 20 lines of
code a minute and if you've got a review
team with multiple people take that 8 to
20 lines of per
of code per minute and multiply that by
the number of people doing things and
you see that we have a much faster way
with the static code measures for these
reasons there are hundreds of research
papers that have explored this perhaps
dating back as early as Porter and
Shelby's 1990 paper if anyone knows one
earlier than that I'd like to know where
this idea first came from but that's my
earliest reference Porter and showed me
1990 and I know large government
contractors that won't trigger their
Tiger teams to look at problems unless
some sort of static code measure wakes
up and says this is a problem now when I
tell people I use static code measures I
get a couple of responses one is well
that's stupid there's not much
information in these static code
measures and I will half agree with that
statement they say why use that why not
use knowledge of the developers you know
how good are your people how long have
they been working with that codebase why
not use a history of runtime defects and
do some sort of John musi reliability
engineering where you're talking about
the trends of defects versus time and or
why not use XYZ and here are references
to some very interesting papers that
have used other kinds of measures and
have done achieve very interesting
results my reply is I use whatever is
available and that changes from site to
site and I dream of the day that one day
I'll work for an organization where I
can collect a lot of data in a
consistent format over many projects
over many years meanwhile in the real
world let me show you the organization
I've been working with for the last 10
years this is a record of the NASA
software independent verification of
validation facility the left-hand side
colon shows the number of software
projects they've been asked to review
over time and you can see that initially
they weren't doing much and by about
2003 they were reviewing about 42
projects a year and these projects were
very divergent very different
contractors different goals in the
middle column you can see some
interesting features initially it was
all manned space flight systems space
shuttle by the middle of that big bump
in 2003 it was mostly unmanned you can
see that in 1996 there was a strange
situation where to have IV&amp;amp;V work on
your project you have to give up some of
your
and that was caused all sorts of
political problems and so there was this
lack of information coming through 2003
NASA reorganized that in the state we
will fund IV&amp;amp;V from a central source
surviving he became a free service to
the projects at which point the number
of projects peaked and the information
flow maxed out 2005 different data was
being collected 2007 the fashion change
they stopped being collected the right
hand column you can see all the
different organizational units around
IV&amp;amp;V Johnson at Texas headquarters on
the East Coast NASA Ames down the road
gone Goddard Space Flight Center and
every time it changed organizational
units the database has changed the
information change etc etc etc not shown
on this chart is all the organizational
change that happened after the loss of
the Columbia or that massive
reorganization not shown on this chart
is the impact of George Bush's 2004
statement we're going to the moon and
not shown on this chart is the layers
and layers of subcontractors and
subcontractors and sub sub subs and
here's me in the middle trying to
collect information from this cauldron
this is not to say that this NASA site
was somehow crazy or out of control I
know many commercial organizations with
a faster pace of change than this so
instead of saying what are the right
features to learn quality predictors
from perhaps the better question is to
ask what are the features available
right now now if you can't control the
data collection but you were doing what
I was doing which is after the data is
collected do what you can and you're
talking about other other groups agile
or outsourced projects or open source
projects or layers of sub sub sub
contractors you do what you can with
what you can get and up to now the thing
I've been most able to get is the source
code so that's why I work on static code
features for defect prediction so that's
a statement that I have to work on that
code base that's not a statement that if
I do that I learn anything that's useful
what is the value of learning quality
predictors just from lines of code and
comment just from symbol counts
okay well I mean not these static code
measures kind of stupid there's any
number of papers that will say so
there's papers that say that the
infamous McKay V of G is just a simple
synonym for lines of code it's highly
correlated to lines of code there are
worthy papers that say things like for a
large class or software V of G is no
more than a proxy and in many cases
outperformed by lines of code so our
static code features stupid well if they
were stupid they wouldn't do very well
the performance figures will be rather
low and if they were really really
stupid there'd be nothing you could
learn from one project and apply to
another there'd be no signal in what
they're telling us so let's try an
experiment let's take data radically
different software projects in this
experiment we had for NASA flight
systems we had three NASA ground systems
and just because we could we went to
Turkey to some white good manufacturers
people doing washing machine software
and we started explaining from one group
and applying to another now before I
tell you what happens let's just point
out that at NASA there's any number of
people that will tell you that some
ground systems have fundamentally
different the flight systems you keep
you can't learn one from the or or
there's not much you can learn from one
imply to another and certainly when I
tell the NASA people that I'm building
defect predictors for their aerospace
systems from Turkish white good
manufacturers they say well what and and
if we look at the processes involved in
here like the Turkish developers
talented people but there and and a lot
of their work is hero based programming
you know single single groups powering
through whereas the NASA stuff does
large software process stuff and you see
a divergent range of languages C++ Java
C the idea that you can learn anything
from part of this group and apply to
another especially from something as
stupid as static code measures that
would seem to follow well before I tell
you what happens the performance
measures I have to degress a bit and
just talk about performance measures
again I'm sort of reluctant to do a data
mining tutorial at Google but just in
case
there's other people watching this video
let us to say that ABCD are the true
positives the false negatives the false
positives and true positives respect
respectively found by a binary detector
and with these this these letters we can
define the recall the false alarm rate
precision accuracy the neg pogs rate the
niggas ratio is important it's the it's
tells us the size of the target if only
10% of your data of your training set
has the defective modules unique pods
ratio is 10 and it turns out that for
heart for skewed data sets with large
and egg pod ratios certain common
measures you just don't want to do for
example have you ever thought that you
could be accurate and miss everything
accuracy air felt for in the situations
for large negative ratios does not
predict for our probability of detection
here's an example where we had around
about 500 modules and on the left hand
column shows round about 400 examples
where they weren't defects and the right
hand example right hand column shows
about a hundred examples where there
were defects the first row shows where
the detector was silenced and the second
row shows where our static code defect
predictors triggered and said I'm
predicted defect now what you'll notice
here if you apply the equations is your
accuracy and false alarm radar looking
pretty good 83% accuracy and 5% false
alarm rate I need to call are we gonna
thank you
okay so notice that our accuracy is
eighty-three percent which is good a
false alarm rates five percent and our
probability of detection is thirty-seven
percent so just beware in any unbalanced
data sets using measures like accuracy
in digression let's go back to our
experiment let's learn static code
predictors using a naive Bayes
classifier why use something as simple
as naive Bayes well we'll get back to
that in a second and we'll do two
experiments we'll do a round robin and a
self-learning experiment in the round
robin experiment we'll train on them and
test on me so in a round robin
experiment you have ten data sets one of
them you'll test on and you'll train on
the remaining nine in a self experiment
you'll take one data set learn on ninety
percent of that data and test on the
remaining ten percent now if you do this
if you do this round-robin experiment
where everyone's defects is predicted by
everyone else's code you get a 94
percent probability detection of over
final bugs 94 percent but before you get
too excited about that please note in
red the sixty-eight percent false alarm
rate so if I rush out to a huge crowd of
people and ask their opinion yeah
they're gonna tell me when I'm gonna
have a problem but they're gonna tell me
lots of irrelevant C's as well so beware
surfing the crowd in the self
experiments online - I'm just going to
learn from me and test on me and I get
detection rates of about 75 percent and
false alarm rate of twenty nine percent
and this is adequate for the statistical
purpose of controlling a large group of
programmers trying to optimize so I'm
gonna maximize the amount of time of
these QA teams trying to work out
whether what the detectives triggered on
was actually faulty now it turns out
with a very simple nearest-neighbor
trick we can turn the round robin stuff
into useful stuff it turns out we can
learn from the crowd in a very in a very
simple way in the last line is a round
robin experiment but for every test
instance we found the case nearest
neighbors in the training set and recall
here that the test is from say a Turkish
wired manufacturer and the training set
is NASA flight NASA ground and other
Turkish systems and with that simple new
neighbor filter we get about a 69%
detection rate and a 27% false alarm
rate which is adequate for the purposes
of controlling large populations of QA
people so from this experiment we can
say it's best to learn from local data
however it is possible to use imported
data but make sure you do some relevancy
filtering on the imported data so if you
don't have data start local data
collection and meanwhile use imported
data filtered from filtered using
nearest neighbor and I recommend you the
promis repository the data up there is
enough data get started about learning
defect detectors now what a question
that arises when I show people this
slide is they say well ok how much data
is needed to build my local defect
detectors when do I switch from using
imported data to local and we'll get
back to that several times in this talk
how much data is enough to learn these
defect predictors now here are some
results that you may not have seen for
many of you this has been you know data
mining 101 but let's talk about several
questions what are the best static code
measures to learn from well it turns out
that there isn't there isn't a best set
let's do a feature subset selection
experiment if I have a bunch of features
and they have a certain performance
let's add in another feature grow the
set and see if the performance improves
and we'll do that experiment until the
performance no longer improves and we'll
do it the right way we'll do you know a
greedy search with an arisin of five you
know nine like five lives before you die
and our target learner inside this
wrapper will be a naive Bayes and the
chart on the table on the Left shows six
data sets and the index of the features
selected now in these experiments we had
42 static code measures and please note
that in a nine experiment shown on the
left hand side left hand side table we
never needed more than three features
but please notice that we found
different features all the time nearly
all the time like the data set PC one
needed features 335 and 37 the data set
MW one needed 23 31 and feature 35
and we can go through that list if you
want but the tallies are on the
right-hand side
feature one was used twice feature three
was used twice no feature was used all
the time no feature was used in the
majority of cases there's nine datasets
on the left you have to appear five
times or more to be in the majority so
what's going on why don't why isn't
there a best data source to learn from
well let me introduce you to the feature
information variance effect this is an
experiment where ten times we take 90%
of the data and then we use something
like the whicker's information gain to
ask what's the information content of
this attribute now if you look at the
actually I'm confusing the midterms here
I should consistently say features all
the way through this talk and attributes
appears on the x-axis so I apologize for
that
if you look at the information gained
over ten 90% samples what you see is
there is a large number of features on
the left-hand side on top of a plateau
this is the space of possible good
detectors and they have the variance can
you see the the error bars showing the
variance over the experiment so what we
have here is a whole bunch of weak
indicators on top of a mountaintop
saying I'm useful I'm useful so what I
want to offer you is that these static
code measures are a pretty dumb way to
characterize code they're a weak
indicator but if we take three four or
five of them in combination if they all
point in a certain direction you can
start saying yes I actually have a bug
and so by the way if anyone ever offers
you V of G greater than 10 as a detector
please point out to them that that's the
empirical basis for that measure is kind
of weak seemingly minor changes in your
sampling policy would mean different
detectors appear at the top of that
plateau in your cross-validation for
example the way you set up your random
number generator will mean different
predictors will look better than others
another question people ask me is what
are the what's the best data miner to
use and these are some recent results
describing a ceiling effect and you
recall that a ceiling effect is an
inherent upper bound on the performance
of some of some
the left-hand side is some experiments
from WVU grad students the right-hand
side is a recent paper by lessman as a
Lin TSE July 2008 now in rounding out
the middle of 2006 I did perhaps one of
the largest defect prediction of studies
to date on NASA data and I achieved a
certain level of performance and I've
been unable to improve that performance
since night since 2006 if you look at
these results on the left hand side
you've got nine different learners and
you've got quartile charts and six of
those nine have a very similar high
values I mean statistically boosting
actually does best naive Bayes does
second best and it is statistically
different to boosting but given the
simplicity of boosting and the
computational cost I'm sorry given the
simplicity of naive Bayes and the
computational cost of boosting I can't
recommend boosting over naive Bayes I
should say what the just just don't ask
yet what's the y-axis what's the
performance measure I'll get back to
that for the same performance measure
and for many of the same data sets
Lessman as our report a similar ceiling
effect can you see just here a vertical
dashed line okay well these yeah and and
and you see that random forests
generates the highest ranked performance
for that for this for these data sets
but can you also see that the rank of
rate of random forests has a large
variance now if you do the stats and
Lessman does the stats correctly in my
view 14 of these 19 data sets are
statistically indistinguishable from the
top ranked person so these 14 here are
have our other Saints and that he's my
beloved naive Bayes just here see 4.5 is
also in this as good as the best just
here even though over here a Java port
of C 4.5 is one of the worst so there's
some issues about the experimental
methods and why we're not getting stable
results but the general message here is
that you
if you run a whole bunch of learners on
these defect predictors many of them
will produce the same results now I have
to tell you what the evaluation bias was
here both these people used area under
the curve of a probably detection
probability false alarm graph and those
of you who know these sort of ROC curves
and I'll show you some examples in a
minute if you do your cross-validation
Zoar random samples of the data and
multiple learners
you'll get multiple points sitting under
on a PD versus PF plot and the area
under the curve is a common measure for
performance and base and on that measure
lots of learners perform about the same
well perhaps we need more data maybe
it's not the learning we need we need
more data well here are some studies on
two data sets there's the the effect I'm
about to report occurs in many more data
sets I just don't have room or time to
go through them and this is all stuff in
the promis repository if you want to
check this please please do so the top
results are picking a hundred modules at
random training on 100 200 300 and then
testing on another hundred not used
during training that's the random
sampling one the bottom one is micro
sampling where you pick n defective and
in non defective at random and learn
from that and in the bottom set we we
set N to 25 50 75 and then we train on
25 examples not used during I'm sorry
then we test on 25 examples not used
during training and the effect is the
same in both statistically there's no
improvements in the random sampling over
about a hundred instances and there's no
improvement in the micro sampling over
over about 50 instances half half
defective non-defective now that's kind
of good news but it may mean a whole
bunch of my graduate students aren't
going to finish we're not doing better
with better learners we're not doing
better with more data and so people like
less and we'll say the importance of the
learner is less than generally assumed
and practitioners are free to choose
from a broad set of candidate models
when building defect predictors which
sounds like the end of the line it's all
over there's nothing left to do well
given that it's only high
way through the talk you can kind of
guess what's gonna happen next we're
gonna change the rules of the game and
when we change the rules of the game the
ceiling effect changes in an interesting
way so let's change the rules of the
game
these learners have an evaluation bias
and depending on your business you
prefer different learners so for example
consider this PD versus PF graph this is
one of these ROC curves receiver
operating characteristic curves and
recall that we measured the performance
of a learner by the area under the curve
when we build different models using
subsets of the data and the false alarm
rate is along the bottom and the
probably detection is along the y-axis
and the sweet spot is 100% detection
zero false alarm which is up top left
and if you look at the characteristic
curve of learners it bows up towards the
sweet spot and doesn't click it if the
line P F equals P D is where you're
probably detection equals your
probability of false alarm and we call
that the no information line and the
negative curve if you have a learner
that's running down that negative space
typically you can negate something and
it throws itself up at at that space so
really the space of learners is is the
curve on top now in that curve on top
there's two interesting regions there's
a risk adverse region where you will you
want high P DS you will accept anything
as long as you get high PDS and given
the shape of this curve the only way to
get high P DS is also to have high PF
consider airplane flight and pregnancy I
flew here and I stood in line and I
sometimes I have to handle some
impertinent questions from security
guards and I'm willing to put up with
that I'm willing to put up with a false
alarm rate so that when I get on the
plane any bomb has been detected before
I get on the plane morning sickness
morning sickness is that condition where
the body is suspicious about stomach
contents and removes them it where it
wears a high false alarm rate in order
to preserve the life of the fetus so
these are risk adverse situations where
you want to choose print learners that
drive you into the high PD and
PAF region there are other business
situations where you don't want to waste
your money chasing false alarms and your
and your you're willing to wear medium
to low probabilities of detection as
long as you don't have any you know any
false lamps so that's the risk cost
adverse region just there and so rather
than assessing these learners on some
uniform evaluation criteria instead
let's talk about your business and say
what are your what's your evaluation
criteria now ash lemon Briand offer an
evaluation criteria that's so different
to what's been explored before that
we're gonna see what happens when we
impose that and what Ashley and Brianna
say is that my probably detection had
better be greater than the effort
involved in processing these examples
the detectors run trigger on a code base
and let's say you trigger on 20% of the
code base according to a shaman Briand
you want to have more than 20% of the
defects in that 20% of the code base
that you've trigger on so in their
preferred criteria the PD has to be
greater than the effort all right so
let's talk about tuning learners to the
business criteria let's talk about
evaluation aware learners every learner
has a bunch of biases there's a sample
bias
you know what data are you learning from
there is a search by assess and there is
an evaluation bias E and in the search
bias for things like decision tree
learning is which is the next best
attribute to split on and for those of
you who know this stuff you know
information gain is the standard search
bias used in C 4.5 and then once the
model is generated it goes out to the
business users who assess them and I
hope by now I've convinced you that for
datasets with high neg Pods ratios you
don't use accuracy but you might use PD
or PF where you might use the eleven
beyond criteria now I believe in all my
reading of the defect prediction
literature I believe the following is
true
typically the search criteria used to
build the theories is different to the
evaluation criteria used to assess the
learnt theory
having said that the natural next
question is what happens if you make the
search criteria the same as the
evaluation criteria you start as you
mean to go on you import the business
context into the inner loop of the
learner what changes now my grad
students act Milton explored that and a
learner called which and which is
incredibly simple it's it's and for
those of you who know you know AI search
we can talk about some of its
interesting features but it's very easy
to describe firstly discretize all the
numeric ranges secondly score are those
ranges according to the evaluation
criteria and sort them onto a stack
thirdly pick two entries from that stack
favoring things towards the top of stack
now you've picked two things that seem
good according to E
combine them put them back on the stack
and search them sort them into their
right space so next time when you go
round to pick two ideas you're picking
things near the top of stack now the
right hand side plot shows the UCI data
sets the score on top of stack and the
interesting feature is that after a
small number of picks 20 the top of
stack score usually stabilizes and in
one case marked in AE after 110 picks we
did better and we'll just double that
and we will run which for 200 picks
that's the which learner now notice that
some there's no search criteria there's
no separate search criteria s in which
instead we can take the evaluation
criteria used by the business and we can
wire it into the guts of which when when
you compile which you actually write
three lines of C saying what is my
evaluation criteria now certainly there
are other methods that could bias their
learning in this way I think it would be
natural to go to decision tree learners
and adjust the decision split criteria
according to this sort of thing I'm not
sure that cost-sensitive learning or
rock ensemble combination is useful in
this context you can do you cross
thousand you can have as many learners
as you like learning optimizations of
area under the PDP F curve and I'm not
sure that any combination of those will
win on efforts versus PD for example so
but but we use which for the following
experiments well firstly because we
build it and secondly this this early
termination of the learning showing top
right suggests that which could be
useful in the future for stochastic
real-time anytime learning but it
doesn't really matter whether we use
which or some variants at the bottom of
three methods all we're going to show
you is what happens is when we tune the
learner to the evaluation criteria we
remove this separation from the search
criteria to the evaluation criteria and
what happens is okay so let's talk again
about ash lemon breeland's evaluation
bias they say if you're a budget
conscious team and if X percent of the
modules are predicted to be faulty by
some predictor and they contain less
than X percent of the defects then they
would say this defect predictor is not
useful they want probably detection
greater than effort so on the right hand
side curves you can see the percent of
the code base on the x axis of the
detected triggers on the y-axis is the
probability of detection and we can see
some interesting curves on this plot the
minimum line where P D equals P F this
is the minimum bound
according to Briand and rome has to be
above that bound the best curve is
interesting we can offer an upper bound
of performance of this device ganesh
guru and his colleagues at university of
baltimore maryland county have pretty
much convinced me that smaller modules
contain a disproportionate number of
errors they offer the following
inspection policy take all the modules
you're looking at sort them by size and
look at the smallest one first
and there's interesting reasons why that
actually works and we can talk about
that if you're interested but supposing
some omnipotent oracle reaches down and
says I am triggering on just the
defective modules and you take those
modules and sort them by increasing size
you get the best curve showing there and
that and at the point of a equals 30 you
know a 30% of the code base is in
modules containing defects we can't do
better than best our learners have will
not do better than best and what we'll
do is we'll express our learners as the
ratio
of their area under their performance
curve to the area under the best curve
so the performance results I'm about to
show you are percentages of best now a
few things happen with that fraction
firstly you may ask how we
operationalize the the y-axis how do we
really know we've found something
because the detectives trigger and they
have a false alarm rate right so we'll
assume there's a secondary process that
looks at what we triggered on and says
yeah it's actually an error it could be
a human inspection process now let's say
that secondary process is Delta percent
effective that Delta is common to all
the curves by dividing the area under
one curve to the other we actually
divide that away and so now we can
assess any defect prediction method
regardless of the downstream final check
and just to give the game away can you
see the can you see the manual curve
there that's what happens when we apply
Ganesh Carew's rigged on our data sets
we do do better than PD PD greater than
effort and we have learners you see the
bad curve there that actually do worse
than manual methods and embarrassingly
one of the learners I've been telling
people to use for years actually does
worse than Ganesh Carew's manual rule
for inspection
happily we do have good learners that
work better than our manual methods and
approach a large percentage of the best
so let's look at some results so we're
going to go back to those NASA and
Turkish white good data sets from the
promised repository we're going to do
ten random orderings of the data and a
three-way cross now
with standard practice to repeat these
experiments I don't need to tell you
folks this it's standard practice to
repeat the learning end times repeating
the ordering to isolate you from
ordering effects in the graph in the
plot in the middle the rank comparing
each row is computed using firstly a
mann-whitney u test to see whether you
your distributions differences it to the
next guy and then you get a higher rank
if you are different and your median
values higher the performance scores
here are the percentage area under the
best curve we have various learners
which micro 20 naive
Ganesha's manual method see 4.5 and
Cohen's Ripper method ported to Java in
the whacker called J rib micro 20 is a
variant of which where we do this micro
sampling thing where we learn on 20
defective modules and 20 non-defective
modules and recall here that we are
trying to optimize for area under the PD
PD versus effort curve and the learners
that have been tuned to that criteria do
best which in micro and the learners
which were developed historically to
optimize for accuracy see 4.5 and Ripper
do worse do do very badly in fact now
naive Bayes which is this method I've
been saying is good enough and it's easy
to do does no better than manual does no
better than manual so please if you have
a certain paper of mine on the shelf
please just cross out certain lines at
the conclusion and the micro sampling
method that only learns from 40 examples
does nearly as well as which that learns
on many more examples now this is
results from one data set we have papers
repeating these results on many more
samples again from the profits
repository and the pattern I've just
offered you repeats across all those
data sets so we have more evidence for
the shallow well after about 40 examples
of this and not it we do as well as
anything else we do better than manual
thankfully and a whole bunch of learners
used quite standard in this field do
very bad when assessed on if at vs PD
okay so so what if what are we saying
here but why does it work
that's a question I don't have any
answer to this question I'm just showing
you that we have a ceiling effect based
on area under the curve PDP F and if we
change the rules of the game and do area
under the curve of PD versus effort
expressed as a ratio of best we can do
better but fundamentally why does it
work well I submit to you that there's
another related question why does any
information retrieval method work on any
technical corpus
do you folks know Andy Marcus's work on
constant location like your your your
changing a piece of the code and you
want to know where else to go look to
find related stuff to change that's a
common maintenance task
Andy solves that using some information
retrieval methods
he doesn't do deep semantic passes of
the code he does simple symbol counts of
the methods and this simple information
retrieval method for large enough corpus
lets him find related concepts to what
you're doing right now why does that
work why do these stupid static code
meters work what's going on is there
some constraint which when we work one
our compilers our languages our target
domains is it is there something about
human short-term memory which means that
we don't work across some wide plane of
possible behaviors we actually are
somehow constrained to a very narrow
valley and in that narrow valley very
simple symbol count methods like the one
I've been showing you here are enough to
offer you guideposts that point you in
the right direction down the very narrow
valley of behavior we actually do what
is this invisible hand that is
constraining the work our technical work
such that these simple methods work I
don't have answers to that question but
I hope to in five to ten years time if
you want to know what will I be doing
for the next decade of my life it's that
last question what is the invisible hand
the Simplot that makes this all this
work possible but these are some
questions I do have answers to ID effect
predictors useful well if you're only
gonna build these defect predictors it's
not useful but if you're going to use
them as a way to control when you
trigger much more expensive QA methods
when you you know bring back the Guru
from her six month holiday and say we
really need you to work on this thing
for a month when you wake up the model
checking when you wake up the more
expensive methods yes these methods are
a good way to sample a large space of
code you're currently not focusing on
and finding what else you should be
looking at are these defect predictors
general well yeah I mean this is strange
to say but I can go to NASA flight
systems and learn a theory that's
relevant to Turkish white goods and
vice-versa
that's pretty strange is it best to use
other people's data Wilton no it's
actually best to use local data but you
don't need much dozens to under 100
examples depending on whether you doing
micro sampling or random sampling and
that's because these static code
measures don't contain much information
we can scoop them out pretty easily
which learner is best the building
defect predictors well if you're
maximizing for area under the curve a
PDP f you have many choices but but if
you're if you have another evaluation
bias it might not be best to use the
traditional machine learning set that's
been optimized for accuracy or PDP f it
might actually be better to tune your
learner to the evaluation criteria that
its models will be assessed to and which
was an example of how simple that tuning
could be and there are certainly other
ways to do that what is the best what
are the best static code features to
learn on well that's the wrong question
best turns out to be a domain-specific
construct and my advice to you is
collect everything that's readily and
cheaply available and throw to the
learner and see what happens which
brings me to the reason that I'm at
Google today
apart from visiting your wonderful
campus we have an NSF project where
we're trying to take seriously this idea
of local knowledge and what I'm trying
to I'm talking to many companies about
whether we can find people willing to
work on some rig where human expertise
and data mining is get together or maybe
it maybe one surpasses the other we've
seen above that the best features the
data set dependent and we call that the
feature information variance effect and
we have many examples where if you go to
a site and say well there's a bug local
knowledge wakes up and tells you a
different story one example from
Microsoft's and data mining colleagues
of mine found a bug in a certain Windows
API and the windows team said app
doesn't matter
what wouldn't mean doesn't matter well
for contractual reasons we have to
maintain that for that compatibility two
windows 3.0 but nobody ever uses that
API now that sort of local knowledge is
important to know you don't want to
spend time fixing a bug knowing cares
about and we've seen that we can do
pretty good learning on dozens 200 to
under 100 examples now one of the
reasons to do data mining is that the
data space is so large that humans fall
asleep working through it if we're down
to dozens to under a hundred examples
learn adequate theories maybe we can
augment or maybe even replace data
mining with human-in-the-loop case based
reasoning so if you think that your
domain expertise is better than my
static code measures I'd really like to
talk to you about trying to set up some
read where we can work together and
happily given this is the start of the
NSF project we have about an hour two
and a half year window to work on this
so we can talk about baby steps and then
working on to more elaborate things down
the way so this is the end of the talk
I've made the case that static code
features can be used to build software
quality predictors they shouldn't be the
by any means the only thing that you do
but they can be the gatekeeper that
controls whether you apply more
expensive methods down the wave but the
static code measures are a shallow well
it's very easy to get to the bottom we
are not extracting more information with
better learners with more data in fact
after less than a hundred examples and
sometimes even dozens of examples we're
doing as well as anything else we I
suspect we're not as long if you stay
with the traditional rules of the game
area under the curve PDP f we're not
gonna do better and so we advise the
rules of the game let's tune let's build
learners specific to the business
criteria barry bean has been talking a
lot recently about value based software
engineering where instead of saying
let's remove all defects at all costs
let's start saying well wait a minute we
have to balance that against things like
we have to get it to the market sooner
so we might trade off getting it to the
market with a few more defects as long
as we can do some market leading work so
let's talk about not my advice to you is
the way to improve these learners is not
to look at the data and not to look at
the machine learning is to look at the
business let's look at the domain
expertise and see if we can push those
in to the learner and so I'm here today
to see whether we can do things
where we augment data mining with
human-in-the-loop processing okay so
that was that was how long we have four
questions John five ten minutes
okay so we have time for questions so
any questions or comments should be able
to be CN if you wanna come in remote
so that so that would be quite exciting
we can really go to the details are the
for those who have meeting us this
afternoon
any questions from the people you say
the method or the hired so the question
was am I do I believe that smaller
methods contain more errors yeah I do I
do I do
there's probably uh can I expand told
you wanna suppose oh sure sure supposing
you've got well firstly it's an
empirical observation and I've seen it
in enough datasets to really believe it
if I can go back if if that thesis was
wrong then on this experimental rig the
manual curve would be shaped the other
way so where I actually took out of this
talk
a comparison of manual up versus manual
down and a manual up we run through and
look for bugs and the smallest first and
in manual down we do the results and in
I can tell you that in eight of the ten
data sets have looked at manual upbeats
manual down and there's a very exciting
papers by Ganesh describing this work
empirical software engineering October
2008 very nice paper now why why well
consider some IDE some object framework
where well what you're doing is you're
not building live systems but you're
building tiny lots and lots of tiny
subclasses in many places across the
space so most of the work is in the
upper levels of the framework and you're
providing lots of small things and so
your business concept your design has
now been chopped up and spread like
matchsticks across the codebase and in
that rig it's entirely feasible to me
that smaller has got more errors
yes let me let me pull up the data I've
got and I'm trying to see where the case
III was but now there was a question at
the back I don't want to attract anyone
else now so so the question is is there
more information out there the static
code measures things like developers
efforts of winning by people well it now
people with epilepsy please don't watch
so in this work know this work was only
based on static code measures because I
made the case that sometimes if you're
in a very dynamic organization with many
sub sub sub contractors you can't get
that level of granularity now I refer
you to the first reference a very nice
big C 2008 paper that does explore
exactly that issue and I also refer you
to Tom Austrian and Elaine why cuz
October ESC article where they ask do
too many cooks spoil the broth and they
asked whether bigger teams have more
bugs as well it seems and they actually
find no evidence for that so does that
answer your question sir and I shut you
off
yep yep yep okay other questions or
comments any any questions from VPN nope
okay well thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>