<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2016: Code Coverage is a Strong Predictor of Test Suite Effectiveness | Coder Coacher - Coaching Coders</title><meta content="GTAC 2016: Code Coverage is a Strong Predictor of Test Suite Effectiveness - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2016: Code Coverage is a Strong Predictor of Test Suite Effectiveness</b></h2><h5 class="post__date">2016-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/NKEptA3KP08" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so now we're gonna get back
into lightning talks first up we have ro
Raoul from Oregon State University okay
hi I'm Rahul Gopinath as HT student from
Oregon State University and this is a
summary of my research along with that
of Iftikhar I have met from the same
University so this talk is about
evaluating test feeds and the best
metrics we have in actually ascertaining
the quality of our test feeds and by
quality I mean the ability of a test
feed to find bugs so before starting a
question so how many of you were present
at G tack 15 okay so in that talk there
was another talk with a very similar
title that said that code coverage is
not a strong predictor for the sweet
quality so this is actually a counter
argument so it was suggested previously
that you know we should not actually
look at code coverage and it's not
trustworthy and we should look at
something called mutation coverage so I
really tried I will I hope to convince
you otherwise
and I also hope to show that the
suggested gold standard that's a
mutation and a mutation score fitter
suite is not only expensive to actually
compute but also inconsistent and it's
very much dependent on the tool that we
used to measure the mutation score so
essentially mutation score is not really
a silver bullet and we have to use in
can we have to use it in conjunction
with this the coverage of it a suite so
well Lane is a program ready to ship
notwithstanding the comments about our
you know testing in production for the
Google Drive the Google automatic car
this is one of the biggest decisions
that we as testers have to make and in
the old-fashioned way we usually stop
testing when we are reasonably sure that
the bugs are completely over not on you
of course so that means that an
effective test suite is very important
and that's that's what we really need to
find how do we actually make sure that
our test suites are effective
and how do we actually evaluate this
test fit effectiveness of course there
are two main things that we all know
that we can evaluate in a test suite
how good is its coverage of the program
elements and secondly what is the
strength of its or occurs by Oracle's I
mean just assertions because if you
don't have assertion then it's useless
so in the previous deep talk Laura
Elizabeth saw who actually did that talk
she suggested that coverage should not
be relied on because it rarely provides
more information than just a pure size
of the test suite so she instead
suggested mutation score where the
effectiveness of a test suite against a
set of mutants is valid how many of you
know actually no mutation score how do
we evaluate the mutation okay so at
least I know that there are a few people
who are around here and I also heard
first-day that Google is also some of
the people in Google are also using
mutation scores so it's not completely
out of the blue the unfortunate thing is
that well as an basic idea what we do in
mutation analysis is that we start with
a program and then for each token in the
program we start introducing changes
what are the changes that are valid
grammatically that if you compile will
not result in an error so we have a
large number of variants to that program
each variant you compile and run it
against your test suite and see how many
of these variants are actually caught by
the test fit the total number of
variants that are caught by the test
feed divided by the total number of
mutants gives you the mutation score
that essentially means that this is the
actual effectiveness of the test feed
the unfortunate thing is that after
Laura's speech programs especially in
the programmers community like hacker
news or reddit they all took it to mean
that we should really completely discard
coverage and the comments were that
coverage is meaningless in the real
world and arbitrary lines in science
such as 90% 100% coverage is pretty much
it they don't provide any utility but is
that sufficient I mean is it sufficient
to ignore coverage so that is what we
set out to find
so to start from the very basics there
are two factors of course you know
assertions and coverage and coverage on
its own is useless without assertions
and assertions have to be paired with
good high coverage test Suites and
Laura's idea is that coverage is
completely determined by the size of the
test feed and that it doesn't actually
tell us anything new because it's so
closely correlates with the size of the
test feed so since the suite is so
easily determinable we thought even
actually running it what is the point of
actually looking at courage and why not
just use a test which size instead that
was that was a point in the previous
talk
however the question is does it actually
make sense to control for the size of
the test suite it's not a good measure
we do not actually know what is a
maximal number of tests that can be
there in a test feed even for the same
program because you can have different
as feeds for the same program and
different tests are not necessarily
cuold in strength there can be some
tests which actually identify a larger
number of problems and there are there
might be tests which are specific so
size for the test fit while it is simple
is not really a reasonable measure
because we don't really have any our you
know end points there is no zero percent
test fit size or a hundred percent of
each size so one of my points is that
even if coverage just measures the sweet
size it's a good thing to have because
at least it tells us how large the test
feed is and secondly none of us actually
cares about the total number of tests
that we run we may have a time budget we
may have other kinds of budget but not
really related to the number of tests so
for a practicing tester there is only
one single question can I use coverage
to measure the test suite effectiveness
and this is what we set out to find in
my research so what we did was that we
started with 250 real-world programs for
the study from github and the largest
was greater than 100 kilo lines of code
so these are not all programs and we
checked coverage for both organic that
is covering that the test feeds that are
written by
people like us and also
machine-generated are tests that are you
know generator by random so we found
that for these organic test suites that
are written by human beings if a
statement is covered then that statement
has 87 percent chance of if there is any
mutant in that particular statement that
mutant has 87 percent chance of being
detected and if you have a statement
coverage then that can actually predict
mutation score with 94 percent certainty
that is once you control for statement
coverage mutation core score actually
provides only 6 percent extra
variability that you can I you can you
know you can evaluate so this is another
things are the read what the previous
blue ones were what developer written
and these red ones are automatically
generated by Rando be so as you can
expect the Oracles are really bad and
there was really 61% chance that if a
line is covered that particular any
mutant in that particular line will be
caught by the test video for by the
randomly generated a sweet so given this
result there's a question that we can
ask
so if coverage is so highly correlated
with nutrition score what's the point of
using mutation score I mean we can
essentially just use coverage and
predict the mutation score instead so
until now we assume that coverage
provides no extra information with with
respect to test feed size that is the
assumption that Laura started with that
was the result that Laura had however
the coasties is that particular question
is that assumption really true remember
that her results were actually from just
5 programs even the very large programs
but still only 5 programs while we
actually have 250 programs and she was
also using random subsets of the same
the suite again and again and again so
he said possible that her conclusion
that coverage is directly related with
the switch size may not actually
generalize to
programs so this is what we actually
wanted to find so what we did was that
we eliminated the effect of the Suites
size statistically and we actually found
that even if you control for the test
Suites size for the remaining amount of
information that is provided by the
mutation coverage 75% of it is still
explained by the statement coverage
score so that means that the previous
assumption or previous cell from flora
from these five programs are actually
not generalizable to the larger number
of programs especially large programs
and more random programs not exactly you
know chosen by the researcher themselves
which is a major issue in the software
engineering research because usually
there is a you know problem that we rely
very heavily on specific programs which
are use of again and again so even if
coverage is a reasonably good criteria
it's still possible that we might
actually go for mutation analysis
because in some sense mutation analysis
comes very close to actual bugs because
we are actually introducing bugs into
the program and evaluating how many of
these are caught so should we actually
use mutation analysis is it actually
ready for the primetime or the question
is should we use mutation analysis as
the primary means of evaluating quality
fetta suite so unfortunately mutation
analysis is beset with a large number of
problems the biggest thing is of course
the total the large number of variants
each of these variants have to be tested
by the complete test suite and there is
no way out of it we essentially have to
evaluate each of them even small little
programs like b square minus 4ac tend to
produce large number of mutants and
secondly so that's mutation score is
really costly and even if you ignore the
size of the mutation analysis there's
another major problem these are called
equivalent mutants the problem is that
these changes that we introduced the
tiny little changes that we make to the
program they need
not always result in a fault some of
them can actually result in programs
which are behaviorally same as that of
the original program these are called
semantic loans or in the in terms of
mutation analysis terminology these are
called equivalent mutants the problem is
that if you have a program if you have a
program which produces say 50%
equivalence none of the test suites that
you can ever write is going to have a
mutation score that is larger than 50%
so and unfortunately there is no general
way as of now in software engineering
research to identify how much of mutants
would actually be equalled given a
program nor can we identify given two
programs whether one are sorry a mutant
and a program whether the mutant is
going to be equivalent to the program so
this is a result from you know Turing
computation this is called Rice theorem
that essentially says that a general way
to do that is pretty much impossible so
that means that a low mutation score is
not generally indicated if it doesn't
actually indicate a low quality of test
fit in general I mean it can be true for
most of the times but if your program is
a unique snowflake then it is possible
that you have a low mutation score but
you actually have it as fit that is
highly effective and even versus
something called a redundant mutants so
redundant mutants are still somatic
clones
there are semantic clones of each other
they have a fault in them but there are
a large number of mutants which are
exactly the same fault so if your test
case detects at least one of them then
the rest of it falls so what that means
is that if you have a large amount of
redundant mutants and your tester suite
catches one of them then you will have a
high mutation score even though the test
suite may not actually have been very
you know effective so we can't really
trust mutation score on that score
either
and finally ah this is a problem with
the current state of the art this is not
a problem with mutation analysis itself
the problem is that different mutation
tools have very different harm you know
they act on very different levels so you
can act
you can have a mutation score that
mutation tool that acts on the assembly
or in the machine code or on the the
source code itself so here I see here I
showed two different ways in which you
can have mutants one on the source code
itself and the other on the assembly and
the problem is that we have on the left
are the mutation score that is computed
by three different tools that act on
three different levels and the problem
is that these tools do not actually
agree with each other that well so you
can see that here so this is the current
state of the art for mutation analysis
so it's not consistent across different
tools so finally the proof of coverage
is actually the proof of it is in the
pudding so if coverage is a good measure
of test suite effectiveness then a well
tested program that we assume by using
coverage should actually result in fewer
bug fixes and we should be able to find
that difference between you know covered
and uncovered programs or covered and
uncovered lines in the same program and
this is what we did next using the same
250 programs so what we did was that we
started with an epoch and then countered
the bug fixes that happen on that
particular line until it was actually
wiped out by a feature fix so what we
actually found is very interesting we
found that if there is a line that is
covered and you compared that to any
general line that is not covered a
coward line is or an uncovered line is
twice as likely to have a bug fix as
that of a cover line and that is true
across treatment block or method level
that is if you have a method that is at
least tested or covered by some test
case wherever that method would have
half bug fixes in its future compared to
a method that did not even have a single
this case covering it we don't have
sufficient hard data for class so you
can see that our p-values are somewhat
low and we don't have significance there
so yes coverage is a very strong defense
against future books
irrespective what are the thing
so in summary coverage is still the best
measure that we have for evaluating test
feed quality and mutation score cannot
this provide a secondary measure so
don't rely solely on nutrition score and
also if you are using mutation score
understand the problems of mutation
score it is expensive it is costly it is
unreliable and provides maybe a little
extra information like 6% beyond our
coverage and more importantly in
addition to the you know the simple
measures like mutation score or coverage
if you actually look at which statements
are actually not covered and try and see
why it is not covered or look at even
mutants look at the mutants that are not
killed and see why it is not killed that
is probably a very good testing
technique in itself yeah thank you
excellent Thank You rogue
so the lightning tuck we'll take one
question whoa wait it's Laura from last
year she's calling she has super sort
supposed to talk to you no just kidding
so I think that covers the first of all
ask do you do you have an opinion or
Studies on code coverage effectiveness
with property based tests so I will
start with the first one first because
this is an ongoing argument between us
whether mutation analysis is ready for
primetime use and you know what my
opinion is so software engineering
community is trying to come up with
better tools to actually remove equal
mutants and you know mutants and comes
with better tools so that is where
engineering community as of now stands
okay and do you have an opinion on
Studies on called convergence effect do
you mean code coverage well well code
coverage effectiveness is pretty good
and I think property baseness are really
good especially things like you check
those actually help you quite well but
the problem with property based testing
is that it's really hard to write on
that kind of test I mean you really need
programmers who are well versed in that
yeah all right excellent so thank you
row
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>