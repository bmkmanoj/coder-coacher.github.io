<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Signal Processing and Communications for Sensor Networks | Coder Coacher - Coaching Coders</title><meta content="Signal Processing and Communications for Sensor Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Signal Processing and Communications for Sensor Networks</b></h2><h5 class="post__date">2009-04-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pz4XuSEOYXY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">much foreign embarrassing Shanno as you
know is one of the brilliant people I've
worked with so that's a trick to do
brilliant research ok told actually
about sensor networks essentially and
it's work done in collaboration with a
large team of PhD students former PhD so
it's really an overview of many things
we have done and now pick out a few
things which I think are you know may be
of particular interest and end up using
Google Earth so there is a connection to
Google ok so the way as I told is
structured is I'll tell you how where's
this work actually or in a originates
from it's from a big Center we're on
EPFL and it's a central not of
networking and sensor networking going
from theory to applications and you'll
see will essentially cross all these
levels hearings at all and as a
technical part are these three central
sections of the talk one is about
distributed signal processing I'll
explain what I mean by this it's not
standard signal processing but it's
something we have for a few years now
we'll talk about some information
theoretic result but only you know
lightly I will say about something which
is called distributed source coding when
you have many many sources all over the
place and you try to code them together
without having these sources talk to
each other and I'll show result
specifically on this and then that's
just what I calls a controversial part
of the talks as always to be a you know
some part that sort of wakes up the
audience
I mean maybe at Google it's not so
controversial but usually I you know I
give these talks at places like Nokia
and so on that our communications pure
communications company and you'll see
that sort of I I put a question mark on
one of the basic axioms actually of the
digital infrastructure which is
raishin theorem and then I'll do the fun
part is actually talking about how we
took so much that work and applied it to
environmental monitoring by doing actual
deployments working with user
communities in environmental monitoring
and showing some results okay good so
let's get going
thanks to the Z organizer and a bunch of
people and funding agencies and so on
and as I said this work was done inside
the center we have been running for
almost 10 years now at EPFL which is the
Center for mobile information and
communication systems usually I present
this in the US by saying it's a great
Center because the budget keeps going up
in US dollars okay so of course it has
been flagged in Swiss francs which is
the money we actually use but it's
similar to us engineering research
centers which are big operations funded
by the US NSF and what this thing does
sorry the network here has EPFL was
leading house eth our sister institution
it has a bunch of industrial companies
that are part of and as I said it's a
large network of about 30 faculty
members 100 PhD students and so on ok so
this sets the stage what what does this
Center do it studies what could be is a
next generation of communications
infrastructure the center was launched
at the time when large telecom operators
had spend a lot of money actually
hundred and ten billion u.s. dollars
Wow these days with the financial
industry going down the drain this looks
like a pittance right well it's hundred
ten billion right it's sort of that
small piece of you know what the bailout
is these days but these days ten years
ago this will take a lot of money and
that's what telecom operators paid to
get licenses for something called UMTS
and UMTS was essentially an epsilon
variation on GSM so there was a lot of
money for not much technological
innovation
and and you of course you know the story
essentially these guys win this didn't
really happen because it was the end of
the internet bubble but anyway when we
looked at the problem we said well we
proposed a long-term research program
we're not going to work on UMTS because
that's totally boring
besides being too expensive for what it
is but we're going to think if we could
change what is the current
communications infrastructure which is
backbones based on on fiber-optic
networks and last mile sort of wireless
access that might be you know 10 miles
but it's essentially the last hope is
Wireless and then you have a classical
wired communications infrastructure and
so we proposed to actually study all
variations of what's known as adopt
networking where essentially you say
well we have a gazillion people running
around with Android phones right
prepared to talk so if you have a
million people you know in the zurich
region you probably can call from from
one of anga through to ETH by doing
nearest neighbor communication multi-hop
through the destination right I also
remember taking this slide and going to
Swisscom to try to sign them up for the
industrial liaison program and they
really didn't think it was funny to have
a wireless service without an operator
okay as you can guess so in the meantime
they understood they actually have to
understand what is going on here because
these things are actually happening
maybe noting their total you know
generic form because it's not so easy
for information theoretic reasons and
delay constraints and so on to do for
example multi-hop for actual real-time
services but certainly in sensor
networks this is a primary way to
actually run large-scale sensor networks
is to use multi hop communication from
all the sensors through intermediate
stations to a base stations where you
collect the data now come back to this
at the end when when I show actual
implementation but from a theoretical
point of view it's extremely interesting
to try to understand what is a vanilla
pure adduct network what are its
capacity is what could you achieve how
would you do source coding in such a
network and so on these are the sort of
questions I try to address today oh
now for you know I'm a signal processing
guy by training and by interest and so
sensor networks is a fantastic field it
was unknown by signal processing people
a few years ago but see it's the
ultimate sampling device of the real
world okay you put you put these
terminals a hundred a thousand whatever
cut into the world and you get of course
gazillion you know huge amounts of data
which correspond to physical
measurements of physical quantities and
then you know the fun is to try to make
sense out of this to communicate and so
on and the basic tenets is that
computation is actually relatively cheap
even on very small low-power nodes
what's the killer is communications this
is all wireless communication and these
these devices are very low-power and
therefore you really have to be quite
smart at doing your signal processing
your compression and then your
communication so as not to completely
deplete the power of these devices okay
now maybe one thing I want to point out
and I presume Google being a you know
the archetypal computer science
companies in the center of your concerns
but it actually does show up in
applications that are being done on the
internet just as well so the old view of
communications is that you have a source
a mobile phone you have a channel a
wireless Channel and you have a receiver
and also a mobile phone and you want to
go from A to B through this channel and
essentially everything is known about
this up to some epsilon details that
some people are still spending time to
figure out and you know this is
essentially known since 1948 from a
construct from a mathematical point of
view and then the rest was you know I
could say implementation details but
that was sort of annoying my friends in
information theory you know if you look
at it from a very abstract point of view
you know Shannon essentially says it can
be done and here is a construction how
to do it and and then of course great
workmen went into it to actually make it
happen like mobile phone telephone but
the current view is very different it's
one where you have many many sources
distributed sources
you have a distributed channel and you
have distributed receivers so that's a
so-called multi-user information theory
problem and this one most questions are
actually open and many of the questions
are in my view extremely hard to solve
and so the contrast between these two
views of the world is very very stark
and of course the view we have is this
one and we try to contribute to this
field by the methods that I'll explain
ok now as luciano mentioned you know I
spend a lot of time in the US and when I
used to go over there especially in the
former eight years of presidency which I
would remain unnamed you know my
American friends would always say you
know we work on Homeland Security so
what are you doing for Homeland Security
right so after a while I had to figure
out what was homeland security which is
not my cup of tea I'll be very honest
and and then I decided I needed a slide
to explain what was homeland security in
Switzerland I mean the US I after you
know many many visits I finally
understood so so I prepared this slide
which is the Swiss version of Homeland
Security for American users and it's the
following it's as well we are in a
mountainous countries - country has lots
of snow in the winter when you have too
much snow you get avalanches it helps
people this is really a security problem
so how do you solve the problem well at
the beginning of the winter season when
the snow has fallen you put some sensors
you drop sensors by helicopter over the
snow field and these sensors should sit
there for essentially six months they
should do triangulation to know the
relative position and whenever the snow
field starts to move they should wake up
very quickly alert the people in the
valley so they can wake up and run away
ok so that's a real challenging homeland
security problem now this is actually
not easy
it essentially embodies all the
questions of sensor networking but you
can also see that even though it's easy
to state and it sort of looks like it
can be known this is very very
challenging to actually do
and part of the team in the center is
actually working on this and they have
actually a great deployment I mean
that's not the actual experiment yet but
in the Swiss Alps in the valley you have
a small valley that is just devoted to
studying Avalon she's right this is and
so they wait for a long time and then
they actually release abolishes and they
are going to do a deployment inside an
Avalon where they can sort of understand
the dynamics by having little balls that
come down with the Avalon when it
actually okay so that's the problem
maybe we'll solve at some point but to
get there we need to have some you know
Prelude in your investigation which is
really what this talk is about all right
so as I said a sensor network is a
sampling device a spacial temporal
sampling device spatially you have
sensors at various locations here and
temporarily you take a number of samples
per second or per minute and what these
sensor networks are doing they are
trying to measure physical phenomena
physical phenomena are driven by for
example partial differential equations
are not some random you know sequences
of numbers or you know related to the
physics of the phenomenon and the
sampling of course is very irregular
because these guys sit wherever it is it
sit and you have all the questions on
you know if you want to reconstruct
let's say it's a temperature field over
this no mass you know how many sensors
should you put how many times per unit
per per second should you measure the
temperature etc and then you get this
amount of data and then you have to ask
yourself how do you do is a compression
problem and then how you lose a
communication problem and then
ultimately what you do with the data and
what understanding can you extract from
there I think then it gives the data
googles they will figure it out right so
okay good and so in in work over the
years we have looked at two physical
phenomena one is light fields and the
other one in sound fields because they
are close to what we we understand well
in terms of signal processing
Luciano has worked both on light fields
and out fields when he was back at
at EPFL and the light field problem is
the following is that you have a
multiple camera system and you want to
know or you have a moving camera and you
want to know how many cameras or how
many views you need to acquire to be
able to reconstruct in a faithful image
of reality right now in terms of
Google's this is like the Street View
problem right so let me just explain so
the plenoptic function as it is called
is this high dimensional thing which is
for example in this room is a camera
with three-dimensional position and
three two-dimensional shooting angle you
have a time index and the other
wavelengths index I counted right and
that would be you know if you would move
the camera everywhere in this room and
you would look in every direction okay
and then you would ask the question okay
that's an infinite number of views how
many views do I really need if later I
want to synthesize any view at some
level of accuracy the Street View
problem is that you know you drive
through the street so you want to
reconstruct the thing is faithfully it
if you want the question is you know how
many times you have to go at what
density you have to sample okay now
there are results on this and I'm not
going to reduce the results on this
particular problem except to say that
there are many instances of this but you
know so people build for example camera
Ras which is two-dimensional version of
the synoptic function plus you know the
angles depending on the optics but these
things are actually being constructed
and are used for for example image
synthesis based on what's called light
field interpolation now we sort of asked
a similar question for sound and we said
okay so if you have an opera hole and
you are recording a concert but then you
would like to render as a concert in any
position you know in the expensive seat
in the cheap seat and so on how many
microphones should you put in that
concert so it's exactly the same
question okay and and you can see that
people are actually serious about
building very large sized microphone
arrays of course these are useful
because you can do beam for
and noise cancellation and so on but in
this part of your case you know it says
okay MIT you know has put the thousand
microphones you know how many
microphones would you actually need to
solve the problem right so and so we
actually build systems like this and
fellow did his thesis de parler Co
supervised with Channel the result is
sort of interesting because it's you
know a sampling theorem is the
ultimately result which says okay if you
want to faithfully record spatial sound
here is a number of microphones you
actually have to put into the row okay
so you know like a sampling theorem a la
Shannon Nyquist for you know you want to
acquire speech or a telephone
conversation well you need to take 8000
samples per second this gives you a
spatial temporal sampling to Europe okay
now this tends to become a little bit
technical so I might actually loss over
it but I think I made my point that
there is an interesting question how
many microphones you should have and how
many of these signals would you acquire
at what resolution to have a face full
reconstruction and the answer lies in to
analyzing something which is called we
call it so plain acoustic function which
is you know the equivalent of the
plenoptic function and it comes down to
analyzing room impulse responses over
space so the room impulse response is
what you hear if I click my my my
fingers here then you'll have an echo
from the room which sort of
characterizes the room and if I move you
know the echo pattern will actually
change and the question is how much does
it change once you see this and you do
what signal processing people do they
say this looks messy let's go to Fourier
domain right and if you take the Fourier
transform of this there is a very cute
result which probably is known by
physicists but certainly not by CS or or
single processing people which is that
this the Fourier transform of this very
very messy function is actually very
very simple form it's actually bound to
this bow tie shape this butterfly shape
this is spatial frequencies
temporal frequency and so if you decide
to acquire sound up to 10 kilohertz this
will put the limit here and then this
will give you a sampling theorem because
the repetition of this spectra in
Fourier domain will overlap you know
starting at some frequency here but up
to this frequency Omega nought let's say
10 kilohertz you can perfectly
reconstruct a spatial temporal sound and
okay if you measure this thing in an
actual room this is a MATLAB simulation
this is actually measured in your own
you have a lot of noise but essentially
you see these butterfly spectrum and
once you have the butterfly spectrum you
can go out and solve many practical
problems based on this purely physical
insight okay which is you know linked to
the wave equation which is a PDE of
course that describes sound propagation
okay so let me skip the theorem here but
simply to say what's interesting is that
once you have this this tool the
sampling theorem and just like in every
other area where signal processing used
you know once you have the sampling
theorem you can go out and make many
statements about what you can acquire
from the real world
- what precision and what sort of
processing you can do on it okay this is
actually a very active area in the lab
right now and its application for
example for echo cancellation in
teleconferencing okay I won't give the
details here but this is a sort of
application okay good I promised to talk
a bit about distributed source coding
then he maybe explains a problem first
okay let me skip history of the topic
but it's a very very simple question you
know source coding which I showed on the
slide where I mentioned Chanin source
coding is you have a single source
speech on a mobile phone you want to
compress it down
- I know 96 9.6 kilobits per second to
transmit over a single channel okay for
a receiver now in all these distributed
cases the basic setup is different
because you have several sources like
say 2
and the sources are distributed so they
cannot talk to each other X&amp;amp;Y or at some
separate location and it will be too
expensive for them to sort of talk to
each other and you want to send what you
see at location X or source X and what
you have at source why you want to send
this to the receiver okay and then the
question is okay let's say you have two
telephone conversation you need two
channels with 9.6 kilo bits per second
or can you do better okay now it's sort
of fairly obvious that if these things
are completely independent of each other
you're not going to gain anything okay
so if you have you know two people
speaking different languages and holding
different conversations unclear that you
could gain anything but many of the
things we are looking at in particular
insist sensor networking the two
terminals look at data as it actually
correlated
okay then the question is can you do
better than simply doing what Shannon
said for the point-to-point is
compressed X compressed Y and sensor
okay and the astonishing result is
actually an old result due to step in
wolf which says that essentially there
is a so called rate region which says
how many bits per second you have to
give to this guy and to that guy and the
rate region is is lower bounded by maybe
graphically it's nicer to see clearly
you need the sum of the rates to be
equal to be larger or equal to the joint
entropy because that's what you would
get if you were jointly code them okay
so if x and y we're present at a single
terminal then Shannon tells you that you
would actually need at least H of x and
y the entropy just joint entropy of x
and y but what's interesting is that you
have two other lower bounds here and
what happens is that the lower bounds
will tell you that you cannot do better
than these guys but actually there is a
region where you can do actually quite a
bit better than what separate coding
will tell you which essentially the
intuition is that you can code X as if
you knew why even though you don't
as is sort of typical counterintuitive
information theoretic results because I
just told you you couldn't talk to each
other right of course if I gave you Y at
X you put code and H of X given Y but
actually I'm not giving you Y and you
can still do it
okay it's however quite complicated to
do it requires some fancy arguments in
certain cases it can be done and I'm
going to show later an example where we
can do it for acoustic fields okay so
just to show that this is not just an
abstract information okay but these sort
of this obsesses information theorists
for the last 30 years because it has not
been expanded extended to the case when
this is lossy compression so what I
showed here is that involved is you have
lossless compression of X and 1 okay now
let me skip this which is a case which I
don't have time to discuss but what we
did is that we said okay so let's look
at the data we have which is acoustic
fields and in the acoustic fields case
we asked a question actually is a lossy
rate distortion distributed right rate
distortion function we said can we do
better compression of a bunch of sensors
that don't talk to each other but that
know a priori that what they are
listening to is driven by the wave
equation okay so it's a case where we
take the physics which drives the
structure of the signals you acquire and
then we derive an information theoretic
result I don't really have time to give
justice to this thing which is it's very
cute not very hard but it uses some
fancy sampling fancy if you if you take
the direct approach but it's essentially
says something very simple is that you
take your your set of microphones and
you do some sampling pattern which is
not a separable traditional sampling
matter but something that is called
winkin sampling and then by magic the
spatial temporal spectrum actually
happens to be a wide spectrum and then
essentially you can do
I deal compression in a distributed
fashion without the terminals talking to
each other and what this in the end says
is that for acoustic fields we actually
that's the work of Bob Kong's broke PhD
student of my territory and myself we
can actually solve the distributed right
distortion problem which is comes in to
be a very nasty problem in classic
information theory okay does it make
does it make a difference well this is a
rate distortion function which trades
off rate with quadratic error and here
yet essentially is a two curves of
either a centralized quarter where you
have everything at your disposal to do
compression or this neat trick I showed
just earlier and they hug each other so
that's good if you don't use this you
get quite a bit of loss right I mean you
could see that you know the rate would
be double or worse right which is not
which wouldn't be good news in 640 okay
remember we're talking about you know
sensors where the rate that is used is
the killer because it will use energy
for communication okay good
let me move to what I said is the
controversial part of the talk and to do
this I come back actually to a
historical perspective and you'll pardon
my insisting here on going back to
Shannon but it's always good to sort of
go back to the initial problem and see
how it was set up so the initial
problems that Shannon did 50 years ago
while idling at Bell Labs the corridors
I guess he will actually trying to solve
a very practical problem which was
simply
speech communication for AT&amp;amp;T right and
what did he do is something it's good to
step back into this because sometimes
our students forget why we do the things
we do them so the problem was remember
communication from A to B right you have
a person with a mobile phone speaking
and it has to go over a wireless channel
to another person that wants to listen
to this conversation so we have a source
which is analog right it's analog speech
its waveform
which has analog values over continuous
time and the receiver lives in analog
world we want to hear you know something
coming out of a lab speaker driving you
know waves into your ear so you can
listen so the end result is analog and
all of this goes over an analog channel
so Wireless Channel it's a physical
reality is analog and Shannon and that
shows the genius of the guy right says
well I am going to solve this in digital
world in a countable digital world
okay so first there is sampling which
I'm going to ignore here so because
that's the trivial part of the result
but Danny says we map this continuum of
amplitudes into a discrete set the
countable set through something called
source coding by using vectors and so on
it's something for an how it's done but
it goes from something that is
uncountable into a world where things
are countable okay and this is
comfortable world
I give it an index here which is a
digital index I give it to a channel
coder which will generate analog values
to go over the wireless channel using
some signaling functions okay so I take
this index and here of course I generate
you know some vectors in some sense
which live in the analog world as a
channel decoder I try to decode you know
which vector analog vector has been sent
and I had you know if I can decode it it
will give me the index of the vector
which I give to the source decoder which
will generate an analog signal now this
is amazing because it's completely
counterintuitive so you would solve the
initial problem by going to this
completely new domain remember this is
1948 there were no digital computers
there was no not even mathematical
theory for these discrete problems and
now the bad news that's a good news but
we should we should you know appreciate
that this is a miracle okay that you can
solve this and you will be very
astonishing that you could solve other
problems by using this magic trick okay
and so we spend a lot of time on on this
little toy problem with Michael Gaspard
was also a PhD student at EPFL and it's
now at UC Berkeley
and and we thought well is this going to
work in a more general case of
distributed sources okay let me skip
this is just a textbook example but we
looked at a very very simple toy problem
because I'm a great believer in toy
problems for understanding the basic
underpinnings of you know it's a bigger
question and the problem is the
following you have a single source s and
you observe it with the sensor network
so you have a bunch of sensors and
sensors which observes the source
not exactly otherwise it would be
trivial you would only need one but each
one observes a noisy version of the
source okay let's say you are in here
there is a single guy speaking okay and
yes you know 50 microphones and you know
there is background noise right and each
of the microphones here gets a version
the noisy version of the source sends it
to a base station and the base station
tries to recover s to estimate s okay
now if you look at this and you know in
if you know too much information theory
okay so sometimes it's risky to know too
much so you know you have studied very
hard and so on then you look at this you
say oh this is the famous distributed
correlated source coding problem because
I have n versions so M signals they are
highly correlated I'm going to use a
very very fancy correlative source
coding algorithm okay then you hand over
whatever you have done to a
communications engineer and if he knows
information theory says hmm
this is a so-called multi access channel
it's a multi terminal multiple input
single output voltage access channel I'm
going to use some very fancy multiple
input single output coding scheme using
LDPC code or something very fancy okay
great
and you can solve the problem but
implicitly you have used a digital
representation because when you have
done source coding you have mapped
analog values into bits okay
and the guy doing the channel coding has
taken the bits and puts them onto an
analog Channel
and so implicitly we have gone to this
discrete domains is countable domain
right which is optimal in the
point-to-point okay now if you don't
know any information theory so we
pretended we didn't know the information
theory right so so we looked at the
front we said there must be a simpler
version to solve the problem so let me
show you the simple version the simple
version is something called uncoded
transmission you say oh I have these
noisy observations when I put them on
the channel in analog fashion the
channel just takes a sun the sun will
actually average out the noise so the
variance will go down as 1 over n
variance of the noise will disappear
magically because the channel actually
helps us solve the problem ok of course
you know we might not want to do this in
reality but as a thought experiment this
is interesting
ok now if you do this and you go through
them now and you do the optimal decoder
here knowing you know that you have M
sources and you know the variance of of
the noise at the sources etc it's not
too hard to show that the quadratic
error as a function of the number of
observers will go down as 1 over m okay
and it's sort of straightforward to show
this now the bad news is that if you use
separation very fancy source coding and
channel coding then the noise will only
go down as 1 over over log n right and
as you know log is very very small very
very slow function and so we actually
have an exponential sub optimality by
using classical separation going to the
digital domain okay so it's very bad
news okay and what's even worse is that
that's a question that Michael Gaspard
likes to ask is that you know in the
world we know the currency are bits you
know when we try to when we transmit
something we sort of put everything into
bits and then we hand it over we put it
into packets or whatever and these
currencies has built a digital world
right I mean from mp3 to the internet
everything is based on a currency which
is the bit okay likes a dollar in the
financial system right and
so here bad news in this very little toy
example in this simple toy example what
we show is that by going to this bit
currency you have exponential sub
optimality okay you go from MSC goes
from 1 over m to 1 over log n and that's
a good question
but you know what it's unknown today
what would be the replacement for bits
right so we re at the same stage as
Shannon was in 1948 ok because before
Shannon everybody was just doing analog
things and you know messing up with you
know analog amplifiers and so on and
Shannon came he said no no you haven't
understood the problem you just have to
new to use bits right and then you can
solve the problem and we have built you
know the entire information
infrastructure based on this and here
I'm just bringing out showing a little
example that in the distributed case for
example in sensor networks this would
actually not be the right way to solve
the problem ok good as you can guess
when I go and you know tells this at
Bell Labs or at Nokia's are not very
amused right so because it's essentially
back to square one ok now I'm not saying
we're going to throw away our computers
you know that's not going to happen and
you will see sensor networks who
actually build and deploy of course use
bits to communicate because from a
systems engineering point of view it's
completely unknown how you would use the
result bikes of one I told you but from
an intellectual point of view I find it
quite intriguing ok good so I promised I
would show that these things actually
have applications and so I'm going to
talk about environmental monitoring and
C is the motivation for this was the
following so we were working on the
theory of sensor networks we have some
people doing some some little toy
implementations and so on and then we
said ok all our American friends work on
Homeland Security fine let's find you
know a problem here which is more
science oriented and so we talked to
environmental engineering people at EPFL
initially and these people essentially
go to the mountains with these devices
here and the piece like this is about
100 cows
dollars it's very heavy you need a
helicopter to bringing it up and so on
and it has a data logger so if there is
a problem you come three months later
and everything is lost
etc and so we said Oh to the guy mock
bug launch with my colleague in
environment you know we have very cheap
devices right ten dollar devices hundred
dollar devices well we were sort of over
selling the point by now they are
actually more like a thousand dollars
apiece but you know that's a hundred
thousand dollars that's still a bargain
because you could put a lot of Z's over
a large area have very very been
sampling of phenomena before you run up
the cost of this and the other thing we
promise to mark panel she said well it's
it's good because we are going to move
you to the 21st century okay here they
would do deployments they would not know
anything about what was happening until
they would go back months later and you
know essentially take out the data
logger go back to town and you know plug
it into their pieces this thing of
course will be online right and this the
cost factor and the fact that it would
be online definitely intrigues these
people in user community in
environmental monitoring quite a bit
okay so we expect that actually this
field of environmental monitoring will
be completely transformed by sensor
networks right I mean when we started
2005 I think nobody was doing this in an
told field now is actually a startup out
of EPFL and those are places that
essentially okay so there is a large
team working on this it's headed by
Guillermo but there was a postdoc in the
lab plus as I said people from
environmental engineering and we called
the project sensor scope for obvious
reasons it's like a telescope but you
know using sensing and we we built a
first deployment in this building here
where channel used to live
still there you have kept everything
that's you know the guy moved so anyway
like Einsteins room in the Patent Office
in Bern right yeah so first we need some
deployments inside the building which is
a you're welcome to is it's a beautiful
building for computer science and
communications I think it's a nicest
building on campus very lucky to be
there and so the first deployment was
was don't there was a summer by a summer
student you know these are always the
guys sort of pushing the envelope and
and picked up by graduate students and
that gave us some learning experience on
how to run actual sensor networks the
basis is is something called the tiny
node which is built by a start-up in
pack significantly pfl it's a sort of a
very low-power version of the Berkeley
mode which is a thing out of Berkeley
that many people use and it runs on top
of tiny OS which is the lightweight
operating system okay and then from
there we said well let's do the real
thing which is to actually go up into
the mountains and out all quite a bit
about this deployment back because
that's a fun one but before going there
you know we gradually did more and more
complex deployments larger scale and so
on we improved the hardware software's
and networking and now have the data
analysis okay good so the basis of
sensor scope is cheap yet powerful
weather stations all these weather
station measures nine environmental sets
of data with sampling rates of the order
of a minute typically and its autonomous
it has a solar panel and it has nearest
neighbor communication based on
unlicensed band to distances of a few
hundred meters depending a little bit
what environment you are and then one of
the stations has a GPRS link to go down
over over you know a regular wireless
network and these guys are you know
relatively cheap sturdy they have been
developed over you know a couple of
years so as to be very you know to
fulfill the task at hand for
environmental monitoring so we can put
them on glaciers on raw glaciers and so
on and they can be deployed very quickly
and are essentially immediately online
at one of the features I'll show one
particular deployment where we actually
used this feature okay then all this
stuff is downloaded through databases
and interface on the web on you know
public web page where you can go and
look real time monitoring you can
download the data and there is some
initial analysis that is done on the
interface some visualization and I think
the cute thing here for environmental
monitoring people is that it's actually
real times are completely unused to
actually have access to their data
immediately they are also unused to have
such large quantities of data I have to
say because now they're actually
spending months and months to actually
just deal with the data okay that's why
I said ultimately probably you know
people like Google will crunch this
stuff for them they don't have their
skill set
in-house to deal with you know megabytes
and megabytes of environmental data at
the time okay so one deployment was done
right on campus and then he also maybe
say what it is used for this project
buys environmental monitoring people was
about understanding the microclimate
over what they call the built
environment so a lot of studies are done
on micro climates or agricultural land
for example or in forests but you know a
campus like EPFL is sort of semi densely
built environment so it's like a city
not very dense city and they want to
understand what insulin
this has on the heat flux on the wind
and so on and so about 100 stations were
actually only pfl campus for nine months
developments have generated quite a bit
of data and according to these people
it's you know this is new sets of data
from which they can you know develop
refined models for what actually is
happening in this microclimate okay well
that's the obvious infrastructure once
you have a network set up you have this
base station so that's multi up
communication the base station talks to
a web server at EPFL it's also I should
mention it's also put two web servers
that Microsoft actually operates one of
the sponsors of this particular project
or if you don't quote I should be
careful okay so let me leave it at that
okay and let me maybe simply say the
obvious right so how does a multi-hop
network work it's actually a homegrown
networking protocol by offering the
highest networking guy on the project
it's what you do is that you maintain
hop tables to the sink but what you do
is you do randomize next hop choice or
this fellow will look for a neighbor
that has fewer hops and he has rights we
hope so he will use this one or that one
but it will use it in a randomized
fashion that's quite important because
these sensor networks this is not the
wired networks these guys these links
have high variation over time and so you
don't want to have to maintain for
example shorter strap tables because it
would change all the time and instead
what you do is you sort of say well I
have a selection of neighbors that are
closer to this base station and I'm
going to randomly or maybe in a weighted
random fashion select one of the
neighbors that also distributes a load
which is very important for energy
consumption okay and for this of course
you need to have
a Hopf table which you maintain and you
also need timestamps because you take
samples every minute and later on you
want to do interpolation over space and
time these guys are very cheap and they
have known they don't have very precise
clock so what you do is you essentially
have a synchronization protocol so
people maintain relatively accurate
clocks with respect to you know this guy
has an accurate clock because it's on a
GPS okay and then you do a lot of study
on power usage and you know I know
Google is very interested in power usage
but for different type of IT
infrastructure but it's interesting here
you have very similar questions you know
different tasks take completely
different amounts of energy and so you
want to optimize how you actually use
the infrastructure so as to minimize
power consumption and this leads
actually to an interesting work that is
ongoing which is if you look at this
infrastructure at first what you do is
that you optimize the power usage on one
weather station right you say okay I'm
going to turn it on every five minutes
and then I take you know five seconds to
rendezvous with my neighbors blah blah
blah right and then you discover that
well that's good but you know this guy
actually doesn't have to work very hard
because this guy has to work very hard
he is a relay for many neighbors right
so this guy is actually going to go down
first because he does much more
communication easing the critical path
right and then you look at the problem
and you say hmm what you really want to
do is global energy optimization okay
you want to have the handover protocol
because you might have more than one
sink maybe several things because this
guy of course also consumes a lot of
power right so you might have a network
with let's say 100 sensor stations and
out of the hundred maybe 10 has a GPRS
enabled link and then what you want to
do is sort of randomize protocol that
hands over it's a task of doing the data
gathering and the GPS link randomly to
one of the ten stations right but of
course each time you change the base
station you have to change this entire
routing table etc so you
want to do this too often right because
and you spend your time sort of handing
over right and that's well known in you
know in all mobile communications a
handover process when you go from one
base station to to another one is sort
of a tricky one takes resources you know
sometimes it fails and so on okay so
without getting into details it's a
beautiful problem which says how do you
actually globally manage power in a
network like this okay and once you see
the deployments you will you'll see that
actually you have to take the
environmental data into account okay so
the deployment is up here and you know
one station might be back there is
actually south so this station doesn't
see much Sun right so you don't want to
use that one as your base station very
often because it doesn't get much energy
right so you have to take statistics
actually on how much energy you gather
in the network with your different solar
panel okay it's a very interesting
global sort of energy optimization
problem versus communication
infrastructure anyway now what we then
did from the deployment we had on campus
we said well let's move from theory to
practice and theory looks like this
there is some nice deployment on the
green field and the practice is this
rather dramatic location somewhere high
up in the Alps if you want to go and
change a battery it's a four-hour hike
okay
so if the thing goes down tough luck and
let me just say why we went there
let me skip the reason why we went to
this remote place and it's a true story
I'm not just doing this for marketing
despite what you might think is that
this high valley where we did the
deployment is the source of some mud
floors I come down and every now and
then take out the train line down in the
valley okay and so the people that are
interested in environmental hazards
actually were interested we would do a
monitoring up there for several months
to try to understand why in the world
these mud flows would actually come down
so the good news is that they were kind
enough to
we know pay for actually doing the
deployment for the helicopter rides and
so on because otherwise it's it's really
a tough place and it's a place as you
can see on Google Earth okay is up here
so much flow comes down this valley you
know it's very steep and so it comes
time very quickly and this is an
interesting microclimate because it's a
rock glacier okay but we'll see a
picture in a minute and here you'll see
a deployment I think there was something
like 17 stations all these stations
appear have no connection to the
wireless network because it's so remote
or it's just one at the very end that
actually is in line-of-sight I think of
a base Tower of switch con and so that's
the one that actually has some GPR
Eisley okay and this is you know sort of
if you like mountains this is a great
great place to go and work on i.t okay
so that that was you know quite an
experience to do a deployment up there
but it's a very remote and along this
project we did something actually fun
which was at some point somebody said
okay it's great to measure you know wind
and temperature and humidity and so on
but actually we we don't know if there
is fog or if it's snowing right and you
can sort of try to figure out but it's
finally know we actually
image-processing people so finally we
said well maybe we should put the camera
out there right and so that was before
Android 8 right so we had to develop an
autonomous cameras that would take an
image every 15 minutes or so with a GPRS
link would sound send images down and
this camera was sitting up there for
almost two years right and would wake up
every now and then even after we have
taken down the sensor network and you
should you you know could see there's no
conditions in or was it a nice day was
it foggy and so on
it had someone very simple onboard image
processing but actually now we are
working exactly on this problem which is
very low-power distributed image
processing to have on each sensor
weather station actually a little
because it's just so useful to have
visual feedback in such deployments okay
so what did we learn from this let me
maybe this one so then you have a
problem this is just showing temperature
evolution over time overseas rock
glacier and what you discover of course
you can also discover it by drilling
five meters down the rock is that there
is a glacier underneath a rock which you
don't you don't see and this glacier
actually is an acronym you know
accumulates cold temperature but it also
accumulates rain when it rains up there
and I think it creates sort of like a
lake which then discharges at some point
and which is the origin of this not for
the analysis of the data is still
ongoing so I don't have definite results
and I wouldn't really understand it
anyway
I must admit okay let me finish with
just a little example of a very quick
deployment so you know these various
projects you know sort of led to a lot
of contacts and then the people running
the path we dig last year which is a
famous ski ski alpinism race in the
Swiss Alps which goes from there map to
Verbier if you're into you know high
mountain skiing it's you know it's a
classic race to go it's it's quite
challenging and they asked us to
actually monitor the race at some this
you know at some specific points for
example dead blown which is close to
Zermatt which is at 3700 meters it can
get extremely cold so they actually used
this data I mean there was visual
feedback and there was temperature wind
etc and they were actually using this
for security reasons because either
weather conditions are very bad it's not
a good idea to let a few hundred people
go over there and that's an example
where you know you can deploy a
monitoring network very very quickly
make it run for a week and then take it
down again
very low-cost sort of monitoring of the
real environment okay let me skip this
the
we skip the conclusion which is sort of
vanilla and finish by you know show you
a New Yorker cartoon which talks about
Google Earth right you probably have
seen this one right so you're at the
barber and oh you want to see the top on
Google Earth now
my prediction is that we are going
actually to see real time sensor network
monitoring on Google Earth you know you
know very soon that the expectations
that you have satellite the ground truth
monitoring etc online on a service like
Google Earth I think is really where we
are heading out very very soon okay
thank you very much any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>