<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2017 Google Networking Research Summit Keynote Talks | Coder Coacher - Coaching Coders</title><meta content="2017 Google Networking Research Summit Keynote Talks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2017 Google Networking Research Summit Keynote Talks</b></h2><h5 class="post__date">2017-04-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5N7QS5vP68o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I don't think I need to introduce I mean
to this audience
so I mean why that is fellow at Google
and also our area technical lead for
networking which means that he is
responsible for the overall technical
direction for networking and he works
with lots of people in this room as far
as technical leadership around around
networking so we're really excited to
have him and began to start us off for
today thank you okay so welcome
everybody it's a real pleasure to be
here first I want to make sure that we
take a moment to thank Naggar she
organized this entire event for us all
and for possible this is about 3x or
maybe 4x the typical event that Google
organizes which I and I think blame or
credit depending on how you want to look
at it for this I really wanted to push
for getting as many people here as
possible
as those of you or Google know I've been
looking forward to this day for a long
long time my greatest pleasure walking
around the halls at Google is looking at
the density of great people that we have
here at Google I think we built a great
team here in industry but I'm also
really thrilled because looking around
here it's tough to think that there's
been much better density of amazing
academic folks anywhere gathered so this
has been something that I've been
excited about I took it upon myself to
try to put together a new talk for you
all so you'll have to bear with me this
is maybe very different from any talks
that I've given before in that I'm not
going to be describing a system that
we've built or some novel awesome
wonderful things that we've innovated on
I'm going to try to talk about how we
think about problems and some of the
challenges that I wish we could solve my
overall goal as a nice was also
mentioning is to get people talking I
think that we've already gotten off to
great start over the past hour plus so
you know please continue to exchange
ideas again I think that you'd be
hard-pressed to find a better
gathering of the best minds in
networking so I hope that some great
connections get made among the academics
in the room and of course back and forth
with folks at Google okay so with that
let me give you my perspective on
networking research at Google I think
where we really benefit is that we're
driven by novel application requirements
and really massive scale so we do
research like everybody else does we're
presented with problems that we don't
know how to solve and these problems
come from our application developers
running very exciting new services on
the other hand of course it works both
ways we think about what we can do on
the networking to make the lives of our
application developers easier to allow
them to do things that they couldn't
otherwise do the impact of what we can
do can be huge we again serve some
substantial fraction of all Internet
traffic and that means that if we can do
something new novel it can have a huge
impact that's the positive the may be
positive maybe less positive
I think it's overall a huge positive is
there is no tech transfer I think that
all the teams that the needs alluded to
we are the product thing if we come up
with a good idea we only have to
convince ourselves to do it but that
also means that if it breaks
we're the ones you have to fix it so we
are the product team there is nobody to
transfer things over to but what this
means is that we can sometimes
anticipate shifts in networking research
and what I've been pushing for over the
past few years is also make sure that we
can give back and publish more and more
of what we do so just to put this in a
little bit of perspective Google has a
long history of really foundational work
in distributed computing starting would
say GFS over to MapReduce BigTable etc
etc leading for tensorflow and spanner
more recently many of you sort of duly
work in the systems and the networking
community so many of these efforts are
probably familiar to you from various
SOS PEO SDI and other papers but again
the thing that hasn't been is clear
is that this distributed computing
infrastructure from GFS to tensorflow
required a networking infrastructure
that you couldn't buy and that's where
we have the privilege of working on
those systems that we do so more or less
matching some of the innovations on the
distributed computing infrastructure
side have been a number of things that
we've gone after on the networking side
starting perhaps with the Google global
cache our content distribution network
to some of our data center networking
work watchtower freedom
more recently Jupiter or wide area
network like before most recently we've
done a lot of work in protocols quick
bbr G RPC etc and over the past three or
four years we've been increasingly
active this is in publishing so rather
than just doing the work internally
we've also been doing more and more
publication of our efforts this is a
subset of some of the papers that we
published over the past three or four
years the on the distributed computing
infrastructure side open source
historically has been about us
publishing papers and others picking up
the efforts and doing some very
important open source efforts around it
I don't have on this slide tensorflow
but I think tensorflow is a good example
of how is shifting things where we're
going open source from the very
beginning with some of our distribution
computing infrastructure and I think
you're seeing the exact same thing
happening on the networking side with
efforts like quick TCP b vr g RPC and
open config these are just four recent
examples of efforts where we're using
these actively at scale within Google
and we're open sourcing the exact same
code for use by the community and the
dot dot here indicates that we're
looking to continue this model push on
this model moving forward as well okay
so with that in terms of a little bit of
background of how we think about
networking research at Google let me
shift over to what I see as some of the
next decade challenges in networking so
we've been publishing a lot in
software-defined networking
and protocols congestion control
protocols etc but I actually do think
that now is an interesting time to be
thinking about what's next because I
personally actually see the light at the
end of the tunnel in terms of some of
the efforts that have been going on over
the past five or ten years or so that's
not to say that we're done we have a lot
of work to do internally there's a lot
of great research to be done externally
but I also see that is an interesting
time to be thinking about how things
going to be shifting in terms of
important topics over the next decade so
one of the I think maybe three general
areas and they wind up being pretty
closely related I'll talk about at least
two of them in some more detail
one is serverless compute in cloud 3.0
what I mean by serverless compute is
that I think we're going to be moving
away from specifying individual virtual
machines that are configured in
particular ways running fairly coarse
grained services to individual functions
that get run and scale out as they need
I'll talk about that more in a second
IOT is a buzz phrase right now the
Internet of Things what I do think this
means is that we're going to see a
substantial shift in security
requirements but also the number of
streams and the number of flows that we
have to manage across the Internet and
back to data centers as well as of
course the security and privacy concerns
that go along with it
and finally I think that we've been
seeing this over the past five years but
it's really starting to hit home and
hurt now the ability to get more
performance and even buy more
performance I mean more parallelism more
cores from individual servers is slowing
to maybe stopping and what this means is
that for us to be able to get more
performance out of our computing
infrastructure more efficiency and maybe
most importantly for us to be able to go
after bigger problems and that's really
what it comes down to it's not about
performance and efficiency so much as
being able to do problems of scale that
we couldn't do before we're going to
have to go after more tightly coupled
general-purpose distributed computing
infrastructure
and what I mean by this is that we have
to be able to bring 10 or the hundred a
thousand servers to bear communicating
in interactively synchronously at very
fine timescales moving toward
microseconds on problems and this is a
big shift in how we build applications
but we can't build applications like
this because of network doesn't get
supported I'll be talking about this one
a bit more as well so overall I see an
opportunity for us to put together a lot
of the themes of the networking
community has been working on for a
while one of these is what I'm here
calling agile scale I'll describe what
that means in a moment but it's not just
about scalability in the traditional
sense jitter has of course been a
long-standing issue in networking I'll
talk about how it hits us at multiple
points in the intent system to really
hurt what we can do isolation among
computing that is sharing the same
infrastructure and what all this says is
that if we can get a Jahlil if we can
manage jitter if we can deliver
isolation we can potentially deliver
transformative performance but it's only
meaningful if we have the basics and the
basics our availability manageability
and velocity I'm not going to talk about
manageability much because my colleague
DeCoste Cole is going to have an entire
great talk on that topic but that's not
to say that it's not a super important
it is ok so let me set the stage a bit
with some of the things I'm seeing on
the compute side and some of the things
that we're going after already at Google
so last decade maybe 10 years ago
starting about 10 years ago we were in
the era of cloud 1.0 here we saw that
virtualization could deliver savings and
capital expenditures to enterprise data
centers so that's a mouthful what does
that mean that means that I could take
my existing hardware that I had and run
more workload on it because I could have
5 10 virtual machines configured exactly
the same exactly the way that I needed
them to be configured running on one
piece of hardware so I could take my
data center
and really run it much more efficiently
and this was a huge one I think where we
are right now is cloud 2.0 and what
we're seeing is a move to hardware
on-demand I can get those exact same
benefits but I no longer have to run my
own data centers to do it
I can now expand or shrink my computing
infrastructure on-demand
without having to figure out how to
build out my computing infrastructure
the hardware underneath at the same time
and still having to configure all my
computing infrastructure I have to
figure out exactly how my virtual
machines are supposed to run in terms of
operating system in terms of network
policy and everything else I've moved
from buying Hardware middleboxes
to buying software middleboxes I still
have to configure my firewall rules just
like I did before in fact I prefer to
configure my heart'll firewall rules
exactly like I did before I want to be
able to take exactly what I did in my
enterprise and run it in the cloud but I
can now shrink up shrink down expand out
on demand and this is also a big win
however I think where we're going is
cloud 3.0 where we can shift away from
servers away from virtual machines away
from individual rules firewall rules for
individual servers load balancing among
individual virtual machines etc toward
computer so serverless compute says I
can focus on what I do best
which is my business logic let's say
whatever it is that my application needs
to be doing not on exactly where my data
is placed not on how I load balance
among virtual machines not on how I
configure the operating system on
individual machines how I attach them to
make sure that they're secure etc and so
from a researching perspective we should
be aiming for cloud 3.0 and one of the
things that I think hurts is when we
think more in terms of how do I take the
things that I've always done in my
enterprise data center and move it
wholesale into the cloud while that's
familiar and maybe easy to think about
in terms of optimization it actually
might be setting us back because in the
end this is
how computing should be working right no
one wants to be thinking in terms of my
thousand virtual machines becoming 2,000
virtual machines and how I configure
them but that I think in terms of being
able to really focus on the next
generation is maybe backward outs so
what are some of the specific items
I think storage disaggregation is going
to be really interesting if we're going
to be focusing on compute where the data
center is the storage appliance I'm not
thinking about where my data happens to
be living transparent live migration we
found is very important and what this
means is that I can move compute around
anywhere within the data center and
maybe even between data centers without
having to expose that out to the
customer so while this isn't necessarily
an end customer benefit from the
perspective of delivering availability
manageability etc being able to move
things around with no impact essentially
what that means is that the customers
and by the way the customers are often
paying very close attention don't notice
as you're moving them around seamless
telemetry I want to be able to have
exposure to everything that's happening
into in my infrastructure not
necessarily to expose to end customers
but to be able to scale up and down my
compute if I'm going to get to a service
model and not be thinking about
individual virtual machines I need to be
able to scale up and down my computation
in response to load but also while
maintaining some higher level metrics
service level objectives in terms of
latency in terms of throughput and then
finally this one is deceptively simple
perhaps but I think has some really
interesting implications for networking
and that's an open marketplace think of
this as an app store or virtual
computing I want to be able to buy
services maybe it's a filer maybe it's
security appliance and be able to plug
it into my network without knowing
exactly how I'm plugging it into my
network in terms of IP address
configuration etc so how do I expose
third-party services into my computing
infrastructure in a secure way while
allowing them to have the privileges
they need to add value for me
okay so what this means is we need to
focus on applications maybe even
functions not virtual machines we want
to specify what the policy is here's my
service level objective in terms of
latency not what the middle boxes are
whether you run how packets are routed
through them etc again so also SLO s not
placements not load balancing not
scheduling that's what the network the
system underneath you should be
delivering and finally from a network
perspective I want to have actionable
intelligence and what this means is that
I want to be able to have queries that
are running live over my data without me
having to go ask questions all the time
so not thinking explicitly about data
processing so what are some of the
research and development challenges that
go along with realizing some of this
I'll put them in order
availability manageability velocity
isolation and performance but I think
it's important to also say how we think
about things in terms of going after
resource challenges development
challenges at Google so many of us
absolutely love performance that's
that's the thing that we'd love to work
on more than anything else it's the
thing we can measure it's the thing that
we can brag about or 2.4 X faster than
the last thing all that actually is the
last thing that we work on at least when
we're going after things at Google and
what I want to stress is that there are
tremendous research challenges that lay
the found that foundation for going
after performance I won't lie I love
performance as much or more than all of
you but it's the thing that you go after
last once you have the basics in place
and it's the basics that allow you to go
after the performance so number one is
availability and we're seeing just
tremendous requirements we've always had
them in terms of developed delivering
the highest levels of availability
five-nines 99.999% one what is
really mean we struggle with that
internally it's easy to say no more than
five minutes of downtime a year
what is downtime mean there's this
failure in our network all the time how
2% of the time there's failure but just
defining what that means for us it's
something that Picasso and I talk about
multiple times a week what what is
availability actually mean and what does
it mean for us to deliver delivering
99.999% of a lability okay so that's the
basics and for us it's gotten even more
important because internal Google
services can be written to tolerate
failures a bit better than cloud hosted
services so as we've made the shift to
cloud it's become even more critical for
us because we can't tell all the
engineers out there
oh here's how you manage an entire data
center going away my internal customers
search they know how to manage an entire
data center going away without anybody
externally noticing much harder to do
for external customers okay so
manageability we've made it available
how do we make sure that it's manageable
we have one of the world's largest
infrastructures it's changing and
growing all the time we can't have
hundreds of thousands of people millions
of people if we stay on certain tracks
trying to manage the infrastructure just
doesn't work for many many reasons
so that's what--that's requirement
number two velocity is an interesting
one and I think it starts really getting
at the hardest systems how do we make
sure that we can upgrade our
infrastructure very very quickly how do
we make sure that we can roll out it
change across the fleet in a week every
week and maintain availability so those
two things are in sort of huge tension
and figuring up system structures for
managing that is probably some of the
biggest intellectual contributions that
we've collectively made at Google
internally over the years and this is in
service by the way of performance in the
end you're going to be constantly
optimizing your workloads going to be
constantly shifting you're going to be
coming up with better ideas you want
your ideas to have impact you want to
roll it out across the fleet how do you
do so safely every week so velocity is
just absolutely key
stranding this will come into tension
with another requirement that I'm going
to mention which is isolation but we
want to make sure that for our
infrastructure that we again have some
of the largest in the world we're not
losing a bunch sitting around idle okay
because frankly that's the biggest
performance optimization we can get is
if we have idle cycles that aren't doing
anything well we can do a lot better and
going from zero to one then we can go
from one to one point one or one point
two so at the bottom of the pyramid and
then comes performance so what I want to
encourage maybe all of you and certainly
and by when I say all of you I mean
Google and academia what ideas can we go
after to enable the basics availability
manageability velocity stranding so we
can all go after performance even more
ok so hopefully I've made this point
availability is just absolutely
paramount for me this includes security
and insecure infrastructure is an
unavailable infrastructure I'll say
maybe the obvious Google's
infrastructure is under attack just like
something's unavailable all the time
it's under attack all the time just 20
24/7 the magnitude of the attack changes
and there been some that have boy deaths
as well but security I'll take as
primary stability is more important than
efficiency and I get to my point about
velocity so if you can i'll trade an
inefficient system for a stable system
already mention the point about network
management the cost we'll be talking
about that more configuration is hard so
here
it's surprisingly hard to impossible for
me to ask the question what does
Google's Network look like right now
I don't think anybody knows we have
pretty good ideas of the truth but we
don't really know more surprising what
did we mean for the network to look like
right now we might be able to get the
answer to that but not not as easily as
we might like so managing config is
super hard
and I think this room for improvement so
I talked about manageability one of the
ways to make sure that our
infrastructure is manageable is to
aggressively automate but this can be
countered to availability so one of the
things that we're seeing is that
automation can run amuck and pulling
human judgment a loop can be dangerous
we covered some of these topics in a
recent second paper so I'll refer you to
that if you're interested and there's a
really great book that was recently
published out of Google on site
reliability engineering but I also
encourage you to look at if you're
interested so build for velocity I was
inspired by just 10 12 years old now
made paper tests on cyberspace defining
tomorrow's Internet at PSICOM what I
took away from this paper was that the
internet was undergoing ossification
because we couldn't change it and this
is really my point about velocity how do
you build systems so you can change it
and the internet was of course was stuck
in this place where the only way you
could change it was to get multilateral
agreement among thousands of
participants
I think we've broken out of that tussle
but I think it has a great lesson for
how we build systems for velocity from
day one build it for upgradability what
we say oftentimes here at Google is
launch and iterate what's the minimum
viable product you can get out there
that's better than the last thing and
iterate from there okay I talked about
stranding but I think that the other
very related point is and these are
interesting counters to one another we
think that isolation is critical but
stranding is terrible so this goes back
to the old debate about work conserving
versus non work conserving we multiplex
many many different applications on the
same physical infrastructure and all
these applications want the illusion of
running on their own hardware
our biggest application developers
really want the illusion of running on
their own hardware and sometimes they
even get it
but in general we can't afford it and
yet we think stranding is terrible so
how do we balance this tension so
purpose-built infrastructure works but
we can't do it very often the thing I
also want to point out about isolation
is that this is really what congestion
control is about it's also applies to
the control plane tremendously so some
of the interesting things that we see is
especially in the cloud world if you
want to or in a server this compute
world what if you want to spin up
thousands of instances of a function you
want to make sure that you spinning up
your thousand instances of a function
doesn't impact someone else spinning up
a million instances of the function and
so it's control plane it's data plane
it's everything in between ok so I'm
running short on time we will have some
time for questions let me try to make
one last point before handing things
over to the cars so that we stay on
targets and maybe this is a somewhat new
thing that we're saying here is probably
from a typical grizzled Google engineers
perspective nothing bothers them about
reading a research paper more than scale
benchmarks that go to 5 or 10 or 100
endpoints so scale is interesting - you
can scale linearly to 5 10 or 100 that's
very good but what we're seeing here is
that real distributed applications
communicate with thousands of endpoints
and real servers host hundreds of
applications today and if we're going to
go to a service world they're going to
host a hundred X lab in the next few
years most of them won't be active right
but what this means is that we can have
tens of thousands of endpoints per
server maybe hundreds of thousands of
endpoints per server all of whom need to
potentially be able to go from zero to
100 gigabits per second in zero time all
right so this is what I mean by agile
scale how do I deal how do I manage with
low latency and efficiency the ability
to manage tens of thousands let's say
hundreds of thousands of endpoints
and have any small subset of them use
all the resources of the server
essentially on zero notice so it's
relatively easy to get great performance
when I have a hundred endpoints all
talking to 100 endpoints for many many
minutes or hours much harder if that
subset of 10 or 100 is changing at very
fine time skills okay and I think I if
anyone is interested I will tell them a
little bit about some of the
contributors computing programming
models that we're seeing and what I
Google would like to shift to moving
forward but for now I'm going to stop
with the conclusions and hand things
over to because I think that now is a
great time to be thinking about the next
shift in networking research and I think
this is really driven by more tightly
coupled compute and a server list
compute model moving away from
individual servers and individual VMs
and I think that to really go after
these things we have a great opportunity
to put together a lot of the threads and
ideas that we've been pursuing over well
multiple decades so I'll stop and did
you want to take questions now nice or
at the end ok so that all hands over to
the custom thanks very much I mean so
just before I'm bringing up because I
neglected to thank negara for her heroic
effort so thank you I mean for doing
that all of you who've worked with
negara communications food
transportation the program the website
everything she's kind of superwoman for
this program so thanks very much and
also just wanted to call out Jeff as
well so all of you know Jeff really well
I was just assistant to Jeff but Jeff
really was working hard to put together
the entire program the format a lot of
that stuff so really want to recognize
Jeff please thank him when you run into
him so next we want to have the next
keynote is by piccoli
who runs as I mentioned our network
architecture engineering and planning
and design teams
all part of network operations at Google
and he's also a distinguished engineer
at Google so please welcome the cache
thank you a nice jsnicka for putting
this together you know I haven't seen
this kind of crowd in a relatively small
room all flat all packed here you know
talk about the performance density in
this room and you know we try to get
better weather for you but we thought
that for this conference
nothing cloud is appropriate so we did
what we could but will mostly be indoors
today so it'll be great
you know I I wanted to start with you
know pointing out something that we'd
learned at Google you know building
networks for you know it's my tenth year
here for for the last ten years there is
a traditional tension between operators
and developers in the outside world
where you know operators do not like
change developers love feature and stuff
don't get delivered because operators
push back and therefore the person want
to launch anything and it just it's it's
a bad cycle right the first thing that
we learn to fix at Google is that you
cannot scale evolve or evil of features
if you actually maintain this developer
operator boundary and so one of the
things that you're noticing is that it's
a research conference but it's hosted by
me coming from the operations side of
the house and I mean coming from the
developer side of the house to offer
actually spend more time every week then
I probably spend with my team or he
spends with with his team it's actually
key to this talk I was going to point
out why this is important because first
is organization if you don't fix
organization you actually don't fix the
product that you build and then we tried
very hard to not have you know this
traditional separation between operator
and developer we both research we both
develop our our necks are equally on the
line when things break we both care
about performance and stability and
scaling at the same time right
so what what what actually wanted to do
is to share some of the lessons that we
learned some of the
some of the things that we have actually
tried to do with our infrastructure and
then really leave with you know seeing
if that triggers some parts that you
have on research ideas and then we
obviously can continue the conversation
as we move forward right
so roughly you're going to talk about
what it really takes to operate a very
large network I will share some actual
stats on how large this network is I
mean talked about I have lived it in a
networker and literally like this is one
of the biggest thing that I think about
all the time I'll share some of the you
know insights as to why high
availability is hard in an
infrastructure like this The Forgotten
plan the management plan one of the
things that we we also talked about and
you know we actually read papers about
all of us too and we we see awesome
wonderful papers in control plane and
data plane and and you know sorry
socialization we would love to see great
papers in management plan there is a lot
to be done there and we're actually
going to talk about why it's important
that we actually fix that plane and then
you know having thought about this or
quite some time here will share how
we're trying to solve
this massive problem that we have and of
course we are not done solving this at
all so really it is here to trigger your
interest in this you know what I call
the Forgotten plan and how we probably
go about solving it right so I don't
mean said for the last 15 years have
been building you know the the largest
cloud infrastructure in the world we've
been building cloud before cloud was a
thing or it was named cloud we have
always built this right and the global
network that we have built is sort of
the fundamental building block of how
cloud works because the the main thing
about cloud is ubiquity you basically
want the services to be available
everywhere in the world
and what makes that possible is network
the network itself doesn't look any
different from any of the other clubs so
whether I out get right we actually have
a very large data center network that
connects all of our servers we have an
inter data center network that connects
the data centers together and we have a
global backbone that allows the services
that run on this data centers to connect
to the internet and to the users right
how big it's actually very big
this is actually showing both our inter
data center backbone also known as
before and internet-facing backbone also
known as B to their overlay together
along with all the various you know
marine and terrestrial capacity that we
have our internet our internet facing
backbone or be to connect 70-plus
locations in 33 countries it's basically
global you have heard a lot about the
intra data center backbone before which
is fully at the end fully phone grown
and really built from the ground up to
really do one thing well to connect the
services between our data centers and
and really to to help them transfer a
large amount of data as well as
replications how big well that picture
is nice you have seen those pictures
many times with other talks so let me
actually share some actual stats our
network today has twenty thousand-plus
circuits
most of those are actually 100 gigs so
you can do the mental math of how big
the network is it's very big and it's
growing at a very rapid rapid pace our
network has many tens of network and
controls actually it's more like
hundreds of network element roles and
they're they're individual functions and
therefore it comes at their individual
configurations and things that happen
with that doesn't plus vendor including
ourselves we actually count ourselves as
one of the suppliers of the technology
it's it's a massively large supplier of
Technology in this network for million
lines of configuration files I'm growing
every single day 30,000 configuration
changes per month and these are massive
concentration changes eight million what
object identifiers that we collect every
five minutes all of that is growing at
an exponential scale every year so when
you look at this this large scale you
know there is one thing that is constant
and I mean mention that that is outage
happens all the time every single day
every single minute there is something
in the network that that breaks now not
all those outages actually lead to
service outage on the network we design
our network very carefully where we're
designed for failure where things break
but really we still try and offer the
same SLO
that or services actually expect a lot
of network uh that by the way is hard
and it's hard because in again I mean
mentioned how we try to be low
infrastructure where often the
requirement is five nine or better now
if you break five nine down into the way
that we let you like doing availability
doing you know whatever it really means
is by a term called outage budget that's
really what matters it's not that forage
availability number to use right so five
nine gives you twenty four seconds of
outage budget in a month so compare that
with what I just did it that every
minute something is breaking in the
infrastructure and you have twenty four
seconds of total outage budget for your
services or your network can be
unavailable to your service rate which
basically would mean that the network
would not meet the NLA that we actually
advertise your services like packet loss
let and see it's episode right it's very
very very tight budget and you know when
somebody loses at five nine you should
keep in keep in the back of your mind 24
seconds it's less than less than a
minute of total outage that you're
actually committing to so we actually
spend a lot of time in figuring out
what's the best way that we can offer
the availability that our applications
need and especially with cloud the need
for high availability is actually
growing for the reasons that I actually
mentioned in a previous talk and what we
did is you know we have this a tool that
we use internally called postmortem
the goal of this tool is to have
completely blameless analysis of why
something went bad you know
infrastructure and then find root cause
and then go after fixing the root cause
so it doesn't happen in the future right
so we have this very rich data set off
you know ten years sort of post-mortem
on things that have broken in the
network and what we did is we went about
classifying
what leads to unleash Elise outages some
of this actually covered in the in the
paper that I may mention the evolve anti
paper it can succumb but we actually had
a very interesting observation out of
this classification irrespective of the
underlying technology that we have in
the network and as I mentioned like we
have a data center network which is
freely Sdn
you know your heart of Jupiter the Intel
data center network which is fully Sdn
p4 and we have a
internet-facing backbone that's more
traditional IP MPLS backbone with some
amount of SD and on it so very different
technologies but when he went to the
classification of voltages in this in
these different pieces of the network we
found that they all have a common cause
of most prominent outage 70% of outages
approximately in all this infrastructure
lead from management operation to the
network now let me spend a little bit
more time on on that that's actually a
very important finding that very did
that we had which is not to say that we
don't have fiber card we don't have
router failures we don't have optics
that goes bad that all happens but all
of that constitutes only 30% of the
cases where the network actually had had
an outage the interesting observation
with the management plane outage is that
most of the time where we see outages
that happen out of a management plan
operation there they're mostly related
to changes that are not intended by the
management plan change that was actually
applied so let me give you an example
right I wanted to reboot an altar and
somebody applied a wild card but you're
putting the router like that that would
be a classic example of how it is
unintended change that that breaks the
network down in many cases which can
also be I applied a change to one part
of the network and I did not realize the
impact of that change to another part of
the network a classic example is BGP
policy on router you know I I might have
applied a BGP policy for one reason into
one Robert and I did not realize how the
nature of the traffic is going to change
in the network irregular and the main
reason for this is these networks are
too big to keep the state in the head of
a network or a software engineer no
matter how smart they are and you know
I'm very proud that we actually have
some of the smartest in the world that
does this and they make mistakes which
basically means that you know any
infrastructure that you run the operator
will make mistake because that is too
much state to keep in a human's head
right and that's why you know most of
these failures actually start from so as
someone who has to you know operate this
very large infrastructure you know the
question that we always ask ask
ourselves what are we trying to solve
right what is my elevator pitch and so
what we are trying to solve of course
the first one is you know we want to
build the base ship base
you know the highest-performing the most
reliable the most scalable network in
the world for google
like that is very forgiving but really
what is the problem statement that we're
trying to solve the problem statement
that we are trying to solve is that I
have these three things that I'm trying
to confront and the optimize for a
reliability scale and efficiency and
what you always find is that it is very
easy to optimize for 12:3 it is
extremely difficult to optimize for all
three so let me give you an example so I
mentioned that 70% of outages have an
inner network when we make a change to
the network right you know when our
network is most stable where nothing
breaks it's between Christmas and New
Year's Eve because we don't make any
changes in the network there are hardly
any outages that happen right so one way
of making our network reliable and
actually efficient and so now I'll tell
you how it's going to be efficient but
whatever making our network you level is
can never make a change we basically
have you know five days of network
refresh time every year and that's the
only time you get to make a change in
the network and the network is going to
be super super reliable it's also going
to be efficient you know how it's going
to be efficient it's going to be
efficient because well you were just
going to keep on over subscribing the
network because you're not making any
change right so you know your unit cost
of using the network well it's going to
going to be very nice it will not scale
right little absolutely not scale
because you're not touching the network
right flipside how can I make it scale
and make it reliable
but maybe efficiency is harder well you
can solve that by money I can make it
very reliable by building everything
twice and it'll actually scale just fine
because I'll actually throw the money of
the problem right it will not be
efficient right so so that sort of gives
you an idea of how it's very easy to
solve two of them but it's actually very
hard to solve all three right our goal
like you know as I think of my charter I
mean charter in a space charter and
others that are that let's be working in
here is to turn this triangle into a
straight line this is what I'm trying to
solve what I trying to solve is we want
to have a mechanism of running and
operating the network while reliability
efficiency and killing
actually coupled an idiot all three when
when when you when you make a change in
the network how how can we get there how
can I get there where we actually are
solving not not the two of them but the
three of them all at the same time so it
comes from the realization that you know
the way to solve the reliability problem
in the network is to make the management
operation safe now let's start from that
point if your management operation is
change then that basically means by
extension that I can touch the network
as often as I want which means that I
can evolve the network as often as I
want which means that I can introduce
new technologies to the network as often
as I want which is how you get
efficiencies in the network because
you're basically replacing an old
technology the new that is just better
more efficient cheaper bigger in size
right so the more often that you can do
that the more efficient your your
network is that also means that you can
scale the network faster right because
you know if you if you go to the example
of the data center technology that I
mean mentioned going from Saturn Jupiter
you know a few orders of magnitude
higher scaling so if we can do this
every week and every month that actually
means that you are scaling it much
faster where you're scaling is not brute
force or spending money right so that
sort of gives you some of the core
tenets of what you're trying to solve
that is why I mentioned why management
plan is so important but we don't talk
about it awesome right if you solve the
problem of making management reliable
you actually are very close to solving
this linear revolution problem right and
then the one thing that is constant in
this is evolution is constant in
Google's network because if we don't
evolve we actually don't launch these
services we don't support new customers
and we actually don't don't you know
reach the new users that we need to use
right so so ultimately evolution has to
happen right so we basically came to the
conclusion that we have to solve the
safe management link problem to get to
there and that fundamentally led to an
architecture very called
the zero touch network so what is this
architecture about right it all comes
from the insight but there is too much
state that operators need to maintain in
their head when they do traditional
network operations and that is the
primary call
reason for network breaking so the way
that we solve that is by reducing the
amount of states that any network
operator has to carry and the state is
managed by what is best at managing it
software not human so effectively it
starts from this following points but
first all little corporations are
completely automated where a network
operator requires new steps beyond
instantly instantiating the intent and
in turn in this case is very high-level
I want to create this MPLS tunnel from A
to B is way too detailed right my intent
would be I need connectivity from A to B
with this SLA it's a very high level
intent that we want individuals apply a
human is never required to apply changes
to network elements the changes that are
ultimately applied to net network
elements are actually computed by a
system based on this this very high
level intent that the network operator
creates so ultimately a human never
interacts with the network element
themselves right the software system
does third one super important right
when you look at you know we talked
about outage budget right that five nine
has an out-of-phase budget of twenty
five seconds there are several ways that
you can reduce outage budget but one
thing that we learned again over many
years the most important thing that you
do is already reduce your MTTR mean time
to repair which means that you reduce
your min time to detection and then you
go and fix the problem as quickly as you
can before it melts down right so the
first that we have here is that we
actually want to halt any network change
that even has indications of things
going bad and then you get the network
back to a state where it's safe and this
happens automatically you don't wait for
a human to apply judgment as to what the
state needs to be the system does that
for you so you basically have reduced
the outage time that you have consumed
our budget area consumed last one
equally important right the system
itself should stop a human from making
any changes to the network better not
applied that are not allowed by policy
what does that mean these are actually
very high level policies these are
these are things like you are not
allowed to take down more than X percent
of capacity in in Metro in a location
right or you are not allowed to reboot
more than this many devices in the
network right and realize that this
policy is almost impossible for a human
to fully enforce however the human
always use intuition where they know
that if I'm probably touching more than
five routers I should not be touching
right so this is bringing the human
intuition into the system but ensuring
the system itself is applying the policy
where the policy is described by the
network operators right so what what
does opera just really do in this system
the operators right the policies that
describes how the network will remain
safe the operator describes the intense
and this intense comes from what did
what they do on a daily basis with the
network not is the elements it's a big
difference in you know how we look at
this right it's not interacting with
elements and substance interacting with
the structure that we have and the
system provides a safeguard right where
the things will go wrong but the system
provides the safeguard so I'll very
briefly touch upon the architecture we
can discuss this more into our breaks
you know let's spend a whole polar time
on it but really let's walk through the
basic idea right so the basic idea is
that what an operator really interests
with is workflow engine an operator
never interacts with the elements that
are underneath arm the works of engines
are essentially finite state machines
where the operation rites of grass and
the grass basically says what I want the
network to do if if I have this intent
right the workflow engine interests with
the network systems through a set of
very well different criterias right and
we call them intent ApS then this
quantity is a very high level they are
actually not element specific they are
they're they're they describe the change
to the network structure right how do
they describe the change in the natural
structure they do that through a set of
network models that we have built and we
have two types of models that we
actually maintain in the network there's
a topology bottle I'll talk a little bit
about that it's actually interesting
research topic and configuration model
right and this this intent API is are
actually making changes to these Network
models again they are not making changes
to the
elements themselves right changes to
these morals are then used to produce
the changes in configuration that you
ultimately want to produce for these
devices all happens in software does not
happen by by by human intervention and
the last but the most important part is
that unlike traditional routers where
the changes are traditionally imperative
where you actually interact with the
devices over CLI in many cases and you
do transactional changes to the devices
all the changes are fully declarative
why is it important it is important
because of you know the question that I
mean asked do we know what state the
network using we want to know what
configuration these devices have we
don't want to derive it we want to know
exactly what the configuration this
device is that so it's fully declarative
we know exactly what configuration has
been pushed in the device before before
it has been pushed and by the way this
is not generated by human there there in
the config to be so if things break we
know exactly how to roll back to a
sestet because we know the previous
configuration when the network is
operating well write a yourself briefly
on on network models there are there two
sets of models that we actually have
built here on earth spent many years
building them because this is a very
interesting research problem really for
us to solve the first one we call
actually let's start the configuration
model now this by the way is open source
we call it open config it basically
describes any of the policy or protocols
that we have to configure in our network
in a structured way and it's an object
model it uses hearing as the language
because it has a standardized an idea
and this the list of the models that we
have is evolving and we actually welcome
many of you to sort of feel it take a
look at the repository that we haven't
github and actually contribute if you
want to the second one is something that
are developed in familiar cool what you
realize is that we we had many
descriptions of what are fundamentally
graphs that describe network structure
we had some for optical there some for
MPLS we had some for bgp mesh with some
for our fabric and cluster it was a mess
and so we ended up developing an
internal models which we call you
or unified network model and the idea
being that any structure that can be
described as a graph in and it's
definitely applies for network but it
could be things like the line cuts on a
chassis for example they're described in
this in the city five unified model
they're there in protocol buffer and
etcetera right last part I'll just spend
my last two minutes in this because
super important and then and then I'll
stop here which is I mentioned that the
only way that you actually can can
reduce your error budget is by a knowing
when the network is melting down and
then stopping the meltdown at the
network I talked about stopping the
bailed out of the network which is you
had declared a consideration you know
the good state you get the network back
to the good statement well done is
happening how do you know when I'm alone
it's happening so what do you what you
realize is that the traditional
kilometers that are used in network
specifically SNMP they're not just not
good enough for doing it the reason
being that anytime you want to have high
resolution data of which which needs to
be exported in high volume out of the
router you start trading off a lot of
performance with telemetry which is a
really bad trade-off to have right so
instead what we ended up going with is
something that we've been using servers
that are with our cooling systems with
with our CPU fans for for a decade at
Google we stream telemetry data we we
don't we don't pre process them in the
element before sending about let's trim
them and there is a streaming collector
and and then you know we are basically
able to to export as much data as we
want from the system because we actually
had the connection system that are built
we relied on yet another open source
technology that Google has open source
called G RPC and G RPC has been a some
of the creators are here and this year
we have this beautiful things where we
actually we have have bi-directional
streaming and we added secure and we
take take advantage of that and we use
the open config models which is
basically if you're exporting
operational state it's no different than
the object model that you have that he
used to configure those states right so
use the combination of those to actually
use shipping telemetry to export the
data combination of this has been super
powerful in basically enabling us to
make very rapid changes to our network
right and the the thought that I want to
Lydia with is it's you know if waiting
on in this point it's performance is
super critical but the way that you get
to performance is by launching things
very rapidly and launching often because
you don't get to you know tweak the
performance in a day you get to twice
the performance by making changes every
week right and the system that we build
you know we focus very hard on how we
can allow changes to our infrastructure
every week or every day but don't set
off reliability in doing so right so CDN
is one answer to that and of course is
not the only answer and would love to
hear more from your thought more about
your thoughts as to how you feel this
problem can be solved thank you very
much for your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>