<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>An Internet for Bulk Content Delivery, not Communication | Coder Coacher - Coaching Coders</title><meta content="An Internet for Bulk Content Delivery, not Communication - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>An Internet for Bulk Content Delivery, not Communication</b></h2><h5 class="post__date">2011-03-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-ZHpdAQ562M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now the talk that I'm gonna give today
is trying to think a bit about the
issues that we're facing in today's
Internet to transfer a large amount of
data I'm talking about data that is a
bog delay tolerant and I'm going to see
the impact that is having on the network
and I'm going to try to discuss some of
the possible solutions to cope with
these problems now before I get started
I would like to kind of go a bit over
the air the different waves not the
networks have been going through over
the years now if you look at the
beginning of the networking it was from
the ferries to the 60s and in there the
focus was wires and it was communication
between people he was voice
communication among people in the second
phase it was after the 60s the internet
was created and then it was the internet
was built on top of the telephony
network it was using the same wires in a
different way and the focus was for
computers to talk to each other and then
in the 90s that's when the web started
and then the web used the Internet as a
platform for people to access content
and it happened to be that the web
became kind of the most successful
application that was running on top of
the internet now if you think how the
internet was designed it was designed as
a very very thin layer it was designed
on purpose as a thin layer to allow for
a lot of innovation to happen on the top
layers and a lot of innovation to
helping at the bottom layers but this
thin layer it was designed so that any
application could run on top and you
know there would be all these
innovations happening on top of the
Internet however the fact that it was
not optimized for any particular
application it meant that it was that
for some areas it was not truly doing
its best and in particularly for content
delivery I think the internet at times
has been struggling i'm gonna give you
some examples so you know after the the
web started we start thinking what
happened soon after the tim berners-lee
created engine in geneva we started
seeing that much of today's Internet
infrastructure
handle contin it's been an afterthought
and for a lot of the times we've been
playing catch-up I'm sure you are all
aware about all the infrastructure that
goes behind the curtains in the Internet
to make sure that we are able to scale
the delivery of content and we have
content distribution networks and large
data centers with thousands of machines
around the world however isner is not
always being very easy in this slide I
have two snapshots of the CNN page the
one on the Left where it shows a regular
CNN page on any day with lots of
multimedia content images very rich and
then on the right it's the same page
during the September 11 attacks as you
can see as people were rushing into the
web in the same way that they were
rushing into the TV to see what was
happening the web just couldn't cope
with a load so the only solution that
CNN had at the time was to strip off
most of the multimedia content that
there was in their pages and then just
provide a very simple text based
information so they could actually scale
the distribution of this content so it
was we started realizing that the
internet was not really very well suited
to be a broadcast medium like the the
television or radio was and more
recently we've been seeing more
struggles with a very large content so
if I were to ask you whether the
internet is a preferred medium for this
stability bulk delay tolerant digital
content um probably that the answer is
that not be young a certain size and if
you think about a lot of movies home
videos they cut data backup scientific
data today are not going over the
Internet are still going over either
dedicated networks very expensive
private networks or parcel delivery
systems like the Postal Service so the
best example is Netflix you know more
than 10 million users in the US 1.5
million DVDs per day so that's roughly
2.5 petabytes a day if you compare it
with some rich recent
figures that Cisco released all US
peer-to-peer traffic is roughly 14
petabytes a day which means that Netflix
is carrying a significant fraction of
the traffic of the video traffic that is
being distributed in the US on any given
day but that's not the only example
probably you know that a lot of times
when you want to do replication of data
across datacenters a lot of the times
what you do is you take the machine you
install the software you put all the
very tiny and then you put it in the
FedEx and then you ship it to the
location where you want to have the
server running even see the ends a lot
of the times and that their business is
to distribute content when they have
massive amount of data to distribute
like for instance the log files that
they need to look into and study they
actually distributed using posts on my
mail service another example the CERN in
Geneva every day it exchanges roughly
about 20 terabytes of data of scientific
data across different universities in
the world on research centers and to do
this we actually built the private
network and if you look at the public
Internet the internet how well it is
dealing with bulk data transfers for
instance the peer-to-peer traffic that
we are seeing today we are seeing a lot
of signs that it is sometimes it's
struggling to cope with that amount of
load it's instance if the current bug
data demand is higher than what the
internet can handle and the solutions
out there are not the most interesting
ones you know we get solutions where you
can play with pricing schemes and you
can change to very sophisticated
congestion based pricing schemes and the
users don't understand the ISPs is hard
for them to to track how to do this then
the other solution is you know you block
this transfers and you know
to complete fiasco for the users and for
the ISPs and and then the fair option is
to have some daytime volume cups however
this is also very hard for the users
because they need to keep track of how
much volume they have consumed and on
the long run it doesn't work my own
experience I don't know if you've ever
tried to move one terabyte of data from
one side of the world on other side of
the world it's actually a nightmare I
had this experience when I was at
Microsoft we had a one terabyte of locks
that we wanted to shift from Redmond
into Cambridge and what was happening is
that when they was congestion in
Cambridge he was not congesting in
redmond and the other way around and the
bill was so so huge that at the end we
ended up going into a playing and
recording everything into a hard drive
and bring it with us and there are a
couple of reasons for this and then this
is a recent report about Tom Leighton
showing that the effect of distance it's
a sea bean seeing as more and more
pronounced every day on the internet
where you actually have this is the data
of how long it takes to download the DVD
file as you go further away into the
network and you can see that the time to
download the file can actually change
between 12 minutes to several hours
depending on how far you go into the
network and another thing that we're
seeing is that not only the further we
travel into the network the more
bottlenecks we are likely to see but
also these bottlenecks are time
dependent and I'm going to show you some
more data later and I'm going to dive a
bit more into it but you will see how a
different types of the lady congestion
levels are completely different at
different times of the day and this
changes in different parts of the world
so what is the real problem well the
real problem is that the internet was is
not very well suited for delivery was
very well designed for communications
but he was designed with
bulk an instantaneous delivery again
sorry short an instantaneous delivery
and what we're seeing with these large
files its bulk and delay tolerant
traffic so the result is a year if
you're trying to rush this large data
files over the network it's either very
expensive or impossible you either get
charged a lot of money because of volume
based charging or you start experiencing
some congestion in different parts of
the network some of the reasons is that
eventually not all the bits cause the
same and actually not all the bits are
equally important and today the network
it just treats all bits regardless of
whether they are interactive or delay
tolerant indiscriminately so let's step
back for a second try to think about
other industries how they solve this
problem and if you look into the
physical world there is actually
industries like FedEx that they've been
looking at this problem for a large
number of years in town so what if we
start thinking about the internet rather
than in telecommunication network as a
cargo distribution network so you could
think that in the same way that you go
to the fedex web page and you say i want
to take this parcel and i want to ship
it from point A to point B and I wanted
to arrive by this time it is going to
church mean this much money if I wanted
to arrive by this other time it's going
to cost this much more similar thing
could actually be done in the internet
with different amounts of data that you
want to transmit from one place to
another one so how does it work with
fedex well fed ex they have all these
local offices all these local branches
where you actually take your parcels and
you give it them and that's it from
there you forget about it and then they
take it and then they have a delivery
network made out of planes and made out
of trucks where they actually try to
optimize the delivery of information
from one part of the world to another
part of the world to make sure that they
can take the cheapest flights or that
they can
when they arrived in the ground that
they can take the shortest path to the
destination and then they have something
that if they have these warehouses where
they actually store a lot of the content
in big facilities to make sure that they
can take the cheapest flights overnight
or they can take the best route and they
store all the content there and they
don't route it immediately now if you
look at today's internet it's very good
at routing you know it's very good at
connecting point a with point B however
what if we were to think about the
internet as a postal service what would
it require well we would have the same
internet that we have today routing from
point A to point B but then we would
need some kind of post office internet
post offices right it would be some
boxes that are close to the to the user
that the user has this large video file
has this information and just upload it
very fast to these boxes and then
forgets about it and then you also have
these transit storage warehouses there
are big facilities that you use to
schedule the information up from point A
to point B and you study at different
places to make sure that you minimize
the costs of delivering from one place
of the network to another place of the
network and obviously you use some
intelligence in the middle to be able to
sort all these things out now there is a
lot of debate currently on the new
internet design and whether we should
redesign the internet from scratch or
not so there is a question of well
should this be a revolution or an
evolution and I think it can be
implemented in both ways would be a
revolution that revolution would be to
think about the internet as a FedEx four
bits so the routers would need to
include terabytes of storage there would
be new routing algorithms that they not
only route across space but they also
route across time and you know carrying
algorithms there and the route across
time but in the time span of
milliseconds or seconds to avoid
congestion control but we never think
about routing in the time span of hours
or even days
and if we were to think about how to
implement this as an evolution that we
would probably think about it as billing
it as an overlay and there will be a lot
of things to learn from peer-to-peer
networks and you could actually think
how we could take some of the existing
City and infrastructures or large data
centers and recast them into with some
transportation logistics to provide some
of the facilities that I discussed
before so if you're going to go and try
to design something like this I think
there are still a lot of questions to be
answered you know how many storage
warehouses do you put and where how many
post offices how big how are you rolling
and scheduling across time and space
where do you store store things for how
long a little reliability fairness etc
and then you can always try to design
such system with two goals in mind you
can either decrease your transportation
costs under model where you are getting
charged by the pic or you can actually
try to increase the data rate transfer
under some flat rate based skin where we
are seeing is congestion and whether
there is no elasticity in how much the
bandwidth can grow and you just because
you're paying flat rate now I'm going to
focus in the scenario where and you have
all these data centers around the world
and then each of them is kind of subject
to some charging scheme based usually on
95 percentile and then what you want to
do is you want to replicate information
across all these data centers and you
want to do it with the minimum cost I'm
not sure how how much how many of you
are aware of how is peace charged for
traffic but is usually based on what
they call the 95 percentile 95
percentile basically says that you take
the month and you divide it into five
minute slots and you measure how much
traffic goes in each of these five
minute slots and then you remove the 5
percentile the largest peak then you get
the 95 percentile and that's how much
you get charged for so it's sort of a
metric of
the congestion that was created into an
Edward at any at any given point in time
and the reason for this is because a lot
of the networks and a lot of the ISPs
they die menschen their equipment and
their networks based on pick our load
out and they manage the traffic
accordingly so let me give you an
example to illustrate what happens if
you don't do something smart with these
large bulk data transfers this is a
typical load that you see in most
equipment to the in browsers or in
servers you know you get this loyola and
nocturnal patterns and imagine that this
is a ling or you're experiencing this
dystrophy and now what you have is you
you want to transmit this cargo data it
could be large data backups or it could
be some replication information that you
want to transmit through from one place
to another place if you just send it
like that on top of this link what's
going to happen is that the 95
percentile that is defined by this red
line it's going to be pushed all the way
up you see all the law gets push all the
way up and in similar way the 95
percentile gets pushed all the way up
and therefore you increase your charges
a lot smarter way of doing is it's a
water filling way well you know we have
the same scenarios before you have these
pics and valleys and then you have this
cargo data that you want to transmit
rather than doing the way that we
described before you can actually try to
fit it into these valleys and being able
to use the resources that are available
during off-peak times now if you think
about this cargo data in certain sense
it can be seen like the fat on the
internet right and we really need to
move this fat we need to make the
internet more slim we need to remove
this forum and uncreate more spare
capacity so we can actually pump a lot
more data cheaper cost now you could
think well maybe the simplest thing the
solution is just to take the data and
wait for the valleys of the network and
then just push them in the valley
and it's not enough know that there is
not enough if you just post data into
the valets you can enter him in
situations like this the problem is that
the valleys and the pics don't overlap
at different places in the world when it
is nighttime in the Europeans daytime in
the US and vice versa and even different
networks if they current-carrying
different types of traffic they pick at
different times residential Network pics
at the certain time and an enterprise
network pics at the completely different
time so the pics and the valleys may not
overlap so if you just follow the
valleys of one of the networks you may
hit the pic of the other one and vice
versa and this is a simple example that
exemplar Isis that that information this
is the load at different links in
routers in Latin America in Europe and
in China if you're trying to send data
in in the linking Latin America when it
is night time what happens is that in
Europe is already one p.m. and in China
is 8pm so what happens is if you are
trying to rush this data and to end
you're going to incur a lot of churches
in in China or vice versa so what you
really need is this transit storage
warehouses so you do impedance matching
between these different waves that have
these maximums and minimums and you need
to make sure that you do the proper
scheduling like FedEx does with their
planes under parcel system so let's
let's see what this would take imagine
that you want to minimize cost you have
this post offices are around the world
that you have these internet warehouses
where you can store your information
this could be your data centers and what
you need to know is you have some data
that you want to replicate I want to
distribute and you want it to arrive by
a certain deadline because of
consistency reasons or because some some
SLA agreement and you also need to know
some
the load when is the different links
links are picking at what times and so
you're able you're able to predict the
consumption patterns of these links or
on these servers and you need to have
some kind of rough idea of the cost Emma
what is the pricing for each of these
links which is usually a concave
function because economy of scale is
helping you and the more you consume
usually it flattens out the expenses and
then what you do is you can do some
dynamic programming I'm going to go into
the details of the actual the
algorithmics of how you you get this but
the end result is that you're able to
schedule things very efficiently and get
some very interesting reduction costs in
how you transfer data from one part of
the world to another point for work so
let me show you some actual results we
took the load from a large wholesale ISP
with roughly 400 inter connection points
and pairing with roughly another 140
ISPs over a three-month period and we
took their pricing of each of the inter
connection points and we assume that we
were placing one of these internet post
offices at each of the Pops and that
there was the number of transit
warehouses at this whole ice p wholesale
I speed and will allow us to route
information from one place to another
efficiently so the example was we wanted
to transfer 27 terabytes from one point
of the world to another part of the
world basically from any of these two
pups from this large ISP and we had a
deadline of two days and we wanted to
compare to transfer policy the first one
was this end-to-end transfer and then
the second one was the internet postal
service where you were using these
postal offices and these transit
warehouses and we wanted to compare the
difference in money and that it would
take you between using one policy or
using the other policy and basically
these are the results and so you want to
take again 27 28 terabytes from this is
from Latin America to Europe with in
today
and the cost of an end-to-end transfer
would be roughly 150 thousand dollars
and with the internet postal service
this gets decreased by about a factor of
20 and the reason is because you're
really trying to use these peaks and
valleys of the network and store
information in those places as much as
you can in a similar way that affects
fedex was doing to minimize the cost of
the transfer now I just give you one
example if you look at all the source
destination pairs in this network and
this is the amount of data that could be
transferred with the Internet postal
service and this is with n to n at the
same cost so if I am just I just have
this much money how much data i can
transfer with one mechanism versus the
other mechanism and it turns out to be
that all the points that are below the
line it means that with the inter postal
service you can always transfer a lot
more data that with the end-to-end
transfer and there are certain points
that actually these difference becomes
quite significant so if i zoom out into
this point and i look at what this
points are actually points in the world
to pops that are have some time
difference in the world map so for
instance information that wants to be
transferred from the US into europe or
into asia and so forth but the
interesting thing is that even if you
want to transfer information across
notes that are in the same time zone you
still get benefits and the reason is
because it's not time only the time zone
that defines when pics when the pics and
the valleys could produce but also the
utilization patterns of those networks
and as I was saying before you may have
networks with completely different
utilization patterns and networks are
serving some type of users or some type
of content and other networks a
completely different set of people and
the consumption patterns and the pics
and values could be completely different
so what about the real physics um you
know we did this exercise what if we
would take these 27 terabytes that we
did for the previous example and we
would like to transfer them from
argentina into spain and so we actually
did the numbers and we said okay how
many discs do we need to feed 27
terabytes and it's roughly very disc at
one terabyte each more or less and this
is roughly thirty eight kilos and then
we went into the fedex website and we
said okay we want to transfer these from
argentina to spain is going to take us
two days how much would you charge us
and it turns out to be that it's roughly
about six hundred dollars but it's only
one shipment but you need over the month
you would have to do 15 shipments if you
want to have a continuous stream of
information once every two days and so
very month you would actually get a nine
thousand dollar charge for these 27
terabytes from argentina into spain now
you compare that to the results are I
described before it was 144,000 with the
end to end transmission it's 9,000 with
fedex and it's 7,000 with the entire
postal service so what this means is
that the Internet Postal Service is not
always a win-win situation there are a
lot of scenarios I review the things
smart that if you do things right you
can actually get a very convenient
solution that is an online solution that
actually provides you with similar cost
to the ones that you are getting with
fedex but with a lot more convenience
and i believe that is an expect room of
data volume and time deadlines and
configurations where the fedex can do
better or worse than the inter postal
service and you just have to look at
these parameters and MP and decide but
it's not is there is no clear winner
depending on the situation one or the
other will do better and then the final
thing that we did we say okay rather
than just taking one of these source
destination pairs I was Argentina to
Spain let's try all the source
destination pairs and see when is it
that FedEx is better than the internet
postal service or not and it happened to
be
that this is the distribution of the
cost for internet postal service and
this is the cost for fedex so it happens
to be that is you draw a line here about
seventy percent of the source
destination peers in the network that we
were studying actually have cheaper cost
than fedex only thirty percent of the
source destination pairs had in care
more cost and sending it over the postal
service so you know reducing costs to
push more data is great i think it's a
it's a very useful thing to do but what
about the scenario where we have flood
rate we pay just a flood rate and then
what happens is that we just experienced
different congestion levels at different
times of the day and this is a lot of
the things that many many times it
happens with residential users they pay
a flat rate and then the congestion
network changes over the day depending
on the usage that is going over that
network so imagine that your user and
you have a large amount of information
that you record it with your digital
camera and you want to transfer it from
here to your family somewhere in the
other side of the world and you have a
high speed fiber connection and you want
to do this point upon transfer the
fastest way possible but the problem is
on as we see before you get this
bottlenecks in the network that increase
as you go deeper into the network and
then you get these congestion levels
that change over time and change the
different places in the world so how the
how can you schedule this transfer such
that you maximize the speed of the
delivery of this information from you to
your family in the other side of the
world so
do such situations exist in reality so
if I try to do a transfer of data from
here to Europe will I see fluctuations
in the speed that I see and does that
speed fluctuate over time and is it
different than the fluctuations that i
will see if I transfer it to another
continent in the others in the other
direction of the world so with some
experiments where you take the sender in
the US and in Berkeley and then you try
to send it information into barcelona
and and then you have another
intermediate node in seattle and you try
to do transfers you know over 24 hours
and you try to do an ftp and then you
just measure the throughput that you're
getting and you're trying to do from
berkeley to see our own front cielo into
barcelona and basically you see these
large fluctuations over the period of
the day where you get the same pics and
valleys as the ones that I was
describing before in in the in the core
routers of the internet you're actually
seen the same thing in the access links
of the internet so let me let me show
you something for a second I missed a
ball off the presentation for a second
and and I'm going to show you some some
data that confirms these these pics and
valleys into the access networks of the
users this is a tool that we build that
basically it's able to track the
experience that users the bittorrent
users across the world are seeing in
their access networks through any ice P
through any in any country in the world
so basically we're able to know what
type of experience a user using
bittorrent is seeing in any of any ISP
in the world now the color basically
tells you more or less the average
download speed that they're seeing
so you can zoom in two different places
of the world and you can go into
countries you can suming into the UK and
and then try to see what type of
experience the users are seeing in there
and let me jump and for instance for the
UK you get information like this this is
the average speed that users are seeing
in the UK when they are not loving
BitTorrent and this is the pig
experience that they're seeing and I me
and this this is over a period of one
day and most of the times for a for a
lot of ISPs you see a very flat
experience so over the course of the day
and nothing changes but then and then
you see other ISPs like this one you
know nothing changes over the course of
the day but then all of a sudden look at
this you get things like this you get
things like during the peak times of the
day the performance of the bittorrent
downloads actually decrease to make room
for other interactive traffic and this
is the case of wine I spin Europe but
this is the case of another isp in the
other part of the world in in America
and you can see that there is a similar
variation into the available rate for
this application however the times of
the day at which this thing happens are
completely the opposite so when it is
taking one side is button on the other
and vice versa so the bottom line is
that we are seeing more and more of
these fluctuations of the available
speed and the congestion across the
world antam these fluctuations change
over time and we really need some smart
mechanism like the one I described
before to be able to schedule the data
across these different peaks and valleys
in the world so what we did is we took
all the data
collected from the tool that I described
before that it basically tells you the
available capacity that a bittering user
is seeing when it is unloading this bulk
data across different eyes peace in the
world and then we calculated what would
be the speed and the increasing speed if
they were to use some kind of internet
postal service like the Y described
before to be able to transfer their data
and across all these different ISPs that
roughly we found about 60 of them that
we're doing active traffic management
and you can see again all these dots if
they are on the upper side of the plot
it means that the speed can actually
increase versus just using the regular
bittorrent and and we got factors of
speed improvement of up to 55 times so
I'm gonna conclude there i think i've
i've tried to discuss that the content
distribution it's something that was not
inherent to the to the internet design i
think over the years we've seen the
interim struggle to be able to cope with
a large massive content distribution and
offerings push the internet to its
limits and the reason is you know
communications in bulk delivery i'll
completely different beast and in a
sense the internet was not fully
optimized for that and I really think we
need to look again at how is it that we
can enhance the current internet
infrastructure with more storage better
temporal scheduling so we can increase
the capacity or decrease the cost and
finally I think you know I described two
words that was the online world on how
to schedule a time in there and then the
physical world the fedex but I think
there are a lot of opportunities to have
to mix these two worlds and kind of make
a system that could actually combine
both of them and it doesn't
it doesn't have to be all the way
physical or all the way online it could
actually be part of the way physical and
part of the way online and so forth and
I think this is a space that I still
open for for design an exploration so if
you want to get more information there
are a couple of papers online you can go
to to our website and you can read more
over it and with that said I'm going to
stop there and open it for questions yet
so the question is whether we've look at
the interplanetary network and no we
haven't but the the problem there is
different is that you get very large
round trip times so so so the problem
many times is that interactivity it is
very hard because of the large delays
and but once you cope with that large
delay then being you need to make sure
that you maximize the available capacity
and the available capacity they are able
throughput once it's maximizing you
should be ok but I can see how if you if
you want to transfer data from one
planet to another planet then you could
actually use intermediate planets at
storage nodes in the same way that fedex
uses these warehouses to store the
content or even use it as storage
warehouses for for the earth because
depending on the rotation of the earth
you can have some satellites or some
planets that actually don't move move at
the different rate with respect to the
earth and they could store information
there such as when the Earth rotates
they could actually deliver it again so
I think that's a good idea right so the
question is we've seen similar problems
from the beginning of the internet with
things like email and then there was
another second part of
question about the probability what
would be some concerns about whether the
whole world has to adopt this before
it's actually being useful so you bring
a good point things like email they
worked in store and forward the only
thing is sometimes most of the times is
the it was not optimized for bug
delivery and it was not optimized for
for cost or delay it was more like for
for connectivity purposes because I'm
not connected online they just store it
in there so the variables through which
you optimizer rrr are different and then
you you need a larger number of servers
across the world to be able to optimize
things properly so it is similar but at
the same time it needs to look at the
space the wider space to make sure that
it optimizes this the solution and then
the second part of the question
regarding the playability I think I
would see this like like a large CDN or
a company with a large number of data
centers or large telco with global
presence just providing these facility
of large data file transfers and and I
think you can do an increment and
deployment over that you know there is
if one of the CDN starts deploying a
service like this tomorrow I can see how
enterprises could actually contract this
service from this CDN in the same way
that they contract hosting they could
just contract a service to replicate
data across the world and then the next
thing would be to provide it to the to
the end users so they could actually
have their point-to-point transfers more
efficiently and I think this is probably
something that will become more and more
important probably the first 10-15 years
that we've looked at content delivery
we've always focused into the very
popular part of the of the of the sip
distribution that you get the very
popular content accounts for most of the
request but we really haven't looked
much at the very long tail how do
we optimize things and as more and more
content is produced user-generated that
content is only for one user consumes it
and then the family one user produces
and then the family consumes it and that
content is really on the long tail and
and it's very hard to use the
traditional content distribution
infrastructures to host that content
efficiently so you will probably need to
start thinking about some of these other
elements like the one I described today
to make sure that you do the
distribution of this long tail of of
information very efficiently right and
so the question is if you don't have
enough servers in some places of the
world and it will be hard to provide the
service and why not reducing some of
infrastructure already exists like for
instance email that they already have a
large number of service with with the
storage I sure what why not I mean I can
think of email but I can't think of many
other services that they also have a
large number of servers and storage
capabilities I think if you look at any
CDN or they have in the order of tens of
thousands of of storage capabilities
around the world but you could even
think about it in a peer-to-peer way you
know more and more set up boxes and home
gateways are becoming capable of doing
the same thing that a pc is doing and
they have a lot of storage so you could
actually take it to the next level and
think of the network think of the home
as an extension of the network and they
started thinking how you could use that
infrastructure there to to to deliver
services like the one I described now
right so the question is I guess the
question the question is that the
BitTorrent data that we showed is ur dad
way of gathering it without having to
send a large volume of information so
I'm going to go over the details of how
we gather that actually you don't have
to send a lot of information so
basically what we did is we took a
bittorrent client and we modified it
on this Victorian client usually a
Victorian client opens 70 connections to
talk to 70 peers hanging parallel is
unloading from four and it keeps
changing for out of in the in the range
of the 72 find the fastest ones so well
within is we took a client we modified
it and in rather than opening 70
connection is able to open 100,000
connections okay so not only were able
to talk to 70 peers were able to talk
two hundred thousand in pilot and then
once every five minutes we switch we
terminate those connections and we move
to another hundred thousand users okay
so over the time of the day we're able
to to talk to several millions of peers
now we don't download any content we
don't download anything the only thing
that we do is we listen to the messages
that the peers are exchanging among
themselves every time that they'd unload
a block of a file so invitar in the way
works in a peer-to-peer network every
time i download the block of the file i
need to tell my friends so they can tell
them that they can download from me I
need to tell my peers so that's the only
thing that we do we have this client we
connect two hundred thousand appears and
then we start listening on the exchange
messages so we don't participate in the
data exchange which is look at the
metadata exchange messages and we do
that once every five minutes over a
large number of of peers and then over
the course of several months then we are
able to map the world right the cost
comes from a real telco so the question
was what are the assumptions that we
used to compare the cost of FedEx versus
the cost of transferring the data over
the network the cost of transferring the
data world and Edward are real costs are
the links based on the real 95 churches
we we took the the actual churches I
think from last year 2008 so the
question is whether we were looking at
the ispe rates or the
link read it was a wholesale wholesale
prices of the links right the question
is whether we looked at any wireless
provider the answer is no but I'm
expecting that I'm expecting that the
results will look similar even more
amplified because in Wireless the
resources are even more limited so the
constraints in terms of utilization and
cost of upgrading and deploying will be
tighter sure sure probably the peaks and
valleys will vary still a lot of people
are using 3g today as mass home
broadband connection and then you know
it's mobile broadband connection so so
you're right maybe they can use it over
different periods of time even when they
are at work or when they are traveling
so maybe the pics that will be rather
than having one pic there will be
multiple pics during the day maybe when
there is a break during lunch because
you have your laptop with you with the
3g card and so forth so the parents will
change and you probably need to adapt
the algorithms to make sure they get up
to those patterns right so the question
is um if you were to implement this do
you need some kind of entity that has a
holistic view of the available
capacities in different places in the
world the same way that fedex has that
information the answer is you you need
to have local information but you need
to have all the local information but
that's something that most of the
networks that have presents in different
points they have when I'm wondering when
when Google puts a server somewhere in
the data center and bond purchases some
bandwidth from some isp they know the
charges they know the utilization of
that link they know what is going to
pick the thing that the thing that you
need to do is you need to predict the
future so it is not it's not there is
not as much the problem of knowing the
cost and knowing the utilization of the
links where you have presents because
you can measure it's about predicting
the future so because what you're doing
is you're scheduling over time
you need to if you have a data transfer
that is going to last over two days you
actually need to predict how the load is
going to look over those two days so you
can actually schedule your information
across the world and for that you need
to have some good prediction mechanisms
our experience is that a lot of these
links unless there is some blocks one
effect that all of a sudden something
drastically changes are fairly
predictable so the same patterns repeat
over different days with some variations
over weekends and so forth but but the
answer so the answer is yes you need to
have information about the links where
you operate this note and yes you need
to be able to predict the future but I
think both are doable so let me see if I
got the question right so the question
is if you start looking at other
applications that are not delay tolerant
that are more interactive like for
instance video streaming video
conferencing is is there enough
bandwidth out there to support those
applications you know I think a lot of
the times the the problems has been on
the access link and the uplink capacity
so I don't think it's being so much a
problem inside the network in the core
of the network as a problem in the
access networks it's a technology
problem and a lot of them will go away
with with a fiber and and with more
symmetric links I think exactly the
point of this talk was trying to say
that you need to make sure that if you
have two types of traffic one that is
interactive like for instance video
streaming or or a voice conversation and
you have something else that is a lot
more delay tolerant like data backups or
replication or large movie transfers you
need to treat them differently and you
can actually have a win-win situation if
you treat not only treat them different
but you can also optimize how you
deliver each of them and then you will
be able to make a lot more capacity for
both of them so the point of this
internet Postal Service was to treat
this large bulk
transfers in a smart way so you can
actually make more room for the more
transit delay sensitive application like
the video streaming one right so the
question is and what type of traffic is
amenable for this infrastructure that I
described before and whether we look at
at what type of traffic in today's
internet could fit this profile so I
think as you said mostly I would look I
would think of a large amount of the
peer-to-peer traffic that is happening
out there which is a significant
fraction of the of today's Internet is
delay tolerant to a certain extent you
know people leave their movies to
download and then most of them they they
download when their way over night or so
there is some certain delay tolerance in
there right so the question is if if we
start cashing in legal content when we
run into trouble you know caching for a
long time it's been having its up and
downs at times we receiving an attempt
in the EU to bond caching altogether
because the cash was hosting some
content on behalf of another provider
and and the second point were saying
that they were not respecting the time
to leave ttls of the CNN page so they
could actually absorb a lot more of the
load so I think you're right in certain
situations you will probably need to
look at what type of content and being
able to to maintain the rules by which
the content provider wants that content
to be treated and some of them they win
a market has known cacheable and some of
them is a illegal content and you're not
going to be able to store it but I'm
seeing more and more of the content that
is being consumed by users go
into more legal ways of distribution
more legal frameworks like for instance
if you look at something like Spotify to
the I don't know if you Spotify it's an
online system where that we provide you
most large fraction of the of the music
content that is out there and they're
using a peer-to-peer system is
peer-to-peer by itself it's just another
medium of distribution but they just
frame it into a way that they were using
it with agreements with the majors and
the providers they said okay let's use
peer to peer to reduce the cost so
bottom line I think you'll have to go
through I don't think same peer-to-peer
and no peer to peer is is can be
attached to saying it's legal or illegal
it's just a different transport medium
and you can actually use it as a
separate way of transferring your
content so what I was saying before is
that the peer-to-peer traffic i'm not i
don't mean the the i mean that there is
a lot of movies and there is a lot of
content that now is being transferred
over peer-to-peer but tomorrow it could
be transferred over a CDN or it could be
transferred over the network like the
one described i understand understand
you Christina so the question is
peer-to-peer networks are very good at
handling the very popular content
because the more people request the
content the more nodes are there to
serve it and therefore you know
naturally the capacity of the net
worried its fund but now you're telling
us there is more about user-generated
content about a user wants to send it
from Argentina into Spain and how you're
going to handle that so that's true that
that's exactly what I was saying before
I think over the course over the period
of the last thing years with being able
to design very good content distribution
systems to deal with very popular
content like see the ends and caching
and I think on the next wave we are
seeing a lot of the content that is
being user-generated that is non
cacheable right and for that traditional
system like peer to peer or like
Syrians are doing are going to do a poor
job because you are always running into
the problem of the of the nice ratio you
store it in the cache and nobody else
wants to see it so that's why the
infrastructure that I was presenting
today is more amenable to be able to
transfer information from one part of
the world to another part of the world
even if only those two entities are the
ones i want to see it now the the part
of what i was talking about peer to peer
is you need to use this intermediate
storage to be able to do the mismatching
between the different pick some valleys
in this and entice of the world and this
storage node rather than being sitting
in the network could be sitting in the
users computer and that's the part of
the peer-to-peer angle not so much the
traditional swarming effect that the
more people come the better it works
it's more like using some notes as
helpers to be able to store this
information and be able to do the
difference between the valleys in
different types of the world you know if
you look at the sip distribution usually
you get we did a study of the ubud
traffic couple of years ago and it was
more on the 64 a rule so you get
something like the long tail is roughly
thirty percent thirty to forty percent
of the traffic so we are very good at
optimizing the sixty percent but then
the other thirty forty percent we have a
hard time so I truly believe if you put
some system that starts understanding
the differences of the two endpoints in
terms of Pixum valleys and you use
something like that you'll be able to
optimize for that forty percent yep
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>