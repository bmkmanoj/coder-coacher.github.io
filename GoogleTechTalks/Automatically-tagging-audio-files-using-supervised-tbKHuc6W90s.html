<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Automatically tagging audio files using supervised... | Coder Coacher - Coaching Coders</title><meta content="Automatically tagging audio files using supervised... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Automatically tagging audio files using supervised...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tbKHuc6W90s" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's my pleasure to introduce you
Douglas HEK HEK his promise are at
university of montreal and actually we
are close to providing a PhD student in
machine learning and music and that's so
we work together often and he he's
interested not only in music but in
general in machine learning and sequence
processing and there's a lot of
interesting problems to solve there but
music is of course a good good problem
as such so I think with that Douglas I
let you Sarah can you hear me okay it
seems to work okay so thank you for
coming I as Sammy said I'm from the
University of Montreal computer science
and international lab for research and
brain music and sound problems and I'm
going to talk a little bit about
automatically tagging audio using
supervised learning and the overarching
goal is to see what kind of information
we can extract from mp3s and waves on
the internet what kind of picture can we
paint of what's going on musically on
the web so there's a number of people
that worked on this project students
Norman casa grande who's now at last.fm
to other students who are still in my
lab colleague balázs kegel and i'm
currently visiting professor at sun
research labs and I'm collaborating with
polymer in Boston however the work I'm
talking about today was done at
University of Montreal
so the structure is straightforward I'm
going to give a very short overview of
sort of machine music listening and talk
about audio feature extraction and then
I'm going to move into some boosting
related work having to do with
predicting artists and genre from audio
features and then predicting arbitrary
tags from audio features but first RK a
quick overview of what's happening in
the world in terms of music there are
really three commercial approaches to to
finding information in audio and linking
it to users be a collaborative filtering
or just having a picture of what's going
on inside of an audio file there's of
course collaborative filtering I presume
most of you are familiar with the basic
concept so amazoncom does this and to my
as far as I know they treat books like
they treat music so they're looking at
by buyer preferences the upside of
course is all the upsides of amazon and
the downside i think for music as
opposed to with books songs are
multi-purpose so the same song on a disc
can serve the uses of you know a dinner
party or jogging or whatever but they're
only purchased once so it's hard to
untangle this multimodal usage whereas
by and large a cookbook is used for
cooking and a mystery novel is used for
you know reading on the beach last.fm
has become very popular it's a social
recommender and it does a better job of
predicting current musical taste because
of the constant tagging that's happening
at last.fm it also gives a good measure
of popularity but suffers from cold
start for the same reason new songs
don't have any tags and so at last.fm we
don't know anything about new songs and
also a problem they fight with a lot all
roads lead to Radiohead which is to say
there's a kind of popularity bias and
social tagging systems we're very
popular groups tend to draw in other
bands and finally there's Pandora
content-based recommendation however
it's it's human labeled I actually think
this is incredible but Pandora
does content-based recommendation from
audio but they do it by having around 40
experts listen to music and then label
you take a piece of paper out and label
a number of parameters and so according
to the guys at Pandora they're doing
about 7,000 songs a month and and so
it's a it's needless to say not very
scalable right like you know what if
they want the four hundred and first
feature right do they have to go back
and listen to these 50,000 songs again
or a hundred thousand songs okay that's
not really what this saw this talk is
about I just wanted to give you a basic
idea of the technologies that are
available I'm proposing another really i
think more modest approach which is
which is to ask the question what what
can we do with machine learning what
kind of information can be extract from
audio and in what ways can we map
acoustic features on to classes or
distributions maybe we have labeled data
maybe we don't and in doing so how can
we how can we take audio which is
largely invisible 22 is invisible to
sort of text-based search engines what
can we extract from audio so that we can
expose some of this audio structure to a
search engine like Google so this could
mean automatic annotation or
auto-tagging which I'm going to talk
about today it could also mean looking
for something a little bit more abstract
like some meaningful space in which to
measure similarity among audio or some
meaningful space low dimensional space
in which to visualize audio but one of
the main goals is to be able to sort of
augment and regularize social tags and
other web data so that if we take a new
song a song that I made at home on my
computer search engine can maybe do
something meaningful with that it can
extract some some information to
bootstrap its insertion into some search
space so the pros of something like this
I think are clear it's clearly more
scalable in in the same month that
Pandora can can label 7,000 songs we can
can label a lot millions and we can also
predict attributes that are not likely
to be tagged by humans like tempo was
the recording highly compressed you know
values that may mean something in terms
of music similarity but which aren't
sort of evident to he
who might be doing some tagging the cons
are also evident acoustic features are
not predictive of everything right we're
not likely to find a good feature that's
going to tell us whether something is
protest music or not unless we also have
the lyrics and also we don't have a very
good measure of quality so we may be
able to say with high reliability that a
particular song is 1960s bebop jazz but
we're probably not going to know that
it's a great jazz solo as opposed to a
mediocre one so we may treat John
Coltrane like all the others and the
final con is really a difficulty for me
not for you is that the engineering is a
challenge so in the university
constructing the databases of music
linking those databases of music to tag
and other web-based textual information
doing the large-scale machine learning
of extracting features and learning is a
challenge in academia and it makes
Google a very attractive place I might
add that you have all of this
infrastructure ready ready to go okay so
I'm gonna have to speed up a little bit
I think so this is for those of you who
do machine learning this is extremely
simple diagram for those of you that
don't just want to give you some pretty
pictures to look at to give you an idea
of what what I'm trying to do I'm taking
a waveform an audio waveform we're
looking at number one feature extraction
extracting some sort of feature from
that audio it's probably some sort of
fourier based feature extraction it
could be a wavelet-based extraction
they're basically there's a big tool kit
full of transforms that we can try and
then we're going to take that feature
and we're going to train a classifier in
the research I'm talking about today
we're using a very simple supervised
learning approach so we have some
features that are extracted from audio
and we have some tags that we've found
on the on the web via data mining that
we know relate to the audio so we have
some tagged audio and we're gonna build
a classifier that will try to predict
those tags from the audio and that will
give us a trained model and then from
there we can take unseen features songs
we've never heard before that have never
been tagged
and we can generate tags for them we
might say oh that's rock or I like to
jog to that or any number of other sort
of qualitative features so this is a an
area that's actually received some some
some research in machine learning and
also in music information retrieval the
challenges this is reiteration from the
previous slide two ago what features do
we use how do we learn and how do we
scale and the previous models I think
the two that warrant serious discussion
r is the work of Dan Ellis using support
vector machines from Columbia there was
some work using decision trees and some
nearest neighbor algorithms and then our
work using ADA boost bergstrom bergstrom
Casa Grande and Aaron are all students
in my lab and biological is a fellow
professor and the approach I'm going to
talk about today is an extension of the
2006 machine learning paper I'm using
ADA boost where we're going to
iteratively apply feature selection to
build a relatively small feature set
we're going to boost simple classifiers
on these individual features and combine
them to predict tags and we're actually
going to treat all of these tags as
statistically independent even though
they're not to simplify the learning
problem as a first step so I'm going to
go through some of the basics of feature
extraction then am I going to go through
a little bit about boosting its not my
model it's a very simple machine
learning sort of the first thing to try
in terms of doing classification and
then I'll show you some results so I
think for music based work the feature
extraction is a very interesting problem
because we want to extract features from
audio which reveal musical content and
by and large at least in the music
information retrieval community many
people have started with speech
recognition features features that prove
to be useful for predicting speech from
audio and from this chunk of work I've
sort of focused on three major
categories
and there there's a whole tree a whole
forest full of ways to do this but for
those of you they don't have this
background we can look at spectral
features so basically features that make
one transform from the time domain into
the frequency domain like a spectrogram
we can look at caps tral features which
are effectively a second Fourier
transform of spectral features for those
of you who aren't familiar with caps
troll it's a play on words maybe you can
figure out what the play on words is its
spectral backwards the the stem of the
word is spec backwards so we have kept
circle officiants and some other special
caps for coefficients called ml
frequency caps or coefficients and a
similar class of features that relate to
autocorrelation and then we have a third
class of features which I just sort of
throw into a bag of statistical features
that are extracted directly from from
the waveform such as zero crossing rate
the number of times the waveform crosses
zero over a span of time linear pretty
prediction coefficients there's a family
of features and so to give you a couple
of examples sort of an intuition for how
we might see structure via these
features here are a couple of
spectrograms and the frequency scale is
logarithmic so their log spectrograms
similar equal to something called a
constant Q transform and you can see on
the left we have the Beatles song hide
your love away versus a Chopin waltz and
you can imagine we're seeing information
having to do with pitch we're seeing
information showing there's a voice
present on the left and not on the right
and we can we can imagine that we can
learn some things from from spectral
features like these caps tral features
move back from the frequency domain into
the time domain and actually what I'm
showing you here are auto correlation
coefficients which are very similar to
capture coefficients we're just dropping
a log transform and we see temporal
order now we see where rhythm where
rhythm lives what tempos are active in a
piece of music etc and so to give you an
overview here hopefully illustrative the
song Pink Floyd's money on the top we
have particular kind of capture all set
of coefficients male frequency capture
coefficients which are very popular in
speech we have auto correlation
coefficients and on the bottom we have
the spectrogram and what we can see is
different kinds of musical structure
appear in different in different
representations so the first one I think
the easiest to see is the middle one the
auto correlation coefficients are
tracking slow time scale change on the
order of rhythm and so what we see for
those of you familiar with money it's an
interesting song it starts off in 78 the
meter seven eight and then switches to
44 in the middle and you can see very
clearly the switch from one meter to
another showing up in the
autocorrelation at the same time in the
spectrogram we can see information
having to do with with pitch and with
changes in instruments that are mirrored
in the mfcc which actually can be
thought of as the low order values of
the autocorrelation and so for example
we can see here at around 97 roughly 100
saxophone solo starts and we see the
evidence for that saxophone solo in the
mfc CS and in the spectrogram but we
don't see that at all in the
autocorrelation the autocorrelation is
invariant to tambor whereas these other
features are picking up tambor and are
invariant to timing changes and so we
can see that we can cover sort of the
musical range of rhythmic will time
scales with certain features and we can
cover the fine time scale necessary to
capture tambor and if we mix these
features together carefully we might be
able to learn something about musical
structure so for those of you who do
machine learning I hope there are some
of you here the first thing you might be
thinking is yes but how on earth are you
going to learn anything from such high
dimensional data one minute of CD audio
yields approx 5.3 million values per
minute
if we do a 512 point spectrogram
computed with 50 millisecond frames we
get eight point six times compression so
something over half a million values per
minute if we use something tighter like
the just the 13 point mfcc coefficients
which tend to be very useful for speech
then we're down to having 15,000 16,000
values per minute but this is still a
lot of data if what we're doing is
trying to make a prediction about a song
over about 30 seconds of audio and so we
ask the question how do we how do we
find a lower dimensional feature set how
do we find a smaller feature set that
still retains some of the some of the
important information so our method that
was published in 2006 and machine
learning is to do a kind of feature
aggregation we weren't the first to do
it but we were one of the first to
actually look carefully at what size
segments work well for for music and so
I for example these are the mfc cs4 the
same pink floyd money' song and at the
top are one second segments so it's the
the mean of each value over spanning one
second and then for a second eight
second 16 second segments and it's also
useful to keep the standard deviation
around so we fit a Gaussian to to every
row of of that matrix over particular
time span and actually we can do more
complicated things like fit mixture
models I mean any dimensionality
reduction algorithm we want to use we
could use here but for the moment we're
talking about something very simple just
fitting independent gaussians and that
raises the question of what's the best
segment size and to decide that we need
to have some sort of learning framework
so what we're gonna do is talk about how
we might learn and once we can learn
we're going to talk about how we can
decide what's a good segment size one
thing we notice ok so how do I motivate
this spectral features that capture
tambor like is it a guitar or is it a
flute or is it a human voice are really
useful only over relatively short time
scales
because that's the span of an you know a
musical note or someone singing I mean
if you take an entire song and look at
these kind of tambor based features what
you see is flute plus Oregon plus drama
plus you know you know pull out any of
the individual instruments nor do you
you nor can you pull out enough
structure to make reasonable predictions
so what we're going to do is train a
classifier on individual columns in this
case individual segments of means and
standard deviations and then we're going
to vote over the predictions of these
individual classifiers that work across
segments and we're gonna vote and take a
winner okay and what we're gonna ask
we're gonna ask the question well what's
what's the right segment size right if
the segments are too big right then then
we start to lose detail we don't have
enough segments with which to vote if
the segments are too small then maybe we
don't have enough data to learn anything
from individual segments also we have an
explosion of data we have too many
segments to to learn effectively and so
actually there's a pretty good answer
for this we tested a range of segment
sizes 44 feature sets Fourier transforms
real kept strong coefficients but it's
not important what the the features were
and we tested for different learners so
we moved we don't have a bias for which
classifier we used we used a de boost
using two different weak learners stubs
and stumps and trees we used a standard
neural network and we use the support
vector machine on a thousand song ten
class data set and then what we did was
we trained each segment individually and
then we voted for prediction and the
result is basically that the segment
size between three and eight seconds is
optimal which is a nice value to have
laying around if you're you know when
you come to this problem later and what
you can see is that if you're looking at
the accuracy of individual segments
actually at least up to 30 seconds you
do get a gradual increase of of
predictive pneus as you look at more and
more data so what I said about needing
to keep the segment size small at least
for mfc sees doesn't seem to hold if you
have lots of data you should look at it
however if you can vote
if you can look at multiple segments and
vote on them then we start to see an
optimum up here so this top row is
showing the accuracy when voting across
all segments now if we have one big
segment or a few big segments and we
vote across it we actually perform
significantly worse than if we have say
10 3.5 second segments vs. 1 30 second
segment and there is a point where it
falls off so for example one point eight
second segments simply aren't predictive
no matter how many of them you have and
this basic curve holds for it with
different degrees of goodness-of-fit
holds for all of these feature sets and
all of these learners though they don't
always all look quite that clean so now
I'm going to move on from feature
extraction and close our feature
extraction with the basic idea that
where we started for our research was
trying to take the simplest possible
feature set we could some spectral
features some capital features and a few
others for different simulations and try
to use a particular form of feature
selection to find out which of these
features are the most predictive ok as
opposed to doing a lot of engineering
and looking for exactly the right
feature set up front so how are we going
to do this feature selection we're going
to use a very standard mechanism called
boosting and in this case we used a
version of ADA boost adaptive boost from
friendship here 95 and the way boosting
works is we build an initial model using
a single weak learner and in our case
what a weak learner is is among the
simplest learners you can imagine we
take we take one of the values from from
the feature set for all songs whoops I
went back too far we take one of the
values from the feature set for our all
songs maybe mfcc coefficient number 4 ok
for all the songs in our training set we
sort that that list of features and we
tried to see if by cutting that list at
some point we can perform
so what can we do if we say if all the
values less than thirty seven point four
in this feature set are greater than or
less than would that give us a
classifier and of course as long as we
perform a little bit better than chance
it doesn't really matter being sixty
percent wrong is as good as being sixty
percent right because we can always flip
the sign for a two class classification
okay and so what we do is we we
iteratively grab a weak learner we
identify wrongly classified points in a
data set and then very importantly we
boost these points so that this the
points that we were unable to classify
are going to receive more attention
weighted by a weight for every data
point for future for future weak
learners and we continue adding weak
learners until we've reached an error
criterion and so you can see here a
simple example of points drawn around a
circle you can see the equation and our
first cut does as good of a job as it
can from separating red from blue and it
increases the probability it increases
the weight of misclassified points and
after many cuts we see that we have only
a few points left ada boost and related
boosting algorithms end up having the
nice property of being large margin
classifiers and also given the way that
we're selecting these learners such that
each learner is tied to a specific
feature we're doing feature selection
where are our criterion for selecting a
feature is based upon minimization of
empirical error so our feature set when
we're done is going to be in order based
upon our ability to minimize empirical
error on our on our training set and so
what we can do is analyze the feature
set and remove features that don't help
us we haven't automated that but for
example we were working with a
particular kind of feature called chroma
grams where we look at pitch on the
musical scale and in running a couple of
simulations with the reason to be large
data set none of these features were
being selected as early candidates by
the booster so we said well it's not
worth bothering with chroma grams and we
throw away a number of points
in the interest of time I'm going to
skip the full algorithm for ADA boost
it's not my research I'm using this as a
classifier and in fact I'm agnostic I
like the properties of a debus but I
think that a next step is to design an
algorithm that actually takes more into
account that some specific constraints
having to do with how how how this
problem is defined so I'll skip that so
a couple of other observations I've made
some of these already the first two oh I
see the third one should say that it's
slow to converge relative to s VMs
somebody some some program made a spell
check correction for me thank you but
it's in linear complexity with respect
to the data set size that is we don't we
don't have a problem with explosion in
terms of large data sets so we can use
boosting to look at huge data sets we
don't have a guarantee that we can
minimize error with the same number of
boosted features that we can for small
data sets but we also don't have this
hard limit that we see with svms and
also to learn multiple classes for
example to learn one versus all genre
let's say we have 12 genre and we want
to predict which genre a song falls into
we can do that using a particular form
of ADA boost which builds independent
weak learners one against all so it's
one learner is trying to learn rock
against everything else another learner
is trying to learn jazz against
everything else and then it's it's it's
combined as it's built into the
algorithm and frankly as i said i'm
rather agnostic about the specific
classifier we're using we're using a
version of a Debus that is known to have
some problems especially not being very
good with noisy data so I'm convinced
that even within the framework of
boosting if we continue to follow the
literature we can we can improve that
okay
so what we have sorry yes
where you
it's an excellent question so the
question I asked to repeat the questions
microphone so the question was when
voting takes place is the voting
insurance part of the boosting algorithm
or is it something that's happening
after the fact right the answer is that
it's not an intrinsic part of the
boosting algorithm we're voting using
the predictions of the weak learners
okay and furthermore it's a very
interesting director for future research
because in fact if you take the segments
and treatment is statistically
independent and then vote on them we're
throwing away a lot of information and
in fact one important piece of
information that you throw away is but
say you have a four minute songs labeled
wrong and so you have all these segments
right and it turns out that eighty
percent of their voting rock but twenty
percent of them keep loading classical
because in fact there's this stupid
classical playing remodel right and so
what you have is in standard boosting
least it treated as hard points right so
so their their weight goes up and they
get even more attention and so what
we're looking at now are ways in which
to propagate information from second
assignment and we're already trying
most simple method which is literally to
compute the weights for segments and
based upon a weighted score or even a
standard mean of the weights for all the
sentence right so we have a wait for the
public upon which is a sudden and
instead of looking
the trooper wait for that store has the
weight of the difficulty of the entire
song and that actually does help but we
haven't had time to to test it why limit
to know how well about and I actually
think there are better ways to do this
where we actually redefine the cost
function of the problem to say well our
cost function is not the return the
minimize however on a segment we're
going to minimize their on some you know
we want to have some some number of
segments in bag you know workflows the
question of conversion
so
so I'll talk about some results keeping
idea of what this approach can do by
this approach I mean extracting feature
some audio and then training boost
the first task comes from 2005-2006 work
done with my students predicting genre
and artist so you're giving the genre
and you're given the artist for a number
of songs and then on a testing set you
need to try to predict songs by the same
artist or songs tamil sanjana obviously
it's a classification problem so you
can't bring in a wholly new artists and
forget different because it was one of
the things you're classified so we
actually need more features for for
these experiments and for those of you
that are familiar you can read the list
and photos to be that aren't familiar
with these kinds of features can safely
ignore the specifics and what we found
was relative to well some of these some
of these our nearest neighbor approaches
are quite old our approach for very
popular test dataset is as far as I know
still say they are anybody sound better
than 83 in fact this is right about
where people are sitting around eighty
percent on this data set if you ask
people to say what genre is some sitting
the task is to take songs from a
thousand some data set and say is this
rap or is this
and you can't say it's half wrapped in
half country and so you know people
disagree about what's popping let's rock
I think more importantly this approach
when tested in there there's a contest
every year at music information
retrieval conference club is Nia called
lyrics machine information retrieval
experiments or something recap and they
had an artist identification contest in
2005 and our approach cater boost the to
eat a moose approaches one with the
stumps i described another using simple
tree building approach that one
described now perform very close to Dan
Ellis approach and actually performed
better than planets approach on
magnitude and dance is significantly
better on the US pop dataset dance a
friend of mine so I don't feel bad
telling you but even he knows he /
fitness is except
they asked them to use it for the
confidence like you have posted it
better and he was talking to now date
setting a week sooner our hyper
parameters on us hot so I think
effectively their the roughly good one
this is important to me because they
stuck a lot of time choosing the right
girl and spending time thing about the
right way to dispossess gas and since s
stands are clearly I mean they've been
state-of-the-art for sort of standard
classification for the last 15 years
it's hard to come along with something
like boosting and not compare test hands
and said you know you're in the same
field I like our boosting framework
because of its ability to scale of the
huge data sets and because of its
automatic feature selection but it's
nice to demonstrate that a serious and
careful use of SBS yield similar results
and in fact the same day was used for
predicting genre the distinction is I'm
going to flip back to artists for ours
we had 77 artists and for John where we
only had 10 genre for one day decide at
six honor for the other and now our
approach both of our
approaches significant ed all of the
other approaches on that the problem
with this task is is both of these
taxable hearts prediction the Java
prediction one is that they were they
were defined as one versus all tasks so
as I said that father hid in 1 John
which is this kind of stupid right and
also we don't want to protect artists
right I mean all the information were
very likely i have from the mp3 itself
is going to be the name of the errors
why don't you just mind our critically
what we want to predict our more general
information so we want what I want to do
is look inside the sound file and pull
out a nice big factor of information
that I know can be used in a supervisory
framework to predict some things and
also be using supervised learning and
probably some some dimensionality
reduction I can come up with an even
shorter vector that really does a good
job of encapsulated that sort of
donations have changed for the musical
space in which a song was sick so what
we looked at was using as as labeled
data some some social tags so last.fm
via its audio stronger interface
researchers to go and and do data mining
and collect lots and lots of tag and so
we collected lots of tags for I think
yigit's on the next slide something like
some something like forty thousand
artists and their song and just to give
you a feeling for what's out there
seventy-seven percent of all of the
tactics have to be with Jean root or
instrumentation these are all things we
can we can predict with acoustic
features some of the things we can
probably predict very well or things
like locale which is surprisingly about
it well this night I mean there is a
Seattle sound right but a lot of this
stuff is you know puts a band in Iowa
that's using a Seattle sound we're not
going lower it's from I and then some of
these other things like opinion or
sighs such as please I already when
we're not going to predict my trip some
consider these tags like seen live or a
useless for us right so someone has seen
a lot on it we're not good at that here
the top 20 tags applied to the group The
Shins who I hope you all know and love I
know love I saw them just a few weeks
ago so you'll see actually some
frustrating things if you're trying to
pull things apart engi get any rock we
have any path we have alternative we
have rock not entirely descriptive tags
and very popular tags I'm hard put to
have someone defined for me what's the
difference between indie and alternative
without just using examples of plans so
what I should stress here is that the
goal the task here is to predict these
social times right but that's not the
long-term goal right the long-term goal
is to extract something from music it
tells us what's inside the audio we're
using the social tag construct as a way
to get an idea of how virgin
and in fact even that I'm going to talk
to no hablo Jennifer let's take a look
so the first thing we have to do is
decide how on earth we're going to learn
something from these tags so let's take
a look at one of them this is this is
the histogram of the tag rock and what
we're seeing is a histogram this name is
the number of bands that had no label
for rock so there were cute i'm saying
before right and then we see okay let me
see like so this is how many bands had
10 labels for rock and at the end here
we see a number of bands that have 30 or
more labels of rock and all of this is
difficult to normalize because we have
some bands that have massive numbers of
labels and others at down and there are
some normalization schemes that we use
that are to our King described here but
i'll let you know they took care not to
to do kinds of normalizations that break
ranks and I would still say it's a hard
problem but what you can see is that or
what I hope you can see it's easier to
see on some other other tags is that
it's not a clear distribution where we
can say okay
this is a Bible or time a little
distribution very jagged okay and so
it's very unlikely that we're going to
be able to predict the actual number of
times an unseen song would have been
tagged right because some you know
popular bands like amigos like tens of
thousands of tags are in popular bands
yeah a few tags that they both are good
examples of particular thing look brah
so what we decided to do instead was was
built ins we computed whether a banner
should fall into a kind of no rock
category where it had very few tags well
to the other tags some rock or a lot of
work we did that for all the tags we
simply didn't use any of these and I
need these algorithms for choosing our
bin boundaries to maximize our ability
to do classification we simply try to
divide each each of these groups of tags
such as we have roughly equal numbers of
no rocks some rock and a lot
we did it by just competing a hundred
billion under debate histogram and then
ticking across bins until we had grabbed
a third of the data than a third of the
data in a 30-day sedation and in fact
for classical that was impossible
because there just aren't that many
things labeled classical so we had we
could only use two bins everything else
was it was basically non her all okay
and so we we used a simplified feature
set just mfc sees log spectrogram
coefficients and auto correlation
coefficients we're going to learn each
tag independently okay so we're not
doing what we did was jean r where we
say something is either rock or country
it's a lot of rock maybe it's some
country maybe it's a lot of jazz maybe
it's no classical and and then later
we're going to use the magnitudes of the
weak learners to to form more graded
predictions that is instead of just
pulling out from the learner a lot of
rock we're gonna actually look at how
strongly the learners voted a lot of
rock and use that as a rough measure of
how confident the learner is this is a
hack and it's also something else that
needs to be better integrated into the
cost function of what we're doing but in
effect the distributions don't support
the kind of regression we'd like to be
doing nor is there enough data to really
do the kind of ranking we'd like to be
doing so in a sense we're doing the best
we can with with with noisy data and you
can see what we have here we grabbed 13
tags all of them purposely to have to do
with genre so that we could compare them
to one another there's no limit we could
we could look at music for jogging or
whatever we want to and we had finally
43,000 songs to learn from and a few the
artists we had to throw out they had no
tags in these 13 classes and then this
left us still with with quite a few
artists and and what we see is that we
don't learn so these are the error rates
for three class learning except for the
one that's marked with the arrow so the
our ability to take a song in a testing
set an unseen song and say whether it is
none some or a lot of say jazz okay so
the error rates for song
are all sitting around sixty percent of
course chance would be thirty-three
percent and the classical one is much
higher but recall that it's tube in not
three been classifications so that
ninety percent is not comparable so this
is testing error using one fold a
fivefold cross-validation so basically
by that I mean eighty percent of the
data was in the training set twenty
percent was in the testing set and we
just let the model run and choose up to
2000 weak learners that doesn't mean it
was done that's just when we stopped the
simulations and we're not google so we
didn't have you know I'd love to let it
run longer I mean it wasn't finished
it's just where we stopped so we could
get the paper out the door okay and
probably more useful than looking at
error rates these are the top 40 okay so
these are this is as meaningful as you
want it to be you can look for bad
examples or good ones but these are the
top 40 classic rock bands as predicted
by the magnitude of the lots of rock bin
okay and what you see is that there are
some largely there are some good
examples of classic rock like
Steppenwolf and Creedence Clearwater
Revival by and large I think it's pretty
good especially if you go down and look
at reggae and compare and then there are
some some really bad examples not
surprisingly as well but by and large we
could have computed also goodness of fit
on ranked order but the data is so noisy
all we're gonna get is some number right
so I'm just showing you these as a kind
of idea of what we're able to do the
electronic worked better but that's not
surprising because for those of you that
are electronic fans electronica I should
have put you'll see quite good selection
of electronica and the reggae is also
interesting because we didn't actually
have a lot of reggae stuff in the
database and so it selected I'm for
reasons I don't understand Bob Marley's
not in there for those of you who were
looking for Bob Marley but you know all
of these are reggae bands we even come
down and pick up bands that have reggae
songs like the police
at the same time I don't want to go out
of my way to highlight successes we can
also analyze and find really bad fits
like well I don't think X is classic
rock for example but by and large we are
able to to learn a lot of this structure
it's also may be interesting to look at
some of the examples of these feature
vectors now now I just pulled out seven
of the other I'm almost done I want to
leave room for questions I pulled out no
I'm sorry eight of the genre just to
have them all fit and I grabbed some
some examples I didn't show you jazz and
classical by the way because those
always work there doesn't exist I mean
whenever you look at these results from
predicting genre everybody can predict
classical and everybody can predict jazz
so my list of classical and my list of
jazz musicians is just perfect right
it's just easy right so I didn't even
show you those but you can see for
example number one Gustav Mahler is
positive classical and I know I marked
the negatives in red- everything else
wilco another band I like now you have
two points of data you can predict my
musical tastes the shins and wilco they
show up as Britpop which is kind of
silly but they also show up as classic
rock electronic that's one of you know
and then strong indie and rock and then
there's some some serious failures like
tom waits on the bottom if i'm not sure
which discs were in there but for some
reason we think Tom Waits is jazz which
is it's very seriously only partially
true right it's just only partially true
another thing that i want you to notice
is that these magnitudes because we're
not our cost function does not it's not
we're not learning a regression function
we're learning these bins these
classifications right there's no scaling
at all to these values this is just this
this is just the weight the strength of
the sum of the weak learners how
strongly they predict whether something
Falls or doesn't fall into this bin and
so we say that Gustav Mahler is minus
three point one four or five Britpop and
that the Beatles are point zero
one Brit pop those values are comparable
but we can't compare that to the
magnitude to the column to the right we
could adjust them we could pretend that
they're Gaussian we could adjust them to
have a mean zero standard deviation of
one whatever we want to do but in the
end these are intrinsically not not
interpretable as as magnitudes this is
just to show you what's going on finally
to show you some nearest neighbors if we
use those vectors actually all 13 of the
of the genre and we look at nearest
neighbors we can for example the last.fm
tags to give you something to compare to
I should have shown you the top 44
last.fm as well to compare I didn't
didn't have time phablet fatboy slim
shows up close to the prodigy and
basement jaxx and apollo 440 we also
have apollo 440 and beck which is a bit
strange but chemical brothers is okay
the beatles for last.fm shows up next to
lenin and the Beach Boys and the doors
but we have them near the rolling stones
and Marvin Gaye and Eric Clapton so
there's a kind of reasonableness to
these my point would be in all of this
is that we're certainly not going to get
the right answer with quotes around it
using our learning right but that's not
what we're trying to get well we're
trying to get her some reasonable values
for unlabeled music and also some values
for for the kinds of information that
doesn't tend to be labeled by users so
we can smooth things out I mean we have
the web we have all of this textual
information anyway we're just trying to
get some values that seem reasonable and
that are useful for describing musical
content and I think to that extent these
very preliminary experiments show that
show that it works finally if you want
to see a little bit of there's no
there's no multi-dimensional scaling or
anything else this is just the values of
the weak learners just as I as I showed
showed you in the red negatives and and
black positives or the previous slide
you can see where some bands show up we
couldn't show you the entire data set
because it would have just filled the
screen with black but you see as always
classical clusters together just like it
should gets itself out of the way so we
can look at other things
and we see kind of reasonableness as we
move up in rock we see rock bands show
up as we move down and to the right we
see serious electronica show up and then
the mixture of stuff in the middle what
we want to see is the same kind of nice
clean low dimensional space when we look
at all of these these classifiers
together and you know we tried it and we
get kind of nice 3d clustering where the
jazz sets over here and and the
classical sits over here and then
everything else is kind of sort of
working but in the end that's not how
that's not how to do that kind of
visualization we're not there with this
model this model wasn't designed to do
that kind of visualization and I think
if we want to do really want to do 3d
visualization of music we need a cost
function that is better suited to
preserving the kinds of distances
between near neighbors that we need to
do visualization that said I think
there's a kind of reasonableness that's
that's preserved it certainly looks as
good as the inner you see these kinds of
graphs and music information retrieval
papers all the time right if you if you
read these sorts of things and this is
as good as all the others whatever so my
conclusions what I'm trying to propose
or what I have proposed is what I think
I think is a simple framework for
machine music listening we extract audio
features we segment those into chunks
and then we do ensemble learning on
those chunks to try to predict some
social tags where this can be
interesting I think where this could be
interesting for i mean i think the model
as it is could be interesting for a
company to jump down a little bit for
companies like last.fm or pandora the
music recommenders but where this
becomes interesting for a company like
Google is when you start to say I'm not
only worried in the kinds of features
that are good for selling music to
people I'm interested in having a huge
vector that I can then map on to text so
that i can generate a bag of text that
sits with every audio file okay
the kind of bag of text that you
generate to sit next to every audio file
on the web for something like Google for
a search engine may not be the same kind
of bag of text that you want to sell
music or to recommend playlist for
people so what we could talk about doing
is instead of doing these 13 tags the
idea would be to learn lots and lots and
lots of things about music you could you
can think about learning quite with
existing signal processing techniques
and learning to extract you know 100
different the probability that a given
musical instrument pulled from a huge
number of musical instruments other
female voices are there male voices and
then lower level statistical measures of
how the music was recorded its source so
that what you can do is generate a large
numeric vector and because you've
trained on text tags have a reason
reasoned way to go back into the space
of text which of course is where Google
strength lies is in doing text based
search okay there are limitless ways to
improve this this is very much a first
step first we're doing classification
over some nun saman a lot but what we
really want is regression or ranking
because that's what we want in the end
is to look at how these relate to one
another in terms of their magnitudes so
it's clear that we need to do something
else with our learning we also want to
try to do some regularization so that we
can take advantage of the idea that as
as you rightly questioned are our labels
are done on the level of a song almost
all of our information that has to do
with music is information relating to a
song or an artist yet the descriptive
quality that we get from audio feature
extraction sits on the level of frames
of data and so we have a mismatch that
we could actually exploit to speed up
learning finally we have a number of
features that are derived from a very
high level of looking at at how how
audition works and how you know how
music works these are what could only be
called the simplest possible features
you would extract for an early start and
and I see two directions to move with
feature extraction one of them comes
from looking more closely at
how rhythm and how pitch are represented
in the audio in the auditory pathway and
also in using unsupervised
dimensionality reduction algorithms for
extracting features that are predictive
for a particular task and also just the
idea of saying if we have the
computational resources just extracting
lots and lots of features but finally
with this what I think is relatively
simple machine learning we can in fact
listen to audio and learn a lot about
musical content and we can in fact embed
songs and artists in a rich space we
don't need to have all of the
collaborative filtering or expert based
labeling that we get from last.fm and
pandora this works it works with
state-of-the-art machine learning and it
could work better and they're clear
paths for getting there but I think a
hint to Google this kind of research is
is important for companies that want to
know what's going on with music on the
web and and so that's why I wanted to
give this talk and i think that's it
just citations this was just submitted
to izmir some of this this was a machine
learning and another is mere paper and
there's my email address so thank you
for your your your patience
so you've talked about extending a few
properties of the auditory system what
about more properties and please yeah so
the question was that I talked about
using properties of the auditory system
what about more properties of music yeah
in fact we were talking about this at
lunch by and large the kinds of things
people label our tambra based I mean
tambor features tend to work really well
for things like genre and I like to jog
to it but there's a whole class of
similarity measurements that people make
that we can't do with low-level features
and the most important one I guess the
most illustrative one is that of cover
song recognition so you like a song and
you want to you want to hear other
versions of that song right to answer
that much I ask you have a web page
about this with a bunch of different
versions of these are a few of my
favorite things the the sound of music
song that was then done by John Coltrane
as it turns out if you go to iTunes
there are like 50 versions of this song
right these download the 32nd snippets
and there's no tamara similarity there's
not even rhythmic similarity if you
concern if you consider that the John
Coltrane version is in 68 and the
original version is in 3 for the
different instruments are used some of
them are Barry Manilow sings a song the
you know I mean all of these things yet
for people listening there's a clear and
obvious similarity between all of these
songs they're the same song right to
recognize this we need we need actually
need both we need a clear understanding
of how people are tracking pitch and are
extracting this long timescale
information that has to do with with
with melody and at the same time we need
the musical constraints so the way I
think about this is is we need what in
speech recognition would be a language
model we need to have some higher-level
constraints on the kind of learning that
we're doing furthermore we start to
treat music than like a real sequence
which is what I really like doing I mean
right now we're taking sequel we're
taking frames and throwing them in a bag
and shaking them up but if you want to
talk about a cover song or something
like that then you have to really care
about how the music valves over time and
I think if you move into that area then
you have to care a lot about how music
works
there was another question yes do that
is showing all the values into your
okay so the question was when we
optimized for the segment size and our
segmentation would it also have worked
to throw all of the segments into the
booster and just let the feature
selection of the booster pick out the
right segment size in principle it would
work to throw them at the booster in
practice when you're not Google you
don't have the computational resources
to do that so when you move down two to
one or two second frames and you have a
large data set you've got a lot of data
also this this feature is this the
feature extraction so the feature
selection part of boosting what we're
doing something really naive we're
treating a feature as a single value
from an fft or a single value from from
an autocorrelation so it's very easy for
the booster these are very very weak
learners and so in the end when you
analyze the the order of of the Boosters
a whole chunk has to fail before you can
say oh right i can throw away this
feature set so what I highly suspect is
that the booster would pick it would
like MF CCS so we'll pick em FCC's from
all these different time scales because
they're nice the very descriptive for
Tambor and then it would start picking
spectrogram coefficients for all these
different time scales and you wouldn't
really see a clear preference of one
time scale over another or it wouldn't
show up in the first five thousand weak
learners right so in practice it might
not work at all we could try it be cool
if it work
yes the question was what about movies
this is an area of great interest for me
and also some people in the lab where I
am at University of Montreal yeah you
know I I think of I think of music
because it's where a lot of my training
comes from and I actually think that
music is a really cool playground for
learning about how to treat video
because you have the sequentiality to it
and once you've understood video once
you've understood this time sequence
then you already got tools for for
extracting information from from other
time series and so I mean I think that
there's a there's a kind of lifting
property it would take a lot of this
research and map it on to video I think
what would get popped in and out of the
features and you know good Jillian's
more cycles of processor time I'm also
interested I mean just in terms of my
research I'm interested in these all of
this new research is happening with you
know you know multiple streams of
information and multimodal learning and
you know ways of trying to bring
together these different information
streams and pull one off of the other
other questions
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>