<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2011: Web Consistency Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2011: Web Consistency Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2011: Web Consistency Testing</b></h2><h5 class="post__date">2011-10-31</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_6fV-6eMSUM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay our next presentation is called web
consistency testing it's a it's a
tradition at gtech to have a token
founder we don't care what they found
just a founder of something because just
in case you know they're they ever make
it just ultra wealth we're like hey man
remember that gtalk invitation way back
when and if they fail we can ignore him
an active act like it was just a big
mistake so Kevin Kevin mentor it is the
founder of mogo tests so he is our
founder and he's done a lot of work and
apache and open source cayenne and
tapestry projects selenium and the
rubber Capistrano plugin I'm going to
have to wear have to talk about that one
over a beer Kevin Menard all right hi
going to talk today about web
consistency testing don't worry if you
never heard of it before I kind of
pulled an old consultant trick and just
completely made up a term but web
consistency testing is new entry into
automated web testing in order to best
describe what it does it helps to frame
the context for this a little bit more
so web testing is a pretty diverse space
but automated web testing basically
falls into one of three categories you
have your back end testing typically
unit testing your data models functional
testing your ural endpoints that modify
that data you have front end in this
usually manifests itself as validating
either your HTML CSS and more recently
we've been seeing some automated testing
of JavaScript and then we have intent
and integration testing again this tends
to be a two-prong approach we're seeing
a lot of collaboration between browser
automation tools and the browser vendors
but as great as that is things get much
more stable much faster there's still
very terse tools so it helps to have
something layer on top of that and we've
seen kappa bar and cucumber and a few
others try to
develop these acceptance tests all right
so this is your typical automated
testing diagram I felt kind of silly
putting this together but it was more
rhetorical than informative notably
absent from all this is the user so
we're really good at testing how a
browser interacts with the server how a
server interacts with the database or
other services the NN integration
testing and the workflow side kind of
models what a user will do and we can
handle things like a user clicking a
button but the primary way a user
interacts with a site at least initially
is viewing it web sites are primarily a
visual medium and I apologize for those
that are visually impaired but my
understanding is accessibility testing
really isn't considerably better we care
a lot about how the page looks this is
what we spend so much time on design if
we didn't we just adopt the Terces set
of widgets we could to to perform
whatever functionality we need to but no
we we do care about design it defines
the site it enforces company branding in
to to ignore how a user views this site
is kind of a big problem you know put
another way it's great if you can test
that clicking a button triggers an event
handler which fires off an AJAX request
to the server which parses the request
interacts with the database generates a
JSON response kicks that back over the
server it catches it and its success
handler or modifies the dom but if you
never see the button then it will never
get clicked or maybe it's partially
occluded or maybe the page just looks
like crap and as a result the user just
moves on so what we're really talking
about is how to test that a page
conforms to a design and there's a ton
of different ways that this whole
process falls apart I jotted off a quick
list here but I'm sure we've all run
into a case where a page just doesn't
look the way we expect sometimes the
developer just doesn't care maybe they
decided they don't want to work in a
particular browser
sometime it's a lack of skill sometimes
the designer just hands over a design
that doesn't work a lot of print guys
coming over to web or the guilty of
doing this we've all dealt with looming
deadlines and as a result we cut corners
and then some of the more nefarious ones
or the the web specifications are
ambiguous in some cases and we've all
experienced how the browser vendors
handle that situation very poorly sit
then we have browser vendor extensions
anyone who's tried to round a corner in
the last three years has probably found
75 different ways of doing that
gradients fall into that as well and
then some of the other stuff is the page
actually works conforms the design but
there's dynamic content but we introduce
a new translation and now you're
beautifully designed page that work
great in English falls apart and German
are Swedish maybe the design only called
for about two paragraphs attacks but
it's backed by a CMS a copywriter comes
in adds a third paragraph and everything
falls apart unfortunately the testing of
this falls on a spectrum kind of between
a rock and a hard place on one hand
could just you've no testing whatsoever
and just ship things and for a lot of
smaller companies this is what they do
or they decide they're not going to
support twenty percent of the browser
traffic because it's just too difficult
then they'll spend the next 18 months
a/b testing the conversion up half a
percent well ignoring that other twenty
percent but anyway or you manually test
and this is where you pay someone to
open you know hundred or a thousand
different pages and eight different
browsers and manually compare them which
is one of the largest wastes of human
talent I've ever come across not only
that but midway through the human brain
just says screw it and shuts down and
the process is fairly error-prone so
most sensible companies will fall
someplace in between well pick the 10 or
15 landing pages that are most important
and will target the top 90 or 95 percent
browser usage and we'll check that the
pages we care about look good in those
browsers we really should be doing this
on every deploy but that that typically
doesn't happen but it's it's a difficult
proposition it's so expensive but we've
all experienced when that falls apart
you know how bad that is you you're
embarrassed by it it could cause loss of
revenue it's just a bad situation so
naturally why don't we try to automate
this and this is a question I originally
posed to myself maybe five years ago but
at the time I wasn't really ready to
tackle it and over the years I've asked
my web developer friends that are a
little more in tune to this than me why
don't we automate it and there seems to
be this conventional wisdom now that if
it were doable it already be done so
therefore it mustn't be doable because
why would anyone have the solution to
this and not share it so when you peel
that away you know say well why don't
you think it's doable and for a lot of
people it boils down to the web page
looks as a usability concern it's an
aesthetic thing you can't test
aesthetics and it's true CSS helps us
define the styling for a page but pages
do have a logical structure to them we
have a header we have a footer we have a
sidebar content areas comments some of
us will take it to the next level one
will have semantic class names and ID's
that call this out or we'll use the
structural tags in HTML 5 when you kind
of look at it this way what we have is a
problem domain and we have a language
that defines data transformations and
what we get is a resulting output that
we want to verify his correct and when
you look at it that way it's really no
different than any other type of
automated testing it's a little more
complicated because your your domains
within a browser the language isn't
really a programming language is the
descriptive one and then you actually
want to test it you need to use a real
programming language but if you're
minish to to make that logical leap then
the remaining question is all right so
why haven't we done this
and one of the biggest reasons is it's
been traditionally very cost prohibitive
when I first started thinking about this
you know five years ago cloud computing
hadn't quite taken off and to get going
was going to cost tens of thousands of
dollars in equipment nowadays these
tests could be run for tens of dollars
and the trend is such that costs will
continue to go down so it's not as
dismal as I've painted it we have seen
some work towards automating this
particularly two years ago Michael Tam
gave a wonderful talk at gtech and
Zurich about his open source fighting
layout bugs library and what he was
trying to do was provide a tool that
work like fine bugs where it would parse
page and handle a common set of
rendering issues such as low text
contrast or texted over you know
collided with div boundaries content
regions that overlapped and this worked
pretty well if you're willing to work
with in Java or the JDM incidentally
around that time I decided I was going
to take a stab at this and apparently
I'm not very good at research because i
didn't know Michael was working on it at
all so I I kind of went about it a
different way and the result of that is
basically what I've been calling web
consistency testing and it complements
Michaels work pretty well and the two of
them can be used in conjunction with
each other so what the consistency
testing is all about ensuring a
consistent user experience regardless of
whatever browser device they're on more
colloquially it's about answering the
question does this page look right this
is a intentionally fuzzy definition
which is a little weird when you're
trying to talk about automation but
pretty much any human can handle this
pretty well so that's really what I
wanted to do as a simple example here's
a page i looked at recently that has a
some news entries in a sidebar these are
two pretty modern browsers we have
chrome and ie 9 and the differences here
are quite subtle you don't see him
you know some of the font waits a little
different we have cases where text is
wrapping around in weird places but in
isolation looking at either of these
pages in that browser I wouldn't see
either of them is wrong as a matter of
fact I'm not actually sure which one's
right you know traditionally we've tried
to strive for pixel perfect design and
that's a fool's errand here any effort
to try to make these look exactly the
same is just absolutely a waste on the
other hand here's another example where
a page looked at in chrome and firefox 6
I took these slides before firefox seven
came out just glad it didn't come out
before the talk in you know this case we
have a form that wrapped around to the
next line and it's a bit jarring I
definitely would think something was
wrong here in as a web developer I would
want to know about that in this
particular case the issues fairly minor
but we've all encountered cases where a
form element is a wrapped or div is
wrapped and it causes the rest of the
page to end up completely out of whack
so what I really want is a function that
that models this process we just looked
at two pages and we discovered there was
a set of rendering issues so that's
where I started with this I wanted to
give some function two pages and get
back set of rendering issues so the two
inputs are paid representations one's a
reference page and this basically
matches how many web developers go about
doing development they'll make sure a
page works and whatever their pet
browser is and then every other browser
gets compared against that and then the
differences get resolved since we're
dealing primarily with how a page
renders there's really two ways that
you'd represent a page one is it as an
image which is basically how a user sees
a page and one is via DOM which is
basically
browser generates the image that the
user would see so there is a logical
equivalence here but you have to do a
little dance to make it work image
representation is great if you're
looking for pixel perfect design there's
a lot of algorithms out there for
comparing images it's a hundred percent
browser independent but some of the
issues I've had with it is when you get
a diff what you see is a set of a region
of pixel differences it lacks semantic
meaning it's it's definitely helpful but
I'm looking at it I don't necessarily
know where to go start fixing things in
that first case we're allowing things to
shift that that becomes computationally
much more expensive when you're doing
image comparisons and then if you have
any sort of slide deck or slide show it
quite easily confused its image of
comparison like in this example we have
what is essentially the same thing on
the page but one is rotating through and
it's on page two ones on page four this
would utterly confuse an image
comparison the dom representation is
actually a fair bit more complicated to
capture and you do have to do some data
conditioning but the benefit of the dom
is with very few exceptions everything
can be represented as a rectangle in a
three-dimensional space and we have very
simple algorithms for determining where
something sits in three-dimensional
space and distances between rectangles
and all that more importantly is when
you're working with the Dom when you
detect an issue you know exactly where
you are in the dawn you have rich
semantic meaning about that your set of
rendering issues that will report will
tell you exactly where a problem occurs
now the beautiful thing about Dawn's are
there trees makes it very easy to
traverse them so when we do this data
conditioning for instance we'll go
through and we'll strip out any element
that doesn't have an actual visual
representation like the head tag
we'll go through normalize things like
the opacity values there's a lot of
browser vendor extensions around this so
in our explorer used to do it through
their filtering mechanism but what we
really want to do is get the opacity
value so you can take a quick pass
through this to your normalization and
trees have a lot of wonderful algorithms
about them on how to traverse them for
the purposes of web consistency testing
we're going to use a heuristic base
depth first search really you probably
you're limiting yourself to other depth
first of breadth first search because we
have to do a completely exhaustive
search there's no opportunity to abandon
search early so looking to use one of
the half optimize search algorithms
isn't really going to buy you a whole
lot others sorry for those who are
unfamiliar with that first search I just
kind of threw up a quick annotation here
but basically you'll move down starting
at the left all the way to you at the
end when you do your bump up a level
move right and so forth and so on as it
turns out depth-first search has some
other interesting properties the Dom
naturally represents containment
hierarchies some of this is enforced by
HTML itself allies always appear in you
else some of it's just enforced by
convention developers if they want that
UL to a pair you know within something
else they tend to shove it in a div for
what have you so this is really
interesting for something i'll get to in
a moment likewise a lot of developers
will adopt this western left to right
mentality which really is a way of
saying that if something appears to the
left of something else on a page it
probably appear as higher in the Dom so
depth-first search allows us to take
advantage of both those properties and
yeah there are some really perverse
things you can do with absolute
positioning that would change this but
those are the exception rather than the
rule and it's fairly easy to isolate
those and deal with them all right so
when we compare the dome we're basically
going to go through and just go
element by element and we're going to
check where it exists on a page and if
there's an issue it squiddy pretty easy
to tell will operate with the tolerance
threshold and it said the left position
between two divs is off by 200 pixels
and that exceeds the tolerance threshold
we can annotate that as we're going
through will apply a few heuristics some
of these were pretty annoying to figure
out hindsight's twenty-twenty but you
know for instance anything with
visibility hidden you just don't even
need to consider completely cut that out
if an element is occluded by another one
so you can't actually see it will still
sit in the Dom but it doesn't matter
where its position because a user can't
actually see it so you can deal with
that that whole slideshow thing that
tripped up image comparison it rears its
head Dom comparisons but in a different
way in particular the slide show itself
will appear in the same position but
each of those panels will have a
different opacity value so you'll just
need to to be aware of that and handle
it what's wonderful about web
consistency testing is we really want to
surface rendering issues not not
rendering differences so taking a look
back at this page here you know it's
pretty obvious that the form is wrapped
around but the the text field is also
wrapped around the submit button is
wrapped around if you're just doing any
naive Dom comparison or doing an image
comparison each one of these things
would be reported as a problem well
what's really the issue I mean when we
look at it the submit button and the
input field are correct relative to each
other we want to report just the form as
being the problem so this is where the
the whole containment hierarchy portion
of depth-first search works out for us
because what we can do is we can
calculate ask you and when we do the sub
search and recurse here we can push that
skew down to the next level and it'll
allow us to effectively normalize the
origin for the
node and then we can start comparing
from there as if there was never an
issue to begin with in what this allows
us to do is if if a page would have
otherwise reported a couple hundred
issues there might only be a half dozen
root issues and will surface only those
issues there's a risk here that you'll
miss a couple couple legitimate issues
but I will never have a false positive
and once you fix these core issues the
other ones will be surfaced after so
this is more of a lather rinse repeat
cycle but the idea is you're never
wasting time trying to track down
something that isn't really an issue so
there are the considerations like I said
we have this notion of a tolerance
threshold if you're looking for pixel
perfect design just set the tolerance
threshold to zero if you're okay with
couple things wrapping here or there you
know you can bump it up empirically
we've found that 30 pixels works pretty
well we've detected about 2 million
issues across half a million pages and
for the most part they've all been been
pretty good one part that trips up this
don't comparison pretty hard is when
there are Dom mismatches so as I was
saying when you have the two trees in
you Traverse Town you're going to do an
element by element comparison if you're
using one of these older versions of ie
and have something like Dean Edwards ie7
script in there is he going to just
inject things into the Dom and you'll
have to do a fuzzy match instead to deal
with this or if you're comparing over
time and the Dom changes subtly as it
turns out just using the same rules that
CSS uses for specifying for specificity
you usually do okay basically you'll
look at the element named look at the
set of classes look at the ID and then
look at the hierarchy rather than just
looking at the position and then there's
a couple other concerns but you know for
instance you'll want to be able to
handle authentication or deal with pages
that exist behind
firewall so further minor things all
together the results here are extremely
developer friendly like I said what
we're returning a set of CSS selectors
we're given the arrow dimension you know
is it often the vertical axis the
horizontal axis is the opacity value off
and we're going to return the error
value so showing a quick sample here
this was that that page with the form
was off and the system was able to
detect that the form you can see the CSS
selector on here actually the form
elements were wrapped in a paragraph and
the paragraph is what was off but by
using that information we can overlay it
right on a set of screenshots here and
visualize the error which makes it
really easy to figure out whether or not
it's actually a problem and then we can
hand off this CSS selector to a
developer and let them know hey this
this thing's off by five hundred sixty
pixels here's another example this is a
typical contact form sitting on the
footer of a page it's off pretty
substantially in the vertical axis so in
the original definition of the function
I said they were page representations i
was using cross browser as a motivating
example but as a seemly turns out this
this approach is actually a
multidimensional in it works pretty well
it basically if you can take two
different representations of a page you
can compare them the same way so another
really great example of this is doing
regression we've all been in that
position where you're working on a page
you want to add five pixels to the right
margin to fix a display bug but that CSS
files linked to by 75 other pages and
you don't really know what the
ramification of those changes are if we
take a snapshot of the page beforehand
and then we make the change and take a
snapshot of all the pages afterwards we
can see whether there were any issues
that were created as a result of that
change and if we maintain full copies of
the code at each stage we can even
present a code diff and say hey
you know you made this change three of
these pages broke and by the way it's
extremely likely it was the result of
one of these three lines of code that
you changed taking a step further you
can even build up internal knowledge
base about this so when you fix the
issue you also have the diff so now you
you know how to fix the problem should
you ever break it again you can look
back and say hey the last time this
broke this is what we need to fix it
another case where we can compare two
different page representations
internationalization so make the page
look great in the language you're
looking for for many of us i'm guessing
that'll be english and then run it
through in a different locale and take a
snapshot it within the same browser at
the same point in time and you can do
the comparison and figure out what
elements are broken in each locale
another case we've come across is just
dealing with bandwidth constraints maybe
you're targeting smartphones and you
want to see what the differences between
edge and 3g load up the page and both
wait three seconds take a snapshot and
you can get a pretty good idea of what's
going to break in that case another
pretty interesting aspect of all this is
there's no real programming required
don't get me wrong building the system
takes a lot of effort but once you do
it's it's not tied to any particular
application it matches exactly how we
develop pages views evolve quickly and
since we already have implicitly stated
that you know if we develop in chrome
that the page looks good in chrome then
that page implicitly encodes a set of
rules about what we want to test against
so this whole process adapts is your
your application grows you never have a
risk of running with tests that are
completely out of date and it's it's
just really interesting way to go about
it because anyone can enter a URL which
is another way of representing a page so
we can make testing a bit
to those that don't know how to program
everyone knows how to compare two pages
so if a marketing guy needs to launch a
new campaign and wants to test his
landing pages you know give him
something like this he can test it
through and if there's a problem he can
give more information to a developer and
a bug report than he'd ever be able to
give otherwise to help them fix the
problem and again like I said there's no
maintenance to the system required I
mean you're going to run the grid and
there's maintenance with that but
there's never a case where I you've all
of a sudden decided you're going to
change to a different web framework or
all of a sudden you're going to adopt
css3 and all of your tests fail because
there is no test that you're writing so
in recap web consistency testing is all
about enabling a consistent user
experience for your customers regardless
of whatever browser or device that
working on we want to match or model the
way developers typically approach web
development but we also want to model
the way views viewers typically take a
look at page and this is all about not
caring about minor issues things that
you know simple text that wraps you want
to find actual legitimate issues and
when you find these issues you want to
be able to fix them as quickly as
possible so we want to provide
intelligent results and by doing this we
can take a process that used to be very
expensive take many hours and make it
very simple something we can do as we
develop and basically move the entire
state of web testing forward so that's
it thanks for your time
so what kind of frameworks you use to
compare it across browsers and across
platforms we build the top selenium grid
so that's how we drive our browsers and
then we we use selenium to basically
drive the browser and to capture the Dom
you kind of just you know take a
serialized copy of the Dom it's pretty
straightforward to do in javascript once
we have that back you know we have a
JSON representation of the Dom so we can
just feed it through a data analysis and
do that whole heuristic base depth first
search comparison I was talking about is
this like a all of us a Saturday so is
it open source or what is it exactly and
I run a company based on this but I'm
trying to push forward a concept here of
a new type of testing you know I want to
get us out of this rut of I mean let's
be honest most people never get past
testing just the back end extremely few
of us will ever test JavaScript on the
front end and I guess slightly more will
do acceptance testing using something
like selenium but none of us on the
whole are really checking that a page
looks the way it should and as we
continue to push things forward onto the
web and we have a explosive number of
different browsers coming out you know
there are there is some consolidation on
how browsers deal with certain aspects
of the specifications it's not as bad as
it was say 10 years ago but at the same
time chrome and firefox in particular
are coming out with rapid releases and
they're picking up different portions of
the html5 and css3 specs smartphones are
really taking off in the u.s. right now
and that's adding a lot of different
browsers and it's it's just not scalable
to manually continue testing the way we
have been
and when you're faced with a tough
problem we're testing is difficult I
don't believe the solution is ever not
to test so that's why I'd like to see
this get no more widely adopted thanks
it sounded like you're dumb testing
heuristics were heavily dependent on the
browsers that were generating the dumps
could you consolidate that with a claim
that this is very low maintenance I
don't think it's actually dependent on
the browsers at all a browser is going
to have an internal representation of
what the page is that's going to be an
annotated Dom we're going to have the
element structure we're going to have
all the properties associated that it's
left position its offset its with its
height and that's going to continue to
be true as long as we have HTML and CSS
obviously this falls apart in the case
of like WebGL where you have something
that isn't gone based at all so it's
extremely adaptable to new sets of
browsers as long as you have a way of
interacting with that browser to capture
the Dom then it doesn't really matter
where it came from in order to to work
with this so in that sense yes it's low
maintenance I mean it's taken me 18
months to get as far as I have and I'm
trying to help people not get stuck in
some of those same ruts i did so i think
you get going considerably faster now
and then once it's running you know you
might run into a couple of edge cases
but for the most part it should be
fairly low maintenance that answer it
should sure I'm not very familiar with a
degree of customizations browsers make
you know custom extensions and how that
affects you okay yeah it affects it
really that not not that much at all
where things get a little wonky is when
you have invalid HTML and then every
browse it kind of does
thing different for instance I ran into
a case where a table wrote was just
created not in a table it turns out IE
will like create a nonnamous element and
showed the table row in that firefox did
something completely different and
chrome did something even different
beyond that so that was a case where it
kind of just discover what happened in
add a new heuristic for that but for
most part the heuristics are pretty well
defined and I'm sorry I don't actually
have this site up yet but my intention
well those in the video and people watch
this many years from now but my
intention is to basically have ontology
of these types of heuristics where you
can go through and have a community
sharing this this type of information
yeah I was wondering about extremely
dynamic content and its effect on where
the Dom might actually shift based on
content generation so if you have a
reference page but that might actually
shift over time uh yeah so everything's
gonna be point in time if your page
kicks off from JavaScript when it loads
and three seconds later it pulls in a
whole bunch of stuff then yeah you'll
have some timing related issues for that
but on the other hand it can adapt
pretty well with things like you submit
bad form data or you hover over an
element because those manipulations are
going to be reflected at the Dom level
as well so you can combine this with
some of the more traditional testing you
do with something like selenium and now
you can test error states as well which
is something I I know very few people
actually do submit some bad form data
and see what happens do you get the
correct error messages is the the first
form element that's off get highlighted
things like that hey Kevin this is
Denali from octa I just want to say
thank you so much for an excellent
presentation with a lot of detail and
explanation I really appreciate it thank
you um but I'm wondering can you kind of
describe like your customers your
business model like how you charge for
this stuff like that sure I was trying
not to turn into a vendor bitch
but I mean yesterday I saw question now
you have to I mean we do it software as
a service we have to run against a grid
of browsers so selling it as kind of
off-the-shelf software would be a
non-starter for a lot of people because
now we're back to that expensive case
where yeah you have cloud computing but
it's still you know pretty expensive to
have a base set of browsers at the ready
all the time for your testing so we do
it as software-as-a-service we're really
trying to make it as simple to use as
possible so we can spider through a site
you don't have to enter all the URLs we
do have non-programmers using it as a
form of acceptance testing but at the
same time we do have an API available so
you can do things like integrate it with
your CI server or when an issue is
detected you know auto file it in the
Giro what have you yeah quick question I
don't know if you already covered it how
do you deal with pages that actually
have dynamic you know Dom's based on
interaction right I mean I was if you
look at your page right you hover things
show up how do you deal with be able to
tell if that renders correctly across
you know yeah I think this is similar
the question she had over there where
you can just you can take a two-prong
approach and integrate this with you
know your more traditional selenium type
testing or webdriver where you just
hover over and once you hover over the
element or you submit a formal whatever
interaction you want to perform you can
just snap shot the Dom at that point and
basically you know then you can test
your various states
so how do you deal with ads do you have
any a detection system in the system I'm
sorry how do you how do you deal with
ads like because if like the web has a
lot of ads nowadays so how do you deal
with ads and dynamic country which is
constantly changing so so edge will
always be different that if you if you
load a CNN com on Chrome and if you
don't load a CNN com like two seconds
later than firefox there will be
different content because the edge 00
ads yes okay sorry um yeah I was trying
to read this and obviously they weren't
following either well this is what that
that whole tolerance threshold thing
comes into play and that was a sorry I
meant to call that out and I talked and
I didn't but if your ad basically sits
in the same position and almost every
site does it might shift the dimensions
a little bit but the result is going to
be basically everything's kind of in the
same place so you set your tolerance
threshold appropriately and you won't
have any problems come up as a result I
have you considered using any other ways
to sort of manage dynamic differences
due to dynamic content like masking or
something like that just curious if you
went down that road then had problems
with it or what one way that I dealt
with in the past and I still prefer
doing it sometimes as using the hash
portion of the URL and basically there'd
be a listener in the JavaScript that
when it sees it it takes whatever action
I'd like and the reason I like doing
that is when i'm setting up test plans i
can just put in the urls and i have a
much better idea of what's going on and
then i'm cutting out the overhead of
using something like webdriver to go
hover over what whatever
son but but at some point you can just
throw it over to a human right I mean if
you try to automate everything you're
going to get it's going to get confused
so do you have like some formal process
where you just say okay I don't know
whether this page is the same it's it's
failing whatever criteria I have it
looks different to me but it's close
enough that i'm not sure that you could
just give it to a human and try it out
sure you had that page at a nice but
where you could compare one page to
another yes so gets who gets that well
this was actually taken straight out of
moga test so so i will show to our
customers but you do have the option of
ignoring things so if it comes up and it
might be you know legitimately a
difference but you just don't care about
it then you can mark it as ignored and
then you won't get get alerted to that
again seems like it seems like it might
be a good good use of just crowdsourcing
right yeah yeah we even look at it and
this is all chemical Turk or something
right okay thank you very much Kevin
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>