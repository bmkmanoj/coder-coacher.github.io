<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IA Memory Ordering | Coder Coacher - Coaching Coders</title><meta content="IA Memory Ordering - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IA Memory Ordering</b></h2><h5 class="post__date">2008-02-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WUfvvFD5tAA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to introduce Rick Hudson is
joining us today from Intel's
programming Systems lab he's been
working for a number of years on memory
models and he has recently helped to put
out a white paper clearly describing
Intel's memory ordering and today he's
going to talk about that and what some
of the motivations were behind the
currently defined model so welcome thank
you
so I'm Rick Hudson on from Intel just to
get this out of the way bunch of names
up at the top are the people at Intel
who worked closely with me on getting
this done I won't go through all the
names but there they are but I think
more than that there's also all the
major language and OS efforts around the
country
were involved in reviewing this work so
this isn't something that came out of
Intel without lots of clothes looking by
people from sea and people by Java and
the people by Linux and they provided
general encouragement and support and we
really do appreciate all the work that
they did also so what we needed to
address some of the problems that were
out there and the current memory model
that we we we would publish didn't seem
sufficient and in particular type safety
had to be addressed in languages like
Java and in particularly this thing
called publication safety and I'll show
you in a couple slides exactly what that
is and how we dealt with it we had to
support this concept of sequential
consistency which is really a high-level
language requirement for a subset of
program in particularly those things
known as race free programs where race
freedom is a dynamic property that just
to bring everybody to a level level
thing or we think race freedom is
defined as multiple access is the same
occasion where one or more the accesses
is a right and and again that's a
dynamic property as opposed to a static
property I don't want to really miss
define what Lambert defined a sequential
consistency so I put the actual
definition on the board there and if you
don't know what that is you probably
actually do because it's what you
intuitively think a computer does okay
and so that's why people like talking
about sequential consistency because
it's intuitively what the first person
that comes to computer what they expect
one of the other problems yes why is
sequential consistency is sequential
gate what happens is in Java and c-sharp
what they're really saying is if you
have a race free program okay we will
give you a sequentially consistent
execution that's really all they're
saying is it's not saying that you need
sequential consistency all they're say
all job is saying is if you if you are
race free you'll be sequentially
consistent okay
another problem we had to deal with was
legacy and and and and Intel legacy is
is not only hardware but but software
out there so we had to make sure that
Linux didn't break Java didn't break so
we couldn't change the memory model such
that these programs broke so a lot of
time was spent looking at legacy code in
this project and we also there was a lot
of motivation in the programming systems
lab to provide a solid foundation on
which to build higher-level concurrency
constructs and by higher-level
concurrency constructs I mean like these
obstruction free de Malaga rhythms
transactional memory any number of other
types of concurrency constructs that
people are starting to now develop as
multi-core starts to happen
or many core ok so
let's just dig right in here so how are
we changing processor ordering which is
the name of our model we actually are
making it more specific and the ways
that we're making it more specific is
introducing this concept of TLO + CC and
TLO stands for total lock order and so
the instructions with a lock prefix
these are things like an exchange or a
lock compare exchange or to you know
fetch an ADD type of instructions that
all of those will have a total ordering
across all processors and then we also
added something called causal
consistency okay
and this basically says the right
visibility is transitive okay so if I
see something and you see if I'd write
something and you see it and you write
something somebody else sees what you
write then they'll also see what I wrote
we managed to get this thing down into
eight carefully crafted principles and
if you understand these eight principles
we're confident that you can go ahead
and move forward and understand exactly
what's going on in the x86 memory model
this talk and these principles are
actually limited to write back memory WB
memory which is typically what people
get when they not typically but what
they always get when they do a Mallee in
NC or do a new in a language like Java
or C sharp we're also assuming that
accesses are naturally aligned which
means that if you've got an 8 byte load
it's aligned or an 8 byte boundary this
is not a difficult thing for compilers
to deal with and that's our primary
target here and it does not speak to
some of the non temporal instructions
like the move nti
which very few people use but they're
mostly used for video cards and DMA
stuff and also doesn't speak to other
memory types like WC gags are right
combining so those will have their own
rules and and and these principles won't
apply but these are the principles that
people that use malloc and stuff will be
using so
what intel wanted to do was we wanted a
memory model that was as strong as
required and by required we mean the
legacy issue and required to move
forward in a multi-core world but we
didn't want it any stronger because we
wanted to preserve for our hardware our
architects the ability to do efficient
implementations okay so a high-level
language and then we also noted two
high-level languages and compiler
optimizations often obscure these
differences so if you host someone out
of a loop well the compiler basically is
getting rid of the you know any hopes
for sequential consistency and so forth
we also were curious if there was any
code out there that was depending on TSO
but not TLO plus CC and we couldn't find
any I think that obviously there's a
difference in the two models but we
couldn't find any active code out there
that actually took advantage of that
difference I think you could probably
write code that detects it but nobody
really has a serious use no one seems to
have come up with a use for that that's
commercially viable TSO is is total
store order that's what Sun SPARC uses
sequential consistency RSC is what
Lamport originally defined and what most
people think and then we're TL o plus C
C so if we implement an SC or TSO then
we'd also meet our
during all the requirements of TL o plus
C C so this implementation or that
implementation will be compatible with
the DLL OCC
it's a question of consistency we
defined in the previous slide or a
couple slides ago that means that every
load and store appears in program order
where program order is what's running on
one processor and all of the stores and
there's some interleaving that everybody
agrees upon if you think if you think
about sequential consistency is why day
an old mainframe would give you where
you just swap threads and you executed
all the program all of the reads and
writes in a single thread in order you
get sequential consistency a total store
order is for most stores all the
processors will agree on what it is if
you you you you you may disagree in the
ordering of stores with total stored or
between stores that you make and stories
that other processors make if you're
familiar with what a store buffer is
this is typically called store buffer
forwarding is what breaks the the total
it's the exception the total store order
that you might find in a TSO and TL o +
CC as well talked about in a few seconds
so those are basically the differences
between the three different models as
you can see since this order is both
loads and stores and this order is just
stores then obviously this is a subset
of that and since we only require causal
consistency that's trivially supplied by
a TSO implementation since everybody
agrees on all store orders okay so so
here are the eight principles you I want
to advise you reading them I just wanted
to show that we could put them on one
slide and if you can sort of figure out
what's going on here that's all you need
to know we're going to go over the eight
principles and we'll stop along the way
to discuss particular problems and try
and give you some motivations
on why that principle is there and why
it's worded the way it is and what you
can do with it
so these slides are going to mostly be
looking are going to be mostly given the
eight principles the next eight slides
will give the eight principles one at a
time on each slide and they'll give you
an example of some some code and four
for those of you aren't familiar with
our assembly language the way you read
these things is you replace the comma
with the word from and and then they
don't read pretty well so it's basically
move 2x from constant one okay so
destination source okay so replace
common with from and and that's what I
do and it seems to work we get it this
way instead of putting the pseudocode C
code up there because we wanted to
really impress upon you that the
compiler is not involved here okay if
the compiler does something like reorder
those two things you know that that's
fair and the compiler can do that and
and we're not going to fix the compilers
and in the hardware okay
in these examples initially the the
memory values are all set to 1 and the
registers are all set to 1 if there's an
R in front of something like there is
over here that means it's a local
register okay so we're just pulling
something from memory typically in a
local register you see these break
brackets around something then that's
the location of the value inside of the
location X so loads cannot be reordered
with other loads and here's an example
where processor 0 or do it as doing two
stores ok and and and and and I'm sorry
processor one it is doing the loads and
processor 0 is doing the stories so
processor 0 is storing ones in X&amp;amp;Y and
processor one is reading them from X&amp;amp;Y
then
neither processor zero or processor one
can reorder their accesses so this means
that if r1 is is 1 which means that this
Y here is seen by that by processor 1
this store here then it will also see
the store from processor 0 ok and you
can flip it around if you want to do if
r2 is 1 then r1 can't be 0 either so you
can't really have the loads can't per
your order with other loads with what
hundreds of people have looked at these
examples okay so the third principle is
it stories are not reorder with older
loads okay so if you load something then
you store something that the store can't
be reordered okay so in this situation
r1 and r2 both can't be one because if
if processor zero sees sees the one then
that means that this move has already
happened okay and if this move has
already happened then the load above it
has already happened and if the Lord
above it has already happened then the
store over here has not happened so if
you if you you you you end up having to
have either process or one happening
before processor two or at least the
store in the load and that force is the
other one not to happen now this is
important and what will actually get to
why this is important but if one of the
the legacy things that we were working
on was was how to build critical
sections and just sort of keep this in
mind for a little bit and we'll get back
to it in about ten slides now we do
allow loads to be reordered with older
stories to the same location or two
different locations I apologize but not
to the same location right so what one
can can can see in this case if we're
allowing these two loads to be reordered
above the two rights then obviously they
can both see zero okay so so that that
actually is allowed by the memory model
if you want to prevent this from
happening mem fence is the the
instruction you want to insert between
the the store in the load and that
suppress this behavior okay okay so so
so here's where we're starting to
diverge a little bit from what
classically was part process ordering
until this paper came out okay and this
is why the ways that we've actually
strengthened a processor ordering we
didn't really strengthen isn't the right
word clarified I think is the the the
correct term here so in a multiprocessor
system and by processes we really mean
just Hardware threads either whether
that's a hyper thread or whether that's
a core that they all sort of fall on the
same we're going to use all word
processor interchangeably with those
things I think hardware thread so what
this says is if I write 1 to X ok and on
processor 0 and processor one sees that
for any reason and then it writes one to
Y and then processor two sees what
processor 1 is written then by virtue of
the fact that processor one is seeing
what 0 has that processor 2 will also
see the right of X which means that
processor one sees the one and processor
to sees the one written by processor one
then he cannot see 0 he will have to see
the one that was written so this seems
like a an odd thing to require in a
memory model but let me show you why
this is critically important to to
building systems so there's this thing
called publication safety which is
really sort of a subset of type safety
that we're going to talk about here so
assume we have processed each of these
boxes of processor 0 processor 1 in
processor to assume we you malloc up
they are new up an object in process
zero and you said it's vtable to
something valid and then you publish it
and this is where the term comes from by
publishing you place it inside of a
globally accessible location in this
case I'll call it slow train okay and
then sometime later this arrives this
the value of slow train arrives at
processor one and he stores that in
something called warp drive okay so
obviously warp drive is named to
indicate that it might get someplace
faster than the slow train and sure
enough if if warp drive comes down here
and gets here faster than the bits that
we're done by processor zero then warp
drive might try to execute a method
it'll dereference the V table and well
oops
you know things aren't good okay so we
have to make sure that so that rule is
here to make sure that if anything from
processor zero can make it to processes
three then everything will make it okay
everything before that will make it if I
didn't then you'd have this problem so
that's publication safety and that's why
that's exactly why we put this rule in
it is to support a high-level language
is your required type safety so moving
on to the six of the eight this is
pretty straightforward storage to the
same location have a total ordering so
if you see we're writing two acts here
in processor zero and then we're writing
another value in processor one and all
the processes in the system have to
agree on the order that that's written
in for example if processor two here
sees 1 2 which is 0 1 then processor 3
has to see the same order 1 2 it cannot
see
now obviously if this wasn't the case
you'd have a weaker model for one and
such models have been proposed but the
real problem is that if you can't ever
agree on what the value of x is you're
just not going to get understandable
programs okay so that's why we have this
rule and that's it
okay so so lock instructions so so what
we're not declaring that lock
instructions have a total a total
ordering an exchange is a is a lock
instruction and you can think about this
just storing values into low exchanging
whatever is in registers with with the
locations so it's part of it's a store
and part of it's a load and so what
we're saying is that if you're using
lock instructions and you can detect the
ordering between processor 0 and
processor 1 if processor 2 sees one of
these orderings then processor 3 will
see all similar orderings will see the
same ordering so in this case if
processor 3 here sees 1 0 which is going
to be processor 0 processor 1 then it
can't be the case that this guy sees
processor one right before processor
processor 0 so why do we need this total
ordering ok so we need the total
ordering for a couple reasons there's a
really strong reason it is that
sequential inconsistency is in fact a
high-level language requirement Java
requires sequential consistency for some
of its constructs in particular they're
critical sections they're they're
synchronized statements as well as their
volatile x'
and it's also required by serialization
is requirement for transactional memory
so that's one of the things you want
with transactional memories to be able
to have a total ordering on the
transactions as they're executing and so
this is this is how you get a total
ordering on our processor as you use the
lock instruction so to make the lock
instructions useful in these contexts we
also say that loads and stores could not
be reordered with lock instructions so
in the case of processor zero we have a
lock instruction that they write
something in to write to one actually
into location X and then we'll read from
read from Y and over here we're writing
a 1 into y and we're reading from X so
they're flipped so the situation where R
2 equals 0 and r 4 equals 0 isn't
allowed because one of these things had
to happen first okay and they have and
you have to agree all processes will
agree on which exchange happened first
right so if you want to use volatile x'
and you want to have this total ordering
okay then you can use an exchange
instruction to do that in your
higher-level language and you'll get
sequential consistency so so now you've
seen the principles and if you're not
asleep yet we'll we'll show you how you
can use these principles an example of
using these principles to reason about
how one might build a critical section
for a language like Java or something in
particular we're going to use these four
principles to build a critical section
that has a specific semantics and then
we'll talk about how we might use some
of the other principles to change those
semantics
lightly so we'll go over how these
things are used okay so on our left we
have sort of a C type of scenario for
critical sections lock acquire some
monitor and then a lock release down
below and inside we're gonna do a load
as well as a a right just just so we can
show you the interesting things and over
here we have sort of how this will
translate into assembly language yeah
the x86 assembly language so I'm not
sure what lock instruction you want to
use to acquire the lock some people use
fetch in air and other people use
compare-and-swap
but it's you know it you know whatever
whatever algorithm you using we can use
so loads and stores are not reordered
with locking structure this is principle
eight okay so that means that this load
over here this this move to r1 from
location X can't move up above the
critical section okay so you can't get
at loads can't get out of the top of the
critical section okay also it also talks
about stores and now that store also
can't get outside above the critical
section so we've used this principle
eight to ensure that nothing can move
above the lock acquire or the monitor
inner if you will
so let's um let's look let's now talk
about not escaping out of the bottom of
a critical section one of the legacy
issues that we ran into is that people
were using a simple store instruction to
release a lock okay so there wasn't a
situation where you had a lock
instruction at the top and a lock
instruction at the bottom several Java
systems including our own we're actually
using a
a simple move of zero into that so we
have to prevent things from escaping out
of the bottom the other way to look at
that is you can't let the release move
up into the critical section that's sort
of the flip of that and principle two
says stories are not reordered with
other stories you have two stories that
can't be reordered so we know stories
can't drop out at the bottom of the
critical section okay so now we've got
three of the problem solved and finally
we also have this principle this stories
are not reorder with other loads so in
this case we can't the load also can
escape out of the bottom of the critical
section so this is how you would build a
critical section using our memory model
this is how most Java systems have built
critical sections and this is what we're
recommending so so we build a certain
type of critical section here and oh I'm
sorry the one final thing that some of
these languages require is that these
critical sections be sequentially
consistent or have a total ordering and
here we use this final principle seven
that says that lot of constructions have
a total ordering so in order to enter a
critical section we always have a lock
instruction and we're going to use that
to always have a total ordering if you
think about it a little bit you realise
that the release if someone else is
going to enter a critical section
they're going to do it with a lock
instruction so you can tie together the
release and the lock instruction and
with a little work I'm sure you can come
up with a proof that says that the
critical sections are actually
sequentially consistent
oh yeah right cool so we've got this
critical section loads can't get out but
what about the problem where we know
that loads can pass stories okay so it's
possible that loads can get into the
critical section okay so you enter a
critical section your exit a critical
section you do a load there's nothing to
prevent that load from going inside the
critical section that seems to be a
perfectly reasonable semantics we
typically call it a roach motel
semantics the reason being that loads
can check in but they can't check out
okay so grad students seem to understand
that better than people that actually
have jobs so anyway what if we want to
prevent what do we want to semantics
where loads can't check in okay or get
into the critical section of that case
you would want to use an EM fence which
would suppress the the final store of
the critical section
with the loads that follow it so that
basically is the the the the the heart
of my talk is you've seen the principles
and you've now seen how to implement
critical sections with them and so the
other thing that we had other problem we
talked about earlier was that legacy
software and legacies Hardware had to be
honored and and I we spent a lot of time
looking at our old processors okay and
looking at our old systems and making
sure that they all agree to what was in
the white paper oh if you'll notice the
white paper doesn't talk about EM fences
because they actually weren't available
on the Pentium so we didn't talk about
we're not trying to hide what they do
it's just that the rules of the game was
was see if you can get a white paper
that goes back to the Pentium
and also a lot of time was spent not
only by us but by you know the the
friends at linux and the people at
Microsoft and the the Java memory model
people of which Jeremy was a member of
trying to understand what they actually
use now what kind of assumptions they've
made over the years about our memory
model sometimes they were overly
cautious for example they were sticking
s fences in when they actually didn't
need them it didn't hurt anything but it
slowed down their programs and so now
there's a clear explanation that stores
can't pass stores maybe some of their
code will get faster so one of the
motivations here was we felt if people
knew what was going on they could write
faster code other feedback we got back
besides the fact that they wanted to
release locks without a lock instruction
was that volatile Reed's had to be
efficient but they really didn't care
about volatile writes and if you look at
say the java u concurrent util programs
they take advantage of this so our
volatile reads can be done with a simple
load instruction and are volatile writes
can be done with an exchange instruction
or any locked instruction so we think we
were successful this was a multi-year
effort by multiple people to get this
out they look complicated but in fact
this is probably the simplest
explanation of memory models that that
most people have ever seen ok we spent a
lot of time trying to to break this down
so people can understand why our memory
model is doing we feel the TL o + C C is
a solid foundation for building a
higher-level concurrency constructs and
now I can get on to doing what I really
want to do which is building these
constructs now that I have a solid
foundation beneath me we get it in ten
pages and there were a lot of spaces in
those pages and in a few slides I guess
we did 25 slides ok
it's available
as an Intel white paper if you have
access to a search engine just type
Intel memory ordering and it should be
the number one hit it says Intel 64
architecture but that's real that's the
x86 as opposed to the IPF system so that
this white paper is available and it's
being moved right now into the public
reference manuals and that's due to come
out any day now and you'll see the white
paper actually being moved nothing more
nothing less into the actual reference
manual so what we doing is we're putting
white papers out now and then by and by
what we're gonna put the stuff into the
reference manuals that's it I guess I
have to show you this but you don't want
to read it it's just an eye chart I had
to sign something similar when I came in
so all I'll force you guys to look at it
and I guess I'll take some questions now
from the audience what's the difference
between El Fez and an offense under this
model
it was for an elephant a nest as well
they're both no-ops and in a right right
back memory so I guess none their intent
is very specific and elephants separates
loads from other loads and an S fence
separates stories from other stories but
as you've seen for right back memory
okay not right combining memory some of
the other memory types we have where
elephant intestines actually are
important okay
since loads are already ordered sticking
an elephants between them won't do
anything and since stores are already
ordered sticking a fence between two
stores also won't have any any effect
okay so if you've got you malloc
something you find yourself putting s
fences and L fences you need to reread
the paper okay and if you if you if you
if you're mucking around in the middle
of some kernel code someplace you might
see some of this okay and that was one
of the reasons why we came out with this
because we really did feel that we
needed to to get a memory model out that
didn't have these offenses and L fences
and they do cost because they do start
flushing stuff okay they are necessarily
not right back memory and they're also
necessary for non-temporal instructions
that are they're operating on right back
memory like to move n tqi or whatever
the level c
fairly reasonable except for item 7
which is the controversial why having
the double order of flock operations now
you did not provide any an event she /
lock operation and already provide any
cheap way of doing the atomic updates
without having up order of structures so
this that's something that's you would
look at the future to provide faster or
more lightweight synchronization
primitives for languages like C++ which
well ideally all we'd want to do is make
the rich of the instruction with the
richer semantics as fast as need be okay
and and it's the the jury's out on and
on how fast we can make these the lock
constructions can we get them down to a
tolerable speed okay so this isn't
obviously you're asking for a semantics
that's weaker than what we already offer
and the only reason you're asking for
that I would think would be for
performance reasons okay so so so your
complaint is really that the lock
instructions aren't fast enough point
taken well we will we will work to make
them faster the solution of adding I
mean if you look at adding ISA as adding
legacy okay then then there's a really a
strong strong resistance let's put it
that way against adding instructions
until we've exhausted all possibilities
of speeding up the instructions we
already have so I don't think we're
against changing our is a but I
that's not in the plans that I know of
right now my concern is twofold one is
I'm just kind of do very slow items
seven makes every lucky struction
defendant of every other law
construction even though that they are
completely family leaders in the
software because lucky structures got
the different memory locations still
have to have a full order so that's I
worry about the locality consequences
when you go to countries of course okay
well we share your concern
okay absolutely I mean I don't know I'm
not I'm not sitting here saying that you
know we don't think about that we we put
lots and lots of cycles in to concern
ourselves about that but no or at this
point this is the memory model and this
is the memory model going forward
recently come out with a very similar
memory model the the I think we pretty
much independently came to the same
conclusion that the only difference in
their documentation that substitutive is
that they specifically allows stuff that
we allow through omission okay I mean we
don't say you can't do it so the rules
are you can and for some reason they
feel obligated to actually say that you
can do some some things so that's really
the only difference so if you code to
this and decide to buy AMD processors I
you'll be fine okay I wouldn't worry
about that that's not an issue but I
would advise you to read the AMD manual
yourself but the one time I read it said
well it's just what we're doing yes this
is going to be
they're lucky spectacles collection good
memory fencing lock instructions that
don't kill remember yeah that love these
sections of the empty but actually
there's a blue pencil you may find this
surprising but Intel has had bugs in
their processors also look at we doing
but I think everybody agrees that it's a
bug I think AMD is actively I'm sure I
can't speak for AMD but you know if they
want to stay in business they're gonna
just like us they're gonna be working on
making sure that that's okay you know I
actually wasn't aware of that bug but
that's that's interesting stuff we
should publish it widely
so the reason you didn't 20 those were
more relaxed model with explicit fencing
is because the state expenses aren't day
long in your postures I don't think that
was the reason nearly as much as it
would break code okay if we had
announced that we went to a weaker model
and that you know locks weren't you know
didn't have a memory fence for example
you know we felt that would break code
obviously it broke somebody's code
you know we wouldn't have been noticed
so so that was really the reason what we
really couldn't start we didn't have a
clean plate like we had with the IPF
okay where we could actually go in there
and you know define of whatever we
thought was the current technology we
had to deal with legacy I I think you
know if you look at the memory models
got weaker and weaker and weaker
you know the Alpha and the PowerPC was
weaker than x86 and the Alpha woods a
weaker than the PowerPC and I think
that's you know people kept getting
weaker and now people are trying to use
other techniques to go back to these
models like ours and try and make the
legacy model work and make it work
faster just like we've done this for
years we've said okay we're going to
keep the same on say but we're going to
make it run as fast as a RISC machine
okay so this is not it's not something
we're unfamiliar with
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>