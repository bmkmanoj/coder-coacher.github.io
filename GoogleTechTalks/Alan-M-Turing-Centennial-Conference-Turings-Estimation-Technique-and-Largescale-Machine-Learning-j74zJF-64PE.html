<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Alan M. Turing Centennial Conference: Turing's Estimation Technique and Large-scale Machine Learning | Coder Coacher - Coaching Coders</title><meta content="Alan M. Turing Centennial Conference: Turing's Estimation Technique and Large-scale Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Alan M. Turing Centennial Conference: Turing's Estimation Technique and Large-scale Machine Learning</b></h2><h5 class="post__date">2012-04-25</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/j74zJF-64PE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">now we're here today to celebrate many
great achievements of Alan Turing and
before I get started on a more
scientific part of my talk I would like
to point out what an outstanding athlete
he was I believe that you just alluded
to it but I did not have the setup so as
I'm sorry if I'm repeating something
that this already said he was a
fantastic runner and though I am also a
very dedicated one I think he would run
outrun me any time in 1947 he ran a two
hour 46 minutes 3 seconds and marathon
now you could say at that point of time
he was 35 years old is that a very
interesting time to run in that doesn't
sound too impressive the world record
after all isn't it like two hours and
three minutes let's not forget that back
then the world record was two hours and
25 minutes so if we scale to modern days
he would actually have run at 220
marathon which is quite impressive he
did participate in the Olympic Trials
for the Olympics that was held in 1948
he placed five and therefore he didn't
make the cut but he actually in 48 in a
different race he beat the guy who took
the silver at the Olympics and in a
marathon discipline so quite some
impressive achievements though that's
hardly the reason why we're here today
but it does speak to an incredibly
strong mind with all really big sports
achievements they do he must have had
the ability to focus and push through
hard times what I'm going to talk about
today is some highly influential work
the turing that during the Second World
War this work is still in use maybe even
more today than it was back then since
it forms a critical component of the
speech recognition so
systems sitting in this very phone or in
the horrible phone menus that you don't
like when you call any companies what
I'm going to be talking about today is
what is referred to as the good Turing
estimate not that there is a bad
chirring Azamat good was the name of
touring's assistant and it was acting he
who wrote up the material and it was
never published under Turing's name
though Turing gave the permission to
good to publishes the work came around
in Turing's quest to break the egg Nick
my machine the Enigma machine had been
commercially available since the 1920s
and it was adopted by many military and
government services by many different
countries most notably the Nazi Germans
already in 1932 polish intelligence had
managed to break some of the settings of
the German Enigma machine but with
tension increasing in Europe the
security was stepped up and the Germans
they increased the number of rotors and
switches on Nick Nick my machine making
it even harder to break that's where
Turing comes into the picture now
September first 1939 germany invaded
poland only two days later or maybe it
should have been earlier france and
britain declared war on germany
September forth during reported in
Bletchley Park which was the main
quarter for the British decrypting
service his main task was to break the
decryption scheme of the German Enigma
machine in the beginning of the war this
change the settings were only changed
daily but during the last days of the
war the changes were much more frequent
the Enigma machine was highly secure
there was a chance of one out of 150
with 12 zeroes after of breaking the
machine that is 10 times the size of the
US national debt maybe you don't care
that much about that one here we can
make another analogy if we think of that
it takes just one tenth of a second to
decrypt using a specific setting of the
Enigma machine it would take four
billion machines if all the codes were
to be tested within one hour now not
even Google i think has four billion
machines though alfred he may know
differently there's always something
going on in google we never know what's
happening but i think four billion
machines that's not what we have clearly
Turing would have loved to work with the
Google infrastructure but that was not
what he had to do he came to Bletchley
Park to work on an electromagnetic
device we can call it call the bomb
which was going to check the codes and
make it possible to break it however has
already pointed out brute force was not
an option it was smartness that was
needed to quickly Lin eliminate
impossible settings that was Turing's
whole program now the bomb alpha showed
a picture of it already it was not a
general-purpose computer it was not a
general Turing machine it could only run
one program it was inspired by the
polish intelligence and therefore it had
to name the bomb they called it bomba as
far as I can read by the way I'm doing
my best according to what I have been
able to find out for historic
correctness but I'm sure that there are
things here that a little bit freely
interpreted so Turing he worked on the
programs for this electromagnetic device
it worked by contradiction there were a
few things that were kind of a security
flaw with the Enigma machine no letter
could be mapped to itself so if you
started it out in a code and you had as
input text with an A and in your
hypothesis you saw there was an a coming
you could halt and move on to the next
code also within one setting and a could
not map to more than one other letter so
if you started out with a and you saw a
k if you then later and your hypothesis
got to than a how to map to another
letter say p you could also halt and
move on later on the scheme was actually
significantly improved not by cheering
but Gordon Welsman that since the Enigma
machine was self as a probable that is
you would think encrypt by setting the
machine putting in your plaintext get
their response out to decrypt it you
would set it with the same setting put
in the encrypted text and it would spit
out the plaintext so he had this insight
that an aim after a que than a que also
had to map to an A and that also cut
down significantly on the possibilities
for for or how quickly they could halt
when they were testing hypothesis so the
Enigma machine machines the first were
successfully created in 1939 and 1940
and by the end of 1940 they had
decrypted about 200 messages at the end
of the war they had many of these
computers at separate locations they
were afraid that the Germans would take
out one center so they kept them at
different locations and interestingly
enough they were operated by women we
also saw that my graven in one of your
pictures it was women that were were
operating the computers the first
computer force 2,000 women during the
Second World War ok so there was a lot
of smartness in order to cut down on the
possibilities but I should also say
Turing was very pragmatic he did
everything he could and there were many
suggestions for how to quickly decrypt
them they work with clean text so for
instance they captured
upon a time an enigma operator he told
them that all numbers had to be
encrypted with letters it turned out the
German word is for one within ninety
percent of the messages so they had this
whole scheme where they tested all
possible settings for is it an incident
incl science here they had all these
things they had weather reports the new
they were sent every morning so they
could use those as plain text many
messages started with an and a space to
somebody not to mention that on hitler's
birthday a lot of the messages they
contained Heil Hitler which was also
easy to use for them to decrypt so they
actually made a reasonable progress on
decrypting these messages however in
1942 a new crisis came a nineteen forty
one a new crisis came around the german
navy changed their encryption the whole
force of matching part was at a loss
Turing undertook the decryption as he's
quoted saying no one else was doing
anything about it and I could have it to
myself so you can probably imagine that
they were all jumping on these things
otherwise with their their plaintext
attacks but the naval encryption nobody
was doing anything about it it was
highly important that they found how it
was done they had various hypotheses but
they didn't know anything for sure it
was a historian that a young historian
Harry hinsley but found a brilliant way
for them to get information about how
the Navy did the encryption the British
had attacked submarines but the
submarines were of course well prepared
for an attack and whenever they got
close to them they would throw the
Enigma machines overboard they would
show all the manuals they would throw
all all the books and settings or were
bored and the British would get nothing
what Harry hensley he thought I was that
let's attack a trawler working as a
weather boat off
most of Norway Iceland some remote area
they're not expecting to get attacked
we'll get them and they'll be quick
enough because they're not supposed to
be sending any information but they have
a curse the eggnog my machine so they
can receive information they'll be quick
enough maybe to throw the Enigma machine
overboard and throw the current settings
or report but in the safe hopefully they
will not throw out the settings for next
month and maybe some of the
documentation will be in the safe and
that was exactly what they did so they
sailed up and out of Iceland they
attacked a poor trawler working as a
weather boat and there they finally got
an insight to how the German Navy did
the encryption and it received some of
the books needed for it so it was a
double scheme it started out with
trigrams depending on the nature of the
message that was to be encrypted the
operator would open a thick book of
trigrams called him open book cable you
can see it was printed with red ink on
water soluble paper so if they were
attacked they could throw it overboard
and soon it would be gone so when the
operator was to encrypt a message and
find a key to use with he and I'm
certain it was a he he would open the
book in the appropriate section select I
by a trigram he would then move to
another section of the book select
another trigram and he would place these
two trigrams one shifted was with
respect to each other in a little table
like this he would freely be allowed to
pad the white cells with any letter that
he wanted to then he would form the four
by creams going across and he would move
to the by graham tables this was a new
thing that cheering didn't know that
they had this double encryption scheme
and they did retrieve some of the bike
ramp tables there were nine of them so
with these diagrams
they could look up by Graham there were
nine of them and it was a calendar that
was deciding what by Graham table was in
use in any particular day so the
operator would find the encryption of
the by grams and that would be the
opening message whenever the operator
was to send a message so the backgrounds
were encrypted and that was sent at the
receiving end the decryption would do
exactly the same they would know what my
creme table wasn't used they would go in
decrypted it was self reciprocal just
like the Enigma machine and thereby they
would get the two trigrams they could
put them in the setting of the Enigma
machine and decrypt the message using
the ground settings of the day as I said
there were nine by Graham tables and
they changed every day during didn't
know how they were changing so and he
didn't actually see retrieve all of them
in the first raid the big question for
Turing was that if he had the trigrams
he could use the same machinery as they
had already been developing for the rest
of the German German army but he didn't
know at any particular day what which
one of the nine by Graham tables was in
use now you would say big deal it's nine
of them i mean just take nine times as
many computers right try them all what's
your Richard problem that is not an
option that Turing say it would have
been an option today we'll just go too
often said we need nine times as many
computers and we will just try the nine
by Graham tables and see which one is
working fortunately he got a handle on
it because it turned out that this
trigram distribution that they could
divide from the messages they could
decrypt based on the settings that have
found they were not uniformly
distributed over all possible trigrams
there was a distribution over them there
is a tendency that the operator would
open the book in near the middle and
just like what we see at Google you know
you start up at the top we're at Google
you expect the good search results to be
but the operator would typically
select something near the top and if not
something near the top just as we see
with go the eyes they jump to the bottom
of the page and he said like something
from there so when they took the
population of all the messages the
trigrams for all the messages that had
to had already been decrypted there was
a part in a word it was not uniformly at
random and cheering could build a model
now for a particular day he could go in
when he had a small new set of messages
for each of the diagram tables he can
say well if i hypothesize is this
diagram table then i get a small sample
from my distribution how well does it
match the true underlying distribution
that I have from all the messages that
we have decrypted the technique worked
well for about 30 messages and wasn't
used for a couple of years let me give
you an example before it gets too
abstract let's say that the top here is
the population of trigrams say they have
from already decrypted messages and now
we have a new day and we've received
three messages and if we hypothesize
that by Graham table 1 we get that these
are the three trigram and table 2 then
we get some other three trigrams now
what would you and I do if we naively
had to find out what by Graham table was
and use well we would probably do a
maximum likelihood estimator we say well
you know I can take of all the the
population I have for each trigram I
find the frequency of the track em in
there already decrypted messages and now
I'll just go in well I have KHQ you know
that had that had a probability in the
old set of about point 18 and i'll
multiply the three probabilities
together and thereby i'll get an
estimate for what is the probability of
biker and table 1 being in use then we
do the same with my gram table 2 we have
k WTH of you that always provide then we
get to y RT where I w0 it hasn't
appeared before so with this scheme
there's no
thing else we can do then assign
probability surabaya gram table to being
in use and that is pretty dumb right and
then we would see we would quickly
realize that this is this is not the
smartest thing to do and there's been a
number of suggestions for what to do
with situations like that and one
suggestion goes all the way back to
Laplace in 1825 is called the additive
smoothing at one later on it has had
many names I got a half and is called
jeff from Jefferies in in geophysics but
it's both empirically and theoretically
it has been shown that it doesn't work
very well and there's no reason it we
should be working very well is this is
not a principled approach to dealing
what's worth with the problem which is
called the missing mass what probability
should we be assigning to all of the
species in our populations that we
haven't seen yet Turing he found a
principled way of dealing with that
problem that's a good touring estimate
touring he took the discussion one level
up instead of counting the frequency of
the single trigrams he counted the
frequency of the tray games that
appeared a number of times he works
instead of with say we have example of
em words k is the set or sk is instead
of words appearing k times so some words
appear one time some words appear two
times some words appear three times sk
with bars round as the size of sk and
will also be denoted by little NK what
Turing is out trying to estimate is the
probability mkay of joy of the so ok so
here comes it you say we take the
example of two we have a sample and
there's some items that appear two times
in the sample what is the true
underlying probability mass of these two
items
the set of items over 2 times out of
that underline as a set of items that
appear two times now if you take a
different sample it will be different
items that appear two times then we are
trying to estimate the probability mass
of those items that appear two times
good a good cheering estimate of the
mass of the elements that appear k times
is a fraction of the number of elements
that appear k plus one time you can also
put it in another way the observed mass
that we see at k plus 1 times that's the
size of the set times how many times
they appear that is the total mass that
we observe at size k plus 1 that
probability mass if we divide by the
sample size is exactly the probability
mass cheering assigns to one level down
so we take the mass at k plus 1 and we
shifted the distribution one down with
these guitaring estimates we can we
calculate what is the count that we have
of anything that otherwise appeared k
times weary estimate the count of the
elements that appeared k times even our
original sample it's just multiple take
the mass we multiplied by M that's how
many we have we divided by the size of
the population and we get this formula
in reality we use a smooth and version
of it why do we use the smooth versions
because actually all the properties that
we can show about the good touring
estimated it goes they go in the average
so it's important that we have a single
sample to smooth out a little bit so we
eliminate the noisy versions of having
just a single sample also as you can see
from the guttering estimate if we had a
hole in this distribution and you sample
distribution would have a whole one down
so is to eliminate some of these effects
so let's see just go back to the example
we had before before you get completely
lost here I have in from the example how
many items
here one time 246 and you can see there
are indeed holes and the distributions
the first thing we do is we smooth the
distribution that's the blue line then
we can recalculate with the blue counts
the mass at k and then we shifted one
down most importantly now we get a mass
at zero we now have an estimate of the
probability mass of all the items we
have never seen before with this we can
go back and we as the made our
frequencies that's the red line by
indeed just dividing by the size of SK
and now we may just find that the
probabilities are reversed before we had
that this one got absolutely zero
because ASA view had not been observed
before but now we have a probability
mass we can assign to the unseen events
and we can get a better estimate of of
of what what by crime table was in use
this was what during he did to find out
how to decrypt the messages from the
German Navy in ninety four ninety 41 and
42 I said a number of times that this
was a principled way of estimating the
missing mass and what I mean by that is
that one can prove though I won't go
through it here today there is a very
very simple proof that when we repeat
this as i said we take one sample we
estimate the mass at k we take another
sample we estimate the mass at k on
average our good cheering estimate will
give the right value it's not a perfect
estimate because this is what we call a
biased estimate we actually have a term
over there that's not zero but what's
important that is goes down 1 over m the
more samples we have from our original
distribution the better and estimated is
the bad news is that it goes up with k
k peeing how many times an item appeared
the good news is however you can say
that the more times an item appeared the
better accuracy we probably have on the
count so therefore it's not a real big
problem then we can maybe use maximum
likelihood and forget two big accounts I
will not go through the proof but I just
also mentioned that particularly if you
use cable zero that's the missing mass
then we get that our estimate goes down
as one away M this is an estimate in the
average as i said but in reality we're
standing with a single sample and we
would like to get the canned cheese on a
single sample laid on using my diamonds
inequality in two thousand it has been
shown that we can actually get a bound
also here it's not quite as good as goes
as one over square root of M and the
dependency on K is also up in the square
root of K it was later even improved by
you shy who is not here but was
mentioned earlier on today that the
fourth root of K so we also have bound
for this single sample I had the great
pleasure of going through some of the
original work this was published by good
with drawings acceptance I and I must
say that good was not a very good writer
and he realizes at some point of time
that this is getting a little bit out of
control so in his own words he says
confusion can be avoided by reading they
expected melior where m of NK of Ages
the expectant asian of the frequency of
the frequency k and so forth so that's
why nobody derives it the way that that
good he he does it why do we care about
this today well here is the speech
recognition system we have frequencies
that are sampled over here to the left
every 10 milliseconds or
does we have the vector of the capital
coefficients coming in we have hidden
Markov models we have context-dependent
models for phonemes and at the end of
the day we want to find out for all
these possible transcriptions of our
utterance what is the most likely word
transcription what what sequence of word
has the highest probability given the
knowledge of the world the world today
is not the populations of decrypted
trigrams it is the text maybe maybe all
the Ingram's that that Google has
scanned in that we can we can form a
model or English language maybe through
another language it is text with several
of millions of word so we have the two
problems that we have large large
corpuses to deal with how do we estimate
good probabilities for a single word
given these large corpus and how do we
compute with it efficiently I will only
touch on the top item because that's
what relates to the good touring
estimate no matter how much text we have
we will still have the missing mass
problem the models that we are dealing
with typically at least at Google we use
Ngram models so the probabilities
forward with the year see it as an
Engram it's and with so instead of
saying that the probability depends on a
whole history of everything that that
went before it we truncate the history
too just in the probability of a word
only depends on the n minus 1 words that
came before it as a consequence when we
need to find the probability of a long
long word sequence we just have to
consider histories of length in and
multiply these probabilities together
but as I say no matter how much text we
collect there will be holes
we'll have sparsity problems we'll have
to find out how we deal with those and
of course their ways around it you can
make abstraction levels and you can you
use these class based models where
instead of having a probability for
monday tuesday wednesday thursday friday
maybe you just have a class day and you
make use of that but then you may have
holes in the class based models and at
least we haven't found any examples of
class based model that outperform pure
and gram models for representations it
has been commonly now excited to use way
to finite States transducers that they
are very efficient for their
computations and they elegantly exploit
the sparsity of the word sequences now
one of the most commonly used in cream
models the deal with the sparsity issue
is what's called a cat's back off model
so here we were out with we have all our
data and we need to assign probabilities
of a word given the history of length n
minus 1 now if that history does not
exist at all then we back off that's why
it's called a pack-off model instead of
considering a history of n minus 1 we
shorten it a little bit we're only going
to condition on the history of size n
minus 2 now if that history does exist
then and if we in our our corpus also
have the hole in 21 so the word and the
history then we're not going to use just
the maximum likelihood estimator we're
going to use a discounted version of the
maximum likelihood estimator if the
whole sequence does not exist we're
going to back off again and beta is just
a normalization factor what is that this
count factor well interestingly enough
it depends only on K the number of times
that an element appeared in the corpus
so already now you should recognize
something and when you look at this
expression here you may also recognize
something NK + 1 / NK this is actually
very close to the guttering estimator
which I'll show you in just this slide
so here cat suggest you use k equal 5 if
we have small counts we use this
expression if we have large counts then
we just use them at the maximum
likelihood estimator and as I said
before they're good reasons for doing
that first of all the council if the
counts a big they're probably more
accurate if then if they're small and
also the good cheering estimator is not
as accurate and the average or in a
single for small K so just to connect
between between the discount factor and
they're good cheering estimate here's
the discount factor that's sitting in
this very phone well actually it's not
because we are we are sending it is
sitting on the Google Network this is
the expression for the good for the
discount factor in the cats model so if
we write it here and say that will
approximate it by some approximate count
instead so it's like a maximum
likelihood ratio but with an approximate
count and use for that approximate count
the good Turing estimator we get exactly
the discount factor in the cats model so
Turing was decrypting the messages of
the German Navy today his decrypting the
messages on our phones and when we call
these phone menus that we don't like he
is still in use and his here to stay
now going beyond this because this is
not the end of the good Turing estimator
today we live in an uncertain world we
don't anymore have necessarily the
billions of text document on which we
can form our bases sometimes all we have
to construct new language models is the
output of speech recognition systems so
we won't have necessarily a nice string
of words in which we can form our
language models in which we can form our
counts and we can compute our good
cheering estimates so we can use the
cat's back off model we instead have
these lattices with weights that
represent the maximum likelihood of the
string given the audience so here's an
example where the maximum likelihood
estimator is actually probably the
bottom light the bottom path here I'd
like my court number while if somebody
had correctly tries to pretend scribe
the message you would get hi this is my
number this is the messy will that we
have to live with today and the question
is how do we learn a language model from
uncertain data like this well remember
the two things we need we need an
estimate of the counts and we need an
estimate of the good cheering on an
estimate of the good cheering estimator
it begins to be too many estimators in
here now the first thing we have to do
is go from counts to expected counts if
we have a lattice like this what is the
expected count that we can associate
with any string what we do is we define
it as simply as the number of
occurrences of a word x times the weight
that the latter's the transducer assigns
to this sequence
but how can we compute that one it turns
out that there is a very simple weighted
transducer that computes exactly these
counts for any regular expression
sitting in here it's here illustrated
for the very simple example of an
alphabet of this too so here if X is a B
and we have a string that comes in it'll
loop over here we eating an A and B and
epson outputting an epsilon the MS is
simple with weight 1 then we'll take the
transition if it sees the expression a B
and then a loop and that they can do in
two possible ways on the input there by
counting the number of appearances 22
assigning the weight of the transducer
to it another thing we have to compute
is an expected value for the an
extension of the good cheering estimate
and what is being done nowadays is that
you take and you now don't have along
you don't have em words as your as your
input you have em lattices as your input
and what you can do is you can you can
sample a path through the lattices and
if you do that in times we can actually
show since I got interested in this a
preparing for this talk that we can
again get a reasonably unbiased
estimator for the good during mass by
repeating this enzyme sample from the
lattices and take the average value so
we can extend the good touring estimate
to this messy world of today and it has
been done in a number of domains
especially when we talk about adaptation
you can imagine that you have a UI box
on your telephone or your your your
browser you speak
to read all that we have would be the
language other the lattices the output
of our speech recognition system and
reform now language models that are
specific to the UI box or you can have
it that you only we work in prison data
for some time that you all you have
you're looking for mining what the
prisoners they are saying to you to the
family when they call home you want to
build specific language models but all
you have is the output of the speech
recognizer when applied to this text
there are more examples if you and
application of it in the paper mentioned
here so I'm going to end here even
though I have three minutes ahead of
myself I hope you're all happy about
that I given a short tour through some
of Turing's work he decrypted messages
under the Second World War the same
technique is used today and we couldn't
have done all that work without it and
it's here to stay this is just one
example of how we continue using his
great work because it was indeed truly
great so thank you
yes i'll talk about in machine learning
the site
direct link yes they most certainly is
like this a good paper from 53 and it
has been an increasing amount who worked
on here over since 2000 on proving more
properties about it because it turns out
to also work so very well so yes do you
see a unification of the different
physical models between say looking at
taksim looking at images because when we
study about this we use language models
for text based analysis but we use
different types collado firm image
analysis you see emerging of the field a
unification here it really comes around
with the sparsity issue and to the
extent that you have the sparsity she
also and images I would encourage you to
to try some of the smoothing techniques
from the text-based the world there's
actually a very recent release of all
these smoothing techniques that Google
put out at their open grm library I came
out just a month ago and I encourage you
to try using some of these but i'm not
sure that you're in the same way have
counts and you never have sparsity
issues because image world is just a
very very different character</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>