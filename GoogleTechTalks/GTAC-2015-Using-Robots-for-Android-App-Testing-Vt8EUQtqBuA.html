<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Using Robots for Android App Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Using Robots for Android App Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Using Robots for Android App Testing</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Vt8EUQtqBuA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Yvette Nameth: So next we have Shauvik Roy
Choudhary, who is going to talk to us about
-- I cannot remember the title -- there we
go -- using robots to test your Android app.
So take it away.
&amp;gt;&amp;gt;Shauvik Roy Choudhary: Hi, everybody.
This is the -- this is my second time at GTAC.
Last time, I led a roundtable discussion on
cross-platform mobile testing.
And now, this time, I'm glad to come back
to talk about using robots for app testing.
So a -- So one thing, right, so I'm going
to talk about, you know, software robots and
not like the OptoFidelity guys who showed
a real robot.
So a disclaimer at the start.
And let me tell you a little bit about myself.
So I got my Ph.D. from Georgia Tech in the
summer.
My thesis was in the lines of, you know, how
to do better cross-platform testing and maintenance
for Web and mobile applications.
Right?
And over the past few years, I've been lucky
enough to work at several companies.
Got a lot of industry experience.
And in the past one year, I have been taking
part in the different entrepreneurship programs
at Georgia Tech, you know.
And that has led me to Checkdroid, which is
an early stage startup.
So if you have any inefficiencies that you
guys think -- in testing, reach out to me.
Find me during the break.
So today's talk will be mainly focused on
presenting results from this research paper
that we wrote; right?
So automated test input generation techniques
for Android.
Are we there yet?
That's the question that we are trying to
address in this paper.
And my collaborators for this project were
my advisor, Dr. Alex Orso and Dr. Alessandra
Gorla from Software Institute, Spain.
This paper will be presented in two days in
Lincoln, Nebraska at the ASE conference.
Okay.
We want to use robots for testing.
Right?
Now, but let's first see, you know, how a
test structure looks like.
So we all are familiar with this; right?
Test structure has, you know, three different
-- four different steps.
Start.
You know, you'll have the setup phase where
you initialize your environment and get it
ready for testing.
The next phase is the exercise phase.
And what you do here is that you, you know,
interact with the app, provide it inputs and,
you know, exercise it.
Right?
In the next step, which is verify, you will
check whether the app, you know, came in expected
state or not, (indiscernible) the inputs that
you gave it.
And final step is the teardown step, where,
you know, you clean up the environment and
get ready for the next test.
Right?
So how will a robot or an automated technique
do these steps; right?
So let's say, you know, we have a robot for
this test setup phase.
It can say that, okay, let me start the app.
Right?
Let me install the app on the device and let's
start it.
For teardown, it can say, let me stop the
app and uninstall it.
For verify, it says that, okay, let's have
an article or a policy that the app may not
crash, it might not show any issues that you
can identify.
But the most important or the most challenging
thing for such a robot is what inputs should
it give the application so that it could meaningfully
exercise the application; right?
And this is the main focus of this talk where
we are going to evaluate existing input generation
techniques; right?
So let me first introduce you to the simplest.
And maybe most of you should already be familiar
with the Android Monkey tool.
Who here has used the Android Monkey tool?
Many of you.
So this is a tool which comes as a part of
the Android SDK.
If you have the Android SDK installed, you
should have this tool.
And you can invoke it using ADB shell, right?
So you can say ADB shell, give it the package
name for your app, and give it a number of
events, let's say 500, right.
And what it will do is it will just pick random
events and throw it at your app to invoke
it to make it do different things, right?
And this tool also has some advanced options,
right?
So since it's using randomization, you could
set a number seed for the randomization, right?
You could also throttle this tool.
You could tell it, okay, between different
events, give this much time or give this much
weight for this many milliseconds.
You would tell it when choosing the different
events, right, focus more on this kind or
this category of events so you can give a
percentage distribution.
And by default this tool will stop after it
finds the first issue.
So you can tell it to ignore issues like exceptions
or timeouts and so on.
This is one example of a robot, right?
And we looked at academic papers and industry
tools and found that there were several such
tools which were out there in research papers
and in blog posts.
So we wanted to see how do these tools compare
against each other, right?
And is there need for additional research
yet, right?
So that's the question we asked.
For this input generation tools, have any
of these tools solved the problem?
And are we there yet?
That was the question we were asking.
And while doing the study, we considered these
tools as robots, right?
We considered them as push-button techniques.
You give this tool your app and start it,
and it should test your app and give you all
the problems that your app has.
So let me clarify a little bit the goal of
testing in general, right?
So goal of such a robot would be to make the
app crash and also to exercise the different
behaviors that are there in the app, right?
So if the robot exercised a particular behavior
or a particular functionality of that app
and if it could not crash that functionality,
then you have at least some level of a confidence,
right?
And for this, typically coverage is the weight
or the proxy to measure this kind of a thing.
Each of these tools is different from the
other based on the strategy it uses.
I am going to show you four things that actually
inform the tool strategy.
The first is the instrumentation strategy,
right?
So each tool has to interact with the app
and know or understand what happened as a
result of this interaction.
So the tool might modify the app by injecting
probes in, or it might modify the platform
to know what's going on in the platform, right?
So this is what I mean by instrumentation
strategy.
The next one is: Does the tool interact with
the application through -- only through a
UI, or does it also send a system events?
So, for example, parts of your app might be
triggered via external notifications or by
SMSs.
Let's say your system gets an SMS and your
app is listening to that.
In order to trigger such functionality, you
have to send the system events in order to
see which tools do what.
The third one is -- does the tool require
the source code of the app to do the testing?
Or can it do it in a black box fashion?
And the fourth, and the most important strategy,
is the exploration strategy.
This decides how this tool is going to explore
the state space of the application, right?
I'm going to cover this in more detail.
There are three such strategies, right: Random,
model-based and systematic.
Let's dive into this a little bit and look
at the different exploration strategies that
these tools have.
So the first one is random, right?
So here it's just like rolling the dice and
picking a random number.
So these tools, the tools in this category,
Monkey and Dynodroid, what they do is they
randomly select events and they just throw
it at the app.
So the good part about this is that you can
do this efficiently, right?
You can randomly pick so many events and you
throw it at the app.
However -- and also it makes it very suitable
for stress testing because you can see how
well your app behaves when so many events
are thrown at it, right?
However, the problem with such techniques
is that they are hardly specific.
You can hardly tell Monkey, Okay, Monkey,
make my app go to this particular activity
or this particular state, right?
So they're not specific.
They're not agnostic -- they are agnostic
of the behavior.
So if you have already covered some functionality,
you will -- the tools don't consider that,
and they might have latency in the events
that they supply.
They might be testing the same thing again
and again.
And, finally, these tools have no stopping
criterion.
They don't know have they tested enough.
Typically, as I have shown in Monkey's case,
you give it number of events to fire.
Other approaches could be to have a timeout,
like let's say I test my app for an hour and
tell me what all you found, right?
So this is about the random exploration strategy.
Now let me talk about the second strategy
which is the model-based exploration strategy.
These tools, what they will do is as they
are exploring your application, they will
build a model of the application, right?
And this model is typically GUI based or a
GUI-based model which can be -- an example
of this model is shown on the right-hand side
for the dialer application.
So this is a graph-based model where the states
are different GUI screens and the edges are
transitions or actions that you do in these
states to go into another state or screen,
right?
So the tools in this category are A3E and
SwiftHand and GUIripper and PUMA and Orbit.
Intuitively, these tools are more effective
because you know what they are testing.
And these tools are also less redundant because
once they know I have reached this activity,
I have seen it before, I don't have to test
it again.
So they are less redundant than random tools.
However, these tools only consider the GUI
state space.
They don't know what's going on behind the
GUI, so they cannot model them.
They currently don't model them in the model.
Now, let's look at the third category, which
is a systematic exploration strategy.
So these tools, what they do is they use sophisticated
techniques like symbolic execution or evolutionary
algorithms to systematically cover the state
space of the application.
So the two tools in this category, ACTeve
and EvoDroid.
For the first tool, ACTeve, it uses symbolic
execution.
What it means is that generally it collects
constraints from within the application when
it's executing.
For example, on the right-hand side, you can
see a constraint which is the on the X and
Y coordinate that the tool has to click.
Once it gets this constraint, it can pass
it to a SAT solver, get solution for a constraint,
and use that to test.
Once you have tested this part, you can flip
this condition to get other parts and you
can test other parts, right?
So this is one way these tools operate.
And the good thing about these tools is that
they can systematically explore, right, more
than -- they can direct the testing towards
one particular part, right?
However, because these techniques are heavy
weight, they are less scalable than random.
So these were the three different exploration
strategies.
And we considered a subset of the tools because
the other tools, they were not available.
They were only in a research paper, but they
were not open source or available for testing.
And some of the tools were focused only on
testing of a very specific part and they are
not generative enough.
As I show in this table, these are seven tools.
Monkey you know, well, is from Google.
The other tools are academic tools: ACTeve
Dynodroid, A3E, SwiftHand, GUIripper, and
PUMA.
The second column shows if the app tool needs
instrumentation or not, does it modify the
app or the platform?
And as we can see, Monkey does not and PUMA,
which is the last tool, does not modify anything.
PUMA uses the accessibility API and through
that, it can know the status of your app.
Whereas, ACTeve, that tool modifies both the
platform and the app to get the symbolic constraints.
Dynodroid modifies the platform.
Whereas, the other three tools modify the
app.
As far as the next column shows, what are
the events that the app supports -- or the
tool supports, right?
So all the tools support UI events.
They can interact with the application through
the GUI, right?
Whereas, only two tools, ACTeve and Dynodroid,
support system events.
I explained the exploration strategy before.
As you can see in this, two tools, Monkey
and Dynodroid, are random tools.
A3E is systematic.
And the other four are model-based tools.
Out of all these tools, only ACTeve needs
the source code.
All the other tools can operate in a black-box
fashion.
Now that I have introduced you to these tools,
let me tell you about our experiments and
how we evaluated these tools.
So before doing our experiments, we had four
research criteria.
The first one was how easy is the tool to
set up and use, right?
And the second criteria was: What are the
different versions of the Android framework
that these tools support?
The third one was: How much coverage can these
tools achieve on general subjects?
And the fourth was: What is the fault detection
capability?
Or can these tools reveal failures in applications
automatically?
So for our experiments, we used 68 open source
subjects.
All of these were used in some or the other
tools on evaluation.
But we considered a union and even more subjects.
And as you can see, this shows the distribution
of the subjects and the categories from Play
Store.
So for our experiments, we ran them on a Linux
Debian machine.
Since we wanted to have repeatable, we used
virtualization technology.
We used VirtualBox for that, and Vagrant to
control the virtual box.
On top of this, we had Ubuntu Guest on which
we had the emulators running.
So for our experiments, we chose three versions
of the Android emulator: 2.3, 4.1, and 4.4.
This was because some tools were specific
-- you wanted specific versions of the Android
emulator, and Kit Kat was the latest when
we did this experiment.
So what we did was on the Ubuntu Guest VM,
we installed this tool.
We removed any default timeouts that these
tools had so we wanted to run all the tools
for a specific time.
And we used their default configuration, which
means that we did not use any specific tool
to avoid bias.
So now let me -- now that I have shown you
our experimental setup, let me show you how
we did this experiment.
What we did is we took each tool and for each
benchmark we ran it for one hour.
Then since there can be non-determinisms while
running the Android SDK, things can crash,
things cannot behave in a deterministic fashion.
So to account for these things, we ran these
experiments ten times and ignored anything
that was outside the standard deviation rate.
And we collected reports.
For the results, we collected two things.
The first thing was every five minutes we
collected the coverage report, how much did
the coverage improve over time.
That's what we collected.
And we also collected the logcat or the logs
from the device from which we extracted failures.
So specifically what we did was we used the
default Emma tool.
Emma, if you know, it does not give you statement-by-statement
level coverage information by default.
So it's presented in the HTML reports.
So we extracted this information from the
HTML reports.
And for logcat, what we did is we wrote a
custom parser based on regular expression
which extracted unique stack traces.
We wanted to have unique stack traces to find
different failures.
And we verified these manually to make sure
that our parsing logic was correct.
So now let me show you the results, right?
So for the first two research criteria, ease
of use and Android compatibility, we manually
set up these tools in our infrastructure and
evaluated them.
So as you can see, two tools, Monkey and Dynodroid,
required no effort.
That was because Monkey comes as a part of
Android SDK.
We did not have to do anything, right?
In the case of Dynodroid, the authors actually
released a virtual machine.
So it was very easy for us to reproduce and
recreate their experiments.
And they could easily use the tool.
For two tools, A3E and PUMA, we had to spend
a couple of days to fix minor bugs, and we
got them running.
For three other tools, ACTeve, SwiftHand,
and GUIripper, it took us weeks to interact
with the developer because these tools were
not maintained anymore.
But we finally got them running.
As we can see on the third column, ACTeve
and Dynodroid, both of them modified the framework.
So they have a tie dependency on the framework
version.
And the version they provide only works for
Android 2.3, Gingerbread.
SwiftHand supports Android 4.1n+ because that's
when it was built.
And PUMA supports Android 4.3+ because it
uses the accessibility API which was majorly
enhanced in 4.3.
And three tools, Monkey, A3E and PUMA can
work on any version of Android.
So now let's get to the coverage results.
So this graph shows you the coverage -- the
distribution of coverage fall across all the
different apps for all the different tools.
So as you can see in this graph, for some
applications, most of these tools were able
to get high coverage.
But the median is very different, right?
So, for example, both Monkey and Dynodroid
have high median values.
But the last three tools have very low median
values.
Let me show you the next result which shows
you the progress of coverage over time.
So this graph shows you how the coverage improved
over time.
So as you can see, in the first five minutes,
all the tools were about to -- could quickly
get almost -- max coverage is 40% coverage.
So this shows that even if we had run this
experiment for another hour, it would have
still been flat, right?
So it wouldn't have improved the results drastically.
The only exception here is the pink line,
which is the GUIripper tool.
What that tool particularly does is it creates
VM snapshots at intermediate steps and it
restarts the application from those snapshots.
That's why because of this continuous restart
it takes a long time for that tool to reach
its max coverage.
Now let me show you the next result.
So this shows the mean of coverage across
all the different tools.
So as you can see, for some apps, all tools
could get 80% or 90% coverage.
Whereas, for some apps, they got really less
coverage.
Most of the apps were in the middle, right?
So we wanted to understand why.
And we saw that on the right hand of the spectrum
lied very simple apps.
Some of these were samples from Google, and
they had a very limited state space.
All the tools could get decently high coverage
on them.
On the left side of the spectrum were tools
-- were apps which required -- for which none
of these tools was able to cover enough state
space.
We found these apps required a lot of manual
configuration.
For example, the first icon there presents
K9mail.
It's an email client.
It requires to you set up an email server,
set up that configuration, right.
And for our experiments, it was not possible
because it's an external dependency that we
were to release it after every experiment.
But we found that this was a great learning
that for such tools it is harder to test them
by any of these automated robotting testing
tools.
So now let me show you results from the fault
detection ability.
So here what we did was we counted the different
failures that were invoked by these tools,
right?
And as you can see, the failures are colored
which means -- which shows the different packages
they belong to.
For example, the blue color represents the
java.lang.
And one representative exception in that package
is the null pointer exception, right?
So as you can see, in the case of Monkey,
it could get a lot more java.lang exceptions
than other tools.
Whereas, for a tool like PUMA, it got java.io
and java.lang.
And the pink color exceptions are the custom
exceptions which were defined in the app itself.
And only two tools, which is Monkey and SwiftHand,
would reveal those exceptions.
So we also wanted to compare these tools pairwise,
which means each pair of these tools.
The next graphic is going to show you that.
So in this chart, above the diagonal, you
see the coverage results for every pair of
tools.
Below the diagonal, you see the failure results
for every pair of tools.
Let me explain this in a little bit more detail.
So let's look at coverage.
So here you can see that two tools, Monkey
and Dynodroid, they had -- 43% of statements
were covered by both of these tools, right?
Whereas, 5% of statements in the subject applications
were only covered by Monkey and 8% were only
covered by Dynodroid.
9%.
Similarly for the failures, as you can see,
six exceptions or six failures were common
between GUIripper and A3E tools.
Whereas, 40 were invoked by GUIripper and
20 were invoked by A3E.
Now I have explained you this, you can see
in the overall graph, in the statement coverage,
we still have -- there's a lot in common between
the tools, right?
But there is a lot more that needs to be covered,
right?
Most of these tools just reach 50%.
And for the exceptions, as we can see, there's
very less common between the tools.
So you need more than one tool to expose those
different failures, right?
So our first observation is that random approaches
seem to be doing their job, right?
Random number of approaches seem to be effective
in this context.
Dynodroid and Monkey are both doing well,
right?
Now let me now present you our findings, which
I'll split in two different categories.
First is best practices that we saw in some
tools which made them stand out and limitations
that we saw in tools that none of the tools
implemented these practices, right?
So let's talk about best practices.
So the first thing was, was that in tools
like Dynodroid, what they did was they invoked
-- they sent system events; right?
Which can -- for example, SMS.
So if your app is listening for SMSs, you
wouldn't (indiscernible) any failures in that
part of code, unless you sent a system event
to it.
So this was a very good feature of those tools.
Then, as I mentioned to you, the tool GUI
took a long time to get its max coverage.
So they should minimize the restarts in the
applications that they have.
This is a very common problem for, you know,
robotic crawlers.
So what these tools do is, they get stuck
whenever they see a screen like this, because
a robot will not know what to enter here;
right?
So specific tools like GUIripper and Dynodroid,
what they did was they allowed to -- in the
configuration, you could specify the different,
you know, user name/password pairs, right,
or different values that the robot could they
use and get past such screens.
Then an application -- the behavior is dependent
on the start state.
For example, for the mail client, if there
is no email messages in its content provider,
it will not, you know, expose any behavior;
right?
So tools like GUIripper allowed the specification
of different starting states by loading the
app in that particular starting state, right,
and letting the tool run after that.
And the final one is that in our experiment
infrastructure, we use virtual machines; right?
So we could, you know, clear everything after
an experiment.
But some of these tools, what they allow for
is a partial cleanup, of uninstalling the
app and clearing its data; right?
So we thought that that was also a very good
feature in these tools.
So now let me talk about, you know, some limitations
or future work that these tools need to improve
on.
So the first one is reproducibility; right?
So a robot might make an application crash,
but it's very hard to actually look at the
robot's log and to figure out what did it
do to, you know, crash the application; right?
We believe that such things should be a part
of the tools output; right?
It should give you a test case, ideally, that
you can (indiscernible) produce that kind
of bug.
And we thought that none of the tools had
this.
The next thing was, when you're testing, you
always test in a hermetic environment.
And none of these tools allowed for you to
specify mocks.
And they all interacted with the real systems.
The third one was that while you're doing
testing, there can be some harmful operations.
Let's say you're testing the Uber app.
If you're not testing it in the right fashion,
you actually might order a real cab; right?
Or you might send out real emails or SMSs.
You don't want to do that.
None of these tools allowed for sandboxing
to avoid these side effects.
You all are familiar with this picture.
There was no robot or no tool which was focused
on finding issues that are across different
devices; right?
So none of them were targeted towards cross-device
testing.
So, in summary, what I showed you was, I motivated
the fact of how you can use a robot to do
the testing.
I showed you different exploration strategies
that current tools use; right?
I showed you details about our experiments
and the results.
And I've shown you some best practices and
limitations in the existing tools.
So all our -- the virtualized infrastructure
is available at this URL.
You can also go to my Web site, Shauvik.com,
and get a link to that.
That virtual machine has all these tools installed
already with all the subjects that we used.
So you don't have to go and build these tools
from source.
You can just get this and use it in your projects.
And that brings me to the end of my talk.
Thanks.
[ Applause ]
&amp;gt;&amp;gt;Yvette Nameth: Thank you.
I think we have time for a few questions.
So, first off, do you have any recommendations
for iOS robots?
&amp;gt;&amp;gt;Shauvik Roy Choudhary: There's a tool called
UI Auto Monkey.
It's available on GitHub.
It's the counterpart of Monkey for iOS.
It uses the UIAutomator API, right, that iOS
has.
So you could use that.
&amp;gt;&amp;gt;Yvette Nameth: These tools may be able to
find app crashes, but how do you then come
up with a repeatable set of steps that can
reproduce the problem and help find a root
cause?
&amp;gt;&amp;gt;Shauvik Roy Choudhary: So this is the hard
part.
For example, in Monkey, you can ask it to
be verbose, and monkey will tell you it did
these steps; right?
And then you see this exception; right?
So what it means is that you have to go through
all these steps, and you have to find a minimal
set of steps.
Right, you have to reduce this to a minimal
set of steps that can help you, you know,
find that problem; right?
So this is actually a very hard thing.
And we are working on, you know, some of these
areas to generate test cases.
But, yeah, so there's currently no easy solution
for this.
&amp;gt;&amp;gt;Yvette Nameth: Can you propose a new way
to compare the quality of tests generated
by these tools beyond those of coverage and
crashes?
&amp;gt;&amp;gt;Shauvik Roy Choudhary: So coverage and crashes,
you know, of course, I showed it.
Another thing is that, okay, your test cases,
right, if you have -- you should have a minimal
set of test cases that can generate, you know,
the same errors; right?
So let's say if you have 100 test cases that
give you five errors versus you have five
test cases that can give you five errors.
And these tests -- and, of course, the same
coverage as well.
So having a minimal set of test cases is also
something that people, you know, look out
for, because then you can run these test cases
fast, right, instead of running (indiscernible)
tests.
&amp;gt;&amp;gt;Yvette Nameth: Cool.
And I think that's all we have time for.
So thank you very much.
&amp;gt;&amp;gt;Shauvik Roy Choudhary: Thanks.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>