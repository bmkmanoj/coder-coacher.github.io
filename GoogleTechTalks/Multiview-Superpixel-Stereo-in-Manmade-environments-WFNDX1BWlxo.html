<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Multi-view Superpixel Stereo in Man-made environments | Coder Coacher - Coaching Coders</title><meta content="Multi-view Superpixel Stereo in Man-made environments - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Multi-view Superpixel Stereo in Man-made environments</b></h2><h5 class="post__date">2008-04-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WFNDX1BWlxo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you for coming I would
to present a talk by Branislav McKusick
who is a postdoc visiting at Stanford
University and Jana costa chica are a
very own expert on structure from motion
and stereo and they will talk on how to
do plane sweeping stereo in many
environments
hi so I'll just tag along here for first
few slides and then I'll pass the token
to Branislav so I just would like to
provide a little bit of a motivation and
put the work in the context of some
related work which many of you are
familiar with so I hope the approach of
acquiring 3d models from images is no
news to this audience and it has been
sort of proven very useful in the
context of many settings there are a lot
of interactive systems to do so there
are a lot of automated systems so in
this talk will present an approach which
tries to advance the classes of
techniques which drive to do it in an
automatic manner and then the set up
base that given multiple views of the
scene you would like to automatically
recover the 3d structure and this is
also known as multi-view stero in
computer vision community and this is a
broad class of application to general
modeling for example historical
preservation sort of urban modeling of
cities indoors outdoors environments and
it provides a good alternative to other
sensing technologies which are commonly
used in this setting for example laser
range scanning because first the the
images have fairly good lateral as
oceans you can capture them in high
resolution and high frame rate and the
sensors are relatively inexpensive and
there is easily accessible their passive
and there are easily transportable so
you can use this technology in the
places where it might not be easy to
sort of the carry the laser rangefinder
so the general ingredients of which will
instantiate here of the multi-vue sara
is the fact that you assume that the
camera poses are known and then there's
some choice of photometric consistency
function which I'll describe what it is
in a second and then the choice of
represent
asian and algorithm so the photometric
consistency function is really the the
crucial part of any algorithm which is
using multiple views in order to reason
about the 3d structure of the scene and
the basis of that function is that if
you would like to reason about some
point in 3d and you would like to know
whether it's automatically resistant if
you take a race and project those reins
into the individual points in their
respective views if the projection came
from a particular point in the 3d then
some photometric properties of those
projections have to be consistent and so
the respective errors should be
different and if it didn't then there
should be some differences but already
on this picture you can easily see that
and as you will see you can have
environments where often if you have
inconsistent points if you do not have
properly designed photometric
consistency measures they will appear to
be sort of consistent in a sense of
having very small differences so if all
the surface of this of the shape would
be uniformly shaded or it would have no
texture it would appear equally likely
to come from different direction so
these are some of the issues where they
are which are the algorithms need to
resolve in order to achieve robust
solutions in the presence of this
ambiguities so the general ingredients
have any algorithm that is that you
assume that you have known camera poses
so we are not going to be discussing the
problem of how to estimate where the
cameras are and that's a separate issue
and then once we choose the photometric
consistency measure we'll pick a
particular representation the class of
problems which will focus on in this
talk is that will actually assume that
we'll have only a sparse set of views
from which the the scene is observed and
this is quite important because if you
have a spare set of views then you can
accumulate this photo mat
consistency measure you have only small
number of exemplars which you have in
order to reason whether the things are
consistent and we'll also discuss how to
deal with the regions which have low
texture and repetitive structures which
are also the cases were the standard
photometric consistency measures under
standard representations do not handle
well and these are all the instances
which you typically encounter very
frequently indoors and outdoors
environments and urban settings so this
is just a couple slides of the review of
the existing work so most of the
multi-vue techniques fall into this
category of pixel based where the
problem is formulated of given each
pixel in some reference view computer
depth of each pixel by viewing them in
sort of perspective views and there has
been a great success achieved in this
type of formulation but most of the
settings assume i'll either highly
textured objects as sort of reflected in
the standard benchmark data set of the
middlebury stereo webpage and data set
and they typically assume that you have
very large number of views and very
small baseline so for example the
picture on the right has been
reconstructed from 128 images taken from
Flickr so the given that you have such a
large number of views then you can get
by with weaker photo consistency measure
because basically every patch is being
seen at least in sort of 50 views so the
question is what can you do if you have
small number of view so imagine that you
would like to do it only from three four
five views or whatever views you need in
order to cover the visibility in a sense
of each patch being visible at least in
three different views and another kind
of setting which is hard to adopt in
urban environments that we often do not
have control settings for a quiz
of these images and often the lighting
changes dramatically and that sort of
causes the problems with the photometric
consistency measure so another sort of
motivating line of work which motivated
us to rethink some of the basic things
are sort of recent developments in the
single view modeling were the single d
modeling techniques which came out in
sort of couple years back assume as
opposed to trying to recover the depth
per each pixel tries to formulate the
depth recovery problem at the level of
Super pixels so given some region over
segmentation of an image try to either
assign a depth or label to each super
pixel such that sum over all consistent
consistencies maintained so the picture
and the movie on the right you see the
single view reconstruction of the work
of home and I froze we're here the
problem is formulated as an optimal
labeling problem where each super pixel
is assigned one particular label where
label corresponds to one of the three
normals so vertical horizontal or or or
sort of the backplane and so here you
can sort of achieve very impressive
reconstructions but the number of labels
is very limited so you can only get hold
of the very core structure and then the
picture on the bottom is an example of
the reconstruction which was achieved by
by the approach proposed by ashutosh
Saxon and Andrew ank were here again
there is a mapping learned between an
appearance of each super pixel and the
depth which then enables you to estimate
the kind of posterior probability of
that given an appearance of the super
pixel and it's probably hard to see in
the movie but you can see that it is not
the case that the front of us out of the
building is actually straight and there
this kind of curved effect which due to
the formulation kind of smooth is out
the entire reconstruction and that's
also partly due to the fact that those
mappings are in a sense quite weak
because whatever you can measure locally
in a super pixel doesn't usually convey
sufficient amount of the information in
order to reliably estimate and propagate
these constraints so I think that's all
I have to say and I'm gonna pass the
token to branch out to see how about
couple ideas how we thought of improving
the current state of the art so thanks
Jana for introduction and now let me let
me continue and let me present our
approach our April for 3d reconstruction
from the collection of from a small
collection of white base line images
especially the focus on man-made
environment and this kind of environment
is very very difficult because there is
often no texture as it is depicted in
the images at the bottom or there is
very repetitive structure texture as it
is depicted in the images at the top so
this usually the reason why the standard
methods fail here moreover we want to
handle the strong or illumination scale
and viewpoint changes and possible
occlusions so the aim of our work is to
handle all such scenarios and produce a
reasonable feasible 3d reconstruction as
shown here it's Jana said there are
plenty of methods for automatic 3d
reconstruction one of them is the web
application our query which utilizes the
many state-of-the-art techniques in the
whole chain
of dense stereo reconstruction so we
just upload a couple of images or your
your collection your data set and it
automatically compute the 3d model and
it returned is back through and you can
upload it from the from their web page
it perfectly boards for many scenarios
and for many scenes however as you can
see they all all the scenarios have
something in common and it is that they
are very well textured and second the
usually to obtain such results you need
to have quite dense Quan a quite dense
set of data so you need many images
however if you feed into the system our
data our building data set you obtain
the result shown here so this is the
frontal view so this is obtained from
these four images on the left and this
is the top view so you can see that the
reconstruction is correct only for small
subset of points and then the rest is
wrongly and incorrectly reconstructed
but note that this method does not take
into account any prior information about
the scene which is which is reconstruct
reconstructed so incorporating correctly
some priors about the observe scene like
these vice planarity you can obtain much
much better result you still have some
some some holes here but this is just
because we took only four images for
that and this is our top view so you can
see that all planes rural dominant
planes were correctly reconstructed
compared to this result
so in the next I'm going to speak about
our method so let's now assume that we
observe a rigid seen by multiple cameras
and we will assume the man-made
environment and we will assume the
piecewise planarity and assumed the na
we have a noun camera projecting might
matrices so we know the calibration of
all the cameras and you know policies
with respect to one reference camera so
let me first explain how standard
sweeping techniques work to obtain a 3d
model so the goal is for each pixel in
the reference image I'll find the
corresponding 3d position so what we can
do as everything is calibrated the whole
setup we can construct a projective ray
going through the origin of the camera
and say okay now now our point will be
somewhere along this ray so let's take
one of these points at the depth D and
project it into the other views and the
aim is to sweep this 3d point along this
way to obtain such projections in other
views to obtain some highest confidence
or the best
best similarity measure of all the
projections with this particular pixel
however this is very sensitive to camera
or camera pulses and very sensitive to
illumination changes so therefore
usually instead of single pixel the
fixed window around the pixel is taken
an instant of sweeping single pixel the
small planner patch is swept but then
buy this thing we introduce extra
parameter which is the normal of this
small planner patch so it is then
projected quite differently compared to
to to the reference image something like
this if we choose another normal then it
is projected in the different way then
usually this problem is formulated in an
optimization framework very want to
optimize over all normals and overall
depth and overall pixels in the
reference image and we want to get a
minimum in sense of a sum of two energy
functions first energy function is
responsible for the consistency of all
the projections across the views so it
operates only on pixels and the second
term is just a regularization term
enforcing the same 3d properties of the
reconstructed pixels which are in the
neighborhood in the reference image
however this problem is intractable to
do it for all normals and order depths
and therefore order of simplifications
are taken into account to get the
reasonable resultant to get the trip
table method for solving this problem
the first approach so I have here three
recent approaches first approach suggest
to do this optimization problem in two
stages first the deaths is estimated and
in the second stage the normal is
estimated in the second work they simply
say
the window around the pixel it's very
small and therefore the normal is
negligible and optimized for depth only
the third approach is the closest
approach to our method it also assumes
the man-made environment and they assume
that there are only some dominant planes
in the scene we aim to reconstruct so
the larger window which I mentioned is
very beneficial so the more robust
method to with respect to camera poles
inaccuracies can be designed and more
about photometric measures can be
designed like a normalized cross
correlation and all these measures on
more than one pixel however if large the
window is more likely it is that the
planner assumption will be violated
because one important assumption here is
that this small window is also planner
and it comes from a small planner page
from the sea so for example it of pixel
comes from the projection of the three
point which is on the facade of the
building and it is very close to the
edge then the half of the window would
not correspond to to to the page on this
facade so therefore there is a drawback
between increasing the lawyer although
the size of the window the last thing is
the marquam random field or the graph
which is built from from the pixels in
the reference image it's very complex
because it is built on all pixels in the
image so we were thinking how to use all
the advantages of having larger window
but somehow suppress these disadvantages
and we ended up with a solution that
lets first pre segment damage into
so-called super pixels and formulate the
whole problem
the whole problem on on the super big
soles so the super pixels is you see
they usually fall or the image gradient
and they are inside somehow consistent
in color for this reason or for this
purpose we use the minimum spanning tree
based super big small method and we
ended up with such presentation image so
you can control the size of the super
pixels so we have large support area
because usually you see the super pixels
are quite large and as we build our
graph as I will show later in our
Markham Randall field on the super
pixels then the computational complexity
is much less just by tau x factor
thousand or something and we formulate
the searching for maximum a posterior
probability of the markham random field
as a library problem is I will show
later so our goal is the same as it was
before 44 pixel-based error but here we
switch to the super pixels and we want
to find depth and normal for each super
pixel in our reference image in this
case it is a second camera so we again
sweep the super pixel along the
projective code and try to project these
small planner pitches into the other
views in such a way that we want to
optimize such kind of function I will
explain it into more details later just
for now you see that we have more more
terms here compared to the function I
showed the beginning just because super
pixels allow us to define more functions
and we can gain from them much more
information compared to a normal fixed
window around the pixels but still do
the optimization overall nor
and over all Deb's is intractable and we
need to take some assumptions into
account so first of all we restrict the
number of normals so as we operate and
we assume that we operate in a man-made
environments the domain and the
directions can be captured we are
vanishing points so first we take the
reference image we feed the lines and
group them in such a way that the lines
in each group across the one point
called vanishing point so for this image
we have three vanishing point here is on
the left here is on the right and one is
almost infinity so and from this since
we have collaborated camera we can
directly compute our normal so unusually
there are freedom in orthogonal
directions in in standard scenarios the
method I'm going to to speak about it's
not restricted to the number of normals
but usually we operate only on three and
it's the sufficient assumption can be
easily augmented by a repertory number
of normal directions so we restricted a
number of normals here in a sweeping
strategy to three and now we need to do
something with with depth so we propose
to sweep along the depth range and
remember only the depth candidates and
optimize this function only over the
deaths candidate and dominant normals
instead of optimizing it overall normals
and all depths so let me now explain how
we obtain the depth candidates so again
the same image and since we assume that
the super pixels come from projection of
small planner pitches we can find easily
homography mapping the super pixel to
two other views and here is the
formulation of the lomography so we know
calibration matrix in all rotate
we know translation vector what this
year only unknown is normal again and
the debt so I let's show it on the
example I randomly picked one depth and
one of the three normals and one super
pixels from the reference image here and
I computed a homography and projected
the super pixel into the other views so
here this is a projection in a second
image third and four here there is no
projection because it's out of field I
mean the 3d patch here it's not
projected to to this view so you see
that I chose wrong depth because the
projection is not a deposition where it
should be so now the question is okay
let's go to sweep along the route of the
projective cone but how to measure that
these projections are okay so again the
same image and now what we propose is
first take all the super pixel
projections and first photometrically
normalize all the pixels inside the
super pixels just because you see the
illumination changes across the views
are quite significant if you compare the
third view and the first view it's quite
quite different so but at first before
normalization we remember the
chromaticity error which is computed
over all three color channels and after
that we transform all the pixels inside
the particular super pixel projection in
such a way that the mean in all color
channels in RGB 0 and the variance is
one so by this we can continue and we
can start just building a simple
normalized histograms so this is the
histogram in three color channels for
all the pixels inside is super pixel
this histogram comes from all the pixels
inside its projection and so on but the
histograms are computed
on photometrically normalized pixels so
the next stage is ok we have histograms
so now we need to compare them we need
to say how this histogram is similar to
one computed in the reference image and
so on for for the other images so we
propose to to design this function as it
is shown here so the first we compute
the kiss ker histogram difference
operator this measure operates on two
histograms so you feed their two
histograms and as a result there is a
number of similarity plus we have here
the difference between the chroma city
vectors which I showed in the previous
slide because the chroma city error
takes or stores the information about
the color so when we have this partial
measurements we want to combine them in
a one composite photometric measure so
we just simply sum them but this is done
in miracle library way than it is
depicted here because we don't take all
these partial distances we do it only
over some subset because we want to
handle also occlusions but i'm not going
now into detail how this is done but we
actually take care of equations so here
is everything again so this super pixel
projections are for the depth five and
now let's sweep this particular single
pixel and increase the depth WC there is
some movement also here there is a
change in the histogram shape so let's
move again and you see that it somehow
approaching the correct position but
still we don't have a projection here
here in this graph all the thin curves
correspond these to these partial
measurements to the similarities between
particular
histograms to the histogram computed
over the reference super pixel and the
composite photo consistency measure is
depicted here as a black bold curve so
when we move further you see now this
projection appears here but still is not
a correct position let's let's go
further so you see now we have all the
projections at a correct position where
it should be and this is also indicated
by the minimum our composite photo
consistency measure if we overshoot this
correct depth again have some
misalignment of of the projection and
also this is indicated by by by change
in the histograms so and these are other
decks candidates we take and best mima
as depth candidates and store them in a
matrix so then for each super pixel in
the image we have like in our
experiments we usually take only three
best minima so 480 pixel we remember the
matrix T and the matrix d has so many
columns as
you mean our commotions this was
actually so you can have for example you
can have a super pixel imagine this one
here and there is a lump in front and
when you project the particular super
big cylinder other views then there will
be the no correct projection because the
lamp lamp we'll go and we'll be in front
of your super pixel you are interested
in so it will somehow poly or your
histogram measure so this is actually
done here that we sum the partial
measurement over some subset k and the k
is obtained in such a way that we have a
threshold here so if the partial
distance between two histogram is too
high the threshold we don't take this
measurement into account and count only
those which gives us some numbers below
the threshold so by this you can choose
only the views where the page was not
uploaded if you don't have if in all
your other views you don't have the
proper projection of your super pixel
bicker something is in front you cannot
reconstruct it
I want to tell that that is the case and
what you
yeah if you don't have a correct normal
for example that of course yeah yeah but
if there is an edge the edge should be
also in the reference image or a you
mean you mean if you have assume for
example and your high frequencies so
it's more stronger or something in this
case oh ok then just it would be
dishonorable
I mean I think it's just hard to
differentiate little potion from
yeah if they're just live in the same
but it is something else okay you don't
know whether it is all closing or your
view is just I mean the very strong zoom
and suddenly some strong edge appears
you just don't take it into account
because your similarity in a histogram
indicates you that there is either
occlusion or something it
inconsistent even though it is
consistent you just don't take it into
account and you take the other views
where there is no such problem
your father's here
yeah but there is an image where do I
have the super pixels you see that they
usually follow the strong boundaries in
the image they usually follow the
gradient so this is actually done a
priori before starting so you don't have
such super pixels having a strong yeah I
know what you want to say that if I
project for example this image too I
mean to the other image very you there
is quite strong zoom and some edges
which are not visible here because
you're far away and suddenly they
started being visible that you you say
that they will change your histogram of
course they will somehow change your
histogram the question is as you sweep
along the ray how the other projections
which are incorrect behaves because you
search for a local minimum maybe you
still will catch the correct minimum
even though it's higher then it would be
if the vet if the edge would not be
there so it's a question but you have a
smoothness term and you have the
regularization terms later on to somehow
cope with such kind of problems as I
will show later on so for now let's
continue and we can discuss it later so
as I said 480 pixel we have this pre
computed matrix and we have so many
color or we have so many columns in the
matrix as the raw depth candidate so if
we have three normals let's say and if
we take three best minima along the
depth so then we will have the matrix
with nine columns so the first number is
the depth verdes candidate cured the
second number is the index for the
normal where it happened and the last
number is just ahead of the minimum
so since we have this matrix so let's
now explain the labelling problem so as
I said the beginning this should be in
the most general case this set P should
should be all normals and oral all
depths for all super pixels but we
restricted it to only two to optimize
everything over the subset of normals
and over the subset of depth and
therefore this will be the S dimensional
vector L so the labeling problem is to
assign a label the 82 h super pixel just
to pick one column from the matrix ts
for each through pixel so let me explain
it on a toy example so i have three
simple silver pixels here and i have pre
computed three matrices t1 t2 and t3
which have let's say now nine columns so
we construct a graph from the 0 pixels
and the graph will have so many objects
as there are super pixels so we have
three objects here each object represent
this particular super pixel and eat
super pixel can have L possible labels
and this label is actually the column in
this matrix TS so is the index of a
possible depth candidate we need to set
all the numbers here and all the weights
on on these edges the connectivity
between the objects is set accordingly
to the neighbouring relation coming from
the input image so if 20 pixels share at
least one pixel then they are connected
in the graph and all connections between
all labels are created and we have two
types of functions so these first two
functions are responsible for a number
earth in the circles here and they say
how probable is that or how costly it is
to assign a label LS to a particular
super pixel s then we have other two
types of energies and they operate
between two super pixels so they set all
the numbers connecting or all the labels
in two super pixels in the super pixel
as s prime means the superb ecologies in
the neighborhood and these are the pair
particular labels the aim is to minimize
this sum of energy functions in such a
way that it will create a path in your
graph just taking or picking only one
label per object and the minimum should
be the minimum along along the edges
connecting the chosen labels for example
here it is shown in a red color and this
corresponds to a label a 22 to the
vector 3 to fear L &amp;amp; 2 so the l has the
dimension of of number of super pixels
of in our case we have usually something
like 400 super pixels so you want to
estimate this vector L this is np-hard
problem so it's not possible to solve it
globally but they are available the
recent solders particularly for this
kind of problem one is by coma growth
and one by learner where you can feed
all your data and get a reasonable
result so the solvers do not guarantee a
global optimum the guarantee that it is
usually the optimum which is found is if
you're very close or it is already the
global optimum so let's now explain all
the energy terms we use here first is a
photometric term and it simply measures
how well the particular or how well are
the super pixel projection across the
other views so we directly take just a
third number from from other stores
matrices and it is directly all set as
it is advocated here so Robert the
minimum is more likely it is that the
super pixel will get the label L as
where it happened the second we have a
geometric term the super pixels have
nice property that their boundaries
usually are consistent with dominant
directions as it is shown here they
usually follow the image gradient which
can be utilized and can be used here so
let s you we already have detected a
vanishing point all vanishing points in
our input image we can compute a
probability of each pixel to be aligned
with a party for a vanishing point this
is for a first vanishing point second
which is number on the right and this is
for vanishing point which is above so
imagine we have a super pixel like this
and the super pixel is a line or the
boundary of the super pixel is aligned
in such a way that it is consistent with
two vanishing points this is the case
which usually happens and there are a
lot of such super pixels here so we have
to find out that the boundary is already
aligned with two super pixels it's easy
because we just follow the boundary the
super pixel boundary and while taking a
look at other probability images and you
can compute
this probability of the super pixel
being aligned with two two vanishing
points then we can say okay if we assume
we have three orthogonal directions in
our scene this is very likely that such
super pixel will be perpendicular to the
second normal which we already known and
then the probability of the super pixel
will be or being aligned with the second
normal be will be very high which
actually are motivated our our geometric
term so for this particular super pixel
the term will be 0 because we want to
force the solver to prefer the label
corresponding to the second normal so
this is possible only because we operate
on on silver pixels because they usually
as I mentioned are aligned and in such a
way that the boundary respect the
directions two vanishing points the
other two terms are responsible for a
smoothness of for regularization so if
we have two super pixels in the image
and they are in a neighborhood we won't
force them to have a same normal so then
we say to the solver okay if you decide
to put to both of them same normal you
will not pay and I think it is for free
but if you decide to put here one normal
and here another normal you will pay for
that so then the last term called the
depth term is the term for forcing
neighboring super pixels in the image to
touch in 3d
somehow penalize more you know having
different normal perhaps if the content
is insane then if it isn't right so if
these are their burden abolish sugar the
weight passionately
yeah it is very easily to be to be
included yeah did but the question is
whether it is really true because
imagine you have a wall you have a wide
wall and you have a picture on the wall
upholster which is black so you will
have two different zebra pixel one is
totally black inside and one is totally
white so the difference will be dramatic
even though they belong to the same
plane so there's the question whether
such kind of thing will help you so but
here we don't use such kind of thing so
these smoothness terms are purely based
on geometric use so this is the last
term and let's say we have to use super
pixels we know we have our center of
projection and now we take all the
pixels which are which lie on the common
boundary and just project them so once
when we take the label belonging to the
super pixel as we can directly take a
look into the matrix we have pre
computed see what kind of normal it
corresponds to and what kind of depth it
corresponds to and reconstruct the
planner page in 3d we can do the same
for the second super pixel just by
taking another label assigned to do to
this super pixel and here our energy
term is based on the distance and it is
based on the medium distance of all
relative distances of the points along
the shared library between the silver
pixels of course this sometimes is
possible in in real scenarios that even
though the super pixels are a
neighborhood in your image they can be
or they do not touch in 3d so it's a
common problem it happens and therefore
we have a saturation measure
here so if the error is very large we
don't analyze it or we penalize it but
by constant penalty so I explain all the
terms so after setting the graph you can
run the labelling problem and you will
get the result like this for the
sequence which was shown in the
beginning so the first image corresponds
to to the assigned normals so here we
have only two normals because the third
playing the ground plane is not visible
here so all the pixels or super pixels
assigned to the same normal are depicted
in the same color the black color here
means that the solver could not decide
because we introduced one extra label
called don't know label if the solar
cannot decide or there is no information
if there are a lot of occlusions then
simply you cannot say anything about
this 3d properties of the of the super
pixel and you simply give this don't
know label so usually it's it's it's
more safe than randomly pick one of the
labels so therefore the sky here it was
marked as a bull as a don't know label
because it's not possible to to estimate
the three poles usually in a standard
density Rho techniques the sky is masked
out manually here this is the row result
from the solver this is a depth map so
all the pixels are all the super pixels
having same depth should be in the same
color or should have the same color so
it is somehow more or less consistent
but not as we would like so we propose
ok let's now to compute the histograms
from the estimated depth so we take all
the super pixels here
we take the depth which we estimated
constructed the histograms for each
normal independently so here we have to
normal so two histograms and you see
that there are two Strunk peaks in each
histogram meaning that there are two
dominant planes which is actually true
so you can take this into account use by
asian formulation and redesign your
photo consistency or your composite
photo consistency measure in such a way
that you somehow encode the prior about
the possible depth in the scene and you
can get much better results here so here
we manually also masked out the sky so
you see that all the pixels now the same
depth i have approximately the same
color so let me show you some results so
this is the result for for our first
sequence so we just do this for images
and computed our model so you see there
are all some some errors but this come
from either occlusions or because we
don't have enough views to to this side
and to to to estimate the 3d pause so
the second example is on the sequence
which is usually used this is the
sequence from mark Paul Feig's group it
contains 46 images and it's usually used
to present or to show the performance of
the been sterile algorithms this is the
result recently published the CV pro7 so
color here corresponds to the assigned
normal so all the pixels on the ground
planes should be green and all the
pixels on the perpendicular planes
should be either red or or blue so this
is our result which is
which is smoother and for our result be
used only six images where they used all
46 images so we can operate on much
lower set of data
so if you've all see now that all the
super pixels are really are they all
rely on the plane so let's move to the
next example 3 so the next example is is
an Oxford corridor data set and there is
no density real method which would show
the performance on the sequence because
there is no texture so the pixel based
method simply would fail here because
all the patches are in constant color
and even though the images are in
greyscale and in low quality but still
we can handle it and we can construct
nice 3d reconstruction with correctly
estimated angles and planarity in there
so we can walk through the corridor and
out of it
the last sequence is again the Oxford
data set it is bottom college so we
reconstructed the model of the building
just from from five images and this is
the result here what we want to show is
that we took into account only three
dominant or normals but there is there
is a roof which is not aligned with none
of the dominant normal so what the
solver over the method did it assigned
the roof to the closest normal so it can
be seen in the video so you see that
they perfectly lie on the plane all the
super pixels so and even the chimneys
are correctly reconstructed the roof is
not because we did not allow the normal
which is perpendicular to to the roof
okay so let me conclude the Borg so I
presented and I wanted to show how super
pixels are beneficial for 3d
reconstruction of manmade environments
they offer less computational complexity
than they offer more about photomatix
measures could be defined and I showed
how the piecewise planar surfaces can be
favorably handled using such entities
and building the whole metal band whole
strategy on such primitives thank you
how do you automatically choose a number
of normals that you need the number n in
all these experiments which shows just
three as usually there were three or two
goals directions your question is how to
choose it in general you can do it like
there are methods for computing the
vanishing points and as many vanishing
points you can compute it so many
normals you can take into account
because these dominant directions are
indicated by the lines you know and so
you can write them around the method and
you can get ten vanishing points which
are not orthogonal but you can say okay
let's now do the optimization over these
normals belonging to this vanishing
points so this is one possible direction
so actually the question is somehow
related to that so in this kind of
scenes that you are targeting there are
methods such as the method of criminy
see you know which you can use to get a
sparse reconstruction of the scene very
well and in particular the edges right
so shouldn't that somehow be used to
constrain if you're going as far as
let's say taking the orientation of the
edges of the super pixels into account
shouldn't you take this much more
reliable you know structure which is the
you know the lines that belong to each
vanishing set as a prior constraint
tormented yeah but in creaminess is for
you mean the Borg where they operate on
single images or instance Ian for
instance of course I mean we don't claim
that the energy terms we define are the
best but you can of course include more
and more information in there without
the changing of the whole framework so
you still can formulate it is labeling
problem on the dead candidates and
increasing the quality of the energy
functions the better the depth
candidates will be and then the final
reconstruction can be more reliable so
of course you can put such things
true true but you know what I mean so I
think there are regardless of whether
you use super pixels or not there are
sort of two alternatives right to two
major sort of algorithmic ways of
dealing with this one is you know I look
at all possible within all possible
positions and orientations and put some
constraints there and try to figure out
this global global Optima so yeah but
nobody can solve right this problem yeah
and so in the end you cannot solve and
you have to apply some heuristics anyway
to the global problem right yeah the
other way of doing this is sort of as a
surface evolution kind of you know
framework where you start with the
bounding box of your scene you know if
you have the method from if you have the
results of criminals method you even
have like the the orientations you know
already that you have to look right so
you can put the bounding box according
to those orientations and then you
evolve that and you try to see if you
can eliminate points from the surface
right and you proceed you know pulling
the surfaces downward because you know
actually the global problem it's
provably not solvable right there is the
photo hope papers which show that you
know there are no such thing as a unique
solution to this problem right so why
treat it as a global problem rather than
you know what is seems more well you
know more rigorous approach which is the
surface evolution right which is you
know I will take this super set of the
scene and I and then I will carve out
all the things that I can carve out
somehow and end up at the maximum thing
which I can say maybe there are you know
ambiguities and I cannot resolve them
but I will give you the maximum you know
photo of the scene right you would be
actually nice to feed the same data in
such kind of approach and then maybe see
which is better and I mean the best
would be of course or somehow try to
combine the things this would be very
challenging directions
so I guess the criminal methods are have
the techniques have been developed in
the context of single view modeling and
you know one can get a lot of mileage
out of that but when one wants to use
multiple views you have to kind of work
out how to fuse things and how to figure
out which things are the same despite of
all the sort of photometric issues which
you encounter if you have wide base line
and the other issue with respect to the
visual how is that this method is
actually computationally fairly
effective so given the fact that you are
basically running the marathon the graph
of the order of few hundred nodes so I
don't want to sort of advocate the
implementation in hardware but this is
something which can actually run in real
time so there is some appeal to it from
right
yeah any combination of these things
where I'm saying is I still it
strengthened of this more on the super
fixers there on the on the MRF so why
not try I mean it might be an
interesting idea to try super pixels and
surface evolution right it I mean just
intuitively it seems to me that's the
first thing I would try yeah it's yeah
it's possible improvement or direction
to go yeah in your error term do you
have any understanding on how much each
of those individual terms actually
contributes and the reason I'm asking is
because you chose the super pixels and
somewhat related to what he was saying
exactly so that they have no texture and
you can't really calculate tomography to
them actually for example here it's not
true because they they're all the super
big souls contain a texture inside and
this check texture is taken into account
in the first two energy functions in
this photo consistency and only in the
photo consistency yes so the early
example where you have to facade you
actually put the super pixels right at
all the edges of where texture had
changes because you said you did the
gradient you mean yeah exactly this one
so wouldn't you have ruled out all the
relevant parts now to give you
contribution for that first error term
there is one thing because for some
super pixels like a super pixels here in
the sky you cannot simply say anything
because as you follow the boundary you
cannot say to which vanishing point it
belongs then in such a case for example
then this term will be set somewhere in
the middle like you don't penalize this
label because you don't know and you are
actually asking here it is sorry so you
want to know what are actually these
weights yes and despair
the first one yeah the first one
actually this first term is the most
important term because it captures the
texture which is inside the super pixels
and the disturb is responsible for the
for the measuring of the similarity
across the views so the rest is actually
these two are for regularization just to
have I mean the piecewise smooth surface
or piecewise planner and this is
actually responsible just for helping
the solver but I cannot say I mean you
want to hear which terms are the most
important or how important which term is
well I'm trying to understand why the
first one can remain the most important
if you've chosen your super pixel such
that they don't have any changes in
themselves that they're basically very
consistent because they still have the
boundary even though inside is nothing
imagine you have a white wall and then
you have some very super pixel which is
in the middle and the boundary is more
or less random but the super pixel let's
say touch is the boundary of the wall so
at least there is a part of the boundary
which is somehow kind of right angle and
the rest is random so this helps you a
lot because when you sweep you fit the
whole shape and in the other views you
simply go I mean along the epic bro line
and usually get an optimum exactly the
place where this perpendicular edge is
in the other views so it is indirectly
also encoding the shape of the super
pixel not only what is it in the middle
can I ask a related question so here you
you compare your approach against a
purely pixel based approach right and
there is this intermediate creation
which is now you super pixels for two
things one is to group pixels into
regions and two to compute features on
the region's right
but in between you can group pixels into
regions but then for each region for
each pixel in the region you can just
compute pixel based measures right and
some all those measures is the region
score so then you still have the old
features you have super pixels and
that's in between and that gives you an
idea of how much they actually gained by
this extra measure but one thing is you
said that you can compute your
similarity measure on pixels on
individual pixel build into the super
we're going to yeah but this is the key
point because what kind of measure you
would use you would compare I mean
normalized cross correlation yeah but
for that you need a window so you need
to go around outside so it will hurt it
means suppose to address all the issues
that your dress with the super pixel but
it's a nice comparison to show how much
all this extra respective boundaries
during your future design helps right
that's all yeah but vs. grouping which
is the other kind of orthogonal axis of
what you do so I just try to break it
into pieces see how much yeah but that's
all yeah okay the only drawback I see is
this computation of your photo
consistency error just for a single
pixel and this cross correlation because
you need a window you need to increase
the window to get much better result
more robust to illumination changes but
as far as you increase the window you
may get any problems and so on so so so
what happens when a large fractions of
the super pixels of boundaries that are
not aligned with the vanishing points of
the building or the scene whatever you
mean for example like the super pixels
like this one you know I mean because
here the planner assumption is also
violated because the gradient here was
very weak oh no actually I mean you can
have walls of buildings that are very
planner but any sensible segmentation
algorithm should give you arches that
have not appeared work right because
there might be stone walls for example
so what happens in this case what's
happening which case it mean
so what happens when the majority of
super pixels do not have boundaries that
are aligned with the vanishing points it
does not matter because the measure this
geometric term which I explained
measuring the boundaries of the super
pixels it helps you only in the cases
where it is possible where you can
evolve the boundary and you simply find
out that ok there is no Direction
consistent with any vanishing point so
you cannot say anything and then you put
this term as kind of constant number or
there is some number saying that i don't
know i would rather not decrease the
cost where you can do it where the super
pixels are aligned then the term would
go to zero so it would go to zero only
for those super pixels where you can say
something about it
you guys
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>