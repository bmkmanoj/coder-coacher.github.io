<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>To Harness The Long Tail Online, Location Does Matter As Does Time | Coder Coacher - Coaching Coders</title><meta content="To Harness The Long Tail Online, Location Does Matter As Does Time - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>To Harness The Long Tail Online, Location Does Matter As Does Time</b></h2><h5 class="post__date">2011-04-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UUFrOosE7t0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">there's been a tremendous amount of
growth than the mountain range of
information available on the Internet
users request for online information can
be captured by a long tail model I'd
like to introduce dr. chetan Kumar and
dr. John Norris who gonna present some
of the research that investigate causes
for some of the reasons some of the
results can be used to design better
online ad placing strategies affiliate
advertising models and better internet
caching algorithms dr. chetan Kumar is
associate professor of information
systems and operation management at
california state university san marcos
and before this he he received his PhD
from roger university so that it's a
it's my pleasure to present dr. chetan
Kumar all right all right thank you all
for being here thank you especially to a
nickel and Oman for organizing this tech
talk my co-author john norris is in the
audience so if you have any really
difficult questions please feel free to
direct it to him well actually i'm
kidding i'll be happy to answer all
questions and he can join us the deal
was that he comes along for this trip
and I do the talking so all right so the
topic of our presentation is what does
the long tail mean for online services
providers and what are some of the
factors that may affect the way in which
we can design online ad online ad
pricing mechanisms affiliate marketing
programs into online marketing
strategies and also separately internet
caching this is the agenda for our talk
today now in order to make this less of
an academic discussion we are not going
to spend time separately talking about
the related literature we try to weave
that into our discussion as we go
forward now we are reasonably confident
of the model and data analysis of our
study
what I think would be interesting for
this audience would be the application
of this model to different settings so
therefore we will try and do that in our
setting now perhaps as secluded
academics we may be completely barking
up the wrong application tree right so
it'll be wonderful to get your feedback
as far as the talk is concerned I'll be
happy to answer any questions that you
may have at the end if you have any
clarification questions as we go forward
during the talk please feel free to ask
me else please reserve all the
discussion questions to the end and i'll
be happy to go back to relevant sections
as well for any questions that you may
have okay so let's start by first having
a brief introduction of the long tail
model now given this audience at Google
I'm sure many of you are already aware
of that so I'll keep this reasonably
short the long tail captures behavior
online in certain situations where the
long-tailed characterizes the behavior
that there are a few very frequently
requested objects which is shown by the
steep end of the long tail and a large
number of in frequently requested
objects that is shown by the tapering
long tail and what is interesting is
that often fifty percent or more of the
total request may actually be taken up
by these in frequently requested objects
and that forms the long tail so a
practical example of that would be
Amazon versus blockbuster physical
stores or Netflix versus Blockbuster
stores or Amazon versus barnes and noble
physical stores Amazon and Netflix can
operate on the long tail because they
can have a large number of in frequently
requested objects but the blockbuster
and barnes &amp;amp; noble's cannot in their
physical store environment and as you
may have noticed recently both
blockbuster as well at barnes and nobles
are either filing for bankruptcy
or are in the process of getting close
to that so that to us tells us a little
bit about the power of the long tail now
the concept of long tail was popularized
by Chris Andersen who built upon an
earlier study by brittany olsen and the
past studies have demonstrated long tail
behavior for online product sales such
as books CDs movies being sold at Amazon
or Netflix for movie viewing for our
study we wanted to step back at a higher
level of abstraction and demonstrate if
this long tailed behavior exists for
online objects requested basically
websites requested so that was the first
aspect beyond that going beyond earlier
studies we were interested to observe if
different factors such as the user's
geographical location or the time of
access affects the shape of the long
tail in terms of the degree of steepness
that that particular long tail has
besides that for me personally what is
also interesting is whether this long
tail model may be used in order to
predict the expected number of requests
online at a very granular level so it is
it can it be some kind of a predictive
model now if all these things are true
then the effect of this kind of long
tail behavior will have implications for
online ad pricing affiliate marketing
models online marketing strategies and
perhaps even internet caching as we see
as we go forward alright so like I said
instead of discussing about 120 years of
research and hammering that at you at
one setting will try to weave that
through the presentation as we go
forward
so let's start with discussing some of
the motivation of our studies now the
two factors that we were interested in
in terms of long-tailed behavior who are
one user location so the question we
have for users located at different time
zones in the u.s. let's say in the four
different time zones are there any
differences in browsing behavior which
affect the long tail that's the first
factor the second factor is time of
access thus the users browsing behavior
differ depending on different times at
which they may access information online
for example weekdays versus weekends so
these were the two factors and of course
whenever you're doing a study you should
you guys would should probably always
ask that so what what do you care now if
these two factors indeed affect longtail
behavior then if you consider different
online ad pricing mechanisms affiliate
marketing programs other online
marketing strategies it doesn't matter
which is a company that is providing
that if we can improve the way in which
these models perform based on
sensitivities to user location and time
of access differences that would be of
interest to online services providers so
that is one set of areas that we
interested at the other area where these
results may also be applicable would be
internet caching which some of you may
be aware is essentially ways in which to
speed up the internet by reducing user
delays once again implications for
differences that may be leveraged based
on user location and time of access may
also be considered for that now given
today's talk topic we won't have
probably have time to talk about the
internet caching aspect we will focus
our attention on the online ad pricing
and affiliate marketing applications
alright so with that we are ready to
discuss our model now essentially for
our model what we are positing is that
the frequency of rick fehst for
different objects online may be
inversely proportional to the rank of
that particular object that is shown by
the equation as our and a characterizing
exponent s which determines the shape of
the tail while Sigma is a normalizing
constant now an initial version of this
was developed a long time back by a
philologist or a language researcher
called George Zipp who first identified
what eventually came to be known as as
if distribution where for the corpus of
English language he anta fide that the
frequency of requests of words was
inversely related to the rank of that
word for let's say the thousand most
frequently requested requested words so
this was a while back so would anyone
like to guess based on the Zipf
distribution what is the most requested
word in English the any other guesses
actually that there is a trick question
that is correct absolutely the correct
answer so the is the most respected most
requested object in the English language
and it is about seven percent of the
time second most requested up english
word anyone not on there there I'm now
I'm not there good try okay of is the
second most requested word so that is
about 3.5 percent of the time all right
now if you were to map out the zip
distribution with f of r on the y axis
and the rank of different objects on the
x axis you see that the shape of the
curve is determined by the
characterizing exponent s
and essentially as the value of s
increases you see the steepness of the
curve increases which means that the
requests for objects are more
homogeneous that means the most
requested objects take up a greater
proportion of the total request while as
s decreases the curve becomes less steep
of flattered which means that there is
an increasing long tail that is the in
frequently requested objects have a
greater proportion of the total requests
so with that we are ready to launch into
our model now when we are looking at
past studies there were a number of past
studies that talk about the Zipf
distribution some paw studies talk about
the long theorem it took us a while to
get to see the connection between the
two of them so personally in a rare aha
moment for me which for me is very far
and few few and far in between I figured
out that if you take a log
transformation of the zip distribution
and you get the expression on top that
map's directly onto a log linear
regression of f of r vs. rank are as you
can see and as a matter of fact the
characterizing exponent s directly maps
to the absolute value beta one of the
log linear regression so when you know
when I figured this out I was pretty
happy about it for about a week or so
and all right it's not a bad
contribution but on further analysis and
further related literature discussion
review we figured out that these log
linear regressions were actually used by
economists for to study income and
wealth distribution Oh a Pareto used it
for the first time in 1896 right so
essentially we were scooped by pretto by
about a hundred 120 years but what's the
century between friends so the point
here is that if we take the connection
between zip and log linear regressions
can be shown by these log transformation
and recently these log linear
regressions who are also used by
brynjolfsson to show the distribution of
online product sales and also by which
is what Chris Anderson built upon to
come up with the term and popularize
popularized the longtail expression all
right so for our data analysis we
procured data from the IR cash Network
which has a collection of ten proxy
servers in nine different us locations
and for focusing in on the location
differences we focused on servers in for
us time zones which include the Silicon
Valley which is right here in your
backyard the boulder which is in
Colorado urbana-champaign in Illinois
and New York in New York so servers in
four different time zones and as a
matter of fact for website request for
the Silicon Valley server chances are
that we may have some of the website
request that some of you in the audience
may have had in the past that we may use
for our data analysis but don't panic
it's completely anonymous so we have no
idea what you search for but we may have
your data and so in order to see
location differences we focused on these
four locations and for the second factor
for time of access differences we took
the data in each of those four server
locations and partitioned it between
weekday and weekend and that helped set
up our model as we go forward so with
that we are ready to launch into our
regressions so we ran regressions for
frequency of website requests on to the
rank of sites at both the aggregate
level that is the pool data from all the
servers which we call all servers
surprise surprise and we also looked at
we broke it down to four individual
server locations add those four
locations that I mentioned and once we
ran the regression we observed that we
see a very high r-squared value between
for both the
all servers pool data as well as for
each of those four individual locations
so this shows that there is a good
linear fit between frequency of requests
and rank of sight for all but for both
aggregate as well as those for
individual locations so if you were to
map this onto a curve where you plot
hits per site on the y-axis and rank of
sites on the x-axis we get a good linear
fit as you see by the straight line
sloping to the left and which again
shows that there is indeed a you would
fit between hits per site and rank of
sight now to you guys this might just
look like you know just another graph
but to me this is a very beautiful
object because once we got this that
basically told us that the two years
that we spent working on the study was
not really wasted so if i remember right
John Norris was in Chicago at that time
and he used to keep email emailing us
these charts and those are wonderful
gifts that you had for us John thank you
all right so with that sorry well so SAS
I believe had natural log of the base
right you know different applications
use different bases but correct so I
believe it's natural log for SAS right
but the results are the same in
respective off to the base 10 sure right
all right so essentially once we see
this this confirms to us that there is
indeed a good linear fit between hits
per site and rank of site which leads us
to confirm our first hypothesis that
there exists a long tail of a large
number of in frequently requested
websites in addition to a few very
popular ones so note that going back to
our earlier discussion about how we this
maps to this zip distribution we also
can confirm from this result that the
frequency of requests the expected
number of requests is indeed inverse
be proportional to the rank of that
particular object for different online
settings so that was our first first
result now we are ready to observe if
there are any differences in the long
tail behavior which is essentially the
steepness of the curve are those for
location for that we need to observe if
there is a significant difference
between the beta 1 coefficients which if
you remember is the steepness of the
curve at those four locations in order
to do that we actually employed a little
creek because if you notice we have four
locations which means that there are
four see two possible combinations of
locations that is six possible location
pairs so what we did is that we
calculated the difference in beta 1
coefficients for each of those six pairs
and going back to our earlier discussion
remember that the absolute value of beta
is nothing but the CAC the
characterizing exponent s of the Zipf
distribution which of course shows you
the sequence of the curve we employed a
paired sample t-test statistic and
confirm that the difference in
coefficient is indeed significant for
each of the six pairs of location so
that shows that the steepness of the
curve at each of those six location
pairs is indeed different which leads us
to confirm our second hypothesis that is
that the users geographical location
indeed has an effect on the diversity of
website requests at each of those four
location and therefore tells us that the
long-tailed behavior is indeed different
in terms of steepness now on getting
this result two questions come to mind
at least to us one is that why does this
happen and two is that can we figure out
some subtle differences between these
four locations in terms of diversity of
website
s in order to group or characterize
characterize these locations based on
that kind of subtle measure now for the
first one to answer the first question
what we did is we pulled out demographic
data for those four locations from
census gov website which by the way i
would highly recommend if if any time
you're looking for demographic data and
we focused on the counties in which each
of those four servers are located so for
example the four counties would be Santa
Clara Boulder champagne and New York and
one demographic factor that we
considered was the percentage of
immigrants each of those four counties
so if you look at that Sifl Silicon
Valley has 34% foreign-born persons New
York has almost thirty percent which is
significantly different from that of
boulder which is about slightly less
than ten percent and urbana-champaign
which is eight percent correspondingly
the language spoke other than English
that is spoken at home that is foreign
languages at home for those four
location are correspondingly of course
higher for saps with Silicon Valley
that's forty five percent for New York
it is almost forty two percent while
boulder and urbana-champaign a lesser at
almost fourteen percent and twelve
percent respectively so that was one
factor that we identified beyond that we
are thinking okay what could be some of
the other factors so for that we pulled
out the per capita income at those four
location now again you see some
differences for example silicon valley's
33,000 almost New York is almost 43,000
boulder and urbana-champaign are lesser
at almost twenty-nine thousand and about
20,000 for vonna champagne however the
differences here are not that
significant so you know we didn't want
to push our luck and say that this might
also be causing it however the point
that we took away from this is because
of differences in demographic profiles
such as the percentage of immigrants
that interns affects the diversity of
website
press at these four location which in
turn affects the shape of the long tail
so that is one factor and then to answer
our second question that we had asked
that is can we think of some more subtle
differences between these locations
between just saying that okay they are
different in order to do that we mapped
the beta 1 coefficients which once again
shows all the shape of the long tail and
we observe that for the New York and
Silicon Valley beta 1 coefficients if
you map that you see that they are much
closer in terms of diversity diversity
of requests compared to boulder and
urbana-champaign which are on the
left-hand side are closer together to
together to one mother which indeed
tells us that perhaps there is indeed
some type of differences between the
coastal locations in the US that is New
York and Silicon Valley what which are
closer together in terms of diversity of
requests and the inland locations which
are closer together in terms of
diversity of requests now we would you
know we'd be hasten to add that this is
just a graphical measure of this all
right we do not know of any subtle
statistical measure to give you this
kind of grouping as a matter of fact I
personally think that there isn't real
no real room for subtle tea in
statistics right it's either accept or
reject black or white so that would be
something to look for in the future that
is to come up with some kind of measure
to group these differences in location
and that is something that we measure
will mention at our limitations at the
end so with that we are now ready to
consider the other factor that we looked
at for our study and that is are there
any time of access differences for that
we question
correct for the population demographics
correct we pulled out population
demographic data for each of those four
counties and just try to identify
interesting differences sighs we did not
see actually we didn't focus on that we
just looked at differences in in
demographic factors which actually would
be an interesting thing to look at sure
we just looked at diversity of the
population that said you're right we
didn't consider sizes so as a matter of
fact at the end we when we talk about
some of the future research areas we do
say that there could be other
demographic factors to consider could be
sized income could be the educational
profile of those four areas different
kind of people work at different places
sure that could be you could have a
larger set of immigrants coming into an
area that has more I think that's a
pretty good point and that could be
something to look for in the future we
talked about that in the limitations all
right so for our second factor that is
the time of access differences what we
did is that for each of those four
location we partition the data between
weekdays and weekends and once again we
rot we ran the log linear regressions
between frequency of requests on to the
rank of the side and when we do that we
again see that there's a very high
r-squared value for each location
between weekday and weekend which confer
which confirms to us that there is
indeed a long-tailed behavior even at
the difference of weekday and weekend at
each location so that was one result
beyond that in order to see the
differences between weekday and weekend
we once again used our earlier method of
the paired sample test statistic to see
the difference in beta 1 coefficients
and when we do that you'll notice that
for example for all those four location
the absolute value of the beta 1
coefficients is indeed lower than that
for the four weekend compared to that of
weekday which tells us that the shape of
the long tail is indeed less steep in
the weekends which means that the
weekend has more diverse requests at
each of those four locations compared to
weekdays and of course correspondingly
once again when we calculate the
difference in beta 1 coefficients and
using the statistic we confirm that
these differences are indeed significant
at each of those four locations between
weekday and weekend as a matter of fact
once again if you were to plot the beta
1 coefficients for weekly and weekend
for each of those four locations you
again see the difference visually that
we had seen in the previous case for
location differences and this confirms
to us our third hypothesis that the time
of axis of the user indeed has an effect
on the diversity of website requests at
every location between weekday and
weekend now why does this happen we feel
we believe because users have more free
time in the weekend that gives them more
time to access more diverse sources of
information online which helps in making
the steepness of the curve less steep in
the weekends and therefore there are
more diversified website requests than
it were in the weekday where you have
less free time so those were the results
of our study we are now ready to discuss
some of the applications and we'll be
happy to get your feedback now the first
area that we considered are how can we
affect the online ad pricing mechanisms
that won't be that we have right now if
the platform provider in this case
Google let's take for example if you
have prior information as to
the expected number of requests for that
particular object online at very
granular levels in terms of differences
in location and time of access can we
enhance the online ad pricing mechanism
in order to do that we consider the case
where consider two markets the one on
the left is the left less diversified
one right which is shown by the more
steep long tail and the one on the right
is a more diversified market which shows
a more tapering long tail let us take
the case of two objects m1 and m2 where
m1 is the more popular object and m2 the
less popular object now consider two
cases the first case is the price that
you set for the online the online ad
pricing that you set for m1 in the less
diversified on the left market on the
left the difference between p1 the
corresponding prices and p2 and the less
diversified market that is in the more
steep longtail behavior should be
greater than the corresponding prices
that you see in the more diversified
market that is p 1 dash and p 2 dash
right so in effect p 1 minus p 2 should
be greater than p 1 dash minus p 2 dash
that's the first case the second case is
for the same object that is being
advertised in these two different
markets for the more demanded object m1
the price that you see in the that you
see for that in the less diversified
market should be p1 the corresponding
price that you see for that in the more
diversified market should be p 2 p 1
dashed and it is no surprise that p 1 is
greater than p 1 dash because obviously
you will tend to have more requests for
p1 in the less diversified market or the
more concentrated market but
interestingly the corresponding price
that you expect to see for the less
demanded object m2 should be p2 in the
first case but the corresponding price
in the more diversified
market should be p 2 dashed because it
is a higher proportion of the total
number of requests in the lil in a more
diversified market that means that p 1
should be greater than p 1 dash and p2
should be less than p 2 dash so what we
are saying is that if you take into
account the diversity of the market you
will appropriately price the online ads
that will take this into account now we
do know that the current online ad
pricing mechanism is of course a bidding
process right so the prices that you see
right now depend on the competition for
a particular keyword or search term or
whatever and of course you also have a
quality score index based on the
reputation of the advertiser the quality
of the language landing page and the
click-through rate you you you
appropriately change that now that is
good that is a good that is good model
what we are saying is that if the
platform provider has prior information
about the expected number of requests
for that particular object depending on
the diversity of the market you can
perhaps enhance the current online ad
pricing mechanisms by setting
appropriate reserve prices based on your
knowledge of the expected number of
requests so that is the first
application so essentially what we're
trying to do is that we're trying to
create an omniscient AdWords Oracle
right and of course you can substitute
AdWords with Adsense or facebook ads or
whatever really doesn't matter now I
don't know if you guys like anagrams you
know where you rearrange the words to
come up with something really smart and
funny right so I try to do that for this
mouthful omniscient AdWords Oracle I
completely failed at it right good think
of anything funny so I basically called
it Hallie right now for those of you who
have some kind of science fiction
leaning you'll probably recognize how
from 2001 a Space Odyssey I just thought
Hallie sounded prettier
so Holly essentially knows exactly what
is the demand for this object at this
location at this time and therefore can
price the cost per click for that
accordingly in effect I presume this
would be analogous to the goal of search
as set by Larry Page rights of you guys
are the experts on that so can does
anyone remember the statement that you
have for the goal of search by larry
page is that on your website i looked it
up i even have it written down on a
sticky in case I get it wrong if i want
to quote that anyone okay so the perfect
search engine will understand exactly
what you mean and give back exactly what
you want right you're famous tagline so
good luck with that one okay so in a
sense this is analogous to that so our
goal is to be able to get this at very
granular levels so for example Hollywood
know the demand for surfboards in let's
say amphitheatre parkway in july
weekends okay for the term surfboard
keyboard key search term or any kind of
AD based on that now that is the goal if
we cannot get to that goal we keep
stepping back at higher level of
abstraction still we have more certainty
about the expected number of requests so
if we cannot figure out the request for
surfboards in amphitheatre parkway
perhaps we can consider mountain view if
not mountain view we step higher perhaps
you can consider Santa Clara if not
Santa Clara perhaps California again in
terms of the time if the goal is July
weekends and we can't get there perhaps
we consider week days if not weeks days
we step higher two months if not the
month then perhaps the summer season if
not the season perhaps the year so
depending on the accuracy of our
we either go as granular as we can or we
keep stepping back to higher levels of
abstraction did you have a question
sorry alright so essentially what we're
saying is that if you instead of
considering an aggregate demand curve
which you see here in with the black
line what we'd like to do is break it
down to greater levels of granularity in
terms of let's say either location or
time of axis or both it doesn't matter
we break it down in terms of the
diversity of requests at granular levels
for example that aggregate curve may be
broken down to the red line which shows
a more steeper long tail at some
location and a blue line which is a less
steep long tail at another location now
note that this model would be robust to
account for differences in ranking that
that object may have ins wherein at the
aggregate level if the rank was r0 and
the corresponding priceless p0 instead
at the granular level you may have
different rankings are one which
corresponds to p 1 and r 2 which
corresponds corresponds to p 2 and
therefore we set online prices at these
granular levels now it'll be interesting
when we are for our data set we are
talking about in the order of millions
for our for the data that we analyzed I
guess a question that one may ask at the
Google scale is that will this be at all
possible at the Google scale of perhaps
in the order of billions now I feel that
once the initial set of rankings are
done for those objects in terms of
popularity perhaps by considering some
kind of sorting and merging algorithms
to determine the rank of a new object
should possibly be once we have done
that to rearrange the ranks should
possibly be able to be done using some
kind of a sorting algorithm an order of
n log n
operations and to insert the new object
would be possibly in the order of n
operations which therefore shows that
you should be able to deal with this in
polynomial time in terms of complexity
so essentially what we are trying to say
is that why what is the benefit of this
two vendors and advertisers so this is
some of the preview of our models that
we are considering for our next study
where what we wanted to ask ourselves
was that what is the utility of the
vendor in this case it would be the
platform provider such as Google in
considering this kind of a granular
pricing mechanism and similarly what is
the utility of the advertiser now for
the vendors utility we wanted to keep
the objective function simple and we
said let it be revenue maximization so
if the platform provider wants to
maximize revenue if looking at the
expression on top if the the number of
the total revenues that they were
getting before they go at granular
pricing levels was the one the
expression on the right hand side and
the new revenues that they get once they
break it down in terms of granular
pricing due to location and time of
access differences let that be the one
on the left hand side for the top
expression as long as this utility is
positive the platform provider is better
off on considering these kind of
granular differences similarly from the
advertiser's perspective we said let let
the objective function be total adds
maximization so all the advertiser wants
is greater click-through graters add
views for the ad so once looking at the
expression on the bottom if the total
adds that the advertiser saw before
considering granular pricing was the
expression on the right-hand side as
long and the new set of add views that
you see is a
expression on the left for the bottom
equation as long as this utility
function is positive the advertiser is
better off in considering these kind of
granular pricing differences based on
location and time of access now you know
when we are considering this one obvious
player to consider beyond this would be
the utility of the consumer now that in
my opinion becomes a little bit of a
psychological question which actually
I'm not very good at a time flying that
is on the one perspective someone may
say that the utility of the consumer may
be better off or may be better if they
get more targeted advertising
advertising based on location or time of
access differences that's one
perspective the earth but the other
perspective could be perhaps the
consumer is worse off because now they
have even more inclusive ads compared to
before right so I don't know which way
that would go but that would also be an
interesting question to consider if one
wanted to look at the utility of the
consumer now since we couldn't get to
that answer one easy way to avoid hard
questions is to not consider them at
well at all so we can skip that for our
future study but that would also be an
interesting thing to look at so
essentially what we are saying is that
the price for any object is should be
proportional to the expected number of
requests which is shown by the
expression on top and two if we were to
substitute back for that expression into
the earlier expressions for utility of
the vendor and utility of the advertiser
and consider the case at which objects
are in this particular example being
sold at two different locations in terms
of granular pricing and you know
substituting back for that expression
and jumping through a bunch of
mathematical hoops eventually we come
down to the simplified expression as
this is the utility of the vendor and
this is a utility of the advertiser for
this particular for these two particular
functions now if you'll notice that
while we are estimating this
the for the utility of the advertiser
the click-through rate which is C we've
assumed that the click-through rate
would be constant to go back up before
the granular pricing mechanism which is
on the right hand side of the bottom
equation and it is constant after as
well when you consider granular pricing
mechanisms so C remains constant for our
model now one may argue that actually
the click-through rate should probably
increase when you have more targeted
advertising in terms of location and
time of access right so in a sense we
are under estimating the utility of the
advertiser in our model which actually
is a strength of our model which is
saying that actually it should be even
greater than what we see so going back
we come to our final expressions for
vendor and advertiser and essentially
using this we should be able to identify
the boundary conditions for what are the
values of r and s where the utility of
the advertiser and the vendor are both
positive for given targeting based on
location and time of access differences
now once again it'll be interesting for
us we we should probably be able to
drill down to this for our data set it
would probably it for us it'll be very
interesting to see how that would figure
out in terms of Google scale and when
you're talking about a much larger set
of objects but I believe we should be
able to identify the bra these kind of
boundary conditions for different
platforms all right so in conclusion in
our study we essentially were interested
to observe if user's location and time
of axis affects the long tail of website
requests we tested our model on
real-world proxy server data and we
confirmed three hypotheses the first
hypothesis that there is indeed a
long-tailed behavior in terms of
diversity of website request this is
true at the aggregate level as well as
for each of the individual for server
locations that we consider and therefore
that means that the
expected number of requests may indeed
be demonstrated by the rank of that
particular object which in my opinion
shows that this is an elegant model as
from ziff earlier because all we need to
know in order to determine the expected
number of requests is the rank of that
particular object and nothing else for a
given distribution of course the
character the characterizing exponent s
will change that but in my opinion that
shows a powerful model the second
hypothesis we conserve confirm is that
the user location indeed has an effect
on the diversity of website requests
which shows that the shape of the long
tail does change based on user location
this is true for all four locations that
we consider the third hypothesis be
concerned we confirmed is that the time
of axis also affects the heterogeneity
of website requests in terms of the
shape of the long tail again this is
true between weekday and weekend for
each of the four locations that we
confirm that we considered and to our to
our knowledge as is the first study to
consider look to demonstrate these
location and time of access differences
for long tail behavior and we believe
that this has implications for online ad
pricing for affiliate marketing or
online marketing strategy and also for
internet caching as the four areas that
we could identify so I think that's that
our limitations which is essentially our
future research areas that we are
considering one is that we used a paired
sample t-test statistic of course
another way we could consider is an
ANOVA for measuring these differences
however with an ANOVA one has to make an
additional assumption about equality of
variances across the samples so that is
a caveat second is for demographic
factors we considered the percentage of
foreign immigrants as you mentioned
during our talk there could be other
demographic factors that one can
consider perhaps it could be the
population size perhaps it could be
income
levels beyond what we looked at perhaps
it could be the education level perhaps
it could be the distribution of jobs
which in turn affect diversity of
requests so that would be a future area
have to look at then as we had mentioned
it may be interesting to develop a
subtle measure to help in characterizing
differences in location that helps us
group them together similarly it would
be interesting to consider a measure for
the extent of difference that there is
between weekday and weekend a measure
for that rather than just saying okay
weekday and weekend are different so
that could be interesting and finally it
would be interesting in my opinion to
also consider more focused geographical
areas so for example right now we looked
at the city level perhaps we could also
look at differences in long-term
behavior at the the zip code level
perhaps it could be the street level and
you know going down to fourth further
focused geographical areas beyond that
one can also consider smaller time
intervals we considered weekdays and
weekends perhaps you can look at vide
days perhaps you can consider min days
hours minutes and so forth however there
is a caveat that one when you're doing
this kind of more granular analysis one
does have to consider the diminishing
marginal returns that one may get at
looking at more and more granule
granular levels in terms of increasing
variability so by Chris pooling as some
of you may know it is you one is more
confident of predicting expected number
of requests at the aggregate level
rather than at the individual level so
that means to use a basketball analogy
one would be more confident saying that
LA Lakers would beat LA Clippers in a
seven-game series but it would be you'll
be less confident of predicting the
outcome of a single game but that would
be an interesting area to look at and
that is identify the identify the fine
line of where the golden
between increased granularity and
increase variation with more focus
observances and geographical areas and
time intervals right so I think with
that we are done with our presentation
if you have any further questions or if
you can consider any interesting mutual
areas of research that we can look at
that I'll be happy to take them on we
are out of time but any questions all
right think that's that
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>