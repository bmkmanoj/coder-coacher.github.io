<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fast, Deterministic, and Sparse Dimensionality Reduction | Coder Coacher - Coaching Coders</title><meta content="Fast, Deterministic, and Sparse Dimensionality Reduction - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fast, Deterministic, and Sparse Dimensionality Reduction</b></h2><h5 class="post__date">2018-01-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nHrLKCyyF30" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all can see that we you I before that
and before that it was a student of a
very numerous gift at Georgia Tech and
you will tell us about fast
deterministic and sparse dimensionality
reduction like Oh for the price of a
single album yeah I'm very happy to be
here and talk about this work
yeah so basically it's it's some work we
did in collaboration with Dan in the
douche handing over both parameter dam
and this is gonna be soon in and so yeah
so let me talk first about what do we
mean by dimensionality reduction but
more precisely in the setting of our
work so okay so the basic idea is quite
simple to to explain we have a set of
high dimensional vectors v1 through the
end in our deep so the way I'd like to
think about this is these are rather
large dimension you're working with but
the number of factors you have you seen
a large that's the kind of region we're
interested in so yeah we want to map
these to a lower dimensional space and
of course we don't want to do this in an
arbitrary way wanted to do it in such a
way that we can maintain instances
approximately right so if these are the
original distances is VI minus PT x
squared Euclidean on then one our output
to be within a 1 plus minus epsilon
approximation it's okay yeah you can see
in this very simple picture for instance
you have two vectors in the unit sphere
right so there's long way to find a
projection which maintains its distance
is the exact which is basically taking a
two-dimensional plane that contains both
the origin on these two points but the
point is that many hyper planes can
maintain these distances approximately
so you can see here a few examples and
indeed it's kind of slag we have these
those freedom to do this projection with
many more vectors at the same time
you'll love the mapping to be data
dependent
yes in our case rules so I guess you're
thinking about these oblivious
I mean our our study our setting is not
about that in particular but it's also
race so good weather questions always em
your fix are you're gonna pick em based
on unity as well oh yeah I mean what I
make ever is going to be dependent on N
and epsilon but not on T okay so a few
things which are highly desirable of
course if we can make this mapping to be
linear this is very helpful process it
allows us to simplify a slightly bit
kind of low distortion property by just
instead of looking at differences or
distances between points we can just
look at maintaining north of certain
papers what these vectors of course are
the differences and on the other hand I
mean we would like to find some kind of
prediction pi which is very easy to copy
so for instance if this matrix price
very sparse or in tightly structure
based in some kind of free transform
then that's highly convenient so just to
mention a few applications on how this
technique can be useful I mean there are
a number number of applications I'm not
going through all of them but I'm just
going to show you a couple so you get an
impression on how these things are used
so for instance for the case of binary
classification we suppose we have a data
set with labeled examples often even
negative here and there is typical this
is something in machine learning that
you can separate this is positive or
negative examples with a certain
hyperplane which not only separates them
but it does so with a certain slack so
there is some so margin gamma where you
guarantee that you don't have many
vectors and somehow this this guarantee
of having you know this margin is in a
certain way certificate that our
solution is in fact quite structure
quite simple what you can do is take any
of these sort of dimensionality
reduction techniques map these vectors
to a lower dimensional space and since
your mind approximately maintaining
distances
basically you can highly reduce the
dimension of the problem losing a little
bit of slack on this on this part just
across
so and extending the separator to the
high dimensional spaces trigger you just
extended by zebras so second morning one
example is the case of nearest neighbors
so here we have a large space because of
P 1 through the end in Rd and for a
certain vector VI for a certain
parameter K we want to know what are the
K closes elements to VI and we can all
start the same question about
approximately here same so the issue
here is that there's of course a naive
algorithm that will go through all of
these input vectors compute the
distances and keep the lowest ones
closest but that's expensive right
particularly if we're thinking that D
and n are rather large that's an issue
so there's this sort of classical idea
of somehow processing the system
instance coming up with a nice sort of
data structure that would allow this to
make queries in much faster time this is
that this processing typically will run
into an exponential in the dimension
complexity that's that's what's known as
the curse of dimensionality and the way
that this been fixed and this is the
practical work of India move on is that
basically you can first project these
vectors using some low distortion
embedding
you maintain distance approximately and
then you can do the processing
efficiently because basically what you
are not paying in terms of the
projection by mention is lowering in the
number of points we will see that in so
ok now let's go into the more technical
aspects of this talk I'm gonna first
talk about the classical Johnson leanest
ROFL emma and and then we'll go and
improve these things bit by bit so
including this fast deterministic and
sparse one-by-one so yeah so the
classical result known as the just elena
starts lamar basically says the
following so the high-level idea is that
we want to come up with a construction
of a matrix pi which is random and our
guarantee is going to be a certain
probability of having high distortion
with respect to a fixed but arbitrary
vector
so it's it's facing a slightly different
way to what I was saying in the
beginning right but you can you can
imagine um it's easy to turn this into a
guarantee for multiple factors so the
point I want to emphasize here and
something that I said before the
prediction I mentioned M skills with
epsilon has 1 over epsilon squared and
if we have a certain confidence
parameter Delta will want to control
we'll be paying a log of 1 over Delta in
the this is a statement to you okay so
how do we handle multiple vectors well
it's just a matter of the certificates
are in random matrix your means for
random walk away well I for now just say
random in a video will explain which
specific instructions we care about so
if you want to handle multiple vectors
in the setting I described in the
beginning you can just take a Union
bound on this so if you if you care
about maintaining north of vector P 1
through the end and you want to have a
constant confidence you can make this
prediction as scalar as log n over and
what kind of distributions we care about
all so first of all the classical result
only the sauce shows that if you pick
this projection as a prediction or not
when a random subspace
this will work with high probability of
course this is not very efficient of
course because you need them to compute
this projection but according to the
subspace so the work of indica Ronnie
what they did is show that you can make
this much simpler you can just bake iid
and Gaussian coordinates and this would
work in the same with the same kind of
probability guarantee you can make this
even more efficient and that's the
proposal of avocadoes which is basically
picking iid coordinates Rademacher so
plus plus 1 1 minus 1 agree perfectly
and then you need to in both of these
cases for the Gaussian on the right
amount you need to somehow rescale
property so basically you want to make
these variants of the same scale as 1
over m because the the height of your
matrix is n and all these analysis of
these high probability Bears are based
on this idea of concentration of measure
which I'll go into now
so the natural notion of concentrated
random variables will look here it's the
notion of sub Gaussian random variable
so the definition of a random variable
being some calcium with a certain
something else a parameter nu is that
basically the moment generating function
of X it's upper bounded by the moment
generating function of a Gaussian with
variance equal to nu squared so some
examples of course a Gaussian is a
Gaussian with the sub calcium parameter
being the the variance but also the
Rademacher sardar one sub calcium so the
picture looks a little bit like this
things which are highly concentrated so
if you look in black here this
distribution also has no variable
Samuelson is is better concentrated
around the origin and something which is
not something our sample to take it
happiest f king so let me ask at this
point are you all familiar with the
proof of Chernov inequality using sub
Gaussian random variables no should I
take a moment and do that it's going to
take just one minute and I can do it so
basically under that assumption is a
random variable so once you control the
probability of X being larger than a
certain specialty I can just useful the
monotonicity of the exponential here
right and even further I can write this
quotient different so when we can see
here's we're basically integrating
something which takes value 1 when this
inequality is satisfied and it's zero
otherwise
so just using Markos inequality I can
upper bound his body and now in position
to use the sub housing property
dominating this thing by the moment
generating function of the cows and then
I mean it's just a matter like all I
need here is that the lamp is positive
so then I can optimize and this is a on
top here we have a quadratic sorry this
should be lambda squared U squared so
this is quadratic in lambda we can
optimize say that okay concentration of
e to the minus it's T over 2 nu squared
so that's why we care about this
akio-san random variables and I why'd
they do those concentration any
questions at this point
okay so unfortunately what I want to
talk about now is something which is
more involved which is basically
concentration for quadratic forms so
this is the so-called handsome right
inequality so the setting here is quite
similar in principle we have a vector of
independent use of Gaussian random
variables and we have the symmetric
matrix which has zero coefficients in
the diagonal okay there's something of
zero coefficients that diagonal is not
important for the result to be true but
it's gonna help us a lot and it's always
satisfied in the setting of the straw so
basically what we have under these
assumptions is that this quadratic form
x transpose ax it's going to concentrate
at this rate and this rig is an
expression that looks a slightly
complicated it depends on how on the
operator normal fee on the Frobenius
norm of a and it has these two kind of
behaviors so let me let me make a plot a
little bit so you can get a handle on
how this looks like this term in blue
here
ethos of gaussian kind of tail it looks
like this over here and the other one is
just on and kind of an exponential decay
in T this kind of here and basically
this minimum is speaking the worst of
both in each beat the still an
exponential concentration is just not as
a promise of Gaussian and from now on
I'm not going to here mad about not
gonna care much about this new squared
term because we mostly clear about the
case of the runner markers which are one
sub calcium so you can completely forget
about the new so a person can just come
back again there's a some matrix that
that it's a symmetric matrix which has
so think about the case of the writer
markers
well these diagonal terms are just
multiplied by this Sigma squared which
are always one that's a deterministic
term okay so now I'm going to show you
briefly how you can prove Justin Lin and
Straus based on this concentration of
quadratic force this is a proof that you
to you
kanan outside so the setting of our
projection matrix is what I said before
we'll pick iid random anchor coordinates
and we'll scale them by 1 over square
root of the height of this matrix so a
number of rows now for notational
purposes I I mean this is the random
object I want to study it's a matrix and
they wanna write it as a quadratic form
so I need to write it in vector form and
to do that I'm basically just stacking
the rows of this matrix and that's what
I defined this Sigma is just a
vectorized version of fine well now let
us note that the kind of low distortion
property that JL matrix is satisfy it's
invariant under scaling right so I have
Norris everywhere not or it'll be not
part of the projection I can just divide
by the norm I only care about vectors in
the unit sphere so the property odl
basically only requires this and I'm
going to show that this this kind of
expression which I mean if you isolate
maybe I can do it on the board but if
you write down the kind of float
I'm property for a unit vector is
basically requiring that this almost 5 b
squared minus 1 is between minus epsilon
and epsilon is is that clear for
everybody so I'm just going to expand
this I'm going to show that this is
equivalent to a quadratic form in Sigma
so if if you look at what what these
five matrix is doing well basically it's
making this row vector inner product and
then you have this sum of the squares of
the inner products you have for each row
right so this is what we get
since this is a unit vector I can
replace this one by a sum of the squares
of the coefficients of V that's what I
did here here and then this is where I'm
just going to expand as a product of two
turns and I'm going to use different
innocence because I want to make all
these cross products so if you expand
that what you will see it's basically
that this is a product of our rank one
matrix in it it is in some quadratic
form on Sigma but we respect your rank
one matrix which is VV transpose and
then basically here we're removing the
diagonal terms so it's exactly in the
framework of this handsome writing
equality before I can make where are you
precisely this matrix is it's very
structured block diagonal each each blog
it's a rank one perturbation four
technologies so just using handsome
right inequality I can show that the
deviation I care about it's upper
bounded by this exponential decay you
think and in this case I can compute
exclusively what these two matrix not
sorry I'm I'm not gonna do it here but I
can tell you that this thing is scales
as 1 over m and the operator norm has
the same behavior so this is what you
get here then it's just a matter of
choosing the right end so you can
guarantee that this deviation
probability is less
okay so now we know we can use the
Hansen read inequality for proving
distributional GL what I'm gonna do now
is show you a new proof of the Hansen
right inequality so I haven't even done
the proof Hansen right
and why do I want to get a new proof of
pants on right over there's many reasons
the most important one if you have ever
seen it all these proofs at some point
use some kind of decoupling step which
allows us to somehow partition this
quadratic form in a nice way getting
leaner turns and you can condition on
those so somehow it's trying to reduce
this quadratic concentration in
something that looks more like a linear
concentration but that that introduces
some artificial random variables so
basically you're picking some Bernoulli
random variables on top of this
quadratic form and this introduce some
extra variance so there is no way that
this proof technique is gonna give you
something sharp in fact most most of the
proofs of Hansen right do not happen in
any explicit constants and they see that
I mentioned before in the concentration
the best we can find in the literature
is something that was like one number 64
yeah and this see the lower it is the
worst concentration so so it really
looks bad constant on the other hand we
want to get a deterministic counter part
of this algorithm and the randomizing
this decoupling is is extremely slow you
basically need to numerate all these
possible realization of this Bernoulli
random variables so what am I gonna do
now is show a simpler proof of the
handsome right inequality with a
constant which is much better so it's
something like three over twelve twenty
constant is optimal it's sharp in a
sense you will see the proof and I can
tell you with sense is sharp I'm it's
hard to say what's sharp here because
and let's go back here I mean this kind
of inequality is something that only
depends on the operator and for being is
fine I mean if you really want to get
something sharp you should use a
completely okay but in the sense of the
dependence of these two things it's
sharp
okay and the group is quite simple it's
based on this very simple lemma that
basically says the following so suppose
you have two vector random vectors
between the pemon and each one of them
is upper bounded in terms of the law in
generating function I want to this back
to the other so you should think of this
wise as being the rudder markers and the
Z's as being the gaussians right that's
that's what we have and then what we can
show is that for any symmetric matrix
with zero diagonal well the moment
generating function of the quadratic
form is dominated by the other and this
in principle is highly non-trivial but
I'm going to show you the proof it's
it's two lines so how we do this well
first of all you write down what this
expectation is so there's a bunch of
independent random variables and what we
can do is somehow conditioned on wife to
you through YK and look what's inside
here with respect to Y 100 so the
expression might look a bit nasty but
basically the only thing you need to
observe is that this is the eilat one
one term is zero everything here is just
a final white one so basically this is a
linear term in one one and this is a
constant so you can use the hypothesis
and replace this Y 1 by Z 1 now you can
take this expectation with respect to c1
outside get the whites you inside and do
this equation and that's all now from
here onwards I mean what we're gonna do
is basically follow the same strategy
for for regular Hanson right so so
really this is the heart of the okay
no I don't think I need that
yeah it's just a zero it's just a series
I know you'll I mean it doesn't even
matter what these signs are right
because you know that you have it for
every lambda so we just need to make the
quadratic term vanishes or anything
exactly that's all so I'm gonna show
a little bit of how this is done once
you have this lemma because it makes
sense but I'm not gonna go through full
details here so what's important from
this inside we can replace I mean if
we're trying to control this probability
of deviation with respect to the
biomarkers we can use again turn-ons
strategy right so we get this
exponential thing the moment generating
function with respect to this and then
by the lemma we can replace it by the
one with the Gaussian and now the key
thing is that the Gaussian is unitarily
invariant so I can look it in whatever
basis I want so why not looking at it
from the item basis right then bye bye
this unitary invariance I can just look
at this and change basis I get something
which is quadratic but then with a
diagonal matrix and so basically have
them a bunch of gaussian squares for the
respective locations I know this DRS I
mean these are the item bodies of okay
that's all supported too sir
so doing this I get something which is a
sum of independent random variables I
can turn it into a product they can
explicitly compute each of these for my
generating function which gives us this
and now you see why the operator unknown
becomes important right I mean these are
the eigenvalues of the matrix whether
whenever we reach anything beyond one
over T oh great or normal Fei we're
gonna run into this thing diverging and
we can also write this alternatively
since it's already in values as I
started determinate the kind of paper so
I should say here at this point once you
have this inequality I mean once you
replace this thing over here by this
determinant you get something which is
completely independent of the
distribution so it's just a matter of
optimizing this thing in fact if you
just use this this kind of OSHA's that
we have in this product over here and
you upper bound them by a proper
appropriate exponential function which
is what is typically gone for the
concentration of chi-squared random
variables you can just follow the
standard proof and you'll get hansom
driving quality with constant C equals 1
over 8 so it that's already much better
than is 164 so what we do here is
do a little bit of work in terms of how
we can optimize this thing with the
determinant you can get something which
is much finer and we call this a lower
bound but it's not very important I mean
you just do this and then optimize a
lambda and that gives you this
concentration it looks a bit more
complicated than the standard handsome
right but let me just tell you you can
dominate this thing by the standard form
of constant right and the constant you
get is this one I promise
so 1 minus cos 2 over 2 maybe dealing
with this exponent spiked say again the
terms is a tight basically now this one
is tight yes yes but then if you want to
turn it into a regular kind of handsome
right expression you can do so with this
so any other questions ok great
so now let's turn to the question of
sparsity of course replacing gaussians
by Rademacher this is significant
improvement but we're still end up with
a matrix which is stats and we would
like to specify it so we can get a
faster embedding now the bad news here
is that it was proved that for
independent entries this is
precipitation salty is only gonna buy
you a constant factor so I think I don't
know the exact constant but I think
maybe one third of the coefficients can
go away but not much more than that so
that's that's what I clear up this proof
and my to check out a lower bound
I'm fortunately there is a way out which
is trying dependent entries and that was
proposed by Kenyan Elsa so the idea is
that here we can think of this as
basically the same distribution we had
before with these iid entries but now we
select a random subset of size s and we
can do this in two ways
either we can pick a uniformly random
subset or we can partitioning blocks in
else blocks of size M divided by s and
in each of these blocks we just select
one coordinate uniformly at random so
for for certain cost parts of the
parameter which is s that I'm going
you specify in a couple slides all we do
is coming up with a way so you can think
of this sparse construction in aside
two-step process first you select a
subset of coordinates you want to
maintain so basically this is the
masking matrix and then only for those
coordinates you're picking this one plus
plus one minus one random realizations
so in fancy linear algebra notation this
looks a saga component wise or Herrmann
product of these two matrices and now
thus the scaling factor is slightly
different it used to be 1 over square
root of M but now since we're only
selecting as coordinates it's just 1
over square root of s so the analysis is
quite similar and not not going to go
through the details but the point is you
can even in this sparse case you can
pretty much do the same kind of arrange
in terms of turning this deviation as
quadratic random quadratic form in the
same vector Sigma and this kind of block
diagonal go there and of matrix we used
to have it's almost the same but now
each of these vectors be our esparza
fide by a different role of of the
masking so is all this setup clear until
this point ok so so the point is that
basically we can follow a similar
argument as was done in classical GL so
I'm gonna set the sparsity to be epsilon
times M so basically forever we have M
coordinates per column and we're
selecting an epsilon fraction of them
and that if if you assume that we had
high probability you have these two
upper bounds you can condition on that
and then just use the regular Hansen
right and since we pick this s to be
epsilon M is two terms over here are
balanced so the kind of concentration we
have is something like 1 over C to
certain power so this is vanishing us
with a number of input vectors
and all were left with is proving that
this event star happens with enough high
probability now this involves to kind of
balance for this matrix we define here
however the part regarding the operator
norm it's very easy so we not only with
high probability it's for sure that the
upper bound on the operator drops
bounded by 1 over s the part regarding
the masking is much more difficult or I
mean significantly more difficult
comparison and if you just write it down
you get this this kind of expression so
what we're looking at here is somehow a
sum on the different pairs of columns of
these expressions that are scaled by the
coordinates of V and and this this qjk
it's it's a it's a random variable but
it turns out that is in song of non
independent random variables ok now I'm
gonna explain right next way but this is
the case and what's basically the
meaning of this thing so what's
happening is that these qjk random
variable what is doing is looking at two
different columns J and K and here I'm
denoting this X by the selection of the
of the masking so suppose that the
sparsity here is indeed one so we're
just selecting one element per column
and theaters to run a random realization
of for these two count so what this is Q
take a random variable denotes is
basically is counting the number of
collisions within these children
accounts right so we're just basically
going through here and seeing what are
the consequences and in this realization
in fact we have no collisions so that Q
is zero but internal it could be it
could be something else of course is
always gonna be upper bounded by yes now
you can see why this thing is is now
independent because we're choosing a
random subset in each of these however
think that this thing kind of the
dependence goes in the right direction
we have somehow negative dependence in
this random girl in this sum of random
very sincere sparse it is one and I tell
you that I selected here the first
coordinate in column J well you
immediately know that everything else
has to be 0 so that's what I mean by
negative dependence whenever I know that
I pick a coordinator that immediately
makes everything else less probable and
when when you have this kind of negative
dependence you can basically go through
with the same argument let me just
replay is this dependent random
variables by by their independent
particles and then you can sort of use
the same kind of concentration argument
and I'm not gonna do it here because
even from here it gets a bit technical
but I think this is the heart of what's
going on okay so now let's go into the
randomization business so first of all
in certain settings it's of course
preferable to get something
deterministic us are supposed to random
and there's maybe some force add for
instance I mean the guarantees we get
with distribution LDL hold with high
probability but we might want to prefer
something that holds surely and the
other hand of course a random
deterministic algorithm will not be
sensitive to these random fluctuations
that we're picking for our projections
there is this this hope that this I'm
not sure and I don't have any theorem
for this but I would like to see whether
for instance is deterministic
constructions you a better job
exploiting the the structure of the
input as opposed to you know this
distribution idea which is completely
oblivious to the to the input and of
course also there is there is a
conceptual reasons to think what can be
achieved with randomized versus genetic
algorithms so let me just in a nutshell
explain what's our basic algorithmic
principle which is based on the method
of the randomization by conditional
expectations so suppose we have a
randomized algorithm which depends on
these independent random outcomes is
plan to escape
you may think of this as the dry the
markers for instance and are
deterministic I'm sorry our our
probabilistic guarantee looks like this
so some sort of expectation of a certain
function upper bounded by a content of
cells so you should think of as
basically being the event of having low
distortion been I mean we can read this
this big expectation by integrating with
respect to the first random variable and
just using a probabilistic method kind
of argument to say in this expect if the
same thing values less than or equal to
Delta their math success our relaxation
of x1 which gives me the same time
expectation will expect you this to the
right upper bounded by Delta and I can
just repeat the argument I I in the end
what I find is a deterministic
assignment of these X's of course this
is only gonna run efficiently if we can
compute these conditional expectations
efficiently which is typically not the
case if you think about this Bernoulli
random variables or rather markers you
have an exponential number of
realizations however there is no need to
do this if we can properly upper bound
these expectations and this leads us to
the idea of pessimistic estimators so
the idea here is that there's some
parameter space P and for each of these
parameters there is an Associated
distribution T of P and we'll say size a
pessimistic estimator in if it's for any
possible distribution DP the probability
of the event we care about is upper
bounded by this size function so the
very high-level idea would be that
somehow you use this some sort of
potential function right so we start
with the distribution that guarantees
this probabilistic state and then we use
so we somehow traverse the space of
parameter sex and using different kind
of piece that everyday so basically what
we're doing is fixing partially these
coordinates and this is your number of
distributions and we end up with
something which is fully deterministic
but whenever we do this we do it in such
a way that we can control this is
failure probably and you can imagine
that somehow
if the side for instance is concave this
will allow me to pick a property
randomization at every step so let me
explain very simply what how we can for
instance the randomizing hands-on write
inequality using custom istic estimators
so here our space of parameters will be
so remember that we have these defectors
x1 through XK there's this matrix a so
our parameters are basically indexing
what things we have fixed which we
haven't yet so there's a vector of
length K which can be either minus 1 0
or plus point what's happening with
respect I mean the the distribution will
pick according to this parameter D he'll
Sigma it will be such that it's smart
enough when the Sai I Sigma is 0 is
gonna be a Gaussian and when you fix one
of these to be either plus 1 or minus 1
we're fixing that CITV that value the
pessimistic estimator who will use is
based on the turn off inequality so
basically since we know that the failure
probabilities have been founded by this
you can use this proxy and what's
important to show of course is said that
you have this concavity property so
basically whenever you have a personal
simontann Sigma and there's a coordinate
which has value 0 currently so we have a
Gaussian there if you choose an
assignment you know Sigma I to be plus 1
or minus 1 these two on average are not
worse than the original see outside so
basically one of these choices is as
good as Scylla sigh of Sigma and then we
can move on so in very simple words this
the randomization is based on that idea
so we started with having no idea how to
fix it science so we have all gaussians
and we go by one by one minimizing this
side potential now a few technical
comments because I mean you need to
compute this this pessimistic estimator
right and and this looks as this moment
generating function
what is shown early in the talk is that
this kind of thing when you have only
gaussians it looks as a determinant of
something and even when you conditional
things you still get this kind of
expression so basically computing this I
will require or some algorithm for
computing determinate say for instance
STD so even if you try to make this
faster you're not gonna get linear time
however for the case of Jael things are
much better because our matrix it's a
block diagonal matrix where each blog is
a rank one perturbation of a diagonal
one so computing determinant of a
diagonal matrix is trivial takes linear
time and during this one rank one update
we can use this rima morrison formula
from linear algebra that also loves us
to do this infinite time so the good
news is that we can run this efficiently
for I mean not only efficient them but
in our time in the case of jail and we
know with an algorithm that runs in this
time s times so n is e of V in this case
is if you pick your whole input all
these vectors P is counting the total
number of non-zeros of the specs all
right so these are some of the spar
cities across all in perspective and
under the assumption that you know this
number of non-zeros is at least as large
as the dimension we give something which
has complexity s times n is you
referring to the sparse version of jail
like oh yeah yeah yeah I'm right right
because otherwise you'll get them here
you'll get the full number of rows I'm
thinking about when you already made
this masking so you get this T number of
and then you have s times T of them now
the masking is a bit more technical and
I don't know how much time we have only
ten minutes okay so I know it's not that
long but I want to show some numerical
so yeah so the thing of distributions
for the masking is of course a bit more
tricky because I said that things are
not independent but in fact we can we
can pretty much ignore that and it's
it kind of works as well so our space of
parameters is basically a matrix right
so where we have this massive matrix
which is 0 1 what we're looking at here
is the probability of selecting a
certain coordinate so this PR day
represents the marginal probability of
making that choice taking this as I said
in the King Nelson analysis not these
choices are not independent we have this
constraints right so for every column
we're selecting s coordinates but what
we can do here is basically we can start
with this space of I mean we can we're
gonna start with this P being called
equals to epsilon uniform choice and
we're gonna somehow traverse this a
little making deterministic assignments
one at a time so if you're familiar with
the technique of of piped rounding by
any means this is very similar so in
fact we use independent Martinez and
somehow we traverse this polytope in
such a way that we don't violate these
constraints so okay I don't want to go
too much in the details on what it says
but basically this is the one you get
using Turner's inequality in the event
that you want to entrust Rubinius lon
and so that's that's what remember that
event we wanted to control with high
probability was this Frobenius norm of a
was upper bounded by something so this
is basically what we're ready here we
just use holders inequality so that we
can make things nicely
he called sewing we instead of taking
products inside of this expectation with
these different choices of columns we
just use holders inequality and we take
these as exponents and basically what we
end up doing in the algorithm it's a
very simple idea let me go to the board
and just make a quick drawing of this
so we have this masking matrix train
this has m rows and the columns so we
know that and what we're gonna do is
partition the space of rows into s
blocks right here
and each block of course will have size
M divided by s so what we'll do is do
randomized by blocks the first floor
we're gonna complete the pessimistic
estimator here for every possible choice
and we're gonna stick to the one that
minimizes the potential that gives me
some choice and then I move on to the
next block I do the same so the whole
point is that I can do this updates
fairly quickly and also of course there
is a point that this this pessimistic
estimator has to be concave on the
blocks so every time I have a partial
assignment and I want to make an update
I make this these financially many
choices
since the pessimistic estimator is
concave there is one choice which is as
good as what I had before and yet again
there is some technical details that
happen here computing this thing in
general is expensive so what we do is
there is a choice of a particularly
input vector set which is basically all
the differences between different
canonical vectors and this we call the
universal masking but it's also present
in the King Nelson where they call it
the code construction and if you make
the assumption that your input is
sufficiently large then your running
time is guaranteed to be of the same
order that we have for the signing and
in this region what do we get is
basically the same running time that you
would get just with a randomized
assignment and then making the matrix
vector product so what we get all in all
is this deterministic guarantee for a
given input vector set so we maintain no
distortion we have the guarantee of the
diction proportional to what you get
with standard deal and the running time
is it's the same what you would get from
the King Nelson analysis so how much
time we have now like five okay so this
this is different pessimistic estimators
is that something standard for doing
musicians or something that you develop
it's one of them no no I a standard I
mean not many people know about but if
people that work in the randomization
really it's powerful and I like it
because it's very natural I mean you can
turn this probabilistic proofs into
algorithms and that's basically for it's
my provocative as Spencer okay yeah
that's from the 80s right again they're
having many works on right Joseph it in
fact there is a version of DL which is
based on this method of pessimistic
estimators we realize after we were done
with this work but it's kind of the same
idea in the dam setting the analysis is
is much trickier though I think so I
can't tell for sure but you imagine it's
something okay
Shema so let me quickly go through some
numerical results and this is very
preliminary work I should tell you that
this semester I'm teaching machine
learning course and I got this this end
of the semester project for my students
so they carried out these experiments
and I got them last week so I haven't
check all details like that just want to
show you a little bit how this looks
like so what we do is we get this
randomly generated data in dimensional
500 and we have a thousand
input vectors we want to maintain the
distortion of those with an epsilon
factor which is point you if you use the
the JL statement we had before this
roughly tells you that the dimension
projection is like this
so one of the questions were wondering
if is whether the randomization is much
lower so here there are a few results
with this instance in size you can see
that the random algorithm is
quick takes like 20 seconds the
deterministic one takes like twice as
much but they haven't completely
optimized so all these tricks I told you
of doing things very fast they didn't
test and so for instance they uses vini
for completing the determinants so I'm
still surprised that this works in this
many dimensions but you can see for
instance okay the success probability
you get with a random one of course is
going to be lower but I'm a bit
surprised that is this low so they also
carried out some is that probably as a
computer basically right so you your
construction is completely oblivious and
then you can evaluate the frequency of
things that were distorted like I mean
you have emigrant 175 how will you speak
that into that sounds point to be said
yeah and what is the Delta there's no
dog here because you're say again for
random measuring you take very clear no
no this is for particular realizations
and then you you you compute the
frequency of things that were distorted
oh yeah yeah and and then you can make
some box squats with more so here
there's finding realizations of this
thing so most of the time it goes pretty
well over 95% that's that's what I would
expect right but every once in a while
you see this you know it's much know
where you can performance so the
distance is it easy or hard for like a
jail so the instance you gave was rather
bright right
yeah maybe it's to say it maybe this the
hardest spends everything right it's
kind of my seats because it's full I may
be able to split things which are in
lower dimension to be actually easier so
for instance in this example what we did
is so there is this 500m ention but the
vectors are actually supporting at the
first 7 175 coordinates so one of the
things that I wanted to
test is whether these deterministic
algorithm will figure that out I mean in
didn't I mean I'm not sure so that the
the results weren't very conclusive in
that sense I don't know if it matters
for the comparison between the
techniques it's probably the hardest set
for thanks much for comparing the
domestic versus rondo - maybe egg may be
easy said databases get in 1997 maybe
let's go test so what else so here are
some pictures just wondering how this
looks like so first you see the the
random projection and second one is the
one we get from the deterministic
algorithm so maybe you can't see from
this far better
the first kind of the first block of the
deterministic one looks kind of a bunch
of diagonal things and that's because
the pessimistic estimator is is kind of
really optimized so in the first block
it looks pretty structured but then from
then onwards are actually also in the
first few columns has this similar kind
of behavior and again it looks more
structure it in a sense but I don't know
how to you know interpret this really
just just to show you how it looks like
okay so all in all let me conclude by
saying you know the image shadow
reduction is it's a key primitive in the
analysis and the dilemma allows you to
do this dimensional reduction quite fast
right so what we do this work is
basically show that you can do this
certain realistically based on this idea
from randomized algorithms and we can we
can basically reach the same performance
and there's this new kind of technical
contribution which is based on this new
proof of the hence right equality so are
a few open problems well first of all
we're not very happy with this
assumption that the number of nonzero
sufficiently large this is not necessary
but this would avoid I mean would
require something arguing in a smarter
way than using this Universal masking
something which is for instance
dependent
right also there's a question I mean you
asked at the very beginning right so
jail transforms are usually used in in
online or streaming settings where you
want to be completely oblivious to the
to the input right so we hope that maybe
there is a way to argue about some sort
of online settings where you can maybe
regret some of your decisions along the
way fixing things so you can still can
do good projections finally there's this
super interesting question about
deterministically constructing our IP
matrices so if you've ever heard about
this well see this these are IP matrices
are basically things that keep low
distortion but on the set of sparse
vectors and that's something infinite
set so I I don't expect that these ideas
will directly resolve this this is a
very hard question but I think it's very
interesting I mean it's something that
is good to keep in mind so with this I'm
I'm done thank you very much I'm going
to mention some applications and also
just in this allegory candidates with
particular other user settings where
this might be might improve the state of
the art let's think so I mean the most
important thing is whether you want to
be oblivious or not so in settings where
you don't care about being oblivious
then I think this can certainly help
because it is you guaranteed low
distortion right so it's more a matter
of what's what really your setup so I'm
thinking maybe some kind of machine
learning application or you already have
your data maybe you want to go with this
because it takes roughly the same time
as the randomized plan but typically for
streaming settings and an online then I
mean as this is it's not helpful and you
which uses the these non oblivious is
there an organization right the one or
is so let me say the masking for
instance its since we are doing this
universe of masking okay it's completely
oblivious so the point is are this
universal masking works for any kind of
input in the right dimensions okay
but then the signing I sighs so that
part of the that's your of making things
German istic is it makes it not only
this right so I mean there is some hope
that we can resolve this this online
question in in a meaningful way because
at least part of the algorithm is easily
missing assess its recipients you have
super almighty can get passed as far as
but not deterministic I mean factor fast
function sparse but sure so you can get
that part but nothing deterministic part
is the parts right here right but that's
usable to compare it with these
techniques where you don't care about
the sparsity of the matrix but if the
end of you
you care about the fast computation the
fact that you can multiply by the matrix
not even Tomas Adria transforms yeah
like you would usually use Fourier
transform as part of the right
multiplication but you can do all the
things we tried this and I think that
somehow these results are uncomfortable
so the regions were fast Fourier
transform is better but there's others
which this thing so for instance the
Fourier transform constructions do not
exploit the sparsity but then there are
these things that are multiplying mnz I
mean they don't run these only to put
the sparsity of the B vectors yeah what
you called an and z parameter right I
thought there are this isn't this what
this whatever maybe Woodruff and
Clarkson did that okay maybe it maybe
I'm wrong about that but I thought that
at least I was looking at all the work
so these Island and Liberty which they
don't exploit sparse but I have to say
I'm not</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>