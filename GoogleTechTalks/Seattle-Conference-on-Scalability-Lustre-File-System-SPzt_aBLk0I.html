<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability: Lustre File System | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability: Lustre File System - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability: Lustre File System</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SPzt_aBLk0I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and with that I'd like to introduce
Peter Brom who founded cluster file
systems can talk to about the lustre
file system before found in cluster file
system Peter was a professor at Oxford
at CMU and an architect at turbo Linux
and red hat and I think it'll be
interesting to hear a little about a
file system that also needs to scale to
a great size but with some different
requirements what we heard about Google
file system from Jeff today so without
further ado who's Peter Brom thank you
very much I have a agenda here for this
talk I will tell you a little bit about
who builds this file system and roughly
where it's being used because I think
that gives you a bit of a background and
then tell you a little bit about what is
included in a deployment of this file
system and you'll see some similarities
with the Google file system and some
differences the similarities by the way
have a rather simple origin many of the
people were all working on the same
floor in Carnegie Mellon in the computer
science department roughly at the same
time and our effector is also another
company called parnassus that built a
file system that's again rather similar
and it was also same floor and so I
wanted to focus on a lot of issues of
scale and I decided to take you through
a tour of scale as you know what if we
achieved today about what is running
today in at our customer sites but our
company is actually very much focused on
future developments we get dragged in
when people need to break the next scale
barrier and you'll see what kind of
people drag us in and so I'm going to
end this talk with telling you about
access skill I 0 which is actually
really an enormous step forward that is
going to take another 10 years to reach
yourself like that but people are
starting to work on this now so let me
get going so our company's called
cluster file systems which very quickly
became CFS of course so what do we
really do we basically build and support
this file systems we have a little
headquarter office in in boulder but
we're really a virtual company mostly
six years old we've met
had any investments which is quite
unusual for for small companies and
we're an open source company it's grown
to about a hundred staff members mostly
engineers but as a company grows you
need more and more people that are doing
different things and you know it's it's
growing fast but nothing spectacularly
fast yeah this is kind of moderate
growth but it's very steady so we know
that there are at least 250 supported
clusters they're probably quite a few
more because our partners don't really
tell us sometimes to whom they sell
sometimes the organizations are very
secret and sometimes they just say so
many terabytes or something like that
and we see about 10,000 downloads of the
software per month I you know I don't
know how many installations that
ultimately lead there's a top 500 list
of high-performance computer systems in
the world that's the fastest 500 that
list gets updated every every six months
and in fact it's going to be updated
today or tomorrow ourself like that so
I'm giving you a results that i have a
year old here and we have about a
fifteen percent share of that of that
list but what's perhaps more important
is that we run on six of the top ten of
these computers so if you think about
really big computers we have a pretty
good haul there on how to do storage for
them and that includes in fact the
biggest on every continent that has big
computers like the United States and
Japan has the biggest in Asia at the
moment that will change soon I think and
then cea has the biggest computer in
Europe at these by the way are in own
computers I think there might be unknown
computers and but I don't know about
them and as you might imagine for
unknown things and so IDC whom we've
never given much money and this is that
sort of the leading HPC high-performance
computer file system and the interesting
thing is that just like what Jeff was
saying things are changing people don't
have one cluster they have more more
clusters and we have now several very
big customers that have so called
site-wide file systems with a couple
dozen clusters sharing this storage and
we're looking more and more global file
systems where people want to go in in
wide area situations and replicate
and so on as jeffers just talking about
so how do we do this business it's it's
basically very simple partners have
picked this up just like they've picked
red head up and you know companies like
HP Dell cray and you know whole list of
actually pretty big companies they ship
our software from time to time and give
us a support contract but we also have a
nice collection of high-end customers so
these are people that you know make a
big investment in storage like a big
national lab or an oil company or a big
bank or so and we have you know a couple
dozen of those customers and they they
drive this company forward so typically
our contracts are in fact very very big
they are not small engagement but quite
big engagements so what's so unique
about this well it's a completely
software only solution and if you start
sniffing around there are and so many of
these things around many of these
cluster file systems are tied to
hardware it's open source and one of the
very important things that we found is
it's modifiable if somebody comes with
new network we will change the software
to support that network if somebody
comes with a different kind of storage
box that maybe has no raid controllers
or something like that will change
things we have quite extraordinary
network support you know in outside of
this high-performance computer
communities this is not surviving known
maybe but there are in fact lots of
exotic networks that are like an order
of magnitude faster than what you can
get with 10 gige and you know very low
latency and we run over all of these
networks natively doing DMA and so on
that was deemed to be important I'll
show you about performance and
scalability this is unlike the Google
file system a POSIX compliant file
system so it there's no special api's
it's not a user level library it's just
a file system and it has high
availability so that if servers fail
operations continue after a little while
so it scales well in in the same way as
Google file system skills you just add
more servers and the i/o will paralyze
and the metadata will paralyze further
and and so it's it's built from little
commodity bricks although we our
customers typically buy more expensive
breaks they don't buy the cheapest
surface that google buys device higher
end surface but this
same model so a good example of the kind
of use that we see was at Lawrence
Livermore National Lab they have the
biggest computer in the world it has
128,000 CPUs it's a beautiful machine
from IBM and in the first week they
created 75 million files and hundred
seventy five terabytes the file system
is probably closer to a petabyte exactly
how large it is and probably pretty full
right now so what you see here that
these files are relatively small yeah
this is only two megabytes or so per
file roughly speaking and we have many
users with very large files and many
users with very small files so there's
not not so much of it it emphasis on
very large files although very high
performance you basically only can get
with very large files so the market area
is that we're working in our extremely
large clusters a typical partner is cray
and you know extremely high note counts
like the biggest cluster in production
at the moment is 25,000 compute nodes
using one shared file system but that's
going up very fast people are talking
about a million compute nodes in 55
years and so it's growing fast and the
medium and large clusters these are
things that you know go from a couple
hundred nodes to a few thousand nodes
you know I would call it a 30 note or a
hundred node cluster a small cluster not
everybody would agree with that so we we
don't have to modify lustre for that at
all and our partners take it there you
know Dell does HP does and so on we
often know very little about it politics
is quite important there and high
availability is the more important at
lower scale if you if you go to these
organizations they're typically more
more picky about it and the third area
where we are beginning to be very active
is in what you could call very large
data centers that I'm not connected with
compute environments but they're just
general purpose large data center so you
could think about like a big bank or you
know the military which has like lots of
feeds into all kinds of organizations
and there we are again I think being
invited to the table for many reasons
like scalability performance but also
the modify ability because the
large-scale storage problems they are
typically new and are typically no real
solutions you can purchase of
people would do it yeah there's still a
lot of people walking in so how does
lustre work they've all come just in
time now we're talking about real things
here and so so luster has clusters of
typically commodity computers you will
see three types of computers in a
typical cluster lots of clients these
clients may or may not have disks but
they see this global file system they
can use it as the root filesystem they
sometimes do but typically they use it
as a data file system and it can be you
know from one client to hundreds of
thousands of clients and it varies a lot
there is a group of metadata surfers and
the metadata surfers handle everything
related to file names and and you know
location management and those kind of
things protection of files is largely
handed on the metadata services and
metadata serves have disks yeah they
need disks and typically these are raid
arrays and and then their so called what
we call object storage servers you could
also call them io nodes and whereas
there might typically only be a handful
of metadata servers like one or two
profile system there are very often
hundreds of i/o servers and these do the
file i/o as you may imagine and there
are lots of different companies shipping
different kinds of i/o servers with
luster some people have just little
celeron boxes with a few disks in them
other people by emc raid arrays yeah and
and there's just a great variation in
some cases its software rate that we use
in linux we have to change it quite a
lot before it worked well but it in some
cases it's it's expensive rate
controllers that other people supply so
there's just a great variety and that's
the beauty of this is just software yeah
but the integration tasks are not to be
underestimated yeah it can take a lot of
work to get a new kind of set of working
really nicely so how does this work
clients do file opens with the metadata
server yeah that's the most important
thing with IO is you open the file you
start reading and writing to it so you
go to this metadata server and you say
give me a file handle for that file and
a lot of information comes then to
declined and the client starts doing
file i/o and file locking with the i/o
servers the object storage servers this
is a simple model and
parallel Isis because there are lots of
objects server so the i/o which is
usually the bulk of the load can be
handled by many servers and skills very
well but when you do things with
multiple servers you introduce very
complicated problems because if a file
has a little bit of data on the metadata
server and then all its file data on
other servers just imagine what can
happen when the power goes off you can
be left with a piece here and a lost
piece there and that kind of stuff and
you have distributed recovery which is a
very complicated problem for which in
particular efficient algorithms are
fairly hard to come by yeah and I will
show you how we have solved most of
those problems so here is typical layout
of a file in lustre and we call
something a logical object for iam so we
virtualize all these devices that we
have a simple API say you know you open
an object and in fact it stripes over
many surfers and here you see one object
that is striped over three servers
that's the white object and you know
that so that grows from one to two to
three and and it will wrap around and
the wrapping around is shown in the
orange object yeah and so you could for
example imagine that a company is
rendering movies and this actually
happening quite a lot that luster Harry
Potter is special effects of Harry
Potter use this kind of model they write
little frames of the movie for the file
on many different servers and it forms
one file so there are lots of holes in
the file because you know one frame
takes longer to render than other files
and so on so it's it's from a POSIX file
it's usually quite a complicated file
and how do we implement these objects
and how do we implement the metadata
server it's just linux disk file systems
although we're about to come out with a
solaris version on C of s because you
have a very beautiful tile system but we
everything here is just stored in
ordinary we call it L disk FS files it's
really ext4 we do a lot of development
on that file system and I'll tell you
more about that so if you took a lustre
cluster apart and say hey let me look at
these disk partitions you can actually
mount them with ordinary files you could
save boy a lot of strange files here
they have numbers do don't have names
and then on the metadata server you
would actually see the file tree but you
wouldn't see any file data yeah you
would see a lot of strange extended
attributes that contained
you know the pointers to where things
are so the capacity and the scalability
features are very similar to what I just
heard from from Jeff petabyte file
systems today clusters go up to about a
hundred gigabytes per second and 25,000
clients that's the sort of scale that we
we work out today so we we support 10
different networks so of these 10 I
think there are four different versions
of a fini band networks in the industry
does have great difficulty to reach
standards is my impression yes so in
there you know there are three different
infiniband companies and they somehow
manage to get four versions of software
where of course one of them is supposed
to be open but you can't make money with
open software they say so that one isn't
used everywhere and so on we're out
between these different network so it's
it's possible with with luster to have
like an infiniband cluster on one side
and another infiniband cluster on
another side and they can talk to tcp or
some other network with with routing
software and and strangely enough this
becomes very importantly you have many
clusters because it's difficult to grow
these expensive networks we you may be
surprised to hear that half of our code
is networking code and this is needed to
get not just rich features like routers
and many network supports but also for
high efficiency you know to get
something like ninety percent out of a
giggi pipe is quite a lot yeah a geeky
pipe actually doesn't max out at a
hundred megabytes per second it's 125
yeah and we have a lot of code but we
actually can get up to about 118 out of
one gigabyte Ethernet link yeah and and
that has grown this network oh to be
very substantial and but valuable for us
many people like to play with different
networks I think the rest have already
said here there is advanced security in
luster is just about to come out a
version with kerberos you you can you
can find some betas on the other mailing
list I think and there is going to be
file encryption and audit logs and so on
there are lots of people that won't want
that kind of stuff
so scalability is the theme of this
conference so until today we've mostly
been capitalizing on growing the object
server pools for for i/o scaling and
this is very similar to the Google file
system yeah it you get more and more
servers you can do more and more I owe
you get more and more capacity and it's
served as quite well up to hundreds of
servers and a you know petabytes of
storage are possible with these days
actually not so many servers anymore you
know with a terabyte disk it's only
about a thousand disks to get a petabyte
yeah that sounded very far away and a
thousand discs they fit in maybe you
know 10 racks or something like that
yeah so that's it that's you know a good
room full of of machines but it's not
something that you can't imagine any
more but as you will see there are many
problems lying ahead that lead to new
scale so scalability to is metadata
clustering and that allows us to grow
the metadata operations many more there
isn't a huge demand for metadata
operations as an enormous speed you will
see that the so yes there are demands
but we can get pretty high numbers today
and the numbers that people are asking
for actually relatively easy to achieve
not thousands of metadata servers but
just a few hundred will carry us forward
for a very long time it looks like on
the other hand there is an interesting
challenge out there there isn't a single
clustered file system yet that has put
through like a million file creates per
second yet is to give you feeling that's
a problem that's not solved at the
moment and the problem is i can i can
solve it but I don't have the dollars
you know you need to get a lot of kit to
do this so I need to persuade the
customer to say you know we will do that
for you and they do sometimes so
scalability number three is to move into
divide area multi cluster arenas where
there are multiple clusters talking to
each other some will be proxying for
each other and another very important
thing is that typically people will
organize very carefully manage migration
so I worked on the code at file system
which had all these ideas you know you
can use it everywhere
disconnected and connected and so on you
got lots of conflicts this was a big
problem yeah that people would update
things on the laptop and then on the
desktop and it banged again each other
people know that and they are avoiding
it like the plague and so they do a lot
of management surrounding replication to
do that very carefully so that they stay
out of trouble and and so that's
something that we're very active with at
the moment to to get more and more
software to help these problems it
sounds very similar to what Jeff said
again so let's start looking at scalable
i/o and then I'm going to tell you a
little bit about scalable metadata and
how we deal with recovery because
recovery is such an interesting problem
and it's talked about so little and then
we'll go further with wild talk about
large numbers so here's an example of a
problem we were working on in 2004 I
think or 2000 yeah late two thousand
four we were asked to deliver a fast io
server and here you see some graphs of
an ext3 or ext4 filesystem these days
and and we're writing to too many files
in this with many threads so the
horizontal axis shows you how many
threats are writing and then the colors
show you how many files you're writing
when you're writing multiple files it's
actually very hard to get good I oh and
i will show you several examples these
numbers so this array can do about 1.2
gigabytes per second so it's you know a
little fridge with fibre channel cards
and things and and what you see is if
you write one file you can get very good
reads you see that on the right top yeah
you get the full bandwidth and the
moment you start dealing with two or
three files it goes to hell the cause of
it is shown in the left bottom discuss
your request sizes that you got out of
the Linux kernel are miserable yeah they
the Linux kernel is extremely good was
extremely good at taking large heyos and
breaking it up into small oils which is
exactly the opposite of other should be
doing yet this was largely fixed into
the tix but a few years ago we were
actually fixing fibre channel drivers to
not do this very strange that nobody had
looked at this here is how we reworked
the block allocator which is now in ext3
the allocator had to become very thread
sensitive to avoid intermingling
allocations and this is the kind of
thing that feels reasonably confident
yeah you see curves of things lying on
top of each other if you write a hundred
files or one file it doesn't really
matter you get more or less the same
bandwidth did this work actually led to
a very beautiful system it was delivered
to an organization in France that builds
nuclear weapons but also helps industry
do simulations for software it's called
CEA and they have 40 servers from a
company called bull I didn't even know
the bull still existed when they first
called me but it was the same company
and they're actually still building
quite beautiful server machines and so
they have 40 servers only each doing two
and a half gigabytes per second so they
put two of these raid arrays behind them
24 fibre channel cards per machine or so
these are not cheap little boxes and
they get about a hundred terabytes per
second out of that system yeah so 40
servers and they have something like
8,000 CPUs or so in that cluster but in
also a very big fridges so it comes in
in units of 64 something like that it's
an ia64 system and the networking in
this cluster is also very interesting
it's three strands of an exotic network
called Elon each strand is about capable
of one gigabyte per second about 800 and
so with 2.5 gigabytes per server you can
see that we're doing very nice plumbing
the pipe into the server and the pipe
out of the server is all equally large
yeah and plumbing is very very much a
part of architecting good io systems
yeah if you have a network pipe that's
too thick you're just wasting money
probably yeah and so we do a lot of
little calculations on envelopes well
there are no envelopes anymore but you
know I'm looking you know how much
bandwidth is the disk half and so on and
so on and then match that up so in 2006
we got attacked by some customer that
said we write a lot of small files and
it's not working terribly well yet and
so we started to work on yet another
disk allocator for ext3 or xt for file
system this one is not yet in the
colonel
we'll probably go in pretty pretty soon
and and this is all about writing small
files that so these people had literally
tens of thousands of nodes writing small
files meaning like you know a few k and
of course what you'd really like to do
is buy a whip and go after the
application developers and tell them
don't do this yeah but that's not the
way the world works yeah and you know
they're your customers you can't hit
them you have to do small i/o instead
yeah and it's very time consuming a lot
of engineering went into it but the
results are very beautiful I'm very very
excited about it so I'll show you what
happens so it actually turns out that
small files suffer from seeks very badly
so they don't really need a lot of tree
structures and so on to look up I was
prepared for all kinds of big misery
dealing with small files but if you just
pack them closely on the disk our file
system developers in the company
discovered that the i/o works very very
well I don't know if I'm saying this
year so Hans Reiser is of course
extremely well known for building file
systems to do very good small file i/o
but his secret is much simpler than his
solution the secret is that he put them
all very closely together he didn't need
these very complicated trees I believe
yeah he could have achieved the same
results with just tight packing as we
now do in EXT 3 here's a couple of
graphs and this shows you again how
engineering helps and this is a very
interesting graph d bench is a benchmark
that simulates a samba server so it puts
a load on a file system that's extremely
similar to a samba server and many
people run d bench one two or three
which means around two or three threads
and it always works so beautifully it
works actually very very beautifully
until you sort of hit 88 threats then
it's beginning to show sign of pain if
you run d bench 64 several the file
systems will crash immediately machines
become completely responsive and the
throughput becomes a complete disaster
typically and so here you see how very
famous file systems like the X of S file
system has excellent the bench
throughput
if you just use a few threads but if you
hammer on it the 64 threads or so you
see that the throughput almost becomes
nothing you can see that the ext4
allocator is a big winner here this has
a lot to do with multi threading and
allocation and the by the way the file
sizes of D bench are a mix from about 4k
to a few hundred cade largest files in
the benchmark are are a few hundred K so
there what we would call quite small so
probably if hamster eyes was here it
wouldn't be very happy yeah because this
isn't what people are seeing normally
and you know you can you can argue this
is not a load that the Linux community
is very interested in but my servers run
in fact usually more than 60 for threats
sometimes few hundred threats yeah and
so it's very similar to a server load
here's another set so here by the way I
think this is in second so higher is
worse here and there are two things one
is unpacking a terrible it's linux
tarble which of course is the canonical
tar ball in a company like mine and and
so entire thing and then remove it again
and what you can see now is that ext3
and ext4 are hugely different so that's
the different allocator and at least for
removing and the fastest I'm actually
kind of par the riser and the HT three
with the new allocator dude they're
doing doing pretty well so this is one
of the reasons we've stuck with ext
three so far and it also has a very good
filesystem checker that's very well
known of course it's a very reliable
file system but I can tell you one big
disaster we encounter with ext3 and I
will do that later in this talk at the
end of this is in sight so here you see
a graph that is an interesting one this
is from a computer called red storm it's
used in a national lab called Sandia
that's in Albuquerque New Mexico red
storm actually doesn't run linux so we
had to port the file system to some
microkernel operating system and this is
a machine with very many clients it has
25,000 clients and we
they did a test here with 10,000 clients
so what you see in the horizontal axis
is what happens with the IO when you
have more and more clients joining and
the there are two graphs at the top it
shows you where every process is running
every every node has its own file to
write to and the bottom is where the
entire cluster is writing to one file
that sounds pretty exotic but you could
it actually happens yes it's 10,000
people writing in one file and so
they're actually very interesting things
like first of all it's not great yeah
because it isn't a very constant graph
also if you don't have a lot of money
you might be very concerned that it
takes two thousand nodes to reach full
speed yeah i mean that's that's kind of
absurd but i don't know what the reason
is it probably has some some kind of
weird reason and what we learned from
this actually was that the erase were
misconfigured and so they had set them
to do a lot of read ahead and read
halves great if you have a desktop
machine yeah it's not great to have ten
thousand machines trying to do read
ahead yeah because there isn't enough
memory anyway yeah and so why are you
doing read ahead and so integration
issues left the right it also turned out
that if you look carefully the top graph
hovers around 40 gigabytes per second
the bottom graph at 25 or 30 and it
turned out that they had hooked up the
disgrace in the wrong way they should
have used logical volumes to use more of
the fibre channel connections so the
single file for some you know rather
silly reason couldn't get the full
bandwidth integration issues integration
yeah you know so it's kind of annoying
that basically the software can do it
and it's still very very complex to get
something like this together when a
machine like this is put in production
it costs a hundred million dollars
roughly they are not giving the file
system hackers much time to play with
this yet they immediately want to start
doing their computations and so the jobs
are typically very rushed and somewhat
compromised yeah and I think different
organizations have even more demands
than than others good so when you start
looking at extremely large file systems
there is a problem coming up yeah so if
you have a petabyte file system and you
have 500 gigabyte drive
very common kind of configuration today
you have two thousand disks or suffer
like that and one disk will bro blow up
pretty quickly yeah and and the
estimates vary it's really very
interesting how much they vary the
manufacturer will say twelve days
between disk failures in in a cluster
like this with with two thousand days I
was in a pessimistic mood when i gave a
talk and I said well I think it's ten
hours it actually turns out that which
is very different I was much closer than
the manufacturers to the reality we
asked a customer that had this kind of
configuration and the day came up with
something like 30 hours and by the way
there is indeed the trend that disc
brake in the beginning then it gets
better and then it gets worse again and
so on so new clusters are very liberal
for breakage and then it's a bit better
and then older clusters are vulnerable
but double failures will start to happen
quite quickly yeah and this is a problem
so the other thing is that at the moment
linux file systems disk file systems
many time and maybe XFS doesn't but ps3
has some kind of limit on the number of
terabytes in it and we've tried to
remove that it's it's now eight
terabytes or so at the moment it we used
to be two or so but there's actually
whole load of bugs left or right in the
system and if you want to push that up
to like a couple of petabytes or so for
this class system it's not possible so
there are lots of limits it also can
only be four billion I notes per file
system and customers are asking me for a
trillion I note so I need a lot of file
systems and I prefer to have bigger file
systems so things really need to be done
here and and the most important thing is
that this hardening problem has to be
done so if you have a raid controller
blow up that has a big cash or if you
have double disk failures or something
like that you need to check your file
system and checking a couple of
terabytes of disk takes hours and hours
yeah and we have actually made speed up
improvements to f ck so it goes like
five or ten times faster now but that's
still not enough yeah it takes hours and
hours and you you you know you have you
have no access to some files that's the
best possible situation in the worst
possible situation there
is no file system yeah if the metadata
server needs to be checked it's very
very bad so Z of S is a file system from
some I've seen a lot of beautiful
research and a you know they put it
really well together you know it's
always possible to find some you know to
be critical about it and so on but it's
it's actually very very beautiful I
think it will probably be ported to
Linux but will take an incredibly long
time you know it took years and years to
get XFS really stable and I don't think
that CFS is going to be stable and
there's licensing problems and
government a lot so there's another
approach to this that says it started
University of Wisconsin they called this
iron ext3 and the way you start
protecting your data is to add little
checksums to everything so your CPU
usage which at the moment is almost zero
on these IO nodes it will shoot up yeah
because just to mention that you get a
gigabyte of data throwing so is flowing
through a server you have to start to
check some all kinds of little bits yet
to make sure that you know if the data
has integrity and that's by the way also
at Z of S does so the approach that
Wisconsin proposed is I think also very
attractive because they proposed a lot
of little fixes so that every time you
make a small improvement and you reduce
the chance of checking but unfortunately
this problem is so important for us that
we will have to jump on both bandwagons
yeah and we we will work with ZFS
surfers maybe with open solaris or maybe
we will help with porting into linux and
we will help ext4 get further it's just
we are in this business of large file
systems we have no choice but to to help
so many people start asking us so what
about one petaflop system so at the
moment the fastest computer on the world
runs at about 300 teraflops and that's
with 128,000 cpus and everybody is now
talking about petaflop systems being
delivered in one or two years and so a
petaflop systems wants to get about one
terabyte per second of IL yes sir that's
a little bit more than 100 gigabytes
it's only a factor 10 more so we have a
quite a good story here we have servers
that do already two gigabytes per second
we also have closed
that have 500 i/o servers and so if you
multiply these two numbers that system
would do a terabyte per second it just
so happens that at the moment we have no
customer that has you know so many very
big servers but there is a way out here
yeah and so the other thing is that the
number of clients that we handle at the
moment is 25,000 and the number of cores
is jumping up very fast so the number of
clients is not going to jump up very
fast for a petaflop system so we for
once this doesn't happen very often we
are very confident yeah the pedal op
systems are not a problem for us yeah we
will we will deal with that without much
much difficulty good so let's look a
little bit at metadata this is a very
short example yeah so metadata in luster
is really very clustered so for example
we can store the name of a file on one
node and the inode for the file on
another note yeah so we really tear
apart file systems there and here is an
example of how you make a directory so
make directory will go to some metadata
server and then that metadata server may
say oh I put the inode somewhere else
for based on some kind of policy and so
it's a complex system to place metadata
and in this way you can get very nice
load balancing because you can for
example place the metadata based on the
hash of the name and this is a very old
thing that comes from the facts days
yeah so you compute the hash of the name
you pick your metadata server and that's
where you go or you can say I do it
based on the network ID of the node so
that nodes get locality on their
metadata servers or you do based on the
user ID of the user so that users get
locality or metadata surface yet
different policies probably have
different applications yeah and we've
already heard of many different
proposals for using this or that policy
so here you see a simple example of some
metadata testing with doing a stat so
you get the attributes of a file the
file size and so on and this is in a
rather a collection of rather big
directory starting with a million files
to going up to 10 million files in one
directory you have to make by the way
some changes to the Linux kernel before
10 million directories works well the
machines freeze for reasons that are not
so very interesting but
important the it never it continues to
surprise me why people ask us to do
these things do because you rarely ever
hear actually do I have time for a
little joke yeah I do and the end so the
first accident with million file
directories is actually really worth
telling you about at Lawrence Livermore
somebody had not cleaned up his data and
he created more and more and more files
and actually built a directory with two
million files nobody knew about this in
most cases people would know about it
because it would stop working yeah but
it didn't stop working and so does this
two minute files at least it didn't stop
working until I tell you what happened
next and so the raid controller blew up
and these raid controllers these guys
have no money and so they buy cheap
brave controllers and so they leave the
file system in an inconsistent state by
the way this is 2002 I promise you that
what I'm about to tell you will not
happen today so they use f ck an f ck
had a check it said a directory bigger
than 32 megabytes must be completely
corrupted and it said goodbye to 1.8
million files if one blow oops they were
gone and so the file system was actually
pretty healthy but the files were zapped
out they were all temporary files that
should have been cleaned up anyway so we
did that very efficiently during the
crafter and so um anyway the interesting
thing about this graph is not so much
that it scales if you get one two three
four metadata servers the other thing
that you see is distributed caching you
see the knee in all of these graphs yeah
discs are dog slow and so once you run
out of cash you will go down very very
badly yeah there is no answer there is
20 milliseconds seeks on disks yeah and
so the knee goes up quite linearly too
yeah and that's nice because that means
that a load can actually be helped very
much by memory in the whole cluster okay
few slides about recovery there are lots
of problems with recovery disk file
systems need to recovery our servers do
very few things synchronously so when
the power goes off they roll back and
the clients will replay the stuff so so
that's a big problem
open and maybe unlink files may need to
be reopened but the most interesting
problems have these distributed problems
so we have files and then we have what
we call the objects which are the pieces
of files that contain the data and as I
said you can lose a few of these and be
left with some half half things have
finished kind of file creations so
recovery always aims to restore a
globally consistent state and I'm going
to show you a few of the mechanisms that
we use I think some of these things are
a little bit original there is so simple
it's hard to believe that they are
original so lustre has a request
protocol it's quite different from other
network file systems at the moment we
make a request like make a directory and
we immediately get a reply directories
made here's the inode number and so on
whatever you want from that reply but
the client is aware of the fact that
this may not have to hit the disk yet so
it keeps the request to replay it and so
there's a second reply coming called the
commit call back and that's a case we
get drawn here is the fifth step in the
thing I will come back to the other one
in moment so file systems and also
databases strangely enough don't have a
hook normally to tell you what you did a
memory is now on this we call that the
commit callback we added it to to the
file system so that we are told when
things commit so that we can stop
keeping this replay information and and
this keeps the possibility of you know
keeping limited amount of replay from
operations and discarding it at the
right moment and that's the foundation
of our of our mechanism now there is
another very important thing we are
using clients to to replay state and
it's sometimes very important that we
know that the client has to reply
because if the client doesn't have to
reply to requests maybe it can't rebuild
the state for for some kinds of replace
and so very funny networking issue here
we have reply acknowledgments for
certain requests and they are needed to
avoid a problem called cascading aborts
it's a known problem in databases but
it's not normally used in this kind of
setting so this gives you a picture
so I'm going to think skip over one of
it I just talked about this slide
actually so there's nothing new here I
can tell you that there are a lot of
problems at at large scale so I'd like
to actually talk here about two things
so if a client goes away for a while and
it comes back with a lot of cash data
may be unfinished operations what do you
do you need versions you need to be able
to say well nobody touched this thing so
I can still dump my dirty data onto this
file yeah and i'm sure that nobody else
has touched it funnily enough file
systems have no versions yeah so xt3 has
no way to tell if a file really change
yeah a 1 milli one second granularity of
an a time or so it doesn't do it yet you
need exact versioning of things another
thing that is very interesting is
timeouts so in busy clusters big large
clusters the timeout mechanisms that all
our PC protocols have they simply stop
working and we see that IO in a busy
cluster can take 300 seconds to complete
for a simple foul right there is nothing
wrong with the clusters nothing wrong
with file system it's just that
everybody is wanting to do I oh yeah and
there are thousands of requests backed
up on these servers and it just takes a
little while to get through them but the
timeout value of 300 seconds is
indistinguishable from a complete
failure yeah people get irritated they
walk away the hit control C and so on so
you need adaptive time out yet so that
in normal situations the system will
very quickly detect the failure of an
operation and in other cases people kind
of know they have to wait yeah and so
adaptive timeouts is complicated because
you have to send extra messages good I'm
going to skip this diagram because it's
kind of a long thing to talk about I
want to finish this talk talking about
XO scale because it's the next thing to
come and it's it's nice to do things
with big numbers so at the moment the US
government is calling lots of meetings
left to right to discuss exascale
computers they are targeting
these 4 2015 to 2020 so about eight to
ten years from now so they if you look
at this today you would be looking at a
machine with 250 million cores my houses
in Canada that's 10 cores per member of
the population yeah so that's this
computer would have more compute power
by an order of magnitude than Canada
maybe by two orders of magnitude yeah
and it's pretty interesting how many
course this is it would be about 2
million CPUs couple hundred cores per
CPU ya in a few years time maybe about
500,000 client nodes that's 20 times
more than we have today maybe not so bad
here we have 10 years time but maybe we
can be lucky maybe the clock speed will
still go up a factor 10 clock speeds are
no longer going up very fast and then if
we had 500 course / soccer the whole
thing would be back to the scale of
today you know 10,000 clients we know
how to do that more or less I I don't do
energy and the am but it maybe it's
reassuring to know that many of these
machines run a nuclear weapon lab so
they can generate energy but maybe not
very controlled and um so the file
system characteristics is that the
minimum bandwidth that they want to look
at is 250 terabytes per second terabytes
per second yeah this is you know so this
is a thousand times more than what we're
touring with today and the reason is
that they want to dump the memory of the
machine about 40 petabytes of memory in
this cluster to disk in about five
minutes that's the current framework of
computer applications to love data
memory it needs to get hit the disk in
about five minutes and the funny thing
is the disks are not the problem there
will be easily be ten terabyte disks and
so you know having a couple of exabytes
of storage so that's not the problem at
all but the next issue is the bandwidth
of disks is a disaster yeah it is not
going up very fast yeah at the moment a
disk bandwidth of a good disk a good
saturday about 50 60 megabytes per
second if we're very lucky it will go up
to 250 in 10 years time yeah but that's
really challenging because there are
problems ahead in the disk industry
and that means we don't need a hundred
thousand discs we are talking about a
million discs and a million discs is a
lot of discs so this is a problem that i
think is not so easy to solve and you
know compression of data may help and
there a couple of other things that
people can do here like staging the data
so i want to tell you a few other
interesting things to this slide we've
already talked about discs are going to
fail so fast yeah 200 discs per day now
that's perhaps not a problem there are
libraries with five million books so why
couldn't you have a computer with five
million discs yeah the Harvard library i
think as five million books so we just
need to take librarians and send them to
sis add courses and then i think
everything is maybe well but we need
very good software to manage disk
failures at the moment many disk
failures result in a disaster even with
very expensive fenders this is not so
widely known i think emc doesn't tell
you these things or no vendor will say
you know oh if disk fails you have a
eighty percent chance that your array
doesn't come back it's not eighty
percent but it's probably something like
eight percent or ten percent if you have
to in the disks failing per day we need
very good software the other interesting
thing is currently we're using rate
technology from the 1980s rate 5 or 86
is Stone Age stuff and there is actually
much better rate from the 1990s that's
an order of magnitude faster to build
it's called parity d clustering and so
on and maybe there is a maybe there is a
solution to the raid problem I'm not so
so pessimistic about this and we already
talked about integrity issues yeah we
need check summing end-to-end
verification that the data is correct
metadata performance is not a problem
there will be 50 to 500 million cores in
a system like this so we need to be able
to do 50 to 500 million metadata
operations all of these cores will want
to create a file but you know today we
can already do 1,000,000 metadata
operations and you don't need very many
discs where this you only need like
100,000 servers for this and so I have
the feeling that it will probably just
scale over 10 years time yeah it's not
like a few orders of magnitude out it's
just like one or two orders of magnitude
out and you know today for example
the blue jean L machine at Livermore
creates 128,000 files and you know
they're complaining to me the sake it
takes about a minute or so yeah and it's
2,000 files per second sustained on a
you know relatively simple server just
one of them recoverability is something
we just talked about at this scale yeah
this is like internet scale yeah this is
it this is really very very similar to
having a city with computers yeah a big
city and so there will always be
something wrong in your network there
will always be serviced blowing up and
clients blowing up so you really need at
the file system level to have restart
ability yeah if something didn't make it
you want to just be able to send it
again and it goes through unfortunately
standard UNIX semantics is not the right
thing for this yeah you need some small
changes to this like versions and so on
to really know what this done already or
not yeah and if it was done what was
really the result and so um so that's
very important another thing has to do
with congestion control on the network
yeah if you don't do resource management
well you will get lots of stuff on the
network that you don't want on the
network so that I have a slide about
that stuff here so quality of service is
going to be very very important in a
cluster like this you don't want to have
accidental bottlenecks if you in a
large-scale machine or in a city
basically if you make a mistake with
research management and you send
suddenly you know millions and millions
of requests to one server you have a lot
of problems for end-users yeah all these
clients will have to wait or crash or
something like that yeah it's not going
too well so negotiating where are the
bottlenecks is going to be very
important this problem is very different
from the internet problem because it's a
read/write problem yeah it's not a
mostly read problem it's mostly rights
in fact that need to be managed very
carefully so I think there's going to be
a lot of exchanges in networks to decide
which servers are relatively empty let's
put the data there because they have the
bandwidth and adapt that as the load
varies in the cluster but networks are
very fast disks are very slow so making
these networks protocols richer I don't
think is a problem yeah it's
the disks you don't want to write
anything that you don't have to write
because they're so slow I have a little
road map here but that's probably best
left so here you can see that in 2014 we
hope to have an ex a flop system you see
a lot of steps along the way that I've
talked about yeah in 2009 I think there
will be right I Aria cluster is coming
and you know right now we're sort of
going into the petascale I think I have
a few minutes left for questions so I
want to stop here go ahead what is the
metadata data structure it's actually a
standard file system but we shifted the
route into a subdirectory because we
also need a pile of administrative files
so you can do LS there and you see
almost everything that you expect to see
and the remainder that you don't see is
in extended attributes but they are
standard extended attributes so it's at
the moment we do it with completely
standard file system no it's all hashed
and so on so I so there are a lot of
changes in that file system so you look
up a hash values and you find I note
usually the extent that attribute is in
the inode and so on so you get it in
memory in one blow anyway so it's hashed
over files yes it's right yeah and and
no more disk accesses actually typically
than writing in a local file system it's
it's it's not there isn't a lot of
overhead
I I expect in 2005 that is a print we
actually got pretty close to having one
on in 2005 but it's it it's going to be
next year i think there's now a customer
paying for this so the question was is
OSX going to be supported yeah at the
declined for Apple that darned a lot of
Apple computers out in this community
they're kind of expensive but there are
some yeah maybe one more question I
think then we're probably beginning to
run out of time go ahead ah do I see a
transition to flash based hard drives a
lot of people are talking about these
drives of course they're very expensive
still I yes if the cost comes down there
will be a transition and the attractive
way to use this is as a nearby cash in
conjunction with with compression so put
some flash right on the motherboard may
be related to a compressed size of half
of your RAM or so yeah so if you could
compress half of your RAM and then that
would be a good amount of flash that
would help tremendously yes sir that's
that's talked about a lot but it's too
expensive at the moment yeah one more
then go ahead for all of your customers
sequential bound there's everything you
talk to us all sequential most of my
well they are my customers sequential
bound actually they they're not the big
numbers are all for sequential I oh I
can tell you that most of my customers
are bound by clearance issues and I have
no idea what they're doing yeah and and
and you know nobody in my company has
clearance and if so I I don't know what
they do but I think the load is very
varying the good numbers come from you
know sequential stuff yeah that's what
was that it's just seeing how do you do
so the object storage servers helped
tremendously with random i/o problems
because you have so many of them you get
an enormous amount of seek operations
and lustre can easily beat NFS on small
file operations for example because it
just has more servers yeah how could it
not in effect it also does fewer network
transfers yeah so but at that point
there may be some other optimizations
like more group commits and so on that
would help and and you know we will be
doing quite a lot of work on this stuff
yeah so it's a it's also somewhere in
this roadmap maybe we can take this up
in the corridor all right Peter thank
you very much that was pretty exciting
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>