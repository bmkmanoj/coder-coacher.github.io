<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 4:  Contrast, MTF, Flare, and Noise | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 4:  Contrast, MTF, Flare, and Noise - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 4:  Contrast, MTF, Flare, and Noise</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tNvFsOvVkOg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to lecture for the photo Tech
edu series onion McClatchy
so today I'm going to be talking about
contrast and and what happens to your
image on the way from the surfaces that
your photography photographing and the
sensor that actually generates the image
so to do that and we have to talk a
little bit about luminosity I'm going to
cheat a little bit there you'll see in a
second
define contrast in terms of luminosity
and then the modulation transfer
function is the way of describing what
happens to contrast on the way to the
sensor and then I'll talk about one 1/2
of what happens to the modulation
transfer function which is flare and
then I'll try to motivate why we care
about contrast at all by talking about
noise so so luminance is defined as the
the light power coming from a surface
per solid angle what we're looking at
here is a is a is a line of eleven boxes
at gradations of 10% a full range of
what you know this projector can produce
and talk about in terms of percentages
is not the way you work when you're
computing lumens or candelas or whatever
coming out of actual surfaces and trying
to count photons landing on the center
but for the purposes of doing math in
your head and and getting the general
idea of how MTF functions work
percentages work pretty well so I'm
going to cheat and for the rest of this
talk I'm going to talk about luminance
in terms of percentages of full range so
but to give you how I really started
cheating to give you a better idea of
what luminosity how it's actually
defined your eyes are more sensitive as
I think Nick talked about in the first
lecture your eyes are more sensitive to
wavelengths around green at 555
nanometers than they are
two wavelengths either shorter or longer
than that and your total spectrum that
you can see ranges from about 400
nanometers to about 700 nanometers
there's two curves here the green one
corresponds to your sensitivity into two
it's a very dim light and the black
curve corresponds to your sensitivity to
brighter light like what we're in right
now there's actually some debate about
what the right curve is for that black
curve and so you can see on this slide a
number of variations around 450
nanometers these are these are actually
defined by the the CIE organization so
so what these curves are trying to
convey is that the same your perception
of brightness for the same amount of
power at different wavelengths your
perception of brightness is different
and so to get an overall value for
luminosity we basically do a weighted
sum of the spectral power hitting your
eye weighted by one of these two curves
depending on whether we have bright
light or not and for practical purposes
we almost always use the the dark curve
here for estimating things about general
purpose cameras
thank you yeah the curve that's drawn in
dark not the curve destroyed and green
the curve that's drawn and dark
corresponding to light levels like what
we have here so just to sort of
calibrate you what we're going to see in
a second this is a ninety percent versus
ten percent alternating series of
squares so the way of ovulation transfer
function works is you can imagine you
have a surface and the surface might
have a series of white and dark lines on
it a fairly bright white that you can
get might reflect as much as 95 to 97%
of the light hitting it if it's a
diffuse reflector it's hard to get more
than that and you're you're sort of
white paint or whatever will generally
get to about 90% on the other hand it's
hard to get down to zero percent
reflection with blacks and I believe you
can get down below ten percent but in
general you know ten percent ninety
percent are are sort of close to the
bounds on what you'll see in ordinary
objects around you black shirts the the
floor that kind of thing and so the
difference between those two is 80
percent light emitted from from a
surface on the left-hand side of this
slide here goes through a lens and
arrives at the sensor surface and there
we've we've lost some contrast the light
areas have gotten darker the dark areas
have gotten lighter
and now we have about the difference
between the two is 40% of full range
forty percent is half of 80 percent the
MTF here would be described as being at
50 percent it's a little simple MTF
typically isn't specified is just one
percentage in fact there's a number of
different ways of specifying it I'll
show you what it actually is later on in
the talk of somewhat complicated so we
always try to summarize it in some way
because many of the complications of
looking at it aren't necessary
so typically MTF will be specified at
different frequencies spatial
frequencies at the sensor surface so
here i've made up to 20 line pairs per
millimeter and 40 line pairs per
millimeter a lion parrot per millimeter
40 line pairs per millimeter would mean
that you had 40 dark and 40 light
alternating stripes in one millimeter so
that would mean that this individual
stripes would be twelve and a half
millimeters wide so once again you can
imagine we have this alternating pattern
on the left
if the MTF at 20 line pairs per
millimeter is pretty good maybe 90
percent but it's rolled off and by 40
line pairs per millimeter it's down 50
percent we'd get an image something like
on the right and you can see that the
higher frequency bands have lost more
contrast
so I want to motivate lenses are
typically quoted at very often they're
coded at ten twenty and forty line pairs
per millimeter so I wanted to motivate
you as to why that would be pictured
here is a fairly typical full-frame CCD
this would be used in a high-end digital
SLR most digital SLRs do not have
full-frame sensors they're a bit smaller
but this will serve as an example so in
this particular full-frame sensor it's
what all of them the full frame means
full-frame for a 35-millimeter camera
that's a term of art I suppose so that
means they're 36 millimeters wide 24
millimetres high the pixels on this one
or nine millimetre nine microns on a
side and sizes four pixels range from
between six and nine microns so if you
if you think about a set of nine micron
pixels covered with it without with with
a pen chromatic sensor that's not trying
to capture color information the the
highest frequency they can capture
without aliasing would be almost an 89
micron stripes of dark and light on the
sensor the corresponding frequency would
be 55 line pairs per millimeter on a
sensor that had smaller pixels you'd
have a higher frequency for
general-purpose cameras most of them
have bear pattern filters on top of them
and as a result what that means is they
have alternating red green and blue
filters over the pixels and as a result
they patterns at 55 line pairs per
millimeter or whatever the pixel pitch
is when would introduce color soirée
and so to avoid that they sometimes have
anti-aliasing filters but in general
they're not interested in frequency much
above about 1.8 times less than their
pixel pitch so what that means for this
guy is about 30 line pairs per
milliliter and so that's why you see
these things quoted at 10 20 40 line
pairs per millimeter for a particular
lens you can read the fall-off of
sharpness from there and get an idea of
what that's going to mean for a
particular sensor that this thing might
be dropping it on to so I thought I'd
show you some measured MTF curves these
are two from a website called photo
TOCOM it's a real asset so a man behind
this website measured several hundred
lenses over the course of about 20 years
and put them all on the web so you can
compare and contrast them so this is a
this is a very good lens I have known
this lines and so how do we read this
chart here what is MPF so if we look on
the left hand side at the lower
left-hand corner of the chart says 9 t /
8 that means it's a 90 millimeter lens
and he stopped it down to relative
aperture of FA and there's you can see
three solid red lines and three dashed
red lines on the chart solid red lines
correspond I believe to sagittal MTF and
the dashed red lines correspond to
tangental MTF and from top to bottom
they are at 10 20 and 45 pairs per
millimeter and that's true of all his
charts and also most industry standard
charts so I should explain the
difference between agile and sagittal
and tangential MTF the amount of
blurriness that a lens exhibits is
different in the direction towards in a
way from the center of the optical axis
of the lens and tangential to the
optical axis and so they actually
measure them separate
and you can see they behave differently
on these charts the other thing you can
see on these charts from from left right
you see these numbers at the bottom if
you have really good eyes in the back
going from 0 to 21 if you remember that
full-frame sensor was 36 by 24
millimeters so it's 42 millimeters
diagonal so from the center to the
farthest corner is 21 millimeters and
the way these lenses are rated this is
also a typical way of presenting a
summary of MTF is they're rated for the
blurriness of the the lens produces as
you travel from the center of the
optical axis out towards the where it
intersects the the image sensor out
towards the corner and so this lens is
typical of most lenses you can see on
the right-hand side that it's blurrier
in the corners than it is in the center
and you can see that it's blurrier
entire line pair per millimeter
frequencies than it is at lower ones on
the there's two charts here on the left
I said it was a relative aperture of f/8
and on the right it has a relative
aperture of 2.8 which is much wider yes
does that mean that essentially all of
the MTF loss comes from blur or is there
significant I've had a lens for I
thought it almost all came from scatter
inside the lens because it was sharp and
flat thank you so what what Rob covered
in the previous lecture was loss of MTF
blurriness due to aberrations and and
not as big what's it I guess its lack of
stigmatism in the lens and what I'm
going to cover in the later part of this
talk is going to be flare which is
exactly what you're talking about and I
have a slide coming up which will sort
of try to show the difference between
the two yeah
so just to clarify for me the you said
the center of the lens versus further
way but of course when you stop down the
lens like to f/8 or even we went further
we really are only using areas near the
center yet we see yeah it falls off even
more drastically at f/8 yeah apt to it
good question so so when we talk about
the center of the lens that it's number
of points along that optical axis so
when you stop it down you're using your
your your cutting the light to use a
smaller aperture through the aperture
stop of the lens which is somewhere in
the middle the the light projected onto
the image plane there's less light but
it's still projecting over the entire
image plane I'm not sure if that answers
your question
the the the radii listed at the bottom
of this chart are on the image plane not
not the aperture so so the issue becomes
the fact that it's no longer it's not
going as parallel or whatever to the
image plane but the light is coming from
the center of the lens to the side of
the sensor yes so the question really
has to do with what's stopping down a
lens really means and and the questioner
is trying to so if you imagine that
there was an optical element just ahead
of the the aperture stop in the lens
then as you stopped it down as you say
you would use less of the periphery of
that optical element and more and you
would be left with only the light going
through the optical axis at that optical
element that's not true of optical
elements at the front and back of the
lens in particular near the front of the
lens all of the rays that go to one
point say in
corner of the image may may pass through
only a small portion of the of the the
front element of the lens and you can
see this if you take a lens and you take
it off the camera and you open up the
aperture and you look in the front of it
you can see the entrance pupil and what
you'll notice is that the entrance pupil
isn't covering the entire element that
entire front element so if you were to
cover some portion of that front element
you would not be occluding any light
going between your eye and the you know
whatever it was getting projected to in
the image plane so and that sort of
highlights the difference between
stopping down at the at the aperture
stop of the lens versus making a what I
think is called a field cut in the in
the in the image plane so so once again
these radii are referring to the image
plane which is not reduced when you stop
down the lens please mentioned notice
that it was kind of bad for the corners
that's usually the opposite of what we
expect
what can you explain to imply that might
be I see so the questioners points out
that it appears that MTF is dropping off
as we get to farther radius from the
center of the image and that's different
than what you expect I see your point is
up what you're getting at is that yes so
your point is that when you stop down a
lens you would expect to become sharper
and so what you would expect is that on
the the f28 graph on the right that all
the lines would be higher than the ones
on the left right so when you stop that
lens the benefit that you have is that
you're using less of the glass in your
optical elements and you're subject to
fewer of the aberrations in those glass
elements on the other hand you're more
subject to diffraction and so
this gets into difficulties lens design
that I'm not really qualified to talk
about but essentially you don't always
get what you expect and stopping down
off and can't make things worse in some
areas although as you say in general
stopping down the lens will make overall
usually makes the overall lens sharper
until you get to a fraction limit and
the diffraction limit for most lenses is
around f/8 and once you start stopping
down more than that usually there's very
little difference between f/8 and f/11
but past that you start getting into
noticeable diffraction effects
yeah Francesco yes yeah absolutely
this has to do with the if we were to
treat this lens as a as a telescope and
we were to look at stars and we were
look at the projection of those scars
onto the image plane we would get
something called a point spread function
and I'll get to that in a in a slide in
a moment and that point spread function
will change shape as we get to different
parts of the image and you can get these
minima from the different ways that it
changes shape that's not a very good
answer wait till I get to the point
spread function slide and and hopefully
they'll answer your question little bit
better so the thing that I wanted to
show between this slide the next slide
is that this is a 500-dollar lens and
this is a 4,600 ollar lens this is the
sharpest lens that the guy who did all
this testing has ever tested it's the
Canon 200 millimeter 1.8 L it's actually
not manufactured anymore and I don't
know why I suspect actually that's
something to do with difficulties in
manufacturing so it's interesting in
that at f-18 it is almost as sharp as it
is at f/8
has very little drop-off towards the
edges which is a pretty magnificent
achievement by the designers so the the
sort of take away from this slide is
that getting significant improvements in
MTF costs really vast amounts of money
and and so this is why typically lenses
our lenses are specified at at 50% MTF
which is probably the largest loss of
MTF in the in the whole system of
getting light off the surface through
the lens onto the sensor there's
actually an MTF associated with the
sensor itself due to crosstalk between
pixels but by far the largest losses in
the lens and the reason for that is that
it's so expensive to do better than 50
or 60 percent yeah
thank you I'll be getting to that in a
moment and they were all conducted
assuming a flat black sensor which
underestimates the amount of flare that
would be in the lens and therefore
overestimates the MTF especially at low
frequencies so it's a very good question
if you know the blower function of the
lens can you reverse it later the usual
name for doing that kind of thing is
deconvolution
and the reason people use the word
deconvolution is because they like to
summarize the blurriness of the lens as
a point spread function that is convolve
with some ideal image difficulty with
that is that the point spread function
changes depending on where in the image
field you are and so it's not a linear
transform deconvolution would be and
would be actually be relatively cheap so
there's sort of two problems with the
convolution one of them is the one I
just said and the other one is that when
you deconvolve see when you deconvolve
you amplify the noise that you have it's
the same as any other signal processing
if you try to reduce inter symbol
interference you end up increasing noise
so you know
to serving
so
right so Chris points out that with a
with a sensor like a CCD you're
essentially doing a sampling in space
and you're going to get aliasing at
higher frequencies than the frequency at
which you sample or half the frequency
at which you sample there's but the
practical difficulty here is that and so
and so Chris's point is that if you
could band limit your your output to be
have no frequencies above the Nyquist
limit then in principle you could
amplify your you could deconvolve your
image and the practical difficulty is
that the the noise process isn't easily
separable from the data acquisition and
so you and so and so you will get noise
even below the Nyquist frequency and
when you amplify to two deconvolve you
will you will amplify that noise even if
you manage to band limit the noise above
the Nyquist frequency yes I I don't have
a graph and I haven't seen a graph my
understanding is that it's quite
adaptable so so that it's it's fairly
poor at the edges and very good in the
middle but of course you keep moving
your eyeball around to get the good part
in whatever it is you're interested in
and this is sort of a fundamental
difference between what cameras have to
do and what the eyeball does right
because it's part of your experience
that you get to move your eyeball and
the camera doesn't get to do that yes
concentrator I'll be roughly the same
yeah so that's basically the last slide
in my talk and now I think you guys have
predicted almost the entire rest of my
talk so if you don't mind I'll just flip
through it and get to all your questions
so here's the point spread function that
we were talking about so this is
actually a shot of forgot the name of
the star but it's a small isolated star
that was shot by the Spitzer Space
Telescope and what we're looking at and
what you can't see too well on this
projector because it has lower dynamic
range than my screen is that you have a
sort of bright central area and then the
rest of the image has also got some
brightness to it but not a lot and so
the the central bright area and you can
see the shape of it it's a bit taller
than it is wide
that's due to the aberrations in the
lens itself and the in the optical train
including the reflector and then the
sort of diffuse and uneven glow all
around it is due to internal reflections
inside the lens itself and and so the
the point spread function has sort of
this this central area which that the
police parade function is the Fourier
transform of the what's actually called
the OTF which is the optical transfer
function
MTF is the amplitude of the OTF there's
also the the phase but essentially the
the central point it's a Fourier
transform so the central point of the
point spread function corresponds to the
high frequency portion of the MTF curve
and the you know broad the broad tail
the broad surround of the of the point
spread function corresponds to the low
frequency portions of the MTF grass and
and that's why it is that lens flare is
responsible for loss of MTF at low
frequencies so ROM Abele talked a little
bit about aberrations last time so I'm
gonna get into flare here so first why
do we have any flare at all it's because
light reflects off of changes in the
index of refraction of the materials
that it's going through this is like any
other wave you know if you have
electrical waves moving through wires
and you have an impedance mismatch you
get reflections off that - exactly the
same thing is happening here and the
reflection is based on the the
difference in the indexes of refraction
blast has an index refraction about 1.5
and air is about 1 and so if you take
the difference you get about 4% and so
when you look in the window of a house
during the daytime and you see that you
can't see into the house very well it's
because you'll get you're getting 4
percent reflection often both surfaces
about 80 percent reflection of the
exterior scene is is coming off that
that glass and and then overlaid on top
of that or superpose I guess is where I
always say it you have the image from
the interior of the house is why it's
difficult to look into houses because
the house would have to be at least 12%
as bright as the exterior in order for
you to even get you know a one-to-one
signal to not noise but interference and
it's very unusual for interiors to be
that bright so in the early part of the
20th century
this reflection off of glass surfaces
made it completely impractical to have
multi-element many element lenses so
they would typically have lenses with
with say six elements
cemented into two groups so there's only
four air glass interface --is and even
that was it examines us like that would
exhibit fairly draconian amounts of
flare and towards the middle of the 20th
century they developed this wonderful
technology of coatings which to a first
approximation almost eliminated the
problem certainly as Jim was pointing
out if you measure the flare of the lens
without having a sensor behind it things
look pretty good so the simplest kind of
coating is if you were to coat the glass
with a
a coating that was one quarter
wavelength fixed so that would be around
like a hundred and twenty nanometers
thick one hundred and forty nanometers
something like that and you were to code
it was something that had an index of
refraction which was the geometric mean
of glass and air then you get these two
reflections and they would be two times
the quarter wave or a half wave apart
and they would be equal in intensity and
so they would cancel and you get nothing
that's gonna vary as you get off
frequency and it's also going to vary as
you change the incident angle to the
glass surface and both of those
complications are responsible for a fair
bit of the Flair that you see in say
eyeglasses which usually have fairly
simple coatings the other problem is
that there really isn't a good material
with an index of refraction at the
geometric mean which is one point three
eight the most commonly used material is
magnesium fluoride which has an index of
refraction of one point two three it's
great material it's nice and hard but it
doesn't get rid of all the reflection
and a single quarter wave coat of
magnesium fluoride leaves you with about
one percent reflection which is a lot
better than four so if you go to a
coating house and you ask them well what
can be done
the answer is lots and lots so these are
two graphs once again you would need
good eyesight I guess to read the labels
so the vertical axis is percentage
reflection and the graphs are going from
zero at the bottom to the top of the
left-hand graph is at four percent and
I've scaled the graph so that they match
that way and the horizontal axis is
essentially 400 750 nanometers it's the
a little bit more than the spectrum of
visible light that you're sensitive to
with your eye and so what's what's
interesting is on the Left we have graph
of a standard three layer coating not
not a not a one layer coating I'm told
these three layer coatings are quarter
half quarter wave coatings of different
index of refraction materials and you
can see that it's a
they get below half a percent of
reflectivity and on the right is a
optimized coating it's four layers thick
and it's been optimized to the index of
refraction of the underlying glass glass
is very an index of refraction I think
from around it's around like one point
four eight to like one point five nine
somewhere in there and so by actually
changing the material stack and the
thicknesses to account for that you can
do even better and on the right we have
a coating with over most of the range
it's reflecting less than 0.1% of the
incident light this is 40 times better
than you'd get with regular glass it's a
huge improvement and so the the net
result is is that coatings like this
made it possible that there are sort of
two things that revolutionize lens
design in the 20th century one was
coatings like this and the other was it
was less of an effect actually was
computers that could run that could run
simulations of rays being diffracted
through the lens and and help the lens
designers try more complicated lenses
yes oh yeah they're a huge thing because
by reducing the amount of reflected
light inside the stack lenses that
you're looking through they're reducing
the amount of flare without these
coatings you would be lucky to get even
20% or 15% MTF at any frequency through
you know any common lens you care to
name for a modern camera
- like the last 15 years or so yeah I
would imagine so what I don't know yes
when
I see so I have read of such things what
they do is they deposit a material with
an index of refraction of let's say 1.2
onto an underlying substrate of index
refraction 1.5 and then they heat the
two up so that the the to diffuse
through each other and you do get a
gradient that I know that's not used for
anti-reflective coatings the
anti-reflective coatings that I know
about rely on cancellation and the thing
that you're talking about would rely on
essentially not making a sharp transfer
so you get very low reflectivity at any
given depth I think the problem with
things like that though is you didn't
have to find some material that was very
close to air and index of refraction and
I think that's the sticky point yes well
actually we are right so there is a
problem with the the best lenses in use
today are used in semiconductor
manufacturing and the problem that they
have with the 45 nanometer production
node is that they can no longer get
there
193 nanometer laser light to focus down
to produce structures that are 45
nanometers in size and the problem is
the air and so they're getting rid of
the air and they're replacing it with
other stuff that has higher index of
refraction and therefore actually gets
the 193 nanometer light to become
effectively shorter wavelength inside
that that material the liquid and
therefore you get you're better able to
image smaller structures
so this is the coding part of the
problem so so as Jim was pointing out
the big problem in all this is the
sensor the sensor is vastly more
reflective than the glass surfaces and
my understanding is that CCD reflectors
are differently reflective than film
they may be somewhat they're about the
same reflectivity I think overall but
the problem is is that C CDs are more
specular than film which was more
diffusive and as a result film would
tend to reflect more of the incoming
light into the baffles or whatever it is
at the edge of the OP of the sensor cell
that absorbs light and see CDs tend to
reflect it right back into the lense
stack yeah Lance discussion that's why
it is it's the next slide okay so it's
not or it is only in some cases and
that's the bottom of the slide so so
here's the problem with the sense of
reflectivity the index of refraction of
silicon dioxide is about 1.5 and for
silicon it's about 3 and so the
reflection off this interface is 11% now
for most of the sensors that anyone here
could afford you are the collection
surface through which the light enters
the sensor is the active surface of the
chip so you have your fabricating
transistors on the same surface the
semiconductor people are completely
unwilling to allow you to do anything to
that surface at all there's no there's
gonna be no magnesium fluoride probably
in that part of the fab it's it's out
it's not a balance so there is no one
that I know of seriously proposing to do
any kind of coatings at all on that part
of the sensor that said there's
something called a backside thinned CCD
and the way this works is that you you
grind the chin but the chip very thin so
on the order of like 15 microns thick
and then you
you have to do different doping profiles
but essentially you you use the backside
of the sensor to collect the photons not
the front side and this has a number of
wonderful advantages one of which is you
can coat the backside so you can make it
any reflective another wonderful
property that it has is that since you
don't have any wires on the backside you
can use nearly the whole area of the
chip as light gathering area and you
avoid many of the problems that front
side sensors have the biggest problem
that front side sensors have this
especially shows up in more modern
sensors with smaller pixels is that as
you scale the pixel down the the thing
that scales down best is the photodiode
which is the part that's getting the
light the stuff that scales down less
well is the wires that are taking the
information out of the pixel and getting
it to the periphery of the chip where
you can process it and so what tends to
happen is that your fill factor the
percentage of the front of the chip that
actually manages to absorb light is
going down as you make your sensors
smaller and this is only making the
problem of having fewer electrons worse
so this is one of the reasons why when
you look at a cell phone picture or
something like that you see really
astonishing amounts of noise it's
because the sensors essentially count
individual photons coming in to the
influence they do even in reasonable
amounts of light they do so so the
reflection off the silicon to silicon
dioxide interface is about 11% the Bayer
filters on most of the sensors on color
sensors are absorbing quite a bit of
light coming down to the sensor and then
of course they absorbs a bit more on the
way back up again and so they reduce
that 11% a bit but on the other hand you
have usually indium tin oxide and
aluminum wires on the surface of the the
pixel and those are quite bright and so
those reflect a bit more so the overall
reflectivity ends up being somewhere in
the range of 10% and this varies by a
fair bit from one manufacturer to
another so and like I said no no way our
coatings on the act
surface the backside in CCDs are used
primarily I know of very expensive
security cameras and in astronomy you
know telescopes so gets worse so not
only do we have a sensor back there but
we usually have an IR cut filter so the
issue here is that the the dyes that the
little these little spots have died that
sit in front of the of the the pixels
that color them red green and blue don't
absorb infrared or ultraviolet light
ultraviolet light is a little bit of a
problem
generally the glass in the lens stack
itself is absorbing some of that for you
a much bigger problem is infrared for
instance I believe it was like as m8
that just came out they I don't know
exactly what happened but the infrared
cut filter had the wrong cutoff
frequency and as a result wedding
photographers started noticing that
women wearing black melded which is a
synthetic fabric tends to be fairly
bright and infrared and these poor women
look like they're wearing purple velvet
instead of black velvet and so Leica has
now issued some sort of program that's
it's you know one of these typical
corporate responses to a problem like
this they will give you two infrared cut
filter that you can screw on to the
front of your lens but not more than
that and there's all kinds of it's a lot
of accounting headache for making an
engineering mistake and presumably they
fix the IR cut filter in their cameras
you're selling now so the the problem
with these are cut filters is that
there's number of different ways of
building them but a one good way is to
use stacks of material like those
broadband and any reflective coatings
that I showed earlier but instead of
using for materials stacked you these
typically use like 25 to 30 and
sometimes they might have both sides of
this what's called a Plano Plano element
very thin piece of glass usually so
they're fairly complicated coatings and
what they're trying to do is
instead of be they're trying to be
extremely reflective in the infrared and
then be less reflective in the
indivisible what wavelengths whereas an
any reflective coating is trying to be
not reflective in the visible
wavelengths so the problem what ends up
happening is that you can't get an
infrared cut shelter it was what's
called the dichroic infrared cut filter
that's the the stacks of materials
coated on you can't get one that has
anything like you know 1% or less
reflection in the visible band what you
can get is maybe 2% or more likely 4%
and that's fairly reflective and so you
combine this with the sensor at the back
and you have this really great path for
sending light in and then sending
another image of it back onto the sensor
I think that's the next slide it is so
the question is where do you put this IR
cut filter so if you're making an SLR
digital SLR and you want to be
compatible with a line of lenses going
back 30 years or more which is what many
digital SLR manufacturers that's there
in you can't assume that the lens has
the IR cut filter in it you could
propose that somebody put an IR cut
filter in the front of the lens and the
problem with that is that these these
coated filters change their cutoff
frequency depending on the angle of the
light going through them and so for a
wide-angle lens that's not a very good
strategy because the the angle of the
light coming through the front element
of the lens will change quite rapidly as
you go from one side one side of the
field to the other yes Lance
what's the downside
so you well so the question is why not
put gold or some other dopant in the
glass to cause it to absorb the infrared
the answer is I don't really know the
there are glasses that absorb infrared
and sometimes infrared cut filter czar
made out of absorbing glasses the glass
that tends to absorb infrared has the
problem that it's not very good
optically
and so having thick sections of it tends
to introduce a lot of aberration sort of
nasty aberrations that you can't really
grind out or come up with a surface for
so typically glasses like that are
handled in very thin plano plano
sections and that's it yes yeah actually
it's a really good solution for a
telephoto lens to put an IR cut filter
in the front because because the angle
isn't changing very much
it keeps the advantage to it is that it
keeps the it keeps the IR cut filter
that keeps this reflective surface far
away from the sensor and as a result
which i think is coming up in a moment
the total amount of light reflected back
to the sensor is lower which is what you
want so there's so you could put the IR
cut filter in the frontal ends good
solution for a telephoto you can put it
between the lens in the sensor which is
what you typically do if you're building
a digital SLR because you build it once
you put it in there you're done and the
problem with that is you get this strong
reflection and so you have to work hard
on getting a good coating to get that
down to two percent and then you need to
also work hard on your software stack
that estimates how much flare there is
and where it is and subtracts it and
then the final thing you can do is you
can do something somewhat like what what
Lance was suggesting you can but rather
than doping the internal lens elements
you can coat them and oftentimes you can
find lens elements over which there is
much sweep of the chi phrase going
through the lens relative to the to the
surface that they're going through those
are good surfaces to coat the the
trouble here is that coating these
glasses can be difficult and there are
yield losses associated with that and so
it drives up the cost of making the lens
but it has been done and I actually
think that most consumer point-and-shoot
cameras actually use this technique to
reduce flare many also have an IR cut
filter in the back there's a lot of
variation in that space so I want to
fairly briefly go over why the geometry
of the back of the lens how the geometry
to the back of the lens affects flare so
normally light passes from the left hand
side of the diagram that you're looking
at to the right so I flipped this around
so my light is passing from the right to
the left here I've got a mirror here
which is actually going to be the
surface of one of my lenses I've got
this object on the right hand side which
is actually going to be the sensor
itself and I got an image on the left
hand side here which is the image of the
sensor in the reflection made by the
surface of one of the lens elements so
if you imagine that that lens surface is
convex then you get this itty-bitty
image of the sensor and as a result when
you have light from the center emitting
back towards that the front of the lens
a small portion of that light will
actually hit the secondary virtual image
of the sensor and the rest of it will be
dumped into the baffles in the in the
sensor cell this is a good thing so you
know that the rule here is that convex
elements pointed towards the sensor in
the back don't reflect so much light
back to the sensor on the other hand if
you have a concave mirror you have the
opposite effect you magnify the image of
the sensor and as a result the amount of
light that's reflected
to the sensor is primarily a function of
the the exit pupil of the of the sorry
the aperture of that of that concave
element and not so much a function of
its distance away from the sensor the
upshot is that these concave elements
reflect about five times as much light
that's a very rough approximation it
depends on the exact geometry but it's
about five times worse than then concave
elements and or concave surfaces and so
this is one reason why when you look at
the back of high-quality lenses you'll
see that there's a heck of a lot more
convex surfaces than concave putting
back towards the sensor so so let me
wrap up here so so far the talk has been
about contrast and why you lose it and
and now I want to motivate why you would
actually want contrast so I think I'm
not sure if I judge this right so the
idea is is that you can read most of
these numbers but not all of them
basically you have 20 you know it's 80
60 40 and 20% contrast so so 20%
contrast seems like you can you can at
least see lines okay so why do you need
more than that you know why would we
accept lenses with 20% MTF and the
answer is noise the be the the
interesting you know in many signal
processing systems the the merit figure
of Merit that you use is signal to noise
and in in photography the signal is
generally contrast and the noise is
photon shot noise I'll cover that in a
second
so as an example of you know well how
much I want to motivate how much
contrast you need to be able to see
through noise so I've got a number of
I've got some text here and what I've
tried to show is that if the text is
large like on the right then so we have
equal noise across the entire
image here what we have less contrast on
the lower lines that lies towards the
bottom of the slide we have higher
contrast in the line towards the top and
the text is smaller on the left so the
idea is is that on the right or all the
way across the top actually you can read
all that text because you have high
contrast and so the contrast is better
than the significantly larger than the
noise and so you can pick out the shape
as we go down you have the bottom line
has contrast which is approximately the
same as the RMS noise and you can still
read on the right-hand side because you
have a very low-frequency shape that
you're trying to recover as you try to
pick up higher and higher frequencies
you need better contrast you need better
signal-to-noise at higher frequencies to
to recover the signal so what this means
is that if you're shooting small text
like some companies I know the the sort
of root-mean-square contrast that you
need from your image has to exceed the
root-mean-square noise by about a factor
of four for a sort of visually
acceptable result now you know so what I
want to do now is is back reference that
to the noise process that you actually
see we have black text on a white
background we might have about 80
percent contrast in the scene that's a
90 percent reflective white and a ten
percent reflective black if the lens MTF
is about fifty percent that do whatever
frequency corresponds to this small text
then then what we're going to end up
with on the sensor is going to be about
you know forty percent of the full white
response in in contrast the required
signal a noise ratio was four to one and
so that means that one-fourth of that
forty percent or ten percent of the
white we have to have noise the SMR
noise sorry the rms noise has to be
around ten percent or less of the full
range white response now there's a
reason why I've set it that way and
that's because and this is my last slide
the dominant noise source in
modern cameras is photon shot noise the
I think dick is going to talk about this
in a later lecture but in summary the
readout noise on modern sensors is so
low that it for for pictures of
acceptable image quality with low enough
noise you're not really looking at
readout noise in a modern
point-and-shoot camera or a pretty good
SLR digital SLR instead what you're
looking at is the photon shot noise and
essentially what's happening here is the
photons come in some fraction of them
make electrons in the in the photo
diodes that get collected and there's a
I think it's a Poisson process
associated with collecting those
electrons and you get noise which is the
square root of the number of electrons
so what that means is that if you need a
noise to white level ratio greater than
if you need a white level to noise ratio
greater than 10 you're going to need to
collect at least a hundred electrons per
per well to get that to get that level
and that sets the low light limit of the
camera it also because this gets back to
forgotten which person asked the
question but essentially as we get newer
newer sensors the sensors have smaller
pixels because that makes them cheaper
to produce but smaller pixels behind the
same lens capture less light you have
less electrons you have a smaller
signal-to-noise ratio and so the overall
image looks worse and and this is
essentially the limit on camera
resolution and it's an interesting limit
because you're we're almost up against
physics the readout noise levels for
instance in a Canon 5d or for that
matter a any one of the Canon point
shoots the I'd make a pixel point shoots
they're all about three three electrons
of readout noise and you can see that
you need 100 electrons to to cover the
just the signal noise for through this
process so the three electrons of
readout noise is not significant
and so essentially they could make it
zero and it wouldn't improve things very
much and so what's left is to get more
electrons and you're either going to
need more glass out front with bigger
holes in it to get the light through or
you're going to need a higher efficiency
sensor and since your quantum
efficiencies are already in that forty
percent range so there's you know a
factor of two and a half to get there
and then you're done
yes no not anymore there used to be and
CMOS got a lot better and so recently
actually the CMOS sensors are very
slightly ahead but I would imagine that
doesn't have anything to do with
fundamentally CMOS versus CCD and I
suspect it has more to do with the
amount of money spent on trying to make
CMOS sensors work versus CCD so so this
sort of motivates the the talk to come
on on sensor noise and and how those
statistics work are there any questions
yes
I've seen the city image and the
negativity exactly that's right so what
you're what you're looking at is the
reflection of the let's see you let's
say you don't follow the directions and
you point the sensor into the Sun or you
point the camera into the Sun you're
going to get a picture of the Sun on one
part of your image that image of the Sun
is going to reflect off the sensor back
off the infrared cut filter and back
onto the sensor again and it won't quite
be in focus it's going to be somewhat
out of focus
but you'll be able to see it another
thing that will happen is it will
reflect up through the stack of lenses
and reflect back down again and usually
it'll be cut off by the aperture stop
and you can tell these that this is all
called structural flare and if you if
you look at structural flare you can see
that some of them are kind of round and
the images of the Sun and some of them
are usually like you know nine sided
polyhedra and what you're looking at is
an image of the open shutter or the or
the aperture stop on the lens from
reflections off of surfaces in front of
the aperture stop
this is at night here chilling
to the
over here the second team is requested
me
if you repeat it probably economy
degrees you get an exact image on the
other side of train yeah this is so I
don't have experience with that exact
effect so I yeah I would imagine it's
probably very lens dependent so I'm not
sure I have a good explanation for you
that could be because your more
expensive lenses generally have more
surfaces to reflect off of are there
other questions no all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>