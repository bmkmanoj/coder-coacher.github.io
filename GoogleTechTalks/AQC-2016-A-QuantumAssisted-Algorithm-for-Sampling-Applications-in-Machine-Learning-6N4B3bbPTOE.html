<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC 2016 - A Quantum-Assisted Algorithm for Sampling Applications in Machine Learning | Coder Coacher - Coaching Coders</title><meta content="AQC 2016 - A Quantum-Assisted Algorithm for Sampling Applications in Machine Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AQC 2016 - A Quantum-Assisted Algorithm for Sampling Applications in Machine Learning</b></h2><h5 class="post__date">2016-11-21</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6N4B3bbPTOE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">next week Alejandra is gonna tell us
about more about machine learning on
quantum devices okay thank you so it's a
pleasure to be here thanks to organizes
for letting me talk about this work that
I guess we we decided to experiment
about a year and a half ago we really
wanted to go into this to see what are
the possibilities for quantum annealing
for machine learning so this work up the
leading author in this work is March 11
idati he's now starting his PhD at UCL
so he came to NASA to start his master's
degree and it's also John Realty who is
also one of the our research scientists
and lupa Biswas
so let me get going and let me just give
you a contrast of where this work
actually fits and the very first thing I
mean you're used to this type of
formalism I mean the way I like to think
of any quantum annealer or the d-wave is
you can work in two modes and they're
complementary so the very first mode is
when I think of the D way of any quantum
annealer working as an optimizer so in
that paradigm basically is the one that
we've been working for about 10 years
trying to solve real world applications
on the device and basically what you're
provide to the device is the HS and the
jl j s and what you're expecting from
the device is just to give you the
lowest energy configuration basically
you're expecting a bit strain that
hopefully minimizes the energy that's
basically what we've been using for full
diagnosis have you seen lattice protein
folding any type of mass application
that we are interested in the second
paradigm is something that is a little
bit different and it's exactly when you
are thinking of the device as a sampler
and device that you're going to use for
sampling applications so how is that
difference so basically with the way to
think about this is now you're thinking
that the device instead of just solving
a combinatorial mutation problem is
actually skippable giving you samples
reliably sampling from a Boltzmann
distribution that's precisely the talk
right before mine and here is the energy
cost function and as you can see these
two paradigms are not in disagreement of
course I mean there's this there's time
or the the assignment with the minimal
energy of course hopefully is gonna have
the largest weight so basically there
was an early work in in 2010 from D way
from the D way focus actually pointing
that in principle you could use the
device to do this but there was a little
caveat and it was actually is they
realized that the device was indeed more
or less consistently sampling from a
Boltzmann distribution that's precisely
I mean and a physical understanding is
partly I mean the word evgeniy presented
in my previous talk so basically there
is a freezeout that actually you get to
the classical point and you can sample
from the classical Boltzmann
distribution but the problem is this
affected temperature you really don't
know what is the effective temperature
so if you're programming the device and
you want to extract samples you input
the HS and J's and you want to start
samples the question is there is no way
you can know a priori what is the
effective temperature why because this
effective temperature is not related to
the to the temperature of the device
it's it's related somehow but it's not
the temperature of the device so
basically what that implies is what they
show in that paper is that you basically
need for each value of H for every
instance you need to fit this parameter
I'm fitting this parameter actually is
is again it's intractable and therefore
you go in a circular argument that
basically to solve the problem you need
to you need to solve an order
intractable problem so that's basically
that's what our work is about so of
course I mean if you were able to do
that the potential is that you can
explore models as the one as Evan II was
talking about here is for example a
representation of a restricted Boltzmann
machine that is just a probabilistic
graphical models this is the model we
are going to focus here in this talk and
basically when you have visible unit for
instance you can think as this unit they
could be binary variables for instance
imagine that you have a picture that is
actually with binary with binary
variables it could be a black and white
type of picture basically those are the
ones where you're gonna coat black white
black white those are the visible units
so those are the bioballs that are gonna
represent your data in this
probabilistic graphical model the top
the top layer is called the hidden units
they are basically just trying to
capture correlations between the visible
units so why is this called
stricted Boltzmann machine not is the
architecture here there is no
connections in principle you could put
actually connections here but there is
no links but there are no links between
the visible units and there are no links
between the hidden units this is
entirely an algorithmic construction in
such a way that you can train this
algorithm very efficiently is not
actually of course I mean if if you had
the opportunity to connect all to all
that would be great but computer
scientists realized that if you do this
you can move much faster than you can
actually make progress with contrasted
averages that is the algorithm that they
use state of the art so it just you can
think of this construction as an
algorithmic purely algorithmic a
motivator so there is still a caveat
even if you do this construction and is
that there is a computational bottleneck
that is actually that's why we're even
in first place talking about why to use
a quantum annealer is the computational
bottleneck is that when you are doing
this training of this model doing this
stochastic gradient as sent basically
you need to calculate this type of
correlations V here stands for any of
these visible variables H stands for any
of these other spins so basically you
need to calculate these correlations
with the notice that here this average
is calculated over the full joint
probability distribution that's the
difficult part because in reality if you
want to get samples or calculate
averages over this model that's again
np-hard so it seems like there is no way
to make progress but but actually
computer scientists came up with a good
technique to do this now of course
basically our proposal is that you could
replace if you had a way to get samples
from the joint probability distribution
from the Boltzmann distribution of the
full Ising model in principle you can
calculate and compute these averages and
then do the training hopefully more
reliably than the state-of-the-art
classical approach that's the promise
and basically the very first thing that
we tried in out in our in our very first
work here is we really wanted to
understand this feature of how do we
efficiently compute this effective
temperature so I think there are two
main contributions in that work it's
been in the relations has been accepted
as we published in PRI soon basically I
think in that country
we provide an algorithm that robustly
estimate effective temperature and by
here robustly what I mean is actually
it's not that it's never going to fail
it's actually is that what it means is
that is the best that we can do compared
to some other simple approaches that we
tried in the last year before actually
we came up with the final the final
version of the algorithm that's the most
robust one that we came up with and
basically the nice the very nice thing
about this approach so this is the
take-home message is the actually you
don't need to do any extra work the same
samples that you are extracting from the
Boltzmann distribution to calculate the
averages are the same samples that
you're using to estimate the effective
temperature so actually you get
everything for free you sample from the
device you get the samples you compute
the temperature and you can move on in
the next step so everything is actually
you recycle absolutely all the samples
so as actually just wanted to highlight
there was a recent word from from D we
actually appear a couple of months ago a
month ago actually within the last month
just actually emphasizing an importance
when this affected at estimation of the
temperature for this type of problems
for sampling applications and then they
were actually concentrating in in this
question I think someone asked in the
audience what happens actually if you
have a hard landscape when you get
trapping in a local in a local minimum
will you thermalize all these questions
I think they were addressed a little bit
more in detail in this type of in this
work so that's basically that's the
setting of our approach and basically
what I'm going to show you now is some
of the results so let me go first I give
you in one slide what the algorithm is
about so the algorithm actually is very
simple from the many things that we
tried actually this is the one that
works the best is you have here a
Boltzmann distribution here it is just
the degeneracy the density of states at
a certain energy and here is your bald
man's fact what are the we are only
limited to the things that we can do
with the device so basically what we
have is the we can rescale for instance
symphysis infamy interested in sampling
from the Boltzmann distribution at a
certain reference parameters H s and J I
am allowed to rescale those parameters
and come up with a new Postman
distribution if I rescale those
parameters like here just multiplying by
X basically what I'm doing is
effectively changing the template
I can think as I have the same problem
but now with a different temperature
it's just basically I'm just realized in
this Boltzmann factors here
experimentally what you need to do is
basically just to compute this log of
this ratio and basically all that you
need to do is experimentally get what is
the probability of measuring let's say
you step in one a day in an energy that
appear in both histograms and then
basically you compute the probability
from one distribution and the
probability of your distribution so this
probability of beta a one probability of
beta one in beta Prime and then you do
the due to the analog for another for
another different energy so this energy
is going to be a variable basically you
can pick any one and it to and that's
basically what you have here so this is
the Delta so the nice observation of
this construction when you do this math
is basically is that this quantity Delta
L experimentally as a function of Delta
should be a linear trend if it is a
Boltzmann type of distribution I'm
basically the Delta beta is simply given
I know X because I know by how much I
rescale the parameters so I know
absolutely everything and basically from
the linear regression I can compute the
value of the affected temperature so
here is the algorithm and here is
actually some experimental data how it
looks so actually you see when you
sample from the d-wave let's say you get
distribution number one distribution
number two you can come you can you can
track values of e1 and e2 basically when
you have experimental data and of course
I mean the distribution is quite noisy
but in principle that's why that's a
typical plot that you get from in any of
these training algorithms and basically
from the linear regression you can
extract the temperature one thing
actually to keep in mind is notice
actually the disaffected temperatures
are usually much hotter than than than
than were you whether the device
temperature the device temperature is
zero point zero 33 in this particular
example is about three times the device
temperature and this is consistent with
the quantum dynamics with the open
system dynamics you are expected to have
a much smaller gap and therefore a
smaller temperature at the freezer point
so basically this is all consistent and
this is what we observed here is
actually the full-blown experimental
data so what we did is we trained
restrictive Boltzmann machine for a
particular data set that is used in
computer science very to do benchmarks
is called the bars and stripes and
basically you want to train a
probabilistic graphical model that is
capable to of identifying or capable of
generating bars and stripes so you just
feed the algorithm barson stripes and
you hope that the algorithm can generate
parson's types so that's basically what
you're doing and then basically here is
the way so what I want you to so
basically the y-axis in this plot is the
gold standard is how is your metric is
the log-likelihood so they larger this
value the parity training and what you
see here is as you move along in the
training iteration so this is basically
steps in this contrasted divergence
update so as I mentioned constructed
that constructed divergence is actually
an approach that you can do it
approximately actually the most standard
one is you'd only do one give sampling
so it's contrasting the righteous ones
but you can do it constructed averaging
100 nobody does this because actually
it's close to the exact result and that
would be extremely expensive but here
our purpose is not to show that we can
beat the classical computer our
approaches that we can we wanted to show
that our method actually is getting Airy
wearing clothes that the effect it that
you need to determine the factory
temperature in order to get close to the
exact result that's basically our main
main point so the dark green line here
is basically the exact result of the
training and they one day you see here
that the one that is the closest to that
one is our approach if you estimate the
affected temperatures at every single
step in the iterations the other the
other approaches let's say for instantly
the orange the the red or the those are
approaches or even this light like
magenta those ones are actually at fixed
temperature this is an assumption that
has been taken in the community and as I
mentioned between 2010 and 2015 there
was no report experimental reports on
this I think mainly because if you see
actually the performance is very poorly
if you make any of these assumptions and
this is understandable I mean you have
an exponential factor where the
temperature if you are missing by a
little bit a temperature this is
actually is going to have critical
effect in your training so basically the
here is just showing that actually our
effective temperature this red line is
just to show
I'm determining the temperature at every
single step in iterations even if I give
you the whole train of the 5,000
temperatures that is limited and you
calculate the average or that curve you
still don't reach actually the best
performance that is if you estimate the
temperature and you use it at every
single iteration so that's basically the
main result by the way if I use the
device temperature as a naive assumption
they actually instead of learning you
actually do learning is out of the scale
so in principle in this is a naive
assumption shouldn't you do that so here
is actually an old plot just a zoom in
one of these regions when actually when
you are doing the training here is
actually how widely this temperature is
varying as a function of the steps and
then basically all what I'm showing here
is notice that the fluctuations are
easily of the order of three times the
device temperature and the temperature
is self average is of the order of three
times the device temperature so here is
actually why this needs to be determined
very carefully if you really want to
observe any good performance when you're
using this type of approaches for
training using the for instance with
devices or with any physical and
Hiller's so this is actually an
interesting result because I think this
is the very first demonstration of this
paradigm of using hybrid quantum
classical approach so basically what we
are showing here is that you can do cd1
that is very quick and very is cheap you
can do see the one up to a certain point
until you plateau and then you say okay
I'm done classically that's that's the
best that I can get notice that when
you're doing the training classically
the classical computers didn't know
about temperature they just know about
weight and biases they don't really have
a sense of temperature everything is
encoded in the weight and the biases so
basically if you want to put this in a
physical device you need to translate
that to HSN J's and the only way to
translate that is by using a temperature
if you don't know the temperature you
cannot restart from a certain point what
I'm showing here is that again if you
use our approach for actually restart
from their estimating the temperature
and restart actually you can still see
how to perform the cd1 so I actually get
better results by doing this hybrid
quantum classical and again I'm showing
here that if you use the naive
assumption of just using the device
temperature actually the learning
significantly drop so actually the
to emphasize and just put in this green
line just to emphasize that if you don't
know the temperature you cannot restart
and that's the only way to match
classical and quantum processors so I'm
gonna skip this because I think I'm out
of time most out of time but I just
wanted to actually just give you this
and neuro is completely new results and
basically what you're seeing here is
again now we're in physician and
supervised learning so basically we're
trying to this is the most difficult
part a time of machine learning so it's
much more difficult than supervised
learning here is an example where we
took some digits one of their rows is
created by a human the other one is
created by the machine so this is a
Turing test for you guys so how many of
you think actually the top row was
generated by the quantum machine let's
look at it just raise your hand okay
got only one how many of these you think
actually was killed by the human they
look human life
well actually human life here I mean is
there were digits full digits but
they're coarse-grained just to make to
make our experiments but in principle
they pota me one other is human like so
and it takes so basically here's the
results actually the one on the top is
the one created by the human and the one
at the bottom is created by the quantum
machine so basically here's an example
number ones number two number three
number four they are directly taking
from human digits human but just
coarse-grained but basically this is the
type these samples were generated
absolutely from a model that was trained
with our approach so what not with our
approach sorry actually these are
completely new different results that
actually the nice thing is also is with
this new theory that we developed for
this particular model in principle you
don't need to use that much a typical
effective temperature you only use it at
the end so that's a nice thing actually
about these new results and with that
actually I'd like to conclude basically
the last resource and that's one Hamill
ID here this is actually these are and
published results the nice thing is that
one of the most the heart is difficult
as we've seen in the conference is
coping with the noise and actually if as
I mentioned when you're trying to train
about machine you are working in the
dual domain basically you're not trying
to get sample you are really trying to
assess what are the h's and DJ is the
best describe you that they deliver over
to the custom it is the HSN J's is not
the
not the bitstream so basically if you
don't have control with the noise in the
program or parameters therefore you are
in trouble so basically the nice thing
about those results that I showed in the
previous slide is that actually
everything is worth more or less in a
black box type of approach where
actually the device is used as a whole
actually everything is you add the noise
and everything is taking is built into
account actually to get those results so
that's the only thing actually you can
get this exciting progress there and the
nice thing also that I didn't mention in
that result is that actually the results
obtained in the previous actually go
much much beyond the restrictive
Boltzmann machine actually it's a it's a
model with a lot of connection so
actually it is a very interesting I
think approach actually you see that
easily you get in the training like
easily after 100 iterations instead of
Python so you get a very boost boost the
result of course I mean we're moving
toward deep learning type of
architectures some of the questions that
are interesting is how detrimental
someone was asking how detrimental is if
you're not sampling from Boltzmann as
you saw actually our indirect approach
was to do the training and as you see
actually was quite robust you still
managed to make progress so I think it
will depend of course from landscape to
landscape and I guess you this would be
dependent on the problem size and some
of the approaches I mean that we are
taking as well as we're trying to
address this question I mean this was
work by Mohammed and by F Denis about
quantum BOTS machine what can you do
what is the power of quantum annealing
when you're actually working with a full
quantum distribution that's something
that we think is very viable and
important for the next research
directions and another thing that as I
mentioned we're just trying to
understand at the moment how can we even
work with the device I mean actually how
can we make progress in seeing some
significant training and how do we
actually think out of the box with these
new quantum annealer now I think they
the research direction I think that
we're pushing is actually scan we answer
this question we have the analog
question in quantum speed-up doesn't
work that we're doing with hard mode
with helmet with everybody I mean I
think this is a very important quantum
speed-up we have the same type of
question in point in machine learning is
can we actually is quantum annealer a
computational resource in this type of
problems can it be and actually can we
show some point to supremacy or can we
show actually some significantly
boosting of over of a speed-up just by
showing for is am playing with highly
connected models or plane
with with they actually using the noise
in the device and just finally I wanted
to advertise we will have a quant we're
hosting a quantum annealing section in
the in the nest a GU conference in San
Francisco so there will be an afternoon
section so please I mean contact me if
you have any questions and this is
mostly for the application type of
related software packages and also
applications here so that's it thank you
very much a couple of questions so I am
under the impression that you need a
sufficient overlap between histograms -
yes temperature how do you ensure that
like these histograms are going to
overlap or what to choose as the scaling
factor and yeah basically so in the
paper itself actually we have a
principled approach actually will show
exactly how X how the scaling should
vary as a function of the the training
actually so this is going to depend on
the in the face of the training that
you're doing I will have a principled
approach basically we keep the two
distributions constant the KL divergence
instead of doing an X constant
throughout in the learning you need to
take into account that the distributions
are changing so that's one thing in the
paper you can find the details but in
principle just just saw so how do you do
it in the first place you need to
experiment with that at the beginning
because there is a still a hyper
parameter that is the distance between
the two distributions and that you need
to experiment a little bit so it's a
hyper parameter in the model yeah so you
just need to play a little bit with it
we have maybe time for one more quick
question if there is one
Hey so your variation over time in the
effective temperature so a from any
given set of samples there's of course
some uncertainty in the estimation and
and also I suppose the device
temperature could be drifting so do you
have any comment on those yeah so yes I
think it's a good question so you can
see actually in the plot itself that
actually I think it is really hard I
mean from what I talked to Trevor in the
past it's really hard that the device
temperature is fluctuating three or this
three times actually that the
temperature itself and the earthing is
that you see error bars here actually so
this is the same training done many
times and actually this is the error
bars that you see and actually the Green
Line is one of the realizations but in
principle this experiment was doing at
least five times and that's basically
the spread from the same problem so
actually this is just just to show the
robustness and that this the strain is
not just one of the fluctuations in the
in the estimation because estimation
could be very noisy as well so this is
just to show that from realization to
realization of the model this is
basically they're robots that you would
get and it still it doesn't cover the
whole spectrum of there are the
fluctuations itself okay with thing we
should probably leave any other
questions till after the we finish this
session so let's let's think again
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>