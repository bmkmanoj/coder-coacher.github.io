<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: The Importance of Automated Testing on Real and Virtual Mobile Devices | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: The Importance of Automated Testing on Real and Virtual Mobile Devices - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: The Importance of Automated Testing on Real and Virtual Mobile Devices</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nOFel7yNAsE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Our next speaker is Jay, Jay Srinivasan.
And Jay is the product manager of real device
infrastructure at Google.
In his past life, he is the founder and CEO
of Appurify.
Some of you may have heard about the name.
Jay is going to talk to us about some of the
challenges of optimizing performance and quality
for mobile apps today.
&amp;gt;&amp;gt;Jay Srinavasan: Hi, everyone.
Thanks for having me.
So, yeah, hi, I'm Jay Srinivasan, at Google
now.
And Appurify, our company, was acquired by
Google about four months ago.
And before that, for the last two and a half
years or so, we've been working on automation
of mobile apps on real devices for quality.
My cofounder Manish Lachwani was supposed
to be on stage with me, but he couldn't join.
My third cofounder, Pratyus, is sitting in
the audience.
We're very excited to be at Google and appreciate
the opportunity to talk here.
I'm basically going to be using slides that
I've been using for the last two, two and
a half years as I was selling Appurify to
other folks.
And the sort of interesting thing here is,
this slide I have not had to change in two
and a half years.
Two and a half years ago, I've been saying
mobile application development is broken.
It's still broken; right?
And it's getting better, especially folks
in this audience are incredibly aware of that.
But a few things that -- so at Appurify, we
probably worked with I'd say hundreds and
hundreds of mobile developers in one form
or the other.
And almost consistently, a few things we saw.
There was very, very limited use of continuous
integration tools in mobile.
It's getting better now.
But it was -- it's been -- it's better now,
but it's still not even close to what you
see in the Web world.
So, for example, some of the top ten apps
in the iOS App Store today are still being
built on one of the developers' Mac; right?
It's -- there's just a lot of -- a lot of
distance to go in terms of the use of continuous
integration tools.
And forget about continuous integration -- if
you're not even going to be able to get to
automated testing if you don't think about
continuous integration.
That's sort of the first thing that we see
significantly lacking.
The second thing is that we find that for
the most part, most mobile tools right now
are poor ports of Web tools or PC tools.
And our premise is, look, you need to build
tools for mobile quality that were built mobile
first, just like you have companies that are
mobile first.
The tools that you use against these companies
need to be mobile first.
What that results in is, essentially, inefficient
launches.
We see this all the time.
You're in a rush to actually ship your product.
As a result, you're focused just on functionality.
You're focused on UI.
You're focused on UX.
And then you ship this mobile app, and then
you see all these stability issues, network
issues, laggy client performance, et cetera
in the field.
And then, by far, and certainly, again, I
want to reiterate, not this audience, because
you guys are obviously very different in this.
But what we're seeing by and large is users
rely heavily -- or developers rely heavily
on users to actually act as their debugging
layer, their test layer.
So, basically, you ship a product, you get
all these issues in the field, and then in
your next update or the update after that,
you fix it.
What this results in is poor ratings and lost
revenues.
I actually think personally the act of shipping
a solid mobile product is actually more important
than shipping a solid Web product just because
it's so much harder to acquire users and you
spend so much more to acquire users in the
mobile space.
So this is an example that I know of.
Two games were launched relatively close to
each other.
The graph that you see here is essentially
daily active users over time.
And the interesting thing is, if you look
at the beginning, at day 0, you see this big
spike.
This is classic for mobile.
You, basically, have a launch budget.
You spend all of this money getting install.
So both of these apps had very similar launch
budgets.
You get all these users.
The difference is, obviously, over time, app
one just started degrading and app two was
stable.
And the interesting thing here is, the genre
of the apps, et cetera, was very similar.
The main difference between these two apps
was actually the amount of time spent on QA
before the product was actually launched.
What you see at the bottom is essentially
sort of the, like, vicious cycle that you
get stuck in; right?
So if you're not focused on stability, performance,
and quality, two things are going to happen.
Your existing users aren't going to come back,
so you will lose retention.
They're going to leave bad reviews, so your
ratings drop.
When your ratings drop, your installs drop.
When your retention drops and your installs
drop, your DAUs drop.
And if both of those drop, your revenues drop;
right?
So it's a vicious cycle that goes on.
So we've worked with a lot of mobile developers.
And I think none of these bullets are going
to be surprising to anybody in this room.
Essentially, our premise is to launch better
apps, you just need to do these four things.
You need to test and optimize our real devices
and emulators.
I call out emulators versus simulators.
I think you need to test on real devices and
emulators.
The second thing is, real devices or emulators
are only 50% of the battle.
Mobile, it's equally important to think about
user conditions.
What network is the device on?
Are -- what are the other inputs that the
device is going to take, location, gyroscope,
et cetera.
That's the second thing.
Third thing, you're obviously not going to
be able to test manually or spend too much
time or hire too many people.
You have to think how to automate.
Finally, back to sort of reiterating point
two, don't just focus on functionality.
Look at stability, look at network performance,
look at lag as well.
So if I go into each of these in a bit more
detail, so this is a picture of the Appurify
rack right now in -- at Google.
And I'll tell you a bit more about what we're
actually planning to do at Google a few slides
later.
But the simple premise is this, your users
are on real devices.
You need to be testing on real devices.
Your users are on a bunch of different real
devices, so you need to be testing on a bunch
of different real devices.
A lot of the crash -- why I sort of emphasize
real devices and emulators here versus simulators
is that a lot of the issues that you're going
to see are actually going to be related to
the hardware itself, it's going to be related
to the kernel code, the amount of memory on
a device, it's going to be related to the
Wi-Fi chip.
It's not just enough to test on simulators.
You have to be thinking about real devices.
You need to be thinking about emulators.
And the cool thing is, there are a lot of
companies now that are actually making dedicated,
shared, or private device clouds.
So Appurify and then as part of Google will
be doing the same.
I know the Testeroid, folks today are in the
audience today, App (saying name) does that,
Xamarin has a test cloud coming out.
There are basically a lot of resources right
now to think about where you can find real
devices to test on.
And the thing is, this is only going to get
more and more complex.
With wearables, with more capable devices,
you're now going to have to think about not
just real devices, but real devices with which
you can change other hardware parameters as
well, like location, the accelerometer, gyroscope,
et cetera.
So that's number one.
The second thing is, testing on real user
conditions.
Going back to the two apps that I showed you
a few slides ago where app two did much better
than app one, we went and did a postmortem
on these two apps.
And what we found -- so what this graph shows
here is the first-time user experience.
So it's the FTUE for gaming apps is very important.
You pay someone to install the app or you
pay to get someone to install the app.
And then you want to hook them in the first-time
user experience.
The FTUE is incredibly important for first-time
games.
What we -- these are two carriers, and I initially
had the name of the two carriers and Google
legal told me to take them off.
On two carriers, what we found was on one
carrier, over the course of the FTUE, the
drop went from 80% to something like 60%.
That's kind of normal.
It's not particularly surprising.
On the other hand, in the other one, if you
look at steps four to five, you can see that
the sort of completion rate dropped from above
70 to below 60%.
And so the issue as to why app two did so
bad or app one did so bad was literally related
to people on carrier one at the FTUE step
4.
And specifically what happened was there's
a network call that timed out on three bars
or below on that carrier.
So this is something where it had nothing
to do with the content of the game, it had
nothing to do with the design of the game
or how many installs you bought.
All it had to do with one step in the FTUE
was not tested on this carrier at three bars
or below.
And that basically resulted in this game,
like, basically millions of dollars spent
developing this game going to waste.
Our point here is it's not just the real devices,
it's also the real user conditions.
You want to make sure you're thinking about
-- so your user is going to be loading the
app, getting into a subway, getting into an
elevator, have 5% battery left, get a call
in the middle of playing with the app.
You need to think about these real user conditions
in addition to the devices themselves.
The third thing, of course, and this is -- it's
the name of the conference.
You have to automated.
Manual testing is simply not scalable.
And everyone talks about Android.
Yes, Android is a nightmare in terms of the
number of devices.
Though I think it's very addressable.
But it's also iOS.
If you think about it right now, you have
more than 40 combinations just for iOS as
well.
So you have to think about automation across
both iOS and Android and Windows, eventually.
The second thing is --
[ Laughter ]
-- you need to also test on -- even if you're
testing on all these different devices.
Let's say hypothetically you have the super
rich company and you've bought every single
device out there and you happen to have this
Army of manual testers that are ready to help
you out.
Even they we have to start thinking about
all the permutations and combinations of user
conditions.
Basically, the space gets very large very
quickly.
The other aspect, of course, is manual testing
is going to miss bugs.
So coverage is super hard for mobile.
Coverage is super, super hard for mobile automation.
But even then, manual testing is going to
make it even harder.
The point, just like there are a bunch of
folks providing device labs right now, I think
one of the cool things about the mobile test
space is there are so many great open source
automation frameworks out there.
It's not something you necessarily need to
pay for.
And at Appurify, we probably saw ten to 12
different frameworks used.
And I think our big learning is there's no
right framework that is better than any other
framework.
I think it really just depends on what your
team is comfortable with.
Some folks loved Appium.
Some folks love UI Automation.
Some folks love Robotium.
Whatever works for your team is great.
The key is to actually just pick a framework
and actually go with it.
The fourth point is this thing around functionality;
right?
And what you see in the pie graph on the left
-- what we did -- So this graph is from a
year and a half ago.
And what we did was we went and read every
one and two-star -- every one star review
in the iOS App Store.
And we wanted to bucket these reviews into
different categories.
The idea was we wanted to see, what actually
drives poor reviews?
What drives poor ratings?
And the blue slides, the 47%, this is what
I call nonperformance-related.
This is basically someone talking about I
don't like this game, or my favorite is always,
hey, I have a chase account and I'm not able
to log in with my Bank of America app.
It's stuff that is completely not controllable
by you, which is fine.
The good news about this graph is that 53%
of the time someone leaves a bad review, it's
actually controllable, because it's related
to a crash or bug, it's related to network
issues, or it's related to client-side issues.
The sort of overlay on this, as I said, we
created this graph a year and a half ago.
Recently, we refreshed the analysis for both
iOS and the Android App Store, and the cool
thing is we actually saw the 47% drop.
So I believe for Android, the 47% is actually
something like 40%, for iOS, it's something
like 45%.
So in other words, it's, if anything even
more of the issues that people actually complain
about are controllable by you.
Now, obviously, the big thing here is crashes
and bugs.
That's where the whole mobile test automation
comes into play.
And I'll show you some examples of that.
But you -- my big takeaway is, if you just
attack, for example -- if you just attack
crashes and bugs sew functional bugs and crashes
in your app, you're basically addressing every
other type of -- like, you're addressing a
space almost as large as the nonperformance-related
ones.
It's going to have a huge impact on your apps
ratings.
So with that said, let me actually talk about
what we're going to do at Google and what
we did at Appurify.
So the solution that we're working on is very
similar to what we had at Appurify, which
is, essentially, we're going to be providing
access to a real and virtual device cloud
which is going to have hundreds of real device
models, and, essentially, hundreds of real
device models and thousands of devices, in
addition to virtual devices on demand.
So, in other words, you don't have to go buy
every single device out there.
You don't even have to go buy most devices
out there.
You're going to be able to access whatever
devices are relevant to you, Android, iOS,
as well as virtual devices.
That's the first part.
The second thing is we're going to allow you
to do extensive testing on those devices.
And that consists of three components, the
first is the robot app crawler.
The idea here is, you don't need any test
script.
Maybe you need a bootstrap script to log into
your app.
Outside of that, it's essentially an app crawler.
We call it a robot because it's smarter than
a monkey, stupider than a human.
It's essentially not randomly clicking on
your app.
What our robot app crawler does is detect
every clickable element on your app and tries
to crawl through your app for you.
It's actually a fantastic way without writing
a single line of test code to actually start
testing your app.
And I'll show you a demo of this.
What you can do is load your latest build,
choose all the devices you're interested in
and then this app crawler is going to start
crawling through the app.
One of the cool things about the app crawler
-- So, again, we had the robot at Appurify
as well as a bunch of different test frameworks.
And I would say more than 90% of all runs
a lot had nothing to do with the test framework.
They were the robot.
Obviously mobile tests are hard to write,
they're going to be brittle, you're going
to have to support them, so you're very selective
about what you're testing from a functionality
perspective that you've written code for.
What we find is after a while, the tests that
you've written never fail because they're
the most important parts of your app and the
dev probably caught the issue themselves,
et cetera.
Where the crashes occur is when someone clicks
on something that wasn't expected or was slightly
off the beaten path, and that's what the robot
is great at.
So essentially, this is a great way to augment
what you have in terms of your test -- your
test coverage in terms of the tests that you've
written, this basically tries to traverse
everything else in your app.
Now, obviously we're going to support test
frameworks as well.
Our approach at Appurify is, as I said, we
did not have a preference on which test framework
was the best or not.
We basically wanted to support anything because
we find anytime anyone actually takes the
effort to build out a test library, we want
to make sure that they're enabled.
So we're taking the exact same approach at
Google.
We're going to be supporting every test framework.
I don't think we're going to launch with all
of them immediately, but the intent is to
be completely agnostic to the test frameworks
that we allow.
And then the third thing is we're going to
allow you to test on relevant user witness
as well.
That means we're going to let you test on
different networks -- different carriers and
signal strengths.
We're going to allow you to change the location
of the device, the orientation of the device,
the amount of available memory of the device,
et cetera.
Now, it's one thing to have a bunch of devices.
It's another thing to write a bunch of test
cases.
Now what; right?
And one of the things again is we strongly,
strongly believe that it's not just enough
to know if your test failed or in the because
let's say a test failed.
You still need to spend an inordinate amount
of effort to figure out what actually caused
that test to fail.
So we found something that was very important
at Appurify is to provide as much data as
possible when a test actually ran.
So what we'd do is we'd capture video, capture
screen shots, all of yourlogues, obviously
the test output, network traffic, crash reports,
bug reports.
All of this data will be provided to you as
well.
So the idea is full automation across whatever
real device or virtual device that you want,
with whatever test framework you want, plus
the app -- plus the robot, and then a bunch
of data given back to you so that when you
actually want to prioritize and focus on issues,
you'll be able to have the relevant data to
go debug it.
Now, I'm going to go through two more slides
and then run through a quick demo.
The two slides that I have here are real-world
examples from our Appurify base that I wanted
to show.
The first one is to actually demonstrate in
our mind at least we saw there was a very
strong correlation between crashes and ratings.
So if you go back to the pie chart that I
showed you, 40% of all poor ratings are crashes.
So this is for an airline app.
One of our customers was an airline, and they
had a fairly complex development process so
what happened is they started using Appurify
but it took them a few months to actually
start integrating the issues that they saw
with Appurify into their app.
But what we were able to do for them, though,
is -- so across eight builds, B43 to B51,
what we were able to do was take every build
that they shipped, run it with nothing more
than the Appurify robot on a certain set of
devices, and we just wanted to catch the crash
rate.
So we were just trying to see if there was
any correlation within crash rate.
And then the ratings that -- the red line
is, of course, the ratings, the average ratings
of this app in the app store.
We literally wanted to see if there was some
sort of correlation.
This is just one example from this airline
app but the quick summary here is we actually
found a very strong correlation between nothing
more than robot finding crashes and the eventual
rating that your apps on the app store.
So this is just one data point that I wanted
to share with you.
The second is actually this is for a startup
that basically had a productivity app and
unlike the airline, they were actually able
to integrate the changes -- the issues that
Appurify is finding more quickly.
When they started using our product, their
crash rate was something like 50%.
It's not because their app was that bad.
They started early in the development process
to actually start running our robot.
Again, this is just the robot.
This is not any test frameworks.
Just running the robot.
And literally what they were able to do is
over the course of a month, take their crash
rate, at least on -- at least the crashes
that Appurify could find, take it down to
zero.
Because it was actually very easy to do this.
There was no manual effort involved.
It was literally the app -- their build continuously
running on Appurify devices and what we do
is say, hey, look, these are the crashes we
caught.
They'd have the video, they'd have the crash
logs, et cetera, and actually debug each of
these.
So the point of these slides is to say if
nothing else, focus on crash -- don't worry
so much about test frameworks if you haven't
written any massive suite of test cases.
Instead, just try to run a robot.
Catch crashes and reduce it, because that's
going to have a meaningful impact on your
app's performance.
Can we switch to the demo, please.
Thank you.
So this is the Appurify dashboard.
We are migrating this to be a Google dashboard
with a lot more scale, is probably the biggest
change that's coming in the Appurify product.
And again, my point is there are a lot of
other providers that provide this service,
too.
I just wanted to show you what the Appurify
flow looks like in terms of what -- it's something
you can do immediately.
So this is our -- this is our main dashboard.
What we did here was these are all Android
apps, taken from the app store so we're not,
like, necessarily telling you one app is better
than another.
These are just apps we took from the Play
store.
And taking a step back, the best way to use
a service like Appurify or any of the other
services, App Flap, Testdroid that I mentioned,
is actually to use it as part of your continuous
integration system.
So all of these tools have a fantastic API
that you can call from your Jenkins build
server.
Every time you have a new build you can push
the build to these different devices, test
it, get the results.
Appurify as well is fundamentally an API.
This is just the simple Web wrapper we've
put on top of it.
You can drag and drop a new build onto the
platform.
So if I were to take, for example, Pinterest
-- I like to show these are all real devices
that are accessible to this account.
And we don't need to go through this list.
I think the only point I'd make is you want
to choose a service that actually provides
the latest OS versions.
iOS.
It needs to have iOS 8 now.
Android, needs to be supporting Android L,
et cetera.
So you need a comprehensive range of devices.
You don't necessarily need to have every device
out there because nobody is using every single
device.
The long tail is really not the issue.
You want to make sure you're hitting devices
that cover around 80% of your customer space.
So once you have a build, the next thing you
want is to actually have your test suite.
And as I said, Appurify by default provides
the Android -- for Android and iOS, it provides
a robot app crawler.
We also allow you to add tests based on the
framework that you support.
We also supported Appium, but the idea is
you can choose whatever test framework you're
interested in, choose the relevant test files.
Once that test case is set up, the next thing
that we allow you to do is essentially choose
a combination of devices and user conditions
or profiles.
So what you see on the left is essentially
our location network profile conditions.
And so if I wanted to, for example, add a
new profile, you could choose -- depending
on -- and we're constantly updating our network
profiles but you could choose a specific carrier
signal on a specific network.
You could choose a specific location that
you were interested in.
And this device is now going to think it's
on that network, under that carrier condition.
It's going to think it's in this location.
You can then choose the devices that are relevant
to you.
And I don't want to clog up the entire, like,
Appurify device pool, but basically you can
choose this combination, and choose the build
that you're interested in, choose the test
case that you're interested in, and kick off
the test.
Everything after that is our problem.
Devices at scale are an absolute nightmare
to manage, and we've spent a lot of time figuring
out how to optimize that.
That shouldn't be your headache for the most
part so we take care of that for you.
So instead, you can now see that these tests
are queued.
As soon as the device is ready, it will run
on those devices.
Now, if I were to look at any specific result,
essentially the type of data that we'd give
back to you includes a few different things.
Obviously the video of the run itself on the
real device.
And this is a grit example of the robot running
through an app.
As you can see, just try and do random things.
Nobody had to write a single line of test
code.
So this is a video of the robot playing around
with the app.
The orientation changes, all that stuff happens.
We give you CPU usage, memory usage, the test
output, in this case the robot output, the
system network log.
In some cases we capture network traffic.
And the cool thing in this case is you actually
have a crash as well, and so -- I'm just going
to skip to the results zip file that we provide.
So if you go into this -- the cool thing is
this comes back as part of the API so you
can have a post processing script that grabs
all the results that come back from all the
tests and if you're just interested, for example,
in crashes, just look for fatal exceptions
and start looking through that.
Or if you want a more detailed bug report,
that's fine.
We can provide that as well.
The idea here is we're trying to provide as
much data as possible from every run.
So if you're focused on just crashes, great,
focus on that.
If you want to now start improving decreasing
the CPU usage of your app, that data is available
here as well and you know what to focus on.
But the point here is you want to choose a
service that has automation on as many devices
as possible.
Again, it's not about the -- there's a limit
to this; right?
You don't need every single device out there.
You probably want to talk about the top 60
to 80 devices.
You really want to think about real user conditions
because that's almost as important as the
real devices or virtual devices themselves.
And once you're running these things it's
not enough to just look at the test output.
You want to make sure you're looking at all
this other data as well because that's what's
actually going to help you fix the issue.
So if we can go back to the presentation,
please.
So that's all I have.
If anybody has any questions, please drop
a note to lachwani@google.com or jaysrinivasan@google.com.
I believe our Appurify addresses work as well,
so jay@appurify.com as well.
Happy to tell you more about this.
I can give you a better sense of the timeline
when we will be launching our Google service,
and we will be doing sort of a limited beta
soon, and if anybody is interested in participating
in that, we'd be more than happy to accommodate
you as well.
Thank you.
&amp;gt;&amp;gt;Sonal Shah: Thank you, Jay.
&amp;gt;&amp;gt;Alan Myrvold: So we have another question
from the moderator.
How do you ensure the video and log capture,
test harnesses, robot agents, et cetera, don't
adversely affect the app on our test?
Like the app only crashes when there's a screen
capture process happening.
&amp;gt;&amp;gt;Jay Srinivasan: It's a great question and
something we basically spent two-and-a half
years on.
So obviously all of the background processes,
et cetera, that are running to capture this
data do have an impact on the app.
We took a principle of saying the CPU impact
from any of our tools shouldn't be greater
than 5% of a recent device.
And I think it's no more than 5% of an iPhone
5, for example, on iOS.
We wanted to make sure RAM usage was no more
than 40MB.
So just at least in terms of performance footprint,
we did a pretty good job.
In terms of crashes, how do you know that
our platform is not causing a crash, that's
a great question, and it's basically we've
been running thousands and thousands of runs
over the course of last two and a half years.
So when there were crashes caused by us, we
fixed it.
So by and large, we -- So what you'd be able
to do is look at the logs and see if it's
our -- if it's our platform creating the crash
or if the app actually crashed.
I have nothing better than my word to say
but we've caught almost every -- we haven't
seen any issues caused by our platform themselves.
There are occasions when our platform is writing
to the same part of the phone as your app
and that's causing contention, causing a crash
on occasion but usually when you find those
types of issues we fix it in on our platform.
So basically through experience we've gotten
really good at making sure we aren't having
an impact.
&amp;gt;&amp;gt;&amp;gt; So I have a question.
So when you talked about the robot, can you
give me some information about how do you
measure what -- what it covers in terms of
the exploration space?
Do you have any information about what it
covers?
&amp;gt;&amp;gt;Jay Srinivasan: So as the simplest level,
when possible, if we can detect what the UI
view or UI WebView looks like, we basically
look at that and then basically create a tree
based on each link in the Web -- in the view.
So essentially we're not really focused on
the screen size.
We're just focused on what's clickable and
what's not.
If it's OpenGL or if there is nothing clickable,
et cetera, at that point it defaults to being
a monkey, at which point it's essentially
a function of the size of the screen.
The thing about the robot is what happens,
for example, if -- so we throw in swipes,
we throw in rotations, we try to lock the
screen, we can turn off Wi-Fi, turn it on,
stuff like that.
So what if the robot swipes down on an Android
phone and turns off Wi-Fi?
That's happened.
So then we basically build a bunch of things
in the robot that if that happens, comes back
to the app and turns Wi-Fi back on.
So again, the basic principle of the robot
is clickable elements in the app and trying
to iterate through it, but there are, like,
a bunch of different catches around that that
make sure it actually stays on the app.
&amp;gt;&amp;gt;&amp;gt; So when you actually find a crash in the
application, do you get some sort of here
are the minimal reproducible steps or are
you just going to have to watch through the
video yourself?
Even better, if do you get some sort of reproducible
steps in log file, can you just do that locally
so you can just paste it into your -- in the
ideal world, local app crawler?
&amp;gt;&amp;gt;Jay Srinivasan: Great question.
Let me tell you what the eventual goal is.
The eventual goal is to keep shrinking the
steps that the robot does until it localizes
the crash.
That's like a distant dream.
May or may not happen.
What we do now though is we have a seed for
every run of the robot.
So you can at least reproduce the exact same
sequence that the robot -- first of all, in
the output log, you'll see the steps that
the robot took.
You'll see the video, the system logs, the
network traffic.
Obviously everything is time stamped.
So from a time perspective you know exactly
what was going on so you can try to reproduce
it locally.
The robot also has a seed value so you can
pass that in through the API and say, look,
I want to run the robot with the seed over
and over again to see if the crash is reproducible.
But, honestly, most of the time, the best
way to do it is actually to look at the video,
see if there's something obvious in the logs,
and if not, then start fiddling around.
&amp;gt;&amp;gt;&amp;gt; So how long is a typical run, is, like,
two hours and then --
&amp;gt;&amp;gt;Jay Srinivasan: It's up to you.
So we've actually been doing -- so we're trying
to do some stuff at Google where we just start
running robot on a bunch of different apps.
And for that, we want to make sure that we
capture all the relevant data in as short
a run as possible.
So we've been running a bunch of experiments
where we've been trying -- so we've taken
100 apps and we're trying to see, look, what
are the different robot tests -- the robot
(indiscernible) is configurable.
You can choose whatever you want.
But the question that we've been trying to
answer is, what is the minimum length such
that you get the majority of crashes.
We found that length is around ten minutes.
&amp;gt;&amp;gt;&amp;gt; Nice.
&amp;gt;&amp;gt;Jay Srinivasan: So you can run it for five
minutes.
But we actually find that five minutes isn't
great for crashes.
&amp;gt;&amp;gt;&amp;gt; But do you do any, like, steps that aren't
optical or visible easily on a screen, turn
off --
&amp;gt;&amp;gt;Jay Srinivasan: Everything is a UI step.
Oh, thank you.
&amp;gt;&amp;gt;&amp;gt; Thank you.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>