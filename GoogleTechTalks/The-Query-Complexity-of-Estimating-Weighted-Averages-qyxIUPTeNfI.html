<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Query Complexity of Estimating Weighted Averages | Coder Coacher - Coaching Coders</title><meta content="The Query Complexity of Estimating Weighted Averages - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Query Complexity of Estimating Weighted Averages</b></h2><h5 class="post__date">2010-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qyxIUPTeNfI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this was some work that I did so you see
it puts been kept guru Swami's
affiliations University Washington which
it no longer is he's in Carnegie Mellon
but he was there when when I did this
work with him and without my chakra
buddy who was visiting from Dartmouth
there and then I went home and and did
continued working on this and eventually
learned from my father into this
that's why he's a sort of a another
author on him ok now I guess I'm going
to probably start with the motivation a
little bit before the definition and
just a little bit of motivation is that
there was a bit of a flurry of interest
in evaluating search engines both
treatment systems and that kind of thing
in that the information retrieval
community may be over five to seven or
eight or something and I think it's
still of interest but it's not quite
that the blood of papers that you saw at
that stage and so one thing that
happened was that some people said well
maybe the metrics people are using to
evaluate these things are right we
should try something different
which also inspired me to think well
maybe what can we improve in
theoretically about this sort of results
okay so the the question that we're
interesting is this we have given a
vector a of weights between zero and one
and responding non increasing weights
okay and what we these are known to us
in events that's very important but
what's not known are some other
quantities why between zero and one
which somehow represents something like
the relevance of something to it surgery
and so why I represents whether or not
the arc iiterature returned whether this
retrieval system is in fact relevant to
our query and what we want to compute is
the score so I'm interested in linear
scores which is to say it's a weighted
average of these things now why waited
because you care much more about than
the first few things for a train bus
system relevant than whether the 100 for
305
and this makes the problem if it this
makes the problem of estimation
inherently actually easier or at least
depending on the white family easier
then if we're just meaning if you just
well let's work out how many of the
documents returned irrelevant that's
somehow actually a harder problem was
all trades and waiting at the waited
okay so what we want to do is come up
with a good estimate for this weighted
mean and we want to look at this few y
values as possible because those
represent asking somebody who actually
read a document assess whether or not
that's relevant well it's expensive so
and what we're gonna do is produce a
good estimate although we're going to be
doing some service and topic things to
do with we sort of in was it's given
we're going to imagine condition as
large as we wanted so what the I use at
the end you told it is hidden wise and
you can choose whichever one you want to
look at and you get unit cost to look at
each other Slenderman
so that I was the fun bit more carefully
okay so I've already spoken a little bit
about that why we're doing this but I
thought the query complexity was a
server a nice thing to do and and I have
a paper some years ago on communication
complexity which isn't terribly far away
so I thought why not revisit something
that worked in the past and one thing I
should point out is that this issue of
metric we've gotten finest of linear
metric here but that's not necessarily
that what the community uses they often
use things like mean average precision
which is over here at the quadratic kind
of quantity were you multiplying some of
the wires by otherwise
what other justifications but it
ultimately turns out to be a quadratic
manager so much of the research he sees
in trying to incepting strategies for
he's slightly more complicated metrics
but I thought well we don't really
understand the linear ones first that's
trying to get some better appreciation
then okay so you could imagine a kind of
a decision tree which is a sort of
randomize decision tree so you can make
choices at all the nodes and the trees
is to which I have you going to look at
so you can sort of have some coins or
dice or whatever you've got a random
number generator and the leaves of the
tree you say okay I believe that the
weighted mean is this so we've got this
randomized decision tree and we accept
that it might fail with a probability
Delta and we also accept that it might
not get exactly the right answer so
we're going to say that the the relevant
we're going to classify this a relative
not absolute error between the estimate
we obtained and the the actual value
okay so we're gonna say it's an epsilon
epsilon delta approximate it with
probably one minus Delta we reach a leaf
that whose absolute difference from the
thing we really pro estimate is at most
Epsilon
so what are the bugs in my slides is on
the next line we have said these people
show that this many samples suffice test
don't the meaner that should read that
you need at least that many to estimate
I mean I don't know why not suffice but
anyway so there's a lower bound which is
1 epsilon squared or 1 Delta which is
something that people yes so then why's
everything is constrained to be between
0 to 1 okay so if you could imagine that
instance if they are 0 150 0 or 1 but
you could also say that there's a
concept of it wouldn't hurt you to allow
partial rivers tennis I think what's the
probability underlying just random
process so it's up to you so you can
choose to sample this or sample that so
you can imagine
decision tree with probabilities at the
nodes of the trees yeah is it from some
no no there's our eyes our feast so I'll
just go back I mean one one thing I come
to retreat a little so the a eyes are
given to you n is given to you then why
eyes have given but they're hidden and
it's up to you to choose which way as
you want to look at and in a sense the
probability is entirely to do with your
choice of samples some sense or the
choice of the wise that you look at so
you're choosing them randomly that's
right okay so that you have a relatively
high like one - still the chance of
succeeding it's important to get this
right now I admit sometimes that each
time I present this I get slightly
different questions about the model and
sometimes good to just stop okay great
so so it's so it's known what the this
apartments of the mean question is okay
but as I said it might be that because
some of these documents are sort of more
important than others that somehow we
can we can afford not to look at quite
so many things as if we were just very
mean now so as well as this magnifier
result as if Iosefa describe two things
in a paper there's one and one is lock
sensitivity which is the says okay the
certain parts the input that we have a
look at let's learn more about that
later and also they talked about another
technique for getting a lower bound on
the mean which mostest and work by some
statisticians okay so let me show you
what what we did so okay so the the
whitening problem in general is a more
general problem than the minton and so
if you didn't if you didn't care
anything specific about the weights then
we show that okay in the same number of
theories you can match the lower bound
for me
but that's not particularly interesting
because maybe there's something
interesting it meant something special
about the families of whites rooms so
one is one so we look at three different
white families one is geometric which is
inspired by some work of my colleagues
who believe that you ought to have
geometrically decreasing weights to
evaluate for a feeble systems another is
another 50 workshop was where the white
sort of diverged so you could imagine an
envious of really really large and then
in these aids form a divergent series
and that turns out to be as bad as the
mean case I mean the mean is clearly
divergent series because the weights of
this one one one some constant repeated
till week there why the weights of the
eighties so so the problem changes
depending on what it is that you're
trying to estimate so if you've got
these weights that go down very quickly
it's sort of easy because you can look
at the things the the things with heavy
weights only on the other hand if the
weights to perform a divergent series
then that's as bad as the mean and so
I'm gonna be nice to have a general sort
of theory here but the best we could do
is something in between case with the
power law which is the most interesting
one which is sort of converges it
doesn't converge as quickly as the
geometric case okay um so our Yi here is
a particular flavor of the under-15 or
Chernoff bounds that was most useful to
us in a little aside that I've been
giving him a present of this is that you
know I did an undergrad in statistics
and yet never learned about Chernoff
bounds I don't know where the Lawrence
it was you know you're better off things
you did okay my PhD is like what's this
better chebyshev's inequality Marcos
what's Chernoff bounds and I guess you
know theory computer science theory
people they're slightly different
questions sometimes from the
statisticians okay so I actually went
back and try to find a forum I wanted
obviously retreat went to JSTOR and
retrieved an old paper from nineteen
fifty something okay so the form I want
is this where you've got these bounded
independent variables okay so that means
they're on one another they actually
have to be I assume I've written this
crappy I don't believe that
and independent okay so you add them
together you look at the sample mean and
and the divergence is nicely bounded
inverse exponentially okay so what do we
do the A's are essentially going to be a
unnormalized probability distribution
for us making we normalize them we
choose we do sampling with replacement
with unequal probabilities between and
do quite easily
okay we choose but each trial we choose
the particular item for the probability
specified while it's normalized wait for
some sense and then if you do some
algebra you can find that with one on
the epsilon squared log when I adults
are approximately trials you can get the
right accuracy okay so that was nice but
not very informative so far but we think
well what can you do with these ideas
right so that was a lot of computation
like a posting there was something more
adaptive I guess in that could apply but
we are converting any different
probabilities and then choosing the Y's
with those probabilities oh I see I was
thinking something more that those that
are not like to find the largest you
have a power lesson yeah
no that's just pretty generic so you go
any this works for any family right it's
just showing that even though weighted
mean in general is a sort of a harder
seems like a more general formula mean
it really is sort of the same okay I'm
just jumping ahead there's a reasonable
thing to do um okay so maybe some of
these aids are more important because of
the weights of the a some of these
things that were important than others
okay so what we're going to do is say
that some of the items that we call
heavy and we're going to definitely
sample those and so that maybe we sure
that somehow we ought to look at the top
ten documents or something like that and
then afterwards we'll run this generic
scheme that I described
that will rewrite everything because it
will renormalize the the weights based
on what's left and this this scheme so
this scheme appears in two other
instances so I discovered only fairly
recently that people at Stanford in 2006
worked on a somewhat similar problem and
I have to admit something strange about
this paper so there's another aside
about this is that it hasn't actually
been appeared anyway yet we submitted it
some way and it got rejected and you see
what the guys in a change he said was it
wasn't fair that Moses Cherenkov was my
adviser because I couldn't I never got
the experience of being a paper rejected
yeah and this is an important thing to
learn when you're PhD student so this is
my first paper rejection
I'm gonna be shocked what do I do so I
got over that yet and we we still
haven't that's also because I need to
just read that thing by those guys at
Stanford carefully to know exactly what
aside from them okay so they have a sort
of similar scheme with these some things
of the heavier and some things in the
lighter and so forth okay this also
appears actually in the statistics
literature so I took me a while to work
out what the statistics relevant
statistically true literature was and it
turns out that it's um you lookups
survey sending is the sort of keyword
that you want and they have situations
where they want to minimize variance say
some unbiased estimator for something
and they get into situations where you
know what people is sequential strategy
which is what we're sort of doing here
with users oh no sorry actually I'm
Reggie really need some kind of drawls
eventual but anyway there's a situation
where you sort of walk along each item
and say okay do I look at this do I look
at this yes or no different
probabilities and they have some things
where you've got probability one at the
beginning and then other things that's
something so this is appeared in all
circumstances okay so we run the hit
we've got the heavy ones we did the
generic skin with the liked ones with
the risk weights so what do we get we
get page or little entries
number of heavy things Plus this one
it's on spread local and Delta
appropriately scale okay
seems good strategy but what's the
appropriate age to use
so one one thing is to say well maybe H
ought to be all of the terms that have a
normalized weight of at least epsilon
over two or something or the order of
epsilon anyway because if there's
something with a weight that's certainly
larger than epsilon and you don't sample
it then you guarantee to be out I mean
not guaranteed there's no way you can
guarantee that you're within epsilon of
the right ends so okay you think well
let's let's look at those things and
then because it's an additive error
relative error so some people have
worked on so other some other papers
have worked with relative errors
actually so you have to look at these
some of these terms and then somehow you
know just do the generate scheme on
everything else the problem is that if
you look back at this formula here
we remind end up with a situation where
the second term is actually
significantly larger than the first and
one of these thought of rules of thumb
that you learn from doing this research
is research is if you adding two things
you're trying to minimize that sum it's
probably a good idea if the two things
are roughly equal so somehow you don't
want the second one to dominate the
first so in fact what we're going to do
is to make H larger and then somehow
their contributions about the same so we
try this with the power law case for
instance okay so we've got a convergent
series and we stir everything up and
then we have H items the weight of the
light things is going to be roughly if
you do the integration of this sort of H
1 minus P over P possibly that okay
assuming that everything works nicely
with Agent P and so forth and if you do
a little bit more how to really find ok
so H should be roughly epsilon the power
2 over 1 minus 2p and don't ask for
interesting quantity but it certainly
balances and maybe this indeed the right
answer okay so we lend claim ok that if
you do this roughly this many more Delta
then if you print that many things then
you'll get a good move bound now note
that this is lower or the
better than epsilon to the minus two
right so when P equals one for instance
where you get you a to one minus two
which is epsilon minus two but as P
increases then this is going to be
responding to be smaller so that seems
like a useful thing to do but the
question was was it easier in fact the
right answer could be get a matching
lower bound and that actually took quite
a while to to work out so okay so get
refer to the paper I mentioned before so
if assuming that the epsilon and Delta
behave nicely in the in on four part
doesn't dominate you've got this
they precise formula for a lower bound
if you want to estimate the mean okay so
we're going to use this result fairly
heavily because we're gonna do somehow
fake the way to the mean problem is in
weighted mean problem even though now
somebody yesterday said to me wait a
minute isn't the way the main problem
more general and so therefore doesn't
include the mean but we're going to do
it in a reduction from main to specific
kinds of weighted mean which is wait a
minute a certain kind of a sort of a
vector
okay so it's all about block sensitivity
for a moment which seems to be fairly
powerful at all imagine that you've had
some part of the input these are these
hidden Y's and you said somebody some
adversary changes all their values maybe
from one than whatever they are to
something else what is this dude what
could this potentially do to the output
so if it changes the output by at least
two epsilon then you know that you have
to look at something in that block
otherwise there's no way you can
guarantee did your with an additive
error epsilon with the truth okay so
then you say okay maybe there are
several blocks like this like there's
some kind of this he's one of the
important you must look at this he's
another party input but if you change
the values the output might change by at
least two Epsilon another path so you
know if you've got these disjoint
subsets at the input you know that you
have to look Lisa at something in each
of these okay
so this you've got so he x one minus two
Delta but basically and there's a slight
difference because you've got expected
query complexity in any case it's still
a reasonably powerful technique you can
multiply the box and stupid or one Marta
to Delta and that's a lower bound now if
you think about the way the main problem
at the very least you've got a little
bit every - whose weight has normalized
weight is at least two epsilon right
because if it changed from one to zero
zero to one potentially than if they
have moved by this so yes so that's a
block by itself that's a rather extreme
case so block is for which the worst
case change would cause a fuse to
Epsilon that's right okay so so that was
one sort of tool we're going to use the
only thing I mentioned is we're going to
use some reductions okay so um again
there's slight flaw in this slide is
that the term Y didn't mean up the top
they should probably have an accompany a
vector right because weighted mean in
general is of course a more general
problem than mean but what we're going
to do is to reduce the mean probably
going to fake the main problem with the
weighted mean problem is a specific a
vector so we're going to say okay if we
can we're gonna start this instance of
the weighted mean that sort of almost
souls approximately solves the main
problem and then because the main
problem is hard that must mean this
weighted mean problem is also hard so
we're going to do is something fairly
obvious we're going to do sort of
partition the a the weights in the
weighted mean problem into some subsets
there are roughly equal okay so the
weights the weights in them in the meat
in the way that mean problem who sort of
glue them together somehow me the glued
set to become almost the same then
somehow you can fake the mean problem
okay we're going to say well if this
subset we don't associate this subset of
weight with a particular and their
accompanying Y values with a particular
x value up here I'm going to say
whatever this x value was the
appropriate Y values down here
saying and then maybe some of these
weights don't actually pee or in any
subset of those I do just throw them
away assume that the Y's is zero okay
now note that we don't actually make any
we don't somehow really copy the
exposing the Y's and course we just sort
of making references so that when you
query this wire you really appropriate X
or over every time you look at a new X
term it forces you to look at a new
subset and therefore a new y term right
so any lower bound when the X's is also
going to apply to the Y's and the other
point is that we're not using any fact
about the X's to determine these subsets
it's purely something to do the weights
and then nature these subsets and
therefore how hard the the accompanying
where the main problem is okay so we're
going to choose the subsets so that
somehow the consequence and the weighted
mean that we end up with is is very
close so within epsilon of the true mean
of the main problem and therefore if
anything that were if song approximated
weighted mean is also going to be the
true excellent of course the major of
mean that's how we're going to change
our lower bound now let's start with so
we're gonna start with two warm up
things and then we're going to get to
the more interesting - the power always
yes it is it easy to choose something
that can flash but it quickly I mean I
just and you'll see so in some cases yes
in power law it took me a few months on
and off thinking about how to do this I
mean it turned out to be kind of nice in
the end but I couldn't quite get my head
around what one we want me to do it's
not the interesting part I'm leaving ok
so let's look at some easy stuff so
geometric weights I mean it seems to be
kind of obvious what you do here because
somehow they've got these very heavy
weights the beginning and then this tail
that sort of doesn't matter and somehow
the size of the tail is proportional to
you know the weight itself all the way
the next way or something like that
right so seemingly the obvious strategy
is you you you sample some things down
at some point and they just throw the
rest away
rigorous so my colleagues believe
published something that sort of says
well we really should be using these
geometrically decreasing weights with
this models how humans read things they
say I follow this link maybe follow the
next one and then listen
so human is sort of reading the outputs
of a retrieval system and sort of a
geometrically decreasing interest in
some sense maybe that's reasonable
anyway if you look at what if you only
obtain a simple lower bound of course
you must look at every VI there's at
least two Epsilon and you do some
slightly annoying things and you end up
with something that's approximately log
3 epsilon over log Q and lock user else
negative so you essentially pop log of 1
over Epsilon
ok now remember that the epsilon is
small sort of over epsilon is large and
so one more than 100 some is a lot
better than whatever font squared which
is our generically lower bound and in
particular it's also an obvious upper
bound because you can just at some point
throw away the tail right so if you you
look at the tail terms and if their
total weight is at most epsilon over to
say well they can't affect the answer
very much we don't even have to look at
it and somehow that the position of
where that tail starts to whether the
sort of lower bound in some sense
position ends is sort of even a constant
determined by Q right so there's
obviously these are upper and lower
bound to match each other than a
constant so instead of epsilon 1 and
epsilon squared we've got log of 1 and
epsilon being the appropriate number of
samples we need to take it in
particularly matchy-matchy case you just
look at the start the be very exciting
but it's worth sort of placing some
groundwork ok so it's something a little
bit more interesting is what happens
when the weights diverge ok now it turns
out in this case that the problem is as
hard as the main problem ok so what do
you do well um
you as a faith amine problem by
collecting together blocks of waste that
are sort of roughly the same and in the
division series case and we're so
assuming that n is really big here ok we
can we can do this because we know we'll
never run out of weight because the
weights series diverges and we also know
that the weights themselves as a bounded
between 0 and 1 so somehow by by taking
lots and lots of weight we can get these
clumps that kind
roughly so what we do is we keep taking
weights until we get a Sun that's at
least somewhere between two over epsilon
2 epsilon plus one we know that we can
do this because of the bound the fact
that series diverges so we carve that
off and then we take another chunk of
weight between insurers Linder one plus
one and so forth
and if you sort of add things together
and so forth and use the one over 24 the
thing is and you get exactly that many
subsets then you can somehow estimate
the mean within epsilon over two so I
guess this time we're doing this like
the two plus an epsilon over 2 processor
made after that for the mean so so you
can it's not too hard to fake the mean
problem here because you've been just
taking sickening chunks so in fact here
that obtaining the subsets is quite easy
now that the one that the thing that was
kind of a bit more interesting is what
do you do in the powerful case so we
thought okay well let's start with using
block sensitivity which is my first and
so you group together continuous
sequences so you start with that some of
the weights are just really be pleased
to epsilon so you you drink those is
block by themselves then afterwards you
form contiguous blocks of weights that
add up to at least your Epsilon okay so
like once they sure for every fixed
Delta your fixed Delta and then reduced
is mean to it but you don't obtain a
sort of a lower bound in the Delta is
not particularly interesting part in a
way the ones you playing with right so
that there's an if you can estimate this
within epsilon of it too
sorry you forget between the mean and
the way that means that son of a - and
you've estimated the weighted mean time
slow the true then you have so many of
the original on the next one so you know
that's hard so therefore you know
okay so alright so in this case we've so
recall we've got these very heavy
weights we have to look at each of those
items individually and then and then we
look at blocks of contiguous weights
that end up at least two Epsilon
now note that those blocks are going to
be a weight between two epsilon and for
epsilon because by now by that stage we
know that all the weights licensure
Upsilon have been dealt with right so
when we don't really shoot by too much
in some sense so this gives us a bound
on the on the block sensitivity of the
number of very heavy terms plus the
remaining weight of stuff divided by 4
epsilon it seems to but unfortunately if
you do this you end up with a lower
bound
expected and so forth but the important
point you and ugly the balance something
like epsilon to the minus 1 over P which
isn't just quite what we hoped we wanted
to get epsilon to the two so you mean
that what we do with the upper bound is
correct and I believe it is I believed
it is I know now that it is uh-huh then
I wanted to get epsilon to the 2 over 1
minus 2p and we didn't quite get there
ok so what then and I spent some time
thinking while maybe maybe I can use
some property of these powerful terms
and I'll actually a little bit you know
naive or something in thought maybe I
can use something about the properties
of these sequences to add certain terms
together and they'll end up being about
the same this is no good
and of course one doesn't actually need
to construct this thing one just needs
to describe a method for constructing
equal weight subsets even if one doesn't
do it explicitly all right and took me a
while looser appreciate that point and
then I started thinking well what is
this what what is this oh sorry I have a
nifty I dislike it but what what does
this look like I said at the moment so
just a little bit more precisely what
we're going to say is imagine what we
want to connect are these subsets of
weights that are somehow
between Rho and Rho minus epsilon over 2
n so we've got these M subsets whether
it be almost equal okay and then it
means that the
OSE I can sort of do this okay so it
then means that this weighted mean that
we obtain is somehow between Rho
something between Rho multiplied by the
total sum of the x's or with an epsilon
over two over there okay and then if we
do the appropriate division we are um
we're getting a a with probability one
minus Delta we're getting an epsilon
over m times Rho approximated for the
mean okay it's probably too many
formulas on this slide actually okay
people look somewhat put off crease like
this time old okay so what's the
interesting thing the interesting is how
do you actually obtain these ways and
collect these weights together to be
roughly equal and I remember thinking to
myself well maybe it's something like
being packing and III know this was I
had a conference deadline I was aiming
for this to do maybe if I tell myself
I'll make the deadline then I can do it
and it sort of worked like this and I'm
it's been packing that I went to my
father I said what is it and through so
the reason he got brought into this was
through a discussion with him he said no
I don't think that's actually what you
want to be doing what you really want to
be doing is a scheduling kind of problem
okay we talked about what might be good
and it turns out if you look at the last
line of this the thing that we want to
do is do Maxima well the thing that
inspired us was the problem of
maximizing the minimum confession time
so you've got some problem with jobs and
you've got n machines and your silage
drops these machines and you want the
machine to sort all finish it around the
same time so you want to maximize you
want to eat you the one who finishes
first in some sense like that as long as
possible and then somehow that will mean
that everybody finished come near to
everybody else it's a rather strange
objective and there aren't a huge number
of papers on it
I talked to somebody Gerhard Berger
about this and he's one of the people
who worked on this and he said it had
something to do with
aeroplanes construction right
discussion some reason I'll get to the
bottom of that one day why anyone care
but I came for different reason which is
I was trying to make these equal subsets
and Jen's out that this longest
processing time strategy is a good thing
so you you start with the longer the job
that takes a little you sought the job
is in non increasing order of duration
and you as you sort of proceed in time
you assign a job to the first machine is
available you sort of sort them and then
assign these jobs and this is look like
what we got because we've got these A's
and they're in this decreasing order and
then sometime we want to assign them to
these subsets so they end up being
what's the same at the end scheduling
problem good minimize the maximum will
it depend
no normally one doesn't want to do that
yes but some this is an unusual problem
which we want to maximize the minimum to
this election if you if you okay well
I'm just saying that there are many
scheduling problems around and this is a
sort of a quickie one that only a
handful of people looked at and it so
happens that it relates to what I want
to do but you know it was just fortunate
that someone had thought of it so even
amazing good um well it doesn't it
doesn't necessarily make things equal
right so well I see what you mean you'd
have to ask me to do is a good point
actually I don't know I guess I just
fear that maybe you could still do that
and somehow end up with something very
uneven hmm epsilon anyway so there are
various strategies that these papers use
but one simple one is to use this
longest processing time heuristic and I
thought well maybe this helps us so what
do we what do we do here so we have um
we have these jobs and what we're
concerned about is the tail okay so
imagine we have a job that has a very
large tail after it's so large that if
we add up all the jobs of any weights
afterwards in the title and divide by M
minus one
the machine's minus one if that's larger
than the duration of the current job
then somehow we know that the difference
in the Machine completion times is at
most the duration of the next job why do
I say that okay so you'll see why that's
true in a moment another way that maybe
other one would have scheduled it there
in that gap well there's not enough time
to finish everything basically okay so
here's what happens
so consider the last job that finishes
finishes when the job started all the
machines were busy before perhaps that
was the very first job now imagine that
this tail is you've got this large table
with jobs that you still have to do and
that that large tail divided by minus 1
is still greater than duration of the
current job ok so we say this certainly
I hope this lasts over the future so
here's the straw bail this last job to
finish and we've got this large pile of
stuff we're going to assign it to all
the other machines while this one is
working but somehow this tail is so
large there is not enough machine
capacity in the time in which it's
supposed to finish to do everything
because they're in minus one jobs and
we've only got time tl if you like x as
in once one machine sorry to finish
everything else so if indeed the total
the one when once one will like larger
than T oh then that would mean they'll
not be enough machine capacity finish
everything in other words this TL is L
would not in fact be the last job okay
so the this can only be tripled
something can only be the last job and
the tail is sufficiently small okay so
this capital n was kind of like a the an
indication of which drivers will sort of
had very large tails
so this is one final slide it's almost
 slide in some sense so what this
many things okay this is good so I know
that I can now place a bound using this
LP TR gonna place a bound on the
finishing times of everything and
guarantee that somehow while the title
is to be sorry that knowing some
properties about the tail and so forth I
can now say that I guarantee what the
difference in finishing times between
these machines are in other words what
the error between the subsets is they
said we wanted to get epsilon to the 2
over 1 minus 2p okay and we know that at
some stage at least up to some some
capital n the the tail is too large so
we sort of we sit anyway it's a somewhat
confusing slide but we set the are the M
and the road almost to do what we want
we can't quite get it right so we can
estimate the size of the tail because
it's just a sort of integral than our
wall okay and when we sort of set
everything up we find that we almost got
what we want we do in fact get a lower
bound of epsilon the 2 over 1 minus 2p
but we also unfortunately get a divided
by P squared in there which may or may
not be important ok so you can in fact
fake the main problem with the weighted
mean problem by applying this opt
heuristic giving all of the subsets that
right and then there's a these are
somewhat crude estimates right based on
these intervals but so that's sort of
the crudeness of the estimate that's
probably causing the P squared to be
there since okay so um I guess the main
thing I'm interested in is what to do
next so one thing is I'm actually
looking at some more pragmatic so this
was a fairly abstract think that the
real people would care much about but um
I'm actually trying to sort of look at
some estimation techniques that minimize
the variance save these estimates with
some unbiased estimators and what the
right things do in that case is the
other 2d things are well okay we've got
divergent waves we've got very
convergent wait
this L or that was something between but
maybe this doesn't actually you know
it'd be good to capture something more
generic you know like why is that this
mode changes from long one or epsilon
then to this epsilon zero one minus two
P and then two epsilon to the minus two
and once you know how sensitive like if
you have the slight changes in the way
that's because some of these series look
somewhat similar like what's happening
and why and to do something more generic
and of course the third point is that
the most interesting question is to
actually compare multiple systems at
once right this is where any consider
between something to estimate
performance at one system so of course
you've got lots of common documents and
they appear with different weights
according to different each one system
may give a document very high weight
level on a very low weight and so forth
and maybe what we're going to do is
minimize the sum of the variances of the
all these things or it's not clear what
you want to do one thought I had the
right thing to aim for is that you what
you're ultimately producing is a ranking
of these systems so you want to sort of
say maybe with high probability we've
got the ranking correct and that's
ultimately what we've been sending so
you know what how many things do we need
to sample so that with 95% probability
we actually got the the rankings of
these systems right so just want to
thank me more and my colleagues asked
William Weber is a is a PhD student and
Alistair offers and just one last
stories that William and I used to sort
of see each other and the Department for
any urine not talk to each other just
grunt at each other as males often do
and in fact the thing that got us
talking and then led to some of this was
that I discovered he went at the same
school as me and and Lawrence for that
matter too and and suddenly that made us
start talking that's a very northern
story I think okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>