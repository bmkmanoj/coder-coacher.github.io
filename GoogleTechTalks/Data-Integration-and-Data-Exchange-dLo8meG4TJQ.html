<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Data Integration and Data Exchange | Coder Coacher - Coaching Coders</title><meta content="Data Integration and Data Exchange - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Data Integration and Data Exchange</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dLo8meG4TJQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Oh
okay so I'm Alan
I'm a PhD student at UCSD in mathematics
and computer science I'm finishing this
year so that's from here my advisers are
Jeff Rummel Russell in Palazzo in Victor
Vienna that's work in databases and
today I will be talking about the
integration in the exchange so it was a
little bit difficult for me to figure
out what would be the right level and I
really didn't know that much about the
audience I had so I'm mostly aiming for
a general computer science audience and
so I'm kind of hiding many details
during the talk and just feel free to
catch me later and ask me more questions
also most of my work is fairly
theoretical so just keep that in mind
and you know for whatever it's worth I
spend more than 10 years in industry
before doing my PhD so at that time I
used to think that most irritations were
pretty crazy so I will talk about
information integration the basic
problem is to put together information
that comes from many different sources
it has many applications of course
including many motivated by the web and
of course over time many different
software tools and solutions have
evolved to address information
integration problems and what I will
concentrate on noise and talking about
relatively recent work on foundations
from around this time in it's likely I
might be missing some earlier papers but
from early two-thousands and so there's
been some new approaches on how to do
information integration and that's what
I'll concentrate on I will first try to
give you a sense of what that work is
like and then I will tell you about my
contributions to it and one of my
readings of this recent work is that it
makes it easier to do a large-scale
information integration so information
integration as a whole is a very large
field it has lots of parts you know
which I'm sure many of which you are
familiar with for example has to do with
taking data from sources that are not
structured and trying to extract some
structure from them and many other
problems and I'm not going to be talking
about that so the kind of situation that
we'll consider it's just a very limited
aspect of the problem within this space
you could say that a small part of the
community
has roughly these goals this is kind of
what we're trying to
research how to express relationships
between their sources and interfaces to
the sources which is usually given by
mappings one general goal is to develop
a set of primitives that then can be put
together to solve different kind of
information integration problems so
there's not just one problem out there
and we're trying to come up with a
general toolkit in this process we want
a trade-off expressivity of the queries
and mappings with efficiency of course
from the theoretical point of view we
just want to understand what can and
what cannot be done what are the
obstructions and in the process we're
trying to apply some existing techniques
and of course when often need to extend
them and develop new techniques to
achieve this just to give a very quick
flavor of the kind of work that's being
done here this is just from the main
theoretical database conference 2006
which is happening in a couple of months
in 2005 which was last year this is just
the kind of papers and problems that
people are considering this just to give
you a very quick sense so there's work
on the exchange and various operations
on schema mappings on computing cores
and this is the kind of stuff that we'll
be covering today so there are many
problems in this space but I have very
limited time today so I'm going to be
concentrating on two main problems one
problem is the problem of answering a
query which puts together information
from several data sources and the second
problem is to create a database that
combines information from several
sources and of course we want to do this
efficiently so here we'll just kind of
outline for you what this is about and
introduce some terminology probably 95%
of you are really familiar with this so
we have several sources of data we have
users that want to access this data some
public interface in the basic problem
the first problem I consider is what the
users want to issue some query against
this public interface the interface has
to go get some data from the sources
some data comes back and then we answer
the query the case that I'm interested
in is relational databases which of
course are made of relations relations
look like these there's just sets of
tuples and every tuple is a bunch of
information put together which we
identify by attributes and
something that we'll be talking about a
lot that you probably know but since
I'll be talking about it I want to
introduce you formally is a schema so a
schema is just a description of how the
data weigh the information the database
is stored usually just the part that I
care about it's just the name of the
relations in the database and the
attributes and the kind of schemas that
we'll be talking about in the examples
or schemas that look like these so now
that I have introduced a little bit of
the terminology I can refine a little
bit the picture I talked about before so
my sources are relational databases over
some source schema and my interface is
just a target schema to the users it
looks as though there was a database
there but in many situations they really
won't be a database they're over the
target schema it's just that the users
will will issue queries as though there
was a database there and then somehow
the different layers will take care of
pulling this information together and
what connects the source databases to
this target schema is known as a schema
mapping schema mapping is in a way a bad
name for couple of reasons
that's not really a mapping in the terms
in a sense of being a function but this
is terminology that's already there I
cannot do anything about it
so the schema mapping will relate source
databases to databases of the target
schema and this database is over the
target scheme are known as solution so I
will be talking about solutions as I go
through the talk so formally the problem
that I care about is this problem I have
some sources over the source schema some
schema mapping and a query over the
target schema and what I want to do is
to compute the answer to the query and
in general I want to be able to handle
queries that are expressed as expressive
as possible mappings are as expressive
as possible but of course in order to do
this I may have to give up some
efficiency so I won't understand the
trade-offs between these two things let
me first give you a very simple example
of a mapping and this is in a way the
way people have done information
integration for a while and shortly I
will introduce this kind of relatively
new approach so in this approach we have
schema mappings that are such that for
every set of sources there's a unique
database that corresponds to them so
these are functional schema mappings
and they are also known as views and
because the target schema is also known
as a global schema this approach is
known as global as view so the first
problem that we want to define is simply
what does it mean
what should be the the meaning of what
should be the answer to the query over
the target schema when we have some
sources so in this case it's completely
straightforward since there's a unique
database there all we want to do is
compute this this query over the target
database so this both gives me a
definition of what the answer should be
and it also gives me a method to compute
it right I can create this database and
then run the query in general I do not
necessarily need to create this
intermediate target database I may just
go to the sources and figure out just
the pieces of information that I need
and then put them together to get the
answer but we want to go beyond views
over the last few years there's been an
or actually over the last quite a few
years there's been an evolution in more
and more expressive mappings so these
kind of mappings that I told you about
are global as view there is another kind
of mappings known as localized view
which is not a strict extension of
global as view and which it really
doesn't make sense for me to tell you in
detail now but then there is global as
view which is an extension of the
previous two and now people are evolving
towards more and more general mappings
so roughly this global as view mappings
are maybe from 2003 maybe leave it
earlier so maybe I got this wrong maybe
it's like 2000 wanted sorry 99 ok so
sorry I got this wrong way by 4 years
and what I want to concentrate on now is
in a way the new frontier which is
considering more general mappings given
by constraints the advantages of these
more general mappings are we have more
expressive power to define relationships
of course but more important advantages
especially for scalability are that we
get simpler target schemas simpler
specifications of the schema mappings
and that we can manage the sources
independently when you're just doing
information integration by using of you
then every time you have a new source
you have to update this view of the set
of views where when we use constraints
we just create new constraints that talk
about the new sources and we don't have
to change anything about the other
sources
so in particular in my opinion this is
one of the things that enables
large-scale information integration so
how do things look under these kind of
schema mappings so in this case we have
a set of sources and instead of having a
single database that corresponds to
these sources there might be several
databases that correspond to these
sources in fact in most cases it's an
infinite number of databases so one way
to think about it is that the schema
mapping is slightly under specified it
doesn't tell me exactly everything about
this target database but it gives me a
bunch of information about this database
and there's an infinite number of
possibilities fitting into this pattern
so the first question that we face here
is just formally how do we define the
answer to our query before it was clear
because we had a single database but now
we have many databases so and we need a
single answer to your query so
conceptually what we do is we compute
the query over all these possible
solutions we get an answer for every
solution and then we put them together
by computing the intersection of all
these answers and this gives us what is
known as certain answer semantics I want
to emphasize this is just a definition
of what the answer to the queries should
be so in principle you don't need to
know about all this intermediate
infinite number of the databases that I
call solutions what you need to know is
just that there is a source the set of
sources over some source schema and you
have a query over some target schema and
you get the answer to this and we have a
definition of the answer to this query
but the problem that we face now is how
do we compute this the answer to this
query right the the method of actually
completing the query over every possible
solution won't work in most cases
because I have an infinite number of
solutions so it turns out that in the
context of some special constraints that
is what previous work has considered
there are some special solutions which
are known as universal solutions which
our databases that have two kinds of
values on one hand they have just
regular values which we call constants
and they also have some special values
which we now call variables or labelled
nulls and these databases are such that
if you can execute the query on them and
then we
move to Knowles you will get exactly the
certain answers so at this point the
picture looks somewhat similar to what
we had with views we get really still we
get this one database a universal
solution that still allows us to compute
the certain answers but we have gained
some expressive power in the kind of
schema mappings that that we consider so
since I've been talking about schema
mappings let me just show you a little
bit what a schema mapping looks like so
of course my examples are all going to
be super simplified so assume for
example you have two companies that have
merged and each of them has information
about people for example the source one
has information on people this is in
this format there is a person ID field
which is a key and then I have name work
phone and come from and the other source
has two relations one is an address book
and the other one is a phone book and
the company decides to merge this
information together by creating a
target schema that contains person and
phone but these are two separate
relations and they should be
conceptually they are connected by the
personality which again should be a key
for person so conceptually the
information flows in this way in the way
that the arrows indicate so in a way
this this graphic diagram is an informal
specification of the schema mapping but
I need I really care about exact
specifications of schema mappings and
those are given by constraints so the
way I specify the schema mapping is by
given really logical formulas of this
kind intuitively what this constraint
mean is the following I mean take for
example constraint number three yeah
so it's about
No so they could overlap you know
yep we don't assume anything at all at
this point there could be some overlap
and there could be you know some names
appearing in one or some names appear on
the other
I asked you anyway to not torture me too
much but thinks about keys about whether
I got all the things right because it's
really intended to be a very simplified
example there are many kind of issues
I'm hiding here
so take for example constraint number 3
which says that whenever the person ID
and the name appears in the directory
relation which I have abbreviated here
then that person ID in that name should
appear in the person relation on the
target side the - this means you can
think of them as variables that do not
appear anywhere else or basically just
information I don't care about so these
constraints the thing on the left is
called a premise the thing on the right
is the conclusion and these constraints
are source to target which means the
premises are over the source schema and
the conclusions are over the target
schema so they are intended to tell me
how the information flows from the
source to the target and look at
constraint number five for example this
is an example of a more complicated
constraint here we have name and phone
number in the phone book relation but on
the target side I don't have any
relation that has name and number
together so I actually have to split it
over to relations person and phone and
what I know relates this to a piece of
information is the person ID but I don't
have a person ID so the way I specify
this in the constraints I just simply
say there exists a person ID and there
exists a kind of phone number that
completes this information on the target
side and this is an example in which
they constrain under specifies the
solution so conceptually what I have is
an infinite number of data based on the
target side with all possible choices
for personality that you can think of
but the important thing is they have to
be the same in the person in the phone
relation and this is an example of
something that you cannot easily do with
views so there's no easy way with just
views to implement this functionality
that you start with piece of information
they are related in a relation on the
source and then that has to be somewhat
split over the target side and that then
you still have to be able to put it
together
yeah so but but then I can come up with
examples that cannot do that but I just
wanted to address that at one issue but
I agree so more formally the kind of
constraints that we care about our
logical formulas of this kind were fiims
ir conjunctions of relational atoms and
these kind of dependencies are known as
embedded dependencies the kind of
constraints that I've shown you in the
previous slide are examples of these
kind of constraints and we often
consider source to target constraints
and target target constraints but later
on in this talk I will also be talking
about more general constraints that are
not limited to one of these restrictions
so yeah
so you have kind of two things two kind
of informations if we're talking about
source to target constraints which are
the ones I've shown you so far when I
had dashes on the left it's not so much
unknowns they're just pieces of
information that you don't care about
that you're not going to transfer to the
right yeah and they're just variables
that do not appear anywhere else so each
of those dashes is just one more
variable that is just simply not
mentioned anywhere but on the right
whenever I have an unknown that
corresponds to an X essentially
quantified variable so really the
unknowns correspond to existentially
quantified variables they correspond to
this y y y1 through I am again I'll go
back to the slide you see on slide five
on constraint number five for example
they the two unknown piece of
information at the personality and the
kind of fun okay so now I will show you
an example that's really intended to
illustrate three kind of things at the
same time when one hand is supposed to
show you how we use this constraints and
it's also supposed to use to show you
what a universal solution looks like or
in fact how any solution looks like and
remember a solution is just a data base
over the target schema that is related
to the sources by the schema mapping and
it's also intended to show in a way how
I would go about computing a universal
solution but you know all you have to do
is just get the general idea out of this
so assume that sorry I saw my start with
this very simple example where I have a
single tuple and source 1 and just two
tuples and source 2 and I want to come
up with some database that relates to
these sources in the way specified by
the constraints so what do I do
well I start by looking my constraint so
I look at constraint number 1 it says
that whenever
person ID and work phone appear in the
directory table then they also have to
appear on the phone table and also I
have to fill in with the constant work
so I'm just forced to put that couple in
there
similarly for constraint number 2
whenever a person ID and home phone
appear in directory I have to put them
in the phone relation constraint number
3 tells me that I have to transfer this
person idea name to the target relation
person and now I have to somewhat more
complicated cases take on
number four it says that whenever the
name appears in the address book never
mind
the address that's what the dash is
intended to indicate then that name has
to appear in the person relation and
also some person ID has to appear in
there but I don't know what it is so if
I was creating a just an arbitrary
solution I could just put in there
whatever value I want but since I'm
creating a universal solution which is
intended to somehow summarize the space
of all solutions I just put a variable
in there which just tells me there
should be a value in there and similarly
for constraint number five which is a
little bit more complicated its phone
number has to be split I mean the name
and the phone number have to be split
over the target and we feel what we
don't know with variables but we have to
put the same variable Y in the person
and phone relation because it's the same
PID and that's what will help us to put
this information back together so as I
said these values are known as variables
and this thing together is known as a
universal solution so it's just a
special database that contains variables
or labelled nulls in our one to show you
yeah they are different just by giving
them different variable names that's
enough for me at this point so that
that's all I need so now I want to tell
you formally what a universal solution
is yeah we will see this in a couple of
slides but basically the space of all
solutions are databases that will you
assign specific values to these
variables and you may have additional
tuples and in some cases you might give
them the same value so formally as I
said we consider databases that have
constant and variables I have to
introduce you to this concept of a
homomorphism which is really fundamental
to the theoretical part of the work so a
homomorphism from a database H of s B is
just a function that sends values from a
to values to B such that every tuple
that appears in a maps to a topple that
appears in B
and it has to preserve the constants a
more intuitive way of thinking of a
homomorphism it's just an assignment of
specific values to the variables and I a
solution you for a source s under the
constraint Sigma is something such that
s a new together satisfy the constraints
and a solution is universal if it also
has a homomorphism into every possible
solution and I will illustrate this with
an example right now so this is a
universal solution that that I've shown
you just a second ago and this is what a
arbitrary solution looks like so we've
said that we have to have a homomorphism
from this universal solution to the
solution and in this case the whole
morphism just simply maps X 2 3 3 3 y 2
4 4 4 and Z 2 home and notice that when
I assign these values to the variables
then I do get a corresponding tuple in
the solution solutions may also have
additional tuples yeah and now I want to
show you how we use this universal
solution to actually compute the answer
to your query so the basic idea is we
just treat this universal solution as if
it was a regular database so if I want
to answer the query give me a name from
the person relation I simply run the
query it gives me 3 topple three tuples
which has each a single value and that's
exactly the answer that I want this is
the certain answers by the semantics I
introduced before to this query and
another way to think about it this is
all the names that I know about coming
from the different sources now if I ask
a query give me personality and name
then I get these three pieces of
information and join an ad with
variables and intuitively what the
variables tell me is I don't know what
the personality is here so if I want to
get the certain answers I just discard
the tuples that mention variables and
this gives me an and the personality for
an and intuitively what this is is this
is the only information that I know
coming from the sources remember in my
example I had three tuples and this is
the only table that has personality a
way to think about this from another
perspective is that when the user asks
this query the system had to be clever
enough to realize that
in order to answer this query it only
has two it should go only to the source
that contains personality and name and
ignore the other sources that provide
names but do not provide person IDs were
for the previous query I went to all
three sources and got names from all
three sources and just one more example
of a somewhat more complicated query
here's a query that asked for name and
phone number so it's tries to put a get
the information that we split before and
notice that we get the correct
information for edy which we got by
doing a join through the variable Y but
the variable Y doesn't actually appear
in the answer so I don't have to discard
any of this information and this is what
appears in the certain answers okay so
all this was intended to be kind of an
introduction a summary to the previous
work and I will now introduce you
formally to the previous results and
then talk about my contributions to it
so I'm skipping a lot of previous
results and I'm only concentrating on
like two technical results that I care
about so these results are from faking
colliders pillar and Miller and Papa
from 2003 they consider the setting were
the queries are unions of conjunctive
queries which intuitively correspond
basically to sequel queries without
anything fancy just Union and the
constraints are source to target and
target embedded dependencies so these
are yeah
you need to join you see the by which is
another view but then if you'd select
name and peers that you wouldn't return
a null that you was under it's kind of
like so the basis do but you cannot
recreate the ability to join on the
Bandit which you didn't know what they
do is see the same it's like well you
can't join so it's kind of well because
how I got here is we have a special kind
of nulls which are labeled null so in a
way this label nulls give you a little
bit more information so in fact the only
information that you need here is to
know that it's the same way appearing on
both sides and that's exactly what you
need for a join so so that's what that's
the only information that we're using
from the null well first of all in this
query it didn't ask for it right in this
way yeah it's just simply because I mean
on the conceptual side simply I was
asking for a query that gave me
personality and name so you want to get
those pairs where you know the
personality and the name on the formal
side the formal the certain answers
semantics require me to answer exactly
that in to know give me the information
I don't know so one way to think about
this label variables is they retain just
just enough meaning for you to do things
such as joints and not a lot more
right okay so the case that I consider
is the case where you start with a
database that has not all your sources
do not have nulls so if you start with a
basis that had nulls to start with you
would need to have kind of like two
classes of nulls you know they the
original nulls and the new nodes and you
would have to distinguish between them
so as I said in in this particular set
of previous results which is the ones
I'm concentrating on and there's
previous work and I'm sure there's work
of in particular alone Alevi right here
if I consider the case where these
people consider the case where the
queries were unions of conjunctive
queries and they mappings were source of
target and target embedded dependencies
which are another way to think about it
they're glove mappings that also have
target constraints and basically the
main results are just two that the
certain answers can be computed
efficiently from a universal solution
and that's what I just showed you in the
previous slides and next that the
universal solutions can be computed
efficiently by a procedure that's known
as the chase this is a procedure that
has been around in databases for a long
time so in a certain sense this reduces
the query problem to the data exchange
problem the data exchange problem is the
problem of actually creating a database
so this shows you that if you want to
answer a query there is a particular
database that you can create this
universal solution and then after that
that one is helpful to you to be able to
compute answers to queries in general
it's not clear that those two problems
need to be related so the work that I'm
interested in so this previous work is
based on universal solutions and those
universal solutions are good for me when
they happen to exist and when they
happen to give me the certain answers
but and they happen to do that in in
certain settings in particular the
setting that Fagin and others have
considered but I need to go beyond these
queries and mappings because I want to
be able to handle more expressive
queries and more expressive mapping so I
will need to go beyond universal
solutions and that's where the technical
contribution of the is going to be and
the techniques that I develop in order
to be able to handle this allow me to
also address
other general problems in databases such
as quick containment under constraints
and implication of constraints so my
results in this area which are joined
with Alan Deutsch and Jeff Rimmel are
the following we clarify when the
universal solutions exist and when they
are sufficient to compute certain
answers the two things are not the same
you may have situations where there
Universal solutions but they don't have
certain answers I have a couple of
slides with the examples but I'm
delaying that for after my talk is
finished so if you want to see the
details I'll show you at that point
otherwise it gets kind of long it's a
somewhat technical example in order to
be able to handle wider classes of
constraints and queries I do two
technical steps these two steps are you
know seem relatively small but they
really allow me to address much wider
classes and they also require me to
significantly change the basic algorithm
the chase so you could say that in this
previous work the chase was used in a
really very primitive way essentially
it's just like it's a very simple use I
need to use make more sophisticated use
of the chase and I need to make my chase
a lot more fancy to be able to achieve
this the the two technical changes are
the following instead of going to a
single single solution that is universal
I go to a finite set of solutions that
universal and the other change that I do
is instead of considering just
homomorphisms
I consider other kinds of mappings such
as embeddings for example and in doing
this I extend the connection between the
exchange and data integration to a wider
setting so I show that under many other
cases it is enough to first just compute
a few databases and then those will be
enough to answer many kind of queries
that I care about
so here there are many technical results
and I'm only giving just your an outline
of them of the main ones so a universal
set solution is a finite set of
solutions that together is universal for
the set of solutions that is for every
solution T there exists some database
you in my Universal set solution W such
that U has a homework film into T that's
what the arrow here is intended to
indicate so in a sense a universal set
solution just like a universal solution
is an approximation to this possibly
infinite set of all solutions and really
intuitively what you want is you want
this approach
nation to be good enough for the query
that you're asking so if the if you get
a good enough approximation for the
query then you get the correct answer to
the query so how good of an
approximation you need will depend on
the expressive power of the query that
you're asking so this is how we use
Universal set solutions we have the same
picture as before and there are some
solutions that are special together they
make a universal set solution and in
order to compute the answer to the query
I just run the query over each element
of this finite set of solutions and then
I put them together by discarding the
variables and intersecting so on the
picture this looks like small progress I
went from 4 solutions to 2 but of course
in the general case the important gain
is that I've gone from I usually
infinite number of solutions to a finite
set of solution so this changes from the
problem being might not even knowing how
to compute it to my actually being able
to compute the certain answers and the
other changes I said is to consider
different kinds of mappings so there is
many different kind of mappings that we
may consider for example injective
homomorphisms or embeddings and it so
happens that each of these correspond
somewhat naturally to certain classes of
queries for example queries where you
add negation or queries where you add
inequality or even more general queries
I don't think there's a lot of point in
my really describing each of these
queries at this point so this is kind of
a picture that is intended to kind of
summarize the progress I've made and
it's you know it's somewhat biased in my
favor so the the global as view case
it's obvious what we need to do as I
said in the case of global as view my
target database is really completely
defined by a view so once the database
is completely defined I can run whatever
query I can I know how to compute on any
database so on the horizontal axis I
have queries like UC Q stands for unions
of conjunctive queries then you secure
with negation you seek you with negation
and in equality which really corresponds
to just general existential queries and
then I also consider any kind of
monotony query of arbitrary expressive
power and by
monotonic I mean that if you shoulda
query on a larger database it will give
you more more results monotonic queries
are not a strict extensions of the
previous classes but it's just one it's
generally more expressive so as I said
this previous this first line of this
chart was pretty obvious how to do as
soon as people thought to do integration
with views they essentially the recent
research is in this part of the
landscape where glove plus target is the
kind of constraints I discussed before
but so those are constraints were those
are mappings for my constraints are
limited to being either source to target
or targets a target but I may want to
have more general mappings where I don't
limit myself to having only source in
the premise and only target in the
constraints and so I may as well allow
completely general embedded dependencies
that's what the next line indicates and
even more general constraints are just
PI two constraints or Universal
existential formulas to allow negation
and disjunction so disjunction is
particularly powerful so the the first
advance towards this general mappings as
far as explaining how to compute certain
answers was this work of Fagin and
others that I just summarized a moment
ago and where my contribution is is in
showing a way to compute queries in this
area and again this is joint work with
Allen Deutsch and Jeff Rummel I want to
clarify one technical thing which is
that in this work in the oneself by
Fagin and others the restrictions are
enough to guarantee that Universal
solutions will always exist so what I
cover here and the rest is not quite as
good I show you that if Universal set
solutions exist with the kind of
mappings instead of homeworkfilms that I
need for my queries then I will be able
to compute the queries but they do not
always exist I do provide conditions
that allow me to check for this so it's
it I just want to be honest about this
that it's not quite as good so I've
talked about Universal solutions and
universal set solutions and as I said
once I have a universal solution I know
how to compute the query and I know how
to complete the query efficiently I need
to close the first part of the gap how
do i compute this universal
and as I said there is a well-known
algorithm for this the chase has been
around since I think late 70s in
databases and it happens to give me a
universal solution this is an algorithm
that ghosts possibly through a loop may
iterate and it does not necessarily
terminate previously when I was showing
you how we create a universal solution
where I was showing you how we apply the
constraints that was a very very
simplified version of the chase I don't
have time to give you a complete
introduction to the chase now but
basically it's an iteration that
proceeds like that it says what
constraint fails what am i what is the
minimum work that I can do to make this
constraint not fail and then it iterates
this animator it forever because
sometimes constraints may interact with
each other causing me to have to put
more and more data into my solution and
it may never it may not terminate but if
it terminates it is guaranteed to give
me a universal solution that's a very
important property of it and because it
may not always terminate and of course
we're interested in termination
conditions so around 2003 people
introduced a very useful set of
conditions that it's quite general that
is known as weakly acyclic so we say
that Sigma is weakly acyclic if we
construct a certain graph based on the
Sigma and then we check whether there
are certain cycles in this Sigma the
important aspect about this condition is
that the condition is effectively
checkable so if you give me Sigma I can
check fairly efficiently whether it
satisfy or does not satisfy this
condition and if I get this condition
then I know that my chase terminates and
then I know I have a universal solution
so let me say this relative to the kind
of constraints I talked about before
source to target constraints will always
satisfy this condition the chase with
them will always terminate but I also
have even in the simple case I also
consider target to target constraints so
those are the ones that may potentially
not terminate and the case that people
have considered is precisely the case
where the target constraints are weakly
a cyclic to guarantee this termination
so my contribution in this area again
joint with Alan Deutsch and Jeff Rimmel
are the following we first show how we
can extend the the chase to obtain what
I described to you before was useful
Universal set
questions and other kinds of mappings
this extension of the chase really
follows an evolution that went in
several steps first Alan Deutsch and Val
Tannen introduced an extended chase that
allows to handle disjunction so this the
regular chase proceed step by step by
starring with a database and then doing
something to the database to obtain a
new database and it just keeps iterating
in at every intermediate step you have a
database in order to handle disjunction
you need a branching chase so it's a
chase where at every step you have a
finite set of databases and then you may
need to branch on some of the databases
and you may get more databases at the
next step and then Alan Deutsche Bertram
Buda Chennai extended this to handle
negation in 2005 and then a lien Jeff
Rama lanai in extended days to be able
to simulate mappings other than
homomorphisms so this is fairly
technical so I thought it's probably
best if during the talk I don't present
the details of exactly how this happens
you can catch me later with questions
then we also provided new conditions for
chase termination which strictly extend
the previous condition so they're
basically better termination conditions
and they can also be checked efficiently
and they are fairly technical to
describe they also involve checking for
conditions in a certain graft I
construct but there are cases that of
constraints are not weekly cyclic but
which my conditions can handle just to
give a very quick intuition of what
these conditions are imagine that you
had just a very single simple case where
my database is a binary relation so you
can think of it as a directed graph this
always makes it easier to come up with
simple examples so imagine you had one
constraint that said if something
happens I have to introduce a two-cycle
into my graph and then you have another
constraint that says if I have a three
cycle then something should happen with
the condition of weakly acyclic doesn't
capture the fact that the first
constraint can never fire the second
constraint but my conditions include
this fact so when I started thinking
about this problem I also started
thinking about a problem that apparently
nobody had thought before which is
this people have been working with the
chase within the context of universal
solutions but also within other contexts
in databases for quite a while but I
don't know of any work that tried to
really characterize what is the chase of
postal produced in other words this is
an algorithm that that's something for
us so be interesting to know what what
was supposed to be its end goal so here
I'm going to give you this result recast
in terms of universal solutions because
that's what my talk is about but this
result is really quite a bit more
general so I said that the chase is an
algorithm that finds Universal solutions
in fact I think it's the only algorithm
that we know to find universal solutions
so a natural question to ask is that's
the chase if there is a universal
solution will the chase always find it
so it's the algorithm complete well it
turns out the standard chase is not
complete so there are cases where there
is a universal solution but the standard
chase will go into an infinite loop it
has to fall in an infinite loop because
I already told you that if it terminates
it does give Universal solution and so I
extended the chase to I produced a new
chase that I proved was complete so
whenever there is a universal solution
this chase terminates here there is an
issue about whether we talk about just
finite moles or infinite models but I'm
not going to get into the technical part
here you can ask me later yes I mean
it's just for a universal existence all
formulas I actually have an even wider
extension but it's for arbitrary
Universal existential formulas you can
have negation and disjunction so
remember this doesn't tell you that it
will always find you a universal
solution or it will always terminate but
it will tell you that if there is a
universal solution then this chase will
terminate it cannot do better than that
in fact I also proved something else I
characterize what a chase like algorithm
means so I provide a formalization of
what is a chase like algorithm and I
prove that basically all of those chased
like our algorithms produce universal
solutions and so none of them can
terminate if there is no universal
solution
so to summarize this this part of the
talk where I concentrated on day on one
aspect of the integration the query and
the query problem these are my
contributions I have introduced a
complete algorithm for computing
universal solutions so from the point of
view of completeness this is an
algorithm that's better than the
existing ones and this is an algorithm
that has been around for a long time so
I'm kind of happy to make a contribution
to it also I have extended the chase to
give me universal set solutions in
solutions that Universal under mappings
other than homomorphisms previously
people really considered focused on
universality and their homomorphisms and
I really provide a very general
framework to provide even more general
extensions here this allows me to kind
of handle wider classes of Croatian
mappings then I also provide a better
termination conditions and in general I
basically provide a better understanding
of what is needed to compute certain
answers that it's in my paper but it's
not in in the talk so now I want to move
over to another problem that I'm going
to talk about a little more briefly I'm
aiming to finish I don't know maybe
about 11:15 11:20 because I had to start
later so in this problem we consider the
following situations our input is a
source is a bunch of sources over a
source schema in a schema mapping and
the output is just a database that's
related to the sources by the schema
mapping so we're just trying to produce
a database that somehow puts together
the information from the sources in the
way that is specified by the schema
mapping and again I want to maximize
expressive power and efficiency and
there are some trade-offs between those
so the first question that we have to
ask ourselves when the mappings are
under specified when the mappings are
not functional there is a choice of what
I'm going to materialize if I have a
view the obvious thing to materialize is
the answer to this view but when I have
mappings that give me sets of solutions
I have a choice as to what I materialize
so in principle I can produce any
solution remember that I'm using here
the word solution and technical sense by
solution I mean at the
over the target schema that corresponds
to the constraints so I can produce any
solution which may be good enough for
some applications I can produce a
universal solution which I know in some
sense is better because it allows me to
answer queries so if it doesn't have
extra cost I might as well do B but I
can do even better Universal solutions
may have extra values in particular they
may have extra tuples with extra
variables but there are also minimal
Universal solutions this correspond
intuitively to minimize queries and it
so happens that in the setting of glass
with target constraints Universal
solutions are not necessarily unique
they are unique up to homomorphic
equivalents but they're not unique up to
isomorphisms but minimal Universal
solutions are unique so in some sense
this minimal solutions are are the best
deal because they are unique so from the
theoretical point of view I'm happy
because I know I'm computing the answer
they are better for query answering
power although really they are not
better than arbitrary in Universal
solutions but they also are smaller and
a third they are the smallest ones that
have this property so we know how to get
B efficiently by by chasing and what
this part of the talk is going to
concentrate on is how do we achieve C so
how do we go from B to C how do we make
this smaller and in the paper were
faking colitis and Popeye introduced
course they argue that we might as well
materialize the core if we can do this
efficiently so the problem with Mini
minimizing though is that minimizing a
general database with variables is known
to be an np-complete problem and we
certainly want to be able to do this in
polynomial time
so in the paper where they introduce
course Fagin and his co-authors handle
the case where target dependencies are
very very simple they just have
equalities in the conclusion so this in
particular in basically our functional
dependencies so then your god Loeb in
2005 extended this to more general kinds
of target dependencies that include
either don't have any existence of
quantification or have target
dependencies that have a single atom in
the
but at this point we were in the
situation that there were many cases
where we could compute universal
solutions efficiently but we could not
minimize those Universal solutions
efficiently so obviously what what we
were aiming for is to be in the
situation where whenever we had a
universal solution we could also
minimize it efficiently and that's
precisely the gap that your God loved
and I closed in a paper that is to
appear in port principles of databases
2006 we show that any kind of target
dependencies that satisfy this
termination conditions I have talked
about before that for those kind of
dependencies we can efficiently do this
minimization process and so this means
that whenever we can compute in a vs.
solution in polynomial time we can also
compute their course in polynomial time
so this means that for the case of glass
+ target it's a good idea in fact to
compute this this core of course this is
if you're a teratogen and you care about
things up to polynomial time so now I'm
going to give you a somewhat more
technical part that is going to give you
an idea of how this algorithm is
supposed to work so how much more time
should I take here are we running out of
time or okay so should I just go faster
through this okay so I'm just going to
then I'll have to skip this part so I
had a basically an outline what of what
the algorithm was which is pretty nice
so I'm just going to go to my to
finishing slides
so to summarize in this talk I talked
about my work in foundations of
information integration and in
particular I concentrate on just two
problems the query problem and the
problem of computing the core of a
solution the second one is to appear in
principles of databases 2006 I've also
worked in other areas of information
integration in particular in privacy in
composing schema mapping were the
pioneer paper bias was by Alan kalevi
and in answering queries using views so
about privacy I just want to tell you a
couple of words when we have the
situation where we have several
databases and we'll provide some kind of
an interface via target schema
we may want to do this not only just to
put together the information but also to
hide some information so you may have an
interface that allows you limited access
to the sources so that's the situation
that I considered in privacy and then
the question is this there's some kind
of a secret over the data sources which
we model as a query over the data
sources and the question is this can an
attacker that is a legitimate user in
other words I don't conceal the
situation of physical attacks or
anything else but can an attacker that
issues a bunch of queries over the
target schema use arbitrary
computational power to put together
those answers and figure out what the
secret was so in that work I provide
guarantees and I provide complexity of
checking those guarantees in the problem
of composing schema mappings it's a
technical problem as I said it's a
problem that's very fundamental in
schema mappings and to pricing it has
now been studied until the work of
Madhavan and halevi in 2003 but I don't
have time to explain to you and the last
is answering queries using views so my
work has kind of gone from more applied
things to more theoretical things on the
more applied things is for example this
work on composition of schema mapping is
something that I started at Microsoft
Research where we started with just
investigating what coding could not be
done and then that actually led to an
algorithm that we you know we were
actually went and got the deists or
there are structures that people
actually use on the product side and
then we actually did a lot of real-world
optimizations and this is just something
that it's been just submitted and this
actually led to some patterns within
Microsoft on the more theoretical side I
have done work in the theory of query
languages in particular on the question
of is there logic for polynomial time
which is very related to the question of
SP equal to NP and I also have
investigated the power of
diagonalization in separating
computational complexity classes which
is also very related to P versus NP and
this has been published in the European
counterpart to Potts and in the
conference of computational complexity
and my co-authors in this work are Phil
Bernstein Alan Deutsch you're got log
from Oxford Russell in Palazzo from UCSD
virtual Euler circuits now at UC Davis
sergey melnik from Microsoft
Jeff Rommel from the mathematics
department at UCSD and Victor V&amp;amp;O from
UCSD and to summarize you know this is
what I'm supposed to have kind of some
slogans so you can take this with a
grain of salt but basically the kind of
thing that I'm working is foundational
research that's motivated by practical
problems that seem practical to me and
more generally one of the things I'm
trying to do is to help to develop kind
of a general toolkit of information
integration primitives that we can then
put together to solve different kinds of
information integration problems so the
fundamental entity that seems to be
emerging an information integration is
the schema mapping so in some way the
principal object is a schema mapping and
what we want to do is find out what
kinds of things we can do with schema
mappings so these two problems I
discussed are two particular cases of
what we do with schema mapping so one
was move a database from one side of the
schema mapping to the other and the
other one was you have a database on one
side of a schema mapping in a query on
the other side of the schema mapping
anyone answer the query and the kind of
techniques that end up using during this
work is techniques from logic and from
graph theory and then you know there is
this workhorse the algorithm known as
the chase that's been around for a while
which is fundamental importance here
okay that's pretty much it thanks a lot
is there a way of computing the answers
to queries without actually universal
solution yeah so actually that's the
work that I am doing next there is at
least a couple of August of the
mutations that you can think first of
all you could kind of analyze the query
and say which part of the universal
solution it needs so what parts that we
need to piece together but what really
needs to happen is this this is of what
I feel that the frontier is is we need
to use this theoretical work on
computing the universal solution but
what we really need to figure out is
rewriting so if like what really needs
to happen is when the query comes in we
figure out queries that we can send to
the sources so the sources can answer
and then we just put just that
information together that's where I feel
like the frontier is right now and if
you want to go even one level further
from that if you want to be really
clever you might want to say well you
have many many different sources and you
you know you you might have costs
associated with this and you may want to
actually assemble your answer at local
add nodes that are local and close to
each other and then kind of assemble it
as you go so that but that's a big
research project basically but yes
okay thanks thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>