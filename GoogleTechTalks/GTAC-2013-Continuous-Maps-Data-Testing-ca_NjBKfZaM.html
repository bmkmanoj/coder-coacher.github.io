<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2013: Continuous Maps Data Testing | Coder Coacher - Coaching Coders</title><meta content="GTAC 2013: Continuous Maps Data Testing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2013: Continuous Maps Data Testing</b></h2><h5 class="post__date">2013-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ca_NjBKfZaM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we are definitely in the final stretch
here of G tak 2013 we have two more
lightning talks that i'm going to
introduce here for you and then we're
going to have an academic talk and then
we're going to round out and finalize
the day with security so definitely
stick around I have a couple more of
these droids underneath the the podium
here so with that I'm going to introduce
ivette namath and Brandon dine and they
are going to be talking to you about
continuous Maps data testing and with
that here you go of it hi I'm Yvette
nameth and this is brendon Dean and we
are both Google testers on google maps
if you didn't see the video earlier you
can take a look but I was the person in
that lovely video that Tony showed at
the beginning of the day and this talks
actually going to talk a little bit
about giving you a hint as to how we
actually do the testing that I was
talking about there so why are we doing
this well take a look something might be
a little wet in Mexico maybe a little
global warming going on maybe the entire
west coast of Mexico is entirely flooded
this is what can happen when a Maps data
bug actually occurs and this is not
actually a software bug this is actually
the raw data that we're using to build
the maps images so let's talk about how
maps get rendered well we have all this
data that's in a large repository it's
coming in from all these different feeds
and creates that world data repository
that you see on the left in the middle
we have a data processing pipeline
sometimes known as our rendering
pipeline which generates images based on
all the different features that are in
the world data so things that would be
in the world data would be features such
as locations city's restaurants roads
etc each one of these has an Associated
geometry which we need to then create a
style for which would potentially be
like polygons for a park containing a
fill color and a stroke width and a
label so if that raw data is crap
obviously the map
coming out is going to kind of look like
Mexico did it was missing a big chunk of
land well we're at a testing conference
so what about testing this data we
actually want to test every single piece
independently we can't just test the end
product i'm currently primarily focused
on testing the end product but in order
to get there i had to first test the
world data with Brendan cool so moving
on there are some patterns and aunty
patterns we want to share with you I'm
not sure how many of you have seen that
photo of the donkey lying down in the
sand this was actually taken for a
street view car a while back and made
its way around CNN just to set the
record straight the donkey is still
alive is not dead um it actually was set
up there and it's just taking a nap how
is this partly into data testing well if
you notice you could test a lot of
things in this picture you could test
whether the donkey is dead or alive that
would be a big feature or you can test
if every single grain of sand in the
picture still exists and you could see
this sort of with beta testing as well
let's say we have new york city do you
want to test the exact geometry of every
ZIP code in New York City do people care
if the geometry shifts one bit or
another or let's say do you want to test
the name of every subway station um you
want to make sure that important
features exist like New York still
labeled granted for a user experience
some things don't really matter and part
of the trick in doing data testing is
determining what matters and what
doesn't um for a naive approach you
could easily do a simple diff test this
sounds great initially and you're going
to be like oh yeah I'm going to test
every feature that won't work and it
doesn't take too much to realize it but
just to throw some numbers out let's say
you had a hundred million different
geographic features in your data dump
what happens if you had a one-percent
failure rate would you be able to triage
that in a reasonable amount of time
and by reasonable we mean time to make a
launch things that you really care about
so moving onward that's all great but
how might you come up with a reasonable
solution and this picture here I picked
it out actually from my Christmas
vacation to Barcelona it's the Sagrada
Familia it's been under construction for
ages now it will probably be under
construction until we're all dead and
beyond there you can still see the
cranes there and they're actively
working and that's sort of like David
testing you need to start and you need
to have some sort of a plan but you need
to keep building up your corpus as time
goes on you want to keep up and go
through and test and test and test but
you want to do it carefully you don't
want to do a simple diff test you want
to look at things like statistical
analyses I mentioned that like if every
subway station changed in New York City
that's something you might care about I
mean there was actually an episode in
google maps where for a very brief
period of time we might have displayed
additional subway stations in New York
City like by a factor of two and chose
some really awkward names for the
subways for those of you who don't know
apparently the New York City subway
system was actually contrived of two or
three different systems and when you get
data from your third-party provider they
may use the historic name so we were
displaying some IRT and bmt names and
when you had stations that in our view
of the world sort of joined together
well we stopped doing that and that sort
of type of day you want to avoid and you
can do that with some some structured
diff tests that test exactly what you
want you want to look do my name still
make sense for these critical features
you would want tests a new york city
exists washington d.c exists and that
sort of a basic smoke acceptance test
what you want to do though is assuming
that those paths you also want to let
see if the state of the
world has changed or not you want to
look and you want to see Oh have my
oceans changed with or within a given
region has the makeup of the region
changed let's say you're doing a data
dump and you're processing it for export
when you find it odd if say a given city
doubled its number of aquariums might be
a bit concerning and along those lines
let's say you had a city that had a lot
more area from airports maybe an entire
Airport went wrong is now covering a
city it can happen so if you try to
build up tests like this once it build a
statistical corpus and divide the world
into regions you can make a lot of
progress what you can do is you can
actually take each geographical region
say a metro area or something you know
you care about and it's something that's
small enough to be understandable by a
human but not large enough so that
you're completely overwhelmed by the
details of what's changed and use that
as a basis for analysis then you can
think well if I had a manual tester what
would my manual tester be doing my
manual tester would probably look
through the area and say well do all my
red still exist do I still have the lake
that's in the middle of the city do I
still have parks these are tests that
you typically see a manual tester do now
we have computers and this is GTA we're
trying to automate this could we perhaps
have a system that goes through and
looks and listens and just tries to
interpret what's changed has there been
a drop out has there been suspicious
gain have you gained an entire new set
of features now assuming you can do this
on one region and you get parsable
understandable and human-scale results
you could probably speed it up even I
mean we're trying to do this fast each
region can actually be executed in
parallel now you have a testing
architecture where you have locally
specific
outputs that actually have quantifiable
and reasonable results that you can
understand and interpret before you need
to launch and that's sort of the sweet
spot you want to be in for data testing
and on that note we move towards
frequency so now that you know that
we're going to be doing meh produces
over all these different areas how often
should we do this we've got this very
very large repository that takes a long
time to make and by a long time I don't
mean minutes everything in the geodata
world whether it's that rendering
pipeline which can take up to a day or
data processing prior to that pipeline
takes we're talking hours some of these
processes take days to do all the
correlation so how often do we want to
do this well we push map tiles on about
a monthly basis but if you're trying to
test the data on a monthly basis you've
got a month worth of changes that come
in from all the different changes that
are happening around us in the
geographic world these are coming in all
the time we've got this product called
map maker that's pushing data changes to
us all the time we're getting new feeds
updated all the time with batch changes
so that would be like trying to drink
from a geyser it just would really suck
it would kick you back on your ass you'd
be bloody and you'd be picking yourself
up and fighting fires trying to get some
version of the world that was actually a
legitimate representation so we're not
going to do that and you know we're all
overachieving testers so we think that
we should test every change and that
actually really really sucks too because
like I described things are changing
every millisecond in the data and they
might be very small and doing this
parceling out of testing into map
reduces actually takes a long time to
unfortunately so every change is kind of
like drinking from a waterfall it's just
is never-ending barrage of water so
we're not going to do that we kind of
came up with this compromise and said
daily daily gives us a really good
signal it's something that we can look
at and say on this day we can see that
all these parks disappeared what
happened on that day was there a batch
change to a feed that might have parks
was there you know some public data
changed about them did some user come in
and decide to just delete all the parks
and why
in DC we don't know but now we at least
have a starting point that sort of gives
us a little bit of a temporal location
so we finally have our happy little
water fountain to drink from and that's
what i'll leave you with great great
thank you Brendan any vet so we have QA
so you can line up and ask questions
questions if you like we can take one
live and in like five seconds I will
have the moderator in front of me um my
takeaway why you get to go to Hawaii a
lot I think that that's what that's
Australia this is actually in outside of
Melbourne there are baby penguins and
little penguins you can find our do your
car okay all right over here I think
there's two questions but they're both
deciding who's going to go first please
don't be afraid um I'm just curious
about I have some users will sometimes
find errors since you can't check
everything but um how often would you
say that your test fine errors in the
data as opposed to user spending errors
um I think the scale of the error that a
user finds versus the scale of an era
that our test fine sir we're talking
massively different like Brendan said
where we're checking for the really big
things either the things that have to be
right like the Eiffel Tower has to exist
on the map because a majority of users
would notice that we're not testing that
like your parcel of land you'd like that
little outline is exactly correct which
is what like you know my aunt complains
to me about when i tell her i work on
google maps but how am I ever going to
know this so I would say that we catch a
majority of the large ones what I don't
know what if I'd give a back of envelope
percentage for that and I would say we
catch very few of the small ones unless
they are systemic like every parcel of
land is misplaced okay great next
question up over here to the right
please hi my name is Igor I work for
nokia maps and my question is do you
compile data if yes do Tess borrow and
compile data hmm that's an interesting
question and let's think about how to
answer this correctly so in terms of do
we test our third party providers coming
in and the actual data package that goes
out to maps exactly yes so the yeah we
we test both and can't really say much
more than that I only like my ND and my
job exactly yeah I always like these
questions great huh we have time for
just one more so please I'm Matt Dillon
I work for Google I'm so high level
question you talk about data and before
now we've mostly been talking about
codes the situation come up where you
have a data push that's totally valid
but it suddenly sets off a code bug that
maybe has been in production for four
months you know and Oh other mitigation
where I give any questions me what's the
mitigation strategy for that um
depending upon how bad the code bug you
found very fast roll backs are
essentially very key to doing a data
push we want to be able to see if we do
have a problem in production to turn it
off but also carrying your data and just
following good high release hygiene and
trying to simulate any type of failure
mode before it actually hits the user
great I don't that is the second most
common type of bug after the raw data
bug so great in with that thank you
ivette Thank You Brendan
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>