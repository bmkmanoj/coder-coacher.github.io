<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability: CARMEN: A Scalable Scienc | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability: CARMEN: A Scalable Scienc - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability: CARMEN: A Scalable Scienc</b></h2><h5 class="post__date">2008-06-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2m4EvnlgL8Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">computer science and director of the
Northeast Regional e-science Center he's
here to speak to us about Carmen okay so
this is a project which is a
collaboration between computer
scientists a neuroscientist so I should
say that I'm actually a computer
scientist so I picked up some
neuroscience along the way but don't ask
me any difficult questions about how the
brain works this this to talk there's
two strands lyst talk so basically
there's a new informatics it's been a
very challenging sort of science
application that we're trying to address
and then this cloud computing is a
particular architectural way of trying
to address these these challenging
problems and the reason we're all
involved in this this collaboration of
computer and neuroscience is because we
all believe that trying to understand
how the brain works is that is the last
great scientific challenge that we that
we have left at the moment they would
have huge implications in a number of
areas so not just in neuroscience but
medicine of course lots of drugs operate
on the brain biology biologists are
interested in how they the brain forms
as a result of our DNA and also computer
science so trying to understand why the
brain is much better at some things then
computers are and surprisingly there's a
very large number of scientists around
the world I was surprised when I saw
this figure 100,000 neuro scientists
around the world all working away trying
to address these problems and at the
moment there are a lot of problems with
the the way in which neuroscience works
so although it's very expensive to
collect the day and I'll give an example
later on have some very expensive data
collection then the debt is very rarely
shared and there are a couple of reasons
for this so the main reason is that when
scientists collect debt with stored in a
proprietary format so they buy some kid
from a manufacturer they do the
experiment the result is stored in some
particular file format they then write
analysis routines which is specialized
to that sort of file and so they can't
just give that that their data to
another group may have other analysis
routines because the chances are that
they wouldn't work with with their data
and secondly there they rarely formally
described the day that they're
collecting in a way in which others can
use it so the right in the log boot be
the type of experiment that they did in
perhaps write down the file name of the
the day that was collected but then
that's not computation amenable so it's
not possible for people just to go and
grabbed it and grab them that I did have
a look and under and understand it and
so a lot of science is supposed to be
about reproducibility then it's very
difficult to do this under these under
these circumstances so there's lots of
opportunities lost to to share data and
you know it sounds like I'm criticizing
neuroscience they're actually very
clever people as you can imagine and
really this is very common in science so
in the Northeast Regional Science Center
we work with scientists across a lot of
demand and it's very typical I came
across a guy Jeff balke who analyzes the
way science works and he has this
three-stage standard scientific model so
first of all scientists collect it then
they published purpose and then they
gradually lose it the original there and
so so whatever science you look at this
is the way it tends to tends to work
until you got all of these sorts of
problems you can't replicate experiments
you can't you can't reuse data and if
you look at a paper and you think I'm
not sure I believe these conclusions you
can't go back and get the original dirt
and reanalyze it so so this is where our
project came in so the idea was coming
to the rescue so we try to provide a
system which allows neuro scientist to
share data but also the codes themselves
because the criticism our men about debt
of being lost is true of codes analysis
routines which are often used by
scientists to generate that you see
published it's very rare that you can
get hold of those either so we're trying
to build a system which allows both of
these things to be shared by scientists
around the world and it's a it's quite a
large project as a circle aberration
between computer science scientists and
neuroscientist the world i'm going to
talk about which is mainly on the
computing side it's done it and your
castle where I am and also your
University then we have groups of neuro
scientists around the UK who are collect
data using the system to analyze it and
there's some companies who are
supporting a sinless so in the project
there's lots of different types of
neuroscience data that people work with
and in the project we're focused on
neurophysiological days this is where
you use typically electrodes but often
now video techniques and you look at the
activity of the individual neurons so
the idea is that you want to try and
understand the way in which the brain
works and the general feeling is the
brain works by using spikes or spikes
the way in which they restored
communicated and processed so you use
electrodes ustream the day trout from
the electrodes you work out where the
spikes are and you try to understand
what those spikes actually mean and the
bottom you if you can crack the neural
code then you get the get the Nobel
Prize for doing that because at the
moment really people only have a fairly
vague idea of how the brain works I
don't know whether you've ever if you go
to a university bookstore you buy a book
on neuroscience they're always about so
thick and if you're like me you start
reading them and after about an hour
you've learned more about the
limitations of your own brain and how
they how the brain works in general so
you start to you start to accelerate you
got faster and faster through the book
for the chapter on how the brain works
and you never find it and when I talk to
my neuroscience colleagues then they say
well yeah I mean that's the whole idea
these sorts of projects because we want
to write that that chaplet so it's not
it's it's not understood at the moment
but they're working on it so to give you
a particular example of rare data which
we would like to be able to share and
analyze so every month at the local
hospital in your castle there are
patients who come in who are particular
forms of epilepsy which aren't amenable
to the drug treatment and as a last
resort that insurgents will operate on
them and what they want is to analyze
data coming from these patients to work
out what parts of the brain to to remove
and I've got two slides which show an
exposed human brain so a warning at this
point so you can look away you want it
won't spoil the floor of the talk and
I'll warn you again in two slides time
when it's over but I just wanted to give
you an idea so this is the surgeons
exposed part of the brain where
feel that the the problem tissue tissue
is and then you can see here's some
electrodes which they're using to
streamed it rough to try and understand
exactly where the problem is so you can
see two sets of four electrodes there
and to move on okay it's safe to look
again now you can see actually that's a
bit offend but you can see the the
activity being streamed from those
electrodes you can see that the spikes
the the the the big dips which indicate
information being transmitted so what
we'd like to do is to be able to analyze
that data as it's coming off the person
to give them to give the surgeons a
better idea of where they should be the
way they should be cutting the the the
brain tissue and also afterwards to be
able to collect this information and
make it available to mothers around the
world for analysis so we end up with a
set of requirements from talking to lots
of these neuro scientists so we want to
share code as well as data capacity so I
wrote I've given this slide before and I
always write vast it storage 100
terabytes well as this was a google
conference I did wonder about changing
at a pitifully small amounts of data but
100 terabytes seems seems quite a lot to
us and when you bear in mind it's not
just storing it you won't actually want
to be able to to analyze it and a lot of
these analysis techniques are very dear
intensive so there's somebody in my
office last week who had an idea for a
routinely authority over half a year to
run and of course as you build a lot
more and more data in the system a lot
of what these scientists want to do is
to do comparisons across whole sets of
data for the types of neuroscience are
interested in so you end up with more
and more computation as you get more and
more deer and the architecture that we
we chose was was this cloud architecture
so the idea was that we didn't want
scientists to have to have lots of
software that they deployed on their
desk we don't want to keep the debtor on
the desktop we wanted to have a out
there over the internet somewhere that
they could access by through a browser
so the idea is they do the experiments
they upload the data in the cloud they
can use the set of services that are
there to analyze it or they can write
their own services and upload those
as well and use those and typical users
won't actually upload code the news
existing code but so typical news will
go on there find it that are interested
in run the analyses get some results
backs and grabs and visualizations and
hopefully that will allow them to do
their site without having to install
lots of software on their desktop when
you look at how you might architect that
clouds then there's a lot of interesting
clouds at the moment and it tends to be
down at that bottom level so the the
Amazon level you can pay as you go for
for storage in pairs you go to run
virtual machines and so we could we
could have said to people okay that's it
you we give you storage and and compute
and you can do whatever you want with it
but as I said we've had this regionally
Science Center it's been running since
2001 and we've worked on I think it's
about 25 different research projects
with probably about 10 different sorts
of scientists and other time you
gradually get a picture from what other
key services that you need in order to
build a science cloud so what we decided
to do was try to build scale when
plantations of these key services so
that we could get the users were in at a
much higher level so they can rely on
useful services in order to do their
science so we rejected that and went for
this and this is the set of initial
services that we that we came up with so
this is what we've built for the calm
and project for each one of them we try
to make them scalable in this talk
because of the time pressure i'm only
going to talk about a couple of these
services now we scale them but i'll go
through them all quickly now so
obviously key thing is bill to store
data the parameter it tends to be files
so we store that in a file system with
fact we use this storage resource broker
from san diego which some of you will
probably know and because the key thing
as i've said is that it's possible for
other people to share to share these
files we need somewhere so that they can
interpret them so they know what is in
them so in our system when you upload a
file you also have to fill in a form
where you describe metadata so it talks
about what the experiment was what part
of the brain it was operating on what
drugs were being used and so on so this
is stored in
metadata repository which you can see
there on the on the right hand side and
that means we can have a registry down
on the left-hand bottom left there so
the people can search for data and
actually also codes that they're
interested in so typical scenario for a
neuroscientist would be the search for a
particular sort of data and then run
some analysis routines across it and
we've got a work for an admin engine so
workflow has been very popular for
e-science a lot of people like that as a
way of combining together composing
different services and also to visually
represent what they what they do it's
much easier to show fellow scientist or
work for graphical representation of
work for then is to show them three
thousand lines of four channel or Java
or whatever so so we have a work for
enactment engine in there to run
workflows okay and then we've got a
security infrastructure so if you say
that scientists okay with we've got lots
of disks we've got our way in which you
can share your day just upload it and
anybody in the world can can see it and
they won't do it so and the reason is
because the reward structure in science
where you get your reward not for
collecting data but actually for
analyzing the day and writing papers
about it so you need to provide a fairly
sophisticated fine-grained security
system which allows scientists control
so initially they can look at the date
or in control themselves then they can
open it up to their collaborators and
then finally once they've got the papers
out and they're comfortable with making
it available they can make it available
publicly to anybody so we have to
provide a security system which which
allows that okay so what I'll what I'll
do now is I'll focus on a couple of
these services so I'll talk first of
about the service repository which you
can see at the bottom right so as I've
said we wanted a way to share codes as
well as data and so that's where we've
got this this service repository and we
use this system that we've had for a few
years which you called dinosaur which
gives us a code repository and dynamic
deployment so users upload their data
the stipulation that we have so we have
to have some common where to represent
the these codes as though they have to
be wrapped as web services so we sticked
a very simple website
WSI web services for interoperability
and the advantage of this is that the
internals aren't important so different
signs different scientists come along
with different sorts of codes matlab's
very popular for for neuroscientists for
example but they also write in Java in
our CC shop and so on and and what we do
then is so the so they upload their
services and we then provide deployer so
for each different type of service so
for example for tomcat services the
upload dot war files we've got deployers
so that dynamically we can take services
from this repository and deploy them on
available compute resources as they are
needed one of the as well as the dot war
file is one of the main ones that we've
started used more and more virtual
machines so we can wrap these we can
wrap these services in VMware virtual
machines put them in the repository and
that's what is deployed and that's
important because you often find that
scientists have very sophisticated
requirements for the environment which
the service run so they need particular
sorts of libraries sometimes they even
need a particular version the operating
system so virtual machines gives them a
way to to encapsulate their environment
so that it it can be dynamically
deployed so this is an animation for for
how dinosaur works so you can see at the
top there the service repository so
that's where they the services are
uploaded into and and it's on the left
hand side you can see the client and the
client is going to send a salt message
to an end point at the web service
provider so as far as the clients
concerned it doesn't know anything about
dynamic deployment or dinosaur there's
just an endpoint that can send the soap
message to if it wants to use a service
so it does that and when the salt
message arrives at that endpoint what
the web service provided does is it has
information about where there is a
repository with a deployable version of
that service contained in it so it
inserts in the head of the sort message
a reference to that repository so
basically once you've got this stage now
then effectively that sort message is
like a closure and a functional
programming language because you've got
the you've got the data you've got the
body of the message you've also got a
point
to the to the code which you can use to
process that message so then the web
service provider sends the message to
what we call host provides the horse
rider can be any anywhere where you've
got some compute resources where you can
deploy services and process messages so
it could be a cloud you could have
multiple hosts provides you can make
dynamic decisions based on cost on
performance about where to route it so
in this case this is a request for a
service s4 and s4 isn't deployed on any
of the nodes that this host provider at
the moment but that's okay because there
is that link in the header to the
service repository so the the host
provider sends a request the deployable
version of the service comes back gets
installed on a node and the salt message
can be processed and returned and so
it's quite so so this system once that
once that once that software is deployed
then you can stay there so and
subsequent accesses to that service you
don't have to deploy every time so you
fact that you divide up the cost of
deploying the service across all of the
the accesses which you you have to that
service and we say this is in contrast
to job scheduling for example which is
quite common in E science but there
every time you you submit a job of the
system you're moving around the the code
as well as the data with this sort of
system where you can dynamically deploy
service once it's deployed it can remain
there and you don't yeah so as I've said
you don't pay that cost once the
services deployed us requests for
service s2 so it comes to the host
provider and s2 is already deployed down
at that bottom node and then so it's
just process just like any other web
service request and we also use dinosaur
for scalability so um one of the things
you can do is what because you've got
this deployable version the service you
can assuming it to stay at the service
you can deploy multiple versions of that
service and then you can schedule
requests around different deployments of
that service in order to get scalability
and we've played with various algorithms
so you could deploy automatically the
service to every node in a pool we've
also tried some adaptive techniques so
here's here's one algorithm that we
that we've used so if you look at the at
the graph along the the x-axis that's
the arrival rate of messages for a
particular service and if you look at
the left-hand y axis this is the
response time and that's plotted on the
lower graph the one with the the diamond
markers and you can see what's happening
here that response times fairly flat and
then once you get up to about arrival
rate of about half a message a second
then suddenly the the response response
time increases and you can see what's
happening by looking at the other graph
the one at the top with the with a
square markers and the right hand y axis
because what's happening is the system's
noticing that the request rate is
increasing and as it does so it worries
about the the response time it measures
that and in order to try to keep that
constant it starts to dynamically deploy
the service on multiple notes you can
see initially it started with it on two
nodes and it goes up to four and six and
ten or twelve and so on until it gets up
to 16 and in fact 16 nodes isn't enough
to cope with an arrival rate of one
message a second for this particular
service so that's why the response time
is increasing but if we had more nodes
in this experiment then we could keep
that response time flat for for longer
so by having deployable versions of
service and ability to dynamically
deploy them it gives you this capability
to descale services as they become
popular is to become use mobile more and
more okay the second part of the system
I look at and think about scalability is
a part which users often use when they
first uploaded data so what they what
they're interesting is having a look at
be the dead and just to check that it's
okay before they run a work for on it to
do the analysis so they looking to see
whether there are spikes in it and
whether the spikes are it be at the
right sort of places so this is a tool
produced by my colleagues in York and
they've got company their side bula and
this gives you an interactive where to
look for to look at the the information
coming from a set of electrode so you
can see each of those those graphs and
the right hand side of that panel are
from different electrodes in a
multi-electrode array and you can search
for
ticular feature so you can draw them by
hand if you want to where you can find
one and to find more that I like this
and then you you you you tell the system
to do that it goes off and locate did it
would you store in the carmen system
which meets that criteria so what we do
for this they've got a system called
aura and when the user presses the the
search button then this sends off a
request and what it does is to paralyze
the request so everywhere where you've
got your data which could meet in
multiple nodes in a cluster or across
even across different different clouds
then there's a there's a component
called a pattern matching control and
this receives the request and it pushes
it down to agents which run close to the
disks on which the day or is held so the
idea is to try to avoid transferring
data across distance so you move the
pattern matching is closer you can to
the data and so the the search happens
in parallel and then eventually the the
results come back and at any time in
this process the client can start to
explore the the data so you look at the
patterns that have been found and look
at the day around it and this is a
generic system for scalable pattern
matching so the my colleagues at your
use it for things like facial
recognition following addresses in aero
engines as well as exploring and you're
a signed statement you can plug in your
own particular search technique so the
one that that we've used in these
experiments is correlation mill matrix
memories which is a neural network type
of technique which which skills
particularly well across large data sets
okay just to bring everything together
at the towards the end of the talk so
this is one of the workflows which the
the sign is to have used so this
represents something that's quite
typical where they get some original
data they run some code which happens to
be an hour an hour service which does
some statistical analysis and they
produce various sorts of graphs and
movies and in fact all so so this is
just showing um so this shows two things
so this shows our portal so I said that
we had a cloud solution and what we
wanted was for you
just use a browser on their desktop role
and have to install software so this is
the portal that they used to to do that
you can see the the users data that they
have access to on the left hand side and
split by a location and then particular
laboratory's and in particular
experiments that have been called then
it can select existing workflows or they
can write their own workflows and they
can right click on a workflow to run it
on the particular data that they've
selected and when they do that well as a
movie on the next slide the the other
thing that we can learn from this is
that we aren't particularly talented at
doing user interfaces so we've we are
actually getting some people involved to
make this prettier and more usable but
you get the idea so this is for example
the typical output that the
neuroscientist one from one of these
workflows and each of the points on the
grid represents an electrode and
electrode array and what they do is they
visualize the amount of activity by the
radius of the the circles you can see so
this is a rather than just present the
sinus with it with the table of numbers
this is a good way for them to be able
to visualize the the waves of activity
sweeping across some neurons in a in a
part of the brain okay so two to finish
it so I'll give some conclusion talk
about directions where this is where
this is going so um so the idea here is
to deliver it not just a cloud for
neuroscience but a scalable science
cloud that we can use across all the
different sorts of scientists scientists
that we that we work that we work on and
so what we've had what we've done is to
try and identify what these key services
are and then try to produce scalable
implementations of them we've just got
funding to look at trying to extend this
across to universities and industries
across the the northeast of England and
project early science central and so
we're building up now as well as then
we've got the neuroscience services of
building up services for other
scientists for example chemical
informatics it's lots a small chemical
informatics companies across the
northeast of England at the moment if
you want to do chemical inform actually
have to buy some kit you have
to buy the software that runs on it you
have to employ somebody to look after
that we're trying to see whether it
would be possible to replace that with
something where people could just open
up a web browser and have access to to
those services that they needed to use
without them having to buy hardware or
software or to manage it themselves a
key problem here though is is
sustainability so one of the reasons I
was interested in coming here was to
hear about what commercial companies are
doing about about clouds so there's a
high cost in managing maintaining all
the software and hardware that we need
and academics like like like those in
the project we're funded to do research
not not run a service and so we have to
worry because we we build these systems
we encourage scientists to upload their
data to it and our funding runs out in a
couple of years time and what happens
then so we have to put a lot of effort
into thinking about sustainability so
we've been looking at commercial clouds
and and the the attraction there is the
the page you go for for storage and
processing so even after the end of the
project if we could cobble together some
money so that we could afford to pay to
to maintain the the data in the
processing then that that gives us a
call for sustainability and we've been
looking at technology such as dessert
there's a company that I do some work
with our dinner and they provide
technology which gives you a way of
moving from private cloud such as the
one that we've got at the moment so we
run that on our own cluster to external
cloud such as such as Amazon so you can
transfer their according to dayton and
and work according to some policy so it
doesn't have to be a big banks which you
can do it in stages but this still
leaves of course a whole because so if
you look at what the amazon would give
us pick one of those vendors it would
give us that bit at the bottom of the
storage in' and compute and then you've
got the the particular services in which
we we argue you need in order to support
science and then the specific science
applications at the top and so there's
still this issue about how do you
support these these services so one of
the things I'm interested in is whether
over the the coming years will be an
emergence of more domain-specific clouds
work
is will provide not just storage and
compute but specific services for
particular domains and so we've started
to run along with we started to look at
this through a company called air in
sport science that I have a look at the
the possibility of providing a system of
science cloud for for users on a
subscription or a pay-as-you-go basis so
that's that's the way we're looking at
the future ok thank</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>