<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Faculty Summit 2008 Day 1: Computing at Scale | Coder Coacher - Coaching Coders</title><meta content="Faculty Summit 2008 Day 1: Computing at Scale - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Faculty Summit 2008 Day 1: Computing at Scale</b></h2><h5 class="post__date">2008-09-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cvLOzjGLnVU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm Alfred Spector and I'm the moderator
I have to my immediate right you are
left or which lay which I didn't quite
pronounced right but it will get better
as time goes on because I practice
machine learning also Rob Pike at
Laszewski and Jeanette wing so we're
going to talk about this computing its
scale now what I wrote was and what we
wrote the frontiers of computer science
are being increasingly impacted by
computing at very large scale that is on
a very large scale computing systems
perhaps in the cloud with possibly
significant dependence on large scale
data sets and or systems with shared use
by vast populations this panel will
explore the potential opportunities and
challenges in computer science research
in this space Jeanette am I on you're
wrong okay do I really need this okay
well I actually I we were told that we
were given three minutes each so I have
three main points to make the first one
is actually just to share with you a
serious interest that the National
Science Foundation has in what we're
calling data in terms of computing and
the recognition of why this is important
it probably shouldn't surprise any of
you here but i'll just mention to you
this observation that there's this
convergence in trends that make this
sort of that ipping point if you will
for our field so first of all we are
drowning in data and I need not tell you
how how much data we're drowning in but
i do want to mention a few facts here
like CERN's Large Hadron Collider will
be generating 15 petabytes a data year
once it starts working there's an
international data corporation says by
2011 we'll have generated 1.8 ze bytes
of data digital data zettabyte is one
followed by 20 10 so we're talking about
a lot of data George Gilder a techno
future evangelist predicts by 2015 that
the US Internet will be around a US
Internet traffic will be around ones
that a bite annually so a lot of data
so trend number one we're drowning in
data trend number two is as was
mentioned earlier a lot of areas within
computer science are taking what we
would call a data-driven approach so
we're seeing in graphics animation on
machine learning speech translation even
computational biology so that's trend
number two trend number three's storage
is cheap truck number four is a growth
in huge data centers trend number five
is data in the cloud it's not on your
machine and finally I think one of the
things that's really going to enable the
computing community the research
community especially is that there's no
easier access and programmability of
these large cluster machines if you will
whether it's renting through Amazon or
using the google and IBM cluster
provided the NSF or or your own huggable
cluster so that and then finally of
course part of that trend is the
programmability meaning the open-source
hadoop for instance that Yahoo's
providing so those are all in trends
that are converging and basically saying
you know we have to pay attention to
this data stuff so that's the first
point I wanted to make and that leads me
into just describing a little bit about
what this google and IBM software
services data clusters all about and how
we really are together with the National
Science Foundation and google and IBM
basically throwing a challenge to the
entire community but the computer
science community the scientific
community in general saying you know now
that we have provided the general
academic community access to this large
cluster what can you do with it and
there are actually I think scientific
technical and societal questions
research challenges that we could be
asking so for instance from a from a
technical challenge of course there's
just how do you operate these things in
it and as automatic
as possible what kind of reliability and
automatic configuration management can
you actually run these sorts of things
and from an application so there's this
idle point of view you can imagine well
the challenge is you know well we know
we can do search but is that all we can
do so we're really interested in to see
how the academic community responds to
now that the fact you can't make the
excuse if we don't have access to this
okay so the other thing I wanted to say
is that this this software and service
is at google and IBM are providing
through NSF to all of you it's really
part of or in fact a seed of this larger
effort that I mentioned before the data
intensive computing program and yes if
you're following your and I said email
you should have seen the solicitation
that came out on the data intensive
computing crosscut and and the clue
cluster exploratory program was really
meant to see that the other point I
wanted I usually make in the same breath
alone maybe it's not pertinent to this
audience is that we welcome other
corporations to participate in the
cluster exploratory I last thing i want
to say about the google and IBM a
partnership with NSF is that it we broke
new ground at NSF doing this this is
this is very different and it was very
exciting for me to kind of be the ones
that be the one to shake up NSF and turn
it on its head and say you know they're
not providing a physical instrument and
no i have no clue of where it is but it
doesn't matter because it's in the cloud
and the director looking at me and like
getting this worried look you know I
can't touch it no you can't touch it ok
so anyway besides all the lawyer stuff
that we have to go through the last
point I wanted to make and this is
really up to the computer science
community you might think that you don't
have access to data but there's talk
there's tons of data out there and where
is it it's with the scientist the
scientists are drowning in data so for
instance not my favorite example now is
the geosciences what do they want to do
they want they had this grand ambition
they want to model the earth from the
inner core to the Earth's surface to the
Sun and all the layers in between and
all the systems interacting from the
hydrosphere the cryosphere of the
atmosphere the sphere just under the
crust and even the biosphere the
biodiversity because any perturbation in
one of those those systems might have an
effect in some other system that's
obviously as you can't you know the
earth is the earth you can't separate it
and moreover they actually have some
specific goals for the near term which
is to be able to predict climate on the
10 to 15-year scale because that's what
matters to us as citizens that's what
matters to your local politician who has
to make a decision about do we recycle
bags or do we ban plastic bags because a
decision that he makes as or she makes
now will affect the citizenry in an hour
lifespan and so that's why it's it's not
you know do climate modeling and and
what is the world going to be like in a
hundred years it's what it's what is it
going to be like in 10 or 15 years and
they want to be able to predict climate
on a regional scale because again it's
what affects us what affects my
neighborhood what affects you know where
my children go to school so that's just
one example the scientists are producing
and generating more data than they can
store they throwing away data and I can
tell you more about that after Rob Pike
speaks ok I med los hauschka I will be
closer to three minutes than Jeanette I
will I should have said that as my goal
I made a big mistake I'm gonna be a bit
redundant I'm afraid but let me just
highlight three things at a minute
apiece one is the educational thrust
which you've heard a lot about from
various people at this summit last year
Christoph basilia and Basilio excuse me
and some other people from the
University of Washington and Google
described the
the course that we taught at UW and
described the original Google IBM
initiative and it's come a long distance
since then six months ago NSF as you've
heard joined the partnership through the
the clue program and now the the data
intensive computing program Christoph is
speaking in one of the breakouts at
four-fifteen this afternoon so if you're
interested in educational aspects you
should find him and sit at that table I
think what's important is that this
isn't vocational training I mean we love
it when these courses send students to
Google it's a great place to work but
the truth is students need to learn how
to think about solving these very very
large scale data problems and we haven't
been teaching our students to do that
and we are now and that's a real change
and it fits anywhere in the curriculum
it can fit into your systems courses of
your programming language courses or
almost anywhere it can be a
free-standing course but it's something
that's got to get into the curriculum
and it's valuable because it's a new way
of thinking it's not just a new
programming language a new programming
paradigm it's a new way of new style of
problem-solving to tackle someone
mentioned earlier that we just concluded
a three-day workshop that the National
Science Foundation and Google sponsored
for a set of 40 faculty members who are
trying to learn how to teach this stuff
and the material from that is all on a
website for those of you who don't trust
Google if you go to la salle scott CS
washington edu / google i have a bunch
of links to things that might be
relevant you can probably find them all
with your favorite search engine but
they're there as well again we had
participants there from not just google
but yahoo amazon we had sort of
curriculum material presented it was
fifty percent presentations and fifty
percent labs for the teachers and
there's tons of material there that you
could use to move this into the
curriculum the second thing I want to
talk about is I'm involved in this
outfit called the computing community
consortium whose goal it's an NSF
sponsored initiative is to try and get
all of us to think about sort of longer
range research challenges and one thing
we did last March under the leadership
of Randy Bryant who's here also
somewhere there was to sponsor or
something we called the Big Data
computing
study group and Randy organized two
workshops in march down at yahoo one was
called the hadoop summit and one was
called the data intensive scalable
computing symposium the hadoop summit
was originally scheduled for a little
conference room at Hadoop and we managed
to have to move it to the san jose
convention center where it had more than
350 attendees the goal there was to
build a sort of community of practice of
people using Hadoop and the goal of the
disk symposium was to build a community
of practice of researchers interested in
scalable computing problems and it
should be clear that this impacts system
design research programming language
research applications support research
and applications themselves there's tons
of work for us to do and an important
element there is to listen to Jeff Dean
tomorrow who Alfred challenge to put
together a a talk for us on what Google
thinks the pressing systems and
languages and applications support
challenges are these are things for us
to work on Google doesn't pretend that
it's answered those questions that at
any level finally at the University of
Washington we've recently created
something we call the East Science
Institute and there are two goals of
this let me tell you what my fear has
been it's that the previous generation
of computational science that is
simulation oriented computational
science was a tiny niche every campus
had a couple chemists a couple
physicists to heppell astronomers the
new computational science is this data
centered science every science is
flipping from data poor to data rich
just as Jeanette was talking about I've
been working for a couple of years with
small oceanographers at the University
of Washington of the project there is to
put two thousand kilometers of
fiber-optic cable on the one de Fuca
plate off the coast of Washington and
Oregon and British Columbia and hang
thousands of chemical and physical and
biological sensors off that grid and
stream data back the oceans are
responsible for a huge amount of the
environment that we experience oxygen
co2 exchange and things like that and we
know nothing about them because we don't
have any data oceanography expeditionary
science you go out in a ship you drop
instruments you come back this is going
to transform oceanography universities
are going to cease to be competitive if
they don't bring computer scientists and
disciplinary scientists together
and figure out how to do the
semi-automated knowledge discovery in
this mountain of data as well as the
sensor networks and stuff like that all
right I'm gonna be even closer to three
minutes I have just a single point but I
has three sub points so on to me that
clusters in large scale computing is
actually about collaboration and pretty
much about collaboration only because if
you just set up one of these things or
if you want to to construct one you
actually need not you know computer
science like computer architects all
right or you don't need you know system
administrators you need you know
mechanical engineers to figure out the
building and the cooling and the power
right you need hardware engineer to do
the servers in the racks and etcetera
like networking hardware for that part
operating systems you know middleware
the applications on top like the
networking it's the whole stack and if
you ignore any of these and also how
these interact you're going to have it
you know sometimes strongly inferior
solutions so just to construct one is a
lot of collaboration and then as Eddy
and Jeanette actually alluded to I think
the key use of these clusters is in
collaboration in several areas one could
be the pure computer science part right
so if you want to understand how the
cluster actually works and how you can
operate it sort of hands off that's a
collaboration between people who have
much more operational thinking and then
people who have much more algorithmic
thinking and you got it put the two
together right and you probably can't
solve it with either half or similarly
in an interdisciplinary way across
disciplines right is let's say you do
you know go to one of these people have
lots of data a biology or geo you know
satellite images and so on and so on I
think this is where the big
breakthroughs are going to happen
because you have to process that data
but the process in itself is difficult
enough and then the science on the Geo
or bio or whatever area is is
just as difficult and one of the
mistakes that I think that we've made in
the past and I've seen as myself as a
faculty member is when we do
interdisciplinary research and you know
you work with a biologist people
evaluate you know we computer scientists
evaluate ourselves by what we
accomplished in computer science I found
this cool new string matching kind of
thing but actually what matters is what
impact you made on biology right do we
understand you know genetics better or
something like that right and so I would
encourage people to you know think about
that when you see an opportunity and
really try to work on problems that
matter in the fields that you're
collaborating with right because that
will automatically but it was good
computer science and good you know
whatever the other science is so I think
my three minutes are up but alright i'll
try to be even briefer but it'll be easy
because i sort of been covered already
but i'm going to change encouragement
into an admonishment I think academic
computer science is in danger of not
being part of this story at all
competing its scale is something that so
far has not been done in academia and I
don't see it that changing very soon
unless you listen carefully to what has
been said and what I'm going to say too
I think one of the problems of courses
you need equipment that's not really the
problem although it's too much spoken
about as being the problem there's also
educational issues and things like that
but that's not really what's hard the
thing is that computing a scale is just
an empty idea if you don't have
something you're going to do with it you
can build all the equipment you want you
can run all the software you want but if
all of it does is generate computer
science papers you're not doing anything
except advancing your own careers and I
think that's that's bad the cs panini
just isn't looking hard enough at what
applications can be done but if you look
at high energy physics by with maddox
geology things like that there are
problems out there that need to be
solved and the cs community is quite
simply letting them down google is part
of a collaboration called the large
synoptic survey telescope it's an
astronomy project that will generate of
order 20 or 30 petabytes several
terabytes per night there's a huge data
processing problem there's a huge data
analysis problem data mining problem
accessibility problem and computer
science as a community has so far
delivered essentially
nothing to help them and as a result
they're going off and they're rolling
their own and they're probably not going
to do a great job but they're going to
be the best job they can with the tools
they have available now this is actually
an interesting exception to prove the
rule in the LSST because as a result of
them canvassing for ideas Michael
Stonebraker has decided to try to help
them and build a special high-speed
distributed data system for analyzing
the kind of stuff they want to do but i
think it just proves my point because in
order to get funding for that he had to
go outside basically make a start up and
ask for private funding from companies
like yahoo you bae in google and i think
that that is an example of what i'm
talking about it's not working within
the academic community itself so the ant
the NSF it's okay the NSF has to step up
to the plate I think they haven't thank
you I don't think they've done enough to
support interdisciplinary research I
think they talk the talk but they don't
walk the walk they don't make the money
available if you have a true energy
desperate community and the cSPD in turn
has to decide that it's valid as we were
said to do science in areas of and
computing if along the way you generate
good science as a whole big big science
has the budget it has the equipment and
it has the problems the cs should be
there and helping the comment now this
is actually a constructive response i'm
not a defensive response and that is a
program that i did forget to talk about
was the cyber naval discovery and
innovation which speaks exactly about
collaborations and interdisciplinary
research where the requirement for a
successful project is advances in both
computer science and some other
discipline and it was the biggest
program that the National Science
Foundation put out this year its
foundation wide every single director in
office in the foundations is
participating sizes the lead and this
program was for all of you and I I have
to admit that I I wish I I wish I saw
more of you sort of proposing and in in
diverse and interesting ways because I
worked hard for to help create this
program for all of you and I think this
is really speaking to
what Rob Pike I think was admonishing
the community for and I ended in
defensive remark the National Science
Foundation actually is putting money
into this and is trying to encourage
into the supplanted collaborative
research but it's up to the P is to
actually respond so ad what's ccc going
to do to catalyze our community to be
involved as a partner in
interdisciplinary efforts rather than a
consultant to interdisciplinary efforts
I think all you can do is point to
examples and there are lots of examples
out there and it seems to me there are
plenty of computing research topics in
everything that's been talked about if
you look at Jim Gray's trajectory over
the past the last five years of his life
it was realizing that for better for
worse we design systems badly enough
that whenever you add a decimal order of
magnitude in one or two dimensions
you've got a new research problem and
that's what scale is about so it seems
to me that there's plenty of what we
consider to be computer science research
if we can just get over our insecurities
about being people's hand maybes I would
say though that you're still focusing on
the computer science problems and of
course your computer scientist so that's
okay but even just doing a really good
job that doesn't generate a computer
science paper but helps scientists to
breakthrough research that's valid
research too and should be done so we
have a bunch of questions here that are
given will will generate some questions
for ourselves but we must show that we
are responsive to all of you so the
first question is an interesting one I'm
not sure how much we can get into it
here we'll we'll take that one Google
and others have taken revolutionary
steps and how computing is delivered
with desktop quality apps running within
browsers however programming these apps
is challenging in the apps typically
cannot leverage all the resources
available locally how do we see the
rebbe web browser evolving to support
hybrid desktop mobile cloud so Robert
Wars you want to take that one I can
just give my opinion I don't have any
insight i think that the browser is
becoming the operating system and and
they're just sort of co-evolving and at
some point you could even argue that you
need something analogous to it java
crypt native operating system for which
the browser is the shell and that's sort
of the end of it of course all of the
parameters in that are bogus but the
concept is roughly right and there's
even things along that line being
developed by people like Dan ingles I
got the balance of power between the
cloud and the browser and the computer
is kind of funny but I think it's
roughly right I think the structure is
viable for scale but the support
environment particularly executing
within the browser which is a crock on a
on a you know on a disaster on a mistake
is just not suitable and without a
complete reset which is very hard to do
I don't think we're going to I'll do a
lot better than the trajectory Ron I
think it's actually an opportunity for
for academia to make a contribution
because the you know the upside is large
and the bar isn't very high I think it's
clear that you know nobody really wants
to use Ajax but it's the best thing that
actually works out there right and so
everyone does use it because you really
have no choice as Rob says it's not
going to be easy to replace this you
know tomorrow with something else but
you know eventually it's going to happen
and when it's going to happen it is
because there is a different model that
makes it much easier to not just program
the client but also orchestrate the
computation sort of between and the data
between the client and the cloud and
which is very cumbersome today because
you have to do sort of everything we're
in the meantime kind of doing things
along the side so for example Google
gears lets you you know store things
locally in a browser accessible and
reasonably safe way but I think that's a
short-term I mean I that API may persist
but it should be implemented really for
something other than JavaScript and I'm
sure longer term we will but in a short
term you got to make you know
programmatic steps but I think there's a
huge opportunity here so if you have
great ideas in this space you know right
some Ajax applications and it tells you
what the problems are and you know write
them again in your better system on the
health question on this one I'll apply
editorial comment to say that privacy
issues are not really the topic of this
panel but there is a Google Health talk
which is coming either next I think it's
next where else it's tomorrow
and that would be a perfect question for
that panel so let me just you lead that
one to that panel so the next one was
what did Google insiders think and I'd
like to change that to what the members
of the panel think are that tomorrow's
undergrads need to know about large
scale computing so ed you want to start
with that one let's see I think when
Christoph motivated the creation of this
course two years ago it was based on the
observation that the students he
interviewed the hundreds of students he
interviewed Google were great at coming
up with algorithms and data structures
for handling small scale problems and
were paralyzed when it came to
algorithms and data structures for large
scale problems and it seems to me that
we're more and more dealing with large
scale problems and people simply need
experience in thinking at that sort of
scale the scale of thousands of
processors and and terabytes of data and
that's what we're trying to provide and
then all the rest of the stuff anyone
else everything you said is right but
I'd like to add that there's another
approach to it which is to instead find
ways to make it so you don't have to
think about that that you can if you can
find a way to make the problem more
manageable by appropriate programming
paradigms or layers or abstractions or
languages then that's that's great too i
mean one of the reasons we can function
at the scale we do is that we found
tools like you know MapReduce and it's
all and things like that that make the
programmer not have to think about scale
because the underlying layers take care
of it and there needs to be more work
done on that stuff as well particularly
for problems that are not as easily
slice of all up as MapReduce solutions
tend to have right I don't think anyone
at Google believes MapReduce is the
solution for all prob said it's a bet
essentially a batch processing
technology and it's highly valuable for
that but I'm sure I'm sure Jeff will
talk tomorrow about the challenges you
face when you're dealing with real-time
interactive computing that requires vast
parallelism that's another another topic
all right so what are the three biggest
challenges in operating large-scale
clusters or is I don't think any
one knows I've got a fiducia you'll have
to take us there's many different
answers and instead of giving you the
three biggest chances challenges I'll
give you the biggest challenge in in
three different ways no seriously so
there's really you know a cluster is
many things to many people right ideally
to the application programmer dry to the
end user quote-unquote it is just a you
know pool of resources and you don't
have to deal with failure and things
magically scale it's kind of like you
know you're an app engine user and
here's your app and suddenly you're you
know your input goes up by a factor of a
thousand but you know except for your
bill going up you know things work right
and so the biggest challenge is how do
you make that illusion actually true
right how do we build middleware systems
infrastructure that to the programmer
removes as much of the complexity as is
possible right that's a huge challenge
you know we have a few answers in a few
areas but you know we don't have the
answers at all so that's the first one
the second one is if you if you actually
operate the cluster how do you control
costs right the energy is a big portion
there you know construction costs you
know capital expense you know how do you
do you do you really physically operate
things the data center industry has been
sort of dormant for the last 30 years in
the sense that your typical data center
design today is the same that it was in
1968 and is hugely inefficient right i
mean hugely inefficient any typical data
center after you account for all the
losses including the losses in your
computer itself and you look at the real
productive electrons consumed that's
typically one quarter of the power that
comes in from the electricity company
right so you have 75% going to waste and
twenty-five percent going to the actual
computation and clearly that you know
this ratio should be very different
that's a big challenge I think the third
one is what we talked about before if
you want to view the end user not as a
scientist but actually as a consumer how
do you really orchestrate sort of
the division of labor between that cloud
I eat a data center to large-scale
cluster and where your user is right
maybe they're on a mobile device right
maybe they have a desktop you know maybe
have lots of storage locally maybe they
don't have much storage at all locally
again it's something that I think has
been thought about for 25 years in
different contexts but hasn't been real
and and now it's really becoming real as
broadband becomes you know reasonably
pervasive even on on mobile devices so
that you could actually have some
communication and not die kind of right
away so I think those are you know in
each of these areas the biggest
challenge and and you know there's
probably a few more expanding on your
last point I'd like to say that I was at
a National Academy meeting about a year
ago and there I commented that there has
to be a change in the way the problem
solving is done structurally because if
you want to do research in this area or
even you know use these tools in this
area you can't be expected to build a
day Center you know fund all that stuff
it's just not going to work and I think
we're going to see a change in in the
way that computing gets done so that
commercial companies you know Google's
Yahoo's ebays of the world Microsoft's
they start providing services analogous
to what amazon is doing but on a much
bigger scale such that they're just part
of the infrastructure that is supported
and we need to find a way to make those
infrastructure elements available at a
convenient way that the entire community
can use it and depend on them as sort of
fundamental infrastructure instead of
things you build with a you know it's
small Beowulf cluster and Iraq and your
university department genet it's a
little bit early for you to answer this
from an NSF perspective but thinking
through say five years old what do you
think the alternatives are the options
are what's the space going to look like
for NSF related activities in supporting
computationally intensive cluster-based
well I first of all I want to just share
with you that the other a DS and the
other science am I on other signs
directorates recognize that where there
where there are problem lies right now
is in the
data and also the software i should
mention they it's not it's not super
computers anymore it's not compute
processing power it's not throw more
processors at it it's they're saying to
me Jeanette we've got all this day to
help us and they they really they and
they understand that it will be new data
structures or David representation or
new that new algorithms or new
programming models and new languages and
sort of new bread and butter computer
science that's going to that they need
and it's not big iron anymore I mean of
course it will be big are in all ways so
I think because you know at the highest
levels the the scientists are crying out
actually for our help and our creativity
and our contributions in fundamental
ways i think i can imagine in 10 years
that you know if if we had the money to
do all this but that's where it would be
different from no one or two super
computers you know in the country or
something like that but i'm speculating
so I supporter take off on something Rob
said and and that is that the pragmatic
version of of what are the problems in
running large scale clusters is we
shouldn't be doing it right and how many
of you have actually used amazon web
services i mean some okay it's me it's
quite remarkable you know dave patterson
his group wrote a paper using Amazon Web
Services to experiment up to the
thousand processor level a month ago and
dave has the slide where he points out
that the remarkable thing is that a
thousand processors for for one day cost
as much as one processor for a thousand
days ok that's a revolutionary sort of
new equation for computing so for a
modest number of thousands of bucks his
group was able to conduct thousand
processor experiments which just would
have been infeasible doing it themselves
you know the new york times used amazon
web services for a couple hundred bucks
to convert their entire archive digitize
their entire archive and werner vogels
shows this slide of a nimodos ec2
instance usage as anybody has
not seen that okay animoto is this
little company they do something that
only your kids in mind would appreciate
you give them an mp3 and a bunch of
JPEGs and they produce a slideshow that
syncopated to the music and they like
many new companies these days don't have
any computers all right they use amazon
web services for this so the graph is
Amazon ec2 instances consumed by animoto
and it's bopping by the way takes about
six minutes of computing to do one of
these things okay so it bops along at
about 50 instances okay up and down over
a ten-day period and on this past April
fifteenth they rolled out a facebook app
all right and they went from 52 roughly
3,500 instances in a day and a half all
right and you don't do that by calling
the dell salesman and saying you know
I'd like another 30 400 of these things
running by Tuesday right so this is just
incredibly powerful in Microsoft will be
providing a comparable service and
Google is providing one and google and
IBM and Yahoo have provided capabilities
to the university community so we got to
get out of the business of thinking that
we have to be able to physically touch
these things in order to do interesting
work and interesting research on them
let's see if there's a question from a
table good microphone for your first
here all right so it's not as if though
that these issues that you're raising
our new we've had high performance
computing for a long long time and NSF
has spent countless dollars on high
performance computing and a lot of those
dollars have gone into what we all know
and no as great good grid computing so I
would like to hear from the panel what
your opinion is of what went wrong so
we've spent so much money and we've been
duly admonished by Rob for very good
reasons not much has come out of that
effort what were the mistakes of the
grid funding effort what what should we
do to avoid that kind of a failure in
the future so I'll try to answer that so
that Jeanette has it and it was before
Jeanette's time right so remember that
but I think two things um you know I'm
not a you to hpc phone because I never
found it that interesting but it's not
exactly a failure right but one of the
differences really is that
the factors involved today are very
different right what really was sort of
a national scale 100 million dollar
program you know just ten years ago you
can basically do your with your pocket
money all almost today right and so this
kind of computation is available to
everyone right including the data in
fact more data is available to everyone
the second thing is that hpc worked
really great for very closely coupled
parallelism kind of things in and one of
the but that made the Box more expensive
I and the model of the box was you use
it all with your big you know weather
simulation right and and everyone else
waits right and then you move off and
you know the next job comes right here I
think the success is going to be based
on having this pool of machines be
deaded amazon or somewhere else these
multiple users at the same time plus you
know you don't need to closely couple
paralyzed might it's a lot more data
mining streaming through much larger
data sets things like that a plus you
got to have a way of tolerating failure
right if you look at the HPC clusters in
their average up time I used measured in
hours right not days basically right so
typically tens of hours and then one of
these on 10,000 processes would fail
right unfortunately you have check
pointed your thing right three hours
before so you can you know fix it go
back you know start right but that leads
to pretty poor utilization and it's not
the fault of the vendor right if you
actually look at you know the failure
that the uptime of a cluster even if
your component is 693 liability which is
almost impossible to do right at a large
scale cluster you get two nines right
one percent failure chance right and
then you get the hours of uptime right
and so in the model where you don't need
the whole cluster closely coupled you
can deal with much cheaper hardware you
can get much Bank much more bang for the
buck to a point where it's really
affordable for almost anyone amazon is
making money at you know the numbers did
it was quoting right and Dave Patterson
is using right so this is reasonably
close to the real cost of computation is
not a subsidized kind of thing and that
I think is what really changes the game
right I think everyone who has petabytes
of data can't afford to process them if
they knew how
and if the processing facilities were
provided somehow but I'm sure that is
the easy part right so I agree with ED I
wouldn't worry about the physical thing
I mean I think it's a super interesting
field and so for some subset of us yes
we will worry about it but that's not
the problem that the you know what sort
of we as a discipline face question
you're on your own question here how do
I vendors are scratching their heads to
understand what is the best way to
design the servers for these big
clusters one problem is that we don't
have the workload we don't understand
what the workloads I'm sure that Google
has a lot of information on this stuff
is there a way to make this information
public what kind of data patterns what
kind of component in the computer are
the most stressed you know this sort of
thing um so great questions user you
know I I know where you're coming from
I'm tempted to say you know it doesn't
matter because that's not the first line
problem that or that you know I at a
cluster level for example how you deal
with failure right at the software
layers above is going to have a bigger
impact on the actual performance you get
out how you deal with networking right
given that you know either you spend a
lot of money for a completely non
oversubscribed you know non-blocking
kind of network or you deal with some
form of Q s right how do you orchestrate
the computations per se right if you
look at the MapReduce you know as an
example it's not limited by the
performance of the machine it's limited
by or its you can get much bigger
speedups if you do the orchestration of
the computation better I'd rather than
the memory fetch better so to speak
right because when you have really large
data sets you're going to spend a fair
amount of effort moving it around right
and not just you know like in a
traditional hpc things it was really you
know gigaflops and
and very regular computations and sort
of things like that so here your best
work load is you know something
equivalent to you know GCC or general
sort of integer ish for us integer age
workload lots of memory lots of i/o but
I think the nodes matter is much less
than the cluster all right so there's a
question here which will abstract from
we have five more minutes is that can we
use that much time everyone ten work
okay all right many of the physical and
biological science are facing a massive
influx of data so processing story in
queering these data is becoming a major
challenge i'd like to just augment it as
what are the Google technologies and
what are the research challenge is more
broadly oh that are applicable so Rob do
you want to take a poke at this
initially I can start I think it's
interesting that that Hadoop has caught
on we weren't able to disentangle
MapReduce enough from our code base to
open source it and I think that's a kind
of a shame but it's interesting that
Hadoop has caught on so much because and
it's caught on not because it is the
right solution to all these problems but
because it solves some subset so well
and easily that it can really catch on I
think that the Google technologies that
that I've seen internally and talk about
that are most important or actually
mostly in the area fault tolerance how
is handled the how it's hidden if you
when you have as Earth say when you get
enough machines together it doesn't
matter how good they are they're going
to break you're gonna have some of them
break and your jobs going to go down and
when I first came to Google one of the
first sort of amazing insights that I
learned here just from watching how
things worked is that there's a sort of
equation that develops out of this if
you have if you're at scale then you
have hardware that's going to break no
matter how good the hardware is that
means your software has to deal with
that and once your software is dealing
with that it doesn't matter how good the
hardware is anymore so you can buy crap
and use it and what your work and in
some sense MapReduce is the apotheosis
of that weird argument but it works
really well and I think it works not
because programming model is perfect it
works very well but it's not perfect I
think it's fundamental thing is how
it hides underneath its lair the all of
the fault tolerance things that bedevil
any attempt to use large clusters and I
think that that's the essence of its of
his brilliance and not the you know the
algorithm per se and I think a lot of
the in my dealings with the LSS heat I'm
struggle to try to get them to
understand that it fault tolerance is
the thing that should be thinking about
first not last and it's a lesson that
they know I'm hammering into them but
they're not quite letting me ham or hard
enough that's kind of at the point i'd
like to make their perfect answer ed any
comment on research topics in the space
or jeanette alright so there's another
Google Health question I'd love to talk
about this because I'm involved in this
on a regular basis and again we'll cover
that at the at the at the breakout on
that suppose we measure efficiency by
the amount of energy expended to answer
a query there is both an interactive
component and a constant component to
the energy expended how has Google's
efficiency evolved over the past say
five years so we don't track you know
dual per query we look at what's or QPS
/ queries or queries per second SRQ PS
per watt so per watt of computation how
many queries can you answer the total
amount of energy per query has risen
over that period because the web has
gotten much bigger and so the you know
the workload for one query has gone up
by a lot like easily factor of ten I
probably more closer to a factor of 100
the efficiency energy efficiency has
gotten quite a bit better I would
speculate a factor of two maybe a little
bit more something like that well
probably more like if you include the
software optimizations actually probably
get five years actually I would say you
know low 10 factor of 10 right just work
we got much more efficient as the index
grew we had to understand how it works
better and so per document searched
we've gotten
much more efficient in terms of energy
spent and then the hardware is a factor
of two problem I'd also like to add that
the hardware community has not done
enough towards energy efficiency in
computing there's there's just some
amazing data that some of which we've
actually published Louise in the back
there this paper a while back but if you
look at how much what the ratio is
between the power that the computer uses
and the compiler that the computer
extracts for useful work it's way way
too bad and for instance an idling
computer uses far too much power even
though it's doing absolutely nothing and
I think as a research topic to help data
center builders I think yours would
agree that'd be really nice if the
computer architects found a way to
control the power budgets and the
computer is vastly better than they do
now I mean vastly questions from the
audience anyone here yep over over the
side yeah Don I want to switch back to
the starting point which was the change
in science from all the data
availability in the need for computing
its scale how does data to centered
science change the nature of science
think about how we learn to be
scientists we got great value from
having intimate contact with the data
and the data collection mechanisms for
example we understand the uncertainties
in the research process as we move to
data-driven science software intervenes
between the scientist and the data how
is that going to affect the overall
judgment process that scientists need to
bring to bear in order to formulate good
questions so how do we help young
scientists develop the judgment skills
at that high level to be great
scientists when automation is in the way
between them and the data and the
phenomena in the world and software bugs
yeah I mean I wonder how many papers are
going to be published about data that's
wrong I mean the data is correct but the
bug in between the data and the result
made it made the result invalid I don't
know as a former physicist that I think
I have something slightly relevant to
say here I was actually a physicist who
became a
person because i was doing computational
physics and at the time eventually I had
to leave the physics feel because nobody
believed that computations I was doing
were real there were simulations that
were getting seven and eight significant
digits that in the end agreed with
experiment but nonetheless I couldn't
get people to believe that that was real
physics going on that was more years ago
than care to recount but nowadays that
is that attitude is completely gone
science is data-driven people trust
computers to give answers but from the
point of view of it a working scientist
the thing that matters is the scientific
training is what matters more than the
computational training when a scientist
gets a number by calculation or by
experiment or by computation if the
number isn't right the scientist should
have the training understanding in the
scientific skill to understand that it's
not right and I think you can computers
can help at scale by providing you know
secondary algorithms to do data
validation data injection test quality
stuff things like that but fundamentally
it comes down to just scientific
training I think that would dominate so
I don't have an answer but I think this
is a really interesting question I
hadn't thought much about it you know it
seems to me that one can argue that the
level of integration of physical things
for example electronic devices keeps
kids these days from understanding how
things work right you can't take it
apart and put it back together you know
there's no batteries and light bulbs and
knife switches and you're arguing that
the same thing is true for the conduct
of conduct of experiments if you're
using data mining and machine learning
to extract patterns from data so it's
it's it's a great point I think there's
a direct analogy well there's data there
is data on this it's not simply
speculation and it plays out in the
history of cockpit automation is one
example where pilots lost that judgment
and we had to reinvent training programs
and it wasn't just data validation new
ways to trust the automation it was new
ways to think about what you were
accomplishing in the world and training
that judgment required a whole new set
of techniques in a highly computing
intensive world yeah I would agree with
you there are and I think agreeing with
the other panelists and in fact you know
we we as computer scientists actually no
you know what we're software problems
can
can actually lead to wrong conclusions
if we're if we don't have that kind of
judgment and the other science is that
is why they still do rely on real
physical instruments and putting sensors
in the ground whether they are in the
Antarctic or you know buried somewhere
in the US and they go and physically go
there and take measurements and try to
validate the experiments that they do in
the lab it's a very expensive way doing
science okay a question over here so a
couple of you especially Rob Pike
admonished us computer people to do more
disciplinary science and account
advances and disciplinary Sciences
things to aspire to I'm very sympathetic
to that goal but I think you weren't
really addressing the complexity of
actually realizing that and I think the
central complexity of realizing that is
that the people in this room well you
know by and large we don't do much we
read email and we talk to people but if
anything's going to happen it has to be
done by graduate students and I think
that's the nub of the problem because I
mean for a computer science graduate
student to concentrate in there five
years in producing results in a
disciplinary science they're shooting
themselves in the foot and I would tell
them honestly that I think would be
great for the world if they are doing
that but they're shooting themselves in
the foot in terms of having a future
career in computer science and so I
wonder if you had thoughts on how to
solve that problem it's all about the
money if if the community changes such
that funding is available either through
scientific disciplines offering money
for computational endeavor or by the NSF
explicitly encouraging and continue to
encourage this kind of thing then by
definition it'll happen you're not in
charge of what gets done directly but
very indirectly you're very important to
what the graduate students work on and
you're very your key part of getting
money right if the money flows in your
lab to develop a collaborative effort
some scientists on this project and what
comes out of that is a graduate student
who's good at collaboration and
understands wit and there's a money cool
that continues to develop in that area
it's going to happen now I'm not making
light of it I'm not just saying this is
a simple problem but I do believe
there's a mistake in your logic and that
is that it's a fundamental problem in
money not in what people's careers look
like that's going to change this so
there's no need a final question so one
of the things that that's drawn me to
respond is because we keep making the
same kind of fundamental computer
science mistakes over and over again so
what Rob Pike said about collaborations
as peers is important and I just heard
about the problem of confirming the
kinds of hypotheses that we use our
machinery to create and if we don't get
the point that if we don't work as
collaborators and peers and provide ways
of interpreting the hypothesis space we
manage for all these scientists we won't
get out of this loop that Rob's
admonishing us to get out of we need to
work and there's a discipline called
artificial intelligence that that
studies the notion of capturing metadata
and reasoning about it right so I mean
there are ways and directions to go and
they don't always come from the things
that I've heard so far there is lots to
be done I think that's a good last
statement there's lots to be done so I
think we want to thank our panelists
thank you all
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>