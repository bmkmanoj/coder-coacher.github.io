<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2013: Testing Systems at Scale @Twitter | Coder Coacher - Coaching Coders</title><meta content="GTAC 2013: Testing Systems at Scale @Twitter - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2013: Testing Systems at Scale @Twitter</b></h2><h5 class="post__date">2013-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/99RABfKNfcY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we have James waldrop here from Twitter
he tells me that he is an avid kayaker
he likes the kayak at night mostly
because his kids are sleeping then which
I can very much appreciate I get to walk
the dog somewhere around midnight so
that's something we definitely share in
common and even though I'm not gonna
explain to you why if you want to go out
to moderator you should put this
question in there about why it's his
product labeled lotto because he loves
this question that's what that's what I
heard so with that James come on up and
we're excited to hear about your talk
testing systems at scale thank you
thanks Tony let's see yes the product
the product it's really an open-source
library I try not to basically say too
much about it
right except clearly the presentations
like this it's actually called Iago with
an eye the font used for these
presentations is sans-serif so please
keep that in mind as you look at the
name of it because I constantly get
questions about logo and people who are
clearly using sans-serif fonts are
looking at this going okay that's an L
it's not an L it's an eye yeah anyway so
I joined Twitter about two years ago a
little more and when I joined Twitter my
first week we had an outage where the
site was not available to users and the
experience for the users was this this
is not a great experience for your user
if you care about actually keeping your
service up although it's an amusing
picture we'd love to you know people
love to make fun of it clearly if you're
an engineer who works at Twitter this is
a very sad face image you do not want to
have this showing up and your mother
calling you and saying hey your system's
down so how do we get rid of the fail
whale in the last couple years we've
gone a long way towards getting rid of
it and I'm basically going to talk
how we do that at Twitter and
specifically the fail well if anybody
really wants to know the the meaning of
it
AOL is a 502 HTTP response code and if
it takes more than 5 seconds to render
your request or to respond to your
requests then we will serve you a fail
well instead and so that gives you an
idea of exactly what's going on behind
the scenes when you see that it's not a
great story
so why the fail well I could I don't
know if anybody I'm sure there are quite
a lot of Ruby advocates in the world and
in this room and a lot of the systems at
Twitter famously were built on top of
Ruby on Rails I'm not going to say
anything bad about Ruby I will say
though that when you have a monolithic
architecture that has all been written
in one big blob of code effectively that
it becomes very hard to effectively
manage that code both in terms of its
performance and in terms of its ability
to make changes to it so the Ruby world
has a lot of interesting ways to achieve
performance these days at the scale that
we care about and unfortunately we
really couldn't do any of them because
of our monolithic body of code that
wasn't able to be iterated on in a
reasonable way so the answer to that of
course was to decompose it and so at
this point I think we have something on
the order of about a hundred and twenty
services in production so we've taken
this monolithic body of code over the
course of the last two years and turned
it into a lot of little services which
are written in many different languages
including many on the Java Virtual
Machine Java Scala closure and then also
some written in things like Python C++
and still Ruby but because we can now
that it's decomposed we can make changes
to it more quickly our engineering
organization of hey I'm gonna build this
particular area of the of the product
and you're gonna build this area over
here actually is able to work that way
because they have their own service that
they can deploy and this guy over here
has
his own service that he can deploy and
so they can work relatively
independently of each other the problem
with this as anybody who's maybe gone
through this or if you think really fast
you might realize that there's an issue
here which is that as you as you move
from this monolithic world to a
service-oriented world testing becomes a
huge problem because in order to test
any given piece of software at a
reasonable to find out if it can handle
production load you need to be able to
stand up that whole environment and in
the case of Twitter and in case of you
know for instance the previous talk
Google that's probably close to if not
impossible and so how do we test in this
new world how do you test systems at
scale when any one service might depend
on 20 downstream services and they in
turn depend on another you know 20
downstream services and in told if you
want to stand up any front in the
environment you're probably gonna have
to stand up 80 of those 120 or whatever
number of services we have in production
sorry so I'm gonna talk about this the
rest of this talk will be about
synthetic artificial load clearly I
wanted to give a kind of a front-page
shout out to all the other ways that we
have to test performance but I will say
that all of them are in production and I
don't know about people in this room but
personally I feel like if I'm testing it
in production and my users can
experience pain or I can take the system
down let's say in production then that's
probably too late to be testing it and I
really want to test it before any of
that can happen but the techniques that
we have to test in production as far as
performance goes are Canaries canary if
you're not familiar with the concept you
have a cluster of systems that are
handling requests maybe a thousand of
them you deploy your new version of the
software to one of them and now it's
taking 1% of the traffic and you get to
see hey is the traffic is it getting
better or worse as compared to the old
system clearly though it's taking
production traffic there's dark traffic
this is a little safer it is basically a
system and we use this heavily at
Twitter
- we have a front end called TFE funny
name right
TFE will take traffic and then if it's
configured in such a way as to do this
it will send part of the traffic to the
or rather it'll send all of the traffic
to the normal production systems and it
will copy it'll send a second request to
a system that's not stood up yet and
whose response it's going to throw away
all right
but now users aren't affected unless for
instance this has happened
sadly the system that it's sending the
traffic to doesn't respond in a
reasonable amount of time and TFE isn't
necessarily configured correctly in
order to handle that let's say pretend
that this might not have happened six
months ago and and lo and behold you
have an outage caused by dark traffic
and then the final one is more
interesting to people who are doing
functional testing tap compare is the
ability to do it do a dark traffic read
or a dark tract of traffic right like I
just described but compare the results
and make sure that the software you're
about to stand up actually sends the
same content as the software that you
have in production today and with that
said I'm not gonna talk anymore about
testing in production well I will but
only in a scary really like
inappropriate way but it's fun so so
what did I do
I joined like I said two years ago and I
got I was I was in the performance
engineering group at Salesforce I came
to Twitter specifically to try to solve
this problem as far as performance goes
and I wrote a load generator and load
generator is internally it's called
parrot it turns out though that there
are any number of trademarks associated
with parrot including on you know most
unfortunately the parrot VM that Perl 6
will run on some day and we didn't when
we open sourced it I couldn't call it
parrot so it's called Iago I actually
had no idea that Iago was this bird that
you see up here
I'm too old to have seen Aladdin every I
crowd-sourced the name I said I can't
call a parrot guys what should I call it
a bunch of people wrote back and the one
that got the most votes was Iago I'm
like Shakespeare
I don't get it but okay sure you got a
lot of votes and then I found that it
was Aladdin so yeah so why did I need to
write a load generator there are a lot
of load generators out there I've used
most of I'm not gonna say all of them
but I've used a lot of load generators
in my lifetime I've used a meter I've
used loadrunner I've used a bee I've
used flood iView I mean I'm not gonna go
through the full list but I've used a
lot of them so why did I feel like I
needed to write a new load generator
first of all a lot of code at Twitter is
written in Scala I wanted to learn Scala
I had written some toy projects in it
and so I was like hey wait what's the
best way to learn a language you write a
prod you write some piece of code that
you already know how to write in this
new new language that'll be great
sure that's an awesome reason to invent
a new load generator right another
reason is that you know you ever get
this sort of thing like if I'm not
actually doing engineering that the
other engineers won't respect me so it's
like if I just bring in jmeter or I
bring in loadrunner and I start running
load against their system I'm not gonna
get nearly as much credit or respect in
the organization as if I wrote my own
right so yeah
another reason hey all those other load
generators sorry guys
you suck there's really no reason to to
use any of them they are all bad right
clearly I hope the answers obvious none
of the above
but when it comes to load generators I
have done this three times before I got
to Twitter and then I did it a fourth
and I open sourced it so hopefully I
never have to do it again but I really
want low generators load generation to
be simple and I have a lot of experience
when it's not simple of what happens and
what happens is that you end up with
very large performance engineering teams
and they're very hard to hire onto it's
a very specialized skill and so you're
sitting there constantly like trying to
scramble and figure out is it gonna
scale is it going to scale and you don't
have enough people to answer the
question why or what if it will and you
don't have enough people to answer the
question or why won't it when it doesn't
so if you're an engineer as I've been
and you want to answer this but
will my new service new whatever feature
think is mode that I've just that I'm
about to ship to production is it gonna
scale can it handle a million QPS how do
you go out and do that you sit down in
front of your computer you're an
engineer and guess what you write a for
loop right so a for loop if anybody's
not ever written one I really hope that
all of you have sits there and you know
you write a for loop let's see I think I
have a piece of yep there we go so
that's a for loop it's we're gonna start
up a hundred threads and we're gonna
make requests as fast as we can against
whatever service it is that we have
stood up and you know apologies this was
not code reviewed so it may not be
actually compiling but it's you know my
sort of theoretical abstract for loop
and any load generator that that you're
gonna use had probably be able to be
better than this if it's not better than
this
then the engineer is just going to sit
down and they're gonna do it themselves
because this takes five minutes to write
if that so that is if you're writing a
load generator or you're gonna be if
you're in that business you had better
outperform the for loop unfortunately
though at least if you're an engineer
the for loop is really appealing and if
you're say J meter and all of your
configuration is either via XML if you
want to do it that way which I thought
most people don't or it's a GUI that is
looks like it beamed in from the late
80s and if you're loadrunner it's a GUI
that happens to be really unstable and
can't do automated testing at all or if
you're a B it's something that's
completely not configurable it'll and if
I'm started a bash on jmeter I'm sure a
lot of people here have used it it takes
forever to actually write a load test in
jmeter and if you want to do anything
interesting at all you're gonna be
writing a plugin and I have done that
and let me tell you I do not wish it on
anybody and so it would be really nice
if whatever this load generator was was
really easy to write new things in it
has to be extensible for loop actually
satisfies all of those constraints but
the
Loup has a problem it turns out that
it's actually quite wrong as far as
testing performance of your code it will
give you nice warm fuzzy answers of I
can handle 40,000 QPS who look at my me
and my product and my service it scales
really well but it turns out that that
has nothing to do with what's actually
going to happen to it
when you get into production why is that
so let me talk about some systems theory
sorry to be academic for a second does
anybody here know littles law raise your
hand if you know littles law awesome
nobody here knows a little salt I'm glad
I put the slide in
so littles law is a really awesome
mathematical result in queuing theory
that says that if you have a service and
request to arriving at a particular rate
that the number of concurrent users or
concurrent requests that that service
will be able to handle can be found by
saying what is the request rate and what
is the latency or what is the time it
takes to service that particular request
and that might sound intuitive and it is
if you think about it like a hey if I
have a hundred requests a second and it
takes a second to satisfy that request
then I will have a hundred requests
concurrently right it's a simple math
but it turns out that the math behind
that is actually pretty hard to derive
and it's awesome that it's such a simple
result but really it's just math so why
do we care what is littles all have to
do with a for loop well if you are a
performance engineer you very much want
to change one thing at a time about your
system so that you can know for sure
that the performance impact that you are
experiencing is because of that change
right flaky tests were mentioned earlier
flaky tests are the bane of performance
engineers because you never get one
answer you always get a range of answers
whenever you run these tests and so it's
really important that you be able to
change something
try it again change something try it
again and get results that you can count
on but littles law actually makes that
very hard for to do that on a system
which is specified in terms of
concurrent users so open versus closed
systems
basically an open system and again I'm
not speaking like I'm not talking about
you know open-source or any of the open
you know like API isn't or anything like
that this is Systems Theory an open
system is a system where requests can
arrive independent of your ability to
service them so in other words an open
system is what you get when you put your
service on the Internet
where your users as much as you wish
they would do not care that you're slow
right now or that there's a big spike or
that Michael Jackson just died they're
going to send you request whether that's
tweets or trying to read tweets
regardless of your ability to actually
handle them a closed system is much
nicer to model it's one where it's like
if you can imagine probably the best
example I can think of is a call center
right where you sit there on the phone
with the poor person at the who's
sitting in front of the computer there
were only 500 agents so that's you know
that's the concurrency right there 500
and they sit there and say I'm sorry the
system is slow right now I'm waiting for
your date of the load that's a closed
system it's awesome because you can just
say all right I need to be able to send
500 threads jmeter can model that quite
well but in a in the real world we model
things with requests per second so in
that connect concurrency is equal to
latency times request rate we need to
model request rate not concurrency so
one more thing jmeter and almost every
other load generator I've used by
default will speak HTTP which is fine if
you have a web service clearly Twitter
as a web service so why isn't that
sufficient well it turns out that when
you build a big service oriented
architecture you're probably not
speaking HTTP on the back end at Twitter
we're talking thrift thrift is a wire
format that is similar to protobuf which
many of you are probably familiar with
if you're not familiar with thrift and
you have to be able to talk thrift if
you're going to be able to test systems
in isolation because you can't stand
like I said you can't stand up the whole
service of Twitter all 120 of them and
then send some HTTP requests in order to
make something happen way down you know
10 systems deep so that you can find out
whether or not you can handle the load
right so you need to really be able to
test components not test the whole
system
supporting thrift is really really
tricky it turns out that in thrift you
actually bite you you coach in your
protocol and you typically do that at
compile time so when you're building
your system you have bound it to a
particular RPC by the by the act of
building it and that means then that all
of your data objects are you know their
at compile time and if you're imagining
a world like jmeter
then it means that milo generator is
suddenly going to have to know not only
your protocol right which is tricky in
and of itself but also gonna have to
have all of your cogent stuff which
means that if you think about it there's
120 systems at Twitter so that means
that now my load generator code is
dependent on all 120 thrift interfaces
that they expose and I open sourced it
so now it's dependent on anything
anybody can ever do which is clearly
impossible so I had to change the mental
model of how this works in order to
satisfy that need to support thrift HTTP
you don't have any of those problems
HTTP is a very well-defined protocol
cookies headers you know post different
kinds of requests but very well
specified and nobody's going to end well
other than HTTP 200 nobody's going to
invent a new kind of HTTP that you need
to support in order to make your load
generator work but thrift thrift is a
lot harder so I had to flip the model in
terms of the load generator so in the
jmeter and any other system you might
use world the dependency is on the
system and instead I flip that and the
dependency now goes from the system to
the low generational library and so I
said when I got up here I talked about
the fact that Iago is a library it's not
a product per se it is a low generating
library you depend on it just like you
depend on any artifact and and what it
gives you is enough stuff that allows
you to write to an interface which
allows you to implement your particular
low tests and I'll show you some
examples here in a minute
and you write to that interface and then
you launch it and it ships with that
dependency comes enough like you know
jar blah blah blah comes along and then
allows you to launch it it comes with
effectively an execution environment
along with it
secondly so not only is a augo different
in that it it flips the dependency model
on its head it also uses finagle if
anybody has used to finagle then you're
probably like a cool finagles awesome
I'm assuming that's not most of you
finagle is a Scala library that we use
heavily at Twitter it's open sourced we
wrote it and a lot of people are using
it and it allows us to do asynchronous
our pcs over thrift or HTTP a really
name your protocol my sequel was
recently added and we have a google
Summer of Code shout out to the Pope's
who organized that google Summer of Code
project this year for adding zookeeper
support for instance so chances are that
if if it doesn't support what you need
to do it probably will be there or is
easy to write finagle because it's
asynchronous solves our open system
versus closed system conundrum that I
described earlier and in an in an
asynchronous world where you have it's
based on Neddie so if you you know which
is in turn based on Java n io so if you
have an async world like that you can
basically say hey I needed a request to
arrive every millisecond let's say that
my arrival rate is a thousand RPS so I
needed a request to arrive roughly if I
had a uniform distribution every
millisecond okay great I'm gonna send a
request every millisecond I'm not gonna
have a thread per request I'm not gonna
care how slow you're slow or fast your
system is a request will be sent and
will arrive every millisecond if your
system is down then I'm gonna get a
connection refused if your system is up
but slow then there's gonna be a socket
hold open for a certain amount of time
but the resource constraints on my test
system are not going to court interact
with the resource constraints or the
problems that you might have on your
service that kind of coupling is death
to a performance test if you get your
your system on your tests coupled with
your load generator then you've created
a closed system and you're no longer
modeling reality at least modeling
reality for most of us here
maybe clearly I described a case where
somebody could create a closed system
but for the most part I believe
everybody here probably works at
internet scale and therefore you are all
needing to model open systems and so you
need something where it's going to
generate those requests regardless of
how slow or fast the services at this
particular build so that's all cool but
what's it mean so I mentioned that I'm
gonna get to some code I'm actually
moving faster than I expected maybe I
should slow down
so we'll have a lot of time to look at
the code but there are some implications
about this model of hey here's a library
you depend on it you write your load
test using this library and then you
just launch it and it runs and it's all
good for me the most important thing I
came from a team at my last company
where there were when I left I think
there were 15 performance engineers and
now I've heard that the team has over
the last two years and change the team
is now something like 50 performance
engineers and their job is to write
performance tests and that's that's what
they do
and in order as the engineering team
grows there they have to hire more
performance engineers right in order to
performance test all the stuff I think
that model is very hard to make
successful especially when I started at
Twitter the company was quite a bit
smaller I think there were roughly let
me think there were roughly 200
engineers at Twitter when I started I
wasn't gonna be able to go and
performance test every single person's
thing they were all moving really fast
way faster than I could move just by
myself and I wasn't gonna go out and
hire another 19 performance engineers
really quick and say okay great now we
can performance engineer now we're gonna
be able to test your stuff and oh by the
way today we're something like 900
engineers which is the size of actually
just Google's test team but never mind
so you know now I would need 90
performance engineers or whatever the
ratio is right let's say it's 5% and I'd
need 45 or so and you can project out to
infinity and realize that in fact this
is probably
not gonna scale because there aren't
that many performance engineers on the
planet right and so it's really
important to me that if you're an
engineer and you're building a service
that you'd be able to write your own
performance test not that you can go and
learn jmeter and not that I force you to
go and God for be able to use loadrunner
but that you actually can just write
some code like you would write your for
loop and write your own performance test
so that's a clear implication of this
dependency model switch is that you're
no longer working in a world where most
of the domain expertise is something
that's unfamiliar to you you're working
in a world where it's your service you
probably already wrote a sample client
when you wrote your service and you just
use that same code to generate the load
test it's very simple similar to how the
for loop is very simple I have been it
like I said I've been at Twitter for two
years I've written to load tests the
entire time that I was at Twitter I
spent the first six months writing the
load generator I wrote a couple sample
load tests that were actually never run
people looked at and said oh this is
easy I'll just do this myself and and I
never actually ran those and there were
two instances where I had to do a load
test because they were testing some
aspect of the system where they're
either in one case no team existed that
was maintaining that system and yet we
were expecting it to get a large burst
in traffic and in another case it was
crossing so many organizational groups
this was the most recent example I had
to test for New Year's Eve
and so I wrote that test of what happens
when 30,000 Japanese people all tweet at
midnight exactly right it's the best
distributed denial-of-service attack
you've ever seen and so those are the
two load tests that I had to write I'm
gonna show you one of them I don't think
I have time to show you both but I'll
show you the most complicated ones so
you get is some sense of exactly how
Harry can it get and I'll talk about
that in a second
there are some unintentional things that
happened as a result of doing it in this
way it turns out I mentioned that you
have sample code already written it
turns out that you actually have to
write a lot less code when I write a
jmeter plugin there is a ton of
boilerplate that I have to implement in
other systems I have to write a whole
bunch of stuff just to get it to do
what I want iago you were the code
you're writing is the code you already
have so you probably are just copying
and pasting it in the worst case or
pooling it in as a dependency in a
better case and so you just don't have
to write as much code it really takes
people literally you might think I'm
exaggerating but it typically takes an
engineer at Twitter who has never been
exposed to the low generator before and
and everybody now at this point I
believe well except the you know 50 or
so we hire every month will have been
exposed to it and but they takes them
about 30 minutes typically from I don't
have anything my manager just told me I
should probably find out if this is
going to scale to all right here I've
got a load test and I've run it and I
found out that in fact it doesn't scale
at all oh shoot I have to go back to the
drawing board or better case it's it's
awesome it also has another really
fascinating implication that I'm gonna
get to hear in more detail in a second
but it it means with finagle finagle
makes it really easy for you to think in
terms of asynchronous operations it has
this concept of a future hopefully many
of you are familiar with that and from
the Java world the signature the future
signature in Scala is a little nicer and
easier to work with and it is isn't it
is in Java and it allows you to compose
our pcs in a way that lets you say
things like first I need to authenticate
this user then I need to check and see
what their acts if they have the right
access then I need to run this request
against this back-end service and I'm
going to scatter gather so I'm gonna run
this request out to a hundred different
services and I'm gonna compile the
results and I'm gonna throw away the
ones that didn't come back in a certain
amount of time and then I'm gonna
collate that into a response that I send
to the user and I can do all that in
about three lines of code so finagle
makes it really easy to compose really
complicated our pcs using that in the
load generator in turn means that you
can write really complicated load tests
really easily
and I didn't expect that at all because
I had never been exposed to finagle and
that was a really nice result the thing
that that means is that as you're gonna
see it means that you can write very
complicated functional test scenarios
then normally before I had a tool like
this the way that I would do
testing and make sure that the service
is behaving correctly at the same time
let me describe a scenario where this
isn't done let's say that you have a
load test and you're running it and your
service is performing really well and
then later somebody has the bright idea
to go and look and see what's actually
happening and what's happening is hey
it's amazing the performance is awesome
it's really really good at serving a 404
right or it's really really good at
serving a 500 because there's an
exception and it turns out that an HTTP
200 with a nice graphic like a fail
whale might to your load generator look
like success but in fact the right thing
isn't happening at all well how do you
solve that problem well of course then
your load generator needs to you know
hopefully pay attention to more than
just the response code but also you
might think that you're just this is the
way I solved it in my last gig you run
your load and you're running a
functional test beside it right and
you're like okay under this amount of
load these scenarios still operate
correctly and so here I've got jmeter
let's say running against the service
and I've got a selenium test or
webdriver test running simultaneously
and that's fine as far as it goes but
wouldn't it be nice if you could
actually say a hundred thousand users
can all do this at the same time or they
can do these 50 different things all at
the same time and it turns out there's
honestly I don't know of any way to do
that other than this load generator Iago
so right I didn't have to load test 100
services so I had a lot of time to
dedicate to making Iago great it's still
not as great as it could be we are in
the process of sinking out a bunch of
stuff to the open source repo bug fixes
and such but I would really if people
are inspired by this talk feel free to
come and make contributions
unintentionally the idea of having a
service which is really solid became the
model for how we do all of what I call
developer productivity are probably a
lot of you it's not a not a realization
I had a loan but we basically are
focused I would manage a team now
focused on developer productivity
and our build system works this way are
the test frameworks that we support
works this way our test infrastructure
setup works this way our deployment tool
works this way and so it became a model
for for how things should work is that
give engineers great tools that are
really easy to use where the default is
almost easier than that where the it's
almost easier to use them than to not
use them and they will do the right
thing and so that's how we're doing
things at Twitter which hopefully is no
surprise to anybody all right
now we get into the meat hopefully you
can all read that I actually don't care
if you can't read that too much because
it turns out that this is just an
example of how to write an echo service
in in finagle using thrift so you can
imagine that you would have a service
like this it takes a you know takes a
method alright it has an RPC that says
echo and then it's gonna reply back with
what you wrote that's just I'm trying to
be relatively open kimono here there's
no hidden like dependencies or well
other than on finagle and things like
that but there's no hidden code here
that's really how the service is written
I cut it off at the end because below
that is all the zookeeper discovery
business and I figured people don't care
here's the client I mentioned I mean I
hope anybody who's writing a service is
probably also written a client for it if
you haven't then you should probably
think about doing that because you learn
a lot of interesting things when you
behave like your customer whoever that
is but here is a client that is going to
send the message hello and when it gets
the response back it's gonna print
something out and say alright I you know
I got a hello back right and that's how
you write that code using finagle
talking to my echo service and it's not
a lot of code right there's a lot of
noise in there about types but other
than that it's pretty self-explanatory
and by the way if anybody here doesn't
know Scala and you're totally confused
my apologies here is a load test I'm
gonna jump back in alright take a look
at that for a second notice that is
calling client echo right I'm gonna go
back client that echo
right so if you see that it's actually
the same code and in fact it turns out
it's less code how is that possible how
could it be less code well it turns out
that Iago takes care of some of the
boilerplate as far as connecting to a
service goes so this load test actually
does a little more than the sample
client it actually if it gets a hello
back then it sends another message and
and logs it right so that's a really
simple example I'm gonna get into a more
complicated example here in a second it
turns out that if I wanted to write a
for loop that just sent echo requests
asynchronously as long as I knew enough
about Java niÃ±o or Neddie or some other
library like that it would be really
simple for me to do that without without
too much effort so great what have I
gotten how can I do better so we have
more complicated problems than that it
turns out at Twitter and hopefully most
of you as well so load testing direct
messages direct messages are a feature
in Twitter where you can send a private
message from one user to another as long
as there's a following relationship
between them with some exceptions that
I'm not going to go into related to
brands where they'll allow you to you
can direct message them without them
following you we I didn't put any of
this in the slides because I was worried
that coms would come after me so I will
talk about this rather than rather than
I have to get slides approved so nothing
that I'm gonna tell you is confidential
per se but I you know at any rate we had
a TV show one of these reality game
shows similar to say Oh
American Idol where people are singing
they get up on stage they sing you vote
and then the America picks the winner
right they go home with hopefully a nice
recording contract that doesn't take too
much of their money and one of these
such shows wanted to use Twitter to do
the voting and this show would had a
five million dollar prize at the end and
they were going to do the voting every
single live show over Twitter and
include those votes along with
everything else but in order boating
online
has to be private at least without some
major exceptions and so they were gonna
use DMS and at the time and I'm not
specifically not in order to not get
myself in trouble I'm not talking about
when this was but at the time we were
getting roughly 400 teams a second over
our system so 400 messages a second
back-and-forth that were of that's of
this type and when you figure out that
most of the TV showing happens in the
East Coast and central together which is
most of the population of the United
States and that all of this voting would
be happening at the same time because
the polls open as it were at the end of
the show and so everybody immediately
goes and they do their voting so you get
a flood of traffic we figured out that
we were gonna have roughly on the order
of 2,000 to 3,000 dams per second and I
and as I said the production traffic at
the time was 400 and so we it was
clearly we were gonna have a large
increase over what we could handle so we
needed to be able to test that this is a
direct message again I'm gonna show some
code and actually I'm gonna go a little
quickly because I have about four
minutes but I'm not gonna go super
quickly that's just a you know
underlying type that would hold the
direct message here's my processor you
might actually be able to figure out
from the code what the TV show is so
this we're actually going to show a lot
in this like five I think slides of code
here so don't get too attached to any
one piece of it this is happening up in
the constructor there is an eval and
then we pull in a config and then
there's a couple of URLs this is using
our API 100 in point instead of our API
one one in point this code was written
long enough ago that there wasn't too
one one in point and by the way none of
it has been code reviewed so please my
apologies I know it's not the highest
quality code I wrote it in literally
like a day so there you go here's some
stuff it turns out that if you're gonna
have five million dollars riding on the
votes that you had better get them right
right you better not drop any votes on
the floor you better not have votes show
up for people you didn't expect and so
the voting like in my load test when I
talk about being able to test curve
nests and Load at the same time turns
out to be critical in this case because
I need to be able to make sure that of
those 3000 dm's per second or whatever
the number is that I'm going to test at
that all of them show up for the right
guy right or girl or whoever it is and
that no no like I said and that none of
them get dropped on the floor arrive at
it well out of orders fine in this case
so I need to track a bunch of stuff it
also turns out that I don't have a
system another system like Twitter
that's not Twitter right I don't have a
copy of Twitter in my back pocket that I
can just stand up and run a bunch of
tests against because that would be
expensive and it probably wouldn't fit
so I need to use Twitter in order to run
this test which means I run the risk of
knocking over production as it turns out
but anyway that was a known risk and so
but it also turns out that I don't have
millions of users in my back pocket and
I can't go create millions of users
because it would throw off our analytics
and our growth team would get really
excited and I would have to break the
news to them that that was just me and
and so I don't have a million users in
my back pockets so I decided you know
what I'm just it doesn't matter I did a
lot of talking to other engineers this
was pretty shortly after I arrived at
Twitter I didn't know much of the system
at that point so I talked to a lot of
people and I said no it probably doesn't
matter you can probably send all these
Dame's from one account so I sent all
the deems from myself literally I use my
own account but it turns out that we
have rate-limiting at Twitter and so if
I need to send 3,000 deems a second if
any of you want to do this you're gonna
get stopped almost immediately all right
but it turns out that my load generator
is running in production and so as
access to memcache which means it has
access to the right limit right it has
access to my rate limit token and I can
go and reset it as part of my load test
so the load test is sitting there
damning and meanwhile it's also talking
to memcache saying that guy no we're
gonna zero out his rate limit again all
the time
over and over and over again quite
rapidly and I mentioned that I want to
make sure we don't drop these on the
floor our customer was using our
streaming API which internally we call
hose bird and so I wanted to make sure
that I was getting the same data that
they would be
so I make a connection to hoes bird so
there's my connection to memcache right
there's my connection to hoes bird I'm
gonna i alighted the connect a hose bird
code because it's not that interesting
and then here's my load test so I need
to pick a particular host to hit I need
to decide what I'm gonna send so I have
a vote randomizer function that I'm
going to call I need to set up the
request I'm gonna make a URI out of that
I'm gonna set the OAuth header with all
the right correct
mouths annotation codes and I'm gonna
make a yog Oh slash parrot request with
all of those parameters and then I'm
gonna map that onto my request call my
service which is that second line up
there and when the response comes back
I'm going to decode it and check to make
sure that I haven't exceeded my diem
limits and if so reset them and then I'm
gonna look and see if it's a if I got a
decent response back meaning that some
HTTP response is supposed to an
exception that I got on the wire then
I'm gonna read that DM and I'm gonna do
some check for missing diems at the end
right it's very simple logic it turns
out that this logic actually had a bug
it's kind of funny I think anyway you'll
see so the way I wrote it originally was
all right I'm gonna send a message and
when you send a DM or anything on
Twitter you get back when you create
something you get back an ID so I'm
gonna get back the ID and the response
and I'm going to be waiting over here
for the stream to give me a DM with that
ID right and I found out that we were
dropping tons and tons of dams on the
floor this is horrible right it's not
working at all what's the problem
it turns out that hos bird was
delivering the DM before I actually got
the response back so our streaming API
is fast enough that it will send the DM
down before you even know what the ID
was right and so I had to create DMS
that have unique text in them just so
that I could match them up because I was
going to get them on the stream inside
before I got the response it's awesome
bug so yeah that's all the code that's
it that's how you test ants so what do
you what do I want you to take away from
this so clearly most of you some of
how many people from Google are here
Google clearly deals in more scale than
we do as does Facebook and a number of
other sites but in general most of the
world doesn't handle the amount of test
load that we have at Twitter just to
give you an example when I went into
this one of the things that I wanted to
be able to do was handle was to be able
to generate a lot of load I had run into
scalability problems with jmeter and
systems like you know where they have
very simple request response patterns
meaning you know give me a counter great
I got a counter back and you can be very
fast at that and 40-thousand QPS was a
lot for jmeter to handle and I knew that
Twitter handled more than that when I
came here when I came to the company so
I wanted something that could scale
really well and I had in my head that Oh
a hundred thousand RPS would be cool
that'd be awesome and one day somebody
needed that much and we were able to do
it and it was no big deal
and then somebody else came along and
they wanted five hundred thousand QPS
and I was like wow that's the large
number okay let's see I did tuned a
couple things and then we did that and
then and then I haven't touched the code
honestly in like I don't know six months
and a month ago I was idly sitting
lunchtime conversation and I hear behind
me two million QPS we tested and I turn
around oh my god what two million who
what really I was like you didn't do
that with our stuff so what did how did
you generate two million QPS that's
awesome I want to know what did that
turns out you can generate two million
QPS
that is almost an afterthought versus
everything else that you get to do with
this so pretty interesting as far as
Twitter's problems go it turns out that
you probably all should want your
engineers to be writing their own load
tests instead of doing it yourself and
you should also clearly want your load
test to be accurate and model the real
world there you go I have all of one
minute so for questions and I'll be
available for lunch
great thank you James
I got my takeaways again this time one
is I learned you were a optimist and not
a pessimist all your four loops had
starts but no ends yeah and I also
learned I have to go learn Scala and
somehow if somebody slipped in like a
next generation list Lisp
somewhere in the world so Scala is it I
also know Foursquare does Scala so it's
growing in popularity we are gonna take
a break here in a minute I think we have
time for one question if we want to go
to the mics so if you have a question go
to the mic otherwise we'll take one off
the moderator then we're gonna take a
break then we're going to come back here
exactly 11:30 and we're gonna go into
the next set of presentations that will
be done by Mozilla and right before this
question two things some people have
been asking if the slides will be posted
yes they will be posted and so will the
video and the content and all these
questions and everything after the talks
so they will definitely be out there and
number two there's a lot of people
standing up in the back of the room just
some to stretch their legs but there's
also people that like a seat so when you
come back from the break kind of squish
into the middle as much as you can so
with that we'll go to a question I like
- all right so here's the first question
it says does Iago allow clients and
languages other than Scala in other
words can developers port their clients
and other languages to write performance
tests yeah so II sorry can you hear me
all right so Scala is a JVM language
which means that those libraries
available to any other jvm language
which means that as long as your load as
long as you have code written for the
JVM and that can include things like
JSON or JRuby for that matter so really
anything that you write on the Java
stack for you can use Iago okay oh
thanks all right with that let's go
ahead take our break and we will be back
at
11:30</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>