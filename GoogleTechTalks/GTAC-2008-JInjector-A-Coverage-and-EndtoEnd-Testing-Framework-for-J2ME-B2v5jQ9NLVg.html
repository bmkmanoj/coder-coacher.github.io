<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2008:  JInjector - A Coverage and End-to-End Testing Framework for J2ME... | Coder Coacher - Coaching Coders</title><meta content="GTAC 2008:  JInjector - A Coverage and End-to-End Testing Framework for J2ME... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2008:  JInjector - A Coverage and End-to-End Testing Framework for J2ME...</b></h2><h5 class="post__date">2008-11-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/B2v5jQ9NLVg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to introduce you to the three
speakers we all work for Google and
we're going to talk to you about Java ME
testing so I'm Julian hearty I've been
the company two and a half years working
purely and mobile QA Michaela sama who's
a PhD in turn and been doing the bulk of
the work that you're going to see today
and Olivier guard who is a software
engineering test been the company a
couple of years as well and basically
helped us to improve the code and did
some studying work and it's a good
example of Google working together so
how many of you got one of these things
have you written software for one of
these things oh good a couple of you so
it's easy isn't it okay maybe not so
we're going to talk about how we do
system level test automation using a
tool that we've written called James
Ector we're hoping to make this tool
open source it'll take us a couple of
months to sort out the details and if
anyone would like specific details just
get in touch with any of us directed by
email afterwards and if not then
hopefully you'll see an announcement on
code at google com so Jeremy which used
to be called j2me so you'll hear has
used both terms what's the main
constraints of it the first thing is
that this little thing ain't got four
gigabytes of memory if you're lucky your
phone's got 64 megabytes of memory but
some of them are down to less than a
megabyte and the original specification
require you to run your application in
about 128 kilobytes it has a limited
number of threads so the more powerful
devices have about 15 threads which
sounds like a loss until you look at the
complexity of some of our applications
where we're hitting a hard thread limit
there we have limited processing power
so the software tends to be optimized to
run these devices now j2me came about
and is very basic it was for mono
screams and potentially no screens at
all so there was no concept of accessing
the file system it was just the minimum
Java that you could fit on device so how
does this device and our software
actually work it takes advantage of
things called J srs java specification
requests
which run through a community process
and people decide whether they want to
implement these on a device by device
basis the modern devices support
something called mid p2 which is a
refresh with a certain minimum number of
Jace ours and the application model is a
midlist life cycle which I will talk
about briefly in a minute the user
interface is called the LCD you I a very
basic user interface think back 15 years
ago on the desktop and this is roughly
where we are today with mobile devices
recently son has announced the
lightweight you I tool case which I
don't think many people have yet adopted
but in theory it improves the old UI
development but typical applications
don't take advantage of that yet in
terms of test automation j2me unit is
about all we've got it's probably about
six or seven years old now and it hasn't
really moved on much from there as
intended purely for unit testing now
before I go any further I'm going to
take you to one of our applications so
this is a YouTube application and we're
going to start some testing on this now
these testicle take a few minutes to run
so we'll come back to them later on this
has normally run fully automatically in
a continuous build process so you'll
notice in the background that we're
using the same capabilities so the java
toolkit to minus things at the HTTP
traffic if we're interested in recording
that and we can bind to memory usage and
things like that going back to the
slides there so Google and our clients
typically our clients a front ends for
other services how did you use gmail
more than to actually ask about earlier
mail services that maybe not in this
conversation so how many of you use the
mobile client okay good number of you so
we have a mobile client is optimized to
get your messages to and from you over
potentially expensive network
connections so the client has to run
unconstrained phones but it runs on
probably 50 60 different main phone
models typically we have about fifty
percent of our code dedicated to you I
so our applications are gooey driven and
they're driven by events general
depressed in by the user so the user
presses the soft keys and our
applications response a soft keys the
servers do work they send responses back
over the air on a good day that takes a
few seconds but in congested networks
and with other problems we might take 10
or 15 seconds well it means is we can't
just rely on a fixed time out for doing
things like waiting for an event to
complete we ship lots of data of the air
so your emails for instance videos on
youtube and we have to go to deal with
those and I'll test how to be able to
detect when there are problems there
typically the way that we test these
applications that suggests most people
in the industry test these applications
is a combination of unit tests which
tests relatively little and manual
exploratory testing which means people
sitting with boxes yesterday I think was
James Whittaker asks how many people
have phones lots of phones to test with
well I personally own 30 phones I have
roughly 500 in the office and that isn't
enough to test these applications now
imagine how is going to test an
application by hand for an hour on even
50 phones that's more than my week and
perhaps I'd run ten test cases per phone
so it doesn't really scale very well
moving on now to the midlet life cycle
so mid 'let's are specified by stung son
sorry and the jvm they run in the
sandbox which means they're not supposed
to access the file system and we can't
start them we have no control over them
so it started by the runtime environment
they have a defined life cycle which is
a very simple life cycle so the
application is started it can be active
or paused or destroyed when it's paused
we're supposed to release resources
resumed we get the resources back we
need and then eventually we destroyed
and the applications can't share memory
and this is important from the test
automation perspective so going to hand
over to Michaela now
any of you have ever implemented an
application using as you do I the
standard graphic toolkit for dynamic
radiation one okay so this is a sample
of code which I took from from the Nokia
forum I actually simplified it and this
code is actually once compile and
deployed is creating that application on
the left so it's an empty form with a
search button in these few lines of code
we have already two really important
concept for as you do I the first one is
in order to put anything on screen we
need to have a reference of our midlet
and the way in which it works it by
assessing the midlands we have a
reference to the display and in that
display we print something we display
our form the other important concept is
acid why it's a common driven graphic
toolkit so in order to fire event we
need to fire commands and this is
actually the way in which this is the
suggested implementation so our
comparison between the received the
commander the listener is receiving and
the command which was stored in
internally somewhere and if these two
instances artists are matching then the
event is unlit so sorry yesterday we saw
out to test out automate the testing for
a simple application using groovy let's
suppose that we want to test the same
application but using a mobile device so
let's suppose that we have an
application containing a search form and
in that cell phone there is a text field
in which they usually supposed to type
something some keyword and then there is
a search button which the user compress
and in order to trigger the search the
code in this light is showing a possible
implementation for the test so the first
thing that we need to do is to retrieve
an instance of the elemental screen and
then we can use type checking to confirm
that that is actually an instance of our
search form and then we don't know if
the text field will be private protected
or public so we can use
flexion in order to assess any field of
that instance and if we find actually
our text field we can then inject the
keyword that we want to use for our test
and with the same approach we could
hypothetically find the command and then
we could fire this command and we could
start the search and then I will our
test will run this keyword if we can
also implement some API which will make
this which will hide this from our
tester and we can hydrate or toward all
the instances until we don't find the
writings and that we want to test
however Java me tradition doesn't
support reflection and it doesn't either
supporting introspection so this code
will actually not compile is actually
not possible to implement this test like
this and the original assumption that we
can get the reference of the elemental
screen imply that the midlet that we are
executing is actually our application
unfortunately if we try to create a test
suite using j2me unit we will soon
realize that the test runner itself it's
a midlet so we're when we are executing
the display get current display on the
midlet we will actually get the
reference of on only the element that
the test runner is displaying not the
element that we would like to test so
actually the example as that I showed
before it's impossible but we can we can
automate testing for j2me for Jeremy
tradition using using something that we
found in the literature so we found this
robot I'm a robot to me it's a it's a
framework for record and replay events
on NC do I it's based on atom which is a
library for bytecode instrumentation
from object web and the way in which it
works is during the recorded session it
actually locks all the of the command
event which are fired and during when
the application is instrumented in order
to replay those events the application
is tested and the expected behavior is
verify
unfortunately this implementation is
highly coupled with the seed why so it
only allows the test application using
those who do i as a as a GUI and it
assumed that all the events will have
the same latency so if yes if you want
to test an application using different
carriers using under different network
condition this is not enough for us
moreover this library assume the right
is playable to be on screen and this is
something that we don't want we need to
wait for our application to react and
this is a strong constrain especially if
you want to to test application which
are making queries or remote back ends
we also need code coverage to measure
the quality of our task to give a sort
of measure of which quantity of the
application we are testing and
unfortunately they the most used tool
for code coverage I'm then cannot run
with within Jeremy tradition because Dan
McReady shun doesn't support custom
class loading as far as we know the only
other tool for code coverage on mobile
is cobertura mobile cobertura mobile is
supporting of cobertura which is a tool
for java standard edition it is it is
really well designed it has a very good
object orientation internally
unfortunately such a good design when
applied to the mobile world generates an
application which is too slow and it
becomes so slow that is sometimes
impossible to test it because it the
test reach a timeout because the
application is not consistent anymore
with the behavior that it should have
however even cobertura instrument the
application so we found that this
instrumentation will be the right
direction in which to explore and we
ended up implementing our
instrumentation tool with J injector and
we followed three Bay's principle well
we wanted our tool to be general purpose
so the the instrumentation is general
purpose and then we instrument the
feature that we want our application to
have here today we are showing two of
them the end-to-end testing and they
could cover it but we have other feature
then we wanted our tool to be
multi-platform we support java me
traditional and blackberry rim we also
applied the injector to one droid but
android doesn't add all the limitation
of the ramming tradition so it doesn't
really make sense to use the injector on
it and we want those who to support
multiple graphic libraries so not only
SE dua or the other framework from sun
but also custom library that we have
internally and the most important thing
we want the instrumented version to be
optimized for mobile we want to minimize
the computation that our instrumented
code is inserting and we want to reduce
the footprint because this instrumented
version is going to run in a device in
which the memory is strongly limited so
how can we use this tool inside our
build process so we are suggesting two
possible uses the first one is on the
fly and the second one is after
deployment so on the fly starting from
the compilot class we can instrument
them in the injector introduced
reference to a set of super classes
which need to be compiled and all these
two set of classes need to be provided
provera fight together and then the
thing that code will be deployed in a
couple of jarring job that deployment
block here is a single block but we have
a deployment for each different model
that our product is supporting so for
example we will have a deployment for
nokia X 60 our deployment for sony and
so forth or we could instrument the
application after a deployment so
starting from each single couple of jar
and jad we could instrument the
container bytecode and then we need to
compile the required support classes we
will prevail file the code to God
together and then we will deploy once
again the application the difference
between these two approach is that in
the first one we only instrument the
application one and then we customize
the deployment in the second one we
start from from an application which has
already been deployed and then we
instrumented so to instrument multiple
deployment we need to instrument the
application more than once then the
question is why did we need to
instrument application why we
just didn't insert some some extra code
in our source code well there are a few
reason for that first of all some
classes some some code cannot be your
Fiji access so let's suppose that the
code is shared between multiple projects
developer then don't really lie to all
of us to insert code which might modify
the behavior of their application and
moreover the design for our end-to-end
testing is really different on a good
design for for a production Cal so for
instance production co need to be small
need to encapsulate fields need to
remove metals which are not needed and
while on the other end and to an testing
need to to expose as much field as as it
need so we decided that instrumented the
code it was making sense for us and the
other point is we might need to do
something similar to Devon see injection
but apply it to mobile so let's suppose
that we have an application which is
using a certain back end we could inject
a dependency to testing back end which
is failing in some in some way that we
know and we could test our application
against those failure so let's let's
have a look at to an example of one of
our test this is a test applied to
YouTube mobile in which we are replaying
the last performance search our test
followed a given pattern and the idea is
that initially we assert a set of
precondition and then we fire events we
wait for the application to react those
event and then we assert assess opus
condition and we continue with the
pattern so in this example at the
beginning it is necessary to check the
initial state of the application the
initial state for this test is that the
home screen will be displayed and that
the search button will be focused if
that is not verified our test will fail
and weekly report or something up that
something wrong happened that implies
that the application after us our top is
not in a state which is known as as the
initial state it does assertion our past
then we can fire a right click event
that right click event would be
automatically processed by the
application
and it will select the search button and
it will display that search pop-up
dialog with all day with all the search
which has been performing previously at
this stage we need to wait for the
dialogue to be loaded in memory and
displayed on screen once it is once the
dialog is displayed we need to assess to
assert the validity of the dialogue so
we need to check is contained and we
need to check that is displayed
correctly if the diagonal if the
dialogue doesn't appear then our test
will fail if the content of the dialogue
is not correct so for example if the new
search field is not selected our test
will fail because that is not the state
that we expect in the application if
that is that is verified then we can go
on and we will fire a donkey event and
we will wait for the second entry in the
dialogue be selected in this case the
star and then when that happened we can
fire a right click event which we start
a search India we need to wait for the
searching for Papa dialogue to appear
and we need to wait for the dialogue
also to disappear which will imply the
end of our search at this stage the
application can fail for different
reason so for instance if if we don't
have any network connection or if the
back end doesn't doesn't transfer into a
given time this this searching for
dialogue with disappear and the narrower
message will appear in the era our tests
need to to check this condition I need
to report the right error message if
everything goes well a result screen
will be displayed and that this stage
our test can verify that the result
screen is correctly displayed on screen
and it can also verify the correctness
of the result in this example I'm
showing the code of a fatass which is
trying to search for gtalk and which is
very fine that Jeter will return a
result containing J injector so the
purpose of this light is to show that we
can encapsulate all the logic of our
test and we can create FBI in order to
buy the the ugliness of firing event and
and we can create tests which are really
easy to maintain so the idea here is
that we are following the page the page
objects layout as it up as it happened
for web development and that's writer
just need to to invoke those object and
to fire and to invoke this method as if
they were using the application
themselves all of those method are
blocking and internally they trigger the
application they fire the right event
and they wait for all for those
condition to be met and if something
happen the determine exception they fire
event saying something went wrong and
it's real easy to understand what went
wrong because we know exactly which one
of this cold is failing as I mentioned
before using j2me unit is impossible to
obtain an instance of the elemental
screen because they took the test runner
of j2me unit is a midlet so we have to
implement our test runner and in this
light i'm showing how we always start
off as a stronger bot form for a midlet
and boss for a remap l'occasion in a
midlet week we can start our test runner
at the end of the startup tomato we can
do that because we know that that made
up we return on the other hand in rim
application start from a main method and
at the end of the the main method they
start an event dispatcher method which
will not return so we need to start our
test runner before the invocation of the
event dispatcher so when we when we
instrument our application with it with
two lines of code depending from the
type of application then during the
execution after the start up the
application will run a test suite which
will be executed into a parallel tread
and and all the tests will have access
to certain entry point on the
application which will is with a used to
to start the testing so now let's go
back to our demo and let's see the
result of our test fit
so if any of you ever executed the test
suite using j2me unit to me unit is not
displaying error messages on screen is
only logging them on the console so if
you run a do to me unit test fit in in a
real device you will have the number of
failing tests but you will not have an
error report which which may not not
really useful to test fit itself so this
is the report there a report of our test
runner and in this case we have a test
failing now there're message is that a
search result for for a query star which
was the test I show you before was empty
so this this was an acceptance test
which was checking the compliance
between this client and the back end the
normal back end for YouTube and when we
wrote these tests the specification say
that the star was supposed to be a sort
of Welsh are returning a full set of
result while performing these in this
client returned at an empty result so
this is a violation of those
specification and starting from these we
could explore the set of call of the
search team and we can only it is
possible for our tester to realize what
went wrong in this search okay now
Olivia will talk about code coverage so
we we show that using instrumentation we
can run end-to-end tests on LG to me
apps I'm now going to speak about
another feature of Jane injector which
allows us to a gate code coverage from
all tests so why do we need to create
such a tool because there are already a
lots of tools like that out there so for
instance tools like email corbett
cobertura already exists and our open
source projects so we we had to create
another tool because the the tools that
are out there are not optimized for j2me
apps and that's
for lots of other tools so for instance
test framework like j units or test mg
are not going to work with j2me up
mocking libraries like easy mal college
mr connecting to work either static
analysis tools and so on and so on so
it's really a challenge when writing a
jetway me up because lots of the tools
we are used to use to improve the
quality of our software are not
available so when we design this code
coverage figure we had in mind to have
something really optimized to have a
small memory footprint and to use as few
cpu as possible the d3 me apps already
use lots of the memory available and the
cpu available on the device so we needed
to use as few resources as possible so
here's an example of three test runs the
first one is when we disabled the code
coverage I mean tests running without
any instrumentation so it's a set of
tests from a common library user cougar
so the second test run is a using tests
that has been instrumented by change
actor and we gather the code proper edge
data and the third one is the same using
cobertura so you can see that there is
change act only add a small overhead and
it's really fast with liquid the tests
and gathers a coverage data so i think
that speed matters a lot we spoke about
test execution speeds in other torques
we want the test to be fast one of the
reason is that developers are going to
run their tests more fun if the tests
are fast and that's the same we can
apply the same ideas to other tools like
code for veg tools so if it takes ages
to get a culture which reports the
reverse are less likely to
use the tool of of 10 so another thing
we wanted to optimize is a memory
footprint so one idea we had in order to
have a small memory footprint is to cut
one of the feature which is a used in a
common culture of edge tools in the
report usually you have for each line
you have the number of times a line has
been executed and it takes lots of
memory to store that for for each lines
so we we chose to only store if a
missile has been executed at least once
and we can stall that in in one beats so
it helped helped us to to optimize the
memory and we also add other
optimization so now i'm going to show
you i mean to explain how we implemented
the the the code coverage gas gathering
controlled atta so we used as Michaela
mentioned earlier we should use a tool
as object web its library that allows to
browse the byte codes really easily and
you can it's using a really smart way to
do it using a visitor pattern and you
can really easily specify that you want
to add miss out call for this class for
this missile so what we need to do for
together code coverage first we want to
initialize some data structure in order
to store afterwards which line has been
covered so that's what is done by the
activate curvature methods which is a
static missile from from the coverage
class and we do that in the during the
the midlet initialization and the second
thing we want to do is at the end of the
of the when the application steps we
want to write a data file that contains
all the information we gathered so we do
that using the correct coverage
and once we have that we need to fill
our data structures so the way to do it
is to fall each line of course it can be
executed we added using the
instrumentation wizards in order to
specify that the line has been executed
so here 56 is an arbitrary number I mean
it's a unique ID which is going to
identify this line and afterwards we
could we can map this index to the the
class name and the end the line number
so once we start once we gather this
information at the end we can write a
data file to the file system so if we
run the code coverage tool on a mobile
device we will have to get it from USB
for instance it's a file which is using
the Elco formats and afterwards using
this file we can easily generate HTML
report I don't know if you have heard
about sergey curve it's a one of the
tool from these packages is a HTML gem
which allows to create a really nice
report and you can browse your source
code bye-bye package with the the code
couraged data overlaid on this report
right so why do we bother with all this
well here is an example of some of the
benefits we had so the first is we're
able to test the event handlers and
event listeners for the applications
simply it's impossible to test that for
j2me applications without doing code
injection we were able to detect a
number of bugs in our application some
of them quite mature applications where
it turned out that developers hadn't
realized that some of their changes had
affected the way the items appeared on
screen and the manual testers hadn't
spotted it either because as I mentioned
at the beginning there's a limited time
available for testing and lots of
different devices the second thing is
that we can
test running so the beginning of this
talk I left a bunch of tests running for
about 10 minutes we can leave test
running overnight on devices and then
come back in the morning and look at
things like the memory leaks and again
we found a number of memory leaks in our
applications which all helps make them
more robust probably for the first time
ever I think we're able to actually test
on real phones so rather than running in
emulators as I demonstrated on the
laptop so typically people would do
their tested and emulated because they
can't fit them on phones now one of the
benefits are doing on phones is we trip
up over all sorts of permissions issues
in complete support such a srs and
things like that which again we simply
couldn't test any other way when injects
dependencies into the code so we can
modify even very hard to test
applications and I'll touch on that a
little bit later on code coverage was a
poor cousin in terms of mobile so a
Google you've heard various people
talking about good testing practices and
code coverage is one of them but there
weren't any tools so the tests were
running blind effectively it was up to
the developers to look at the results
and now we can actually give them
standard code coverage in our standard
tools using standard file formats the
Elco format now it all sounds simple and
you've seen a couple of screens that
show you two or three lines of code in a
pretty graph well that's taken about a
year to get make happen to make it
pretty and easy and the nice thing for
developers is that the system tests are
actually written in a style that models
j unit and j tony unit so it means they
don't have to learn yet another way of
writing their tests so life isn't
perfect and here's some examples as the
challenges we face so the first is it
does introduce overhead typically an
application when it's instrumented with
code coverage and tests is two to three
times the size it was that means we do
have to work hard to squeeze it onto a
phone so even starting at the youtube
tests on my sony ericsson phone takes
about two or three minutes so that's
quite a long time still better than me
sitting there and testing manually i
promise you but longer than we'd like
the second is is one of the paradoxes
that we now have a new way of automating
the testing developers like automated
testing but when we actually present
results to them they say are but can we
reproduce it on a real
and maybe it's your instrumentation code
that's causing the problem maybe you're
the memory leak nutters so typically we
then have to be a find a way to
reproduce this manually on the devices
and the other thing is Michaela Bradley
showed you four lines of code of how you
implement a test and underneath that a
hundreds of lines of code that does all
the hard lifting and to write those
requires in-depth knowledge of Java so
it's not the sort of thing for a junior
test that a writer a junior developer it
does require someone competence in Java
we've found that writing the test
because we're digging into the guts of
the application means that we highly
coupled which is undeserved okay I'll
hold it what the heck so it'll keep me
awake if not you lot so we basically
have to implant new decorators for
classes that requires knowledge of the
source code now in comparison robot army
doesn't need knowledge of a source code
it's able to put the instrumentation in
without that I'm not saying that it
would do it that way but it is possible
and we've chosen not to do record and
playback because we want to create a
depth into calling our applications so
our testers the people writing the test
need to understand our applications very
well and the changes do have side
effects the behavior of the application
again our instrumented code does slow
things down slightly and we need to
maintain our changes as the application
changes so here's a very trivial example
before you all shoot to me of a typical
problem we find in code where a
calculation is actually being performed
in the draw method now good software
practice says you would do the
calculation beforehand and then display
the results of that calculation in our
instrumentation we've had two instrument
the equivalent calculation and to make
sure we get the correct results which is
just bad bad bad but that's what we do
until such time as we're able to
convince people to change their code
which generally happens so before
actually move on to questions I just
want to make one little point about the
joys of gtech so I was in New York last
year and I met Michaela there by chance
one of the brakes he was recently had
done his master's and he was just
starting his PhD in UCL we got talking
about j2me which is my subject in terms
of testing and he joined Google's an
intern for three months we like to lead
herbed so he's a long-term intern at
Google while we're waiting for him to
finish his PhD so he's done great work
I'm very very proud to have let him
g-tech so with ask any questions tell
mckaela wants me to run the demo for you
can tell he's proud of his own work so
we are we are in the process of a person
of open sourcing it so it will probably
be downloadable in a few months at the
moment it hasn't been released yet
so other questions I think we should run
the organs question answer at this point
so what happened is that we have a
continuous build so in practice solve
this test runs every time there is like
a new code checked in and in well in
general this should be used as a sort of
regression test but at the moment we are
using it as a sweet after the unit test
so what happened is that we actually
find box which are going to be fix it
like really soon so basically did by the
time I report them developer had already
fixed them that is what happened and in
general the suit always terminate it
doesn't happen that that it crush the
reason why this happened is because we
are we are actually modeling tests as in
a normal unit way so we have a setup and
a teardown and if if a test doesn't
doesn't complete over a certain time out
which is like really long we actually
restore the initial state of the
application if possible if that is not
possible then we are forced to to
terminate the full suite because then
the other tests will not be able to to
check the initial state but that is
really unlikely to happen
so in order to instrument a graphic
library like we need to have different
instrumentation for each different
library the the common principle are the
same but we just need to support each
different one so it like if you have a
if your company has a custom library
which they implemented internally and
you want this tool to test that library
you probably need to two other set of
helper class which will help the test to
interact with that library but I mean is
something that we did internally at
least three time and by the time we get
experience with that it doesn't take too
much time actually the code if your
libraries is well designed then it takes
just few time sorry so testing pass and
fail that they pass three times and they
fail the whole time flicky test oh well
if we is the same for any other unit
tests if you have been like a good
developer in writing your test then your
tests are not flaky if you introduced
some like point like for example when it
was displaying the then I out of our
task we have those waiting point in
which we need to wait for the
application to load so those are
typically the most critical point so if
a tester is is good in writing the test
so it is able to explain exactly what it
means to wait for the application to
load than those tests we could always be
correct they will all be lonely fail is
something bad happened but if the
development was lazy or if he didn't
have enough knowledge of the application
and we put some some condition which
weren't completely correct then the test
can become flaky but so it happened to
me to have a couple of tests which were
flaky and as we said before if the code
is not properly design so if the code
smell is difficult
instrumented so it happens few time that
the developer were trying to fix that
code and as a result some some of our
tests become flicky at the beginning
they were failing one over a few times
and then with more the original code was
changed the more those times were
failing and then we had to refactor them
and the critical point is in the weight
that is the more complicated logic in
the test but is usually at least usually
contained in some support API so you are
just to fix it once for your library and
then when you said you can use it in all
of your tests yes
so the short answer is yes then the real
answer is a the the wtk from San already
provides you one feature it directly
inside the emulator to to introduce
latency in the network and that is
introduced at the virtual machine level
and I strongly recommend you to do so
instead of if you want if you want to
inject latency in your code is probably
because you want to run it with latency
directly in the device but yeah you can
do that but I mean you can just run your
tests in a position in which your cell
phone doesn't have a good connectivity
so it's much simpler but yes you can
other questions free beer not even
that's going to stare them now okay well
thank you very much
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>