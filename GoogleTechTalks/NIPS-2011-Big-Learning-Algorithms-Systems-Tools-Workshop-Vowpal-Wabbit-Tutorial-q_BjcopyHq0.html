<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Vowpal Wabbit Tutorial | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Vowpal Wabbit Tutorial - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Vowpal Wabbit Tutorial</b></h2><h5 class="post__date">2012-02-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/q_BjcopyHq0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt; LANGFORD: So, I suddenly got a use for
it, say in reference will go with the other
project because it's been going for a while.
So, one of the first goals is to create a
really good, fast, scalable machine learning
algorithm.
And it is not [INDISTINCT] the most important
there.
We're not just being [INDISTINCT] it's [INDISTINCT]
we get started.
[INDISTINCT] is also become supporting research
into machine learning algorithms.
And you might think that this is sort of antagonistic,
but so far, it's been working out very well.
Often you get new ideas about new machine
learning algorithm and trying it out for [INDISTINCT]
and if you figure out a good machine learning
algorithm, in terms of their [INDISTINCT]
and structures, it's been the same irregardless
of their [INDISTINCT].
So things like [INDISTINCT] and whatnot.
The code base is pretty simple, so--and we
would like to keep it that way.
And immediately what is important is it just
works.
So in general, there are a lot of libraries
that you can use to try to enhance a particular
program and we didn't use that.
So, that's sort of painful but it's very good
[INDISTINCT] because it's just use--you just
[INDISTINCT] and it typically works.
So, we have these in use in our recent [INDISTINCT]
and this is my favorite application.
And we normally use it to help people find
true love.
That's true.
I remember talking to [INDISTINCT] and [INDISTINCT]
on camera but, you know, I start [INDISTINCT]
talk to him and--so, allow me to go but it's
true, it in harmony and he said, oh the harmony,
yeah.
The story of my [INDISTINCT] me and [INDISTINCT]
forms, you know, it's true.
You know, you've been [INDISTINCT] but now
I'm going to--times are changing.
Okay.
So this is the outline.
I'm going to go through the basic online default
linear algorithm which gives you a sense of
sort of how things work.
And then I'll discuss kind of what goes wrong
with basic online linear learning and how
to fix those things.
And then Miro will talk about the LBGFS implementation
and then Alekh will talk about the Terascale
Learning from the cluster.
It's probably Alekh's part is the most exciting.
Okay.
So, I want you to [INDISTINCT] first.
First thing we're going to do is [INDISTINCT]
this is the [INDISTINCT] pulled down in a--into
a binary classification test then [INDISTINCT]
you have a bunch of lines and each line you
have a label and then you have a bunch of
features, there's feature 13, in sets of value,
[INDISTINCT] 2 and it just kind of repeats.
And the essential thing about this is that
this free [INDISTINCT] by conventional standards.
Nothing that you use just 424 megabytes.
This is, by the way, a five-year old laptop
[INDISTINCT] unfortunately is how long it's
going to take to actually learn each dataset.
And we see [INDISTINCT] examples of the farthest
columns keeping track of progressive validation
loss.
What happens is you--it evaluates--you get
a prediction, you see the label, you compute
the loss for that label, for that example.
I mean you could get the average or all the
examples of this loss, right.
So, the important thing is that you compute
the loss before you can take away the assembly
now.
And then that's [INDISTINCT] average loss
of 496 which is pretty decent.
And [INDISTINCT] about 60 seconds of loss--which
is quite easy on a five-year old laptop, single
port.
So, maybe we should mention a few other things.
So this is the same number of years that have--I
restarted that for every printout.
This is the progressive validation loss of
that printout, right.
So this gives you a sense of having to changing,
kind of, instantaneously or simultaneously
or else this is going to be a total progressive
validation loss.
First of all this loss is very nice if you're
making one pass because you could press it
when you pass that loss.
And let's keep checking the number of examples.
I guess there's a bit over almost 300,000
examples.
And there's something like 60 million features,
non-zero features.
We only got the non-zeros.
And this is the label [INDISTINCT] for that
example at the point you're printing things
out.
This is the prediction.
You can see the predictions are converging
pretty well and this is--this is the number
of features for that example.
Okay.
So, it's just default BW running on this dataset.
The default BW does approximately this.
I'll describe how it's different in a moment.
So, you start with all your weights having
value of zero and then you get a feature vector
which is some potentially large dimensional
vector.
And then you make a prediction by taking that
product between the weights and the features.
BW operates in a sparse fashion, so you can
have an enormous number of potential features
such as use of the one that actually matter.
And then we're going to put this in interval
01.
And then we're going to learn a label, which
is either a zero or one.
And then, maybe there's an importance weights.
So, the importance weights are kind of a more
advanced feature.
So, whatever the importance weight is--if
you say, this example is twice as important
as this other example.
And this comes up in many natural ways.
So, a common way it could come up is maybe
you have on the [INDISTINCT] or it's maybe
it's like, you know, 1% positive and 99% [INDISTINCT].
So, maybe you down sample the negatives.
Just for [INDISTINCT] reasons or for other
reasons.
And then you want to represent that the down
sample to the [INDISTINCT].
Okay.
And so after you do the label, you can see
an update.
This is just to [INDISTINCT] descent, right
here.
Then we have the learning rate times the derivative
of the squared loss which like you collate
the negative features and we also multiply
them by the important weight.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Yes.
[INDISTINCT] squared box.
The default BW is the squared loss which is
probably the simplest one to optimize.
Okay.
So, are there questions about this?
Yeah.
&amp;gt;&amp;gt; Just to [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: The--but the thing is there's
an automatic clipping going on.
So, BW by default will not output prediction
which is above the largest label it sees or
the below the smallest label it sees.
And it kind of defaults to zero one and that's
something that--that's the small triangulation
label you've used.
If you want to clip it at a different range,
like say, minus infinity and plus infinity,
then you--there's a flag that let's you adjust
that.
Now the reason--you might ask why you do that
and the answer is it works better that way.
Is there another question?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Data.
Ah.
So there's a default data.
The default data is 10.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: So, yeah.
Choosing data is often a little tricky, but
we made it far less tricky.
For this particular algorithm it would actually
be very tricky.
But this is not actually the algorithm I just
showed you.
There are some tricks we'll get into.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Yes.
We need to go into how we define the problem.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: So the answer to that, I believe,
is yes.
And then it...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Yeah.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Though it look like squared loss,
it took a flip.
We--you could get something like that.
And so, the derivative end of the increment
will be lost.
And then because we clipped before we can
do the gradients.
Okay.
So, then the question about the input language.
So, there was there was a particular language
that I showed you here.
Here.
This is a very standard kind of machine learning
language where you have pre-computed entities.
Right here.
So, one of the tricks in BW is that you don't
have to, well personally you don't have to
specify the value that will be vocal 1, and
you don't even have to specify a number, you
can just specify a word, for example.
And then the task function of the BW, which
will compute an index for you, okay?
So that will be keenly helpful in any situations
where you're dealing with ifs.
So, the input languages specified here is,
a portion at the beginning which has crude
label or sample information.
And then you start getting in the features.
So, that the--so, we have a string and then
you have an optional float.
If it is the float that you can specify then
it defaults to 1, right?
SO then--and then we have the namespace.
A namespace is--think of the namespace for
now.
Because that sort of look as though the namespace
is just an empty string, which it can be.
So, then we have features which can be a string
or just a string.
And if the string is an integer then that
index is used.
So, when it's missing it just defaults to
1, and if the string is an integer it defaults
to that integer.
If the string is just a string, then, you
know how you use the hash function to compute
an index in your way to it.
This is a very important trick, because it
gives you about an order magnitude improvement
in efficiency, compared to using a dictionary
which is the standard way to compute lots
of numbers for a string to index map.
Okay.
So now, the labels which you expect, we have
an important weight there I need to discuss
is, you can specify a date.
So, what does that mean?
So, you're computing a dot product, that dot
product can be offset by the base value, something
to specify.
This allows you to do learning of residuals
essentially.
And that's--that can be helpful in some situations.
And then there's this last thing as tag.
The tag is essentially an arbitrary string
but doesn't contain special characters like
bar, which is used here, or space.
And a tag just has information which lets
you identify the string.
However, you--other example however you want
to identify it.
So the last thing is the namespace.
BW allows you to group features together in
namespace.
And the language for a namespace is exactly
the same as the language for a feature.
And if there's a float which is not 1, then
they get multiplied by the floats that are
inputted in the namespace.
And then, when we have features grouped in
the namespaces we can take grueling operations
on those namespaces inside of the core running
algorithm.
This is again, sometimes a very important
optimization.
Okay.
Are there questions about the language?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Yeah.
So, often you want to interact two sets of
features, right?
And I suppose if you were at Yahoo! when you
wanted to display a news story to the user
while you have features from the new story,
have features from the user.
Better be interacting them or else you're
not going to be getting anything interesting.
Okay.
So, here's a couple of examples, the basic
three examples.
This is a simple--this is just to cut down
on the RTP1 dataset.
This is a tag.
Let's go back to that.
So, there's a slight modification from a previous
version of BW and that, you can--you can put
an apostrophe here and then you can put swipe
tag and you can have a space.
But, if you don't to swipe apostrophe you
can just not have a space here.
Probably better to use apostrophes in this
that work.
And this is maybe a more complex example.
We have a label and of course waves.
We have a tag.
We have two namespaces.
Drew was the first one who--Drew [INDISTINCT]
was the first guy who convinced me to do a
tutorial.
And his [INDISTINCT] is right there, because
this for a student who's in his class and
figure out if this works.
It's important to we use the Drew figure and
the [INDISTINCT] features and the interaction
of the two of them.
Okay.
So, let's take a look.
And now, there's this how do you execute information
in the BW?
There's a lot of different options for that.
It seems to be everything you can think of.
So, these things, you could put this through
by a file.
You can use [INDISTINCT] again.
You can use a daemon on some port you specify.
You could tell it to run over the examples
that it sees multiple times.
You can tell it to use a class file.
Now here's something important.
So, I showed you how big this was.
It's 144 megabytes, but the truth was this
dash C intersects using a class file, okay?
And it turns out that, if you use a class
file, it's about to affect your [INDISTINCT].
This is in parse to the [INDISTINCT] the cache
file is a very optimized format that it can
parse very quickly.
And that means that it doesn't [INDISTINCT]
the problem very much because the class file
is still pretty large, 336 megabytes.
And then if you want your--by default the
cache files are not compressed.
Sometimes you have do want to compress them
as well.
And so, you can do these compression on the
class files.
All right.
So, yeah?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: It depends.
It depends.
There's a wide variety of IO speeds from a
slow disk to a fast XO speed.
And for a slow disk or a fast CPU, you could
be useful but often it's not.
Often it's more of a state savings.
And so, maybe you could just save the limited
amount of disk available for some reason.
The amount--so, BW's past format is actually--I
mean it's not compressed like these are compressed.
But there's no wasted bits in the--in the
obvious augmentation.
So if--often compressed will give you less
than a factor to load into their dataset.
Okay.
So, you can also output things.
Because if default diagnostic information,
that's a really useful for debugging.
So for example, if you are running along and
you discover that your numbers of feature
is just one that means something is very wrong,
right?
And you can--you can--the key thing is if
you could debug before you can--you can get
through the entire dataset.
So that's often helpful.
You can output predictions to a file.
You can output raw predictions before the
[INDISTINCT] to the file.
You can send examples to a BW [INDISTINCT]
running as a daemon.
You can also send just the raw text to the
BW that's running as a daemon.
Advantage of using send to is that you get
the compressed class file format over the
network.
And then there's this audit option.
Audit is very [INDISTINCT] to debug.
So, audit let's you--it turns--for each example,
what the featured name was or was at, featured
indexes after that, what the feature value
is, and what the [INDISTINCT] value is.
You can just watch BW run, run by line as
it goes through things.
[INDISTINCT] and it's that helpful.
And the last thing which can be helpful is
this one [INDISTINCT] you can turn off the
diagnostics.
Okay.
So, you can also manipulate examples inside
of BW.
So, you can tell it--ST, you tell it to ignore
the label.
So, there's no [INDISTINCT] going on.
With dash Q, you can tell it to interact with
the features.
But dash ignore, you can tell it to ignore
the other features.
You can tell it to not use a constant.
You can tell it to sort the features, which
is helpful for compression purposes but incompatible
with in grams or skip grams.
So, sometimes you want in grams as features,
in which case you can't use your features.
And then the last thing you can do is you
can tell BW that even though it's an easy
integer in text you should treat that as a
string and just use a hash file to compute
an index which is sometimes can be cool.
Are there questions about this?
Yeah?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: No.
BW just runs through files in order.
And that's actually very important because
if you're doing online learning then if you
have all pluses and all minuses you're going
to lose.
I don't have a shovel.
I guess it would be easy to include that but
it could've been.
I think often if you're working with large
datasets, the format that you put the dataset
in is designed to set in and then you're either
choosing to, you know, put it in a sorted
format or a time word format.
Both of those work pretty well with online
learning.
Okay.
So, now [INDISTINCT].
[INDISTINCT] is kind of complex but let's
just think about it [INDISTINCT] in a time.
So, we have a learning rate that's L and we
do multiple passes and we can indicate learning
rate between the passes but by default that's--that
doesn't do anything.
D is actually 1.
And then we have this kind of funny power
thing, P. And so, now I'm going to be talking
a little about how learning rate works.
So, if you have a constant learning rate you
never converge.
But that can be what you want if you turn
this [INDISTINCT] right.
If you have a learning rate which is [INDISTINCT]
1 over T, that's very aggressive.
But they can be justified when it samples
their ID.
If you don't believe they're samples their
ID then 1 over 0.5 or E goes over 0.5 is a--is
a good choice.
So, there's a lot of learning theory who says
that if A is like 1 over T though a half,
1 over [INDISTINCT] T, then that's going to
minimize awful.
Okay.
So, things in between zero and one can be
justified one way or another.
You typically won't think about 0.5 or 1,
to think upon your end of the problem.
And then you can also switch the last function
which is often less so the default is squared
less.
There is also logistic and [INDISTINCT] and
quantile and classic loss.
So, the class is what I just showed you.
There's these some differences especially
with the data loss [INDISTINCT].
So, this is the important weight of the example
[INDISTINCT].
So, I guess [INDISTINCT] the point that we're
dictating according the sum of the important
[INDISTINCT] sum of examples.
Okay.
So, let's--this is--this is how you specify
update rules and then there's [INDISTINCT]
weights because BW uses a hash function.
You need to tell it how much--there is a hash
function to use.
The default is 18.
[INDISTINCT] margin numbers are useful but
you're going to see smaller numbers [INDISTINCT]
because it [INDISTINCT].
Because [INDISTINCT] will recur, this is why
[INDISTINCT] to address that.
You specific a [INDISTINCT] model.
This is very helpful if you're trying to--if
you're trying to understand what's going on.
You can say printout the regressor and then
we will format.
You can tell it to save after each pass and
then we will pass through the examples.
Often you want to use them for the early stopping
to figure out which regressor you're going
to go with and then you can just save for
last.
And there's a couple--and this is [INDISTINCT]
of what is says.
And so, I'm going through these basics relatively
quickly just to give you a sense of what's
there.
I realize now that one thing that I forgot
is the truncated gradient is now there as
well as L2 regularization of the online learning.
And you use those [INDISTINCT]
so you can do L1 and L2 regularization of
the waste.
Okay.
So, that's the basic system.
And now, let's use of the default system.
But I guess it's important to think about
how the computer [INDISTINCT] the features
again when you compute the change in the prediction.
So you would know that x2.
And the equation just what is--what is this
change, what does it estimated value going
to be and leave the computer disk.
Think of this is computable in both form for
all the losses that we care about.
In particular for squared loss, you have this
1-e is the [INDISTINCT] instead of one [INDISTINCT]
e to the -h. H is the importance rate, data
your learning rate that can go there.
So this is similar to imposing a gradient
to be aware of these.
The imposing gradients also taking into account
the curvature of the loss function.
But this is--maybe it's a little bit nicer
because it's--you guys can see the update.
The [INDISTINCT] gradients are in place [INDISTINCT]
you can actually represent them for several
cases.
So, you know, naturally this is very helpful
when you have important rates.
It doesn't entirely solve important rate problem
but it helps a lot.
Surprisingly, this helps even when you don't
have an importance rate problem.
You mean, when all of your importance rates
are one.
And why does this help?
Well, unless you do something like specifying
important rate [INDISTINCT] default rate,
maybe it would take at one of over, let's
call it D, the default.
So what's going to happen is, these importance
weight update--this important weight and varying
update can never overrun the label.
If you have a very large learning rate or
very large importance rate, often what happen
is you're to shift your rates until you reach
the label.
So, if you have really large importance--a
pretty large learning rate to start with,
all you have you to do is you kind of click
in a learning algorithm optimized for the
realizable case.
Where it's a possible predictor for you.
You move your weights all the way over until
you get this perfect, correct.
And you're on to the next step.
I think it would be a very nice default.
Okay.
So, are there questions about these?
Here's a graph we did, so it's using the important
way to wear update or just the standard update
or a whole bunch of in parameter settings.
And the--you can see that everything's is
[INDISTINCT] on this line or below.
And there's--and the whole bunch of important
way to wear updates is you have an accuracy
which will bring you to one.
If you recall there are a bunch of parameters
you get to [INDISTINCT], right?
So, each of these does is different tuning
of those parameters.
Okay.
So this is a very use3ful pick.
This greatly reduces the amount of searching
over parameters you do.
But there can still be problems.
And the problem can be that it takes forever
to convert.
Such that the who haven't done consumer learning
very much, these will come up.
Because it's seldom--they have pieces which
have range, you know, -3000 to +8000 and they
have [INDISTINCT] which have a range from,
you know, 0.01 to 0.03.
And then the--it takes it a very long time
to convert.
And the--I think the problem is that, [INDISTINCT]
is not very good in dealing with features
that have different ranges, with different
scales.
But I think it's helpful to think of the--of
the things in different units.
So, every dimension is a unit.
And what you want to do is--you want to think
of the link--the upper learning algorithm
as a unit transformer.
It transforms from units of the input features
to something like the probability or the--or
something like that.
And then--so then the features that [INDISTINCT]
information machine, the gradient--it's a
[INDISTINCT] of the ways at units of one over
X because if the body of x doubles you want
to have the body of w in order to keep the
same predictions.
All right.
And then, you know, the gradient has units
of x, of xi.
So, you have this kind of strangeness where
you're adding gradient to the widths.
And you it doesn't add up.
So, it's a problem.
Just it's very strange people have used.
The first thing you can do is just don't do
that.
So you just have--you just look our recently
normalized.
But the more fun one, you're seeing more sophisticated
update.
So, John Guze was one of the people who worked
on this.
You can have [INDISTINCT] have per-feature
learning rates which depend upon what you've
seen to that feature previously.
Now, what happens is for each feature on each
previous example, you have some predicted
gradient.
You can square that, add that up.
It's going 
to be a value that controlled the learning
rate.
And now, the learning rate that featured i's
and of course we'll do our base learning rate,
divided by this per-feature quantity through
the power of P. In John's paper it was a half.
These are now--this is a gradient.
These units of x squared because it's gradient.
Those--the natural use of this LI quantity
is just x squared.
That means if P was a half, the next units
here would be x.
So that deals with part of the mismatch, and
that alone is very helpful.
You can use the other powers.
The--one that interests me right now is equal
to one, in which case, you don't have any
of unit problem anymore.
This is--this is a very aggressive type of
learning rate.
So when we use a P which is less than 1, and
we still have the units issue, and we take
care of that by renormalizing the update by
the norm of the feature squared to the this
is around--this one minus P, and one minus
P. The one minus P power.
You can turn this off.
It just makes some people feel squeamish because
it actually alters the optimization--the optimization
criteria.
So, you can turn it off if you want and then
it will work worse and you can feel happy
that you understand better what's going on.
Okay.
So, now--so you--this is what it looks like.
There you go.
So, this is exactly that norm with the power
of P1 and the--my 31 [INDISTINCT] learning
range is 0.5.
So, I'm not being as aggressive as I was previously.
Okay.
So, now we're running along.
And if you remember, the average learning--the
average area rate was previously like 24--9.0--it's
not 0.05?
And we're doing better.
That's encouraging.
Okay.
So then now, we get 0.04.
So, this--so we only have 1% improvement in
our average progressive validation loss rate,
but this is the--this is the best single-pass
progressive validation loss rate I've ever
seen in this dataset.
Okay.
So, I think we're--I'm done with my part.
Oh, I should mention--because the--yes?
Is there a question?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Yeah.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: Well, the [INDISTINCT] is there
by default.
In this squared loss?
So, [INDISTINCT] which happened to be zero-one.
And in particular, if you're using [INDISTINCT]
minus one.
Because of the way the loss was [INDISTINCT]
multi-class is not there.
It'd be easy of course to do one against all
but I think it's not a good idea.
Hopefully that will change in the near future.
Yeah?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: That's the evidence of the BW
there.
In some ways...
&amp;gt;&amp;gt; Average?
&amp;gt;&amp;gt; LANGFORD: Yes.
And so, in every single thing--you start it
with an error rate of like 0.5 and you ended
up with an error rate--an average error rate
of 0.04.
&amp;gt;&amp;gt; Okay.
[INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: That's right.
Yes.
You can see that to some extent, right?
As you--because of these--of the printout
here.
Okay.
So, the interaction of adaptive updates and
important [INDISTINCT] variant updates and
the--by normalization is complex.
It's there.
You can see it, but I only--I think this is
[INDISTINCT] need to write a paper about it.
It does work quite well.
So, I'm done with my part.
So, next Miro's going to talk and then Alekh.
&amp;gt;&amp;gt; MIRO: So, just as an outline.
I'll do a quick warm up about the Gradient
Descent and Newton Method.
I'll talk a little bit about those [INDISTINCT]
and then [INDISTINCT] how to--how to pick
their LBGFS on and [INDISTINCT].
So, we'll be solving like the most regular
[INDISTINCT] problem I can think about.
So, the implementations we have address the
two convex [INDISTINCT] kinds of things [INDISTINCT]
where our losses are bounded from below.
Okay.
It's between the extremely well-behave.
And I guess the objective that we were working
with is not just value, actually like almost
all the time [INDISTINCT] so, it's a super
duper okay.
Now, our objective basically the summation
of loss of [INDISTINCT] examples plus an algorithm
regularization and as [INDISTINCT] told you
that loss can be [INDISTINCT] weighted for
example.
And regularization doesn't need to be uniform
across all of the coordinates and the coordinate
that you think [INDISTINCT].
Now, let me just show you the one run of [INDISTINCT].
Let me just show you one run of the [INDISTINCT].
All right.
[INDISTINCT].
Okay.
Now, you can see very much.
[INDISTINCT].
Okay.
So, maybe up to--now you can maybe--hopefully,
you could see the top here.
So, I'm running BW.
Of the three setup switches, it's seems they're
related to the way where I'm getting my data.
I'm getting my data from the [INDISTINCT]
file and then the loss function I'm using
holistic, frequency is also simply data related.
These are the--these are the options that
actually LBGFS cares about.
So, that the first of them since we've turned
it on, the backup optimization, there have
been online optimization that is done by default.
The second one specifies the memory parameter.
I'll tell you a little bit more about it later.
The third one specifies the regulation parameter.
That was the one that [INDISTINCT] objective.
And I'm on the [INDISTINCT] termination, right
there.
Termination is right here in our case is very--it
is very [INDISTINCT] weight of the loss [INDISTINCT]
this will rather be tricky, the loss is rather
small and for the method like LBGFS essentially
because it [INDISTINCT] pretty fast [INDISTINCT].
Now, there is a printout that I--that was
the reason of me trying to make it smaller,
with this printout [INDISTINCT] is a little
bit large.
But I'm just making sure the headings of the--of
the columns.
In the third column, there's an average loss
and this average loss would be--is the value
of my objective.
So, this is nothing on my loss [INDISTINCT]
that I would do [INDISTINCT] after each pass
[INDISTINCT].
Now, I found out also there was the magnitude.
Just to give you an idea about the [INDISTINCT]
conditions.
You know if the--if the optimum, the derivative
is equal to zero.
So, it seems like it monitored the magnitude
of the derivative.
It's a--it's actually the optimum where it's
there [INDISTINCT], I think.
The third column is--it's that it derivative
in a slightly better metric.
So we do all these.
We do some kind of default preconditioning
that speeds up things quite a bit.
So, this is the same thing but now rather
than doing simply the optimum square, I am
measuring like probably more geometrically
suitable [INDISTINCT].
Okay.
Now, the other two columns are related to
the line search condition, you know, the [INDISTINCT]
and I'll tell you a little bit more about
this later.
And finally like the there is--like this is
the other formula this time.
Now, let see what I can [INDISTINCT].
Now, maybe I can let this get through the
[INDISTINCT] isolation.
You can look at the average loss.
You see, it's decreasing now.
I'm doing [INDISTINCT] loss, so that would
be that [INDISTINCT] different numbers that
were the default before.
It decreases and here--and it recreates another
equation you see that it decreased too much
to declare the conversion as well.
But actually people are holding more information.
So, here it became more rapid.
But still, you are getting [INDISTINCT] converted.
Hopefully it will [INDISTINCT] this is that
is norms.
Yeah, it's definitely norms.
Okay.
Now, that we have both conditions [INDISTINCT].
They are there for diagnostic purposes and
after I'll tell you later--when I'm done later
you'll be able to tell that, &quot;Okay.
It sounds fine.
It doesn't seem like you're running through
so much issue.&quot;
Okay.
And in the [INDISTINCT] can you see time.
And there is a couple of things that I'll
get to you because I just [INDISTINCT], okay.
So, those are what's after the presentation.
So how about let's generate a batch of optimization
of it to work.
That's--as a warm-up think about the gradient.
You initialize the weight vector and then
you iterate.
In this case, the--in gradient it's saying
this, I'm moving into direction of this 33%
which is the direction of the gradient.
Now, there is--there is a lot of different
way out, so think about it.
And so let's take a look at the gradient recent
update.
Another way how to think about the gradient
descent is that we are--we are doing some
kind of like a local quadratic approximation
to our function.
And I'm using this--one of this [INDISTINCT]
increase with derivative but then it has some
kind of like a second of the curvature turn.
And we are choosing the next weight as an--as
an optimal.
Now, this is--there's a different way how
to think about the gradient descent and how
you think about this like we are just using
some kind of very naīve quadratic art here.
Now, so pick any that you think is better.
Well, the--a better or it's done something
that that can after a moment of thought that
you might be--might think that, &quot;Okay.
So, gradient is like the first order after
the expansion.
Maybe by the second order then the things
will be better local.&quot;
And this is essentially the idea behind the--behind
the Newton method which you--which expands
into the--into both quadratic--in both linear
and quadratic part of the [INDISTINCT].
And then you get, so you can update.
Now, in general this--that the center [INDISTINCT]
when you [INDISTINCT] the time.
But, if your matrix is large then this--then
this, it gets difficult, okay?
It gets difficult.
And this essentially--this exactly the idea
of that the--that LBGFS is--this is the problem
that LBGFS is trying to address.
So rather than working with the Newton update
[INDISTINCT] perform the quality Newton update
that is of the common form.
There is--there is data update that it's some
kind of scaling that's typically often by
line research.
And there is a--there is some kind of a matrix
AP, which is a lowering approximation of the
inverse exit.
This is how would I [INDISTINCT] cleared should
be a signal condition or to produce a signal
using those.
Now, the rank of this--of this lowering approximation
is what would be specified by the user and
then was the memory curvature on the--on the
command line.
People were actually would be seeing that
this is specially rounded out.
I will show you why.
And in this case because I have this lowering
approximation, I do not need to discourage
any computation that--well, but instead I
can do something that's that only corporate
small--for small rank I've been.
That's the reason why ou're doing it, essential
[INDISTINCT] and it turns out that the update
of this inverted population is also been difficult
to obtaining update but this is the [INDISTINCT]
now, let's take a look closer of that--on
this update.
I told you that there's this line search--this
line search.
Let me just show you how it is--if that's
how you will suggest to be done, say for example
it wasn't converted with [INDISTINCT] and
then let me show you with the VW then.
So, VW [INDISTINCT] okay?
So, the direction of the step is determined
by this--the very closer you can update and
then the steps I've chosen that is something
called Wolfe condition I was going to give
you any fusion about this Wolfe condition.
The first Wolfe conditions is--so what I'm
showing not in the slice of the function in
the direction--in the update direction.
Okay.
So, remember I have some current function
about the step B and I'm trying to figure
in my next book.
I'm giving the direction and I will--would
like to know how far they're both to go, Okay.
That's--then to go here.
The first Wolfe condition is trying to make
sure that I am decreasing sufficiently.
Oh, to my initial local decrease.
So, the way to--the way you it does it, it
looks at the perimeter at the point it draws
a tangent.
So, this is want a tangent.
And then it multiplies this tangent by some
kind of scaling [INDISTINCT] that's more than
one.
This isn't some kind of scaling also.
And the first Wolfe condition states that
after the end of the line search, my function
now we should [INDISTINCT] low this--with
the tangent, Okay.
[INDISTINCT] so, in words that [INDISTINCT]
okay?
Now, like I--the way you can rewrite this
that would be good in the function that would
do at least--this is actually like upper bound
method scale numbers [INDISTINCT] and what
I'm out and get the diagnostic is it simply
the ratio.
I rewrite it first and then the upper bound
and the out bound.
And the thing that we correct Wolfe one is
the different ratio.
Now, let me show the second Wolfe condition
is thing what I'll be showing is the strength
[INDISTINCT] of this second Wolf condition.
And the second Wolfe condition is, instead
looking at Wolfe progress and making and matching
my first quarter of finally system.
Remember the first [INDISTINCT] states that
the great end should be [INDISTINCT] now,
I'm looking at the version of this along my
line.
So, I begin with some kind of [INDISTINCT]
I would like--I would like--I would like to
make sure that the magnitude of the result
after the end of the update increase in by
some other [INDISTINCT] then we mention the
second Wolfe condition.
And again in words [INDISTINCT] here, I'm
scaling on the data.
Is more than one and I want to make sure that
my new--the new tangent that's the direction
that the scale number.
Okay.
And again I think this things is special because
[INDISTINCT] think about and I go to the ratio
again.
If you're wondering what is strengthened part,
strengthen is because of the [INDISTINCT]
the original Wolfe condition that's on the
actual stuff.
So, some rise the Wolfe conditions we are
actually we are monitoring to the progress
ratios and we say that the [INDISTINCT] conditions
are satisfied if they are bounded--there properly
bounded the way from my--from my [INDISTINCT]
okay.
It is like geometry--it is geometry behind
this.
And so in VW we are not actually importing
Wolfe function.
And the reason why is that the--I have run
into, it might be a trouble regarding this.
Instead we'll give you report.
And so, if you see that they are starting
to hit this orders then you know that okay,
perhaps the VW is doing right now is that
is not the right thing and it opened first
project let me add a better line.
Those would be essentially for you to be there
and hopefully improve what you hear.
Okay?
Right.
Oh, it's okay.
I was making sure.
So, it's like...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; MIRO: Oh, then it's doing a bad job.
I mean, you would--you would, but I think
that this video, one way to locate a sort
of problem, right?
If you have a back classifying in the end,
definitely you may get the reasons.
So I'll prove it.
Yes.
So, let me tell you what can done.
So, essentially, as long as you--throughout
the run your body to weight.
You're probably about from zero and [INDISTINCT]
might be going at one.
Then you know that in the [INDISTINCT] you
can always be [INDISTINCT] and make the biggest
[INDISTINCT] happy, okay.
Now, let me tell you what we do because we
still need to do something.
You got to think.
You got to keep progress like this.
So, actually that one is the same.
Maybe on this life, it's essentially, what
we do, we just make sure that our laws [INDISTINCT]
This is the simple thing, like, we just make
sure so this correspond [INDISTINCT] equal
computer.
So, if our logs [INDISTINCT] decrease then
we have very simple [INDISTINCT] strategy
that the best thing to work [INDISTINCT] okay?
Now, let me [pause] Exactly.
But [INDISTINCT] anyway like suggesting could
be quite tiny and so, if I could [INDISTINCT]
produce zero, in fact, it going to make it
quite tiny.
So, the action that's will be--the only thing
we verify is the law of decreases and the--this
is all essentially just verify the Wolfe [INDISTINCT]
and the [INDISTINCT] then, okay.
Now, let me just summarize it for you and
do this essential almost [INDISTINCT] to a--the
line search part and termination in VW.
In the first termination--interaction where
to do something.
No, It's moderate.
Rather than--rather than simply using our
pre-conditioner and trusting it too much.
We actually, do--we evaluate the curvature,
we may evaluated the directional derivative
in our direction and to perform it instead.
And this give us--usually, we do [INDISTINCT]
because of you got the right idea about the
scale of this [INDISTINCT] The [INDISTINCT]
between the first situation.
And from then on, we rely on the--on [INDISTINCT]
approximation provided by all [INDISTINCT]
and as you saw--actually it was to show you
now.
The previous slide because I can explain to
you what it is.
So, in the first--in the first step, the--I'm
reporting the magnitude of my direction [INDISTINCT]
and the initial step size here is determined
according to the Newton condition either by
Newton method index direction, okay.
So, this gives you this step size.
From then on, I'm just using one.
And I'm relying on the [INDISTINCT] approximation
to do their job.
And it does--it doesn't very doing the job,
okay.
If we overshot then the step sizes would be
[INDISTINCT] and you would see VW report [INDISTINCT]
And the last thing is the termination and
I had told you that the termination--either
when you finished the 20 passes that you specified
on the comment line or when the [INDISTINCT]
decrease in the loss would below [INDISTINCT]
again, you can make this termination specially
equal to zero.
In this case, this does finish your [INDISTINCT]
your number of passes, okay.
And in summary, theses are the most irrelevant
switches.
The first of them turns off turn on LBFGS
is on.
The second one is operating [INDISTINCT] get
to the zero now.
And that's the new setting.
[INDISTINCT] equal to zero and the if LBFGS
[INDISTINCT] can be choose.
It's kind of terminates some of the history
but it doesn't try to push you regularization
off.
It tells you that look, I detect this low
curvature causing the regularize effect.
And about that--but again, in many of large
data protection is, turns out most [INDISTINCT]
as much of the problem as I [INDISTINCT] okay.
Now, the third one is the memory [INDISTINCT]
meter which specify the rank of the inverse
Hessian matrix and the final is the termination
threshold, okay.
Are there any questions about it?
What do you mean?
The first Wolfe condition like if you--if--for
alpha equal to zero, the first Wolfe condition
only set like it's equivalent could be treat
of the loss or--the [INDISTINCT] in general
like--yes.
Second one to be evaluated and we can't do
anything about it yet.
The second one could be a [INDISTINCT] yeah.
Holy, in this case--I don't know, I haven't
thought about it, it probably should have
included that's all--I mean, in this Google
itself it was like--I don't know--I don't
know.
So I guess, so we just kind of rely so by
accumulation criteria it's kind of rely on
the fact that this possible.
&amp;gt;&amp;gt; It definitely works, I was thinking about
[INDISTINCT] but again it's all been a sort.
Okay.
Anymore questions?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Oh, I guess I let [INDISTINCT] tell about
this some more.
So I guess for this one, so without showing
you I didn't want [INDISTINCT] fast, it's
like--I--most probably the [INDISTINCT] so
if you know there's something, so it's be
back about--I mean it's--the process to the
data, I mean like the computation--did the
computation over had headed by the LBFGS is
not--I mean, is not--it is much more...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Oh, no, it doesn't have like you just to
check Wolfe conditions, if you probably think
it doesn't take a examine, its inner products
between the--it's all base on inner products
between the vectors of the type D and the--I
do not--I check it only after I had from my
entire native, okay.
And 
so I had maybe 50 million premises that's
the type D but I have 100 million examples,
so essentially the--but calculating of the
gradient is the dominating--it is a dominant
cause here.
On is it like--so, from [INDISTINCT] okay,
so the...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Yeah, yeah.
Exactly, that's what it is, so we are calculating
gradient anyway because we need to this to
update.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Yeah, so that's what I mean, yeah.
Exactly right because it's not a problem,
that's why we are important, okay?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; No, no, no, no.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Oh, I see, you mean, so as I said like
there's nothing online here, so the--so, I
mean, bed size--no, the bed size full data.
Bed size is the number of examples, right?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Right.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; I mean...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Okay.
So, yeah, this is a systematic of discussion
okay, so yes, okay.
Yeah, but we go through like the entire data,
okay?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Yeah, so I guess like I did some--so the
reason why I said it [INDISTINCT] is that
on the--so I--when [INDISTINCT] like I never
saw anybody using anything larger [INDISTINCT]
between, it doesn't seem to give you too much
of improvement, the reason why you might want
something smaller is if you have large scale
data set and any of use number of parameters
it might--it might not fit in to your ram
and it might cause some swapping because remember
that the storage is increasing linearly with
it's--with it's number of terms, I mean, it's
the memory parameters counts the number para--the
number [INDISTINCT]
&amp;gt;&amp;gt; So just to--just to kind of check the--check
the context that this coming from, this is--this
is the last slide from the VW tutorial that
John gave last year and--yeah, the contract
is the derive but--and the most interesting
one [INDISTINCT] scale up to really last problems
and that one that you're going try to talk
about it in particular, we're going to try
to see how a bunch of basically very simple
and kind of some of the ideas, no, no, other
of the ideas modifications of think we know
already.
How--by collecting the right pieces you can
design a system that actually seems to get
up really, really well.
And so, again, to understand the work some
of the [INDISTINCT] in this work let's quickly
go to some of the designer integrations that
were important.
So, the first one was--that we wanted whatever
we do to be compatible with Hadoop, so a couple
of [INDISTINCT] so, we can't do that.
But actually there are--there are good reasons
to meet the objective people asked for the
last time--for a few years optimizing Hadoop,
it indeed grow up to the systems level making
the file system very efficient for the [INDISTINCT]
let's go back first with the machine learning
it's still at work with the system, so we
kind of wanted to draw among the systems aspect
of it.
But at the same time we did recognize that
Hadoop has been producing [INDISTINCT] not
really a good attraction for machine learning.
Mostly because display and learning put through
the optism, you cannot try to run them artificially
by essentially running mass reduce again and
again and again.
And that's [INDISTINCT] significant we had
clear system.
And so we wanted to have a--have a system
that's more integration friendly.
Because we can afford at least using John
[INDISTINCT] we're talking about algorithms
that I thought having [INDISTINCT] events
a lot of those algorithms that people have
used to [INDISTINCT] problems for machine
learning, just [INDISTINCT] Another practical
consideration that we took into a comfort
that we don't want people to say that--and
there's a problem with a lot of toddler learning,
toddler computation off and like you have
to kind of sit down, you have to redo your
algorithms, you have to redo your core in
honestly completely different ways.
And we didn't want people to tell people,
that look you have to throw the years of engineering
network that has gone into efficient learning
algorithms design.
So we wanted to introduce minimum course over
head for internalizing the existing algorithms.
And finally, again, most systems for this
integration, we wanted to have communication
and computation balance clearly.
Unless you have one resource being significantly
cheaper than the other then it's not a real
reason why you would want to imbalance these
two integral common [INDISTINCT] that imbalance
in either direction [INDISTINCT] the performance
of the system.
So, and finally, you have to keep in mind
that we do want the system to scale up not
just to light problems but to scale up to
large clusters as well.
Because we don't--we don't want a system that
maybe gives you speed up when you use it on
high work and after being more--like if you
try to go up to more than this side of the
[INDISTINCT] Okay.
So, and what I'll be--what I'll be describing
hopefully the next 30 minutes or so is, I
hope compatible version of AllReduce--AllReduce
is kind of [INDISTINCT]that section that's
been going for a while [INDISTINCT] and one
of the nice part about this system is that
they--the system makes it compatible with
Hadoop.
It provides various routines for parameter
averaging that you can combine with online
learning that's why we got very, very basic
distributive system.
And you use this very directly to get thousand
implementations of the online algorithms just
by mildly parameter averaging at the end.
And then makes them even better by combining
with this written implementation soft.
So, that algorithms suggest a quantitative
[INDISTINCT] and with--or the--or the [INDISTINCT]
after we point--if we have enough time experimenting
with the system seems pretty robust at a systems
level.
We've granted up to about how the nodes work
70,000 node hours and seems to have [INDISTINCT]
failures on the clusters we've tried it on
and seems to be pretty scalable in terms of
speed ups.
Okay.
So, before I go into the [INDISTINCT] system,
I just want to tell you that the system will
run on a single machine as well.
You do not have to just buy a cluster before
you can start running without [INDISTINCT]
now.
And even if you have a two-core machine you
can just very simply use at least the two
cores as well.
So and the basic--so the basic approach is
going to have a very sort of this--there's
always going to be these two things which
are going to come up in the next few slides.
And as I go--as I'll explain, there's going
to be--there's going to be the spanning tree
construction which basically a sort of--which
will be just a close step for the algorithm
that sort of [INDISTINCT] right at the beginning.
And then that process will stop for all intense.
And then you just launch--for instance, if
you have two cores on a single machine, you
just launch [INDISTINCT] twice each time running
on a different core.
And you distinguish between the two different
grams by giving them two different node IDs
which specifies that these are two different--two
different runs of the process.
And we're going to--we're going to go into
the parameters that we need to specify for
the slides.
Okay.
So the main parameters that we need to--and
go over the distributive systems that you
need to have--you need to tell your nodes
what the spanning tree covers and stuff.
So the idea is like there's a server that
only drop for which is to stand up can [INDISTINCT]
which is going to--which is just going to
be a tree.
And you have a--you have bunch of working
nodes that are actually run a learning algorithm
that are going to run with Hadoop.
And so each of them connects the--this stance
[INDISTINCT] single machine you would just
check this in low cost.
Unique ID is a--is, you know, the--an ID that
[INDISTINCT] you could have on computer, you
could have several different processes in
order you can check using [INDISTINCT] running
if you have that sort of a scenario unique
ID, just--that's why it's the ID of the particular
job that just, [INDISTINCT] associated with
and--this is the--this is mostly again, most
of these arguments are more optimize for the
cluster parallel implementation as it's going
to come up.
And total reflects the total number of nodes
that are running so this will be one or two
or hopefully thousand.
And node ID--node reflects the node ID of
a cluster in a cluster parallel job.
And again, if you actually run for in something
that Hadoop cluster, most of these arguments
will be kind of implicitly recorded from the
environment, you wouldn't have to actually
sit down beside that.
Okay.
So, but before I get to a non-Hadoop first,
it will--how these things work on Hadoop,
I just want to go again emphasize that all
of these things--well, in complete generality
you can have on non-Hadoop cluster and things
will still work.
So, let's just quickly go through how you
would do that.
So, you first need to set-up as I--what I
describe as the spanning tree server, the
node--the server that's going to set-up the
communication anthropology for the algorithm.
And you can run it basically--hopefully if
you have a cluster, you have some sort of
scheduling machine--scheduler running.
And whatever node is running that scheduler
you can just launch the spanning tree process
on that same node by launching the spanning
tree program.
And then each of the worker nodes, which is
actually going to run the machine learning
algorithm is going to [INDISTINCT] VW.
And how will you--either through your scheduler,
how will you under--figure out, you will have
arguments like node ID that you would--you
would have to specify for the particular node.
You will again need the job ID now in order
to identify what job the node is associated
with in case you have multiple jobs using
VW running on the cluster all at the same
spanning tree server.
You will just--and again, this will be--this
will take all the other learning statistics
and input statistics and output statistics
arguments that VW states in the--in the centralize
implementation and of course this will take
the spanning tree server as an input.
Okay.
So the next--so I'm just going through the
basic very simple invocation detail first
just to give you a--give you a--hopefully,
give you an idea that it's not really hard
to just run the thing.
And then we will actually talk about the interesting
side with this what actually goes on in the
system.
And so in order to run it on a simple Hadoop
cluster--running it on a Hadoop cluster is
actually relatively simpler than a non-Hadoop
one, even though the command was significantly
scarier potentially.
Now, you would--again, you would launch a
standard [INDISTINCT] on the scheduler or
you'll get a computer.
And now the very [INDISTINCT] I mean, most
of it, if you're--if you wanted a Hadoop from
that, you're pretty welcome too but you are
just--you are just simply running these things
to Hadoop streaming as not only jobs.
And there's some parameters that are [INDISTINCT]
limited [INDISTINCT] to Hadoop, like just
a couple of [INDISTINCT] actually do not--and
so, you can--you can control the amount of
memory you allocate to VW by this argument.
You will [INDISTINCT] the input and of course
to the mappers.
And basically you would--you would have--typically
had some sort of very simple [INDISTINCT]
script that you will end up writing that the
mappers will actually run.
So, if you ever see that there is this a simple
script run VW rather [INDISTINCT] that I will
show some important aspect of--in a little
bit, that kind of serves as a mapper and it
takes a couple of simple arguments and we
are running as a map [INDISTINCT] to use it.
And what the algorithm--what the system has
produced at the end when you run with the
command is each mapper is going to run an
instance and eventually you're going find
on your [INDISTINCT] systems the model sorting
a file [INDISTINCT] directly modify in that
outer directory.
And they include arguments to the script that
VW can be used to specify the arguments you
want to give VW.
So, those were all typically you can see it.
Okay.
So, there are couple of--kind of just simple
and super things that you just--you sort of
have to do because Hadoop doesn't tried you--tried
them to you enough nicely.
And those are basically--things like Hadoop
does not implicitly give you away to control
the total number of mappers that will be launch.
And as I told you, our system kind of needs
to know what is it.
So, [INDISTINCT] the total about how was the
total number of mappers.
So, that can be done very easily.
The script gives an example of how to do that
and the script is available in the [INDISTINCT]
and essentially there's a way to tell Hadoop
what--what's the [INDISTINCT] that each mapper
could be used in and you can manipulate that
[INDISTINCT] sizing the total amount of data
you have in order to figure out like what's
the total number of mappers that have been
being launched.
And another thing [INDISTINCT] that I quickly
want to highlight, one of the reasons to use
Hadoop was this very nice functionality of
speculated executions that will actually help
us be--a lot more of us to things like node
slow downs.
And that's more on that.
Okay.
So, there are two main additions behind what
John and me would have describe to make the
system work with cluster parallel implementation.
The first thing that I've been kind of [INDISTINCT]
to words is an implementation of this AllReduce
communication infrastructure that's compatible
with Hadoop.
And the second thing is we're going to--we're
going to have--basically some existing [INDISTINCT]
my vision like this already present some modification
of those and very, very simple additions to
what the [INDISTINCT] think just to make them
work in a parallel with AllReduce.
And we'll see how this involves just very,
very minimal core changes.
Okay.
So, now let's start getting a bit more into
the communication protocol.
The communication protocol is a [INDISTINCT]
that the spanning tree server which is in
a set of just parallel to pull up this communication
topology just runs as [INDISTINCT] and listen
for a connections from the rest of the mappers.
The mappers just connect to this several work
TCP their node ID and the job ID.
So, the master knows that, okay this is the
node number which is running for this task.
And if it ever receives two workers that have
the same node ID and same job ID, then it
knows they are duplicates.
So, this is actually something that [INDISTINCT]
then you turned on speculative executions
with Hadoop.
So what speculative execution does is if some
process of is lagging behind, Hadoop will
launch that process on another node.
So if maybe this node is clog let me launch
it somewhere else.
And now this node might have already connected
to the master, a second node is launched,
connected to the master and now the master
knows that two nodes are running the same
process, I should keep the [INDISTINCT] them
and that's something that [INDISTINCT] currently
does.
Like it does--the idea is that if you have
few slow nodes in the system, you get away
with it because they are re-launched somewhere
else that hopefully not slow and then we end
up keeping that one.
Yes.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; I'm not sure.
I'm not sure Duke [INDISTINCT] at the moment.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; That's right.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Okay.
So, and as I said, the other--the other things
like node ID and job ID and such things you
don't really need to specify anymore then
go.
At least on a Hadoop environment, this can
be figured out.
And again these are all present in the scripts
that are provided in the [INDISTINCT] to help
you run the course very easily so you should
go take a look at it.
And then, what happens once this--okay.
So now, the nodes had connected to the spanning
tree server.
Well, so this--as you must have already guess
from it's [INDISTINCT] the spanning tree server
creates basically a tree.
So it creates a tree topology on these--on
these mappers.
And you can't try to just--how by the--some
aspects of how this topology might be--might
be good based on the closeness of nodes although
there might be--there are probably room for
optimization there.
But once this--once this tree topology has
been decided in terms of node ID by the master
it just tells every node, &quot;Okay, this is your--this
is your parent and this is your children and
the tree.&quot;
And this point, the master's job is where
it can--it can terminate.
And now each node knows its parents and children.
It can connect to them via TCP.
And AllReduce is now going to run communication
on this spanning tree.
Okay.
So, what is this AllReduce thing?
So, it's a very simple abstraction.
It basically has a bunch of nodes connected
by a tree all of them starts with a number
or a vector of numbers.
And we're going to adjust to the example of
summing up these numbers all so you can very
easily implement several [INDISTINCT] and
I'll show you a couple of quick examples.
And--yes?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Yes, except MTI is not easily [INDISTINCT]
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; So as I've already alluded to Hadoop, for
instance provides you this very nice speculative
execution which [INDISTINCT] and so this is--this
is going to be a significant [INDISTINCT]
because the algorithms we launch can be slowed--can
be driven by the slowest node in the system.
And so, if you have speculative execution,
make sure that the slowest nodes are not that
slow, that's the significant speed up.
There are--of course there are benefits such
as, Hadoop provides you [INDISTINCT] scheduler,
will provides you [INDISTINCT] system, make
sure that problems have located close to the
data and so on.
So there are--there are benefits of keeping
the two.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Okay.
So, what it basically means is that say 90%
of your nodes have finish processing their
data, 90% of the mappers.
Then basically for the remaining 10%, it will
reside them in some other places.
And so basically the 10% are being slow because
they are not really getting scheduled on the
process [INDISTINCT] then the new ones that
have scheduled will probably finish much faster
because they are hopefully scheduled on clear
nodes.
And that's basically the semantic of...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; Yes, so it might-it might [INDISTINCT]
it will hopefully be [INDISTINCT] processing
time of the fastest node [INDISTINCT] not
[INDISTINCT] node.
And so, yeah, that's kind of a price you can
end up in due to you running the descent life
support but as we will see, we do get pretty
significant speed ups, yeah.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: No.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yeah, yeah, we don't.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yes.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yeah, in practice it seemed to work
well so far, you--but--yeah, you definitely
could imagine cases that are--it doesn't.
Okay, John tells me...latency is not that
big of an issue, and again as I said you can
do many, many things, you can count the total
number of notes by only using one, you can
add these things, you can add things, you
can add these things in fancy non-uniform
base, you can gather--you can do and gather
operation where you get each note's number
and the current note cannot--the current code
cannot provide three routines built on top
of all the dues although it's very easy to
add several mode.
So, currently we have just an accumulate that
gives--that gives you a sum of vector value,
things and accumulate scale that just adds
numbers and accumulate average that does rated
and non-rated averages for the parameters.
Okay, so, just to give you a very quick idea
that this is not involve, serious code of
change, I have here a very simple code snip
it from what--from the LCD code that John
was using before and this is of course, this
is a very parallel algorithm.
Right now, this is not--this is not the one
that you're going to--one I eventually used,
but here was--the very simple parallelization
strategy I'm using this type--each one--there
are--here are notes, each one is running sarcastic
grid and design completely in parallel, again
I just have it--the parameters and all I have
to do is to add this one call to accumulate
and well, I have different portions of accumulation,
so, I have a couple of extra lines but really
with one extra line, you go from the serialized
implementation to the parallel implementation.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: In this case, yes, it is a blocking
operation, so, there is synchronization there.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: You can--you can do that but it's
typically doing it by some sort of a gossip
algorithm it's much lower than just doing
it over a spanning tree, if your--if your
network is not too [INDISTINCT] so...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: It scales to at least a thousand,
we've tested it that far and we haven't--it's
a--it's a--turns out it's a bit tricky even
at yahoo to ask for more at once.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Currently, we've used only binary
spanning trees.
Probably a good idea to try others we haven't--we
haven't labeled that.
Okay.
So, now, in the next few minutes, kind of,
let me just give you a brief overview of how
other algorithms can be parallelized using
[INDISTINCT] and what are--what are the design
considerations that you would actually end
up--we'll actually make.
So--okay.
So, for instance, now, we can take L-BFGS
that we will discuss.
What--all it requires in order to do the entire
L-BFGS update, it requires the gradient of
the loss function and it requires that valley
of your loss function.
These are just two calls to all [INDISTINCT]
and if you just want to run this in complete
synchronization over the entire data, so again
the--you can--you now have a parallel synchronized
L-BFGS implementation with very minimal code
overhead.
Similarly, you could do a very similar thing
that conjugate gradient you have to, of course
do more accumulation now for the Hessian and
so on.
So it's--it is really--you can implement several
things with very minimal strict mode overheads.
But now, I want to talk a bit more on the--of
my vision algorithm side of things.
Okay.
Misha?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: We haven't found any, although Miro
very cleverly wrote the BFGS code in such
a way that if you invoke it with memory zero
then it implements CG.
So...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: So, you know, that is still available.
But, I don't we're seen a case so far.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Okay.
So, let me see, for the optimization [INDISTINCT]
I am going to have a kind of, go ahead and
just get very basic and [INDISTINCT] about
optimization algorithm.
And so we have two parts that we heard all
through these, okay?
We heard about the [INDISTINCT] algorithm
that have--they a very nice property they
initially convert very, very fast whether
you want them to converge to a [INDISTINCT]
accuracy.
Their latent convergence behavior is extremely
slow.
At the same time, an algorithm like L-BFGS
is a fantastic convergence once it turns into
a [INDISTINCT] but initially converges quite
[INDISTINCT] algorithm for that region.
And so this graphic kind of tells you that
they can fit together quite well.
And so the very simple way you can try to
combine these two ideas is the following.
So you have--you have your nodes that are
running online gradient for just one task
in complete--without any synchronization at
all, in complete isolation, under local data,
you average their parameters at the end.
This is not clearly, by no stretch of imagination,
this is the optimal solution.
But it can be considered a good enough solution.
So you just take this solution, use this to
[INDISTINCT] L-BFGS, and use the--then you
work until convergence.
So this requires two quals to--two quals to
[INDISTINCT], you can--there's no need to
use just one online gradient, you can--you
can do two or three or more.
Typically, we have seen--you don't really
need to do that many, something like five
already suffices.
And now, we have [INDISTINCT] of the global
data but that might not be the--that's usually
not the complete information from the global
data.
And now, by solving--by solving L-BFGS actually,
you're trying to optimize a global function
better, right?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Uh-hmm.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: So how would you implement synchronization?
So we are currently not synchronization, doing--we
are not synchronizing, when every stochastic
gradient update.
We are only synchronizing after one pass over
the data.
And that--I don't know how to prove that that
is going to give you the right answer at the,
maybe you have inclusion on that but--okay.
So, in general, the--we experimented with
just running several passes without averaging,
I haven't run with averaging.
But at least without averaging you can do
repeated passes of online gradient [INDISTINCT]
by averaging and empirically searching through
the L-BFGS algorithm helps, John.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yes.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: We cannot--we cannot run the problem
below ten nodes.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: But I--guess both the--both the
data search we ran on.
Were things you can't really--if you--if you
run them on one machine, it's going to take
a few days, so.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yeah, yeah.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: So as--basically.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yeah.
The curve should look a bit better now with
speculator, that's right.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; If...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Let me see, I--I might have that
slide.
No.
I don't have that slide, but--but we did--we
did compare to sub-sampling and there is a--there
is a significant gain to be had from, running
on the complete data.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yeah.
That's true.
That's true.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Yes.
And that's very important, otherwise if average
it --you are very--in a homogenous plate,
your average it's, kind of, not going to make
that much sense.
Yeah?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: So, because the implementation has
changed--no, no, no, I used to know what would
happen--at the end up my internship, I don't
know what happens right now.
So...
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: quals pass all the data.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; ALEKH: Look [INDISTINCT] I mean it's not,
an easy for [INDISTINCT] affect, we've worked
with so far, I mean, if [INDISTINCT] termination
less than or equal to around an hour, so it's
not going to run for that much long, It's
kind of, that's--okay, so--okay so, yeah,
I need to measure [INDISTINCT] so working
with a group here did give us a significant
[INDISTINCT] because, speculative [INDISTINCT]
does actually significantly help, if you still
see the maximum time spent--the maximum time
spent in computing at the slowest [INDISTINCT]
with and without [INDISTINCT] execution is
quite clear that we really gain from having
this.
We tested a few different strategies and currently
the one strategy kind of, seems to be the
best in terms of getting--getting to a pretty
small training and [INDISTINCT] pretty fast.
So, I'm out of time to wrap up, sort of, one
way definite demonstration that kind of happens
through the whole framework is that, all you
do is really use of a very nice and powerful
abstraction for machine learning, it's conceptually
very simple and yet just works--fits quite
well with natural design of machine learning
algorithms, fixing great with Hadoop for additional
robustness.
Unfortunately I don't have as much time to
give you intuition about hybrid optimization
strategies that switch from online to batch
behavior but they really seem to be pretty
effective for the particular task of--task
of distributed optimization and kind of--it
does really scale up to a about a thousand
nodes that was sort of the goal of this [INDISTINCT]
so I think that if you have any final thoughts.
&amp;gt;&amp;gt; Yeah.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: This is my, to do list, and that's
what I'm going to be working on for the next
year at least.
Are there any questions?
Yeah?
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: I see, so we are using a text
input, but we're caching things so for later
batches you're just using the guest format.
That means the first task is, you know, about
a half the computation time.
So there is some penalty there.
Now you asked if you can use Hadoop streaming
with a binary format and that's difficult
because Hadoop is not a big [INDISTINCT] so
you need to do things like you decoding and
you encoding and things like that.
Which is possible but it adds the next--a
whole area of complication.
&amp;gt;&amp;gt; [INDISTINCT]
&amp;gt;&amp;gt; LANGFORD: I--I don't--I don't know all
the details of what exactly--I mean, the Hadoop
people prioritize what they're working on.
It's hard to say, okay.
And making things a bit clean is something
that I've wanted for a long time.
Okay, so thank you.
I hope you learned something.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>