<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Sparse and large-scale learning with heterogeneous data | Coder Coacher - Coaching Coders</title><meta content="Sparse and large-scale learning with heterogeneous data - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Sparse and large-scale learning with heterogeneous data</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/i8nXOFPbNqU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so what I basically was saying that I'll
gonna talk in this talk about the
specific challenges that are the
classical statistical techniques faced
with when they try to deal with
real-world problems and they involve
massive scale of data sets divers
information and sparsity which these
parts algorithm which both have you know
important consequences both for
interpretability of the algorithm it
also for economical cost and i'll talk a
bit more about that later but so
basically what i want to do first to
start off with is what I've been
personally strongly interested in which
is heterogeneous data manipulation so
basically just you know let's look at an
example here for example you want to
classify webpages according to their
topic and this just happens to be very
relevant you not to diss all use but I
kind of use this example very often
actually so you want to do a
classification where basically one data
point is a web page and you want to
predict euless about politics or sports
or something else now where you can use
to you know classify this web page is
all kinds of information you could do at
the text which is available in that web
page but maybe the image is present in
that web page might also tell you
something about that it might be the
specific HTML structure of the the web
page there might be sounds present in
the web page also you might know
something about the link structure of
the web which tells you which web pages
this page is linked when you might want
to use that information to predict the
topic of the web page or you might have
you know log data about users coming
into the web page you know which domain
they actually come from and know which
link they use to actually leave the web
page so if you want to use all that type
of information now to make predictions
about the topic of the web page and it's
obvious that it's not going to be just
you know a classical statistical talk
where we a task where we have one big
vector of numbers which then will be
classified into one or another class but
we actually have information in very
diverse formats which is not necessarily
a miracle anymore and we have to try to
deal with that and use all that
information that we have available to
make that prediction okay so on top of
that you know this is a massive task
because obviously there's a lot of web
pages out there that we want to try to
deal with this is another example of a
research topic that I recently got
involved in but so basically imagine
that we look at music and we would like
to basically jointly model audio content
of me
and words describing the music and so
why would we like to do that well one of
the reasons could be to build a joint
statistical model of those two such that
when you're basically giving a query in
words about a certain type of music you
would like to see for example imagine
you would like to see a rock song which
is somehow romantic with drums and
guitars then you would like to find
build an automatic system that pulls out
from a library of songs based on purely
the audio the sounds that actually
reflect what you want to listen to not
because there's text around that sound
that actually says rock or romantic or
guitar or drums but really because the
audio content really reflects you know
the features of drums of guitar of
romantic songs so what we're trying to
do here is basically build a joint model
for very heterogeneous data
heterogeneous descriptions of the same
sound audio content and a review of the
song and so by building that joint model
we try to do what I just described which
could be you know annotation of new
songs or the inverse basically given a
query retrieve sums from a database that
actually match that description based on
the audio content of the song sparsity
is obviously important here because you
know when we do this annotation we just
want to have a small set of words that
are most relevant persone this is
another example of an application that
I've been working on quite a lot in the
last three to four years in
computational biology so imagine for
example that we're looking at you know
an animal or a human being and you know
we're looking at a whole bunch of genes
or proteins there and we try to predict
what is the function of those genes or
proteins and there's a whole diversity
of information again that we can use to
try to make those predictions we can you
know basically expose those jeans to
different experimental circumstances and
measure whether they're up or down
regulated so basically we obtain the
so-called microarray data which gives us
under different experimental
circumstances a reading for each of
those genes or proteins and so that is
obviously very a miracle data because
every color here can we transfer
translated to some kind of number which
tells us whether you know the genius app
or downregulated but something you know
very different from that numerical type
of information could be when we know
what is the amino acid sequence
corresponding to a protein which is
basically just the sequence of letters
all right from a certain alphabet or we
might have information about how the
proteins that we try to putting the
function
of interact with other proteins in that
you know metabolism in that animal or in
a human being so we might also have
information about how the hydrophobicity
of the different amino acids varies with
the length of the the protein here but
again the bottom line is we don't have
just a vector of numbers that we try to
classify we here again have a diversity
of information that we try to use and
integrate now to predict what is the
function of a specific gene or a
specific protein right and so again
sparsity could be important here because
imagine for the biologists that we could
tell him afterwards well you know the
interaction information and the sequence
information is very important to predict
the function but microarray data forget
about it it's very interesting for the
biologists for interpretation reasons to
know that but also financially it's
important because deriving microarray
data costs a lot of money ok so again
coming up with a sparse out when
selecting which information sources are
important is an important step here so
what I'm going to try to do in this talk
is basically cover some of these
challenges that I mentioned before
including the diversity of information
one that I just talked about a little
bit by giving two examples of problems
that I worked on one of them is
specifically classification algorithms
for dealing with heterogeneous
information sources and then there's a
second one on the sparse principal
component analysis which mainly deals
with these sparse type of algorithms so
to talk about the first one I'll first
give a short introduction on basically
the philosophy behind kernel methods i'm
just wondering here how many amongst you
are familiar with what kernel methods
basically do okay so most of you it
seems like so basically after that
introduction then I'll kind of talk
about what I've contributed in the field
myself and what I think is a good
approach to deal with these
heterogeneous information sources so
basically in kernel based learning what
we do is we embed data and data points
here x1 through xn in a high dimensional
Euclidean space and the important part
here is that each of these data points
that we're looking at is actually
described by some piece of information
which is green ellipse here but that
piece of information could be anything
it could be the address on a person it
could be the sex of a person it could be
you know how basically web pages are
linked to other web pages
doesn't need to be numerical information
per se it's just some type of
information which we then use to come up
with this high dimensional Euclidean
embedding and then once we have that
embedding we run some linear algorithm a
classical statistical algorithm in at
high dimensional space corresponding to
a whole bunch of acronyms all you know
depending on what kind of data as you
want to do and basically from that data
analysis we can then find the patterns
that we want now the thing here is
basically the reason why this kernel
based learning came up 10 15 years ago
was if this data is actually described
by numerical vectors here then this step
here this embedding step could
correspond to a highly nonlinear
transformation which basically
corresponds to giving us the possibility
now to come up with a non linear
algorithm here because obviously finding
a linear algorithm in this high
dimensional space here will correspond
to a nonlinear relationship between the
data points in the original space here
all right and this was really the
original motivation of these of this
kernel based learning framework to come
up with nonlinear versions of linear
algorithms now recently in the last five
six years let's say people have noticed
that you know defining such an embedding
can also be done for non-american on
numerical information and so basically
it gives us a handle to deal with data
that is you know more diverse it is not
described by numbers anymore which is
basically a type of data that classical
statistical algorithms had trouble
dealing with because they always work on
numerical vectors basically now the
basic thought of this kernel based
learning framework here is that the
embedding of these data points which we
can denote by Phi of X i fi of XJ here
so basically this is this high
dimensional Euclidean space this
embedding is actually not explicitly
defined so we will never explicitly
specify the coordinates in this high
dimensional space here what we'll do
instead is will basically implicitly
define the embedding by mentioning the
inner products between each pair of data
points in this space here all right and
those inner products can somehow also be
interpreted as some kind of measure of
similarity between pairs of data points
which is an interesting interpretation
to just easily understand this framework
in a more general setting now to define
this interpret in this high dimensional
space one can either use the soap
old kernel function which basically just
defines the inner product between each
pair of data points xixj or you can
basically also define what's called a
kernel matrix which is basically a
matrix that has as many rows as there's
data points and as many columns as there
is data points and then one specific
entry in this matrix let's say on roll I
and row J gives you what is the
similarity measure or the inner product
between the image of the ight and the J
data point in this high dimensional
space here ok so basically this matrix
contains all the inner product
information or similarity measures
between each pair of data points in this
place here and therefore implicitly
defines this entire embedding here again
without explicitly you know saying what
are the coordinates of the embedded data
points over there now the thing is in
this kernel based learning framework
that most creativity now needs to come
at the stage where this kernel matrix
needs to be defined or derived from the
information that you have and when you
can do that in such a way that most of
the information that you have about the
data is contained in that kernel matrix
then you can actually expect that you
have an embedding that reflects very
well the information at hand and
therefore you know you can have a
successful in your algorithm that you
apply afterwards now to design this
kernel matrix here basically you can be
very very free in your creativity the
only thing you have to make sure that
it's a positive semi-definite matrix
which means eigen value is positive to
ensure that actually corresponds to
implicitly to some underlying high
dimensional embedding okay so again this
kernel based learning framework what it
basically does is it has these two
modules in it intrinsically where first
we define the embedding of our data in
this high dimensional space by basically
designing the kernel matrix then in the
next stage we only use the information
in this matrix here to run some type of
linear algorithm to extract patterns
from the data and so both basically
those two phases can happen
independently you can design all types
of kernels for different types of
informations and then you can basically
divide these on all types of algorithms
for different tasks and you can just you
know mix and match between the two so
this this framework is actually applied
for a variety of tasks in machine
learning for dimensionality reduction +
cheering and so on the tiles that I'm
gonna focus on here is specifically
classification and for that does there's
been many successful applications you
know in text classification face
detection and so on but anyways let me
just talk about specific task did I want
to focus on here which is binary
classification so basically let's assume
we have n training data points where X I
is the description of the item jecht and
where why I tells us whether it is you
know in a class minus 1 or a class plus
1 so it's basically a member of either
of two classes that's why it's binary
classification important to realize
again here is that X I is a description
of the ID object ok so this description
again can be in a miracle but it doesn't
necessarily need to be in a miracle here
at this stage ok for example X I could
be the file about a patient and then we
could know about you know these patients
whether they are actually ill plus minus
1 or whether they're happy because
they're healthy and also in the class
plus 1 here anyway so the challenge here
is can we now design a classification
rule search that given a new X i we can
actually predict the label why I with
minimal probability of error that's
really what it's all about in binary
classification and so one of the ways to
address this problem could be to first
of all try to represent our data again
in this high dimensional space you know
the kernel based approach I mean in
general you can do also when you just
have numerical vectors you could try to
represent things like this and so one of
the ways to go could then be to try to
separate these two classes of training
data points which are here represented
by let's say the green dots for the
class minus one and red dots for the
class plus one let's just try to find
some hyperplane that separates those
broad classes okay defined by some
parameters W and B and once we find that
hyperplane we basically have a
classification rule to classify future
data by just looking on which side of
the hyperplane a new data point comes ok
so imagine we get some new data point
which basically comes here in the space
well then we decided to actually part of
the korean class all right so which
basically just corresponds to the
mathematical formula down there so
anyway so one specific algorithm known
as support vector machines or maximum
margin classifiers will now position
this margin in such a way between the
two classes the minus one plus in the
plus one class such that the distance
or the two norm distance to the closest
point from each class is maximized okay
there's obviously different ways that
you can position this hyperplane here
but this is a very specific algorithm
that will try to do it in such a way
that the margin between the two classes
maximize so basically this distance here
is try to maximize try to be maximized
so now you can obviously say like
Windows classes are not separable what
can you do then well if they're not
linearly separable we can introduce some
slack variables sigh I which allow some
of these data points to be actually
inside the margin like this one here on
the other side of the margin ok so we
allow ourselves to make some errors and
then basically we try to maximize
besides that the margin for the data
points that have no error okay by
introducing these slack variables I I so
now without going through any math here
let me just formulate what is the final
mathematical formulation of this
specific algorithm here basically in
order to find that hyperplane we have to
solve an optimization problem to find
the optimal position of that hyper plane
and it turns out that that optimization
problem has two terms in one term as we
said corresponding to maximizing the
margin and the other term which traded
off against is a term where we try to
minimize obviously the error we want as
few points as possible to you know
commit those errors here and so the
subject our constraints to which this
optimization problem is subjected is
just that we try to impose the fact that
most of those data points are correctly
classified which is the red part here
and if that's not possible then we allow
some errors like which we here try to
minimize okay most important about this
is if you look at this optimization
problem these constraints are just
linear constraints very simple to deal
with the objective has a square here in
a linear term here so it's basically a
so-called quadratic programming problem
which brings us in the class of convex
optimization problems which are
basically nicely conditioned problems
that are quite easy to solve and also
they have one global optimum so we don't
have to deal with local optimum and
optimizing this which means that the
results are reproducible so again it's
basically an optimization problem that
we can solve quite easily also for
larger scales to analyze the problem a
little bit more we derive what's called
the dual problem now without going into
any detail again here
the dual problem actually some kind of
sister problem which we can derive for
any type of optimization problem and
what it does for us it gives us
additional insight into the problem and
maybe also some kind of alternative
formulation which is easier to solve ok
now the inside is going to give us here
is the following so if we derive it it
turns out it's actually a maximization
problem over some variables alpha and
there's as many alphas here as there is
data point ok don't worry why that is
but that's just the case here now the
main thing to point out about this dual
problem is that the way the data points
appear is through the inner product Phi
X I transpose x phi x j alright and so
basically it turns out that if we know
these inner products and obviously also
the labels why I here we can actually
entirely formulate and also solve this
problem here which basically means that
if we know the kernel matrix K which is
the matrix that contains all these inner
product and we know the label
information that we can actually solve
this maximum margin classification
problem alright and so the nice thing
about this is that this observation
makes this algorithm entirely fit in the
kernel based learning framework because
kernel matrix and labels is enough to
actually train the classifier meaning
find that hyperplane ok there's
something else now if we derive this
problem here this dual problem then on
the way to get there we pass by an
optimality condition which basically
defines the relationship between w which
is the parameter of that hyper plane and
alpha which is the variables of this
sister optimization problem let's say in
the duel ok now using that result allows
us to derive one more thing here which
is if we want to classify a new data
point X we need to evaluate this formula
here basically see on which side of the
hyperplane it falls now by substituting
this result for W in here we notice one
important thing again which is that to
classify a new data point the only thing
we again need to be able to is evaluate
the inner product between basically all
the phi x i's the train data points and
this new data point phi x phi of x here
so again by knowing the inner product
information or you know the kernel
function between all the train data
points X I and this new data
point X here we can actually now also
evaluate the classification rule and
classify classify new data points so
bottom line here is we can train fit the
hyperplane and evaluates our classifier
meaning make predictions for future data
points entirely when we know this kernel
matrix or inner product information or
similarity information so as I said this
basically makes this all fit entirely in
the kernel based learning framework
because these matrixes enough to do
anything we want afterwards and in this
case the algorithm that we apply for
classification is also known as support
vector machine so this basically brings
it at the end of the introduction now
what I want to do now with this learning
framework here at this point is try to
use it to deal with heterogeneous
information sources and you know somehow
the intuition is there already because
these methods can actually deal with
non-medical information by the
definition of this kernel so it's kind
of like obvious what is the path to go
the non-obvious part of the path here is
imagine we have different sources how
can we integrate them all right and so
that's really the challenge here the
challenge really is for example imagine
that we want to classify proteins
according to their function and we have
all these information sources available
we know that the final goal is to design
this matrix here because once we have
this inner product matrix which again is
a matrix of let's say n proteins by n
proteins which are the data items that
we want to classify according to their
function here so once we have this
matrix here we can actually you know
derive for example a support vector
machine classifier now we want to design
this matrix obviously based on all
information that we have at hand and so
the big question is how can we squeeze
all that information into that matrix
here ok so the approaches I'm proposing
here to do that is basically an modular
approach so basically we first want to
focus on every single source of
information individually and extract the
relevant information for that source
into one kernel matrix once I've done
that I want to design an algorithm
actually can merge all these matrices in
an intelligent way basically given the
learning problem try to find optimal mix
of these kernel matrix
so basically in pictures what I want to
do is for each source of information
that i have here i want to design that
matrix and so that's already like a step
forward because right here we're like
basically dealing with a whole bunch of
sources of information that are very it
Raj Aeneas while right here we have just
a set of homogeneous colonel matrices of
known sighs okay so this already makes
it you know much better defined what we
are dealing with exactly and then in
next step we now need to find a way to
basically merge all these matrices
together into this final kernel matrix k
that we don't want then want to use for
our classification problem all right so
the advantage of this approach is that
we can actually by splitting it up and
basically designing matrices per source
we can focus on designing colonel
matrices for specific types of
information so we can just focus on you
know music on a web page we can focus on
images on the web page how can i extract
information into a kernel matrix I don't
have to deal with everything at the same
time the nice thing then is that in the
second step the learning algorithm to
merge all these matrices can actually be
standardized because it knows what the
input looks like is this a set of
matrices and so the nice thing then is
that we actually also have the
flexibility to ignore information which
is irrelevant for the learning task
because obviously we can tell the
algorithm try to discover which matrix
of this whole set of matrices is
actually irrelevant to predict the topic
of a web page for example you know
certain sound present on webpage might
be relevant and we don't need to look at
it also it's very flexible if we have
new sources of information that we want
to use in this task here you just
basically design a matrix for it and it
rolled into the algorithm and can just
rerun it well if you would take for
example a graphical models based
approach and you want to kind of model
all these sources of information that
you have by a huge graphical model you
get another source of information you
need to append something to that
graphical model and into you you know
rewrite part of the code to actually
train the adjusted graphical model while
here we just throw in another matrix
okay so to give a couple examples of
this first phase now of colonel design
since i assume that many of you already
know like how gaussian kernels and
polynomial currents and so on work i'll
just give a couple example of these more
you know reasons last 56 years kernels
that have been
proposed to actually deal with non
numerical information so first type of
information here that we are looking at
an example for is imagine that we have
proteins that we want to classify and we
look at the amino acid sequences that
represent those proteins so basically
the information that we have available
about our data point about our proteins
or variable length discrete strings okay
so for example the first protein has an
amino acid sequence which is represented
here the second protein has this
sequence here and so now the question is
how can we come up with a good
similarity measure between those two
proteins which is based on this
information about the proteins again
colonel design and it's kind of nice
that we only have to be worried about
that question we don't have to worry
which would have been the case in more
classical statistics about first
representing the string of letters here
by a vector of numbers and then we could
actually have an algorithm dealing with
these vector of numbers and obviously by
transforming this into a vector of
numbers there is you know information
that could go loss or it wouldn't be as
obvious sometimes to transform this into
a vector of numbers than to say how can
I measure similarity between these based
on information that I have because for
example here what you could do is you
could say well you know if these two
strings of amino acids have a lot of
common substrings for example like here
there is some of these sub strings that
are in common between the two amino acid
sequences and they're pretty long and we
could say well they're actually quite
similar but if you would deal with two
proteins that are actually sharing only
very short and only very few common sub
sequences here and we could say well
they're actually less similar so it's
actually sometimes very obvious based on
the information that we have about our
data to say where are two data points or
more similar or less similar why it
might be much harder to transform the
information into a vector of numbers
okay so basically what we're doing here
is exactly deriving similarities is
designing curves not obviously there's
one thing here which is we have to make
sure that that process leads to a
positive semi definite kernel design
because these matrices need to be
positive semi-definite now it turns out
that this type of kernels which is
called string kernels does lead to
search positive semi-definite
design another type of information that
we can flexibly deal with here is for
example graph based information so
imagine that the data points you want to
classify or the vertices of a graph and
the information that we have about them
is the connectivity described by the
graph so this could be you know a whole
bunch of proteins that we try to
classify according to their function and
the connections are whether they
interact or not or you know more obvious
example here is that these are just web
pages which you want to classify
according to their topic and the
connections between them are just
feelings between them basically all
right so there is a kernel which is
called diffusion colonel now which can
actually be used to establish
similarities between the vertices of the
graph so between the proteins or between
the web pages it will establish
similarities between those based on the
connectivity in the graph where
basically two nodes that are connected
with a lot of short paths will be will
end up to be much more similar than two
nodes that are only connected by a lot
of very long paths okay now again this
is based on like some random walk
process on this graph and so on there's
some math going on in the back but the
bottom line is again that by using this
philosophy you can actually
mathematically design a similarity
matrix between proteins and proteins or
website web pages and web pages that is
a positive semi definite kernel matrix
okay sure
meet
actually happens when you mix it with
you
so you're perfect by the SDM go Adam
well it's really remarkable analyst you
know I'm reading that in practice these
two
so it fits in remark and actually it's
easier for me to answer it a little bit
later but i'll get back to it so
basically what we have right now is
indeed assuming at this point the whole
bunch of matrices that are positive
semi-definite which are designed each of
them for you know specific source of
information and so now the next step
what your question is actually aiming at
is now specifically how can we design an
ultimate is going to mix all these
matrices together in a stable fashion
but also in a way that it can tell us
which made wishes are relevant and which
ones or not okay now the philosophy
behind this mixing process is the
following remember that to be a valid
kernel matrix that matrix needs to be
positive semi-definite okay now it turns
out in convex analysis that the set of
these matrices that are positive
semi-definite forms a set which is
called or defined as a convex cone
without saying too much about it it
turns out it's a very nice set over
which to optimize because it's a convex
set it has a nice shape and people can
actually efficiently all right efficient
code to optimize over this set of
matrices okay the other thing now is it
obviously if we want to optimize over
the set we have to be guided by
something to pick the best candidate
from here okay so we need to define a
cost function to assess you know which
kernel matrix would be the best one to
go with now if we can design our cost
function in such a way that we restrict
ourselves to convex cost functions then
we have two very important ingredients
here for our higher level philosophy
which is we first of all want to learn
the optimal candidate from a body which
is our asset which is whoops a convex
cone the one of positive semi definite
matrices and second of all we want to do
it according to a convex quality measure
which is exactly the two ingredients we
need for a technique known in
optimization convex optimization as semi
definite programming it's basically an
optimization technique that deals with
optimizing convex cost functions over
the convex cone of positive semi
definite matrices okay so think about it
like linear programming for example
tries to optimize a vector of numbers
which basically just a linear vector you
know subject to a bunch of linear
constraints
we're doing here we're trying to
optimize a matrix subjected to the
constraint that this matrix has to be
positive semi-definite okay and that's
exactly what semi-definite programming
tries to do this when you're trying to
learn the kernel matrix for a single
form of information are trying to learn
the kernel matrix of the combination of
other penalty good questions so like at
this point I'm basically just kind of
you know lining out the higher level
philosophy obviously this philosophy
which kind of leads you know ties into
this optimization technique we want to
buy this now to the specific setting
that we're looking at which is we want
to use it to integrate the constructed
journals and we want to apply for a
specific case of large margin
classification right and actually it's
good to do this to actually you know you
could say let's now try to pick the best
kernel matrix for this very specific
setting where we have just one source of
information but the problem also is you
have to deal with overfitting so it's
actually good to kind of constrain
yourself a little bit and not search
over the entire comb but search over a
subspace and for example the subspace
that we proposed here is an intersection
between that cone and a linear subspace
okay because we want to learn the final
kernel matrix K as a weighted linear
combination of these kernel image which
is sched eight and we have defined so
far or derived so far so basically those
guys are given because we designed an a
priori and those weights 8rj are to be
optimized all right and so what we want
to do is you want to learn optimal
kernel matrix for this very
classification problem as this way that
some obviously constraint to be positive
semi-definite and this is really where
your question now comes in because
obviously if one of these colonel
matrices here is not positive
semi-definite because we impose this
constraint here we can just go ahead now
the next thing is that we want to
obviously find the optimal k for this
specific binary classification task
where you know large margins are desired
so remember this kernel matrix this guy
here will reflect the final embedding of
our data points and now we're wondering
somehow about how should we pick this
final kernel matrix which criterion
should we use now what we know is that
you know a whole bunch of data points
packed tightly together is
I'm going to be good because it's not
gonna allow us to have a large margin of
separation between two sets of data
points well if we could have a kernel
matrix that corresponds to an embedding
where there is a big margin between the
two sets of data points that would
obviously be much more optimal so one of
the questions here is can we actually
try to optimize the kernel matrix or the
embedding such that we have a large
margin of separation between the classes
and is that a convex criterion now it's
kind of hard to go into the mathematical
details of this here but it turns out
that actually trying to maximize the
margin with respect to the kernel matrix
with respect to the embedding is convex
is a convex quality measure and I'm
happy to talk after you know the talk
with anyone about this why this is and
so on in a more mathematical way but I
decided to omit it here just to not like
you know stir the people that are not
really interested in the deeper math of
this basically the bottom line is that
using this linear combination as a way
to integrate the different kernel
matrices ki and using this as a cost
criterion to actually select the best
possible candidate k leads to a semi
definite programming problem which can
be formulated in a form that is actually
suitable for optimization tool boxes and
this is basically the results that comes
out of you know doing some algebra and
deriving that optimization problem so
basically whatever is in the red frame
is actually unimportant I mean the
bottom line just is that this is a semi
definite programming problem and to
illustrate the applicability of this
approach to one specific domain which is
domain that I've worked on quite a lot
so far which is computational biology I
picked out this example here actually I
picked out two the first example is an
example where we try to predict which
proteins in yeast or membrane proteins
and release that the reason why we're
interested in this problem is because
it's actually quite important so
basically membrane proteins or proteins
that anchor themselves in the membrane
between let's say the outside and the
inside of the cell all right and
therefore they're quite important
because basically if I'm a malicious
virus and I want to do something wrong
inside the cell I'm never going to get
in but I can get all the way to the
messenger here
sending the wrong message and tell the
cell to destruct itself okay if I'm a
pharmaceutical company and I want to
design a drug to prevent that what I'm
going to do is try to you know first of
all know which proteins are membranes
membrane proteins and then design a drug
so that it basically goes on top here
sits on top and prevent anything bad
from the outside to send a bad message
to the inside okay so it's actually
important to know how these membrane
proteins look which means first of all
we have to be able to identify them and
so about thirty percent of the proteins
in most cells are actually membrane
proteins so we're looking at are quite
important plausible things to identify
here so again the approach here is quite
similar to that I said before we have a
whole bunch of different types of
information here unimportant which ones
exactly we are using but again so the
approach we use here is to design a
kernel matrix for every specific source
of information and then in the next step
basically mix all these together using
the algorithm that we are just proposed
and if we do that then the results
should look like what's coming up soon
enough hopefully it's coming I'm sure
it's going you see basically the
vertical axis here are a performance
measures so this is an ROC area under
the ROC curve this is the number of true
positives at 1% false positives in these
two plots here and basically the columns
corresponds to different classifiers
basically the first seven columns here
corresponds to a support vector machine
classifier that only uses one kernel
matrix okay so basically here this is
the kernel matrix based on sequence
information this is a kernel matrix
based on interaction information and so
on and the last column here is basically
the result of using the optimal mixing
algorithm with a support vector machine
basically using all information to make
predictions about membrane proteins and
so what we can see here is that
especially according to this performance
measure also according to this one
although it's less outspoken we see that
there is an improvement of using all
information sources by optimally mixing
them
to using only you know a single one of
them and so here in the bottom we can
see also what is the weights of the
different kernel matrices in the
combination which obviously here are
just one because we're just using a
single kernel matrix here we see
basically what is the relative weight of
the different kernel matrices and so an
interesting point here is that for
example there is a lot of blue stuff in
here and so these three blue colonel
matrices here are actually three
matrices that are derived from sequence
information amino acid sequence
information now if you talk to the
biologists it's actually quite logical
that that information is quite heavily
used to make predictions about membrane
proteins because it turns out obviously
that everything inside the cell there's
a lot of water there while in the
membrane there's not much water so the
hydrophobicity profile of these proteins
is very very outspoken where you have
like a lot of amino acids that like
water because they kind of go around in
the cell and then you have like 30 amino
acids that don't like water at all
because they just go through the
membrane we end over a very fixed link
length so amino acid sequence
information is very important these
predictions here from a biological
perspective it turns out that if we make
these combinations here that there's
also a lot of blue in the final
combination so there's somehow you know
some kind of parallel area so what are
the phase 4 sem is looking at which or
is it was better which ones don't so how
old is important you just say look set
all the news to one just you know make
your distance facially the solutions
yep that's a really good question
actually so basically this is a question
I couldn't answer very well a couple of
years ago when I initially did this
research but now through more experience
what we've noticed is that if you only
have two or three sources of information
then usually it's better to just put all
coefficients equal to one if you think
they're all rather than somehow and it's
not going to make much difference when
you fiddle with the weights when it
becomes important is when there's a lot
of sources and when quite a few of them
could be noise it's like you know if
train is vm on three or four features
and you have you know enough data then
if there's one noisy feature eliminating
it or including it is usually not going
to do too much of a harm necessarily but
if you have you know thousands of
features and a bunch of them could be
noisy then it's better to do a good
feature selection because it's going to
make your algorithm more stable
something similar happens here really
like if we have all of sources of
information and some of them could be
noisy then it's good to do a selection
and basically have weights involved
where some of the weights could go to
zero and actually they do so they do
have a tendency to get sparse so
basically the algorithm has a tendency
to exclude I haven't really explicitly
said that but the algorithm has a
tendency to really exclude information
sources so basically in that setting
with lots of information and potential
noise it's much better to do this
usually yeah you add a couple of
redundant sources yeah and at zeroing
out those or will it sort of spread out
the mass
well so redundant it's redundant sources
are a very interesting issue here
because so you're looking at a linear
combination and like LPS viens for
example have that same problem if you
have two features that are a copy of
each other then you know if you look at
the optimization problem there's no
reason why one feature should be chosen
about mother because it's just a linear
combination so you can exchange them all
the time and so the same thing happens
here so like one of the good things to
do would be we haven't done that yet but
we notice there is a problem if well
there's not a problem when there's two
sources just like their weights are
exchangeable so you kind of like you
should a priority maybe run some code
that checks whether these final matrices
are you know quite distant from each
other and they're very close and they
might just you know basically code
redundant information it might just want
to urge them something it priority
before you kind of like you know try to
assign weights to them but there's
really because usually has really been
tearing out wait federer redundant it
just ends up having it's wonderful yeah
that's that's what's happening and
basically the sum of the weights of the
redundant sources are always going to
add up to the same number but amongst
themselves like an optimal rate you know
you can kind of swap it around so I'm
going to change anything about them
tomorrow k also a lot of dominate like
if you keep at it oh no no that's not
gonna happen no because basically since
again it's linear combination they kind
of like all merge together and become
only one matrix you know and the guys
basically go on the radar of anyone else
and anyone who's like certainly as
strong to compete with them I'm also
numerically sorry do you have to do
anything America with these colonel
maybe I different scales completely yeah
so the best thing that I've noticed to
do is basically normalizing by basically
centered at first which is always good
for stability because it can occur
matrix which encodes interpreter far
away from the origin you get you know
entries that are very comparable to one
another and you get a numerical
instability so you first made it shipped
to the origin this by centering
everything and then you can normalize it
by projecting it on the high dimensional
unit ball and that very often but that
does it make the diagonal one so it
makes basically the self similarity of
every data point equal to 1 which is you
know seems very reasonable to do because
one of these problems is think about it
if you have like amino acid sequences
that were very long there are self
similarity becomes very high because
they have a lot of common substrings if
I only have three amino acids as a very
small protein myself similarities going
to be very low how many have you know
six our common substring with myself so
you want to do that normalization to
basically put the main diagonal all
equal to one and usually when you do
those scalings then these algorithms
work the best so you have like you know
basically an equal scale pretty much for
all these matrices and one person the
beginning you talked about massive data
sets how high does this yeah yeah so
this is this is a question I'm going to
answer it is like in one slide how come
in a little bit here so this is another
algorithm let me just kind of quickly go
through it because there is a more
interesting stuff to say the basically
what we try to do here is predict the
function of these proteins is and
basically there are certain functions
that we try to picture for each of these
you know prediction problems here we
fought the performance again on the
y-axis and we compare our method which
is in in yellow with Markov random field
method which is
read here okay and so basically it turns
out we use the exact same data as the
mark of random field approach that they
developed in university of southern
california and in the end for most of
these clouds it turns out that this SVM
based approach actually outperforms the
market regular person the reason why is
i think is really because it's just
quite easy with kernels to encode
information while if you want to encode
everything into some kind of graphical
model sometimes really hard ways we use
a graphical model to you know encode
certain types of information line
microarray information and so we we
discussed this and we think that that is
actually the main reason for you know
the superior performance here anyway so
one of the good things about their
approach also was that they also notice
that if they use all the sources jointly
they can also include the performance
compared to any single source so
basically in that sense we had very
similar conclusions from research now
anyway so there is one slide here and
underneath this slide goes actually a
lot of work pretty much I think that
paper that I ever will work long is on
in my life basically the question you're
obviously is is having a convex
formulation for this classification
problem and it is the end of this story
and it actually is not because you know
obviously if you use general-purpose
packages for convex optimization then
they will offer you algorithm run in
polynomial time which is much better
than you know np-complete situations but
again the scaling in terms of number of
data points can be pretty bad like for
example here it turns out and Pyrrhic
lee that it's killed like to the fourth
power of number of data points which
means that you know you can basically
deal with five hundred or a thousand
data points and you're done okay so what
we did that is we dedicated a lot of
time to look for an algorithm which is
actually a more efficient here and based
on you know a similar line of research
and what jump lab did with sequential
minimal optimization but also joined
with non-smooth optimization because the
big problem here is that this problem is
much harder to deal with in as a mo
setting than just a regular SVM problem
because this one is non-smooth well as
the m1 is smooth but so it turns out
kind of you know joining these two kind
of ideas or lines of research that we
can come up with an algorithm actually
scales but between linear and code
ethically in the number of data points
here okay and so this is purely an
approach that is based on you know
making the optimization problem itself
more efficient obviously the next step
now is can we actually instead of
changing the power here just you know
change the N and basically try to maybe
a priority already kind of you know
sample down the data set to actually
scale this up you know even more but so
there is research being done actually
very recently there was also called
released on the web actually to do this
which doesn't even need any general
purpose tool boxes anymore but so it
turns out yes so you know you cannot
just basically that the gel crew buses
are good to just try something out
quickly but if you want to scale it up
there's no way and you really have to
dedicate quite some time to you know
making the algorithm or more efficient
by using its structure and so on so to
get back to your question now so
basically what I said there is that you
know the linear combination is subjected
to a constraint where it's positive
semi-definite so you can throw in
whatever you want they're also if it's
you know negative definite it will just
you know make sure that the combination
itself in the end is positive semi
definite now it turns out that if we
constrain these weights to be positive
and we have true colonel matrices which
are positive definite and we actually
know that the combination is going to
post this time to be positive
semi-definite already for sure because
you have a positive linear combination
of positive semi definite matrices and
so that actually allows us to
reformulate the problem as a quadratic
Lee constraint cleric programming
problem so we kind of get rid of this
heavier SDP formulation because SD peas
are known to scale pretty badly so
that's already also one of the steps
that we actually incorporate in trying
to get more efficient algorithm so in
the original formulation we could
incorporate you know indefinite
information but in the more efficient
algorithm it's not possible anymore
since we made this simplification step
so what I want to talk about here is
actually it's a shorter part of the
stock but it's kind of like focusing on
an other element that I'm really
interested in which is actually coming
up with data analysis tools that give
you sparse rules in the end or sparse
evaluation algorithms so just to give an
example here obviously everybody knows
principal component analysis where we
try to you know find a low dimensional
representation of data for example
a one dimensional representation that
explains most variants of a two
dimensional data set how this is done is
obviously by just looking at the
covariance matrix and you know looking
at the eigenvalue decomposition now
these techniques principal component
analysis have a lot of applications in
different fields here but there is one
issue with principal component analysis
so it's actually you know it's optimal
in a sense that it will give us the
lower dimensional subspace in the data
that explains most variance in the data
it's pretty efficient to compute as well
eigenvalue decomposition but the problem
is that these eigenvectors are very
often not sparse because they're often
linear combinations of all variables so
what I mean is if you have the
covariance matrix a here and you write
down its eigenvalue decomposition and
those vectors X I hear the eigenvectors
are the principal components and
basically are the vectors that construct
that subspace that explains most
variants of my data now the problem is
that this usually is a dense vector with
very few zeroes in there which means
that you know in order to explain most
variants in my data think about for
example you know you have a whole bunch
of gene expression data to explain most
fans in my data I will have to still
involve all the genes which means that
the biologist doesn't get an answer to
this question which genes are most
relevant now okay because this X I here
is going to have very few zeros all
right in finance there's also an issue
with this because these vectors in
finance very often are actually related
to portfolios that one wants to optimize
so basically these excise that I
mentioned just before the weights in
these excise will be used to actually
determine in which ratio the different
assets should be taken into the
portfolio and if all these entries are
nonzero it means that you need all
assets in your portfolio to have the
optimal portfolio which means that you
have to pay transaction costs for all of
them so it's also not economically very
advantageous to have these non-sports
zeigen vectors or principal components
so basically the bottom line here is
that these vectors X I our principal
components usually have all non zeros in
there which means we need all genes for
example to explain the variance in our
data so the fact that it's not sparse
gives us lack of interpretation there's
basically no feature election that
happens that can help the interpretation
for a biology
is for example or we can have cost
driven reasons as well you know
transaction costs for financial
portfolios that you know we'd like to
have principal components that are a
little bit sparser in image processing
there's also reasons to have actually
sparse factors like for example in
non-negative matrix factorization where
it turns out if you can have sparse I
ghen faces that actually the zones that
are not sparse often corresponds to like
you know very specific parts of the face
like ears and eyes and noses and stuff
like that so basically you know driven
by these applications here we sat down
and we're thinking like well you know
how can we reformulate the optimization
problem to get the first principal
component you know for data with a
covariance matrix a but Serge that is
actually much sparser then when we would
just solve this optimization problem
which just gives us you know the first
principal component ie you know the
maximal variance Direction
one-dimensional here in this case and we
just look for the vector X we didn't you
know some cloud of data ok i guess this
optimization problem looks very familiar
i mean it's just the first eigenvector
of a matrix ada we try to determine but
now what we would like to do is you
would like to do the same thing try to
find the vector X that maximizes you
know the variance when we project all
the data onto it we just kind of
normalize its length to be one but now
we actually want to have an additional
constraint here which is that the
cardinality meaning the number of
nonzero entries in this vector X here is
smaller than or equal to some number K
so we just want for example ninety
percent of the entries of this vector X
to be zero so we really want to enforce
feature selection ok so basically what's
going to happen if we could solve this
optimization problem is because we put
more constraints on our solution here we
want it to be sparse well probably it
will not be as optimal the amount of
variance that we can explain will not be
as big lets you know what's going to
happen now besides that there's
something much worse that happens which
is that this problem here all of a
sudden becomes a non convex optimization
problem so if we omit this constraint
here find the first eigenvector of a
matrix very simple very efficient if we
add this constraint which is known as a
very hard non convex constrained it
turns out that this hope
becomes very hard to solve it's
basically a combinatorial problem that
you get now without going into any
mathematical details again here the
challenge is if we ever even want to see
how you know the explained variance goes
down when we constrain the cardinality
here if we even ever want to investigate
this we have to be able to solve this
problem so how can we find a solution
for this problem well one of the
techniques which is known in a convex
optimization is semi definite
programming relaxation so basically what
you do is you try to find an approximate
problem of this problem here using a
certain technique which basically which
solution will allow to derive a solution
for this problem here but obviously an
approximate solution again so I'm not
going to go into any details of that
technique here but basically the bottom
line is that by doing a couple of these
relaxations here we can come up with an
optimization problem which looks like
this and basically this problem here is
transformed into an optimization problem
of a matrix X again where the objective
is linear in the matrix X this is just a
trace of a times X subjected to a bunch
of constraints here so again what we're
trying to do is we're trying to optimize
a matrix X our optimization variable
subjected to two simple constraints here
and on top of that a constraint that
this matrix X has to be positive
semi-definite again the fact that these
constraints are here is just a complan
sequenza of all these relaxations that
I'm doing here but the bottom line is
that the problem down there again is a
semi definite programming problem so
basically this non convex hard
combinatorial problem here can be
approximated by a problem which is a
semi definite programming problem IE a
convex optimization problem which can be
solved fairly simple in polynomial time
okay yeah oh there is a conflict of
timing here okay I'll get through it in
two minutes I'm actually at my final
example so so basically this is just an
example showing how the approximation
approximates the real problem so
basically the horizontal axis here is
this K value that we put into the
optimization problem to constrain the
cardinality of our principal component
okay and so what's plotted here on the
y-axis is the true cardinality of the
solution obviously we want the true
cardinality to be you know maximum equal
to this value of K here WC since this is
an approximation is never going to
happen and so what we see is that this
line is still pretty close to the
diagonal line which would be the optimal
it's not jumping all the way up here so
it seems that approximation is actually
behaving pretty well this is an example
from finance but let me give quickly a
last example here which is the example
from biology basically the point here is
this is a cloud of data right it's a
cloud of data which is driven by gene
expression information and basically
every dot here is a single rat and on
that red we have measured the expression
of you know eight thousand genes for
example there is four types of red rats
compared basically corresponding to
whether they have a certain disease or
not but the bottom line is we want to
visualize the data here right now to
visualize data what we could do is we
could do just principal component
analysis and plot the data that we have
in a space which is based on the first
three principal components which is what
we do here the problem is that these
three principal components need all the
genes to be measured because each of
them is a linear combination of every
single of the eight thousand genes okay
here on this on this side we apply
sparse principal component analysis and
so the advantage here is that we can
actually visualize the data in a way I
mean if you compare the both it's not
that it's much worse we can still kind
of very easily see that there's
different clusters of rats here but we
are able to represent it in a basis
where the basis vectors themselves or
sparse principal components which means
they're only linear combinations of in
total 14 genes here which means if now
you know I tell this to the biologist he
can go into his lab and he can just
basically measure 14 gene expressions
instead of eight thousand of them to
represent this data visually which is
much cheaper and also easier to
interpret okay so anyways the same
problem poses itself here this is an sdp
how can we run this on large-scale data
again you know so we need to do
something special here which I'm not
going to talk about and basically this
concludes my talk which kind of talked
about these different challenges
including heterogeneous information
sources and sparse algorithms for
large-scale problems thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>