<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Enabling Streaming Experiments at Netflix | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Enabling Streaming Experiments at Netflix - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Enabling Streaming Experiments at Netflix</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5aDqes-EUKg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and next up we have mineral mishra from
netflix to talk about enabling streaming
experiments at netflix good afternoon
everyone my name is manal mishra and i'm
here to talk to you about how we staged
rollout of the netflix streaming player
in order to enable frequent streaming
experiments of this video player so a
little about myself I started my career
at xbox when it used to be a big black
box with the green X on it and then I
moved on to the zune team I can describe
the device for you but I doubt anybody
remembers them the logical next step
after working on a games and a music
digital service was to work on a video
service so here i am at netflix trying
to make this streaming player delivery
more efficient and reliable so a little
about netflix some interesting stats we
have about 69 million customers over 60
different countries which translates
into ten-plus billion hours of video
watched every quarter and at peak we are
about thirty seven percent of the
downstream internet traffic in North
America good reason to make the
streaming a little bit more efficient
right the last point is the unique
culture that we have and in the coming
slides I'll touch on some of the
elements of the culture and how it
influences the engineering at Netflix so
not so long ago a new happy user signed
into netflix discovered a video that he
wanted to watch and hit the play button
only to see the screen how many of you
remember the screen well when I first
saw the screen my expression was similar
to the primate on the screen so this was
a forcing function for netflix to
changed this experience and embrace a
plugin free solution which was provided
by html5 we worked with the w3c standard
to basically define extensions that
would enable adaptive streaming playback
of protected content so what is the
streaming player client itself before
that it the the implementation is
completely in JavaScript and it's
basically downloaded and executed in
your browser context so visualizing the
flow on the right when the user hits the
play button the website connects to the
netflix service gets the links to
download the JavaScript asset and it
would go it would go pull the the
JavaScript asset from the CDN and your
playback would start so what is there
some of the key features apart from what
you can already see on the UI which is
you know visible to the user the key
thing is adaptive streaming the other
features are authorization and
authentication we want to make sure you
paid us the logic to assess decrypt and
decode of the protected content stats to
understand our customers quality of
experience and logging to understand to
basically collect debug information and
on you know errors that you might be
experiencing so a little bit about the
streaming experimentation itself
metaphorically speaking internet is
built out of pipes of different sizes
which is delivering data at varying
rates sometimes same size pipes can
deliver data fluctuating rates like the
internet connection at my home if you're
listening cable companies
can my monthly bill also follow the
fluctuation I would I would really like
that well Eddie at the other end of
these pipes are a plethora of smart
devices with a spectrum of capabilities
and our job at Netflix is to balance the
video experience triangle across video
quality re buffers and delay of course
this is very high level if you talk to
the streaming experts they will probably
draw polygons for you with multiple
vertices so the in addition to this
balancing act the netflix a be test
culture has sponsored a big effort in
order to make the screaming experience
much better and this only gets more
valuable as our aspiration to expand to
the rest of the world by the end of 2016
another key tenets of the netflix
culture is to improve the development
velocity while not compromising on
quality it's a pretty essential for any
Internet company to you know operate in
within this tenet so we have seen a lot
of companies implement and execute one
of the variant of these pipeline I'm not
going to talk about our pipeline but I'm
just going to talk about the block of
which requires deploying the RF
streaming player to production and how
we use that to mitigate some of the the
risks so can I redeployment is very
popular and we heard some of the
speakers talk about it as well basically
you take the latest version of your
software take few servers server
instances and you basically enable them
in production these these instances are
running in parallel with their
production code and basically you're
collecting metrics and analyzing them to
s
the health of your latest code two key
challenges for us was that the software
that we own don't run on the servers in
the Netflix cloud they're running on
your laptops and PCs and second we don't
have instant instant access to the
metrics like these service applications
too so when we started off we decided to
leverage the the canary Clary testing of
our website which runs on the production
servers so what we did was we we pushed
we pushed the new version n plus 1 and a
variant of the existing version n Prime
onto the CDN and then we used the canary
and control clusters of our website to
kind of point to these new instances the
elb would basically split the traffic
depending upon the ratio of instances
that off server that you had in each
cluster unfortunately we didn't have a
lot of server instances to dedicate for
Canadian control so we would receive
about 1% of the traffic for the the new
version of the player and once the
player the users receive the players
they would basically report stats
partitioned by the player version so how
did we assess the health of our new
version of the player we basically use
some level you know some basic level of
statistical analysis prior to that
determining what metrics we could use to
determine the health was the key we
categorized our metrics into three three
groups raw count which contained the
start play counts and aerial counts in
production qie metrics which is quality
of experience correlates well with the
video experience triangle that I was
talking about earlier and the error
rates of the start player rate and the
end player
so if we drill down into one of the
metrics on the error rate and look at
some of its components we would
basically calculate the deviation sorry
we would calculate the ratio of mean
values for the baseline and which is the
control and the canary metric and we
would check deviation to assess the
health of the canary the amount of time
that you ran you can arey for the more
sample your you know more sample data
sample you collected and the lower bound
and the upper bound limits against the
deviation to you know determined
pass/fail and other states in the in the
Canaries course and then this would
translate into an uber Canaries cover
which would basically could be used to
determine whether we want to roll out
our canary to everyone or we want to
roll back so how do we how did we do
with the velocity and quality this chart
shows our deployment frequency over the
past 15 months until june of this year
and you can see more than sixty percent
of our deployments happened after we
started the whole canary process so it
gives a lot of confidence to be able to
do that in terms of quality there were
five issues that we actually detected in
production and we could actually trigger
a roll back or roll forward depending
upon the the scope of the bug so what
happened since June well we had a
website update and we had a swanky
looking website now with the whole new
tech stack it also gave us the
opportunity to modify our hold
deployment process so what we did was we
moved the logic to a layer we move the
logic for sampling production into a
layer that we control rather than
depending on the elb to kind of split
the graphic between server instances and
like I said it was basically a function
of the number of instances in the
cluster which determined how many users
actually got
the latest version of the player so it
basically evolved into a more phased
rollout kind of a system we had granular
control over how many users what
percentage of four users got got the
latest version of our player which
basically helped us remove sampling bias
at every stage we were also able to
basically determine platforms with low
usage of course not all browsers are as
popular as chrome so we wanted to be
able to assess health on those browsers
too we didn't have to manage any server
clusters or you know maintain any server
clusters of course the drawback over
here was we were not able to do any
comparison of the raw counts so this
chart shows the new were the new player
uptake rate as we gradually control the
the percentage of users that get
allocated with for the new version you
can see that there is a ramp up time for
version n plus 1 around 120 minutes
before it reaches a stable distribution
in addition to that there is a key
problem the analysis that we do of
version and vers versus diversion n plus
1 was flaky only because we with version
n we had a larger sample size so which
resulted in metrics being a little bit
more stable compared to the version n
plus 1 where the sample size work was
smaller and we had a more noisy metric
collection so comparing to do those two
metrics would actually give us
inaccurate results time so we fell back
to our original the way of doing things
we introduced a new and n prime version
of the player which was a variant of the
current production version and we would
we were comparing and prime against n
plus 1 where it was a little bit more
apples to apples comparison
so in the end it's basically a balance
between the customer impact you have and
the signal evaluation rollback and
performance of the whole experience have
implications for our customers whereas
the metrics that we select the sampling
frequency and the statistical analysis
that we use have an impact on how strong
the signal you're receiving from your
ear analysis system with that I this as
we are just you know scratching the
surface of this whole process if other
folks have done this before we would
love to hear from you and also any other
feedback you have on the system
questions thank you so what kind of
issues have you detected with this type
of analysis have you how are you able to
pinpoint the issues after you detect
that the build was bad so the one issue
that I can you know recollect was
basically we deliver we deliver video at
different qualities depending upon your
depending upon the bandwidth and your
device capabilities and there was a
sorry repeating that we basically
deliver video at different qualities
depending upon your network and your
device capabilities and there was a bug
in the adapters framing layer where the
average quality of the delivered video
dropped significantly and we were able
to compare the the average weighted bit
rate that we track as a metric across
the two bills and determine that you
know we introduced a bug and we could
roll back instantly so this is kind this
is highlighting one of the issues that
we found there are several issues around
it a lot of lot around error rates as
well
and one last question yes do these
experiments affect enough users to cause
significant complaints by those users is
there a way to mitigate this so they the
short answer is no because we're doing a
lot of these experiments behind the
scenes and we're doing it very
frequently you could see we were
releasing our streaming player almost
twice a week and we do extensive
analysis on the the quality of
experience that you receive when we are
doing actually a BTS and if there are
any issues we are instant we can
instantly resolve it and you know give
you a new version of the player so we
haven't had any customer complaints and
we are hoping we won't have any in the
future as well thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>