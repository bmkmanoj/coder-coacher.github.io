<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: I Don't Test Often ... But When I Do, I Test in Production | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: I Don't Test Often ... But When I Do, I Test in Production - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: I Don't Test Often ... But When I Do, I Test in Production</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/xkP70Zhhix4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Our next speaker is Gareth Bowles from Netflix.
And Gareth is going to talk about how Netflix
reliability tests software when their data
and production and content is continuously
changing.
&amp;gt;&amp;gt;Gareth Bowles: Thanks, Sonal.
Hi, everybody.
Thanks for having me at GTAC.
Netflix has some interesting problems in the
area of test.
We have a very complex distributed system
which is changing very rapidly.
Building those systems is hard enough.
But testing them is even harder.
And we found that traditional testing methods,
although they're still very important for
us, really fall short.
And so we've had to come up with some different
methods of ensuring that our systems are reliable.
I'm going to go through three of those methods
today.
The first one is called the Simian Army.
It's a set of software monkeys that essentially
set failures into a production environment
rather than trying to simulate them in a test
environment.
The second method is running code coverage
analysis in production.
And then, finally, I'm going to talk about
canary analysis, which is a way of verifying
new features and changes, again, in a production
environment.
I'm Gareth.
I sell frozen bananas at Netflix quite often.
But I also work on the engineering Tools team.
We're responsible for developer productivity
at Netflix.
And our goal is to ensure that any developer
-- we have about 800 developers -- can build
and deploy their software as easily and as
frictionlessly and as frequently as they want
to.
Just a little bit about Netflix to kind of
set the problem space up a little bit.
Is there anyone here who hasn't heard of Netflix?
Anyone a customer, by any chance?
Thank you.
[ Laughter ]
So on the surface, obviously, we're an entertainment
company.
We serve you streaming TV shows and movies
on any kind of device pretty much, you can
watch any way you want whenever you want.
But also we're, essentially, a technology
organization.
It takes a lot of engineering work to deliver
that video to customers.
We -- I would say well over 50%, probably
more like 75% of our staff are engineers or
involved in technology in some way or another.
So we have a pretty big engineering challenge
ahead of us.
A few facts and figure that is might be interesting
to you guys.
We have more than 50 million members now.
We're in 50 -- more than 50 countries.
We have all of the north and South American
countries, plus most of northern Europe now.
And we're regularly expanding into new countries.
Our members watch more than a billion hours
per month, which is a pretty staggering figure
to me.
That's about 114,000 years, just to put it
in perspective.
And we frequently clog up the Internet by
accounting for --
[ Laughter ]
-- 34% of downstream traffic in the U.S. at
peak times.
I think YouTube is probably number two in
that race.
But we're trying to stay ahead whenever we
can.
[ Laughter ]
A little bit about the architecture of the
Netflix streaming service.
We are a service-oriented architecture.
We have a couple of hundred individual loosely-coupled
services serving up the movies and TV shows.
So at the top of this diagram, we have all
the interconnected devices that can play Netflix
content, game consoles, PCs and Macs, phones,
tablets, DVD players, TVs, et cetera, et cetera.
They collectively account for over two billion
requests per day into our API servers, which
are the front-end for the whole service.
And then those API servers generate over 12
billion outbound requests to those hundreds
of different services that make up the overall
architecture.
A couple of the main services are listed at
the bottom here, for instance, the personalization
engine, which serves you up a list of unique
recommendations based on what you've watched
and how you've rated it; movie metadata, which
tells you who was in a movie, what it's about,
et cetera, et cetera; very importantly, on
the right-hand side, the AB test engine, which
allows us to serve up a different experience
and a different set of features to a different
subset of our customers and then measure the
impact of those changes on their viewing habits,
whether it makes them watch more or less,
whether it makes them stop watching in the
middle of a show.
And then we can feed that data back and decide
whether or not to keep the new features.
We're constantly testing multiple features
like that in production.
I'd like to pause at this point and watch
people's faces when they see this diagram.
Anyone kind of guess what it represents?
&amp;gt;&amp;gt;&amp;gt; Your distribution network.
&amp;gt;&amp;gt;Gareth Bowles: It actually -- yeah.
It's supposed to -- really, rather than looking
at any detail, it's just supposed to show
you the complexity of the overall system.
It's generated from one of our monitoring
tools.
And it shows the interrelationships between
all of the Netflix services and their data.
As you can see, it's pretty complex.
A little bit about the platform we deploy
on, which impacts how we test.
We use Amazon Web Services, which many of
you, I'm sure, are familiar with.
We're probably AWS's -- one of the AWS's biggest
customers, anyway, next to Amazon themselves
who use it not only for their e-commerce sites,
but also for their streaming service that
competes with us and I don't know if any of
you have heard of before I mentioned it.
We use AWS because we have pretty large peaks
and troughs of traffic depending on what time
of the day it is and when people are watching.
So using AWS allows us not to worry about
provisioning hardware at all.
We can scale up and down just using software.
Some of the key concepts that AWS gives us:
Machine images.
Those are effectively snapshots of a virtual
machine that encapsulates the operating system,
plus the individual Netflix service that's
running on that type of instance.
Instances, the actual running virtual servers.
We use elastic load balancers.
Those are the front-end to our services and
distribute traffic evenly.
We have security groups.
Those effectively are software firewalls that
ensure the connectivity between services is
correct.
Auto scaling groups are pretty important and
enable us to scale the number of instances
we're using up and down, based on various
metrics, such as system load or requests per
second coming in.
And, finally, availability zones and regions
are a pretty key concept that I'll go into
later.
Amazon Web Services operates in multiple geographic
locations, called regions, such as U.S. east,
U.S. West, Asia-Pacific.
And within each of those regions, there are
multiple availability zones which effectively
equate to physical data centers in different
areas within the same region.
So here's one of my big problems.
The system is so complex that there's no one
person who can tell you how it all works in
detail.
We've got world experts in the various specializations
of our service: Personalization, machine learning,
video encoding, et cetera.
But our system just changes so fast and is
so complex that there's no one person you
can go to and say, &quot;Hey, how does this all
work?&quot;
I want to spend a little bit of time talking
about the types of failure that we've had
to deal with in production and how, ideally,
we want to test for those types of failure
so that we can avoid them happening again
and actually impacting customers.
It's a constant issue when you're running
on a cloud provider where the underlying hardware
isn't under your control.
One issue is that the service can go down
in one or more of those availability zones.
There was a -- excuse me.
Wrong button.
In June 2012, there was a major storm in eastern
U.S. which caused a pretty widespread power
outage that caused many, many Amazon instances
and specialized database servers to go down.
It had quite a large impact on Netflix streaming
customers.
At the end of each of these slides, I've put
a link where you can go and learn about the
outage and how we dealt with it.
Even worse than that, we can lose a service
in an entire region.
There are a couple of services, such as the
elastic load balancers, which aren't zone
specific.
The load balancer can instance traffic between
load availability and availability zones.
If a region goes down, you can lose the entire
elastic load balancer service.
That actually happened on Christmas eve 2012,
when an Amazon engineer who, I don't think
he was ever named publicly, I'm sure he wakes
up screaming still from this thing, but he
made some kind of deployment error that caused
multiple ELBs to go down in the eastern U.S.
Unfortunately, for Netflix, that was our major
region for serving traffic to all of the countries
that we served at that point.
There was a fairly major impact on traffic.
Luckily not as big as if the outage had happened
on Christmas day, because that's one of our
biggest day of the year when everyone gets
the Netflix subscription for Christmas, eats
the turkey dinner and collapses on the sofa
to watch Netflix.
A third way that AWS can go wrong happened
just this year in late September.
A large number of instances can get rebooted.
Some of you who are AWS customers may have
heard about this one.
So what happened was, within a period of about
a week, Amazon did a rolling reboot of thousands
of instances across all of their regions to
patch a security bug in the software that
actually runs the virtual machines.
In this case, Netflix had learned from various
previous outages, and it had very little impact
on us as an organization, and pretty negligible
impact on our customers as well.
And there's a blog post that we wrote about
that if you guys want to learn more about
how we coped.
So as you can imagine, these type of problems
really impact our major goal, which is availability.
We want our members to be able to stream Netflix,
whatever time of the day, wherever they are,
and get the optimum streaming experience.
We also want new users to be able to explore
the service and sign up for new accounts as
easily as they can.
And then once they're signed up, we want them
to be able to activate and start streaming
with the minimal effort.
That's a little bit about how the technology
works.
I also wanted to talk very quickly about how
people work at Netflix, because it's an important
factor in how we do things and how we test
and how we build software.
We're pretty proud of the Netflix culture.
We published a slide deck on it which is linked
there.
It's got 126 slides in the deck.
You can pretty much boil it down to those
two words: Freedom and responsibility.
Let's take a look at what that means from
an engineering point of view.
It means that each of the development teams
that make the couple of hundred services that
comprise the Netflix streaming experience
can set their own deployment schedules.
They can deploy as frequently as they like
and at whatever time they like, as long as
they work with other teams to make sure dependencies
aren't impacted, obviously.
The teams manage their own capacity and auto
scaling in terms of -- so they're responsible
for knowing what the load is on their service
and reacting accordingly.
And the responsibility part, all the developers
are in the pager rotations and they can get
called anytime in the day or night if something
goes wrong.
So they've got a pretty big incentive to make
sure that their software works.
So how do you test this stuff when everybody
is deploying independently, effectively, things
are changing very fast, and no one person
really understands how everything works?
It's a pretty major problem.
All kinds of failures can happen, as we've
spoken about.
Hardware can fail.
Your power can go out.
Sometimes the backup generator will go down,
too.
That's happened in reality a couple of times.
But apart from those incidents that are out
of our direct control, as an organization,
we can inject all kinds of failures, too,
without knowing it.
And software bugs obviously are always happening.
Nobody's 100% bug-free.
And people make mistakes.
We've still got human factors within the deployment
chain, and someone can make a mistake and
screw up deployment that can impact customers
potentially.
To some extent, we can design around those
problems and we can make design decisions
that would avoid them.
Some pretty obvious stuff like exception handling,
trap predicted errors and handle them in the
code to make sure they don't crash the code.
Redundancy we obviously use because we operate
at large scale.
So if one individual instance of a service
disappears, we have other instances running
that can pick up the slack.
We can use design patterns like the circuit
breaker pattern to provide a fall-back experience
if a service goes offline or gets degraded
that other services depend on.
Good example of that is if our personalization
service is unavailable or degraded, then instead
of just showing you a blank list of the movies
that we think you would like to watch, we'll
show you the top 20 movies from everybody
so that you still get your list, and in a
lot of cases you probably won't even notice,
if you've got the same taste as everybody
else, anyway, that there's a problem in the
back-end.
But is that enough, he asks?
Probably pretty obvious answer there.
How do we know we've succeeded in making the
right design decisions until something bad
happens in production?
Does the system work as we designed it that
all our availability/resiliency tradeoffs
and design decisions actually work correctly?
And another important point, how do we avoid
drifting into failure?
And by that I mean because we have so many
thousands of instances running at any time,
it's a constant effort to make sure that the
configuration of those instances stays consistent.
If configuration is not consistent, then it
can lead to the types of problems that are
very, very hard to debug in production.
So one obvious answer -- excuse me.
A lot of you are testers here so you'll probably
say you just need to test this stuff more.
Easy; right?
And Netflix is very good at testing.
We have developers who are very dedicated
at unit testing, and they have good test coverage,
all the Internet tests get run on users' local
machines in this part of the continuous integration
pipeline.
We have test engineers who write very complex
integration tests, again which run as part
of the integration pipeline.
We have a performance engineering team, which
is continually looking to stress the systems
and look for performance bottlenecks.
Now, here's the real problem that's hard to
deal with by traditional testing methods.
It's hard to do exhaustive testing to simulate
not only all the failure modes that you've
already come across, but to try and predict
failure modes that you haven't even seen yet.
Here are some of the reasons why that's very,
very difficult.
It's very, very hard to generate accurate
test data because our data sets are so large
and they're changing so frequently.
We have Internet scale traffic.
Obviously, we can't -- for cost reasons alone,
we couldn't generate 34% of downstream Internet
traffic in our test labs.
[ Laughter ]
We could try.
There's very complex interaction and information
flow, as I mentioned, between the services,
which makes it pretty hard to write up a comprehensive
test plan.
The services are independently controlled
and deployed, so it's hard to keep track of
deployment schedules and what's live at any
particular time.
And we're constantly trying to innovate.
So we're adding new features all the time,
and those are very hard for a tester to keep
track of, too.
So here's another way we came up with: the
first testing and production method that I'm
going to talk about and spend the bulk of
the time on for the rest of the presentation.
You can cause failure deliberately in a production
environment to validate those resiliency decisions
that you made.
You can test your design assumptions by stressing
them out with real traffic.
And this is a huge one.
Don't wait for failure to happen randomly,
because it always happens on Sunday night
at 11:00 p.m. or some equally inconvenient
time.
Remove the uncertainty of that failure by
forcing it to happen regularly when you've
prepared for it, or somewhat prepared for
it at least.
So that's where we came up with the Simian
Army, a set of loveable software monkeys that
inject all kinds of chaos into our production
environment.
Chaos Monkey was the original monkey way back
in 2009 in his original form.
He's been redesigned a couple of times in
between.
What Chaos Monkey does is to randomly terminate
instances in a service cluster.
Cluster is a term that we use to identify
a group of instances that are running the
same version of the same Netflix service.
You can set Chaos Monkey up with a predictable
probability of terminating instances and you
can also set it up within a particular time
of the day or week so it's not running randomly.
You can actually predict when instances are
going to get terminated.
That obviously simulates a failure situation,
very common in a client environment where
instances can go down at no notice, usually
due to failures in the underlying hardware.
We run Chaos Monkey during business hours
so that there are engineers around to cope
with any problems that the monkey generates.
It's now the default for production services
at Netflix.
So if you write a new production service and
deploy it, Chaos Monkey is going to be enabled
for your service unless you explicitly disable
it.
And I'm happy to say that after running it
-- whoops.
Excuse me.
There's a previous -- Sorry.
I missed a point I was going to make there.
I'm happy to say that nowadays, after we've
been running Chaos Monkey for a couple of
years, that it pretty much goes unnoticed
and instances are getting terminated without
engineers even noticing them.
And obviously without any impact on customers,
either.
So once we got happy with doing individual
instance terminations, we wanted to make some
bigger monkeys, and so we came up with Chaos
Gorilla.
Obviously bigger than a monkey.
He carries a pretty large weapon there.
I'm not sure what that's going to do, but
let's see what Chaos Gorilla does.
It simulates an Availability Zone becoming
unavailable, which is one of the actual failures
I mentioned that happened back in 2012.
The big benefit we get from that is it validates
multi-Availability Zone redundancy which is
one of our design principles.
By default, when you deploy a Netflix service
within a given region, your instances will
be deployed across all of the Availability
Zones available in that region and the traffic
will be balanced between the zones by elastic
load balancers, usually, so you have so redundancy
built in if one region goes down, but we wanted
to test that before that 2012 problem happened
again.
That's the multiple AZ deployment that I just
mentioned.
Unlike Chaos Monkey, we don't run Chaos Gorilla
all the time.
That might be a little too high impact.
So what we'll do is it announce it ahead of
time.
Our reliability engineering team will set
up a time when Chaos Gorilla is going to run.
There will be an incident room where people
can sit and watch the progress and engineers
can react to any problems that happen.
Again, we've run Chaos Gorilla a few times
over the last couple of years, and we're pretty
confident now that we can cope with outages
of an Availability Zone or a given AWS service
within an Availability Zone.
So, even bigger monkeys.
Unfortunately, picking the gorilla, we already
got the biggest primate in actual physical
existence so we had to come up with a fictional
monkey, which is the Chaos Kong.
Chaos Kong is designed to simulate an entire
region going out, which was the effect of
that elastic load balancer outage on Christmas
Eve 2012 that I mentioned.
And also theoretically, it could happen -- for
instance, Amazon's U.S. region is all in Northern
California.
So if there's a really, really big quake,
it could potentially take out all the data
centers within that region.
To deal with an entire region going down,
it wasn't just a matter of redistributing
traffic to different zones within the reason.
We had to use an active-active strategy which
actually involved rearchitecting our systems
quite significantly.
Active-active means we have a complete copy
of all the services and all the data running
in at least two regions at any one time.
And if one of the regions goes down, we can
switch traffic to the backup region.
Again, we run Chaos Kong on a schedule every
few months on a defined schedule, and engineers
can get to the incident room, watch progress,
react to any problems.
Chaos Kong was run a few weeks ago, I think
was the last time, and there was pretty minimal
impact, especially negligible impact on our
customers.
So those are the types of failures we can
deal with when an instance just disappears
or a large group of instances disappears,
but what can often happen and is very problematic
in our kind of service-oriented architecture
where there are lots of dependent services
is that services can get degraded.
So they're still running, but for some reason,
they're not running at their optimum level.
And for that we designed a monkey called Latency
Monkey.
It simulates an instance or a group of instances
having a degraded service in various different
kinds of ways.
You can simulate things like CPU getting maxed
out, memory running out, very high I/O or
network bandwidth getting maxed out, for instance.
And the point of it is to ensure that that
degradation doesn't affect other services,
and for the services to be able to react to
them and handle -- implement some kind of
fallback behavior like the one I mentioned
with the personalization service earlier.
In a 
couple of cases, we actually -- with Latency
Monkey runs, certain teams found dependencies
that they didn't even know they had at the
time.
So that was a useful scenario where they were
able to find unexpected dependencies and define
what happened to their own services when those
dependencies had problems, and then fix those
problems.
Next is Conformity Monkey.
That handles the issue of configuration drift
that I mentioned earlier, where if you're
not very careful about the configuration of
all the thousands and thousands of instances,
you can get very subtle problems due to the
misconfigurations that can cause very hard
to debug problems.
So what Conformity Monkey does, it's a service
that runs over all of the service clusters,
and it applies a set of conformity rules to
all the instances running in those clusters
and then reports back to the service owners
of any rule violations.
Some example rules that we look for are things
like standard security groups not being applied.
If that isn't the case, then services can
have connectivity problems, or they might
be more open to requests from unexpected parts
of the Internet than we would like.
If an instance has been running for too long,
that can be a problem.
For instance, they might not have security
patches applied that we need.
Then they have out-of-date versions of the
Netflix services running on them.
And every instance needs to have a health
check URL so that our monitoring systems can
verify that the instances are up and running
and healthy.
So if that URL isn't found, it returns an
error code.
Conformity Monkey will tell you about that,
too.
A couple more monkeys that I don't have time
to go through in too much detail.
We have Janitor Monkey, which isn't really
a test monkey, per se.
It's used to clean up unused resources.
Because we're constantly deploying and redeploying
and updating, we can get a lot of stale resources
such as Amazon machine images which have some
old versions of the service that we're not
running anymore or launch configurations that
are out of date.
So Janitor Monkey will just go through and
delete those on a schedule.
Security monkey is similar to Conformity Monkey,
but operates on security-related issues and
configurations specifically.
Then there's Howler Monkey which we don't
have a Netflix logo for, I found, so I had
to get clearance from a real howler monkey.
That highlights an interesting problem where,
because we run on Web services, they have
a fixed amount of hardware available and our
AWS accounts have limits on, for instance,
how many instances of a given type we can
run at any one time.
And we don't want to discover that when we
have a traffic spike and we need to scale
up and then find out we're going to run out
of instances, so we use Howler Monkey to warn
us beforehand if we're reaching those limits
and we can get them expanded if we need to.
You can try out various members of the Simian
Army.
They're available on our github repo.
Currently, Chaos Monkey, Conformity Monkey,
Janitor Monkey, and security monkey are available.
There are some more coming soon.
One very important point.
You don't just have to use AWS in order to
be able to use the Simian Army.
If you happen to have an internal VMware setup,
then the Simian Army has been ported to run
on VMware as well.
So I encourage you guys to try it out if you're
interested.
I'm going to skip this slide and come up to
this one later.
Let's go on to the second type of testing
and production that we do, which is running
code coverage analysis in production.
And our theory is if you don't run your code
coverage analysis in a production environment,
then you're missing out on a ton of really
useful data.
Let me explain why that is.
I'm going to go through this slide.
Sorry, these are in the wrong sequence.
What do we get from running code coverage
in production?
We can get real-time code usage patterns in
terms of the actual paths that get executed
by real customers generating real requests
that run over the software as opposed to basically
telling you how good your tests are by tracking
the coverage that's generated by running tests.
We also use a tool that counts the number
of times that an individual line of code that's
been executed so we can find which code paths
are more commonly used and which ones are
very rarely used.
That enables us to do things like focus our
testing by prioritizing the frequently executed
paths and looking for frequently executed
paths that don't have very good test coverage.
And it even lets us identify dead code that
isn't executed at all in a production scenario
and can be removed along with the tests, making
maintenance of the code a lot more easy.
We did that by using the Cobertura tool.
I don't think I mentioned before but we're
mostly a Java shop at Netflix.
Nearly all of our services run on the JVM.
Cobertura was the best tool we found when
we started this initiative because it does
give you a count of how many times each line
of code is executed as opposed to just the
binary indication of whether or not a line
was executed.
We made it easy to enable in production, so
we include the Cobertura JAR files in our
base machine image which all of the services
build on.
And you can just set a runtime flag to enable
them by adding them to Tomcat's class path,
and then the data starts getting collected.
You can enable it on a single instance within
a cluster of multiple instances of a service
to minimize performance impact.
But we actually found when running it, the
performance impact was pretty low and typically
less than 5% performance degradation.
We usually run code coverage in production
for maybe a day on a particular service to
make sure all the code paths get covered and
we've got all the traffic patterns that are
coming in, and then we'll compare it with
data collected from the same service in a
test environment to see how effective our
testing is.
Final testing in production is what we call
canary deployments.
This is a term that some of you may have heard
of by now.
I know Google does canarying in some of their
services at least.
Some of the Googlers here might have heard
of it.
The original term came from the old coal mining
days when coal miners would be very vulnerable
to coal gas in the mine shaft which could
explode at short notice and was pretty poisonous,
too.
So they would take a canary down in a cage
which is extra sensitive to the coal gas.
If the canary started coughing or keeled over
and died, then the miners would know there
was going to be a problem.
What does canarying mean in terms of a distributed
system?
It means that we -- When we have a change
to one of our services, we'll push that change
in production to a small number of instances,
leave most of the other instances running
the previous version.
But we'll switch real traffic to the new version
to see how it behaves under a real customer
environment.
We use our open source tool called Asgard
for doing the deployments and we use a concept
called red/black push.
I think some other people call it green/black
-- green/blue push.
It's basically a concept where you have two
versions of a service running at any one time
in different clusters, and you can vary the
number of instances in each cluster depending
on which version you want to switch your traffic
to.
We'll monitor the new instances very closely
to detect any kind of regressions in performance
or any functional problems.
That was initially done as mostly a manual
process by watching graphs, real-time graphs,
but we now have some pretty sophisticated
automated canary analysis where we can do
things like automatically spin down the instances
of the new service if they detect, for instance,
a performance degradation that's over a 10%
barrier.
Again, that's left up to the individual team,
so certain teams will have a very low tolerance
for problems, and other teams that may be
just starting out with canary deployment,
don't quite know how their production environment
is going to behave, will have a larger tolerance.
Just going to go back to the &quot;what's next&quot;
slide.
It's a long way back.
Sorry about this.
Okay.
I wanted to just wrap up the talk by talking
about what's next for our production testing.
One thing we really want to get is finer-grained
control over the types of failures that we're
injecting.
There's a pretty big difference between Chaos
Monkey bringing down an individual instance
and then Chaos Gorilla or Kong bringing down
a whole region.
So that's something we can look at, bringing
down a group of instances or even a group
of connected services, to see what happens.
We are constantly looking for new failure
modes, ideally, to predict them before they
happen in production and impact real customers.
And our real goal is to make chaos testing
as well understood and as well practiced within
companies where it's as relevant as regular
regression testing is.
And -- oops.
I'm sorry.
Okay.
All right.
That's the end of the talk.
Sorry about the technical glitches there.
Thank you for listening, and I think we have
a fair amount of time for questions.
&amp;gt;&amp;gt;Sonal Shah: We do have time for questions.
[ Applause ]
We can take some live questions first.
So while the speaker comes up, is there a
story behind the whole monkey theme that's
going around?
&amp;gt;&amp;gt;Gareth Bowles: I'm not sure -- sorry --
&amp;gt;&amp;gt;Sonal Shah: The whole monkey theme that
you have going around in the test structure.
&amp;gt;&amp;gt;Gareth Bowles: I think the idea is monkeys
are chaotic creatures.
Get a barrel of monkeys loose in a room and
they'll cause general havoc was, I think,
the idea.
&amp;gt;&amp;gt;&amp;gt; Okay.
I've got two quick questions.
The first one, what sort of release schedule
do you have?
What's typical of a -- of a heavily used service?
Is it once a day?
Once a month?
&amp;gt;&amp;gt;Gareth Bowles: It really varies.
The API service is probably one of our -- as
you saw, that's the front end to the entire
Netflix infrastructure.
That's probably one of our more advanced teams,
and they'll deploy multiple times per week,
two or three times per week.
Right now, some of the services, we'll do
every couple of weeks, maybe every month at
the absolute extreme.
But within that, the deployment frequency
can vary pretty widely.
&amp;gt;&amp;gt;&amp;gt; Did you see a noticeable increase in the
frequency of deployment once -- as you implement
these monkeys?
&amp;gt;&amp;gt;Gareth Bowles: We did, yes.
And due to other internal testing initiatives,
too, some of the more traditional test methods,
in general, people are doing much better testing
at Netflix as we go forward.
And I think the kind of community effect work
as well, as soon as teams or other teams that
we're deploying two or three times a week,
they decided they want to do that, too, there's
all kinds of advantages with smaller incremental
changes, for instance, that make the quality
go up, which some of the previous speakers
alluded to today.
So it was kind of a mass effect once it got
going, that most teams wanted to do the same
thing.
&amp;gt;&amp;gt;&amp;gt; Okay.
And last question, Chaos Kong, he's cool,
he's pretty powerful.
How did you convince the guys who were responsible
for revenue to put Chaos Kong on --
&amp;gt;&amp;gt;Gareth Bowles: That's a pretty good question.
I'm pretty sure we ran Chaos Kong in some
kind of a simulator environment.
I can look that up for you.
So we at least verified the concept and we
knew roughly what the impact was going to
be.
But, in general, we're a pretty engineering-driven
company, right from our CEO down, and we are
just lucky that we can do bold experiments
like that and have the backing of the higher
executives, including the people that look
after the revenue.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>