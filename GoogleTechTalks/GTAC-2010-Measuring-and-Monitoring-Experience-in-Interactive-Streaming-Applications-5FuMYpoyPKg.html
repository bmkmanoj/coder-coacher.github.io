<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2010: Measuring and Monitoring Experience in Interactive Streaming Applications | Coder Coacher - Coaching Coders</title><meta content="GTAC 2010: Measuring and Monitoring Experience in Interactive Streaming Applications - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2010: Measuring and Monitoring Experience in Interactive Streaming Applications</b></h2><h5 class="post__date">2010-12-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/5FuMYpoyPKg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you Google and a GTECH for giving
me the opportunity to speak at this
forum yes so let's get started so about
me I'm a swish anchor as a kid mentioned
I'm working adobe systems I'm working in
that part of Adobe Systems which which
which develops product on the flash
flash ecosystem so I deal with products
like Adobe Connect flash media server
flash media gateway and have been
working on the performance and user
experience engineering topics for last
45 years so what applications so as I
said my bridge so i'll be talking about
media streaming applications which are
so big it ends up nowadays on the
internet right so uh we have a so the
property is typical and you know basic
properties of these application and they
involve streaming multimedia right
interactive real-time a collaboration
when I say interactive real-time
collaboration it's all about humans
right the other primary consumers of
these features and services and SAS
delivery that means the entire service
is delivered over the web over the
internet so some examples of like in a
wipe or video conferencing or wipo video
calls are like enterprise rich
collaboration applications which
involves webinars like features you know
which involves webinars which have a lot
of things like screen sharing and things
like virtual classrooms which have
real-time white boards and stuff so uh
some examples of the product lines on
these classes application include but do
we connect cisco webex Office Live
Meeting skype and gtalk among others so
this is agenda for the talk basically
I've structured the talk in three
sections will be looking at the first
why quality of experience is a risk and
it's it's more of a risk in case of such
human centric applications we look at a
specific case study with respect to wipe
quality of experience of problems
how we solve that and then we'll use
that case study to derive a generic
quality of experience risk mitigation of
process guidelines of framework so to
speak then we'll have questions so why
quality of experience is at risk an a
big rig so let's let's see it kind of
pop quiz I'm a genius and we are used to
go through quizzes right so so consider
to the applications one is I like it and
writes minute I mean the names are a
giveaway but just give some sample user
feedback right on using say for example
the audio quality was crisp and clear so
if you take a user which take me baby
now this is sample feedback the audio
quality was crisp and clear our webinar
was a huge hit and participants heard
consistent pops in audio and webinars
with occasional big drops so the next
feedback video called quality so
seamless it was like conversation is for
real and video had intermittent freezes
throughout the two classes of feedbacks
and other questions so which part would
rather settle for and which product
would you rather recommend your friends
right so Allah not ask for a show of
hands because it's kind of a no-brainer
here right so we selected it's just one
dissect you know that feedback so so
there is essential difference you know
between a classes of like you know
applications which are built on
transactional processing from users
perspective for example a airline ticket
reservation system right so if you go to
a website and buy an airline ticket
right and you as long as your payment is
cleanly debited securely from your
account and you get a ticket to print
like you can wait for 20 seconds you
know to get the ticket I will be happy
it will be happy customer but what we
are talking about here is human emotions
like human emotions getting exchanged
through the service which is provided by
these interactive collaborative
applications and you don't want you know
while you're talking to your friend you
know
you want to be oblivious of that
technology around you I think you don't
want to you know when suddenly that pops
are like glitches come in the headphones
you become aware the headphones like you
become aware of the computer right and
the entire human centeredness of the
other communication gets affected right
so so the example which is sure earlier
life you know it has was the feedback
was white chalk and cheese right so it's
it's not that way usually it's usually
it's very subtle right if you if you go
back into the field right now people
have opinions about which is the my
preferred internet you know wipe
conferencing application like you know
I've heard people who prefer skype your
gtech over X Y Z competitors and if you
ask them for y like you know they can't
do very specific reasons they just say
that okay I like it right you know
that's kind of a general feedback so
this kind of opinion which which builds
over time you know and it leads to the
preference or rejection of application
which kinds of starts up the business is
kindly tightly I mean tight with a tie
to the opinions and of course like you
know preference is driven by perception
and perception spread so we can only
know directing the in the modern world
of a social intelligence if you can call
it or Facebook so if you have a
preference it spreads it has its own way
of spreading it is so so we better
control the you know the experience
before you know it starts affecting our
business okay so that was the problem
from a user experience perspective so
what's the problem from engineering
perspective here so I I couldn't find
the source of this graph which I used to
sing this and some of the testing
trainings right so I cannot quote the
source but this basically your presence
to brings that you know we would like
you know we're in the product life cycle
so so as I think we're hearing the
morning sessions also it's very costly
to find a problem like you know bug here
and then you would like to find the same
problems earlier in the life cycle so
the cost
increase many fold as we start for
discovering the bug later but that's not
the real problem here the real problem
here is it's even it's very
counterintuitive to even acknowledge
that experience is at risk why because
you know risks are like human centric
and subjective right so I was just
talking about the subtleness of opinions
and always starts you know building
preference so if you want to model that
abstraction you want to capture the
attributes of that it's it's very
counterintuitive and I'll tell I mean
from a if you if you come down to the
low level specifics of this problem
we'll see that so for example like you
know I i had a service which I give from
Adobe Connect and similar services are
provided by say competitors or like
companies like Cisco WebEx our office
live meeting so say one of the
Possible's part of my stream delivery
which happens is like I have a flash
player the meeting add-in right meeting
adding is a special flash flash player
which is built on top of flash player I
the flash media server have a flash
media gateway which you know bridges my
which is kind of protocol translator for
the flash rtmp protocol to the rest of
the world PSTN world well it talks to
the third party conference service
providers and sip servers and they
finally land up at a telephone like you
know some so there are so it's it's all
happening I mean only thing which I have
in like you know a control over is is
you know this portion this portion is is
in my control I mean I have this is in
the clouds this is in the clouds and
this this this is my system code and I
don't even own lot a part of my system
code which is delivering my service so
you know what happens is that you know
the problem which it's it's compounded
is that each of the teams of these
products which we own our separate teams
there sit at a separate geographies and
they don't always you know they are
there they are smart engineers who are
making a you know always improve my kind
of improvising on the flow control and
congestion control congestion control
gardens and coming up with
you know ways to optimize the service
delivery of streams but one person's
gain the other person's pain you do you
know it is a somebody when we try to
combine the services of these team we
get a different level of service which I
promise to my customers right and in
that situation right who is going to own
assurance of experience when I start
integrating this up different products
different you know the service provided
by different products which which
themselves are kind of a distributed
nature so I need to find a way to a you
know a deterministic way to do you know
discover bugs in such kind of
applications quality of experience bugs
I used that term bugs because although
it's it's kind of a you know it I was
one of the feedback I God was to use the
term problem or issues but i still use
bow because that needs to be solved and
that's that's kind of and so all the bug
you know you have a happy customer so so
we want to you want to you know the
first step toward solving or like you
know if there is a experience problem or
like you know I sure experience this to
acknowledge that there is a problematic
and a way to understand the problem is
to measure it so you know we we we need
to find ways to objectively and
repeatedly measure and baseline such
problems which will with the I mean
without which a very organized and
structured debugging is not possible so
right now the whole process of a you
know the experience a problem discovery
is ad hoc say say at this part of
climate presentation is ad hoc the
debugging is quite a structured and and
the prevention is something which is way
off i mean we first try to solve the
problems so right
so keeping up taking this concept in
this preface of what we want to try to
achieve right I'll let's go through a
case study which this is a actual we
solved a production quality of
experience but and I will come back and
see how we can derive as I shouldn't set
my parent agenda that how will it
derived as a generic resume integration
process guidelines right so here it's
all started I mean it was around a
couple of years back right so there was
this feedback which we got from the peel
and feed book was not really consistent
it was it was it was not like the part
is bad it was a certain percentage of
users that when we have this meetings
when we have this you know audio
conferencing meetings over large number
of audience or even bike you know which
which is like you know a meeting room
which has more than 20 30 participants
or whatever so we the listener started
experiencing small pops and glitches in
the wipe right and they seem to grow
over time right and this is a random
feedback you know I mean if you go back
to go back to develop us a QE I'm the
first thought when they listen to the
system everything is fine the first
thought is that all I mean they must be
imagining things right and so there so
it came it became a kind of a pattern
and it the pattern was a strong enough
so that we could not ignore this
sentiment right so we started to
research like you know how do we solve
this problem so as I said so what's a
one of the challenges we face like you
know customer feedback was started
getting consistent but we're not able to
reproduce that in the labs so for
example you know when you when we
started when the developers in Q II
started working on that so any random
you start start hearing noises I mean a
small pops and that's okay we have the
problem and you know that was that was
kind of false positive because that
didn't appoint to anything and British
change like you know we used to wish to
go and search for people who are very
sensitive to hearing like you know we we
want to like it's kind of crazy we go
and search for people but strong and
hearing were strong adhering particular
kinds of science sounds and cuz it
started crazy you know managing the the
problem discovery and debugging process
and of course like you know things were
like I mean the feedback from the
customers was I mean we could not kind
of get to the exact reproducibility of
the problem to start solving it and
using third-party tools had inconsistent
her third party tools had interested in
consistent results why we we could have
started measuring you know the audio of
freaked by recording what happens in
with third-party tools which which are
available is that the measure the audio
and video of the sound card right and at
this they put some put some algorithms
and gather the quality metrics now doing
that that is not tied to our system code
right I mean there are variables there
we and more importantly as we will come
to it they they give certain metrics
which are quite esoteric if I may call
it and they I cannot write you know
directly map to my experience so and
more importantly they are not there they
lie outside my power system code so if I
start if you have to implement
testability you know I have no way to
implement that so this what we came up
with a hypothesis at that time which
kind of roof correct that essentially as
I mentioned the the start of my
presentation that the entire service
delivery right is happening over the web
so there is a single pipe which is
delivering that service you know the
pipe which is coming the network pipe
which is coming coming to the client so
essentially the you know the incoming
multimedia data stream you know which is
a hitting a the transport layer
interface and it goes through our
application layer right which so the
transport layer itself has a qos
q as parameters like you know you can
have a bandwidth conditions which are
existing locally at the client at that
time there is a application layer q s
parameters like you know parameters
which I give in my client and the server
which start controlling you know the
congestion control and the flow control
algorithms right and on top of that
right we have this experience layer now
they said that's a that's the
abstraction we want to quantify right
and hypothesis is that the quality of
experience layer is some function of
transport layer curious and application
layer q s right so so that's the
hypothesis we started so there's the
point like you know we can if you start
have to implement a testability that's
the point weeks can start you know we
start owning our own system code we can
start putting hooks or writing API is to
get data to mine so that's what we did
right so we did was that we put hooks at
that interface of the transport layer
and the application layer q as in the
flash player and we started tracking
okay that this audio packet has arrived
now right so it's a couple of things
what so when we so we don't aim input
from a cognitive psychology research is
that human ear cannot discern
disruptions in audio quality of that
audio quality in terms of pauses right
if there are two packets are at the time
between arrival of two packets is within
150 milliseconds so that's a heuristic
which we got essentially that what that
means is that if the packet start
arriving later than 150 milliseconds
there is a there is a disruption to the
experience now we want to mind how bad
is that disruptions right so that's
that's the thing we want to quantify so
as I said we implemented that the
transport layer and the transport layer
interface we implemented a instability
to capture the arrival of a white packet
when they're with the coming so
essentially I mean taking the so this is
the simple math is also a measure of
jitters but we'll try to model this you
know measure in terms of what our
customers are saying and you know where
do you want to go to solve the solve the
problem essentially as I said so we
defined so this is the inter packet
delay ipd if for packet 10 is packet any
time of arrival of packet n minus time
arrival of like a 10-1 right so as I
said if you have the hypothesis that is
in the packet delay is more than 150
millisecond we have a disruption right
so we give we classified the so what we
say there was a if I PD is less into
packet delays less than 150 milliseconds
it's a good audience a perfect audio if
it's more than 150 millisecond its paws
right and we classified the pauses
according to you know according to the
duration of duration of the pause so we
called a a jerk you know of 150
millisecond and 400 milliseconds are
kind of configurable values and we kind
of evolved over like a reset over time
in terms of messing about customer
feedback but this is the kind of you
know the abstraction which we gathered
from the duration of pauses and we we
this was the hypothesis so what we did
was that we implemented a visualization
engine right and we started the
visualizing this packet says they're
coming and we asked we started listening
with our earphones so it was kind of
proof of concept because we could see
the pauses right and rather than hear it
because you know we put ingenious across
and we had this visualizer on and we saw
the big you know we had this blue and
red dots walk across and there were big
pauses and okay I heard it pause and you
saw see this big blue or red which
signifies the glitch along cause walk
across so it was kind of a assurance
that you know we are on the right track
you know this is this is kind of giving
us stats on experience and this is a
potential way to baseline a wipe quality
experience test so moving ahead I mean
so we started working on the metric so
but these were not these metrics were
not time series metrics right so are we
to convert them into time series metrics
because that's the way we could baseline
the test i mean we want to say that if i
have polling a the test you know at a
particular interval if my monitors are
my agent is polling then what is
happening in that interval i want to
capture my test at that level right so
there also we we say that you know this
was like a again if we talk about
experience of causes are two measures to
it is like how soon do the pauses occur
right and how long are they when they do
occur like so if we take these two stats
at least two tires they kind of give me
I can convert this into you know a time
series metrics for the measures which we
got earlier we converted that into
frequency and duration distributions
over time and what we got was you know
what we got was a visualization or
quantification of the experience so at
this point of time like you know so this
is of course in reference to a
standardized test or a test context
which well i'll just come come in a bit
so if we if we have a standardized test
fire if you keep the condition all the
condition constant and we have only
varying one of the variables we would
like to understand then you know in that
respect you know we have a we have a
baseline so this is a example of a not a
good test like you know you have lots of
pauses so you have lots of pauses here
which are over this but there are some
you know phases in the test which are of
good where we did not have any pause
growth and all the all the the duration
was within acceptable limits so this
kind of you know we had a way to
baseline
quality of experience test so we went
ahead and now what it boils down was to
you know you set up a test matrix test
matrix which includes the context you
know you you create a baseline test set
with the high-risk context and and okay
before that before that we you know try
to regress in the customer issue and we
had this a test and which proved that
customer was correct timing customers
are always correct so we saw that
actually what what they were saying
where we're kind of you know getting
that in numbers that there are some
disruptions in pauses occurring and they
seem to be growing over time so this was
kind of a long test we this is a three
and a half hour test and the data
started building over time so we got
that okay we have found the problem and
we found this to be repeatable so the
two objectives you know we had a
measurement system in place we found a
way to you know a way to repeatably
reproduce this problem in the lab okay
so as I was mentioning the baseline so
we this is kind of a lot too much into
graph it basically means that we
recreated this baseline test I mean
which which have different parameters
and these are this is this is a debug
this is the developer site graph where
we kind of narrowed down that this area
which is creating pauses this is this
test1 test2 test3 are basically
combination of our application layer qsr
parameter set up at the client side and
at the server side and we there was a
injection at this point right and
unfortunately unfortunately this setting
was our default setting right so around
seventy percent of the customers were
hearing or like you're experiencing the
problem so we went ahead we so the
developers were able to identify the
exact point in the code the race
condition which which
no actualized this problem and they went
ahead and fixed it and sure enough when
we fixed it right when we reduce the
build again the problem was fixed right
and we went back to the customers and
they validated it over time that you
know it's it's it's coming fine and you
did a great job in solving that problem
so so this was a so we this was kind of
a you know we started so this gave us
the confidence that you know we can
apply a this pattern hero this in
various area so we the entire product
you know is is built on features like
this we have wiped you know we have a
white boards we have a screen sharing
you know which which are typically
happens in a in a conferencing or in
enterprise conferencing a session right
and we wanted to model this take this
pattern and you know and try to use that
in other features as well so this is a
summary of you know how we solve the bug
three feet as i said we we found a
measurement method right and we could
reproduce the problem in the lab we
could baseline create baseline tests and
could solve the problem so another other
things which are which were happening
here at that like you know as I as we
saw in that a graphic of why baseline so
if if I if I even if I know the context
and somebody gives me a baseline I can
look at the graph and mention that you
know the experience would have been like
this without actually being present at
the part of it is without actually
listening to the audio so I so there was
this process of you know getting the
experience course unattended so we
wanted to like you know quantify you
know you know sanitize that process like
we want to really what we want to do is
put us course the experience right and
how do
we kind of come to that so in this
section we'll we'll just try to see how
do mind how do we mind you know the
generic approach which we which we would
take to do this you know the score
mining you know giving this course
rather than a human human being having
to analyze the metrics manually we'll
look at the test test bed set up and
this was a you know I as Esteban was
mentioning yesterday that we all have
interpretations and understanding of
testability particularly after this G
tag so this is off this is like you know
we discuss that you know we how we can
have a automation in the test bed which
aids the testability of this entire
process right and we'll look at the
example of how we can arrive at acuity
score decision tree and use it in of
course in monitoring and baseline
processes so this is about the concept
so so just to give this what we want to
do is like you know we want to arrive at
the other other scores like you know we
define the labels so this is a
supervisor supervised learning where we
know the labels are labeled beforehand
and that helps at this point because we
are dealing with as I said the human
centric attribute so we would not get
into clustering till beyond we
understand the system better so we of
course like you know we get the high
risk contact so as I said that every
test baseline is reference to some
context like and the context may be
deterministic which is from a which can
be managed from an application system
code right or it may the variables will
be in deterministic like like user load
all right are things like a local band
with conditions or usage profile what
kind of what kind of media profile is
being you say using talk is it a
conversation type of exchange or it is
music when we are doing screen share
like is it a fast motion explain
exchange with a lot of colors or it is
just a small a code review thing where
you know the codec will take different
parts depending on the kind of curve and
the compression options available so we
could so these are the contexts like you
know what idea is to identify the higher
this context we don't want to over do
these things we want to you know use
this for the most you know a critical
feature so we want to you know do a
risk-based analysis of what the context
we want to include in the baseline test
set which is basically some kind of
combination of this deterministic or
indeterministic variables of the system
and of course like as I mentioned
previously so we have this Q metric
which itself is a function of Q like us
parameters which are with the weather
there the net the transport layer q s
parameters or the application electronic
us parameters existing at the client
side so armed with this too I mean this
is just a what we do is like you know we
could we create the training data where
I right I mean we we create a we carry
out a test we define a week we derive
the quality experience classifier labels
we you know do the test so that the the
curve the school classifier has has high
high confidence and low impurity in
terms of you know applying that in the
in in in real life scenarios and you
know / we pass it through the test data
and finally what we have output of this
is a we create virtual observers that is
we want to do the you know the
experience mining unattended rather than
you know having to analyze all the tests
and context test and respective context
manually so um so and the you know this
is a very expensive process like you
want to carry out a lot of tests we
don't know what to do with the with the
test results and we invested on our
frameworks you know to to aid this
process of a i would say experience
experience baselining and we have so
this is not a so I'm part of the testing
theme so probably I would not be able to
show the demos but
interesting it was it would have made
but one of things we start with is that
developing agent so I was always
fascinated by you know when I take my
card with a workshop and they plug in
that computer and the in the engine and
they can get all the stats of my how my
engine is doing so we kind of you know
replaced our client so whatever meeting
client we have and we made a custom
client which includes this quality of
hues parameters and qoe parameters after
we have a say after we have defined the
decisions p we can plug the QE QE
parameters also here and we we made this
agent context-aware like you know it can
pick up context like you can ping the
server with a xml api call and pick up
what's the load on my server like right
now or it can pick up local context like
what is my cpu state of the client or
what is my network bandwidth of the
client so it picks up context and we put
these agents you know and integrated
these agents in different test processes
right so we have integrate put these
agents or probes in our load test
framework or as part for our van test
van emulation framework family you know
we we invested so what what happens we
invested on building a central QA
architecture so somebody when using a
test we so there are a lot of tests
which needs to be done and we don't want
people to go and save that say the
results you know manual is somewhere and
copy it to the file server so you want
that we automated that part of the
process we used to turn the agent the
results it picks up the information the
results can automatically archived at
some remote location right and it
provides some other services like
statistical analysis services we do we
do can do we can observe these quality
metrics in real time right we can do
overlays so there is essentially so you
know it so if we it's for its eight
provides are the concept of client and
that test for quality of experience and
I mentioned the term quality metric
warehouse there because I call it a
warehouse because it's one-way traffic
data goes in and time is a very
important dimension there
so this is a you know the other thing
which this air agent helps us to do is
to attack the context or like so I'm
observing the test when I'm creating my
when I'm bring my training tests is that
I want to tag the context what I'm
observing right because that's very
important in getting my quality
experience or decision tree of
classifier so this is this example
sample example of a video test what we
are doing is that the back end has a
graded context we are pushing the you
know server with calls like five call
stands called 15 calls this is the video
calls with are coming on the server and
these are the two clients which which
which is one client is is andrew is on
the left is underlying conditions sorry
one client is under land conditions and
the other client is assimilated a dsl
with cydia conditions and we start we've
been we the test is in progress and this
happens in real time when i'm giving a
representative view we can just tag the
labels right and what happens is that so
we have experience here and we can just
tag so it is a this is a concept of
client and a test so we are two clients
on the test which have to moving context
one is one is at the server side which
is load and the other is as a client
side which is the changing of the local
bandwidth conditions so we tag these
contexts and these are very important so
when this test is saved right we have
these a matrix which are which are saved
together with a with the context which
helps us in getting to the decision
crane so this is the classifier for
example we we further for the wipe this
is one of the sample a simple classifier
which I made so for example which has a
context the red boxes are the context
right we have a default settings we have
one publisher or like you know one
subscriber that's that's the that's the
that's a setup the test site you know
this this matrix would behave
differently if you do it in the lab or
we do it in the production at a customer
site so based on the context and based
on these for you explains parameters
with what we can gather is experienced
labels right
so this is a classifier which we derived
after doing a lot of baselining tests so
essentially at this point of time so if
I if I have this matrix and this is this
is something what happens in in in real
time is that we have the matrix we apply
that classifier into the thing and we
can identify areas of experience
disruptions at real time this is a you
know something which we can directly put
probes in our in our in our in our
production systems to these tests and
they can you know based on it some kind
of alert models or rules and clustering
rules these system can send out feedback
that hey I mean for this this period of
time your your experience was was not
good enough so this is an example of you
know those automated alerts you know in
which say this for screen sharing screen
sharing latency it's gives me that there
is experience issue it gives me the
metrics and the and the clustering rule
which it failed and it also gives me
that it gives me the you know context
like you know the client and server
context so it gives me the objective so
if there is a problem we start seeing
this pattern over time it gives it gives
me you know what is the problem right
the measurement of the problem and the
possible ways to look to solve this
problem for the for the for the
developers so it take the system back
again you know we use the same test we
we put in our CI test and you know we
get a label so we here you know each
whenever a new build comes in comes in
so we have it as I was saying like you
know we build a service out of
integrating many products like flash
media a flash media server flash flash
media gateway flash player and you're
confident agreat the whole system up in
the continuous test bed and we run this
test as functional test and what it
sends out you know as a week I like you
know is is experience labels experience
course so if there's some disruptions
and you know we start observing them we
can go back with the developers and
start solving them so this is how I mean
we take the entire process and integrate
into our baselines process
monitoring process into our this this
preventive process like continuous
integration tests and we have a labels
of coming on to mark the you know the
experience capability or experience
delivering capability of the system
which is being request and of course
like this is all this test we don't do
it like you know we kind of standardized
this test and they always happen over a
sanitized context because we are kind of
regressing them in the CI so net this is
the summary of a you know the the risk
mitigate risk you risk mitigation of
process you know we need to we which is
kind of we need to identify you know the
QA risk and risk factors will differ we
will define the QA risk metrics and were
to carry out proper relevance analysis
of what quiesce parameters and what you
know this context you want to use to you
know have a measure happy you know
measurement system in place that
relevance analysis is very important we
develop the client side agents we well
we run the run that tests you know we
create the baseline test we model them
for model them for taking out the QA
classifier or decision tree and then we
plug the QE classifier is gently back
into our monitoring monitoring systems
and and we can you can get preventive
alerts or a scores are based on
clustering rule or questionable failures
so and essentially parallely what we
have done is we have invested on
creating a test automation
infrastructure which doesn't solve this
problem directly but which aids the
humans in in carrying out this research
and and kind of get to the point of
experience assurance at a faster pace so
that's a summary of the whole talk so
I'm this was I'm done so any questions
okay from here
you know you mentioned a code where
where you know the the audio packet
enters my mate is my application like as
a as a rtmp packet so flashes rtmp
stream and III i could so as soon as i
get a audio packet there i just logged
the time of arrival packet so i used for
example we are dealing with a inter
packet delay so i use the UNIX epoch
time and so that i can always get the
time in milliseconds the reason why i'm
asking this question is because you know
when we were testing multimedia
applications and we wanted to record ok
the delays in terms of know you might
want to call it latency be used and
adobe product to do that adobe audition
yeah yeah ok so now why didn't you
consider to use an adobe product itself
yeah because yeah that's what I was
saying so why I could solve this problem
in all ways is that the point the you
know that I I so I have I work in the
adobe connect so this I put the hooks in
the code in my my system code if they
use audition right it is a as good as
any other i would say audio caption and
processing i mean i would say my name
tool which create which records the you
know audio of the sound card and we'll
just you know lay out the wave pattern
so that is not very useful in P button
in in my case because I a lot of
variables in reduced so we are playing
at the sub 150 millisecond range and we
we want to we don't want to get into
like you know how the outage or audio
processing is being done how it is being
handed to the sound card how that is
being rendered so there's a lot of noise
which gets introduced to my test results
so we wanted to capture it at the you
know the transport layer the point which
is coming through the pipe and hitting
my application the quality agent that
you mentioned on the client side do you
deploy that both to customers and to
your quality to your QA environment
so do you use that just for automatic
tests or manual tests in your
environment or do you also monitor the
production environment with that yeah
it's it's it's both actually so when
when we started start out with this
process so I mean let me just go back so
when we start out this process you know
I think you know this step till here
these are here like this needs manual
investment like you need humans to
research and come up with a model right
so once we have confidence on that model
we can go back and deploy that in
production monitoring so that's what we
do I mean once a once when its kind its
kind of ready to show it as we have high
confidence and we kind of have metrics
to measure the confidence in that case
it gets very I mean useful production
monitoring results as well hi what kind
of latency overhead does the monitoring
introduce I the monitoring that you're
doing of the packets does it introduced
any latency in the communications uh no
not not really because you know the
latency so the point at which we are so
if you're texting specific example the
point at which it is a capturing the
patent you know that is the point where
it is first hit i mean if latency is
introduced that is introduced both that
so that and moreover I mean what happens
is it so we have kind of measured that
that aspect and that has found to be a
negligible in terms of it's only 5% of
noise it introduces and the important
part of it is is that you know it's kind
of common for all the tests right so
that way it cancels out if we if we are
observing that integrations that weight
is common for everything so so
essentially these things are useful when
you start you don't have a regression
analysis right in that cases the error
which is introduced which itself is not
big enough to impact the test itself
that kind of cancels out and we have
left with you know we can we can do it
we can do a regression comparison in
that case I would have imagined that the
traceability of the quality of
experience issues would be a serious
problem in practice in the example you
gave it seemed like once the problem was
identified that the engineers didn't
have
much trouble identifying the cause of it
is that a serious problem in practice
and if not why not what is the problem
which you talk to identifying
identifying the cause yeah the root
cause of the a lot of experience issues
so I'm defying so you know we work with
this I was mentioned that there's a lot
of different products which provide the
services so this service is kind of an
integration of lot of these products so
to its it starts like you know you first
mean start with weed you don't have any
idea where the problem is and you do
this baseline test you you zoom in with
specific area then you create test and
you zoom in further right so one of the
problems which the salt is that you know
if I have a you know flash team and I
flash media server team there's their
separate teams they you know they this
they develop separate algorithms for
processing so there's a no owner if you
start owner to you know achoo the
services which is being delivered by a
flash player and flash media server put
together and putting it in a different
context like you know we we have a we so
one of the service offerings we have is
is the hosted which is delivered as a
enterprise SAS SAS service so there are
a lot of other external factors come
into the picture for example the u.p
have you no third party conference
service providers like a wire and rebate
you know who kind of pitch in to give
certain services you know service
offerings so these teams how these
features play out in the context of the
you know Marilyn back into the world in
random situations which have random
causes that's this so we kind of try to
create ownership of that that part like
you know we integrate this and we
integrate it with the real life use
cases these are the you know these are
the boundaries of service so another
other thing important aspect of this is
like you know we always take these
baselines back to the product management
and say but hey you're you know your
aura is not going to work
if there are 20 people speaking at a
time of the meeting is it ok right are
you fine with that right they said ok we
are failing or like you know we can call
it out in the tech nodes or whatever of
light so this also helps to define the
boundaries of the system and which which
is acceptable to users and whether those
are confirming to our business
objectives
yeah actually we we saw that was that's
yeah that's applicable for things like a
wipe so we we had started on that
because the mass score and the are
factors right so again a way to so we
calculate those more schools are out of
you know getting the sound out of some
of some of the sound I mean recording
the sound off the sound card and then
you know putting it like you know tool
together that matrix so that kind of
didn't assure us of you know where we
are going in terms of positives this is
kind of we wanted to track metrics in
terms of pauses so they were at a
different level of abstraction and they
were not kind of pointing to the
portions of a code which are solving the
problem but what we are trying to do now
is that we have enough information from
within the product itself that we can
apply the mosque or algorithms to create
a you know quality score based on moss
rather than this so that's that's kind
of in progress we just went through the
research papers and applying the same
same RFC's to calculate the most goes
out of the product parameters itself
yeah so thank you everybody and thank
you again for giving me this opportunity
to speak at seatac thank you
you
you
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>