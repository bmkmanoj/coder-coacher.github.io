<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The National Institutes of Health (NIH) and Computational Infrastructure for... | Coder Coacher - Coaching Coders</title><meta content="The National Institutes of Health (NIH) and Computational Infrastructure for... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The National Institutes of Health (NIH) and Computational Infrastructure for...</b></h2><h5 class="post__date">2008-01-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/gRnHm-j7KVg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">to this is McKenna's from Wisconsin but
it's the more quickly I Walker quite
some time and this is always challenging
work what likes always the person that
helped
cut through the red tape and really
interesting programs I feel like I'm
talking to an empty room why do I feel
like I'm talking about empty room all
right I thought what I try to do today
breath invite me to try to give you some
idea about why we support informatics
research at NIH we actually support
quite a bit of informatics research at
NIH most of you probably aren't familiar
with NIH so let me tell you National
Institutes of Health is the largest
supporter of biomedical research in the
world our annual budget is 30 billion
dollars and almost all of that
eighty-five percent of it is sent out to
the universities and academic health
centers for conducting research so we
spend a lot of money supporting research
throughout the country of that money we
conservatively estimate that 500 million
a year goes into information technology
research activities that's a lot of
different things it isn't just it very
little but it's actually computer
science but some of it is some of it is
software development some of it is
development of databases so there are a
lot of different things my own estimate
is that the 500 million a year is
conservative that's what we formally
report through the government but that
in in a relatively short time something
like twenty or twenty-five percent of
our budget will be invested in
information technology activities now
that doesn't mean just information
technology research it means may be
supporting clinical studies and ten
percent of their budget will be devoted
to databasing and and
curation of data and that sort of thing
but there is a there's enormous amount
of money here to be invested in and
spent wisely and what I'm going to try
to do for you today is in a short time
do two things I want to give you an idea
what motivates us to to do in
information technology research so I
want to just kind of walk through a
couple problems of interest that are
important biological problems i'm
assuming that most of you weren't
biologists is that right so biologists
in the audience there's no biologist
neil and so on i may I started out with
that assumption that mode nobody was a
biologist so I'm not going to I'm just
going to try to tell you the kinds of
problems and give you some inkling of
why the inside of information technology
whether it's computation whether it's
simulation whether it's modeling whether
its database activities are necessary to
do to meet those challenges that we we
see today so I'm going to outline the
challenges in a bunch of different areas
by listing problems more or less then
I'm going to focus on one area in more
detail what we are calling data-centric
science science that depends on large
amounts of data both visualizing the
data computing with the data visualizing
at curating it displaying at analyte so
forth and i'm going to describe for you
some of the investments we've made but i
honestly think that what's happening at
in the commercial sector with google and
and i don't know whether I'm allowed to
say Amazon and Yahoo and those sorts of
things but they're all doing things
involved in in cloud computing which is
going to significantly change the way we
do business and allow us to realign our
investments so that we can more
efficiently take advantage of these
opportunities that exist in the
commercial market that google it may be
leading the way in and then finally and
I I probably won't do this but I have so
i have a total of about forty slides 20
slides are just what i talked about now
20 slides are just kind of once through
quickly
one pagers of a bunch of centers that we
support we support a whole lot of
centers a lot i'm here in california a
number of at stanford for example that
that are focused on specific problem
areas a laugh and software development
and we fund these at something like a
million three million or four million
dollars a year to collect the necessary
students and postdocs and faculty to
just to undertake those efforts so
without moving let me just move quickly
into some of the areas i can see that
doesn't work from here unless i point my
computer right isn't that interesting i
want to yeah so I've just made this
statement I'm repeat issuing it is that
the biomedical informatics and when we
mean by biometric we use the term
biomedical informatics to distinguish a
number of different terms that are
commonly used in the community people
use bioinformatics people use
information technology people use we use
the general term biomedical informatics
to refer to computation and simulation
to modeling the use of computers for
modeling's and we distinguish between
simulation and modeling and that
modeling what's often exploiting in our
helping us assemble to assimilate the
data we've taken and then the data
centric science for collaboration then
I'm going to describe and this is this
is the newest area and probably the most
rapidly developing area in the
biomedical research community in the
area where I I see us investing billions
of dollars over the next year relatively
shortly so I've just taken some clips
here and I'm not going to go through
these in detail but I I wanted to give
you a flavor of these this is from the
science the 125th the anniversary issue
of science in which they outline 125 big
science questions of those big science
questions or things like dark matter and
a bunch of astronomical questions but a
large fraction of them maybe something
just under half of them of those 125 are
in the biomedical region this is the
areas where there's a lot of
unknowns and I'm the next two slides
I've just selected and copied from the
article the sidebars that give the names
of the of the problems and these are in
the molecular ariad largely there are
things like what's the orange and a home
occur Alec chirality what it keeps the
intercellular traffic running smoothly
why are some genomes big and others
compact how many technologies lower how
did well that's another so these are
areas that each of these areas without
too much difficulty I could convince you
that there's a big component of
computation or simulation or modeling or
combining lots of data in order to make
progress in understanding or dealing
with that major problem here's the
second major problem here's a second set
and so this is a total of 24 of the 125
what triggers puberty there's a question
that what if we knew what triggered
puberty may we avoid triggering it going
the other direction right how many how
to vertebrates depend on innate immune
systems to fight infection immune system
is a huge complex system that will never
be understood without detailed computer
models to help us grasp these things
anybody that Wade's into the immune
literature rapidly runs into what I call
the anti language it's just so confusing
so many things happening this is couple
of this change this to more things
happen over here and and trying to grasp
and keep all of that stuff before us is
going to require the development of
detailed computer models these are this
this is the second 12 I've shown you if
I had shown you the other there's 24 or
48 more 24 model that all have to do
with diseases specific diseases and I
would argue that the computer
informatics here biomedical informatics
is going to be the key to understanding
how these diseases what the the source
of these diseases the otology of the
etiology of these diseases and the cures
because so many of them are related to
the genes or to the proteome to the
proteins that are expressed in doing
this
ah move to the end
I get rid of these funny transitions I
only did it one here so one of the major
here what I've taken here are a few
areas just to give you a sense of what
are the research areas again I'm
focusing here on what motivates us to
fund computing centers modeling centers
centers that are doing simulations
centers that are collecting data in a
meaningful way and allowing us so in the
narrow area of bioinformatics one list
of things I think this is a wikipedia
lists in fact if you do Wikipedia
bioinformatics this will be a list of
things that you'll get analysis of gene
expression high-throughput image
analysis prediction of protein structure
all of these areas are intense
computational areas require substantial
involvement of computing resources
considerable sophistication in terms of
software development and algorithms I
want to mention one area that some of
you probably have never heard of called
metagenomics and it's an interesting
area it's a recently emerging area
there's a fellow named craig Venter who
was the head of the genome human ahjumma
the tiger the Institute for genomic
research he's just founded a new
Institute out at UC San Diego devoted to
metagenomics which is an intense
computational effort it's sort centered
at San Diego because the supercomputer
center is there and they had a great
deal of computer expertise which would
allow him to take these advances in
bioinformatics and and in metagenomics
it was the bioinformatics the
capabilities that allowed for both DNA
refinement amplification but most
importantly for shotgun analysis so the
way people sequence jeans today is not
just take along jean and march through
and figure out what the sequence is what
they do is they just take a mess of
jeans chop them all up and look at
little segments of those jeans and
sequence maybe a hundred or a thousand
or ten thousand length which is a very
small fraction overall they do thousands
of these fractions and then they use
computers to line them up and figure out
where they overlap and ultimately figure
out what the full sequence is in
metagenomics is a
brand new feeling field where they took
their able to take environmental samples
it used to be that in order to sequence
something you had to take a single
organism grow it up to a huge amount of
it so you can extract enough of the same
kind of DNA to do analyses of now we can
work with individual cells and in fact
what they do is they just take a good
mission cells chop extract the DNA from
all of them chop it all up sequence all
those pieces and then use the computer
to puzzle together which parts of them
go together and how many different
organisms were even in there to begin
with this is a major insight and here
are some of the observations that were
made in just the last few years in 2002
and environmental shotgun sequencing was
able to show that 200 liters of seawater
just collected in seawater had five
thousand different Virant viruses the
total number of viruses we used to say I
used to work in in marine organism area
and it used to say a litre of water
contains a million viruses now that
probably seems I always stunned me when
I when I recognized me you just take a
liter of water you collect a million
viruses out of the middle of the ocean
not by the wear the suit the treatment
plant sewage treatment plant releases
out in the middle of the ocean so he
collected this is a vendor project 200
liters in a 200 litre sample found five
thousand different viruses there are
many many more than five thousand two
hundred million probably but five
thousand different viruses many of them
unknown had never been seen before
because they are very small yes show
that there are over a thousand virus
species in a human stool that gives you
pause for thought so we're surrounded by
and there are a million different
viruses in a kilogram of two pounds of
marine sediment that's amazing so these
organisms are virtually unknown until
this type of process which is enabled by
computational capabilities only can
never be done without them this enables
to ask questions and to look at things
we've never been able to look at before
and we're finding astounding things
these organisms in the sea for example
just in the sea
dramatic effects on health they have
dramatic effects on whether the global
change environment is heavily engaged in
trying to understand what those
organisms are and how they these are
just the virus viral ends of it not the
bacterial or the small organisms so
without the computing capabilities we've
never even been able to do these sorts
of things here's a list of challenges
that all have computational components
this is from a meeting that Bret
Petersen here organized last year or two
thought yacht took meds Oh 2006 more
than last year now at NIH sponsored by
the computing research association which
was aimed at trying to figure out where
computer challenges could what are the
computer ciao computer-assisted
challenges in biomedical research and
here is one invent one of the
participants list this is john willie
and he heated headed up one of the teams
and the working groups that tried to put
together and i'm not going to read these
to you or even go through them but
suffice it to say that there are
substantial numbers of things that we
cannot do today that we would like to do
having to do with the structures of
jeans and the structures of proteins
that that without computer without
investments in computer technologies of
software and hardware we aren't going to
be able to handle these sorts of things
so we're talking about some major
changes which are going to be
precipitated by the development of this
technology and as often is true in
especially true in biomedical research
but it's all true in all research is new
techniques and new technologies often
bring greater insights and allow us to
ask new questions that have never been
thought of before sometimes these
questions there's no reason to pose
these questions if you have no ways to
answer them but with a computer
technology we now are beginning to
having ways of of hopefully addressing
these kinds of questions here's a graph
also by John Willie they'll published a
number of years ago earlier which tries
to convey something about the complexity
one of the reasons biomedical research
is so much more difficult and say
something simple like high energy
physics it costs as many more
domains of scale in length and scale and
time so many of the activities they
range from the small activities atomic
activities on the lower end to very
large long-term activities that are
almost geologic in time on the upper
right hand in the in the ovals that are
listed in color there are our areas
where we have begin to make have begun
to make headway in almost all the
headway in those particular areas that
are cited are done by computer models or
simulation capabilities so we go from
the beginning to where you actually do
quantum computing computing of the
individual molecular properties from the
atomic properties to the molecular
dynamics we recently had one of our
investigators at the center was able to
successfully dynamically model the
behavior of an entire virus and and
there are a whole bunch of questions
about viral behavior and pathogenicity
which are dependent upon its its dynamic
behavior and cannot be studied there are
no techniques for studying it so it's
required that we do computation and then
suggest things that might be
experimentally done to confirm the
computations the areas in the boxes are
areas in those space in the time scale
length scale region that try to give you
protein folding occurs in this time
length and on this time and this time
length and this length scale for large
complexes down to small complexes and
those are areas where we haven't really
made much progress at all all of these
areas are areas in this complexity
diagram which require investment in
computation areas many of these
particular our computational biology
some of them are modeling or simulation
so continuing on why we do these things
this is the side bar from an article in
nature which appeared in 2006 the title
of which was it was an issue devoted to
scientific computing and these are the
milestones the major milestones by that
authors the set of authors that produce
this their idea what was the major
milestones and scientific computing
since nine over the last 50 or more
years right 2006 1946 some of you will
recognize berners-lee here as one of the
highlights not surprisingly let me
highlight for you just a few others that
are present in this down in the bottom
we have John kendrew uses computers for
the first time to build the first atomic
model of myoglobin from crystallographic
data this the top one is a particular
pleasing one to me because the the
center with which I'm associated with at
NIH funded this work by child Mulder and
Wesley Clark and also by Jerry Fox later
at Wash U on the development of the link
computer this is in 1962 we supported
the link computer development which was
a as a graduate student actually got to
work on one and it was this massive
bunch of tapes and had a lot of A to D
converters and it was used for in
laboratories neurophysiological debt
debt laboratories for collecting data
and storing an in digital format this by
the way that effort led to the formation
of that wonderful company Digital
Equipment Corporation which doesn't
exist anymore but that was the precursor
to that company and it was all founded
out of biomedical research of course the
computing power shows that for medical
imagery is you see these images all the
time but the amazing thing is is that we
have we are at the point where we can
image you're thinking processes and
that's truly amazing to me I mean I just
it's like moon shots and weather radars
and maybe I would see them all the time
so people get a newer to these things
but I'm still amazed by these things and
this is an amazing thing that we can do
and again that strongly strongly
computer related let's see where the
protein data bank was first established
in 1971 that's the first big general
database this is before the gene bank we
didn't know very much about genes in
1971 this was in nineteen ninety this
work was done largely but well both on
both coasts it was done at NIH and was
also done it you
Santa Cruz that using the development of
program called basic local alignment and
search blast and these are the things
where you try to compare a sequence here
with a sequence from another a protein
sequence from another organism say do
they have the same protein in and well
of course they don't always have the
same protein they are the same set of
genes what they do is is they have they
have small variations so you have to
make allowances for things that have
been inserted or things that have been
removed and look for alignment so this
is kind of a fuzzy alignment and as you
can imagine this is it can be fairly
computer intensive it's also a process
that can be highly paralyzed and so that
the evolution of parallel computers has
significantly advanced our abilities to
work with blast and in fact this is the
precursor of the work that enabled the
shotgun sequencing that I referred to in
metagenomics that is allowed just in the
last few years to discover all these
amazing facts about how many organisms
were such a minority in this world and
in a truth be known number wise there
are you have more more bacteria in you
than you do human cells by number count
and the reason you can i can say that is
is bacteria are quite a bit smaller than
human cells so they don't outweigh you
there are about ten percent of your mass
but but they're more than 10 times the
your number I have one more one more one
that craig Venter develops a shotgun
technique I've already said mentioned
that and I'm going to describe this one
a little more detail because this takes
me into the area this is an investment
that Bret Petersen right here when he
worked at NIH was a major player in
developing this program to try to
provide the initial infrastructure
needed by scientists that would allow
them to both share the tool to integrate
data across different laboratories and
then the provided access to analysis
tools using what we think of today is
web service or wet web services so I
mentioned that one other use of
computers is visualization I just wanted
to give two examples of visualization
these visualization this is a from Chris
Johnson at Utah at a center that we
support there
and it's just giving a variety of
examples of imaging the head or the
human brain and it's interesting each of
these modalities brings out different
features of the data so visualization is
a way of dealing with complex data in a
context of a subject and so there is a
great deal of an effort great deal of
effort involved in finding new and
interesting ways to be able to display
the complex data so that we can begin to
understand what it is we're collecting
I'll say more about complex state in a
minute because I'm going to come to the
data centric thing this is a second well
this is the wall they call this the wall
it's a series of monitors that are
synced together and provide a massive
image these are the bezels between the
individual monitors and it's driven by a
single image but these huge images
besides being glitzy and gee gee whiz
sorts of things allow scientists to take
a global view and recognize systemic
variations in this case they understood
that there were channel across these
this is a blow-up of a slice of a brain
at a level where this is the membrane
and there are the colors are due to
specific binding fluorescent probes
which identify different types of
molecules so these are lipids and this
is glial cells and so forth and they
were able to recognize a regularity in
channels that across this wider rate
that had never been recognized until
they were able to display all the data
because everybody been looking like this
before I never been able to look at
enough resolution over a wide enough
area to be able to obtain the
regularities that exist and for which
there's deeper meaning and we now know
there a deeper meaning so it's as if you
looked at a Google map and you looked at
everything at the lowest level at the
two-block square thing and you had to
look across the country you first of all
take a long time but secondly you never
really get the broad perception of how
far you are from this city or that city
or so forth you never get anywhere
either I can I'm sure
so one of the things that I've been
hinting at and I want to now focus a
little bit on is the emergence of what
we're referring to are lots of people
referring to as a data-centric science
this is from a talk given by Tony hey
but it's several of us have made these
same observation I just lifted the slide
for him so I'm citing him Tony gave a
very nice talk at the global grid forum
this past summer in which he cited a lot
of the data centric issues and of course
since he's now at Microsoft Microsoft's
focus and how they think they can help
the world do these sorts of things he
also had some kind of pointed remarks to
make about Google so you might Google
Tony hey and global grid forum and take
a look at this talk for yourself or I
have a copy of it it's it's publicly
available so we outline the different
kinds of eras of science from an early
stages of experimental science where one
look to see what happened theoretical
science computational science to the
science of today which is heavily
data-centric science and this is a
feature that I think is is patently true
but it's also I think I often refer to
my colleagues at the National Institutes
of Health as being having an Egyptian
problem because they're in a constant
state of denial about this problem they
just do not appreciate the massive
amounts of data that we're generating
nor do they nor to most people even
appreciate how much data is being lost
so that the truth is we generate our
laboratories many of the laboratories
today easily generate a terabyte of
information a date that's a phenomenal
amount of information and the brain
images that we're I was talking about
earlier are a petabyte some of them are
almost a petabyte in information now
that those are very extraordinary cases
mostly they're there on the order of
gigabytes two terabytes to bring in the
brain scan you get when you when you
went to the doctor was more like a
gigabyte but but the highest resolution
research images now are approaching
terabytes and will soon approach
petabytes and we often forget about how
I mean you guys don't forget about how
big information is but most the people i
talk to don't think very hard about a
petabyte but I tell them a petabyte is a
lot of information this is the Jim Gray
data brick story even with these Vance's
in high-speed networks at 10 gigabits
per second it still takes 12 days to
archive a petabyte at 10 gigabits per
second 12 days to archive a petabyte so
we throw petabytes around a lot but but
in truth we have to use the data at its
remote site we're not going to be moving
petabytes or even exabytes around in any
in any of our lifetimes I think no
matter how fast these things develop but
i want to say well i keep forgetting
over here some of you many of you have
seen these kinds of data this is just
one of many different forms of of the G
Wow sorts of things I'm plotting
exabytes on the vertical axis this is
taken from less canned land our they
this is there's a site at Berkeley which
says how much information I think it's
most recently updated about 2003 but
they did some very careful assessment
somewhere around 2000 to somewhere in
here and doing some extrapolations other
people have tried to certify these
things this was actually presented quite
recently and I by a fellow at IBM who
had made some effort time Robert Morris
who had made some efforts to try to to
verify some of the data beyond 2003 but
even if you blur your eyes these data
numbers are enormous so this information
this is a log scale right this is the
years this is what we say as all you can
learn all you could learn in a year all
you could learn in the year maybe I
could learn in a year maybe you can be a
little bit higher but it is a log scale
all human words ever spoken at this
level here all human documents were
talking about ten exabytes now you know
and I know that how that information is
is gathered there are a lot of if these
but it might be off by a factor too or
pi but it's not off by a factor 10 or
more so these are enormous numbers all
dis storage and this gives this is the
number that gives you the exponential
growth of information so as of somewhere
in here we were producing more
information each year than all human
documents prior in all history and well
that's 40 45 years forty thousand years
but that's for all practical purposes
hello recorded history of and beyond so
we know we're producing a lot of
information so the that's a point that
needs to be he said before I talk about
how to deal with this the use of the
network's the communications the
high-speed computing and the data
sources i just wanted to refer back
while i was going through i ran across
this quote from lick litter see my new
look later is JCR Licklider he died 1990
but he was at RP in the i think 40 years
ago probably almost 40 years ago and he
was involved in development of the of
the internet and this is a quote from
him I don't have the precise date but
it's in the 60s sometime I think and
it's amazingly prescient quote I think
as far as how we see today the beauty
and advantage of how I see it anyway the
beauty and advantage of the internet all
the stuff linked together throughout the
world you can use remote computer get
data from a real computer and use lots
of computers in your job that's actually
a quote from him 40 years ago you know
it was a prescient guy so I want to talk
a little bit about the challenges of the
data size one of the things that we're
faced with in biomedical research as I
mentioned is that we're collecting
enormous amounts of data this is just an
example from brain data but it we could
use lots of other areas so these data
from a brain where you take a cellular
level till you get down to the almost of
molecularly this electron microscope
this is optical this is MRI imaging but
if you have a full brain image dat
millimeter resolution in color it's four
and a half megabytes 10 micron which is
still far from molecular is
four-and-a-half terabytes
and if you're operating at micron level
which is still above molecular it's four
and a half petabytes it's a huge amount
of information out of information
remember 12 days to back up at 10
gigabits per second we're also dealing
with enormously complex data and it's
heterogeneous so the challenge to us is
not only to dealing with massive amounts
of data its massive amounts of different
kinds of data so we look to people in
the geosciences and the high-energy
physics community and the astronomy who
have been handling massive amounts of
data for a long time and we look to them
for guidance for how to proceed but in
truth all of the data they have is
trivial it's all the same kind it's on
the same scale it's modestly homogeneous
sometimes it's a little more I was just
up at the high-energy physics group and
Stanford yesterday and they were
describing how much data and the
different kinds of data they're getting
so it's still a difficult problem but
it's nowhere near as mixed as the kinds
of data that we're getting and these are
just some examples of data that one of
the types of data metabolomic data that
is how we break down chemicals in our
cells protein interactions regulatory by
owner work alignment disease motifs
patent protein classifications these are
all data that are being collected and
generated in laboratories across the
country and for which we at NIH are
paying for a lot of it we spend billions
tens of billions of dollars collecting
this data you know what first of all
most of this data will never ever be
seen by human there's too much of it
nobody can look at this data so and
secondly most of the data will never be
seen by anybody or any machine so it's
wasted it's deflected and gone so we
have a real challenge to try to take
this information begin to make use of it
and you in ways other than the narrow
uses for which it was originally
collected if we want to share data
collect data on genomic data or
distribution and one animal and see how
it applies to another animal or to
another human we need to be able to
preserve that data we need to be able to
check the data and if as I said
of the data we are collecting most of
the data well for all practical purposes
none of the data will ever be seen by
human if you don't see that data just
think all the problems that presents
you're going to have to figure out how
to check whether the data is worthwhile
whether it's the data you really want is
it is it left-handed data and then it
meant to be right-handed data you have
to have machine algorithms which can
anticipate all those kinds of questions
and certify that the data is what you're
looking for and it's a quality that you
need those are enormous problems and
those are the kinds of problems that
we're beginning to beginning but have
been and we're increasing our investment
to try to manage and figure out global
ways to be able to respond to those
needs there are also barriers to the end
to efficient data use the nominal HR and
coordination everybody calls something
differently so these are this is the
issue of developing ontology in the in
the biomedical arena we have an enormous
problem having to do with privacy we
have to have consent of the patients if
it's a patient's data for that dinner to
be used for the purpose that it's which
it's used so there's a long trail of
tracking about consenting data there's
the best practices and culture gaps
culture gaps is a big one and one that
we informatics I'm going to solve but
it's a huge problem having to do with
people we always make the joke that we
say that many biomedical scientists
think of data mining as data mine now
not to be shared because they want to
make advantage of it before anybody else
does so I'm going to describe for you
briefly in four slides here and then I'm
going to kind of end up what are an
effort that Brett and another mostly
Brett put together to try to begin to
handle how do we share data and how do
we how do we collaborate and deal with
in the infrastructure necessary to deal
with these monstrous data so we put
together something called the biomedical
informatics research Network and we
intentionally use the term biomedical
informatics because included in this
network would be resources that allowed
for storage of data
resources that permitted computation on
those data assembling computers
supercomputers accessing clusters that
were available on the network and
allowing them in a transparent way to be
used by individuals who would put
together pipelines of data say take data
from a b c and d merge that data bring
it together perform these analysis which
might include anonymization of the data
might include normalization of the data
might include referencing it to atlas
and then would include some computation
on that data to determine whether or not
there was a difference in size between
in the pituitary gland of individuals
that are reflected by Alzheimer's for
example that might be something would do
and there's appropriate infrastructure
software infrastructure that involves
various levels of middleware on top of
hardware and networking now the
interesting thing to me is is that we've
invested in all of these things we paid
for the hardware we've supported the
networking connections gotten by the
firewalls and all the institutions of
pain in the butt put together middleware
to help people to access the different
computers and a transparent way to sign
on build in authentication authorization
activities accounting activities all of
those things that who's used what data
when and then on top of that we finally
get to the scientific and applications
pipelines which allow us to do things
interesting with the data to the
scientists figure out whether the data
is the same ask questions about whether
something that occurs in one animal also
occurs in humans whether this protein is
similar to this protein whether this set
of genes is contributing to this feat of
this particular disease type those kinds
of questions are all at the top of this
I'm thinking and many of us have been
thinking that most of this part of this
grid of this this pyramid here is going
to be we've often wondered how can we
continue to support this so our critical
feature has always been we want this
whatever we invest to both scale and
to be extensible to other systems than
it was built to address this is critical
scalability is not the kind of thing
your average biomedical researcher
thinks about fact most biomedical
researchers can't tell you what
scalability means so we thought a lot
about how we do to scale this and we
hadn't come to decent solutions except
for the oh then when is it the deus ex
machina and then a miracle happens yeah
yeah we thought maybe things would take
care of it well you know something a
miracle has happened in the last several
years we've seen the emergence of cloud
computing here we've seen it at you
amazon we've seen it at we've received
we've begun to see it at Microsoft even
amazing and we and we've seen in that
Yahoo that kind of computing the support
of resources and computational resources
which will permit large virtual
organizations and what we're doing here
is assembling virtual organizations that
have a common interest in data and
research and analysis tools and a
computational need to apply those tools
we're assembling that group and we're
assembling an over an infrastructure
that we've been providing and hard
concrete terms but there's no reason
that virtual organization shouldn't be
supported on a virtual set of resources
so it's I believe that the emergence of
many of our problems and the address of
many of our problems we're going to see
very active pursuit of those virtual or
utility computing or cloud computing
type of activities wit and our
investment is going to shift to focus on
those areas that have to do with the
knowledge management the the data mining
the integration tools that are that are
thorny issues but issues that can be
done and we what we hope that places
like Google are going to be able to to
lead the way and provide that kind of
activity and entree for these data so
let me close with that I let me close
with this this is just to give you an
idea that's a reality check this is a
group some of you may know about tech
cast org
this is an organization that makes its
living making predictions and and
selling subscriptions to these reports
that they're right but they provide a
little bit online for free and I clip
these from their web pages to for their
free parts of their web pages and it
gives you a little bit of idea that they
plot the most likely to enter the
mainstream against the experts estimate
of the the likelihood that it'll occur
and the grid computing you can see up
here is in the 2010 frame frame a little
over two thousand ten with an eighty
percent certainty and I would that
sounds about right I mean we can do grid
computing now but but in terms of the
mainstream they're talking about
entering the mainstream we're talking
about three years from now two years
from now I think that's about right so
we're we're already behind the power
curve and getting the tools to make use
of that grid computing it's also
interesting some of these other things
that you look over and they they had one
that just had to do with medical
research and they call telemedicine not
what I call telemedicine but they call
telemedicine the access to health
records and in prescription records and
things like that and they think it's not
even going to be close to available
until 2015 and there's a lot of people
working on that and it's a very big
monetary market the other thing they do
is give you the sizes of markets for
these things in their analyses and it's
quite quite amazing so that I think I'll
close and i'll skip going through the
details of the and if there's any
questions I'll be happy to
sure
architecture
that
huh
so what are there is that we're
investing is that you don't you can
repeat for either any other repeat the
question is are there any architectures
that would address these problems better
than the existing architectures so i
want to say fsm jeez is that the fpga
right fpf SMGs fpgas having a morning
east coast thing so there are two or
three groups working on fpgas in an
attempt to address specific
computational problems and there are
specific data storage structures that
are being proposed and developed
commercially to try to enhance the
ability to select data and do local
computations on as part of the database
structure but they're not widespread I
believe they're very early in the
development from what I know of it do
you mmm oh yeah we have one investigator
Klaus Scholten at the University
Illinois he's also doing FPGAs by the
way it's part of his project but he
recently I wish I could remember those
numbers he he's been working with NVIDIA
and taking some of their processors to
do molecular dynamics calculation so
these are highly paralyzed abul kalam
that look at how molecules basically
vibrate under potentials defined
potentials and for very large molecules
light and he was the one that did the
virus a very large collections of atoms
and he's been using he has worked with
NVIDIA to get there to teach a course
had gotten executives from their
scientific end of the nvidia to teach a
course of illinois with them fly out
each week to teach a course about how to
program these things because the
programming is largely undocumented and
he he's published he just published some
of this stuff where he's done for NVIDIA
processors and gets a couple of
teraflops as I remember from four
processors which is pretty amazing I
mean because the cost is the total cost
of the things is a couple thousand
dollars
so and those are dedicated to doing
molecular highly paralyzed about
molecular dynamics calculations so it's
conceivable that that isn't a new
computer structure it just happens to be
a very high speed programmable processor
designed by the gaming industry right do
you know how many no not your feel good
any other questions nope they give
grants to patterns absolutely
mean like Google so here's a guy working
at Google who worked at NIH but yeah
people make application there you have
to write applications we have small
business applications people who Google
when be eligible for that but people can
write applications from research
applications and secure funding for
specific projects I think it'd be rare I
know can't ever remember getting one
from all right and so and there's also
regular issues of rest request request
for proposals which are then awarded or
requests wrap request for proposals we
distinguish being pressed for proposal
request for applications request for
proposals mean where we're interested in
awarding a contract and we award
contracts when we have a product in mind
so we can specify what the deliverables
are we award grants then we have request
for applications with our grants when we
don't know what we're doing you know we
get we can't specify what the product is
so and formerly a grant is for the
benefit of mankind and the contractors
for the benefit of NIH right that's the
formal distinction between the two thing
else well thanks for your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>