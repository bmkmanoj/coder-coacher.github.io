<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Genome Question: Moore vs. Jevons with Bud Mishra | Coder Coacher - Coaching Coders</title><meta content="The Genome Question: Moore vs. Jevons with Bud Mishra - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Genome Question: Moore vs. Jevons with Bud Mishra</b></h2><h5 class="post__date">2012-04-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VWePa16iX20" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to introduce mark Donner one of
our engineering directors here enjoy
thank you very much welcome also I'm
buds warm-up act so welcome to google
I'm going to take a couple minutes and
tell you a little bit about the New York
office and some of the opportunities we
offer here if I can make it change
slides okay this is an interesting
building it's the second largest office
building in Manhattan it's about 2.9
million square feet was originally built
as a warehouse by the Port Authority
early in the last century I was derelict
for a long time was bought by some
developers who rehabilitated it and it's
also actually one of the major peering
points for the internet for the
Northeast so your packets have probably
been here before there are four large
freight elevators each of them big
enough to hold an 18-wheeler and I've
seen it done that go all the way from
from ground floor to to the 16th floor
though actually two of them have been
disabled one of them was turned into a
set of fire stairs the other one is
somehow closed off so they're about
3,000 google employees here about half
of them engineers and half of the sales
people of business people of various
sorts so we're big we're the largest
engineering center outside of Mountain
View for google we as you can see and
you've heard before we focus on quality
of life and food as you can tell I
definitely partake this this is actually
one of our cafes during the daytime and
we do silly things and have fun we have
some fairly well-known people and let's
see if we have various
as people visit here because we're we're
fun we are in fact hiring let me tell
you a little bit about today we'll have
the talk at at 635 and Q&amp;amp;A after that
i'm not going to read buds abstract i'm
sure you you saw it posted on the on the
meetup website I've known bud since we
were grad students together many many
years ago at least five thank you thank
you pick up your nickel afterwards but
but is the professor at quran institute
at nyu and we he and i both started out
originally as electrical engineers
hardware people with some math
background we became computer scientists
but went on into biology and has done
some remarkable and entertaining things
here since then he also has the
distinction of naming his two children
Tom and Sam in such a way that they're
their initials are also their short
names but
they hear me i also have a daughter
Kimberly in the Romney struck camp so
since it's being videotaped I don't want
that so I'm going to go to my talk
Oh
so I'm going to tell you a few of the
things that I had been thinking about
for the last 12 years it's mostly in
biology and sort of walked into biology
totally accidentally but I seem to have
gotten trapped sort of my nepali danica
russia have cut me in but can't get out
and i want to say you thinks about why
it has remained so interesting so those
of you who can recognize him great but
those were you turn don't know him his
name is William Stanley Jevons is one of
the earliest computer scientists that I
know he actually built this computer so
this is a machine that he called the
logic piano and the idea was to take
logical formulas involving variables a b
c and d so four variables and negations
and also has an or operation which he
called a conjunction i think the idea
that all it should be called a
disjunction is a more recent logical
investment and of course being a good
Englishman he also has the punctuation
had a full stop their period is actually
better known as The Economist who
invented the marginal theory of utility
but for us to Dominic computer scientist
a statistician logician and an economist
he also is remembered for a paradox
that's known a jetta one's paradox and
first appeared in his book the coal
question in 1865 this is when he was
about 29 x 25 he had already written
this marginal fury of utility after
James what introduced his more efficient
coal-fired steam engine somewhat
counter-intuitively England's
consumption of coal skyrocketed not with
Fanning the higher Ephesus so this was
various
rising and he gave an explanation for
this and this paradoxical effect has
been called a backfire or any bar so so
this is sort of the picture that New
Yorker had to describe this efficiency
development so more efficient we make
things the harder it gets to achieve the
goals and this idea of rebound comes up
in many places one place is let's so
much in computer science but in genomics
and biological sciences and which has
followed their own worse enough rules
law so this is garden more and offers
all of us know about Moore's law and in
I think 1965 or so he looked at number
of components for integrated functions
and plotted them and allowed love but
the lawn over time flood and inclusion
that the number of components will keep
doubling every 18 months or so and
interestingly enough that train has
remained similarly we have also talked
about a Moore's law in biotechnology so
if you look at the curve so one is the
cost for mega base of DNA sequences the
cost has kept going down but
interestingly enough around October of
2007 it seems to have taken over a much
faster more slow so unlike Computer
Sciences once law we have a doubling
effect every five bucks so that also
reflects in the cost for a whole genome
so not surprisingly we hear about being
able to sequence a genome for about
thousand dollars in knots on distant
future so what does that going to look
like so even if we look at what's
available now we can project it forward
and because there are lots of talks
about what's going to happen in few
years
about six months ago you could get 200
gigabytes pairs in about eight days on
one of these alumina Isaac machines so
that means that i can actually sequence
your genome 50 times over in about eight
days of course eight days is still a big
problem because we can get it down to
fuel it two days like five days or four
days one would be able to accomplish the
XPrize we haven't done the XPrize here
there is an X Prize in genomics which
requires you to sequence hundred people
in 10 days costing no more than about
ten thousand dollars per person so we
are inching towards that but yet we
haven't yet produced error-free applet
epic genome assembly algorithm so to get
you oriented your genome is about three
billion base pairs if you are not
counting the haplotypes that means you
actually have two copies of all your
autosomes and X X chromosome and a
y-chromosome to sex chromosomes
currently if I look at the two the pair
of autosomes the cumbersome 1 through 22
then sorry 21 then those two copies are
very similar to each other the current
method is not able to distinguish them
so the sequences we have generated the
whole genome sequence we have is a
general typic signals so one of the
questions is how hard is it to get to
upload epic sequence so to be able to
tell your father from your mother we
can't do to that now well there are some
arguments that if you spend a lot of
money with lot of work we could do it
but there are algorithms reasons and
other technological reasons why it has
been very difficult the other side of
the question is do we need it
maybe genotypic sequences are good
enough so we're going to look at that
and the other where the related question
is if this moles law keeps going forward
at this rate will it be possible to do
hyper typic sequence assembly so here is
the sort of the picture we have some
sort of reference sequence so you can
think of craig Venter Jim Watson or
Bishop tutu a positive sign their fuel
such reference dinner for few humans
what we would like to do is to read your
dinner and somehow align them to this
genome so that we have location
information and look at some base fare
that differs in the population so these
will be the single nucleotide
polymorphism and if we know subtle
threat for the people that are closely
associated with you polymorphisms then
we could say that these are associative
except disease or threats and look for
clear by genes that each I quit them
that for segregated so that way we could
look for jeans or markers that could
explain various diseases like
unfortunately we haven't been very
successful at doing that so there was an
article in Scientific American with the
title revolution first one which is
great because phenom assist you can go
on vacation until the revolution
restarts but the question has been also
why we have this situation so we can
produce large amount of genomic data
very cheaply and the cost is going down
but yet we are not able to understand or
explain this state so in fact we can
actually create lot of this kind of
information short rates over 100 base
pairs that could be aligned to the
reference sequences we have but yet
that's not good enough explain why we
haven't been able to use that data to
explain complex diseases so there have
been lots of different explanations for
that but my explanation is that we have
to worry about things that we don't have
in the genomes things that we can't see
in the so these are the genomic dark
matter dark matter most of the reference
sequences are generally because I
explained we somehow combine the genome
that we get from your father and the
mother into some uncensor sequence that
may not be anything related to either
the father of the mother and we also
like long-range information so your
genome could be actually rearrange the
pieces of genomes that could have worked
they could be duplicated there could be
inversions and the equivalence locations
so one piece has moved from one
individual to another we can't see them
in this picture about if I really a
genome I can't actually place about
thirty percent of the reeds anywhere in
the genome I don't know where to place
them given that i have only for
reference sequences okay if you do
sometimes use a different technology
where you can read very long pieces but
with low resolution so you find some
physical markers one such technology is
a as an optical mapping technology that
i have been involved with then you can
see that there are lots of structural
variants the lord of arrangement that
could not be inferred from the non
reference sequences or the short reads
we get and when we do genome-wide
Association studies based on the genomic
data that we can get there has been
inadequate and explaining commodity so
that's my Germans Giovanni and backfire
but you know is we can continue to
produce more and more data short reach
the lengths of the length of this vide
become shorter they have had more errors
but we have increased the throughput and
reduce the cost and we're done this
better than anything in computer science
but yet we have all this data that we
have to move from the sequencing machine
through some data processing into a
cloud and do all the things we can do
but come up with nothing this is clear
okay and sorry we're drowning and data
deluge so so that's the problem that I'm
going to get your attention on so down
here you have sequences the classical
sequencing technology was due to sanger
but there are many more new ideas that
have come up so this is a technique
based on pyrosequencing and the cheapest
one is based on a simple ph meter
because as i incorporate each base of
threw out some protons some ions and if
i measure that and by associating with
the cycles and this change in the ph i
can actually L will face am elated and I
can do that in massive amount in fact I
can do it on a transistor and can build
a digital device but each transistor
will tell me what base are they so I can
take the classical computer science
hardware technology marry that with
genomics technology and can get this
biotechnology Moore's law so i can
create massive amount of data once i
have that so these are going to analog
data because all i'm doing is measuring
ph or from bring pyrosequencing
measuring some photonic data that's
analog data so imagine that the data
coming out in four channels one for a
141 for c and 1 4g in this analog data
has to be converted into a digital
signal so i have to call them to a TCG
and that's what the base calling will do
and once I have done this base call I'm
going to take them over a network and
into a cloud and I'm going to find the
position where it came from and once i
align them i should be able to see small
mutations may be one base has changed
single single nucleotide mutation or
maybe there is short insertion or
deletion and once i have that I'll be
able to tell what kind of locations you
have what kind of polymorphisms you have
and then from that I could be able to
predict your trade you personalized
medicine find the population you belong
to population stratification so forth
and that's called resequencing but often
I may take those short sequence reads
and combine them to piece together your
genome and I would like to do that hyper
typical right so and there's lot of
stuff I can do many of them are
interesting algorithmic questions or
machine learning questions that will
happen in high-level sequence analysis
but right now we have improved the
technology here we can get a huge amount
of data and in fact after I baseball you
probably will put in a hard drive and
FedEx it to the pub that's probably the
technology we will be using for one so
we are finding it difficult to cross
this boundary is that going to get
better so I have to store this data I
have to move it over a network well the
trouble is that the storage disk density
is doug is doubling annually and that
trend is going to continue I'm going to
double this data that you know make data
every five months and in this problem is
going to get worse for me every five
months
unless somebody comes up with a better
network or better storage system so
that's the first issue so here is a
simple solution so we have sequencing
machine we have base calling so we get
100 base rates of 80 cds you a massive
amount of them and we're moving into the
cloud where it will get alive what
happens if i take the reference or
whatever analysis system i have and move
that analysis to the signal generation
in particular i'm going to take my
Bayesian models and the Bayesian priors
put it directly into the sequence
ignition so the idea would be as I base
call after I've read the first I'll
basis when I go to the I plus first base
if I know where that first I basis girl
then I have a prior about what the next
place is likely to be because you don't
defer very much from Craig rendering
most of the places okay and here i have
an analog signal if i combine that I can
improve my base call it moreover since i
have an alignment only thing i have the
same is the own differences in fact i
may not even send those if I found a
mutation in bracket one I can call it
and say that you have this much less
than there's no so in fact what I have
to send is very little so most of the
analysis instead of putting the analysis
in the cloud I want to move the entire
analysis all the Friars to the
sequencing mission doesn't make sense I
could instead of keeping just one
reference sequence I could keep up a
10,000 upper typically sample sequences
there they could tell which population
you come from and within that population
what mutations you have and how likely
that mutation is going I can also tell
if you have a de novo mutation
that means you have a mutation does not
ever seen in a human population de novo
mutations have gotten a lot of in
interest right now because we think
first of all we think there are
unusually large number of de novo
mutations in humans compared to other
primates and also sorry that's too but
the arguments have been that the one of
the arguments are in file as there are
many theories so one of the arguments
has been that humans have a lot of p73
mutations so p73 is a gene that does
embryonic surveillance so that means
when you are conceived there is a system
that checks okay you're not the
embryonic lethality process will
eliminate you fairly quickly what people
notice that there is a gene called mdm2
that seemed to have a mutation and has
been positively selected mdm2
unfortunately causes cancer so why would
a cancer-causing mutation we positively
selected because of mdm2 controls p53
p53 does somatic surveillance that means
it actually watches your cells in your
body and if there are errors p53
initiates and a publicist so by reducing
the effect of p53 you have actually
gotten rid of an effective somatic
surveillance but the same thing can also
reduce embryonic surveillance and give
you higher fertility so you could trade
off cancer for higher fertility so it's
possible that we did that trainer and we
are tolerating more mutations than other
species
so that seems to be a plausible
explanation but we do know that we have
more de novo mutations p53 is a tumor
suppressor his question is p53 p63 p73
are these dominant or recessive why is
the tumor suppressor gene p53 so what
that means you have to knock out both
you need two hits to stop it so p73 is
similar so as long as you have one in
Turkey Oh fine so anyway there's a
digression but the idea is to move all
that analysis directly to the sequence
formation so your sequencing machine is
no longer going to produce a CT geez but
just the location of the mutations and
some sort of confidence of how much the
more likely that the mutation is correct
so how would we do such a thing so
remember i'm looking at a bunch of
analog signals coming out of sequence
efficient so yeah say series of n such
whip bits or bases that are coming out
and what I would like to do is to
translate into a sequence of a TCG in
fact I'm going to extend that alphabet
to a TCG plus an insertion or deletion
so it is a six letter alphabet so I have
six to go n possible matches and I'm
suggesting that we look at all 6 24 n so
the way i will do is initially i could
hypothesize the first base is a TCG and
the second one is similarly for each one
a TCG so i have a huge tree and each one
will be scored by some some scoring
function it will be some sort of
likelihood that given the
signal I see and given the alignments I
have what is the likelihood that
particular base is correct a particular
path in space correct so I'm going to
try to fish out that one path out of
this exponential in many possible ones
right so in the beginning in the early
stage if my error rates are low of more
or less rely on the analog signal but as
I go forward since my sequences will
have very few alignment the underlying
genome the underlying prior will get
more weight so that's all I need to do
so i'm going to do a branch-and-bound
something like a beam search and keep
expanding this tree and pruning it the
advantage the interesting objective is
that given certain error rates and given
how the structures of the genomes very
most likely finding the finding this
path will be relatively easy most of the
times the probability is very high
probability my branching factor will be
something I want plus epsilon we're very
small at soon so so that's what I'm
going to do so the first thing is to
turn it into your base and formula so
given the analog signal X sub K on the
KH cycle I want to find the probability
that the underlying basis B and I write
down the base formula and what I have is
the conditional probability that given B
what is the probability that i will see
the intensive execute it depends on dr.
like the ground and from the gina and
know what is the conditional probability
that appear base is actually be and also
you can similarly cover the conditional
probabilities if it is not the base p
but the other one so if I'm looking for
a not b is TC orgy and if i turn that
into a log i cleared and do some
simplification so I'm cut one term that
corresponds with
Karen service but I get from the
sequencing machine and the other term
that looks with a line so what I have to
do is to calculate this alignments very
very fast all right so as I read each
base had to calculate for possible
alignments and i will tell me what the
next phase is likely so i need to count
the next phase very accurate the first
part is easy yes they do have a basin
right I have a base and bias right so so
what I have to do is to be very careful
in how I introduce the bias so what I'm
going to do is to have a waiting car and
I'm going to weigh it in terms of what
errors I'm going to get in the
technology in each cycle and of course
I'm going to read the same genome more
than once so so I'll get certain
confidence and I have to combine them in
the sequencing machine to improve the
data so if i write write write write so
if I have more references i can reduce
that bias right right right right right
that will do the same thing i will show
you examples of that internal rotation
so you didn't know if I assemble so the
first thing we do is do a linear
transformation to get rid of certain
cycle by cycle errors there are some
systematic errors also there are cross
thoughts so each channel atcg will
affect each other so we do a linear
transformation and we model the
intensity as a the conditional
probability of the intensity
corresponding to base and if you do that
the raw data will look like this it was
a linear transformation you get fairly
clean
data set so for example when looking for
a my intensities will be distributed
like the blue thing at different cycles
and anything does not a forbidden so up
to about 50 or 60 cycles I can very
easily distinguish them but as I go
forward in cycles they will get noisier
and noise and the idea is that I will
trust my intensity in the beginning and
don't want it to be biased by my base
and fires but as I go forward I may
increase the bias so we'll create a
weighting function that actually adapts
to the technology the second thing is
that I want to do the alignment and call
the next phase rapidly and for that
we'll use a suffix clear but actually
compress if you can photos willard
plasma and by this calculating origin
imagining in business I could count the
next phase play rapidly the debate the
illnesses will tell me exactly what the
next phases are and that's all I need to
do ok so I got both the information
fairly easily you know there's a little
bit of work because I have an all model
and I have to call a p-value because
that's what I'll do to combine the data
I could do this in batch and combine all
the reads I've gotten so far to create a
new prior and overwrite the other primes
i have but i think a better approach is
to subsample the human genome about
10,000 times take 10,000 individuals and
human genomes we could do a capture
recapture analysis for european
population it seems about four thousand
individuals are sufficient for the
entire agreement is about technology
that means you're fairly close what are
the people come in my 10,000 samples so
you have already a cause in our second
cousin in a fairly small setup so by
using ideas like that you can
speed up these things while controlling
the base advice so one of the reasons is
that humans went through a population
bottleneck about 60,000 years ago so in
spite of all the notifications and all
the things we talked about the Lord of
similarities at short distances that
there could be a lot of a rearrangements
and a copy number changes that will
short distances this could help so here
is the server the comparison between
what this algorithm does and the best in
the market that other people are
developed not surprisingly as I increase
the veins in the prior is coming from
the reference I do better a lot better
so in fact these are different weighting
schemes so clearly by waiting more and
more to excel friends on most of the
bases I'll do correct I'll call them
correct but that's not a good approach
because of my base and bias I could be
missing your mutation so one of the
things to look for is what is the
probability of false positive that i
will call a base and mutation while it's
not a mutation so in fact this actually
helps false positives can be reduced
just by weighing it further and further
but if you use your reference with
higher way that we increase the false
negative so the sensitivity will drop as
you include the way so bye-bye trading
up between the weights and the false
positives and false negatives you can
turn it to finding new mutations rapidly
and correct so the idea is to
so so so that will depend on the
coverage and that will depend on the
false positives and false negatives I
want to throw a right support it'll
depend on the technology so for aluminum
essence it's about 10 or 20 x cover it
sufficient because you are going to
still hit the most of the correct
mutations in the first 60 basis remember
it my false negatives don't drop up
until about 60 days to person so over
that range I can actually call them
quite well so all the other ones are
just adding to that so I'm not going to
call false positives very often but i'll
miss some we're now compensate for that
to the coverage so so one way to do this
is to start sequencing in a clinic like
clinical application and start calling
up to a confidence level so some of the
mutations will not show up until a 2
hours 3 hours most of the patients will
show up in about an hour so the longer
you call then the better confidence you
have on the mutations so so the trick is
to tune your confidence level and how
quickly want to call it the other
interesting thing is that all of this
can be implemented in hardware fpga and
even if you have millions of lens
falling simultaneously you could do this
in real time one of the problems would
be then you'll be doing a lot of context
switching so you have to take the states
and store them and restore them so you
think about it but that's pretty much
the only from that this hardware will
face so you need locally some sort of
way of storing and reach two
States rapidly but the idea is that
instead of doing this the classical
technique where all of this data is
going to go to the cloud and then get
called you want to turn your sequencing
machine into a clinical mission okay so
that seems like you get did a
compression you get accuracy you can
stop the data deluge is right at the
source but I think this idea is much
more general Astro number you could
imagine putting a base and priors about
your exoplanets right in your synthetic
aperture riddance so you don't have to
move all the data back and forth but
right so they should be able to call
directly when they see one as long as
they have a bottle of the transit times
bubble up but they're looking for same
thing in genomics a lot of the things
can be moved directly to the device
instead of being in the club okay
sorry what do I this data reduction so
right now the scale is not that hard so
we're talking about embedded system for
the body we believe right now it's not a
difficult one because there are about
million lens and actually each cycle is
about 30 minutes because they have to
wash and do all sorts of things
chemistry but we believe that both all
those both those numbers will scale up
so we expect the cycle time to get down
to about five minutes physically right
the question is in terms of do I want to
do it in real time or not so right now
we have an implementation with one
single epic but I can speed that up
right now it's not a problem because
cycle time is half an hour and I can get
real-time response in that cycle time
but as that improves no no no I'm not
talking about that I'm talking about
right no no I'm not talking about that
right something right some things of
that size that goes into a sequencing
mission right right this much we're
right right right right okay okay we're
not talking about something very
cumbersome okay so what would we what
should the future look like but in order
to get around the falls we would like to
have her typical sequence of a 10,000 so
it should be accurate to characterize
all the polymorphisms such as slips in
doubts we are
we want to characterize genomics of
human populations so blending know all
the mutations duplications gene
conversion so gene conversion is the
process in which one allele and forth
other a lil to become the same
recombinations and migration but
important population publix can have
happened in humans in flying
subpopulations we understand effects of
positive selections and gene sweeps and
also negative selections and rare
variants we like normal algorithms to
stochastic model population structures
so population structures means that our
populations are stratified into many
pieces of population wishes we like to
understand that process to give you an
idea is not we don't have a panic
populations not any two random
individuals will mate in fact one out of
eight males is a direct descendant of
Genghis Khan or his grandfather we don't
have a good model of all of these we
would like to do Association studies and
understand origin and progression
diseases and go for individualized
medicines and another thing that sort of
very crucial is to penalize when
somebody says their child is autistic
what does it mean and the Society of
psychologists change the definition of
autism who will that change anything
same kinds of problems on a DD or any of
the other amino types we talked about
and then the goal here is to understand
causal genes and that requires us to
understand better what we mean by
causality and without that the air inter
process could be highly flawed
ultimately to do this yes which is
I think so I think you can I mean the
and the technology is not that difficult
I think the moore's law should continue
would like to go for a thousand rupees
Gina which is about twenty three dollars
for six billion a pathetic basis only
135 billion u.s. dollars for the entire
human population less than amount of
money you need to bail out Greece so I
think this was it I also like the idea
of Freeman Dyson that we should aim for
a lab laptop genome sequencing so he
says I'm proposing now to hijack most
prediction and applied the wild the
sequencing machines that now exist on
Marvel opportunity but they are
cumbersome and expensive but biology now
needs is a single molecules offensive
but can handle one molecule at a time
and sequence it by physical rather than
chemical which is I find very
interesting a single molecule mission
could be much cheaper as well as faster
than existing machines it might be as
small and convenient as a laptop
computer so what we would like to get is
a laptop sequencing so what should the
moles look like it should be
miniaturized can get smaller what that
means it should be able to work with one
single bien Emily one at a time good
work with single cells that's important
because if I am working in sac answer we
know that tumors are highly
heterogeneous each cell is different you
would like to be able to take extra DNA
from every single cell it should be nano
scale so to be able to actually query or
work with any single gannett and should
at the work in conversation okay so
that's what today for bears right so we
can't approach that speed yet we should
be able to work with my only remind
amount of materials what that means is
we should avoid chemically changing your
DNA often that happens if I do PCR or
something fun
any amplification can introduce errors
would like it to be non invince by query
again it I got destroyed they should be
able to come back and eat again I should
be asking for us doesn't have to be done
well done and another idea i think is
important for Moore's Law is that it
should be absolutely I should be able to
abstract so in fact geneticists should
not have to understand kinds of errors
that happen to the chemistry in the
sequencers right now we do have to wear
your fur so should be modular i would
like to also emphasize that we should we
have mostly computers all then physical
and then commit the main reason is that
the computer's not musleh is still going
to dominate and i want to hitchhike from
the computational russula in physical
sensing it's relatively easy i can go
from optical systems to atomic force
microscopy to Raman scattering and so
forth and I want to keep the chemistry
then because usually they're hard to
speed up I would like to see arrow
resilience so you have to think about
how to build reliable technology out of
unreliable parts so instead of trying to
eliminate errors that the very basic
level like to allow that but still make
the technology reliable and one of the
interesting questions asked is there are
0 1 block this probabilistic analysis we
could use that we do except that means
in my technology there are parameters
that I can so tune that it will generate
only easy instances of hard rock I want
to tune the technology so that it does
that through the high probability i can
create the correct solution this is
these are all the things that we have
used in computer science and benefited
biotechnology hasn't really done that so
so here is a technology from optical
mapping where the single-molecule low
resolution maps and these are
each modest one is a single molecule and
these are about 300 kilo bases you can't
read single base at a time but can read
certain physical markers on that and
that can be combined to get very high
resolution feature of container and that
can tell you rearrangements haplotypes
so and so forth but it can't tell you
single base position you want to combine
these two you actually get you to Upper
type sequencing so those of you who
don't know anything about sequencing the
problem is relatively step further
explain you take a your DNA and break it
into small pieces so this is cos shotgun
sequencing and these short pieces can be
read so imagine this short pieces about
thousand base pairs and sometimes I can
read that poem length or it can be a bit
longer five kilo bases and I'll read few
hundred basis for four seconds so the
data I get are some 100 base 200 phase
or seven other page reads and sometimes
they come in fair and I know the
distance be okay that's your data and
what you have to do is to there is no
location in photos so when I do this
reading I don't have any information
where they come from all I know are the
rates and what I want to do is to
somehow combine them to reconstruct
that's the arm and the way we solve it
is by using a greedy algorithm I take
pairs of reeds I overlap them and score
them in the amount of overlaps I have
between a pair greedily I choose the
first most overlapping pair i combine
them replace those to buy the new one
and continue doing them okay deal is not
a good algorithm it's highly suboptimal
register our best idea
this is what Craig mentor claimed to
have done but actually they do make
bears they did he use my pears so it's
not a few shotgun right so but but the
algorithm this IR is idea goes into most
of assembly algorithms the few are the
good ideas so one is to turn into a
powder one is to create is overlap
layouts into a graph and look for a
Hamiltonian path of it another idea is
so realize it the nodes are so you take
every gamers words of length K and
structured so that you have 2 kms
combined by edges there are the prefix
and suffix common prefix and suffix of K
minus 1 set and you look for an Euler n
path it seems like an easier problem but
then if you have a look then you don't
have a unique solution and also if there
are errors it is also an entity a
problem so the main problem here has
been that somewhere there's an
np-complete problem larkin and greedy
algorithm if the deal if your genome was
completely random the greedy algorithm
will to fight one and so with this
because the graph that we can rectify
this would be a chordal graph so that
could have a linear time now they're
finding habitats are Hamiltonian paths
and also this problem the problem is
that DNA is not perfectly random it's
not random sickness or so Facebook okay
so these are those sets of algorithms
that people have developed and I have
categorized them in terms of overlap
layout greedy and sequencing by
hybridization and there are some other
3d style algorithms by seeding and
extending one of the interesting things
to notice is that almost all all overlap
layout consensus algorithms which was
used by solera stops around two thousand
eight
and all the sequencing by hybridizes and
algorithms start around 2000 so what
happened then is that we went from
sanger sequencing to next-generation
pyrosequencing or other approaches and
the read length got shorter and we got
higher overlap and that required us to
change the set of underlying algorithms
so it's sort of like you know if i
change the architecture of a computer
you have to throw it all the burning
systems all your languages and all your
forefathers at least so why is it that
we don't have algorithms than a
technologically advanced given that is a
whole efforts but why didn't we have a
common set of eight so so one of the
things the basic idea is that we are
going to take two reads and B and look
for overlaps so one of the things to
worry about is that sometimes I'll read
from high prime to three prime sometimes
I'll read from sleep i'm from pipe
sometimes i'm reading your watson sprint
sometimes i'm beating your freak
strength and i have to be careful in
doing the overlap correctly so this
could be normal or any but given a set
of overlap I can always tell if it is a
feasible set that means it's an
admissible layer satisfies some
properties and that's easy to check but
not all admissible layouts will generate
a correct sm so we need to put some
constraint and one of the constraints is
to optimize the cumulative overlap which
translates roughly to finding the
shortest commands of us given lots of
strings I want you to find the shortest
super strength from which all the
strings could have been derived with the
error processes and that problem is
np-complete
but this is not a correct formulation
because if I have repeats it says I can
always do better by compressing the
decrees overlapping all the repeats in
one question repeats exist so we have a
problem that formulation is not correct
but but we worried that this is
np-complete so we need we have two
problems one is to solve the complexity
issues and also structural it should be
correct right so we want to find the
repeats correctly because otherwise all
the rearrangements for the inversions
translocations Oh victim right it is
important so some of the problems we
have is because of the repeat there's
also another issue that if I ha thought
if we can be greeting that means your
father and mother more or less similar
and there's a region where they deferred
I may not be able to tell that these are
a habit of ambiguity for a repeat ok so
sometimes I haplotype big differences
could get encoded as it have to be
careful before right now these
algorithms are not modeling that correct
but let's stick say with NP completeness
so the basic idea is that for the
computer scientist I shouldn't be going
through this so if I think of its
sequence reads as downs and overlaps as
nodes then the shortest common suffering
from is similar to visiting all the
towns using the roads in a minimum
distance to so it's sort of like a
traveling salesman's prom which can be
reduced inside and to Latin square and
to sit up n by n see them so if i can
even in the simplest formulation if i
can solve this problem I can definitely
consider this and by answered why should
ok interesting and it is a easy to
verify if a solution of a silica prowl
is correct I give you a correct
summation and check if you exhaustively
try all possible configurations you can
find the correct solution but this will
take a long time nobody has a rigorous
argument to convince us that there might
it might not be a better efficient way
to salsa
so we don't know if p is equal to NP
it's possible have a positive aura de we
can solve this very complex silica in
constant time you can't prove that in a
broad not all instances our heart i'm
flying to georgia i can choose a easy
sudoku if i'm flying to india they can
choose a devilishly hard and i right so
not all of them are equally difficult
but if you try to create a Sudoku puzzle
at random with high probability it will
be easy to solve um so what do we learn
from this well so how to cope with
anything completeness tell the pilot
just think about months come up with a
simpler problem that Gatley looks like
or it but shortest command superstring
does and solve the easy problem even if
it gives the wrong answer LOL is to
learn to live with incomplete or
information won't be the biologies to
cheat I like this one is an experiments
and technologies so that they only
generate easy instance of the hard rock
and solve them correctly in fact using
the optical map again sequence an e coli
in about four hours and you can prove
that is np-complete it should take
enormous monitor idea is that under
certain parameters these are simple
solve the problem by exhaustive search
but learn to consider in the Subspace
intelligent it is to go together try all
of the above here is my example of
traveling salesman's to this is a fair
sass master of all the towns in the
world this was done using branch and
cart these are examples from sat solvers
this is mini said this is 600 lines of
code and one can solve millions of
variables this classical tplo algorithms
so if we can do that we should be able
to do genomes so the idea is to find NP
easy problems so this is not a term I
coined it's from what I've already
so to generate easy instances of heart
problems so one of the things that one
can do is to add a condo construction so
that gets around the problem that just
optimizing overlap constraint is not the
optimal one is not the correct so one of
the ways i can get around that is to put
my prayer concerns that means if i have
two reads and i have distance of wheat
pizza my solution should agree with
three probably beats and i have a mate
bear for some read from the repeat
that's outside of the repeat and i
should be able to distinguish with
another thing you can do is you
constrain that so that the local
coverage follow some sort of poison for
us if you have compressed repeat it will
have a larger local overlap so that
constraints will get around it but the
one I like the most is the kinds of
constraints that come from very long
range loading industry so I want to get
300 k before it can be megabass long
single molecule from you and get very
low resolution physical markers on there
so if I assemble something I can check
validate that it is correct yes
yes well I mean besides limiting about
right right so you'll see those physical
markers in the repeats so if you had
repeats that are mega base law then
optical maps won't be satisfactory but
looks like there's only one megabass lon
repeat in the Givens so we know that but
you could overcome that if you had 23
megabass long single body that's a
little hard because of the sample
preparation phone so the idea is to just
go back to doing an exhaustive search
you start with the start node look at
overlaps and look at all possible ways
you can combine two over that expanding
to a tree the right to the same to the
left and take all the score functions
that I talked about in the and all the
functions and find the optimal path left
right the one problem with this is that
my branching factor is very high and
I'll live need lot of memory amount of
time and in fact as I increase the
coverage I branching factor will
increase in the more pieces will overlap
so it's a nice idea but looks like
you'll run into difficulties but other
than that if I can burn I'm going to do
fairly well so instead of looking for a
good solution quickly as gritty does my
job is to find bad solutions quickly and
eliminate them and find a lot of them
very quickly so the first thing that
happens is that if a overlap would be me
over not to see I would like to see a
overlap is simple project if I'm doing
something incorrectly switching repeats
and so and so forth I'll see this kind
of violation very quickly and eliminate
something like ninety percent of the
branches will get eliminated by this
next thing is to look at large number of
branches if I'm going along the correct
solution with a i will have reads be 150
n that will overlap but also i'll have
this prize if you follow that we want to
so if I can quickly detect this I can
collapse these branches into single
branch so most of the time I'm in some
intergenic region and I will see this
collab all the time except when i get
hyper typic ambiguities attributes if i
do that then these two these will group
into two groups because I will find
subsets that will not overlap with each
other okay and when I do that when I see
that I simply look at it I go forward on
both direction and now I have long
distance constraints they simply score
both branches if I have only one branch
that succeeds right then I know that
this one is not correct and it is a
repeat but both of them satisfying these
external long-range constraints what do
I know have an appetite that means you
father's chromosome has gone that way
and your mother's coming the same thing
solves both hyper typical abilities and
repeats and there are lots and lots of
technologies that can give me long rated
and I don't need very high accuracy all
I need is a little bit of information in
some approach some very fast algorithm
which can be done using hasn't yet
refreshing to say which one is likely to
occur so speed you can get the speed
because most of the times you are going
with a single branch so it is actually
doing the video in fact this algorithm
often gets the cake with the outlets and
most of the repeats don't go very far to
about 200 300 basis so these trees are
not making so it does go blow up into
exponential growth occasionally but they
are very rare and they don't go for too
long so that's about it so we did some
comparison and this algorithm does
better than the
editor's I don't want to go too much
into this the question still remains how
do you know that what you have assembled
is correct so suppose I just give you a
three billion atcc and see say that
that's years you know how do you know
that I'm incorrectly but that's pretty
much what we did for ten years ago one
idea has been to create in silico
simulation and run this algorithm and
see how well they match but how do you
know that you're in silico simulation is
actually matching the correct you know
that's like one of the ideas that we
have been exploring is to find various
features in the assembly in the layouts
and try to use learning algorithms to
say which features are likely to give us
better answer and by doing PCA and I see
analysis we found that there are very
small number of features that are
sufficiently good in fitting so so we
analyzed several assembly algorithms and
essentially did something like deceiver
operator cars ROC curves to compare this
so here is some advice to young
dynamicists mole versus g1's sequencing
data going too fast but without leading
to your benefits data compression better
based on algorithms parallelization and
scaling or what genomics needs bigger
clouds is not a substitute for thinking
we can solve all the problems by just
having clouds we need to trust but
verify genomics is a wicked problem what
that means is that if i define the
problem in solve it my solution tells me
that I have not correctly formulated the
problem for it so what I I fell but I
learned how to reform a little bro so we
need to keep going at it until we have a
correct formulation massive data is not
going to solve our powers when we base
call the world calls with us but when we
G was PG muscle on base calling doing
low-level sequence reads is not the
hardest bronze the hard for office and I
think we should be optimistic none of
business tempest and I want to dedicate
this to my mentor ill wolf thought made
a lot of interesting things in computer
science from handy when I face new
phones and that's the end I'm sorry I
took too long
assumption is that the base genome gets
better as you test more people like that
would be be because then you're your
base of comparison your sample gets
better and better you pre sequence more
people yeah not necessarily we will
understand more polymorphisms more
variants but we are assuming that if I
am able to have a typically sequence
then I will understand the population
structures day so one of the firm's is
that what we have done is done general
epic sequencing and from that we have
tried to do population stratification
understand the genetic relationship
between people and use the genotypic
data from the population to impute
haplotype phrasing what I am suggesting
is other way around we do have to type
correctly using the technology and use
that impute family structure understand
who is related to in the reason I'm
arguing for that is that when we did
linkage analysis when we looked at
families and the genetic analysis and
family we've done well so it's not that
there is a fundamental problem that we
can't overcome and also we know that if
I look at two years and quatres in have
the genome sequences correctly I can
always do better so would like to drive
to it sampling and understanding the
population white genome structures much
better than we do now in am arguing the
geography assembly is not giving us that
information because I'm losing the
information between how you are related
to your father and mother and
grandfathers
yes
um this as you're speaking it reminded
me of the the folded program for protein
I'm was wondering if you thought of is
it similar in nature to that problem and
it thought of approaches like that that
taken more spacial based or like em
playing a crowdsourcing I suppose some
people have suggested that but i think
is actually some fundamental questions
that we can do algorithmic so the basic
think about the technology and
algorithms before we but the difference
between the two problems is that the the
protein problem is like not solvable by
the current technology and you think
this one is I'm not an expert in
proteomics so it's not I mean most of
the protein problems are relatively
smaller than this but proteomics has
done very well because we have put bias
evolutionary constraints so there the
from is that if i write down the
optimization far from routine full name
is also NP coupling but there i'm going
to argue that is possible the nature has
only selected easy problems already
between protein did not fall rapidly it
has a selection disadvantage so maybe
all the proteins we have are actually
easy for us to solve but the way we are
trying to solve it is not correlating it
right so one of the ways to think about
that if i do homologies they take pieces
of proteins and find homologous proteins
or domains and steal their structures
but use the known structures for those
and i can do better
because there is an evolutionary
constraint thus helping so I think
protein folding is a fundamentally
different kinds of crops there the
easiness is already there here I need to
do something in technology so that these
pieces sort of fit together enough
constraints so that it can be turned
into an easy problem I mean some sense
this I mean I find it very difficult to
think of it biology and completeness
there is already already witnesses for
it in the in the nature summer we're not
using that witness to it is very fun so
somehow if we collected the right kind
of information the problem should be
simpler
I was wondering when when you have those
two sequences and you're calculating the
overlap can two sequences have more than
one overlap in other words like this and
then like this yeah the repeat yeah and
then you would always choose the bigger
overlap but currently that's what people
did what is the state of research an
area of physical sequencing like you're
the person who are close to science
there's a lot of interest I don't know
if people saw in the newspaper about a
month ago there was a company called
Oxford nanopore which announced that
they can read DNA about 48 kilobases 100
kilo bases by just pushing the DNA
through a small hole so that there is a
quality of their read one of the error
they claimed about four percent error is
better than shotgun sequencing no it's
slightly worse but but you know there
hasn't been enough you know that it is
actually an available technology and
another question is about proteins if we
can work from evolution in proteins we
can work backward in genomics to because
genome was not mutated randomly to it
was properly reassembled with from
pieces of our genomes and arenae and
others pieces and we can limit our
searches and they fry you know the
historic data we can concur it is enough
historic data so if we get 10,000 humans
I think I'm gonna historic data so right
so there is a lot of similarities and I
can write on that and that that's why
the first part of the talk emphasize
that because after that genome assembly
is not a prob if I'm enough but you
don't need to use only humans you can
use a lot of difference pieces and if
you go deeper in evolution you can
compare what types of mutation you saw
and if you see a pattern in mutation you
can limit your search
is only based on those mutations as
possible the genome evolution is
somewhat different because there are
intergenic regions and introns under
neutral selection so they can mute it
and also we know that a lot of copy
number changes most of evolution
actually runs by duplication so which
actually encourages repeats so classical
example was p73 p53 p63 that we talked
about hemoglobin myoglobin there's lots
of examples of that and that can confuse
our section slightly different from
proteins but but the idea is a good idea
thank you for your talk maybe you can
help me with a little bit of simple math
so we have four values for each base
pair and I think you said there were
three billion base pairs per / sequence
and so that's what like six six
gigabytes per / read if it's two bits
prepared whatever my maths right 6 6
gigabytes per sequence so that's
basically what you need to store for
austerity and future analysis and things
like that much less what do you what do
you think what is the number what does
it come to for sequence on disk I don't
need to store the entire sequence I can
just store the difference you have from
the US craig Venter Jim Watson whoever
you choose as long as I know that I've
got the correct sequence even if I don't
at the storage that could suffice so
it's not the storage that's provided but
the currently the way we collect the
data that's you make a fair amount of
your topless about bringing the
reference library to the sequencer and
doing analysis at the sequencer and so
is transferring the sequence but that
would have to be fairly flexible and
robust because as we go forward we want
to do new analysis against maybe a
different reference so right the more
you put in in in hardware and move to
the machine the easier it is to get a
step with
the latest technologies the latest right
but i can i can always this is a
lossless compression i can always
restore I can always recreate the data
in the cloud because I have the
reference and I know the changes not the
difference so I'm not losing anything
I'm not transmitting a story event I
information thank you
just to follow up but on the last
question of bob kuzava interest myself
the obviously the FPGA can relate
reloaded it will so at that point it
doesn't matter what changes happen in
the cloud that improve your data you
just do a revote feel to reload of the
firmware other thing I can also relate
to even if i'm talking about 10,000
references even a hundred thousand
reference it's not an actually huge
amount of data because the genome is
roughly divided into pieces ridiculous
kilobases pieces roughly on average in
each of those only come in very small
number of variants seven or eight so
it's the underlying pieces with some
small mutations are very few all I have
to do is to organize you your DNA with
respect to those building blocks so a
hundred thousand people is not really
that much data it's more than a right it
is that
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>