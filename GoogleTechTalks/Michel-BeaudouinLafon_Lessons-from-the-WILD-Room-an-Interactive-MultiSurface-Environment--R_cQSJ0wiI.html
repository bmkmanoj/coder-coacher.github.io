<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Michel Beaudouin-Lafon_Lessons from the WILD Room, an Interactive Multi-Surface Environment | Coder Coacher - Coaching Coders</title><meta content="Michel Beaudouin-Lafon_Lessons from the WILD Room, an Interactive Multi-Surface Environment - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Michel Beaudouin-Lafon_Lessons from the WILD Room, an Interactive Multi-Surface Environment</b></h2><h5 class="post__date">2011-05-18</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/-R_cQSJ0wiI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a pleasure to have Bashara born
before and windy mckay here and we both
are leaders in the field and five years
in the field of human computer
interaction so Michelle our speaker
today and is the professor at the
University of Harris right and so he's
going to talk about their recent work on
large screen Molly service interaction
it's very exciting to see this project
the project is not only to define new
interaction techniques also to redesign
computing architectures to provide
better support for new form factors and
interaction techniques Thank You yang so
thank you really for being here as a
young said I'm from university but I
stood in France and this year together
with Wendy we're both visiting
professors at Stanford actually thinking
of spending a second year there before I
start the talk i would like to give you
one slide down on what we do in in the
lab in Paris called in situ for situated
interaction in French it's intelligent
situate so the group is headed by Wendy
and we work on all sorts of things today
we'll talk about falls under the first
and last categories in here mostly
interaction in visualization paradigms
we try to create new interaction
techniques and visualization techniques
for all sorts of environments from kids
to the office to interactive rooms as I
will talk about today and the last one
is engineering of interactive systems we
try to build tool kits and software that
sometimes gets distributed to put our
ideas into practice and also to have
tools to develop our own research the
second theme is mediated communication
which I will not talk very much or not
at all today but we've done and still do
a lot of work on helping people
communicate especially in what we call
the intimate social network so the like
like anti-facebook if you wish kind of
thing where you want to have channels of
communication with your really close
real friends and family and
participatory design which crosses
across all these the topics where we
engage with real users we have real
problems to inform our design process
and involve them in the design process
all along from from early requirements
discovery to prototyping to evaluation
to redesign so go see our website
institute a Tellarite @ FR to see what
what we're doing so to open the top
today I pull this image from mark visors
you know visionary paper in 1991's been
20 years that the paper has been
published on the concept of ubicomp and
this is a sentence from the opening
paragraph of the paper the most profound
technologies are those that disappear
they weave themselves into the fabric of
everyday life until they are in this
indistinguishable from it and the
question we can ask is when years later
are we achieve this weaving of
technology into our everyday life I
think not maybe for you here yes because
you're you are Google so you're in a
high-tech environment but for the rest
of us maybe not so much we have all
sorts of hardware as wiser and
envisioned all sorts of sizes and shapes
of devices although interestingly things
like tablets are very very recent but
when you go to the software level things
are not really well integrated what I
have to plug in my iphone to sync why do
I have to sync what does it mean to sync
an iphone if i were to share this
presentation with you not like sending
you the slides in advance but you know
as i show these slides letting you have
them on your laptop so you can annotate
or do whatever you want with them or if
at the end if you have a question and
you want to flip to go to the slide
where you have the question there's no
easy way to do this today we know it's
technically feasible but the
infrastructure has not been available to
do this
and you know it took much less than 20
years for the gooey paradigm to go from
the alto logic 0 of star 2 to mainstream
but for you become it's still not there
and this is even more true at the
interaction level Ricky mother showed
this pick and drop interaction technique
which is extremely natural and intuitive
15 years ago almost 14 years ago but we
still don't have it today I mean I know
there are apps that do you know bumping
and things like this so it's sort of in
the air but again we don't have the
underlying infrastructure to make this
happen sort of routinely in our use of
computers so I think we're far from
having really put the you become vision
of Mark Weiser into reality so
interaction you become can take many
forms and here arguably I'm going to
concentrate on one how do you become
which which is not the whole story you
become oftentimes is seen as you know
millions of people with mobile devices
and running around in the streets here
I'm going to concentrate on what i call
interactive rooms which is part of that
of the grand vision I think in this type
of environment like we have today with
mesh people around the room with shared
displays the problem is we don't support
multi-device interaction we don't
support collaborative interaction very
well and we don't support situated
interaction so I think the reason for
this is not the lack of research there's
been a lot of research in all these
topics but sort of getting to the next
step of going from this what I often
called point designs like the pick and
drop very nice example to architectures
models and tools that then pave the way
to turn this into software everybody
uses so how can we have a way to
interact in these types of environment
more seamlessly than we do it today how
can we do it in a way that it's not just
putting graphical user interfaces on
on every laptop or PDA or or smartphone
but something that is more usable by
real humans so if you look at this this
graphic designer here he's working on
the table and you can see has all sorts
of tools is using a pen I actually don't
know all the words of these things in
English the the ruler and you see there
is a mug there so the mug it's
suspicious suspiciously clean so I don't
know if he's using it to drink coffee or
tea but maybe he's using it to put pens
in there or maybe is using it to draw
circles the point here is that he has a
lot of simple tools at his disposal to
do fairly highly complex tax passed
which is to draw these diagrams and if
you look a bit further his environment
also has documents in front of him on
this board and their stacks and books
and on these on this desk so here we see
someone who is highly skilled and uses
fairly simple tools to do complex things
now when I was putting this slide
together this is why my desktop look
like and here first of all I have most
of the space taken over by the tools
these tools are fairly complex to do
fairly simple mundane tests like you
know picking a color or picking a font
and so on the computer we tend to have a
situation where we have complex tools
and we only do simple things things with
them here was not trying to do much with
that image it was just trying to reframe
it and color it in things like this so i
think there's this big contrast between
interaction in the real world where we
are we easily grab simple tools and put
them at work in ways that are not always
anticipated and the computer where
everything is very highly constrained
and in fact this notion of tools and
instruments is something that has a
really interested for a long time this
is a few pages from the non-secure pd by
by dido and Dalembert in in 1751 and
this was one of the early encyclopedia
and one things did was to
document all the tools that were used by
people in the various trades at the time
and so you can probably recognize some
of them some of them probably unusual to
you interesting thing is I read recently
about someone who studied all these
tools and there are all in use again is
still today so humans have been creating
tools for as long as we've been humans
it's a distinctive trade of the human
species compared to animals there are a
few examples of animals using tools but
humans have been creating tools to
mediate their interaction with the
environment and I think there is
something there interesting about the
power of tools if you put it in in the
context of Gibsons ecological theory I
don't know how many of you are familiar
with the Gibson's work or at least the
word affordance maybe rings a bell so
for Gibson the notion of an affordance
is the fact that the environment offers
possibilities for action and the is
talking by the animal and humans being
one kind of animal detect those
affordances or seek those affordances in
environment and so appearances are not
properly is per se of the environment
there are relationships between the
subject and the environment now if you
think of tools in this context when I
pick up a pen on the table suddenly
changes the affordances of environment
before I have a pen the affordances for
rioting are very limited well if I'm on
a beach i can write it with my finger on
the sand but if i'm in this room and I
have no pen I don't have a tool to write
once I have a pen environment in my hand
sorry suddenly a bunch of surfaces
become writable I can ride on the table
probably I shouldn't but that's a
different question and so tools are very
powerful way to change the affordances
of the environment
it also works in in New in the opposite
way where I can seek objects with
particular affordances for particular
tasks so if I'm in the wild and I want
to draw on the surface maybe I can use a
charred piece of wood which you know
will allow me to to draw and we all know
in the real world that we use tools in
ways they weren't intended for so if I
need to drive a nail into a wall and I
don't have a hammer I can use a rock it
can use another metal tool I can use my
shoe whatever so I think tools are
really a powerful metaphor for
interacting with computers and maybe
over the years the word direct
manipulation that we always use when we
refer to graphical user interfaces has
been taken too literally because if you
look at GU is today's with menus and
dialog boxes and let's and all that the
interaction is is often if not always
mediated with the things we interact
with the document the the data or
information we interact with so um back
in two thousand i published a paper on
on this interaction model called
instrumental interaction to me the words
tool and instruments are being the same
thing i like instrument for two reasons
one is from instrument you can go to
instrumental which you can't with the
word tool there's no objective but more
importantly i like instrument because it
often echoes the notion of musical
instrument the notion of skill of
virtuosity and the notion of having to
learn an instrument to get the best out
of it so instrumental interaction is a
model where interaction is mediated so
if i am to navigate a document because
it's bigger than then the window i can
use a scroll bar which is a navigation
instrument but if i'm gonna touch
surface maybe i will use another
instrument where i use flicks to to
navigate document i could imagine other
ways that maybe I could have something
another device on the on the side of my
computer to turn like
you do with scrolls in in some cases so
what's important with instruments is
that in general they r fi a command so
the verb navigate or to scroll becomes
an object in the interface like a scroll
bar and to the user that makes it
something that is very well easier to
discover and fairly natural as it
parallels some of the things we do in
the real world in addition instruments
can be polymorphic in that single
instrument can apply to various
different types of objects so for
example if I have something to apply a
color should use the same instrument to
color any type of objects on my on my
screen so this combination of
reification polymorphism the ability to
turn an abstract verb into a concrete
object and the ability to have this
object or instrument applied to two
large calorie of objects gives a great
power to this to this metaphor and I
don't have time today to go into more
details about this but there are sold
papers that we've published about this
and I can talk more dark questions so in
two thousand we did the proof of concept
it was a application that combined by
mental interaction marking menus tool
glasses I think today tits the first
real application that combine all these
techniques which we knew for a long time
but had not really been used together
and it was a great success they have now
over for you I think even sixty thousand
downloads and this was an application to
edit colored pitcher nets which you know
not that many people in the world
interested in doing so with that many
downloads I think it's a large share of
the population of potential users it was
a great success but where the thing
loops back with you become is that a
couple of years ago I worked with the
Danish student on the notion of
ubiquitous instrumental interaction I'm
saying well let's get these instruments
out of the applications themselves and
really separate them like instruments in
the real world my pen is not attached to
my piece of paper I can use it outside
the context of writing on the on a piece
of paper so what if we do the same thing
and detach those instruments for from
from their immediate environment so we
did this system called Vigo where you
could pull a tool palette or a color
palette or other things from a shared
surface and put it on your smartphone at
the time this was a nokia i think and we
created this notion of multi surface
interaction and having instruments that
span there's multiple surfaces so in a
sense we didn't much more than
recreating things like the pick and drop
from ricky moto but we put it into a
consistent framework where you could
imagine other types of interaction and
you can make it work in sort of in a
more generic way than the technique that
recommended had introduced so this gets
us to to the main topic for the talk
today which is the the wild room so over
the past two years we have explored this
notion of multi surface interaction in
an environment we created other lab in
Paris called the wild room and wild
means a wall-sized interaction with
large data sets and for us this is an
example of something I think we don't
see enough of energy I research these
days and I call it integrated research
and to me the work that went and that
led to the Xerox star in 1981 is a good
example of what you can achieve with
integrated research of course a star
with the result of 10 years or more of
research at Xerox PARC but what is
important is that they put together a
bunch of technologies that they created
from the ground up they created the
hardware including you know part of the
of the CPU and really the the guts of
the of the system and the software
including the operating system do the
application software they created a
specific keyboard they reinvented or we
design the mouse etc etc and by creating
everything from scratch they had
tremendous freedom also they had a
specific target population which were
executive secretaries and the reason I
think we have this that metaphor today
is because they were targeting this
population and they created a metaphor
that was meaningful for this population
if they are targeted another type of
population probably they would have come
up with a different type of environment
the point here is that into any research
is about putting together a set of
existing technologies in ways that
haven't been done before discovering the
problems that creates solving those
problems and solving them for target
population because you cannot do
everything at once and so with the weld
a platform we are modestly trying to do
the same thing we want to study
interaction in interactive rooms where
you have collections of interactive
surfaces we have a well display
interactive table we have mobile devices
and he'll devices and we can track
motion in the environment and we want to
see how we need to redefine interaction
in this environment and we're going to
create the underlying software to make
this run not only in this specific
platform but hopefully in ways that
could could be translated to other
similar environments so I'm going to go
quickly to describe the hardware so we
have a wall with 32 monitors 30 inch
monitors you have the size here here you
can in feed so it will be 18 by 6 feet
the pillow resolution is about 130
million pixels and it's mounted on four
cards and each screen is itself on the
pivotable structure I mean attachment so
we can actually configure this in
various ways
it takes time to do it it's not a push
of a button but it's important to see
how better configurations can work for
example we like this triptych
configuration if it's completely flat we
find that it's hard to to use the wall
efficiently because of the parallax we
decided to have what we call now ultra
high resolution with the cost of having
this French door effect of the bezels of
the screens the idea is that those
bezels will go away one day is there is
enough technological push to make them
disappear but the benefit of the ultra
high resolution is really important so
here is I'm sorry the image is dark but
here is what you would get if this image
of the world were displayed with a
standard projector sort of like this one
you see the size of the pixels it's
extremely pixelated this is part of
friends right now if you go to what
people call high or very high resolution
like the the wall they have at
University of Toronto which has 16
projectors the same physical size as
ours but 10 times as fewer pixels so we
have 10 times as many pixels as they do
you get a better resolution like this
one and on our wall you get this type of
resolution and of course the projector
cannot display it accurately so you have
to see it for yourself but imagine the
monitor you have in your desk with that
resolution on the whole wall and what
that means is then you move and you use
the space differently you don't stay
away from the wall because as you
approach it you see more detail so you
have this sort of physical what we call
physical pan and zoom where the thing on
the world doesn't change but as you move
closer you can see more detail and of
course as you move backwards you can see
the overview and that completely changed
the type of interaction now it's more a
critical on on this place like this this
is a map of part of the internet where
I've put a close-up here and you see
that there are all these labels that are
the names of the switches on the
internet and on our wall we can have
this image full screen full
solution you can get up close and read
all these labels you don't have to pan
and zoom or use lenses or any of this
thing and we think this is this is
really critical for the type of
interaction we're looking at so we have
a multi-touch table nothing special
about it it has a much lower resolution
than the wall which is actually
interesting and draws people kind of
away from the table a lot of the time we
have a motion tracking system using the
Vikon the well-known commercial Vikon
system we can track objects and people
in the whole room with high accuracy and
a no lag and we use mobile devices
handheld devices so we've used the ipod
touch a lot iphones ipads we have
android devices now we also have
something called the gyroscopic mouse
which is a mouse that you can use in
midair and we create our own devices
when needed and we found that people
really like to have the they're sort of
own personal device to interact in this
environment where everything else is
shared come back to this moment the wall
itself is powered by a 16 computer
cluster we put in you know everything we
could in terms of memory top end
graphics card because we're not you know
graphics computer graphics specialist so
we wanted to make it easy for ourselves
to create software for this thing it's
hooked up to a computational cluster
although we haven't used much of this
connection so far but each computer
drives to screen and and that means that
every time you want to do anything on
this wall you need to create a
distributed system that will run on that
cluster so there's no way to to use
tendered applications of the shelf so
the key point here if you compare this
to other environments of the sort a lot
of the other environments and
uncertainty of the walls with scene were
very large but didn't focus on
interaction and instead we are focusing
on interaction and collaboration the
fact that we have this very large size
ultra high resolution and multiple
surfaces all the
together provide unique affordances for
interaction is in this environment and
we created with off-the-shelf components
which made it really cheap and easy to
reproduce elsewhere so now our extreme
users so our users are scientists we are
in university near Paris that is well
known internationally for especially
their mathematicians their physicists
and and also biology and so we linked
with labs that all have problems dealing
with data today so it goes from
astrophysics to mechanical engineering
biology particle physics etc particle
physics for example they get data
straight out of the LHC at CERN and they
haven't figured out how they get to look
at it they literally don't know how to
look at it another example is
astrophysicists this is an
astrophysicist from yes and this is the
first time we can see this image of the
deep sky at full resolution and what
full size this is half the image there's
another half and we can scroll between
the two and this image every single
little dot that you can sort of imagine
is there is not a star it's a galaxy and
this is data they work with every day
except they do it in their office with a
17 inch monitor they have satellites
that have been you know ten year twenty
years project that they finally put in
the sky is the Planck satellite sends
four thousand by four thousand pixel
images every 10 seconds in 10 bandwidth
then and wait in bandwidth again they
store them and they haven't figured out
how to look at them what to do with them
another example is chemistry and biology
very complex molecules as you can
imagine the one on the left is in fact
running under primal which is a an open
source molecule visualizer system and
with the software we've created we can
run
on the wall and you can turn the
molecule and do all the things you do in
pi model in terms of visualization in
real time on the wall with the system
running on the clustration below but
before neuroanatomist they're not
interested in big images they have
high-resolution scans with brains but
they are interested in comparing the
brains and they told us you know I want
to have five hundred brains displayed
and be able to look at them and and
rearrange them to compare them directly
and figure out which are the third have
a pathology and which are fine and so
here the size of the display is useful
for comparison and what came out of
working with is scientists and starting
to understand their need is that the
large surfaces like the wall we're not
necessarily for displaying large images
and in fact when they are there to
display large images they still need to
navigate with things like panning and
zooming this image we see on the left
which is an assemblage of images by the
Spitzer satellite of the center of our
galaxy the full resolution image is four
hundred thousand pixels wide and they
need to work with all of it that's 20
times the size of our wall so we have an
image of Paris that is 27 Giga pixels if
you take all of google earth I it's a
few petter pixels so you know when they
want full wall images it's still not
enough and they will still need to
navigate in those large images often
times where they want to do is compare
homogeneous data like this brain scans
Azure all different brains they look
very similar to you and me but they are
different even more often they want to
aggregate heterogeneous data a brain
scan some DNA sequence data pulled out
of a gene bank papers pull out of
various databases or from google scholar
and they want to put all this together
and discuss it in small groups and the
last thing they want to do and they all
want to do it is to say well this
environments
grace but how do I bring in my
colleagues working you know at Stanford
or can bridge or somewhere else so how
do I communicate remotely and
collaborate remotely and this is
something we're not covering yet
although we have a project net area so
we work with them not only to understand
their needs but to create the solutions
and so we do participatory design
workshops like this one where we play we
act out interaction techniques so here
it's it's using the wizard of oz
technique someone in the back of the
room is is controlling this web page and
I'm playing with you know how do we
scroll this with simulated and held
device do I want to scroll up or scroll
down how does this work we do the 2d
version well pan and zoom version here
with controlling Google Earth on my
iPhone and pretending that it's
controlling the full size version and
I'll skip to the end here here is
another way in fact we found that this
pinching gesture was not very efficient
in this kind of midair environment but
doing a physical tracking for panning
and zooming was a lot more natural and
of course because we track the position
of the devices in 3d we can do that and
then we have our own take on the pick
and drop which is the shovel so you've
on the table you gather the stuff you're
interested in you go to the wall and
then you throw it on the wall and that's
the kind of interaction we we can
support and the thing is we work with
this is a session of within our cells
but when we work with scientists they
spontaneously come up with these types
of interaction and so the notion of
multi surface interaction is natural to
them we don't tell them we're going to
do multi surface interaction now tell us
what what you think we let them imagine
things and for example the
neuroanatomist came up what they need is
to control his brains in 3d and one of
them pulled out of his bag a 3d model of
a brain and it turned out to be
a printout of his own brain he had gone
into the MRI and had done 3d print of
his brain so he's walking around with
his brand in his in his bag doesn't like
other people to touch it and then you
say well I want to hold this and turn it
in 3d and have the brains on the on the
screen turn in parallel and of course
ken hinkley had done that back in 93 but
it was interesting to see that they
spontaneously came up with this type of
interaction so a multi surface
interaction the way we see it is this
notion of instruments that move edit
content seem it seamlessly across
surfaces so the bottom we have the
shovel interaction I should be four on
the right we have interactive paper
people want to write on paper and be
able to annotate things on the wall or
on the table with paper annotations in
the middle we want to be able to use for
example the table as a sort of a sorting
area for what's on the wall and all of
this can be interpreted in this context
of multi surface interaction and an
instruments generic instruments for the
environment and we also want to let
users come in the room plug in their
laptop whatever that means and be able
to seamlessly transfer content to the to
the various surfaces from their own
laptop so we created a small video here
which is acted out so it's a bit wooden
but it's a current system as it works so
here we see claimants using the table as
a staging area for the wall so is using
images that the astrophysicists actually
use for real and then these images can
be picked up by James here who's using
an ipod touch as a sort of laser pointer
and there is a little interface on the
ipod touch so you can move or resize as
we'll see in a little bit and to bring
content to the wall we have various
content providers so one of them is
interestingly email we have a gmail
account for the wall and the program
that monitors that the email account and
when something arrives
it takes really long time to write this
email so let's give this part so as soon
as it detects the document attached to
an email it throws it on the wall and
someone can pick it up and drag it in
this in this environment so here we see
that is using the same device to scale
since these are PDF documents they scale
at full resolution on the on the on the
wall now here is brought a web window
honey's laptop and is sharing that the
snapshot of that window so that's from
an unmodified laptop anybody bring the
wrapped up to do to the room could do
that now with a little bit more software
we can also transfer live windows from
from a Mac machine so here we put it on
the table we've changed the
configuration so down the table is
adjacent to the wall so I slide things
from him to the wall and as interact
with that the web page in that case on
his laptop the the one on the wall is
updated in real time we could also
interact with the version on the wall
directly and here is our fake physicist
Fanus is pretending to write a an
equation on paper with the inert Oh hen
and again the same idea is that you can
write annotation is going to put the pen
down and the cradle and the document is
going to show up on the on the wall so
this is an active video but it all comes
out from these sessions with with
scientists and the kind of thing they
would like to do is in this environment
to work and that's some of their real
documents and it's not necessarily a
critically difficult to implement but
what's important is how similis it has
to be for them to be able to use it and
here's another example where are we
grabbing a photo and sending it over
over email so
when we work with those scientists we
understand the type of interaction they
would like to have for the environment
we also we realize that we need to
reinvent everything pointing at data on
the wall you're not going to do it with
a mouse on the table navigating
documents because of the size it's
probably not going to be the same thing
as we do today on laptops menu selection
window management all these basic tasks
that we have refined in the good
environment have to be reinvented and
redefined in this environment so we've
done a little bit on the on the
elementary parts of this pointing and
navigation when pointing for example
given the size so the maximum amplitude
and the resolution which defines the
smallest target you want to point out
plus the fact that even if these were
touch screens you still want to be able
to point at the distance because
sometimes I'm going to be standing far
away and I want to highlight a
particular element of the of the wall we
need better than laser pointing laser
pointing just doesn't work at least we
haven't been able to make it work so we
created a set of dual yes well
technically it works but you don't get
the accuracy you need so if you want to
so in the video here I think we have
something that shows so this is a
Manhattan and we're going to zoom in so
you can see the sort of kind of
resolution you get on the wall and at
the distance where he is standing which
is personally like here to the wall you
cannot control the laser pointing to
point at a target of the size that you
want to be able to point out so what
we've created is duomo duomo mode
pointing techniques where you have a
course technique to approach the target
which is laser pointing and then you
have to enter a mode which is a little
bit cumbersome to adjust that the the
pointing we tested various techniques
and we can select targets that are
I think under 4 millimeters which is two
or three times is better than that when
what was done before but it's still a
little bit problematic that you have
this mode switching but we try the
gyroscopic mouse where you play with the
acceleration like you do on the laptop
but we couldn't get to the well we
couldn't get the right curve maybe
someone can do better but we couldn't
solve it with a single-mode technique
the paper that was presented last week
at sky and was one of the best best
papers best of KY papers looked at
panning and zooming and hear what they
did because I'm was not directly
involved in the work so but when he was
is compared 12 interaction techniques by
looking at three design space build
around three factors so the first one is
do you want to use one hair or two hands
pan and zoom we know it works well to do
you know penning with one hand zooming
with the other usually zooming with the
non-dominant hand then there is the
question of do you on linear gestures or
circular gestures and since you
navigating large ranches circular is
appealing because you don't have to do
the clutching that you need when when
you do with the mouse on the on the
table the third dimension is do you want
free hand gestures or do you want
gestures that are guided either in the
2d surface like a handheld device or
even with a 1d device like the scroll
wheel on the mouse so i can give you the
whole results but it turns out of these
12 techniques to came out as the top
techniques one here we see is by manual
linear motions using a tory surface so
you can see your hand on the left doing
zooming with one hand and the right hand
doing the pet depending now the
interesting this went too fast oops the
story so i'll pay it again because it
went fast so by manual linear to the
surface so on with the right hand she is
panning and with the left hand she's
controlling zoom
on the ipod now a little bit less good
in terms of performance but the
advantage of work with one hand is you
do this to task panning and zooming with
one hand using two areas on the handheld
surface 14 padding one for zooming which
means you have to lead sequentially but
still performs better than most other
techniques things that work don't work
well at all is the type of free and
interaction that we see in movies so
here's uni manual circular motion with
3d free hand she's holding a device but
and you know this is the kind of thing
you seen in Hollywood the users love
that in terms of you know it's cool I
can you know pretend i'm tom cruise but
the data says it's more tiring it's less
accurate and and it doesn't work as well
so i don't think we have the final story
on this but we have explored these
techniques and and we know the different
trade-offs and the thing that could be
explored further and those that are more
problematic the last section on the of
the talk here is going to be on the
software so you know we're working with
users to understand the type of
interaction they they want to to use in
this environment with a basic research
on things like pointing navigation and
the next will come things like selecting
menus or this type of elementary task
and then we do the engineering of you
know how can we build an environment of
software environment that makes it easy
to develop applications for for this
type of system now typically in our set
up we have 16 computers driving the wall
we have two front end computers in front
of the of this cluster we have a
computer driving the Vikon tracking
system where the computer in the table
we have each of the handheld device is a
computer itself so we have 20 25
computers every laptop that someone
brings it is an extra computer and to
create a seamless experience I was
talking about we need to develop
distributed applications for this
environment so one thing I worked we're
very well for us was this notion of
replicating the full application at each
node of the wall and this is very
different from what everybody else
apparently is doing on these large walls
which is to use some kind of variant of
an OpenGL back end that will sort of
pipeline the rendering on the cluster in
a way that is transparent to the
application so I'd only she knows things
like chromium or equalizer or C glx and
number of these things the reason this
didn't work for us is first we don't
want to rewrite applications from
scratch and our approach allows us to
reuse legacy application second it
doesn't work well as well for for
interaction so I will show you the types
of things we've developed first we have
something that we call the wild input
server and here the issue is we want to
be able to easily reconfigure the
interaction environment we buy your new
gyroscopic mouse we want to be able to
try it and to plug it in in existing
applications so we want to extract the
sort of lower level of interaction
processing of low level events outside
of the application so we use a system
that was developed by someone in the
group of years back called icon the
input configurator which uses this
visual language of blocks that you
connect together it's not unlike max/msp
for those who know these types of
data-driven languages the nice thing is
that it is visual it is interacted I can
change this diagram at runtime and we do
it very often when we have this
participatory design sessions to try
different things and so what this gives
us is is the notion of logical devices
which are built up from hardware devices
for example the ipod touch we use as a
laser pointer and as sort of input input
device itself
in fact at the hardware level is two
devices we have the gesture tracker the
motion tracker and we have the ipod for
the application it's one device that
it's it had its in you know it's an ipod
that happens to to know its position in
space so using these blocks we can
present the application with this
abstraction of a pointing device in 3d
and when you're developing your software
you can do this at home or in your
office you don't need the wall you can
replace the RIKEN tracker by well we've
done it with the wii it could be a
kinect and when we get into the room we
just change the configuration and it
works and so this has really been a key
in the flexibility we have in redefining
and reconfigure interaction on the on
the fly then we have a rendering engine
called the global interface user
interface toolkit vtm which was
developed by a menu and pete liga on the
picture here and poured it to the wall
and the way we do this is we run the
system 16 x 1 time per computer or
sometimes 32 x 1 time per screen and
there is a front end to the application
that just synchronizes those 16 or 32
replicas each replica has its own camera
on the part of the scene is showing and
by doing the really simplest possible
thing we have extremely good performance
and we've done this for this toolkit
we've done it for the primal system I
should be for for visualizing molecules
we've done it with software from the
physicist that I'm not allowed to show
so we've done need time and again and it
works very well and then we've developed
the substance which is a something a bit
more ambitious i would say where we're
trying to develop an infrastructure for
developing full application in this
environment where you can easily add and
remove parts of the application as it
runs so i get a paper at chi last week
some of you were there I don't have
to get into the details here but we have
a data model a data oriented programming
model so on these various devices we
create these trees of data and then we
attach functionality on these trees and
we have a way of sharing these trees
among different computers through either
replication or mounting and gives us an
extremely flexible environment and the
video the long video I showed before was
was created using using substance and
then and we use it now routinely and
also substance directly implements the
notion of instrument that I have
introduced at the beginning of the talk
so another application we created with
substance is called superstars grease
which is French for grey matter which is
a software that our neuroanatomist
really wanted to have and we display 64
brain scans to two per screen and we use
their application called anitha mist
which is complex application for 3d
visualization of these thank you and now
wait so for 3d visualization of those
brains and we didn't want to rewrite it
so what we did is we wrap their
application inside our substance
middleware and what we are able to do is
something like this where we have this
plastic brain that is track in 3d and
its position directly controls the 64
brains displayed on all the screen and
as you see it's very smooth and unreal
time interaction now there are issues
with the control with the brand because
of course you have to stay steady if you
want to the display not to move so we
need the clutching mechanism but this
application was developed very easily
within this environment and so unlike
any other system I know to run on walls
like this we we can use legacy
application without reprogramming
reprogramming them so lessons at the
hardware level we were able to put
together a platform with off-the-shelf
components
and still very flexible we focus on
interaction and collaboration not on the
graphics rendering we engage with real
users through participatory design we
try to understand the affordances of the
platform and use the interaction
instrumental interaction model to create
a unified interaction in this context we
discovered that people like to use
personal devices to interact with this
shared data and we build software from
scratch we're being lazy by putting a
lot of computing power so we don't have
to do too much work ourselves and wait
for Moore's law to do the work for us
and using little tricks like the OSC
protocol as very efficient glue to
create these loosely coupled distributed
applications one aspect that in cover is
this notion of collaboration remember
there was a slide with four images and
the last on the ride was communicate and
time again these users as other
potential users of these rooms come to
us say well but I want to share this
with my colleagues at the distance and
so we just got a grant from the French
government that I'm coordinating it's a
30 million dollars the project to create
nine rooms similar to this wild room not
all the same some are immersive caves
some are 3d walls on our multi-touch
walls we want them to be different
because we think if this technology
really becomes real not everybody will
have the exact same configuration like
neither buddies using the same type of
laptop today so we are going to build
these nine rooms and they will be
interconnected with the high-end audio
video system I will study remote
collaboration and try to scale up from
single room to a network of room and
that's the end of my talk and I'll be
happy to answer any questions thank you
nice chance so are any of the scientists
that you were quickly actually been
using the system for
we haven't reached the phase where it
can be used for daily work so there are
there are a couple of issues first is
the data they work with is not for
everybody's eyes so it stunned me that
you know I thought this guy's was
everybody's but the images they have
from these satellites they cannot share
they cannot share with other labs so
there has been an issue about this the
second thing is you know it's just been
a few months where since the system has
been stable enough that you could you
know leave them the key and work for
four day at a time in there the third
thing is that we ourselves are using the
this environment almost all the time and
part of this didja scope project that i
mentioned at the end that will be that
this room will be more accessible to
outside users so we're right so for
example in our lab will have two walls
and one would be more for our research
and the other more for outside users so
we're not quite there yet but I have to
say I've been quite pleasantly surprised
that they were did he spend so much time
with us and they were so willing to come
to the lab and try things out and do
these sessions so they're really
interested and they also see this is a
technology that is affordable for them
they come in an essay Oh two hundred
thousand dollars when those walls yet
we'll put that in there really in a
different scale than they aren't
but then if you talk to the biologists
they're not there's also questions of
where they physically work so in some
cases you know if they're two miles away
that's close but it's not close enough
so
the there's another thing that we
haven't we're not supporting well enough
yet is we make great efforts oh that was
easy for them to bring their data into
the environment as the video was a sort
of illustrating but the question asks is
okay so what do I get at the end of the
session how do I get stuff out and
that's something that we need to think
about to make it really usable and a
more regular basis by them so two
questions I don't know when first okay
music
when you see how some of these
scientists have come to the room I've
seen their own data like the
astrophysicists like the biologist with
the molecules there were like kids in
the candy store and so they immediately
see how not only it's just useful to
them and cool but how it could change
their practices of doing research I have
a concrete example of a different
project was done in another country in
France with archaeologists and all they
were using at the time with the big wall
you know the Silicon Graphics what was
it the reality what do they call it is
rarely tianjin things you know high-end
projectors and the semi immersive walls
and it completely changed their practice
of doing archaeology research what they
would do is they bring all the
archaeologists archaeologists who had
worked in a given site in the world
bring them into this room for two or
three days for workshop you know or for
a week put together all the data that
collected on that site and then
collaborate and discuss like they were
not able to do before and what I I see
clearly is that this will have similar
effects for some of these scientists
maybe not all of them but certainly for
for the biologists the neural
neuroanatomist and for the
astrophysicist it's pretty pretty clear
to me you wanted to add something Wendy
we're funded by the French government we
don't have any commercial ax to grind in
fact our charter if you will is to do
things that are great for French people
great for Europeans and grapefruit world
sort of in that order but
now one of the things that comes from
the first part of the talk so this is
really two tops that Michelle gave one
is about the wall and all that cool
stuff and the other is this concept of
instrument
and I don't know if it's clear to you
how radically different that is then
what is sort of out there right now
right now everybody is tied to Microsoft
that kind of application the idea is
interaction is evolved from a graphical
user interface on a machine like that I
mean they used to be bigger and now
they're smaller but the idea is you get
this application in one silo this
application and another this one in
another they don't talk to each other
you've got your data tied to this one
and if Microsoft changes the format
that's too bad for you if they change
the way you interact with it that's too
bad we're saying into fat what you want
is data some kind of format to your data
and instruments which allow you to
manipulate that and those instruments
could come from that device I'm sure
somebody's done an iphone or ipad or a
big wall or a table or whatever the idea
is that you extract the interaction from
the data once you do that then you're in
a whole different room and then it goes
back to your laptop because it has
patience for how you deal with doing
stuff on your laptop and how as well as
and what's interesting giving this talk
at Google is that if I work Microsoft
I'm scared to death and if I were google
I'd be like yeah because that this kind
of month you know you're all about data
and how to get to it you haven't been
about interaction frankly google subsidy
directions all right commits it but this
doesn't interaction this is how you deal
with interaction with lots of different
types of data so it's kind of like the
show is French she's far more modest
than I'm I'm an American i cooked about
like his but it's really a whole
different way of thinking about how you
do with interaction in beta and and
we've got real world people who are
extreme users that we're trying to push
it but there's this is not a another
interaction
okay so despite ignored ahead the bag
there than here and here oh yeah yeah
they did right
I don't know what it is they did right
they had a wall they have a wall that
we've seen it it's twice as big so it's
really impressive but to interact with
it the last time we were there there was
a computer in the back you had to be you
know at the table using a mouse to move
to put anything on the wall turning your
back to the wall now we with the wallet
is in San Diego which is sort of the
same as in UCI we are collaborating with
the gym hollins group at UCSD and as we
speak we have a student from Paris who's
visiting them this week and is going to
install some of this software there I
think what the projects that San Diego
you see I but so screwed Chicago and the
University of Texas is one all these
large walls have been driven by people
who come from scientific visualization
and themselves coming from heavy-duty
come through graphics it's all about the
graphics rendering and then they realize
oh how are we going to interact with
this and I think their architecture for
the rendering is not appropriate for the
type of things we want to do and they
don't seem at least until now to pay
much attention to interaction the people
who do pay attention to interaction in
these environments are the Raven
balakrishna's group in Toronto career is
Chris North a virgin Virginia Tech maybe
one or two others but in these other
projects we try to use C glx we looked
at another software is called it start
with an s so that's anyway and and they
are really designed so you create a
whole application from scratch for for
the environment and I don't think this
is really the main problem here there's
enough graphics power so you can do
things in a silly way like we do and it
works so
like three years ago and what he said is
that they don't get users they get demos
but they don't get your users we
actually get really anxious we haven't
got real users using it on a day-to-day
basis because we have access problems
but there is no question that they
absolutely want it for them they can
walk in with their data and do useful
things at the start that is not true all
they can do in the big walls it really
well let they do and check out what they
do that there's a network okay it's
called the optic portals they do use it
a bit for teaching so using the wall in
front of the classroom with some time a
remote participant but interaction there
is really slideshow like so it's very
limited last time I was working in that
domain and I kind of lifted with the
process that there's no doubt that
physical artifacts are great to interact
with but they Yuri need a lot of them
and also reminded about the Gallagher's
travel with one of the countries and
keeping the Phillip philosophers were
walk around with sacks of objects so
that they can show them and sometimes I
had the feeling that that was what we
were proposing that gets cute to have
the brain and but but what's what's the
next thing then you have a pod and then
you how can you how can you find few
items few artifacts that are actually
powerful without going all the way down
introducting reduction well I'm not sure
this is so much our problem in again in
this picture look at the number of tools
he has you know incisors yes Penzias
rulers it has all these things yeah so
taking taking hundreds of years to get
to that set of 2 and i think that we
need to work on big enough which tools
might have enough cover tunes to be
actually used right and my answer to
this is that a key aspect of this notion
of instrumental interaction is
polymorphism is make it so you can
design a tool that is independent of the
object it is interacting with so you
have a very loose coupling between the
two and for example we have a move
instrument we have several move
instruments in fact we have one on the
table we have one in this handheld
device all it needs to know is whether
the object you're targeting has either a
method to say move so the object is
aware that it can move or you can also
make it more risky saying well if the
object has an XY position maybe I can
you know directly modify this and see
what happens which is what we do in the
real world all the time we use you know
you use a screwdriver to open your kind
of paint you use your knife instead of
this screwdriver so you know you you you
discover those natural affordances and
what we want to do here is to do this at
a software level and doing this
decoupling between the instruments and
the
the objects the data the content
whatever you want to call it a thing is
is key there will be a scalability issue
but you know I think if we solve the
problem of this decoupling and support
this polymorphism of being able to use
an instrument with various objects even
though the objects were not necessarily
designed to be used by this instrument
then we can address the scalability
problem if it comes up but I would like
to have this the problem of too many
tools we're not at this state yet
because everything is so sort of as when
you were saying you know inside silos
and and and very constrained so and the
other thing is I think you have to
relate to the work practices of your
users and if we provide a flexible
enough infrastructure so when the guy
pulls the brain out of this of his bag
you can say oh yeah let's stick three
markers on it and make it your
instrument then I think we can adapt
also the instruments to the specific
needs and so I always say that
instrumental interaction is both a
combination of some very generic
instruments navigation selection
pointing etc and very specialized one's
for you know power users and I think
this is the way it works out in the real
world so we had Jimmy I know if I
answered your question
question which is actually was at
microsoft before here yet trouble
actually in here the trouble isn't
actually that they aren't aware of this
video is that the problem that in fact I
was actually impressed by the number of
product people that share this idea oh
we could we should conclude material
between all our displays the trouble is
that there's once you get to the brass
tacks and actually how to come up with a
scalable practical solution or
infrastructure that connection do
it becomes very very difficult and there
hasn't been quite I want one of the
frustrations that had some of the
academic work in this space is that they
make a lot of good demos but there's
actually a very little concrete like
cute proposals for actual infrastructure
that could actually adopt it at scale so
so that's one of the reasons why doing
this in France is radically different
than us guess in the u.s. you've got
university system where you have a
professor and graduate students we have
a group of faculty
and we do larger scale projects so the
cpn project show
a full scale
patient and then barbican we're talking
about here yeah I think people i mean
really i don't think people have tried
hard enough and i think no no but i
think in a sense of saying you know
let's start with the clean slate that
also gave a very nice keynote a twist a
couple years back so you know don't try
to get your thing to be compatible with
Microsoft Word or these things you know
try from from scratch this is exactly
what web zeros did with with the star
and you know this days anything you
probably should Chi if it's not an app
that really can download and play with
at the end of the talk is sort of not
you know of interest there's more
innocence there's more creativity in the
hardware side because people understand
that if you're playing with fancy new
technology in other we can have it but
in the software side it is it's very
imperative that it has to be compatible
with what's out there so which is agree
that you have to rewrite the software to
do well but rewriting the software has
been done several times almost since its
fall which it was iphone ipad we all
those massive yeah but within the same
at least 44 for mobile devices within
the same general framework of
essentially the GUI just getting down to
the right size and adjusting the widgets
here and there but not the vision of a
seamless interaction when there's
nothing in iOS and Android in Mac OS and
windows to support this kind of
multi-device interaction
this area the world was all about
applications and it was really hard to
feel figure out that you do anything
else and then the web comes in and
you've got that like a very weird roll
or what the level of interaction
and then you get the app store that and
all of a sudden now we've got software
and suddenly now I mean I envision you
know a an environment where you can buy
instruments the way you buy apps today
know if someone comes up with a nifty I
mean we're teaching a class at Stanford
where we're exploring these ideas with
students and and whatever guinea pig is
the color picker and you know if someone
wants to come up with a nifty or color
picker why is it today I cannot use it
instead of the standard one I mean
within the application i'm not saying
going to web page that has a web app but
why can't i change the color picker in
Word and in Photoshop to use a different
one and this is what this is about and
to do this you need to get these things
outside the app itself and then once
you've done that you are in a completely
different economic model for software
where you don't sell you know big
bundles that do everything but it's it's
more you're going to the store and
buying your dependent and the brush that
you like now I don't know another
business manner i'm not sure how to make
this work and maybe it's too late to
have this kind of revolution happen but
i think when you say we're in a position
in research where this is what we should
be trying at least yang anshuman first
shift the focus from application-centric
to activity or paper essentially fine so
but I still see a general discussion
here regarding our general generality
and the specificity of a tool or two
right so a one hand we want to really
tailor to for specific activities so so
we can be effective and on the other
hand we want to to divine to see our
general tools so coomer's are very good
at using tues in different situations
for different purposes so I'm just
wondering what's your take on these two
arguments I think they are not
incompatible and it's certainly the case
they are not incompatible in the real
world you know if you are a graphic
designer you're going to be very picky
about the exact pen and brush and this
and that you're using and you are going
to go for specialized tool if you are a
kid in kindergarten you don't have the
same level of or sure you know so i
think i see an ecology of instruments of
tools where you have some general
purpose ones that are pretty good at
what they do but but you know and then
you'll have specialized one and me the
user will be able to pick now there's a
aspect of this which when you could give
a talk about which is this notion of
learning and quite apptivity how once I
got a tool I can make it work better for
me through configuring and adapting the
tool and the tool learning from me and
this kind of thing which we've worked on
these things as well so I think we have
the opportunity in software to make
tools that sort of support this
transition from journal used to more
expert use but but I think to start with
you can have generic tools general tools
as well as specialized once I don't see
anything
strange in in this because it's again
what what we do in the real world it's
small it's the same question or issue
that is the notion of instrument it's
really interesting but you still see the
general tension between natural means if
you will direct manipulation versus
something more powerful but less direct
with more translational transformations
in the middle among all the tools you
demonstrated the replica of a physical
model seems to be most intuitive that
anybody can relate to but it's also
probably least powerful because it only
fits that one particular task as you can
see once you start using it you realize
actually it's not that good because you
have to treat her you have it's not
stable you may need a clutch once you
had a clutch is already not so direct
and and so on so forth imagine the
people drive complex vehicles instead of
using indirect controls they actually
use a replica of their model itself it
will be actually not all that powerful
so this tension between direct
manipulation or very intuitive
affordances versus the more powerful
tools continue to be a challenge I
completely agree it continues to be a
challenge and I don't think you know
this is just saying this is an
instrument solves that problem I think I
think you have this notion of mediation
and and I think it's important to
recognize that you have mediation and
that mediation can be ill sort of simple
like in the case of the plastic brain or
can be more complicated and you have
more sophisticated instrument I mean if
you take the example of searching things
on Google you know the search box and
the Google of main page is a searching
instrument and it's it's very simple
except that if you want to use it in a
powerful way you have to learn all this
syntax that you can use now what I
envision is that you know people could
have better interfaces for for food
search box so you wouldn't have to learn
and sent this syntax the interface for
navigating the results is also very poor
in Google's case it's page after page
after page well now in some cases you
can scroll but I think you can add I
think the problem with the tool metaphor
is that if you take it too literally you
only think in terms of pens and rulers
but you have instruments that are
complicated the driving a car is a
complicated instruments with a lot of
interaction between you know what you do
with the steering wheel or with a
throttle and you know what it does in
the end what makes it work is that you
can understand it with a simple mental
model so i think the whole challenge of
Asia in general is to hide the
complexity and make complex things
simple and that's certainly true of
instruments as its as it is of other
approaches but I see when do you really
wants to say something different you
want to have is because you want to
develop expertise you get a wonderful
not
long ago a few months ago at Stanford we
talked about this precisely there's the
walk up and oh I can soosh that and it
goes up on the wall and isn't that
wonderful and it is true wonderful but
think about so when I was little I
learned how to play the violin and I
pity my poor parents who have to listen
to me for the first several years
playing the violin because it's not
something you can walk up two inches to
play you have to learn but once you've
learned you can make wonderful noises um
and the whole idea of making it worth
while for people to learn how to use
instruments and become an expert opin to
play the only way that makes any sense
is if instruments are separate from
whatever random version of whatever
piece of software that Microsoft is
given your Adobe or I'm very anti but
but but the idea is that you the other
piece of that's wonderful about this
idea of instruments is that their
incremental e-learning and that you
should be able to develop expertise if
I'm a touch typist I want to have that
ability transfer everywhere I type I
don't want to have it just touch type
this way here and touch type that right
there which is precisely I mean I know
you know this point because you made
wonderful that I just I'm sort of like
just want to make sure that I mean the
learning and the ability to anybody
expertise and to have tools that
encourage virtuosity i think is very
important you know one of the things
that was that I like the most of the Chi
conference was none of the talks and the
new research that I saw in so many I
didn't see many of them but it's the
talks by by his two card by Terry
Winograd by Larry Tesler by bill Buxton
all of them had to do with historical
perspective on the research they've each
done and and if you go back to Engelbart
Engelbart was really against the idea of
things that are easy to use because all
the things the powerful things we do in
the world require training require
expertise we require virtuosity and so
we want to encourage people to develop
this virtuosity and not pretend
everything is to be easy like a
four-year-old can do it so I think I
think in the HCI will we've sort of been
lying by saying we can make things easy
i don't think easy is the right is the
right take on it it is giving more power
to the users to do things that are on
the contrary a lot more powerful but
through metaphors and and and the
affordances that make it natural more
than easy i would say is the question
there
we have an existing approximation
so on my
what units
sound you can play directly on keyboard
and you're done for you
injectors on the other side
but
various processing boxes international
rate is hardware software when you can
plug those into this
works
you can get really complex highly
romantical software control
when controllers whatever you want to
confront those and where you can go the
other direction you can plug in an
electric guitar is to make that
right yes yes being their senior I've
done that so is wondering you tell us
what's different about what they're
thinking from this model which to some
of this is already familiar yeah I think
I i well I think I think to some extent
there is some truth to that and it's
been driven by the fact that musicians
have seen the value of using computers
while wanting to retain a lot of the
expertise they have with physical
instrument and other working computer
music has been to turn traditional
instruments with a piano or whatever
into force midi devices and transducers
and that's not just for performance
music but also the the mixing consoles
and all these devices where musicians
really understand the importance of the
physical aspect of interaction and I i
think what what and you get this to some
extent where you know i can plug any USB
mouse in my laptop and it's going to
work so we have a lot of this in the
hardware side of input devices but it
doesn't translate as soon as you go into
software and i guess this is what we're
trying to get out yeah Wendy's
currently and and it really is related
to this idea that this notion of
instrumental interaction really resume
that word with these guys and so a lot
of things that we're trying to do it's
looking at that but they don't follow
all the way through with that either
composition process for it they're
better at that four points we're working
with hope that's a whole other talk okay
Clark well we can thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>