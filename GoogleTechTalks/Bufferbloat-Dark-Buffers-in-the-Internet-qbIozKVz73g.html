<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bufferbloat: Dark Buffers in the Internet | Coder Coacher - Coaching Coders</title><meta content="Bufferbloat: Dark Buffers in the Internet - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bufferbloat: Dark Buffers in the Internet</b></h2><h5 class="post__date">2011-06-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qbIozKVz73g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">let me turn the proceedings over to
professor Cerf dr. professor
well good afternoon everyone my name is
Vint Cerf I'm your chief internet
evangelist in case you didn't know that
so may all of your packets land in the
right bit bucket it's a real pleasure
for me to introduce Jim Gettys who has
an interesting and varied background he
used to be an astronomer but and he's
been corrupted by computers like many of
us have and the consequence he joined
Digital Equipment Corporation and he
worked on something called Project
Athena not the one that you know of
currently but another one many years ago
in a galaxy far far away but in the
course of that work he was one of the
creators of the X Window System
something which some if you don't know
about you should learn about and the
reason is that a lot of what we do at
Google in a sense is X Windows on
steroids if you could think of you know
html5 as being a way of producing a
visible result from some remote
processing system which of course is our
cloud paradigm so his background and
experience is very much in keeping with
some of the philosophy that that we have
here he was the editor of the HTTP 1.1
spec which had a very important effect
on our business because it lets you have
one server with many many different
websites
whereas before that didn't work out very
well he worked on Linux for handheld
PDAs something which should resonate
with our Android experiences and after
that he became among other things
vice-president of One Laptop Per Child
which I think some of you will know we
were involved in to some degree in
helping to bring to fruition so he ended
up at Bell Labs
and he's he says he's supposed to be
working on immersive teleconferencing
but the one of the things that this
triggers is a desire to understand why
the network doesn't support accurate and
adequate teleconferencing and it is in
consequence of trying to use his home
network to do this sort of immersive
two-way interactive collaboration that
he encountered the famous now famous
buffer bloat problem and so I think this
even if there is debate on the
conclusions that the gym has reached on
this subject it's important to know
something about the symptoms which he's
uncovered because it may have a severe
impact on our own ability to deliver
timely services and we all understand
that Google is all about low latency and
so anything that gets in the way of
latency will get under Larry page's skin
that's not where any of us want to be so
if this is an issue that we need to do
something about Jim is the guy to help
us understand it so Jim I turn it over
to you and I thank you very much for
taking the time to join us today
this fully started as a as a personal
this is really a personal history I
really need to acknowledge many many
people whose health along the way and in
fact some of you have helped and I'll
provide pieces to this puzzle my
apologies if I've overlooked your your
contributions and we'll take it from
there then assembling a puzzle and the
way I characterize it is something that
many of you may have heard the internet
is slow today daddy I know I did with my
family again and again and I would go
try to debug my network and yet problem
would typically go away though a number
of times I was able to get it to go on
one enough that I was making support
calls to my ISP and all that sort of
stuff
so about a year ago I was I was trying
to prove that the antique blue box a few
of you know what that is and it's not
I needed to be thrown away so I was
playing with it and doing simple-minded
tests performance tests but also
monitoring my latency and I saw this
horrible behavior of one to two seconds
latency with very rapidly rapidly
varying jitter and since I'm tend to be
a systemic type I then tried the same
thing without the blue box I got the
same result and this was bizarre at the
time I was busy so I had to put it on
hold for a couple of months and I got
back to it late last June or early July
try to figure out what's going on in the
intervening time for the third time in
the last three years lightning had
struck near my house and all of my
existing home network kit had been blown
up again and that actually plays into
the story because I tended to have the
latest and greatest hardware whether it
be Ethernet switches or cable modems
with white also made it more
entertaining to try to debug um so I
want to take us back a little bit in
history for a moment to the 1980s and
90s in that era those of you who lived
it through it with me remember that we
had really severe congestion problems
all over the internet and unacceptable
agencies there was tons of research done
there's a whole set of algorithms that
go under the rubric of a QM active queue
management that were developed at the
time the most widely most famous and
widely deployed is one called red which
will play into this later but in any
case Brown the late late 1990s things
seemed to ease down and fart because a
lot more fiber got into the ground
so people actually caught up with the
demand at least temporarily and red was
deployed people were turning it on and
so in fact the congestion problems we
experienced relate went down to a dull
roar and so a lot of us myself included
included thought the problem was solved
this is probably old hat to most of you
in this audience I
sometimes I get surprised what people do
and don't know so one of the properties
of PCP is it will it will fill any
buffer just before before a choke point
or not give network paths given enough
time and there's an unstated design
assumption in TCP which is that if
there's congestion there will be timely
packet loss which allows it to react
it's a it's an end-to-end servo system
okay if you don't do that in a timely
fashion
it destroys the congestion avoidance
control loops in TCP and we judge other
transport protocols in general TCP is
just one of might a few as that they are
at least as good as TCP we judge they
may be reasonable to deploy on the
Internet
so now I ask the questions what happens
if this timeliness assumption is
violated by a lot and you know if you're
trying to if you're working hard and
trying to avoid packet loss lots of
Engineers think that draft dropping
packets is evil we should never do that
now should we but in fact in a congested
network it is as it is essential or
timely packet loss right now so we don't
have any cm deployed to to be to occur
or the network to function correctly
okay so you'll see this map a lot in
this talk I have to talk about things
all over the internet you'll see the
same map again and again the different
things circled the trying to keep you
from getting completely lost so here I
am at home on my four little laptop
talking to a machine to MIT it's almost
exactly this network configuration
though there were a couple more hops in
the middle and then this particular
diagram that's almost exactly what what
my initial tests were we're doing so
I've seen this problem in April and I'd
been trying to chase it but lightning
had done me in and and I was having
trouble
producing it so I try to find out what
could be doing funny things to my
network and I was suspecting Comcast
they call power-boost
and so it turns out that that rich
wounde is lives in the adjacent town to
me and I had already sent a mail saying
hey let's together get together for
lunch and the morning I was going to
have lunch with him I finally figured
out how to reproduce my problems again
well that lunch was really covered up
pile of topics and he handed me a bunch
of puzzle pieces one of which was hey
there's this big buffer problem Dave
Clark had warned him of this several
years before he'd been trying to chase
it and he'd never been able to prove it
pushing back on vendors to to keep their
buffering minimal but it's hard to do
without the proof and we explore topics
of how I might rule in or out the power
boost feature and so on he also told me
that to my surprise that red wasn't
always turned on I filed that away for
future reference that surprised me and I
thought it should be that there he also
told me there are issues around ECM such
that it's never been properly deployed
he also gave me a pointer to to the ICSI
net eliezer project on any of you
whenever you go to a new network should
as the first thing you do run metalizer
if you find out what's broken about your
network you've just sat down at it's a
nice little java thing that tests as
much as as the net eliezer group and
figure out how to test in about a 5
minute period and it will tell you all
sorts of wonderful things so I highly
recommend it that dent is how I I was
able to know that even without looking
at my interface that ipv6 wasn't on
Google Wi-Fi so the real smoking gun I
got was the next day after after lunch
and this is a wonderful tool called
smoke ping and so I happen to be copying
or are sinking the old excellence or
diem archives from my house to MIT over
this 10 millisecond long path if you
look at this smoke paying is both
reporting Layton sees averaging will
above one second okay
along with bad packet loss just while
copying a file are actually a whole pile
of files about 20 gigabytes of stuff and
the only times when the latency is low
is because trying to surf the web or
read mail was so painful I occasionally
suspended it just so I could do
something else this is the daddy the
internet is slow today problem and you
can do it to yourself I was doing it to
myself
I just realized on how to reproduce it
forever so in any case I decided having
in a previous life that I probably
should a packet capture and look what
was that was going on does this look
very bizarre to me kind of a second path
I should see maybe a millisecond or two
of additional latency but not a second
of latency so I mean I just took
Wireshark and there were these bursts of
really strange behavior I could see
periodic first I could see that just in
just in in smoke king varian in where
shirt and so I resurrected after several
weeks of looking around my old skills at
TCP trace and and expletive light it up
graphically so you can actually see
what's going on and so I did that and
what I got this is by the way what
spurred of an optimal single TCP
transfer it supposed to look like on a
properly functioning network instead
what I got was this okay there are a
number of things to note here why is
there something like a quarter half
megabyte of data in flight on over 10
millisecond why is it doing this
horrific bursts things which is exactly
lined up to all the weird things I was
seeing in the in the packet capture all
the sort of stuff just this is over a
period minutes by the way why is it
oscillating it like ten-second periods
and this looked like no TC
I expected at all never occurred that
way okay so the next week I went down to
to New Jersey and played around with my
in-laws FiOS service Hey
and so where I am now is playing around
with their with their home router
basically provided by the FiOS guys and
both wireless and wired and doing the
same sort of capturing the same sort of
data it's not as clean because I wasn't
able to walk the family out so there was
some cross traffic but I got this out of
that guess what it's very much the same
sort of thing that happens to be several
hundred millisecond latency over over a
20 millisecond path you know again this
is bizarre and over wireless things were
even much worse so if you look at it in
detail it has the same sort of signature
but again 400 millisecond kind of Layton
sees out of this and the like so I'm
seeing this stuff in cables not working
right now FiOS isn't working right
what's going on here guys
so at this point having had to stare at
TCP to some extent in the 1990s I knew
enough of the right people to call for
help and I did to a bunch of people
including Greg here and later dick sites
but a bunch of other people as well who
looked at the traces in more detail van
looked at the data as well it turns out
that since I was using Linux on both
ends there were even timestamp data in
data sets I already had so and after
about two weeks of discussion the CC
dent on this we even had to accept that
congestion avoidance was had been
defeated by all of this note that
pacifying traffic can't really help you
there's only a single queue in these
broadband devices there's no way you
know the telephony services that a
carrier provides and ISP provides is
being separately for
from your data service you have no way
to cause your VoIP traffic to
preferentially go over your data service
and any of the broadband stuff right now
so you know and it fundamentally can't
help you all I would do at most is to to
move who when and where you got yet aim
a little more later okay
here's here's the key data set for
broadband at ICS I and I'm very I'd like
to thank them very much for for the
ability to show their to use their data
this was stuff published last June at
Nanog and again later this fall and this
particular set of diagrams is split out
by by different technologies you can see
cable DSL and fiber and it turns out if
you look at these carefully you can see
that there's bad latency in all of the
technologies and in this case green here
is half second latency everything above
this is North half a second now a normal
telephone a standard for good audio is
actually only 150 milliseconds so if we
took this drew a line where 150
milliseconds would be it would actually
be back here now the other thing as this
is a lower bound it turns out there was
a bug of metal laser and it would
sometimes miss identifying the buffer
sizes the latencies are often much worse
this by the way these black lines are
for second Layton sees okay so the
broadband is edge and all the
technologies is broken the only question
is to what degree not a not a pleasant
situation what triggers this all you
have to do is to saturate your path got
to fill the buffers those that induces
the latency the latency while they're
full so this can be all sorts of things
youtube uploads or I once caught Google
Chrome with my permission
uploading instead corpse to Google for
crash analysis my network went to hell
and I looked at what it was doing in the
background and by god Google Chrome was
uploading its corpse
email with large attachments to get
torrent pop that later
file copies backups potentially things
that exists lots of other things did
here tall conferencing so on and so
forth and I'll talk more about web
browsing in a little bit which is
another interesting topic in and of
itself in this area
okay so latency isn't the only thing
that happens here this is also inducing
abnormal packet loss but not hugely
dramatically so but it seems to at least
in on my circuit do so in bursts which
is not so great but the this causes
timeouts and lots of little protocols so
if you've wondered where you've seen
occasional DNS lookup failures or or
DHCP failures on on busy networks this
is pretty pretty good reason to believe
it's this sort of stuff what you want to
do is to think what protocols are you
involved in which have those
presumptions of timely behavior that's
now being violated by by the by buffer
blow of course gamers hate latency they
know that's more than almost anybody so
ICSI has proven that broadband edges
it's plain broken but unfortunately I've
gotten really paranoid and I think it's
almost everywhere and I'll try to prove
this to you now um and I hope you go
away as paranoid as I have been since
about last October or even before that I
feared in August and in that mail
exchanged what I was going to find when
I poked and unfortunately my worst fears
were confirmed so along the way I caught
my this behavior on my home router again
by a smoke thing here I'm I'm observing
eight second latency this has happened
to be a a time I managed to catch it
graphically I can induce a trivial as I
described in my blog so yeah so so I
wanted to understand
okay Broadband's broken now what else is
broken so I think the home routers were
broken so I started doing experiments
just around my home routers and like so
the cell eventually having tried three
or four commercial home routers all
which were lousy with about this I
decided I really wanted fit with to play
with things and I've been talking to a
number of people including Ted chaough
and the like along the way I'd already
understood that other buffering like
Linux is transmitted hue might be
involved and so I installed open wrt so
I could actually twist the knobs on my
own the router and I remember the day I
twisted the knob and absolutely nothing
happened two days later I finally
realized that now I was copying from my
laptop upstream which means that the
bottleneck is the 802 11 link which
means the buffers are in my laptop and
as soon as I twisted the knob on my
laptop latency so any time you were
Broadband's bandwidth exceeds your
wireless transfer rates at home the
bottleneck shifts okay and so you think
you fixed in one place well there's lots
of other places it can be doing so I
already have this problem in spades I
happen to have a house with a big
chimney so it's really easy for me to
get lower bandwidth at times then my
broadband particularly since in the
process of doing experiments I up my
service to higher broadband service ok
well what's going on here of course is
that since most of these home routers
now are based on general purpose
operating systems we have that's where
this is coming from and modern os's turn
out to have a laundry list of places
where they like to hide buffers without
much thinking let's do a simple
calculation like 256 packets which turns
out to be a number we see pretty
frequently as a word or three million
bits so that's even at 10 megabits a
second is
of a second what happens when you're too
busy conference in your fair share is
one megabit per second or less you can
easily go from a third of a second to do
where nothing works okay yeah along with
the interesting pack of losses I also
think are probably occurring though I
love to see more real data on that so
you may say as I sort of sort of
questioning myself I don't I see this on
Ethernet this is an interesting story by
itself so you do actually see it
trivially on mac OS x and linux on a 100
megabit Ethernet I just plug my gigabit
nic into a hundred megabits which
copying just to another machine
saturated the link and it turns out that
our Ethernet drivers typically have 256
packets of ring and they're 250 packet
entry 56 packet entries roughly plus or
minus some in in their interfaces
windows is particularly interesting
because it doesn't happen to suffer from
this I wandered around Microsoft's web
site after an hour so I discovered that
they they almost certainly ran into this
problem didn't quite understand it but
put it in a very pragmatic thing to do
which is if your by default windows all
versions rate limits how fast it
transmits to keep from saturating 100
megabits which that explains that and as
soon as for example I I put the NIC to
10 megabits per second on Windows I saw
a border 250 packets worth of buffering
and the driver from the dole I use a
simple computation so where does this
hurt us well it hurts us in lots of
different places essentially in your
machine rooms though you know often not
in our handsets potentially and all over
our laptops and the like so it was a
grave surprise to me to understand that
that this kind of cue management is
necessary and buffer management is
needed in our host operating systems and
it is not there currently
okay this is a little bit more
complicated but it's really back to the
1990s congestion phenomena um if you've
got a lot of people sharing the same
network and there's a lot of buffers all
over they can start to interact and
that's indeed what we see in 802 11
environments and 3G kinds of
environments so you can see phenomena
where the latency goes up as more and
more people use the network and you know
peaking at some point people timeout
before the packets go away
so so the time the packets may get
delivered but the people have timed out
so you weren't observing packet loss in
the way you thought it was their loss
was the persons moved on not because the
network didn't necessarily transport
them this is exactly what Dave Reid
reported on end and interest about 18
months ago and got flamed out about so I
now have I now know that this is true
for in 3G networks both in the RNC's and
also in backhaul networks so there are
problems in multiple places there some
of our carriers have real problems
they've not fully understood I believe
okay so you'll remember that one of the
puzzle pieces handed me is all Reds not
being used everywhere and I want to
understand why that was or other aqm so
in August or so I called up van and
asked him not a wonderful story and
several of he tried it Kathy Nichols
about ten years ago wandered into his
office and proved to him in a period of
an hour or two that that the red
algorithm is fundamentally flawed and
that result is as 100 papers in the
intervening decade have proven it
requires careful tuning and to be used
effectively since if you tuned it wrong
it can hurt you some network operators
have been understandably leery of
turning it on when they should and
therefore some networks are running with
Hugh management to keep the buffers
under control and the routers and some
are not
you can get some of the some of the
ideas of what's wrong with classic
red by looking at a early version of his
first attempt to publish if that did
happen to escape onto the internet
called
red in a different light the first time
they tried to publish it yet the program
committee didn't like the fact that that
he used a diagram of a flush toilet to
explain explain the end-to-end
servomechanism in in the in pcp the
second time i was a blinded review they
got back reviewers comments which which
said this can't possibly be reasonable
the authors of this paper should go read
the fundamental work on the subject EG
flight in jacobson 1993 shall we say
that ban tried but you know it didn't
happen and so now i'm in the unenviable
position of trying to to encourage them
to get this paper finished and out the
door um as it has a much more cogent
explanation and a potentially a better
algorithm than then classic bread so
where are we seeing this well we're
seeing this in any place you've got a
lot of wireless networks and you see
hotels you see it in slum ISPs there's
about there's a paper characterizing
residential broadband networks which
specifically tried to look for cue
management and the head ends of the
different technologies and it looks like
from that paper about one-third of those
of those head ends are running without
an EQ management as well then we have it
in 3g as i've already mentioned i have
no idea what the state of LTE is but
i've also three years ago was seeing 22
second round trip times to over a
satellite link to the Ministry of
Education in Peru I mean lots of places
that it ought to be that it isn't it
hasn't been running in any case let's do
a little bit of trivial math let's this
is to try to convince you that given the
huge dynamic range of modern wireless
networks you need to be very careful
even when you're sharing that busy
network and
be only getting a few megabits per
second even a packet or two of buffering
is significant but what happens if you
go to 25 packets or or more and then
unfortunately there are various things
that the you know - committee thought
we're good ideas that often are not like
trying to retransmit excessively and
stuff like that so that's a different
discussion I'm not very expert at in any
case what you get out of all of this is
a really ugly behavior and it's really
complicated when you actually see it and
look at packet traces you see this the
conference's using you know 211 schools
and hotels all of us have seen the
cesspool of all networks and that sort
of thing this is sort of a a we had the
OLPC mesh network and even non mesh
networking meld and we didn't understand
we actually had to set up a full test
bag to fig try to figure out what was
going on under load and we did not
understand what this was going on I now
believe it was one of the three and
probably the second most important
phenomena we were suffering with maybe
the most important hurts me to know at
the time so this is the laundry laundry
list which of course the real laundry
list is much longer than this you think
about places where packets can hide
their buffers can hide you're almost
certainly correct
so wireless chips themselves may have
multiple packets of buffering in the
network device drivers may squirrel away
one or more packets and there are the
ring buffers and that sort of thing
there's the if you're running a VM
system on top of another then you've got
two stacks potentially of a buffer is
one on top of each other you know so you
have problems obviously in your 3G
things your backhaul networks just again
and again even even what you think of as
Ethernet switches have buffering and
then they have to to function correctly
they've read pointed out to me remember
the number correctly he said well you
know to do a gigabit on a loaded network
and not brought packets on a switch you
need eight milliseconds of buffering
which I haven't thought about but it
makes sense
um so in any case
this gets to be really ugly so the there
are other places you need to worry about
I was using smoke ping and inside of
Bell Labs monitoring our network and it
turns out that we have very
sophisticated classification for VoIP
and a number of other things on all
configured on our routers and no queue
management so nothing is signaling and
controlling the queues by telling the
endpoints to slow down and for various
reasons this is going to get worse I'll
go into that I think a bit more in a
moment
ah let's see we've had there are tunnel
devices where you can have this I seen
this and our IPSec infrastructure the
bottleneck there seems to be where it
touches ground in and the firewall
complex as far as I can tell them from
the round-trip times I expect to find it
in other places question marks or places
I haven't actually looked but expect to
find it you know things like like
encryption buffers or relays of various
sorts you name it
so buffers are only detectable when
they're next to the to a saturated
bottleneck at all other times the
buffers empty and don't hurt you so you
can have buffers throughout your network
and you only get it when if and when
that that buffer becomes adjacent to a
bottleneck all the other times they're
darked they don't seem to hurt you they
cost you money and power but but they
aren't actually causing hurting latency
so you have lots of these sorts of
things in in so I think I've gone
through most of this so yeah I hit that
button long so in any case where do we
have it well you know as you can see our
poor network map is getting awfully busy
with circles isn't it you see why
paranoia well I've gotten really pretty
paranoid
okay so oh I don't know it's two or
three four years ago I forget exactly
when I was noting that the browser guys
were were changing how many parallel TCP
connections they would use originally an
HTTP we put a limit in the spec saying
please don't use more than two TCP
connections simultaneously the reason
why we put that in the spec was at the
time there were insufficiently buffered
dial-up modems in common common spread
use in the 1990s and so if you ran a
pile of TCP connections at once you
would get horrific packet loss when too
many packets arrived at these dial-up
modem banks hello good evening
so we said guys you know be friendly
I'll do something nice so when this when
the browser started sprouting six or
even more connections in parallel
recently I thought about it and was
nervous this is three or three years ago
but but I had no reason to worry about
it much well unfortunately it's a real
problem and it's a problem that's now
being compounded at the server end by
the changes that various people are
making or want to make with the initial
congestion window to raise that from for
I guess the current proposal is a Turk
and I don't know the exact state of
things and the Google complex last I
knew was was set at 8
due to a mistake in a and I found out
the ITF there there was a blog post
where AB was somebody who had seen this
noted it the the Microsoft web site was
at infinity that turns out to have been
a broken load balancer and they fixed
that so please don't we're almost load
together please don't don't flame at
them at this point I've been assured
that it's been fixed or is being fixed
the problem is if you go to an image pay
image heavy web page what happens the
browser opens up a pile of TCP
connections and simultaneously makes a
request on each and back comes some
number of packets at least of order 4
and of course if the congestion windows
larger it may be more like 10 for each
of those connections
but of course which started websites
things like that it may in fact easily
be many more than that some browsers in
some circumstances doing more than six
connections at once it's easy to get a
hundred or more packets effectively
coming back at your four little lonely
little broadband connection to arrive
essentially simultaneously and they go
splat now there's enough buffering there
that that these buffers that are in our
broadband edge will absorb these these
huge impulses but it takes a very long
time for the bits to trickle out from
there and so even on a 50 megabit
broadband service that I now have I'm
observing at times 150 milliseconds of
bursty latency when I get an image heavy
web page now if you ever have delusions
you'd like to have high quality
telephony or voice or video
teleconferencing is I would like to have
and I think many other people here would
like to have you'd sort of like if if
the other people around you or even
yourself in routine web browsing didn't
cause dropouts in your in your audio and
video and if you didn't have so much
latency in your connection to get around
it that it's like talking to the moon
which it often is we tried to deal with
this a long time ago an HTTP 1.1 with a
thing called pipelining it's never been
widely deployed in browsers almost all
servers supported to one degree or
another opera has has had support for it
a while the the Missoula folks seem to
be having some pretty successful
experiments with finally running it
recently only a decade or more later
than it should have been there's a lot
of brokenness that has crept in in the
meanwhile so there's yet more band-aids
that you have to do to work around the
broken web sites in any case I want to
warn people both working on the web
server side and the browser side the two
together are multiplying guys we need to
step back and think about this
multiplication it's not good
to bump from two and four to go to six
or more and ten big difference guys and
we're in the middle of doing this just
good
I don't think so okay so then there's
some other interesting subtleties about
all of this as I was thinking about all
of this I realized ha actually actually
as Dave Park I guess in conversation
said oh yeah BitTorrent we do this too
so I believe and nobody so far has come
to me and tried to claim I'm not right
that at least part of BitTorrent
problems were incorrectly diagnosed in
all of this the buffers were already
very large in some of these devices when
BitTorrent deployed and it turns out
that Windows XP does not enable window
scaling so it never gets more than 64
kilobytes in flight at once so it
actually takes multiple TCP connections
to saturate a typical broadband device
so I think they misdiagnosed this and
the fundamental regulatory thing from my
point of view is that operational
problems in the Internet should not be
able to be kept secret I think they're
they were getting more and more
complaints from customers or costing
them in the ear with with with more and
more service calls from the kids who are
downloading whatever and their parents
were following up as a networks program
I don't so that's sort of point one I
think this helped trigger I've looked at
the timing I think this is true I'll
trigger the whole discussions we're now
having sometimes Yemen around network
neutrality the second thing is it turns
out that all of our broadband
technologies actually don't use the data
channels for telephony services DSL it's
an analogue split the cable guys have
put it on a separate channel same thing
as true I'm told about fibre so this
means that that conventional sip VoIP or
Skype is that a fundamental disadvantage
right now - - what is he can provide I
don't believe this was intentional I
know the network the the conspiracy
theorist will have a field day I really
don't believe them okay I'm just been
arguing with
with such a person recently and I've I
think convinced him um but the
fundamental issue out of all of this yes
yes if we want to deploy anything with
reasonable Layton sees if we don't roll
up our sleeves and get this fixed we are
going to be able to play these services
and have them actually work we've got
more and more XP is going away so this
higher and higher fraction of of
applications are able to trivially
saturate these links and we have more
and more applications that are
saturating the links this is getting
worse
okay so what can you do about it so some
of you by the time you go to bed tonight
it's somewhat of a pain Ulis when I say
you I'm talking about people in the
audience here the typical hacker here
can start to deal with this they're not
ideal mitigations but you can help you
can do this a lot quite easily with a
lot of the middle range or high end home
routers have you'll find buried away
under the QoS section sometimes
sometimes it's called bandwidth shaping
knobs that you can twist and you want to
set them below what your ISP actually
provisions for you if you do that you
can keep the buffers in the broadband
gear from filling okay at least under
many or most circumstances now
this unfortunately defeats things like
power boost switch or turbo boost
whatever they call it I never get that
right it also can't fix everything in
that broadband connection so yet but you
can go do that immediately and I'll show
you some results of that in a second and
I'll obviously educate people that well
you know you probably need some cue
management in your piece of the network
you better turn it on that's always
entertaining because people who deal
with operational networks are are
justifiably very reluctant to go messing
around with their router settings
particularly with anything that might
ever conceivably hurt them we have to
fix we need better queue management
algorithms part of the problem here is
that
red is not a great algorithm and yet van
says it has no prayer of working in 802
11 there are several other alternatives
that were exploring but we need to do
get moving on this so let's see so that
sort of thing so this is where I started
okay this is the first the first mode
thing I showed you
so having twisted my knobs comes with
some cost of my bandwidth significant
cost of my bandwidth what I end up with
notice this is still one point two
seconds so now I end up with this got a
bit nicer guys okay so there I am with
you know variants of little you know ten
or fifteen milliseconds that's more like
it guys the twenty milliseconds this
happens to be the from where where this
is being observed from so so it really
has made nearly tours the magnitude
difference in my latency and jitter so
that's sort of nice and many of you
could go do this today first thing you
do is run metalizer when you get home
and it will tell you things like
buffering and there's also the
measurement lab tools which are
specifically trying to test those the
specific thing okay we have yes Dave
yeah that's what I did myself to make my
home network work well okay it's not the
end of the story just love the what the
wireless hop I'm just that was just to
make the broadband problem go away okay
I still got the home router and host
problem okay which we have to fix that
too we have to fix our operating systems
that's why the home routers are broken
and so on as well so but if you
carefully make sure that you always have
more bandwidth on your wireless link
than you do on your broadband link the
bottleneck will always be in the
broadband link so if you can't control
something if you can at least shove the
bottleneck into a place where you can
control it you can win temporarily
there's nothing there's very little new
here Dave this is this is assembling a
jigsaw puzzle that we've defeated TCP
congestion avoidance that's that's a
real surprise or a few little puzzle
pieces I think we knew it's assembling
the pieces here that's the that's the
whole bit here so this whole thing is
getting worse
there's this downward compatibility
constraint that hardware vendors have
we've gone through multiple generations
of Ethernet and so the values that were
selected for to make things go fast over
long haul big pipes to supercomputer
centers that's where the knobs were
we're tested because that's where the
research funding goes has been to go
fast and has caused us to set our knobs
for a condition that in fact we
typically don't have and each generation
it's been getting worse it's not even
been scaled by the amount of bandwidth
of a particular media at all so we have
this in Ethernet we have this unit 211
DOCSIS so on and so forth so that's a
piece of it
the next is more and more apps are set
are saturating including web browsers in
little bursts are saturating the edges
XP is going away so if that's that's
good in a lot of ways but this is why
you may have heard reports of of Vista
or Windows 7 forming more poorly I know
I've seen some of these reports more
poorly on a home network than Windows XP
did ok
there's and of course the other thing is
that memory is now just for most places
dirt cheap and so people don't even
think about it ok so so you can't get
memory chips small enough if you did so
there's always too much memory so why
don't you just throw it in there seems
to be the added the other major another
major problem is that our our typical
tests are not testing for latency under
load and so it's not visible to people
so just go to views go to speedtest.net
and copy a file from your house to to
your favorite Google server sometime
see what speedtest.net starts telling
you then you won't like it but the tests
aren't doing that needless to say we've
had certain conversations with certain
people about trying to cause some of the
tests to be make this obvious this is
not hard to test for then there's this
other interesting phenomena we know is
going on if you have a hardware vendor
who's making hardware and their hardware
and or firmware is slightly buggy if
they can paper over a performance
problem they have to make some arbitrary
bandwidth spec by doubling or
quadrupling the buffer size memories
free we already have it on it won't cost
us anything we'll just turn it up to to
get that that bandwidth point we I know
this is occurring from talking with
people who are in the position of
putting people onto approved vendor onto
an approved vendors list for a major is
P so that phenomena this is the one that
got Dave Clark really upset this was the
thing where he realized we had to start
so we really have to - and I'd love
people to think about this what's the
right marketing metric they've stolen
the speed word from us speed used to be
really latency but it's really you know
they now they now say that that the
capacity bandwidth is speed it's not of
course what's the right marketing metric
do we really need two numbers or we can
invent one they can both somehow capture
bandwidth in relationship to to the
latency as well Bob Brisco had some
interesting ideas there and you just
push on a bit more to see if we could
get to a simple nice marketing number so
we really have to change the marketing
dialogue so that when people go into
into to buy get in a in a store that
that's one of the features on the box
okay that sort of thing well what's the
right figure of Merit we really need to
have something simple change the
marketing dialogue
well it gets complicated we can have a
larger discussion yeah I mean is latency
over which path begins at it because
there you only measuring the one hop or
right so you see the that's the sort of
thing so I was at the IETF I gave
essentially this talk an abbreviated
form at the transport area talk this is
some of the stuff that that I put
together for that talk of what the ITF
should do people here have a lot of of
these puzzle pieces that they can help
I'll worry about you can immediately
start mitigating problems various places
please do obviously worried about your
your handsets both both wireless and
cellular biology rias sorts you might
think about them market pressure on
vendors you're in a good place for that
two browsers
well we already had a long discussion
about Brian's browsers I'm like either
that's an a I'm but but it is 1.1 is
already hugely widely deployed I don't
have anything against speedy at all 1.1
is out there in the long term too but
their short term things you might be
helpful in the short term the evil the
enemy of the good is the perfect
it turns out that ultimately the latency
starts being more dominated by total
number of bits and running lots of TCP
connection starts wasting you bids yeah
I understand I mean look look I wanted
something like I tried to get people to
worry about something like speedy in the
1990s I'm not you know fine just get it
done is the point not arguing against it
at all just we need it out we need
things like that out there be that as it
may in any case there's lots of research
to do here and we deploy you see it as
an interesting question
I'm giving it's working this out there
that would be wonderful to not
necessarily throw packet away
particularly when you're talking to
various wireless guys the concept of
dropping a packet on the floor they've
sweat blood to get across on some highly
lossy link then go down very well just
politically I can tell you if we can if
we can apply ECM safely it's going to be
a much easier sell on certain parts of
the industry so what all is going on
here I'm trying to wrap this up and get
into general questions then to ask that
I start just getting it out there
because conventional publication would
take too long
um so I gave talking Murray Hill and at
the idea I'm here this week because you
know so it brings you here and then to
ask me to do his next upcoming in I
Triple E internet computing editorial
whatever that's called I forget what's
called XPS or something there's a cmq
case study being put together for this
on some of those papers will end up in C
ACM we've set up website which has you
know which has bug tracking and mailing
lists and all that sort of stuff
one of the possible aqm algorithms is a
thing called stochastic fair blue that
had been sitting out of tree in linux
for several years it's now actually up
tree and in the latest Linux release we
need
start testing that van also pointed us
at some very interesting buffer
management work done by folks in Ireland
Panji Lee and the other guys names Julie
is someplace United States now an
algorithm called EB D P has been
reimplemented and and needs testing I'm
actually running it on my Linux machine
right now it does seem to be working
better than the well we had a little bit
later
John Linville who's the Linux Wireless
maintainer is maintaining a deep load
testing tree so we can integrate patches
for people to not have to necessarily
catch the kernel one patch at a time for
things they've taught who's in the room
here has been working mightily recently
to to get us to the point where open wrt
can take that kernel and start start
running morgy bloated we need lots of
help from people on testing this stuff
and making it better
at the moment we decided there for
various reasons since we've got both
operating system problems and device
driver problems both to try to keep our
life simpler so the initial home router
target is a wmdr 3700 it cost about 150
dollars you can go buy one today pulp
the cable industry has already started
to deal with mitigating buffer bloat at
the ITF I found out that they've they've
at least now are able to control what
the or we'll be able in the future to be
able to control how much buffering is in
the cable modems from the CMPs and so
the problem will get somewhat better it
won't be solved rather than getting
worse because they were already testing
150 to 300 megabits that would have had
buffering they would have had buffering
that when plugged into say a 10 megabit
per second or provisioned a 10 mega
second would have been loaded by factors
of 15 or 30 and at minimum so this is
this is at least one step in the right
direction is already happening or for
the yeah for the cable guys I applaud
them for that there's just tons of work
to do all over our systems please come
help you know please help you know in
all the different places and
so it's all we all are together in this
in this problem so what's the picture so
what's been killing latency in the
internet it's the dread dread puffer
fish for those of you don't know about
about puffer fish
they are highly poisonous that's what
they're what the Japanese call fugu
which is which people sometimes like to
slightly poison their lips with and
sometimes more than their lips so that's
been poisoning things as be its bloated
its spiky you know that sort of thing so
that's what's been going on and it's
time for general questions yes Dave how
can YouTube help me well and we're
recording this talk today so we need to
get it up on on YouTube what can you
change that's an interesting question
when I was when I gave this talk at
Apple this morning they there was a
person in the audience there who said I
should draw to your attention that that
YouTube seems to be sort of piling as
many packets as possible into into the
network all at once and that the flash
player that Hulu uses is actually
friendlier whether that's true or not I
don't know but it may not be true as I
said I don't know you know so so yeah
well and of course all these things well
you've got big buffers hidden in various
places means that all your all your
control loops of trying to figure out
how they're supposed to behave are gonna
be really hard to make work well and
they're gonna be very all over the map
because the amount of buffering is all
over the map okay
um yes Nick
yep and that poisons all these little
protocols the DHCP the RA the you name
at all the little background things that
don't take much bandwidth they actually
care about those delays yes sir
okay so the story I heard from Dave ran
is when I met with him and for lunch
he's an old friend last fall was that
order ten years ago there was a certain
Taiwanese vendor of home boxes that from
which a lot of the home kit was
descended for a long time and spread
where if it ever saw an easy end that it
would just out not crash and that's not
the only instance of that we've seen
some other instances there used to be an
ECM Hall of Shame which unfortunately
has gone missing on the web the database
rotted or something
it used to be somebody is keeping track
of the broken stuff
Steve Bower at MIT has been studying
this in a really major way he's he's
probed the top million or so alexa
websites to see what's going on there
and he's looking at some other stuff he
wrote did some preliminary reports on
this at the the Qaida workshop in
february my blog as a pointer to his
slides so you can see a bit of the
work-in-progress he's doing to try to
understand how much is broken the actual
state of deployment of on the server
sides going decently well it's gone from
about 1% capable of supporting ECM to
think of the number was 12 to 14 percent
in the last two years and if you look at
the timing of when the operating systems
released with with server ECM support
turned you know enabled in others this
is a server is willing to talk you see n
if the client says it wants to if you
look at the timing of that that's
actually not too bad a result it does
seem to be getting deployed and of
course particularly in particular point
areas like the handsets and the like
where the traffic patterns may be quite
different than from the home where a lot
of the broken kit may be we may be able
to use it we may not I don't know I mean
that's that's research that we need to
do to know if we can start really using
it if you'll forgive me for intervening
I want to make sure that we have a
formal thank you for our speakers so
that I can sneak out before more
dirt is cast at the TCP then already has
been cast oh by the way yep and I think
it's Philly's boppers UDP can just as
easily follow and I understand I'm just
making a bad pun
anyway Jim I really appreciate your
coming out and raising this is an issue
I hope there's something that we can do
to contribute to the solution so let me
formally close the session you can stay
here for as long as you can stand that
then people are wanting to ask questions
but let me officially thank you very
much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>