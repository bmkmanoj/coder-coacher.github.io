<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google Faculty Summit 2009: Statistical Machine Translation | Coder Coacher - Coaching Coders</title><meta content="Google Faculty Summit 2009: Statistical Machine Translation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google Faculty Summit 2009: Statistical Machine Translation</b></h2><h5 class="post__date">2009-10-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/y_PzPDRPwlA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Franz oh I'm here principal
scientist and director of the machine
translation research team here at Google
and so what I want to talk about is
statistical machine translation just the
progress that has been made in the last
years and in general in the field but
also here at Google and so why is Google
doing machine translation I guess you
have seen this slide probably today five
times and five different talks so
googles mission is to organize all the
world's information make it universally
accessible and useful and with making
the book corpora available to everyone
you can search that and the web is there
and Google is doing search and all those
things but the language barrier is still
a barrier which is very significant for
many many people if you speak English
you can access a lot of information but
many people on this planet don't speak
English and so if you only speak Arabic
you can only get access to one percent
of the information even if you you find
all the books in English and can find
them but you cannot really get access to
the information and therefore machine
translation is really cool we believe
too that in order to really provide
access and make it useful to anyone out
there so I want to talk about some
topics of the trends there so one is
that machine translation significantly
improved in quality in the recent years
and so just to give you an example here
is an Italian text and not sure how many
know Italian but it doesn't matter so
much so here is the machine translation
output and a human translation output
and normally people would expect machine
translation and it makes mistakes is
always funny and it's hard to understand
and it's confusing and all that and so
here if you read through those two
different texts try to find out which
one has been produced by a professional
human translator and which one has been
produced by the machine and I guess it's
quite challenging but some people
actually figure it out but it's quite
hard right it's it's both texts around
fluent both texts have minor mistakes
but but you can get a lot of information
out of machine translation let's for
Italian to English Italian and English
are relatively similar
which is so it's easier to do that so if
you look at the same text and look at
the translation from Hindi to English
then you see many more mistakes there is
lunar probe procurement or some word
which got transliterated which the
system didn't know but you can still
kind of get a rough idea of what this
text is about and can get access to
information in there it's just not as
fluent as it works for Italian okay so
what so now I want to go through some of
those things which which caused
significant step forward in quality in
machine tone that we are now able to
produce that kind of quality in machine
translation and so one key thing is that
now people are doing a static machine
translation that's what we are doing at
Google and what now a large part of the
research community that's working on
machine translation is doing so the the
basic challenge is natural languages are
very ambiguous a lot of different ways
to express things nuances in the
meanings of words meanings depend on the
context and and so to produce a
translation you have to make many
decisions you have to make decisions
under uncertainty and and what should we
do there where we should try to make
optimal decisions and that's what
statistical decision theory is about and
so the statistical approach provides
there all this advantage that we can
combine all the fuzzy and and arrow full
knowledge sources that we're having and
the key is that we then learn from
nature so we learn from large amounts of
parole documents and at Google we have
access to lots of different corpora and
can mine those for translations and the
more data we provide there the better
the translation qualities I will show
later some slides and then we measure we
are very metrics driven there then we
measure the success by performance and
unseen test data and when we implement
something we see if it improves and if
it improves we keep it in the system and
if it doesn't we don't do that so it's
it's now in the old days machine
translation was built in a very
different process than what's what's
done here now so I don't want to go into
the details there's not much time in the
details of how the 30
machine transmode all that we are using
is really working just a rough overview
so in the end it's a linear model that
we have all kinds of different knowledge
sources each knowledge source has a
weight associated with it we combine all
those and we try to basically find the
sentence in the target language which is
according to that those knowledge
sources the best one and and basically a
lot of the work in improving the quality
of the system is to provide extra new
knowledge sources and the key for the
key element in the translation model is
what we call that phrase table or the
translation dictionary and so it's a
starting point what we get is just we
have large amounts of parallel text
large amounts of corpora which are in
Chinese and English and from that we
have to infer now how Chinese words
translate to English words and the
approach ruler is basically to analyze
co-occurrences between those and then to
bootstrap from that if you have some
knowledge about some co-occurrences you
can iterate a few times over it and get
better and better estimates over that
and the infrastructure if you do that
over gigantic amounts of data that's
very natural to do that it's just
MapReduce and then you run that on a few
hundred or a few thousand machines and
you basically learn then how all those
words in all those languages that we are
working on relate to all the other words
and so it's part of that process we
basically know that not only what we we
learned and which word corresponds to
which word and from that we extract then
which phrases correspond to which phrase
in a very simple process which just be
memorize basically all the different
phrases that have ever occurred there in
those corpora and associate with them
some numbers quality metrics that tell
us how good is that phrase and and
depending on various other piece of
information in which context it fits
well and which doesn't and but but in
the end it pays get a very very large
table which is stored and which then
describes how to translate from Chinese
to English for example so a phrase which
has been seen for example many many
times that's a good phrase or Fraser has
been seen in many
different types of text or many
different web pages or so that's for
example a good phrase but there would be
other metrics like if the phrases before
and after words are aligning well then
the phrase which is in the middle it's
probably also a good phrase right poetry
is one of those things where if you send
it into the system you get new kind of
poetry out so it's it it adds to it in
interesting ways so so that with respect
to the mall and there's a lot more to be
said about the way those models work but
but I think the essence is it's all
data-driven from from parallel corpora
and it's in the end it's relatively
simple models it's and the key is there
that you do it over gigantic amounts of
of training data
another key which was introduced there
is that we now use in the in the field
in general automatic evaluation metrics
so in the old days if you try to
implement an improvement in a machine
translation system you put something in
and then you had to ask human raters in
order to know whether there was a good
translation or not so now what we are
using is automatic metrics where we
compare the translation that's produced
by the machine translation system to
human reference translations also on
large amounts of text and then we
basically say if it's very similar then
it's a good translation if it's very
different it's a bad translation and the
key is this metric is not perfect but
it's good enough in order to drive
progress in the field so we have a very
fast development cycle then if I have an
idea I want to implement that I can
verify it and and get a translation
quality very quickly it's very easy to
compare and assess results if one
research team is presenting some numbers
others can compare to that very cheap
evaluation and another key is it allows
and direct optimization for that once
you have a metric which you can
automatically compute you can also tune
your system directly to basically
optimize directly with respect to
translation quality
so the web has actually large amounts of
pearl texts for example so if you go to
the United Nations website they have all
their resolutions and all the meeting
nodes and everything is available in in
all the six languages right right it's
much smaller than the whole bit but the
whole web is so gigantic that the amount
of Perl data still is rather significant
and then United Nations you get for
example there about two three hundred
million words for each of those
languages and then parallel and true 300
million words that's so for those
language that's a pretty significant
corpus and I'll show later some numbers
which show basically how much data you
need in order to build a decent system
it's all then I know a special kind of
language United Nations kind of language
which might not be the best thing if you
want to translate poetry but but but
still it's it's then the key then there
is it's good enough in order to get
understand what the text is about so
here now two more parallel data is
better data so what I show here is a
bunch of numbers which which says if you
have 1 million words of Perl text or 10
million Perl words of text for different
languages Portuguese English French
English what's the improvement in
quality and the metric that shown here
is this blue metric where larger numbers
are better and it's a metric which is
from 0 to 1 and and about point two is
is a level where than the translation
quality becomes reasonable but then we'd
be willing to basically give that in
front of our users and say that's good
but but language like the Hindi language
also that's then above this level but
quality is not as good as we want it to
be but but if you see that with 1
million words just very few language is
just Portuguese is above this level but
then with 10 million words many more two
more languages here jump over it but
then if you just go another factor of 10
and add more data then all languages are
above this point to line or than if you
go to a billion words then
even more all of them reach now 25% and
so you see the curve seems to slightly
go down in slope a little bit but it's
still if you if you add more data it's
always gives you a better translation
quality so that's one thing ah well
Portuguese is a language which is rather
similar to English in the sense of its
its that has a very similar sentence
structure and the Romans invaded Britain
and therefore against the English
language deviated from the Germanic
roots and went more similar to the
French and Italian and at least with
respect to what what measures for our
statistical translation model and so
those are just very similar ones the
other ones like finish for example it's
very challenging because it's such a
rich morphology especially for small
amounts of training data then this is
really challenging because many words we
just have never seen but it's
interesting then that finish also has a
steeper slope that if you have a lot of
training data then actually it's good
that you have so much information in a
word which we're in finished it's all
the the changer and and and what what
case and descendants is having it's all
embedded in a word and that is actually
helping then a lot if you have large
amounts of data but to some extent it's
also we have to be a little bit careful
those numbers are all actually on the
same corpus they as comparable as you
can get them across different languages
but there is still a problem that then
the data has been produced by different
human translators right and then it
might also be some of it here is that
the Portuguese translator was more
consistent than the French one for
example and therefore the gap is there
but still it roughly gives an indication
about also the quality of our online
system our Portuguese system is one of
our best ones so another graph here
which goes to up and to the right is
basically improvement over time so this
now includes basically data and
algorithm improvements and those are
some of the languages which we
launched recently so hindi thai and
hungarian and there you really see
significant improvements over time that
then so in there you see we find more
data for those smaller languages and
it's quite challenging for some of those
languages to find data for chinese large
amounts of it are out there it's one of
the United Nations official languages
and so on but for Hindi or Thai there is
not so much data so our algorithm has to
be searched deeper in the web in order
to find those texts but as you can see
there is another very nice progress over
time for for many of those smaller
languages for which so far it was very
hard to find machine translation systems
out there so another curve here it shows
the same thing for the language model
where basically so here now the amounts
of training data for the language model
language model is basically the model
for which we touch with which we touch
the waveform nets of the target sentence
and it's just an Engram language model
where we count Engram to compute
probabilities based on how often
something has been seen and as you see
here so there are some details in that
graph where I don't wanna go into but
the more training it whenever you double
the amount of training data for example
for for the very best amount of data
that we have here it's a news data we
get 0.6% absolute gain in that blue
score metric so and because it Google we
can double there quite often we get high
scores and layer in the edge so one
challenge then obviously when you have
those very large amounts of data is that
those models are becoming very very big
so the Freys table or the language model
that's a very large table but it has
hundreds of gigabytes of data that we
want to have random access at
translation time so one challenge there
is we cannot just fit that on on one big
computer or if we would put it on one
big computer that computer would be very
very expensive but what we need is to
have then many translation machines
which perform the translation accessing
one shared pool of that data so what we
then a large amount of work went into we
during a distributed language model or
the same thing than distributed
translation model architecture which
allows and very efficient access random
access at that time when we needed to
their data so a lot of work actually in
building the machine translation system
to produce high-quality exploiting those
large amounts of data is actually core
systems work to make sure that we have
very few millisecond round trips
whenever we need a language model
probability to get that back and to
patch those up in the right way and so
on so another graph here is and also a
big reason for the progress in the field
of machine translation is that there's a
huge excitement right now in the
research community many people are
working on in the natural language
processing field there were very few
papers a few years ago here 98 which
discussed that and now there's really a
few hundred papers every year that
discuss statistical machine translation
a lot of interest there and one reason
is it's it's relatively straightforward
for a very small team to build a decent
machine translation system by taking off
the shelf resources and the open source
software there there are parallel
corpora which are available standard
evaluation sets and and to build a
system with which is with which you Lang
can write a paper and do interesting
research with there was not possible ten
years ago where the rule-based systems
there was a very complicated system just
proprietary access at some companies and
so it was just very hard to do their
relevant research but now it's really
out there and PhD student can on his own
do a lot of interesting things so one
challenge then obviously is if you want
to use very large amounts of data how
can you do that that's a challenge if
you're a small research group so at
Google one thing that we have done there
is to provide the Google ngrams so which
is a set of five grams counter on a
trillion words on English web and made
that available through a GC so if anyone
is interested in that you can contact
their Luc it's a nominal fee
just and I think on five DVDs all that
data and what we are working on right
now and it should be available hopefully
very soon from ADC also it's a
corresponding data set with ten
languages ten European languages or all
of languages that are shown here and
also counted on a trillion words overall
but each of them counted on roughly 100
million 100 billion words and so with
that kind of resource then people can
build language models and can build
machine translation systems for all
those languages so so with that also we
hope that there's more research
happening that that builds machine
translation system translate from
English into other languages so a lot of
funding and especially government
funding US government funding that goes
into machine translation always focuses
on for example Arabic into English or
Chinese into English for us it's
actually more interesting to go from
English into other languages our usage
is much higher from English into Arabic
or English into Chinese than the other
way around because all the information
is available in English and it's all
about providing access to information ok
so the next step is now we can build
machine translation systems from many
more languages so here is a history of
Google Translate by the way who is who
is a user of Google Translate who has
used Google Translate ever not every
hand goes up so so here the Google
Translate code started in 2001 and then
we used off-the-shelf third-party
product rule based machine translation
system and then at some point we started
a research project here can be basically
built can we invent the state-of-the-art
in machine translation by exploiting our
computational resources the data that we
are having and and the computational
infrastructure that we are having to
really make machine translation better
to provide it for many more languages
and then in 2006 we started basically to
release 3 languages which were than this
our own statistical machine translation
and engine Chinese Arabic and Russian
and then since then we in
in 2007 october 2007 we replaced all of
our rule-based system with that
statistical engine and then moved on
every quarter we added five to ten
languages and now at this point the most
recent edition was Farsi we have 42
different languages from which you can
translate from any of those languages to
any other language and and now it's
really possible also to look at
languages where a few years ago it was
just unthinkable that any one bill would
build a machine translation system even
if you would have asked me a few years
ago if you can build a statistical
machine translation system for Yiddish I
would have said no that's just we won't
be able to find the data and and and so
it's it's it's it's not going to be in
the foreseeable future but actually what
we have now is a research prototype for
a Yiddish translation system so Yiddish
is a Germanic language written in Hebrew
script a very small number of speakers
it's one of those endangered languages
and there's just very little modern text
written in English but really the
question is can we build with those
techniques of statistical machine
translation system normally you would
say statistical machine translation is
not really ideal for this challenge
because where's the data right you need
a large amounts of data in order to feed
that into the system and so and actually
we also there is not so much data out
there we found a few hundred thousand
birds but then there is really also a
lot of interesting algorithmic work in
order to explore it similar languages in
order to improve a language like Yiddish
so here we so the dictionary is
available so you can add dictionaries
and the translation quality knife
improves Yiddish has specific morphology
issues which you can deal with but then
German Hebrew and polish are very
similar languages all the knowledge that
we have learned from those other
languages for which we have really large
amounts of training data you can put
them together and put into the system
and then it seems that a lot of text
also is misspelled that we found so
spelling correction is setting there
also a lot so overall we get close to
that level where we said machine
translation quality is
reasonable so here you see a translation
of our very current version of the
system which is actually even better
than those 19.5% here of the same text
that you have seen before
and it's I know you have some struggle
to understand some of those things but
still it gives a rough idea about what's
what's happening there and if you know
some German there are some funny kind of
if you sound it out you know what it
means even if if it doesn't seem to make
direct sense so which ones later that
into Yiddish so so so basically
therefore also we have the same text
available in all those different
languages so that's part of our test
data which we then had the same text in
60 or so different languages right well
it's basically some his translator
produced the text for us okay so another
trend is machine translation everywhere
so I might be biased but that's
obviously one of the goals that I would
want to wanna see so one of the things
is just for many years that's what
Google is providing since 2001 this text
translation interface where you can go
there paste the text in translate it
into some other language now we have
basically for those 42 languages from
any to any other one or you can paste in
a URL and then have the web page
translated to you but then what do you
do if you don't know what you want to
translate right you want to know some
information you don't care about that
you have to you first have to find it so
one thing that we have launched in
reason is cross language search where
you basically can type in a query and
then you can search a different language
so for example you a teacher SLR camera
and you search you try to query in
English and you search the chap a nice
web and then what you get is basically
search results in Japanese so the query
okay translate into Japanese search
results in Japanese and then translate
it into English and if you click on the
English search results you will go to a
translation of that page and can
basically find out information as if
that all were it
English or if you know some Japanese you
can click on the Japanese result and go
through there and so another one for
example the caption translation but
actually was mentioned already before
that for all those videos which have
online captions then you have this
option to just translate it right so if
an Arabic speaker wants to see a Obama's
weekly address which always has
subtitles you can just go there and get
the subtitles on the fly translated so
and there are many other integrations on
which which have been built and what we
are also working so Gmail for example
you can email in a different language
and obviously you could go and paste it
in but but it's much nice if it's
directly integrated or a recent one also
which is actually quite exciting is the
toolbar integration which allows in a
very seamless surfing in different
languages if you if you then just go to
a page then it will translate it
immediately so you can basically make
the whole web in your language and it
really changes the way then you can
currently if you if a link which has a
Chinese title on top you would never
click on it because it's what you might
not understand if you're tiny speaker
might be different but if you don't then
you wouldn't click on it but with that
that read remove that barrier you will
be able to understand what's behind that
google translator toolkit so that's for
integration with human translation and
YouTube that's also cross lingual search
we have a chat integration Google Reader
SMSs translation and the idea would be
machine translation should be just there
wherever you need it so it we have it
right now that you have it in Google
Translate but there are lot of very
interesting integrations which make then
we it much easier to get access or to
talk to people communicate to people if
it's right there in that application ok
so what about the future what are the
challenges and opportunities that are
there in the field so here I go through
some really technical things which are
challenging for us in order to produce
better translation quality
so one of the things is that our models
are currently and the models that are
used in the research community are
currently very good in in figuring out
local dependencies if something is local
and depends on each other then Engram
models capture that pretty well
astonishingly well actually but if
things are long-distance dependent on
each other then there is a challenge so
here's a mistranslation of a German
sentence translated into English and so
here's a German Health Minister Guler
Schmidt and then later there is a word
it's which should refer to the Health
Minister but but it's a she so it should
not refer to as its to it or then the
German has this annoying way of putting
verbs at some part of the verb is in the
beginning of the sentence in the other
part is at the very end of the sentence
and and in English that's just done
differently and therefore then the
translation here of Hesse and once we
checked it basically then there should
be said it has protected that she has
rejected that those allegations are are
just not at the right place so in
general everything which is
long-distance and where their
long-distance dependencies it's quite
hard to get them the problem is that
it's just there's very weak signal there
a lot of possible long-distance
interesting relationships if you just
look at it from a co-current point of
view but which of those are the were
loved and want right so this has and
this was we checked it
those are it's an interesting dependency
and and the same with its and the just
entity in the beginning of the sentence
and there are a lot of lot of work is
there some tax base translation models
to past translation model the first we
order the German sentence make it more
English likes it make out of German you
produce German Prime which then looks
like English and then you translate it
locally and in in the field there's this
big debate about how much supervision do
we need how much free banks do we need
so how much annotation do humans have to
give there or how much we really can
learn purely from data and with large
amount of data some of those things
which so far we weren't able to learn
hopefully
came below and so it's this conflict
between linguistics or only statistics
another one is just a wider context
which goes in beyond a single sentence
so the current approach that we are if
you go to Google Translate and you
translate something every segment is
translated more or less independently so
document comes in and gets split up into
many small documents and that makes it
impossible that we can very quickly
return to you the translation of a
webpage because it all happens and
paralyzed but then obviously there are a
lot of dependencies which as a result
are not really captured well and a few
years ago
machine translation quality was not so
good that at that time it probably was
not worthwhile to work on that it was
not the next low-hanging fruit but now
for the really good languages like
Portuguese or Italian or French those
kinds of issues really are those
problems which are still remaining and
and so therefore things like enough for
a resolution so that you know this word
relates that word before and then as a
result you have to translate in a
certain form or just topic adaptation or
non-local word since this immigration if
you don't know what the meaning of this
word is locally you have to figure it
out based on a larger context but
relatively little work has been done
there one challenge is that the metric
that we are using there is not very
sensitive to those very subtle changes
which are subjectively very important
right if you refer to the person in a
different form of or you use the wrong
sense of a word after what the text
trust might not be understandable but
this automatic metrics which measure
similarity human translation would not
be significantly affected so there is a
lot of work also to be done in improving
the evaluation metrics that we use
morphology is another big challenge I
mentioned that before with finish so
source language morphology is actually
not the biggest problem there if you
have very large amounts of data for
Arabic for example with very large
amounts of data replicates a very rich
morphology is rather complex but once
you have large amount of data it's
actually helping more than hurting
because it's just very precise if you
have an Arabic word there you know quite
precisely what the English translation
would be if you have a Chinese word you
don't know that
it's singular plural or many other
things but the much harder problem is
then the target language morphology if
you go from a language like English into
Arabic or into Finnish then English a
lot of things are just not specified you
just know them from context but if you
produce an Arabic translation or a
Finnish translation you have to be very
specific about everything so you have to
understand really in detail what this is
about in order to produce the right
translation and here again a lot of the
research actually goes into English only
but for us the other direction is more
interesting and and therefore we are
seeing the problem probably more than
others another one is this reliability
or confidence measures issue so some
translation mistakes are worse than
others so if you have a mistake in
translation where just a preposition is
wrongly for get an article then that's
one thing but but so these are three
mistakes that we actually did at some
point in our system where a hotel goes
to monitor our I have no idea
he fled sure going to Tom Cruise that's
that that actually that happened also to
a time where it was very inappropriate
that this would come up and it but the
interesting thing is you can then look
where you learned that from right and
sometimes it's just a mistake that those
statistical learning algorithms they
appeared something which is not correct
but sometimes it can really be a right
translation because translation that
what you expect normally from a machine
translation is very literal translation
but human translators sometimes - very
interesting and in fancy reformulations
so in this specific case the text that
produced a translation was a very good
translation produced by a human
translator which did exactly right but
it's just that his lecture is just most
famous or was there the most famous
actor in Australia and then Tom Cruise
that was for Spanish I think it was then
the corresponding actor in Argentina who
would that be well and then you choose a
different actor because just he Fletcher
wasn't so much known there and there the
choice was Tom Cruise and in that
context it was a perfect translation it
was the the the it was not really
referring to that person but it was
referring to the concept of what's the
most prominent actor in that country and
and so
they are very interesting subtle
challenge and how we would figure
something like that out that we don't
learn this kind of mistranslations or
the other one was also a very weird one
well there's some story behind you can
ask me later at the cocktail hour how we
could have translated psychosis the
codes the codes in the pushbacks player
so the challenges are they're really to
filter bed and non-literal trance
training examples and or if that we
could at least give users some kind of
indication of that whether we are not so
sure now about this translation right so
currently we always produce the
translation and sometimes we have some
indication that's the best translation
we can produce but we are not entirely
sure about it then another interesting
topic I guess probably some machine
learning researchers here so the way
then in the end in machine translation
we are combining all those different
knowledge sources in this linear model
in doing the the weight optimization is
actually very different than I guess in
almost any other field because we are
using what what is called your minimum
airway training every every statistical
machine translation research group is
now using minimum airway in order to
tune their system weights so there's
this linear model and the training
objective is directly to optimize this
blue metric and when I say directly
there is no it's it's really directly
exactly that blue metric as a result the
objective function if you look at it is
piecewise monotone function where it's
four along one of those feature
functions a piecewise monotone function
which is very Wiggly but but
interestingly then you can compute the
global optimum of that problem if you
basically put in compute that over a
lattice of for one dimension you can
really find exactly at that point which
is the peak point for that and here is a
zoomed in version for that so really the
the error surface is rather complex but
the challenge is that this just scales
to a small number of features so we can
train
tens of features with that but a lot of
problems in machine learning then you
get obviously much better if you can
train millions of features with that but
so far that doesn't seem to work too
well everyone is using that kind of
technique can be scaled at up to really
large numbers of features in machine
translation another interesting one is
user feedback interactive machine
translation so currently we have some
feedback mechanisms in Google Translate
so if someone sees a bad translation
recognizes it's bad you can tell us this
is a correction and then we might use it
in a future version of the system or
they're as if we will translate a
toolkit so that's the integration with a
human translator framework rather then
if you want to translate a text you can
use Google Translate as a first version
and you just correct the mistakes if a
sentence a correctly don't have to do
anything just go to the next sentence or
then you fix up some of the mistakes and
then obviously we have feedback by just
monitoring our discussion group or bug
reports or people blog about it but I
think there is a lot more possibilities
there to do interesting things in user
interfaces and to just do more
interactive machine translation that the
translation is more a dialogue between
the system and the user and especially
given that machine translation is making
mistakes and will continue to make
mistakes for the foreseeable future in
order for user then to really figure out
if it doesn't understand something there
would be interesting ways to interact
with the system and I think interesting
question really is can use a feedback
data for something like machine
translation really comparing value and
significance to what we get as normal
parallel trainer will be find on the web
it's all data-driven so we find the text
as it is on the web and that's that's
the ground truth on which we operate or
I might not sure I'm not sure if I
understood right wait wait so yes so and
that's actually what we are looking into
right now with respect to so here the
long distance dependency what we are
looking into to use and their grammars
in order to analyze the sentence and to
reorder it and make it similar to the
English sentence and and but that's
ongoing research so the system that you
that produces this translation here it's
just pure data-driven and and then you
get this thing out and ideally what we
would want is that that we learn that
from data right that we don't have to
system to tell it because there's so
many special rules and exceptions here
that in German it's this and some other
language something else and you might
not find a parser for those of the
languages and so on so ideally and in
some sense it's it's in the data because
as children they all learn that very
easily if a three-year-old german knows
where to put the verb and and so the
computer should be able to figure that
out too
but in the meantime we would we are
looking into things like that to just
explicitly put that knowledge into the
system
right so this place right so far the
automatic rammer induction work has been
I don't not very well working or so but
there recently days there's no but but
there's very interesting recent work
there where I think we're the field is
taking off there and one key thing that
I believe will also make that possible
is actually parole text because
monolingual text alone I know for a
three-year-old he has a lot of other
information that people are pointing and
telling him things and he makes mistakes
and then he wants to fix them but for a
computer only the text is it's just very
hard or it turned out to be very hard
but now with parallel text you have the
same text in a different language so
this is all everything is basically
replicating a different language and as
a result you can learn a lot about local
dependencies and what's what's now a
different dependency so it's something
in English its local together that's a
good indication and in German it's so
distant that's a good indication that
whatever is distant in German actually
also is semantically belonging together
so now people are starting to use pro
opera and automatic grammar induction
all-in-one and they're actually more and
more paper in there I think it's a very
exciting interesting research area right
right right
no no so we are looking into into into
those things and and it's really that I
believe now with the with those parallel
corpora and to basically have this
partial supervision from them and it's
not a super mission for the grammar
induction right we kind of create the
supervision artificially because then
locality and things like that so it's
not really a supervision saying this is
a phrase but it's it's very fast one
that I I'm pretty sure we will make
significant breakthroughs there and I so
I personally I'm of the opinion that all
those annotated corpora that we are
creating like penn treebank things like
that where large amounts of text have
been annotated that those in the end
won't be necessary that we'll be able to
learn much more just from the data
itself we just need to have the right I
know the right inputs to it so parallel
training data for example it's one of
those inputs which wasn't there before
and also in very large amounts of text
right if humans annotate a million words
that's a large amount of work that has
to go into that but what do you have
here then is hundreds of millions and
billions of words of this
semi-supervised annotated text and and
and in trillions of verge of just more
lingual text which also has a lot of
information in there so just my last
slide so I put talked about a lot about
the challenges and an interesting
research problem and things we are
looking into but overall I want to close
with basically all these enormous
opportunities ahead so currently the the
improvement curve of the machine
translation systems is really very steep
over a few years ago you couldn't find a
Hindi English High English machine
translation system and and now they are
there they have interesting quality and
quality is improving significantly so I
think it's not unreasonable believe that
a few years down the road we'll have top
40 languages in a quality that's similar
to the Italian English quality of our
system right now we'll have many more
machine translation languages for which
we can do machine translation I don't
think it's impossible that we go from
the current 40 to a hundred languages in
a few years so 10,000 languages from any
of those languages to any other and and
really all the language is like like
Yiddish or so we're which are minority
language is where it's just a few years
ago it would have been impossible to
think about that and then machine
translation everywhere that it's really
integrated into all those places where
you need it that if you want to find out
something that the language is not a
barrier breaking down the language
barrier thank you I'm not sure if you
have some time for some more questions
or yeah try translating the Wikipedia
because there are so many different
versions of the same subjects that's an
extremely interesting subject because I
have found that different versions the
different languages in the Wikipedia are
basically made up for from different
people so the style rre or even the
information contained in each of them is
completely different
so the problem of translating the
Wikipedia is very challenging I think
well so what we're actually providing as
part of the google translator toolkit so
the google translator toolkit is a if
you're a human translator you can upload
a text and it gets machine translated
and then you can post edit that so it's
a efficiency tool for a human translator
and it's integrated into Wikipedia in
this form that you can upload a
Wikipedia article and and then there's a
machine translation there you would
correct it and then you can send it back
into that language in which you
translated it
so that basically makes it possible that
then there's I know the English
Wikipedia is probably a few million
articles that those articles which are
not existing in some other language you
could then relatively easily and
straightforwardly make available there
and and basically in all an integrated
process whether there's a there's a
human translation tool which is all
those features like translation memories
have that if a similar sentence has been
already translated that would be in
there and things like that and then
machine translation that if your
sentence is already machine translated
well you wouldn't have to type that
again and making that possible I'm not
sure that exactly answer your question
but it's basically then I think there
are a lot of opportunities there to make
it easier for people to create
translated content by using machine
translation a professional translator
they typically don't like machine
translation because they just they might
be faster without machine translation
but a semi professional translator just
a normal person producing a translation
so if I produce a translation for
example it's much easier if I start out
with a machine translation and then post
edit it yeah okay so Google seems to do
a great job of gathering information
from the things that people are not
doing on purpose like the flu search is
telling you where the flu is actually
happening you talked about these
parallel corpora or that were
intentionally created is there and are
you looking for sort of parallel texts
that are created by you know bloggers
translating each other's postings and
other sort of stuff that that could
substantially increase the amount of
parallel text that you can work with
right so what we try to do and I know
it's it's ongoing work is to find all
parallel texts that's out there so if
there would be a blog post that someone
made and someone else translated it then
our goal is to find it okay it's a
challenging problem right there is so
much text out there and and every of
those texts you have to use as a query
for every other text but but
fundamentally
that's what what we'd like to achieve
that then alter all text that is
available in those corpora that we can
use for that we would find their yeah
how well does your technique work for
translating scientific papers into other
languages well so I guess it's in
general if we have text of a certain
nature in the training data it works
better for that as if we wouldn't right
so if we if someone asked us to
translate United Nations resolutions it
would be pretty good because there's
just really large amounts of that type
of text in there so now for scientific
papers it's it's more challenging
especially also because the topics are
so diverse that then in this area and in
the terminology there is very different
from from another area so there are all
those very many very specific domains
and we would have to have a no training
data that covers that vocabulary and
that that structure so it depends
there's there's and sometimes there are
large amounts of abstract which are
available in translate in many different
languages and that's an interesting pool
then in order to learn something about
that type of language in that style but
I guess I know in general probably the
answer would be currently it's not so
good but what we are working on finding
more parallel data and then that should
become better okay thank you
I think we have to finish</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>