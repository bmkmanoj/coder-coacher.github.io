<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Graph Identification and Privacy in Social Networks | Coder Coacher - Coaching Coders</title><meta content="Graph Identification and Privacy in Social Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Graph Identification and Privacy in Social Networks</b></h2><h5 class="post__date">2008-12-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zW5BJ7Av-80" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">go into semester mostly address and she
does a lot of work with data mining
yielding structured unstructured data
file before it on her web page was like
to the fish thing and today she's going
to talk about that identification of
trash ok so i think this especially to
this crowd is like no news whatsoever
there's this and that works everywhere
in particular one of the largest wrap
around the web but there's a lot of
other kinds of data that can be thought
of as graphs or network so their social
networks communication networks
financial transaction networks
biological networks and a lot of other
one and as a matter of fact you can go
on the web and time these kind of nice
visualizations of webs or at least it
looks kind of nice around my laptop than
it does up there but this is a picture
of the internet or you can find
something like this which is a food web
which describes different species and
who eats who and who decomposes who and
so on but one of the things that's
interesting about these pictures is it
actually there's a fair amount of work
that goes involved in even constructing
the pretty picture so for example to get
this internet map you have a bunch of
trace route information you need to line
that up in order to get the graph and in
fact we actually had a discussion over
lunch about this in food webs if you go
and read this paper there's a lot of
work describing or figuring out what the
right level of abstraction to model the
web is and so on so well we have a
wealth of data so we are inundated with
data describing networks a lot of the
data
noisy incomplete and it's at the wrong
level of abstraction for analysis so
what I'm going to describe and kind of
the first part of the talk is the
problem of that I call identification so
how do you take a bunch of incredibly
noisy incomplete information describing
a network and then kind of consolidated
and abstract it in the right way to do
something useful with it but then
there's kind of the flipside of well
there's all this noisy data out there
and you can join it together and you can
get a lot of things in maybe you didn't
want people to get so there's this kind
of privacy element two of you know how
all this observational data that's out
there for completely different reasons
can now be kind of lunch together and
meshed up in certain ways so that you
can get information that maybe you would
prefer people not to get and so I'm
going to start off talking about
identification so there's a lot of
real-world datasets that are inherently
relational so there's social networks
biological networks communication
networks citation networks but then
there's also the kinds of networks that
I imagine you guys deal with which is
you have query logs and you have
relationships between the query logs and
the quick flick blogs and all that kind
of behavior as well however in many
cases the observations are noisy and
graph identification is a problem of
trying to infer the correct grass that
you then want to do your science on from
this kind of what I'm calling a data
graph and as an example so trying to
infer say
a organization hierarchy where you can
figure out for example in the Enron
email collection you know who the crooks
are you know how they're connected and
things but what you're trying to get it
from is from you know the email and the
email is a graph that describes you know
who emailed to and so on but it doesn't
necessarily have the mapping from say
the email address to the individuals and
their position in the hierarchy and
you're trying to infer this kind of
organizational hierarchy or social
network from the communication patterns
another one is you know in biological
data you may be interested in actually
being able to figure out the which
proteins interact but from you're doing
this from incredibly noisy you know
high-throughput gene array data and so
on another quick example is you know
internet security where what you're
trying to get out is a real
understanding of the network and
understanding of you know which of the
hosts are benign versus which ones are
compromised and in reality the data that
you have is not kind of at the right
level necessarily to infer that so the
solution is to kind of take this noisy
data and try and infer the information
graph and the key thing is that the
dependencies are hopefully in the data
in the nodes and the edges and so on
such that we can make use of that and
exploit it to get out the grab
that we're really interested in and the
nice thing is usually that information
graph is going to be much smaller than
the original observational graph you'll
have this huge observational graph that
you're trying to compress into the kind
of graph that really has what you want
in it so I'm going to go through kind of
some of the processes that kind of
inference processes that are required
for doing graph identification and one
of the simplest ones is ok you have a
network and in that Network you may have
some of the notes labeled and here I'm
just showing you know green labels and
orange labels and what I want to do is I
want to get out a fully labeled graph so
I want to infer the labels of the un'
unlabeled notes and I want to do that in
a way that obviously makes use of the
information in the things that something
is connected to so if Green is spam and
oranges you know advertising or
something like that i'm trying to figure
out what the classify the different
pages well it may be the case that you
know spam page is usually linked to
other spam pages or it may be the
there's other kinds of correlations that
you can make use of so one of the most
basic kinds of inferences that you want
to do is this kind of just label the
nodes and four so it's a classification
problem but as a classification problem
in networks another kind of inference
task is predicting the existence of
links so it may well be the case that
you have a lot of observed say
communication links and what you're
trying to do is infer another kind of
link a relationship like so-and-so works
for so-and-so so someone says the boss
of someone and so on so it's from one
kind of observed link I'm
and fir another kind of semantic link
but again it's a prediction problem but
it's a hard prediction problem and part
of the reason it's a hard prediction
problem is because just the kind of
prior probability of any edge is just
going to be so low that you oftentimes
have a lot of difficulty in being able
to do these accurately another aspect is
in the data graph in many cases you'll
have multiple representations for an
entity and so you have to do a step of
entity resolution which is actually
emerging the nodes and I'm actually
going to go into a little bit more
detail about the algorithms for doing
this in a minute once I go over a
high-level overview of the different
problems and then the last one that I'm
going to mention is group detection or
community detection so finding clusters
of nodes in graphs so these are really
kind of basic inference problems that
are useful so doing collective
classification doing link prediction
doing entity resolution and doing the
group detection and ideally what you
want is you want to combine all of these
inferences together you don't want to do
each and isolation you want to group
them together in order to get out the
graph that you want to use whether this
is a query graph where you've aggregated
a bunch of queries together or other
kinds of graphs so as I said I want to
go through one of these in a little bit
more detail which is entity resolution
if anybody wants to hear more about the
other problems I'm happy to talk about
them but entity resolution is one of the
things that my group has done a lot of
work with and in particular we've done
it in the context of graphs and network
so how do you make use of the graph
information to help you do the
resolution and just as an example what
I'm showing right now is a teeny tiny
fragment
of a co-author network so the nodes are
authors and the links between the nodes
are that they co-author to paper
together and this is from a real data
set that was used in the 2004 info
visconti and actually they had done a
lot of work hand cleaning the data so
there's not supposed to be any errors in
the data but if you look at the data
carefully and go through the process of
figuring out exactly you know which of
those references are in fact referring
to the same you get on one side what you
have before was kind of a big kind of
spaghetti mess and then after doing the
resolution you have this nice tight
click of co-authors and so I want to
emphasize the importance of doing this
entity resolution step before you do any
other algorithms on your data because if
you applied any of the standard you know
social network measures or anything like
that on the original data set you get
completely bogus results so the
centrality is wrong the betweenness is
wrong you know all of these things are
wrong so it's incredibly important to do
this before doing any other network
analysis yeah Peter
yes and so um it's it's it suppose the
studio audience so what Peter asked was
you know can't you use the network
measures to help you do this resolution
process and actually that's very much
the flavor of what we're going to do is
try and have an inference process that
makes use of network structure to help
correct the resolutions but yeah and so
typically the entity resolution can be
thought of as kind of a clustering
problem where you have say a database of
references where you have these strings
and what you're trying to do is you're
trying to cluster them together to find
out the kind of hidden underlying
entities and there's an identification
problem which is to map all of these
strings to the same individual and then
there's a disambiguation problem of
figuring out that ok j smith in one
context refers to this guy and in
another context refers to this guy and
traditionally it's been set up as a
pairwise classification problem where
basically you take one reference say
this james smith reference and you do
the cross product of it with all of the
other references in the database and you
can clearly see the problem so first off
if you're going to have this kind of
setup you're going to need to pick a
threshold once you pick a threshold
there's a precision recall trade-off and
how you do that but in this example for
james smith in one case when i compare
it to j smith i want to say yes
they're the same and in another case i
want to say no they're different and
clearly given the information that I've
shown here there's an inability to
disambiguate and then when people do
this as a pairwise classification
problem typically then there's this
Nisshin well do you do transitive
closure afterwards or not and as I'll
show in some results sometimes that's
the right thing to do sometimes it's not
the right thing to do so it's not
obvious so what we're going to look at
is relational entity resolution and this
is exactly you know let's use the
network structure to help us and the
network structure comes in because
things occur together and so we want to
use the relations to help improve
identification and disambiguation and
you know there's a whole host of people
working in this area and so this is
something that people are very
interested in you know in doing
information extraction and needing to do
this kind of entity resolution how can
you make use of structure but let me
first try and motivate with the original
example in this picture what i'm showing
is i'm showing two of the hey sue
references and the square notes i'm
trying to decide do these refer to the
same underlying entity or do they refer
to different ones and in this case what
i've done is i've shown the shared
co-authors in the center and the
non-shared co-authors on the side and in
this case they do in fact refer to the
same underlying individual they do have
two co-authors in common so I can
resolve them here's another case where
the kind of similarity between the names
is exactly the same but they have no
shared co-authors so in this case it
turns out to be a case where they're
actually distinct and Fernando yes
but not completely because frat you the
spells right has not and we be the other
one and not being in an idea right so so
you are allowing yourself some slack
there in insane oh I can go too much
that even though they will go out to
ship the leg then on exactly into the
quality yet right right so definitely
it's looking at the similarity of the
co-authors it doesn't have to be an
exact match and figuring out how to do
that in the right way is kind of the
focus of the inference algorithms so you
know at this point for me some kind of
transitive identification like it you
say if you were doing you know automata
quibble that's right you say oh those
two notes are the same because everybody
everything to go to either is the same
or is equivalent as well once those I
may be put right so sorry sorry the
question was you know what in this setup
am i doing an exact match on the
co-authors and if I was then I wouldn't
be matching these because of the fact
that they have non-shared co-authors and
am i doing like more of a transitive
closure at this point when I'm would I
then kind of merge a bunch of things and
at this point I'm more kind of
motivating the problem that and so at
this point it's looking at each decision
as a pair wise decision are these the
same or not but that leads perfectly
into the point that i want to make on
the next slide is that these things
depend on each other so in the case
where I for example do decide that these
two elmendorf references are the same
maybe the name similarities enough to
make me decide that then that's going to
give me the additional evidence that
would help me decide Oh may
I can merge the singer references so
very much there's a question of how do
you chained together the independent
pair wise decisions in order to do a
good job in the resolution and so
there's a couple algorithms and I'll
just talk about them kind of at a high
level so one of the ones first ones is a
relational clustering algorithm and the
relational clustering algorithm is very
simple you can kind of understand it as
defining a similarity function that is a
weighted combination of both just the
object level similarity and then how do
we capture the graph similarity so some
sort of measure of how similar the
neighborhoods are how similar the in the
running example how similar the
co-authors are and this is a place where
being smart about using then kind of
network metrics and understanding what
domain urine for how you should do this
similarity is important when we were
doing it in the context of co-author
networks we looked at a variety of ways
of comparing neighborhoods and
originally we thought that doing like
the right the multiset kind of compare
do a matching of all the sets of
co-authors and you know aggregate over
that was the right thing to do and you
know first off that's really expensive
doing a multi-set comparison like that
and then we found that if we just
projected it onto a set and kept counts
that first off it's hugely more
efficient but also it didn't hurt us
that much in terms of performance now I
think that that will vary depending on
your domain whether that's accurate but
we were surprised how much having a
simple relational
molarity metric actually ended up being
fine and then you know this is a basic
algorithm so it's basically a greedy
clustering algorithm where we're going
to merge the cluster of references that
are going to have the biggest impact on
this objective function again that's a
simple setup and then the algorithm the
key thing that's different from a
vanilla greedy agglomerative clustering
algorithm is first off the first step
there's no way for any real entity
resolution problem that you want to
actually compare all pairwise references
so that's just not going to scale so all
these algorithms have some flavor of
what's called blocking or use of
canapés and so on to quickly put
together references then maybe are going
to be the same and then distinguish
references that there's no way they're
going to be the same and so there's some
sort of bucketing that goes on that kind
of groups together the references so
that you only compare entity resolution
for those things that have been bucketed
together so this is absolutely essential
and we used a kind of off-the-shelf
method for doing this I think there's a
lot of interesting research to be done
in you know how to do this the best way
and then the the final difference is
when you're doing the clustering once
you merge some collection of references
you actually have to be smart and update
all the other references that co-occur
with those so you have to build a decent
index structure that will allow you to
easily get to those references and be
able to update the similarities and
that's something that you want
have in um yeah
different weights to them but
you
so um that is something that I'm like
doing this at such a high level that is
not showing but definitely you would
have different weights for attributes
but as you said it's still as I've set
this up here a single weight vector for
all of the references and actually it's
very interesting to think about making
that more adaptive and we have some work
that does make it more adaptive that I'm
just kind of presenting the most vanilla
one but it's certainly the case that
this is really kind of packing in a lot
of things that the weight would be for
example in name references you would
take into account the names more
strongly than you know their height or
something like that but still you want
to make use of those okay and then we
also have another model which I won't go
over but a kind of variant of lda
applied for entity resolution and but
let me get to some experiments that
we've done so these are three different
citation data sets so their co-author
networks the first one is a small data
set obviously describing computer
science papers in particular machine
learning papers the second one archive
is a paper describing high energy
physics data sets and the third one is a
larger data set describing a collection
of biology papers now in the first two
all we had with the author names but in
the third one we actually had additional
attributes keywords topic language and
so on and we made use of these in the
relational clustering algorithm and the
bass lines that we compared to is we
compared to an algorithm that only did
made use of the attributes
and you know it was relatively smart
about dealing with names we also use
this algorithm and applied transitive
closure we also did what we called a
kind of naive relational a classifier so
I made the point that the relational
information should be done jointly so
you should change together these
references well at the same time you
could just throw in the co-authors as
additional attributes and that seems
like information that's clearly useful
so are we really getting the benefit
from the joint inference are we just
getting the benefit from having extra
attribute information so we tried to
test that by including something that
just toss those in as additional
attributes and then you know there's a
huge amount of issues with actually
evaluating entity resolution but what
I'm going to present our pair wise
decisions over the references in the f1
measure of these and if we look at these
we see you know this focus on the
relational clustering algorithm we see
that it certainly does outperform the
baselines of the attributes either the
ones using transitive closure or not and
you see the effect here that sometimes
transitive closure helps and sometimes
it hurts you we're also outperforming
the naive one and so you know that's
interesting and you know I won't get
into the difference between these two
algorithms and I'm happy to talk about
them offline but both of them ended up
being two different ways for doing this
relational cleft string but now it's
kind of more interesting to look at this
in instead of the column wise or instead
of the row wise the column wise and so
if you instead look at
sites here versus archive versus bio the
interesting thing is sites here yeah we
get some improvement but it's not that
much so this is the computer science
papers for the physics one yeah this
actually is you know an important
difference but then for the bio-based
one we get a huge improvement and so
what is it about these data sets such
that okay sites or maybe I'm not even
willing to pay the price to do the
relational clustering for the entity
resolution but biobased I definitely
should be willing to do this and if you
think about it and actually look at the
data sets the structural characteristics
so the graph properties of these data
sets are quite different so first off
sites here is just not that ambiguous
it's a small data set there aren't that
many names that are that similar and
it's a computer science data set the bio
base actually this was a data set that
was set up as a challenge so what they
did was they initialed all the first
names they focused on Asian names and
bio papers tend to have much longer off
their list so that combination of
effects having much more ambiguous name
references and much more kind of
information content in the co-author
information with enough to make this
much more efficient are much more useful
so I think this element of understanding
your domains and understanding how that
structure is useful and then making a
call whether you're willing to do the
more expensive inference algorithm or
not is actually a really important
message not just for entity resolution
but for a lot of inference problems
okay so any questions here yeah sorry so
these are pair wise decisions over the
references and it's the f1 measure so
it's kind of combining these things no
no and no words okay so they say okay
I'm going to do all the eigen potential
identification yeah so this were setting
it up as sort of an unsupervised setting
but it's interesting to think how
supervision can come in in terms of
setting the weights and so on so I
didn't emphasize this but the results
that I presented were over the best
decision boundary for each of the
algorithms separately and so on
point some kind of cut off in the fair
all right then then you he's how can i
sample from a network so the question
was for these kinds of data sets you
know what we presented was kind of
unsupervised best performance overall
cut-offs so how would you go about
choosing a holdout set and using that to
validate so two points to make one of
the things that we found is that
actually and this is consistent also
with what Andrew McCallum has found is
that you actually need a very small
amount of labeled data to set these and
so that's the upside but the flip side
of how you sample these data sets is a
huge huge huge problem and we don't have
it so much in the context of entity
resolution but for some other problems
looking at how people have done the
sampling for the network data you get
completely different results based on
how you do these and as far as I know
there's people that have called out the
problem but I haven't seen any great
solutions but if anybody has any
pointers definitely let me know um ok so
I do want to do the flip side which the
flip side is privacy so on the one hand
we have tons of observational data we
love to you know align it together and
do our resolution so that we can do the
best science that we can another hand
all of the inference problems that i
talked about actually map very well to
different privacy concerns and privacy
concerns that have been studied in
cases in the literature and some that
we've just started looking at and so I
want to go over these kind of a high
level but basically entity resolution
corresponds to identity disclosure so if
I can figure out who asked a query based
on the query I've disclose our identity
and then I'll go through these in the
context of a well-known online social
networking site which is Facebook and so
what I'm showing here is not a real
profile it's actually been munched
together from different pieces and one
of the things that we see here is that
people have certain kinds of attributes
so that attributes and food things like
political loyalties and locations and so
on not surprisingly you know this is a
social network so we have to have our
friends so we have our friends over here
and then we have groups so most of these
networking sites have the ability to
join groups and that's actually a big
part of them and so eight many sites
allow you to control the how much you
make public so one thing you can do is
you can make your whole pry your whole
profile private and then you don't see
any attribute information you don't see
any friendship information or you can
you know not show your attribute
information but include your friends
link on the other hand it turns out that
group affiliation is actually something
that is usually not under your control
so that's under the control of the group
owner
and so the many of the groups you can go
to you actually do see all of the
members of the group and there's not a
way to change that setting and in the
context of privacy then we have the
notion of identity disclosure which
occurs when an adversary is able to map
a record to a specific individual and
you know obviously on Facebook lots of
times they want you to be able to map it
to a specific individual so it's not an
issue but in other cases with query logs
and so on this might be an issue and the
privacy literature has mostly focused on
doing this based simply on structural
characteristics and the structural
characteristics are really kind of how
identifiable are you based on your
degree and so on so it's not taking into
account attribute information
necessarily a nether a type of
disclosures attribute disclosure so
figuring out the value of users a
attribute-based possibly on you know
people that they're connected with so
maybe you can infer something about a
political affiliation based on the
connections that are exposed in the
network corresponding to link prediction
there's a notion of when an adversary is
able to figure out that there is a
relationship between two people based on
some other kind of information in
particular when that after that
relationship might be sensitive so
figuring out for example the example
that i've been using before that these
two queries were made by
same user or maybe in a social network
your you don't want to be able to infer
the two people are friends or you can
infer their friends but you rather not
and for other relationships and then
finally there's a the issue of being
able to infer something about a user
based on the group's or predict their
group affiliations so typically the way
the anonymization process goes is people
have some sort of network data they have
some sort of process that they go
through and kind of change the data and
then they want to be able to say whether
or not they can guarantee that there's
no privacy breaches and then you can
make this public and so obviously
there's important issues like after you
anonymize the data is there any utility
in the data left one way to make sure
there's no privacy breaches is to not
release it at all and I'm not going to
get into a lot of details in this whole
process the key thing that I want to
highlight is kind of in the context of
graph identification how to think about
some of the privacy problems there and
first off in a graph there has been a
lot of work on anonymizing nodes and you
can think of this as just you have a
table describing a bunch of individuals
there's something called for example
some of the early work k anonymity which
goes by and basically takes records and
hides attribute values
such that then things that share the
same values are all in an equivalence
class and basically the idea is there
have to be at least K members in each of
these equivalence classes so now the
question is what if there's a graph so
what if there's links between these
things and how do you anonymize the
links well you could just release them
you could do some kind of partial
removal so randomly remove you know X
percent of the edges you could do
something that kind of tries to combine
together the node anonymity methods by
doing something where you group together
the nodes and now the edge is no longer
tell you what nodes in the cluster they
connect they just say they connect some
note in a cluster with some note in
another cluster or you can do other
things like instead of keeping the
counts of the edges you can just say
okay there was an edge a friendship edge
between someone here and someone here
you don't know how many there are there
was a works for relationship and so on
so you keep the fact that there was a
link or you can remove all them it turns
out looking at these anonymization
methods even just to kind of understand
what is the effect of having these links
and what are the effect of these methods
you can see interesting things so for
example these are synthetic data results
where I'm varying the density so how
many links are going between things and
we see that for kind of sparse graphs
then
basically the the to cluster methods are
actually doing a reasonable level and
then the interesting thing is as you
kind of go up in complexity then the for
example the cluster the constraint
clustering does well but the problem is
you have to really think about how much
information is actually being preserved
by these things so depending on what
your analysis task is they kind of
utility of these things and one of the
surprising things is the doing something
like a random removal of edges when the
graph is highly connected actually does
about the same and obviously is a much
easier thing to implement so these are
kind of preliminary results because of
the fact that you know people really
haven't looked at anonymization in
network data that much another issue
that we looked at is how this group
affiliation information affects the
inference and so this is if I wanted for
example in for a attribute value for
example political affiliation knowing
that someone belongs to you know this
social network is obviously giving a
huge amount of information and so I
won't go over the the different methods
in detail that we compared but basically
let me try and give you a sense of what
happens on for online social network
site so one is flickr the others
facebook the other is dogster which is
actually online pet work for dogs where
you can specify your friends and family
and it's an sana me there were basically
we were trying to see how much does even
if you make your profile private the
fact that there's a fair amount of
people that you link to either through
friends or through groups that don't
make their profile private how much can
you infer about the attribute values and
we compared I know I flash through them
so I'm going to try and give you like
the high-level bit and then anybody that
wants to know more details about the
algorithm we essentially used one
algorithm where we only used attribute
information and kind of trivial in terms
of just using ok what's the predominant
class so that's like a very basic
baseline the other class of methods made
use of the friendship links so using the
friendship links can I in for something
like political affiliation based on the
political affiliations of your friends
and then the kind of 3rd class of
methods we used were ones that made use
of the group in group membership
information and the first thing to note
is that actually so this kind of set in
here are ones that use just friendship
information and the surprising thing is
friendship information actually wasn't
that useful in these settings you know
it definitely was better than nothing
but clearly these were not very many
domains are very assorted if so it's
really easy to do this based on
friendship information however using
group information which unfortunately is
something that is usually public you can
do a very good job at inferring the
attributes and in fact we had one method
that just used groups is basically a
feature but another method that was
smarter about the selection of the
groups and it turns out that certain
groups are very homogeneous and so if
you select out the groups that are very
homogeneous and only use out those it
does reduce your coverage so that's not
but you can do extremely well so here we
get like in Flickr what we were doing
this we were inferring the country of
origin of the user and in facebook it
was the political affiliation and we're
able to do quite well and I'm again I'm
not giving you all the details here but
this paper has has more information I'm
happy to chat with people afterwards so
what's the connection so we have this
notion of okay inference allows us to do
graph identification if I can't do
identification then I'm you know able to
guarantee privacy so the good thing I
tell my students about working in this
area
you know you can publish either way if
you have the positive result or the
negative result and so just to kind of
show my group Elena here is the one
that's doing most of the work on privacy
and Galileo here is a one that's doing
much of the work in graph identification
and actually this is them after they
took me out to lunch for to celebrate me
getting tenure you can kind of tell by
the relieved look on their faces and so
in conclusion we have this the graph
structure and the attributes matter so
making use of both the graph information
and the content information is important
there's a lot of killer apps and the one
that I wanted to point out at the bottom
and search I think there's some
interesting things with abstracting
click and query graphs and while there's
important pitfalls there's also a lot of
benefits and payoffs thank you yeah
social network is rather interesting one
is many designs posts
so that's interesting so one of the
things that happens in social network
data is sometimes the relationships are
directional and so a can be the friend
of B but B doesn't have to be the friend
of a versus other times they're
symmetric and either they're kind of
enforced to be symmetric there's
something about the tool that enforces
that versus not that's definitely a very
interesting question i would imagine
that if you looked into the data sets
that you would see effects of that
especially if it's from the user
interface that was kind of forcing that
but it's something that we haven't
really studied i didn't go into the
details of how we did the sum of the
group inference and one of the ways
simple ways of doing it is basically
take everybody in a group and say
they're friends so make that that an
explicit click and then feed it into
your algorithm so there's a lot of kind
of interesting issues with how you do
that representation and they can not
only have important result in artifacts
and the data um but they also can have a
huge impact on computational efficiency
too if you're naive about how you do
that so certainly as social scientists
sociologists would be very interested in
you know you should whether or not in
the design you have it directed and you
know they'll look at how many non
reciprocated relationships there are and
so on so
I didn't quite catch up
the characters
yeah so the point was that you know some
networks myspace are kind of globally
connected versus some others you're more
likely to have kind of different islands
in them and that can have impact on a
computational efficiency any other
questions yeah
there was one mark
yeah yes so the question was about the
robustness of the entity resolution
algorithm and so we looked at robustness
in two senses so one is just the
algorithm that I actually sort of
presented is just a greedy agglomerative
clustering algorithm so how sensitive is
it to that the fact that it's a myopic
kind of algorithm and it turns out at
least what we found is it was much less
sensitive to that than we had expected
so it was actually surprisingly robust
but then the other question that you ask
is ok well how robust is it to actually
the removal of links and so on and
that's actually something we're very
much working on right now any other
questions
so actually characterizing how important
that edge attributes are versus the
attributes of the reference are is very
important and that's something that we
did look at in the case of trying to do
online entity resolution so we have some
work where instead of viewing entity
resolution as this you know I'm going to
do this offline process clean my
database and so on it's more of a ir
perspective of okay you know I have this
reference how do I pull in enough
information to then disambiguate the
reference and for that we were looking
at different properties of how ambiguous
the attributes are how ambiguous the
links are and trying to kind of have an
adaptive method that would take those
into account so make use of the links
that are most likely to reduce the
ambiguity
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>