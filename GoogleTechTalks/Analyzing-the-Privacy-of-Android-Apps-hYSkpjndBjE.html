<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Analyzing the Privacy of Android Apps | Coder Coacher - Coaching Coders</title><meta content="Analyzing the Privacy of Android Apps - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Analyzing the Privacy of Android Apps</b></h2><h5 class="post__date">2015-08-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hYSkpjndBjE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">
PRESENTER: It's an honor for
me to introduce Professor Jason
Hong from Carnegie
Mellon University.
Most of us know his work
on privacy and security,
such as the work
on PrivacyGrade.org
and his work on computer
and human interactions.
AUDIENCE: Do you
have another chair?
AUDIENCE: We've got one.
We've got one.
AUDIENCE: We've got
one over here, y'all.
AUDIENCE: OK.
PRESENTER: His work
has been recognized
in a number of venues, but
for us this evening, it's
that he's going to talk about
something that we care deeply
at Google about, privacy and
mobile, which are both our very
strong priorities, and
I'm personally very
excited about the work that he
has done to talk about this.
So thanks for coming and--
JASON HONG: All right.
Thanks.
Thanks.

So this is a last-minute
conversion from PowerPoint
to PDF, so if you see any weird
colors, that sort of explains
it.
So thanks for having me here.
I want to talk about
some of the work
I've been doing with analyzing
the privacy of these Android
apps and just give you sort
of a sense as to what we've
been up to as well as
some kinds of reflections
as thinking about
privacy in general
and what kinds of things
we can do as a community
to help improve things.
And just want to sort
of give a broad overview
of what our research
team has been doing.
Now, I think in the near
future, our smartphones
will pretty much know
everything about us.
They're going to know if
we're depressed or not.
They're going to know what
our transportation needs are
and probably even what
information needs we have too,
before we even know
what those are.
And I think there's
two big reasons why
I'm making this statement.
Smartphones are incredibly
intimate devices.
Here's some fun facts I
found about Millennials.
82% of Millennials sleep with
their phones right next to them
and primarily use it as
their alarm clock, too.
Also, I found
another stat saying
that 90% of these
Millennials check
first thing in the
morning their smartphones,
and one in three even
admit to using them
inside the bathroom, too,
which is a little bit gruesome,
but ask no more questions.
And yet, remember, this is only
just the number of Millennials
who admitted to doing this, too.
The other thing is that it's
not just the smartphone devices
that are intimate but also
the data that's intimate.
So these are just screenshots
from my own smartphone.
And if you had some
of these algorithms,
you could guess a lot
about this individual,
about me here, so who I
know, who I call, who I text.
Even just on the
who I call part,
you can even infer
easily who's my spouse
and what the relationships
are with other people,
and that's some of the past
work that my team has also
done, too.
Also, where I've been
going, where I go to,
using Foursquare and GPS data,
and also all this different
kind of sensory data, too.
And so I think we actually
are at this major inflection
point in history.
We're going to soon be able
to analyze all human behavior
at a really unprecedented
fidelity in scale,
and this is going to enable us
to build incredible new kinds
of services for health
care, transportation,
and sustainability, not
only if we can really
manage all these kinds
of social issues, too.
And so this is where the
privacy part comes in,
and these are just a few
examples of screenshots
from recent news articles, too.
So we've got Facebook Messenger
app, an employer being
sued for GPS tracking,
and also on NPR
talking about all
these fitness trackers,
and somewhat on your wrists,
sometimes on your phone, too.
And so here I wanted to
focus specifically on apps,
because this is sort of the
main kind of dissemination
of how we use all these
smartphones right now,
the main kind of function
of how we can make
use of all the functions there.
And so Pandora a few
years ago was actually
under federal investigation
for collecting too much data
about its consumers.
So, for example, Pandora was
sharing location data, gender,
unique phone ID, and
phone number with dozens
of advertisers without
making it clear,
and I actually don't know
what the status of that is.
This was a few years ago.
Path and Facebook, they uploaded
your entire contact list
to the server without
your knowledge, too,
and this is a really huge
surprise, even to me.
I study this stuff, and
it wasn't even clear to me
that this was going on, too.
Fortunately Path stopped it.
It's not clear to
me whether Facebook
has stopped doing this.
But at this point, they
also already have the data,
so it's already a done deal.
But this is not unusual, too.
So there's a whole bunch of apps
that have unusual permissions.
So for example,
there's a game here
that wants your location data,
microphone, and unique device
ID.
There's also a flashlight that
wants location data ID, network
access, and unique
device ID, and even
a Holy Bible app
that wants this, too.
So even supreme
deities need your help
to figure out where they are.
So what my team's
been doing is we're
looking at how to do--
how we can improve
the state of the art here.
I'm going to give you
an overview, really
a summary of all the work that
our team's been doing here.
So first I'll start out
by talking about some
of the interviews and surveys
my team's been doing with app
developers to try to
understand what do they know
and what their incentives are.
Next I'll talk about
privacy grades, this website
that we developed to analyze
all the privacy of these apps.
I'll talk a little
bit about what
we've been doing
with text mining
to try to understand what's
been going on with these apps
as well, too.
And lastly, I'll end up with a
little bit of a few reflections
as to where my team's going
next and my philosophy
on things, too.
So for the first one, what do
developers know about privacy?
We did interviews with 13
app developers and surveys
with 228 app developers to
understand what kinds of tools
they use, what
their knowledge is,
what their incentives
are, and are there
any good points of leverage.
There's actually more
details inside that paper
if you want to see
more, but I'm just
going to give you a quick
two-slide summary of this.
The first one is that
third-party libraries
are really problematic.
So I'm sure you all know
the Android apps, basically,
there's a whole bunch of
different components to it.
Some of the components are made
by the developers themselves,
and some of them are
made by other people.
So it's sort of like
Lego pieces that you're
grabbing from other places that
offer you useful functionality.
And the problem with
this is that it turns out
that a lot of developers
don't know or understand
the behavior of these
third-party components at all.
So, for example, a
few of them didn't
know they were using
libraries, and the reason we're
saying that is because they
had very consistent answers
about behaviors of these
kinds of components
that they were using.
Some of them didn't even
realize that these libraries
were collecting data.
They just said, well, we
can't compile it and run it
without location permission, so
we're just going to add it in,
and now it works.
And even if they did know
that it was collecting data,
it is not always
clear to what extent
that these
third-party components
were collecting data.
So here's one quote
from one participant.
&quot;If either Facebook or Flurry
had a privacy policy that
was short and concise and
condensed into real English
rather than legalese, we
definitely would have read it.&quot;
But it's just like
reading an actual privacy
policy for all the
components that you're using,
and the developers found
it very burdensome.
The next part was
developers actually
don't know what to do either.
So they knew that
privacy was a problem,
but they-- so they had the
awareness but not the knowledge
or necessarily the motivation to
actually operationalize on it.
So there's very little awareness
of existing privacy guidelines.
So there's the FTC
guidelines and the State
of California guidelines,
but often they
didn't even know [AUDIO OUT].
And often they would
just ask others
around them, what
should I do, or just
use search engines
to try to figure out
what other people were doing.
Developers also had
low perceived value
of privacy policies.
So they viewed them mostly
as protection from lawsuits,
as you see in this quote here.
&quot;I haven't even read
our privacy policy.
I mean, it's just legal
stuff that's required,
so I just put it in there.&quot;
And so reflecting on this--
I'll talk a little bit more
about this at the end,
but here's a preview.
Developers really need a lot
more help here, in general.
So there's a lot of best
practices for security.
So we all know we
should be using SSL.
We should be hashing
our passwords.
We should be using lots of
randomization for things.
We know what kinds of
common attacks there are.
I think it's less clear as to
what the best practices are
for privacy, and
it seems to vary
for different kinds of data
that you're collecting, too.
So, for example, some
of the developers
know about PII-- Personally
Identifiable Information--
but even then, it wasn't
always clear what counted
as PII for the smartphones.
Developers, in
general, also have lots
of problems to deal with, too.
So there's just the
app functionality,
which is the most
important part,
but how to deal with
all the bandwidth.
So make sure they're not killing
the bandwidth on your phone,
make sure they're not
killing your battery,
how they can make
money off of this.
So privacy is actually
fairly far down the list.
So we need to try to
find ways of making
developers' lives
a lot easier, too,
and this is going to be a
theme I'm going to echo again
throughout the talk.
So for the next one I'll talk
about PrivacyGrade website
that we've been creating.
If you have short questions,
too, feel free to ask.
Otherwise we can keep the
longer ones to the end.
So this is the
website that we've
created to try to
improve transparency
of what the behavior
of these apps
are, and we're also
aiming to assign privacy
grades to all one
million Android apps.
Here is sort of the preview
of what the website looks
like right now.
You can also go to the site.
And so, as you can see,
we've assigned grades from A
through D to all
the different apps.
We stopped at D,
primarily because we were
worried about getting sued.
Just to let you know, it
took us about five months
to get this through
the CME lawyers,
which I guess for
me is pretty long,
but I'm guessing for a
corporation it's actually
pretty good.
Yeah, so we stopped-- it
took about five months.
And here's an example of what
it looks like for a single app.
So for the drag racing app,
we assigned this one a D,
and here at the
very bottom, you can
see what the sensitive
permissions used by this app
are.
So if we scroll down,
you can actually
see some more of
the permissions.
So basically it's
three different columns
that just go back.
We've got the permission, which
is already known from Android.
We have what it does, which
is our explanation of what's
going on, which again is
fairly straightforward.
The new part that we're
adding is the why.
So this is our inference
as to what the app is
doing with your data.
And here you can see it's
doing a lot of different kinds
of things with your data,
or it's potentially doing
lots of things with your data.
And at the very bottom,
we also describe
what third-party libraries
are used by this app.
So we just show this.
Here you can see all the
third-party libraries
that we could easily
identify using this.
And so we have Flurry,
Chartboost, Millennial Media
and so on, and we also give
a description based off
of what we've grabbed from
the website as to what
this library is doing.
And this is really our
main point of leverage,
is to analyze the libraries
to make our best inferences as
to what the app is
doing with your data.
So this is really the
heart of the idea here,
and this is actually one of
the most important concepts
in HDI2.
It's known as a mental model.
So on the left side
is a Nissan Maxima
that my brother used to
drive, and it turns out
he was actually driving
it wrong for over a year
before I pointed out
what he was doing wrong.
So just for fun,
does anyone here
have a guess as to
what he was doing?
AUDIENCE: He was stuck in third.
JASON HONG: Yeah, my brother
was actually driving it
in third gear rather than
fourth gear for over year,
and so one time I looked
at it and it was like,
something's not right here.
And so yeah, and this is what
the mental model is, which
is you think something
works one way,
but in reality it actually
works another way.
And as you can see, if
your expectations are not
matched with the
reality, then you
have problems in operating
things correctly.
Whereas you can see the
one on the right side,
there's no way you get it
wrong, because the mapping is
very, very clear.
Now I'm also the
younger brother,
so I have to make clear that my
brother is a very smart person.
Especially since this is
recorded, I have to say that.
And in fact, you can also argue
that my brother's smarter than
me, because he also drives a
Tesla Roadster, and I don't.
OK, so this story about
the heart of the idea, too,
so if we can figure out the
mental model of these apps,
then we can also
try to understand
what the potential
privacy implications are.
So what we're doing is
we're using crowdsourcing
to compare what people think
an app does to what it really
does.
So, for example,
most people might not
expect this motorcycle
game to use location data,
but in reality it does,
and so we consider that gap
to be a privacy problem.
Whereas if you ask people, does
Google Maps use location data,
everybody says,
yes, it's obvious.
So we consider that to be
less of a privacy problem.
So this is not complete
and comprehensive,
but it's also a good way of
operationalizing the problem.
So, for example, some
of these services
might be doing things
on the back end
with the data, which we
can't easily look at.
So this is the first study.
This was done by Jialiu here, so
you have your resident expert.
And also I should say you guys
were very smart in hiring her.
[LAUGHTER]
Let me know if I can
further embarrass you.
[LAUGHTER]
And this is the results that
she had from her initial study,
which I thought was
actually pretty good.
So this is what-- usually when
studies work on the first try,
and it's better than
you expect, that's
usually a pretty good sign.
And so here you can
see the apps and what
people's comfort levels
were between negative 2
and positive 2.
So people, looking at location
data, maps using location data,
were very comfortable with
and same with GasBuddy.
But the Brightest Flashlight
and Toss It, which is a game,
they're very uncomfortable with.
After asking that question,
do you expect this app
to use location data?
And here's also a
summary screen shot
of some of the user interfaces
that Jialiu also created, too.
So, for example, for the
Brightest Flashlight,
we're organizing things
by level of surprise.
So, for example,
95% of users were
surprised this app sent
their approximate location
to mobile ad providers.
And on the right side,
you can see the same thing
with dictionary.com.
People were surprised about the
use of the phone's unique ID.
One thing that we also found
that was really interesting
is the one that's
highlighted there.
25% of users were surprised
that this app sent
their approximate
location to dictionary.com
for searching nearby words.
So this is one where we saw a
really interesting contrast,
where people were very surprised
to find out that dictionary.com
used location data at all,
but when we explained what
it was for, then the
comfort level actually--
and the surprise level
actually goes down.
Comfort level goes up.
And in case you're
wondering what it's doing,
you can actually search for
what words people near you
are looking for.
And so one funny kind of
incident that happened
is, one time I was
in Washington DC,
and I was doing an
interview with CBS News,
and I was showing the journalist
this app, and lo and behold,
what words were people
searching nearby?
It turns out in Washington
DC, it was corruption.

First off, you don't
know what that is?
And secondly, you're
actually looking it up there.
OK, so you can ask
Jialiu for more details
or you can look at
the research paper.
But then the next question
here we're looking at
is, how do you scale
up this analysis?
So it took about
two weeks to analyze
56 apps and only simple
math will tell you
this doesn't work very well.
So the next idea
we had is, can we
combine crowdsourcing
with static analysis
to analyze the app store?
So, again, we're using libraries
as the main point of leverage
here, because they give
us a little bit of insight
as to what the purpose of
the permission request is.
So for example, location
used by Google Maps library
versus an advertising
library gives you a hint
as to what's going on with it.
And so here's some
more of the stats
that we have on PrivacyGrade.
So you can see that there's
a whole bunch of really
popular kinds of libraries.
This is sort of ranked
by the number of apps
that are using the library.
And you can also see in
the different categories
that we created four of these as
well, so targeted advertising,
social networking,
utilities, and analytics.
There's a long tail of
distributional libraries,
so we just focused on
the top 400 libraries.
And so there are definitely
things that we don't capture,
but for the most
part, I think we
capture the vast majority
of these kinds of libraries
and their behaviors.
And so what we did
is we crowdsourced
people's expectations of
a core set of 837 apps.
So an example question
would be-- &quot;How comfortable
are you with drag racing
using your location for ads?&quot;
So it's a little
bit like Mad Libs,
if you every played
those games, where
you starting filling
in the blanks,
and we asked 20 people per
question of these as well, too.
And here's the heart of how
PrivacyGrade currently works,
is that we have this
particular model that
tries to guess how comfortable
or uncomfortable people are
based on their behavior.
So we basically
aggregated the results
across all the
different apps to try
to understand the behaviors.
So, for example, if you
look at the x-axis here,
we have Internal Ads,
Analytics, and SNS.
So SNS is Social
Networking Services;
Analytics, straightforward;
Ads is straightforward.
Internal, in retrospect,
was probably not best name
we could have come up with,
because it sounds like it's
just only using it internally.
It just means that's the custom
code that we can't analyze
easily what it's doing,
so we'll probably
rename that in future work.
The y-axis we have the
different locations,
Phone_State, Contact,
SMS, and Account Usage.
So one concrete
example here, you
can see that using location
data for social networking
in the very top right corner
right here, we have 0.28.
So that means that people are
generally neutral to mildly
comfortable with using location
data for social networking.
But if you're
looking for ads, you
can see that it's negative 0.3,
so that means people are less
comfortable with that, too.
And the blank spots
here are basically
cases where we didn't
have a lot of data.
So, for example, SMS
used for analytics is
very kind of a rare scenario.

Here's the overall
stats on PrivacyGrade,
and this is as of April 2015.
This is when we did
our most recent crawl.
No sensitive permissions means
that we assigned an A plus,
and so you can see
about 6.8% of these apps
we assigned an A plus.
The other grades are set at
quartiles of the entire grade
range, so just do
really simple things.
And the big surprise for
me is that, if we just
did this simple approach,
that the vast majority of apps
actually get an A. I was
actually very, very surprised
at this.
I thought it would be
the other direction,
but it actually turns out
that not that many apps really
use that much sensitive data.
Then we have B's, C's and D's,
as you can see there, too,
so roughly 3% to 4% as well.
Now, this is a
really fun one here,
which is how have the grades
changed between the two crawls?
So we did a crawl back in 2014.
So October 2014 is
when we finished
that first run of crawl.
And then the other one is
what the current grades are
based on the April 2015 crawl.
And so what I'm going
to highlight here
are some of the
interesting parts.
So as you can see, if we just
highlight the x-axis here as
to identity, so
there's a lot of apps
that still have this exact same
grade in the previous crawl
to the current crawl.
So this, as you might expect,
but the interesting part
is this 83.52 for B to B. You
can see that's the lowest one.
And so if you go to
the next part, which
is how many apps have
actually improved their grade?
This is actually pretty good.
It's actually better than
I thought it would be.
And, in fact, you can see
the biggest one right here
is apps going from B
to A, so 10% there.
So I don't know if I can
claim credit for this.
I'm happy to immodestly
claim credit for it,
but I think this is
actually a pretty good sign.
That means that some
of these app developers
actually are
interested in privacy,
and I'm hoping that we have
some influence on this, too.
AUDIENCE: Well, or user
expectations change.
JASON HONG: That could also
be a possibility as well, too.
AUDIENCE: Is it
possible, because
of the change of the permission
model in [INAUDIBLE]?
JASON HONG: Yeah.
This is definitely possible.
The thing is that we don't
have a root cause analysis,
so it definitely could be.
We don't know for sure,
but I know for sure
that there's at least
one developer that
has changed their
behaviors, which
I'll talk about in a second.
This is another one that I find
actually really interesting,
too, and I don't have
a strong analysis
here yet, because it's
sort of beyond what we
can do right now.
A lot of the apps
were actually deleted,
and this is across the board.
Even apps with high grades
were removed as well,
so you can see that
for A plus, a third
of the apps that
we had given A plus
were removed between
the two crawls.
Interestingly enough, the C's
and D's had the highest one,
with about 50% of those removed.
I'm actually not sure
why they were deleted.
Whether it's because
of the privacy
issues, because they were low
quality, because there were too
spammy, or some other things.
I do know that it was
much harder for me
to find bad samples over time.
So whenever I talk
to journalists,
I'm always trying to
find more examples,
and it just becomes a longer
process every single time.
Our team also suspects
that some of these apps
are re-uploaded too under
different developers, so
the most spammy ones.
We actually do have the
star ratings for these,
so we'll probably just
look at how well does
this correlate with
the star ratings
and see how many of those
were removed as well.
But right now this is
just conjecture, too
So we do have a mobile app now.
So we just created this,
one of the undergrads
has created this.
What it does is it scans the
apps you have on your phone
and just retrieves the
grades from our site
using a simple JSON
protocol that we created.
We just now need to submit
this to Google Play.
I have to admit that it's
a little embarrassing
that, you know, I
did upload this,
and then it was actually
removed by Google Play,
because we're scanning
the apps, and we
didn't have the
notification saying that it
scans the apps on your phone.
So we've actually
added that, and now
we just need to do the last
checks before we upload it
again, and hopefully it
won't be removed this time.
Hopefully our account
won't be blocked.
But, yeah, so this will exist.
This is actually the
most requested feature
from all the people who
give us feedback-- is there
a mobile app that we can use?
Another thing we've
been doing is also
looking at how to improve
the grading model.
Right now, as you can see,
the grading model we use
is actually very simple.
It's just a very
simple linear model.
There's another person--
his name is Song--
who is developing a richer
machine learning model,
and what we've
been doing is just
doing lots of feature
engineering here.
So, for example, we grab
all the Google Play metadata
and text description.
We do app subjective
qualities, so star ratings,
number of downloads, and so on.
We're looking at app quality, so
does this-- and these are weak
proxies of quality.
I should note the size.
So for example, does
this app have a website?
Is there a privacy policy link?
How many resource
files are there?
How big is the app?
How many activities
there are, and so on.
And then there's also
privacy-related feature.
And again, these
are weak proxies.
Some of them are weak
proxies so the URLs and code
and popularity of
permission purposes.
So by permission purpose I
mean permissions location
and permissions ads.
So by the popularity
of it, you know,
location for ads and
games is very popular.
So generally you might
guess that that's
a lower concern in
general, whereas location
for ads in business apps is very
uncommon so the concerns there
might be higher.
We also did a lot
more crowdsourcing,
and we have a lot more
data, too, so 16,000 ratings
versus 7,000 in the
previous studies.
And here's sort of the
preliminary results.
We still have some more
analysis that we're doing,
but just for the Pearson
Correlations, the Pearson
correlations are
just like-- well,
just think about it
as a correlation.
On the left side are the
negative ones; on the right
side are the positive ones.
So there's some unusual cases.
So,
For example, on the left
side, the installation size.
The larger
installation size then
seems to be a lower
privacy grade.
Audio count, too.
The more audio files,
too, that they also seems
to have a lower privacy grade.
We're not sure what that means.
But there's other ones
that do make sense, too,
so for the more mobile analysis
you have, the more targeted ads
you have.
Then this also works, too.
Oddly enough some of
the app quality features
were also positive as well.
So as you can see,
the positive ones
are much larger than a lot
of the negative ones, too.
So, for example, the number
of receivers you have,
your permission
count, the permission,
which is also unusual
the permission
popularity by category, layout,
XML, and number of pictures
you have, too.
So these again are just sort
of the weak correlations,
and this is not really
part of the model,
because the model is going
to be nonlinear in this case.
So we're looking at how to
do some of these expectation
maximization approaches
to improve things.

OK, so there's a
lot of limitations
to our current approach as well.
So PrivacyGrade
works for most apps,
but for the most popular
apps, it probably
does not work as well, because
the most popular apps have
a lot of custom
code inside of them.
And so that's what I'm going to
talk about next is can we also
guess what things are doing
inside with custom code?
We also didn't
analyze the back end,
because there's no easy way
to do that right now for us.
We also only analyze free apps.
We found out there's
limitations on downloading apps,
too, so that we can
only download about 20
of these paid apps per day.
And so there's no
easy way for us
to analyze those, unfortunately.
We also assumed that most
libraries have just one
purpose, and we did
analysis of this.
That's true for about the
vast majority about 350
of the 400 libraries
were looked at.
That's generally true that
there's one major purpose.
However, we discovered
there's more--
a few of these apps
or these libraries
that do have multiple purposes.
The most common one is
analytics and advertising,
so we've seen apps that
used to do just advertising.
Now they're doing advertising
and analytics and vice
versa as well, too.
So we might just create
a new category for that.
In terms of the
impact, this is also
a little bit of a preview of
how-- my thoughts on how we can
influence privacy practically.
Even though PrivacyGrade
looks like it's for end users,
it turns out that we weren't
really targeting end users.
We're really targeting the
journalists, government,
and developers, too.
And so, with respect to the
journalists, &quot;New York Times,&quot;
CNN, BBC, and CBS have been
helping to popularize this,
so that's been very helpful.
For the government,
some of our earlier work
helped lead to FTC
fines, and I also
managed to scare the
bejesus out of a lot
of these congressional staffers,
which might be good, too,
so trying to tell them about
these third party libraries
and what all the data is
being captured about them.
And, again, there's at
least one developer--
and it's also a pretty
popular developer, too--
that did change their behavior.
So this is actually from
a press release they had.
&quot;In light of the results from
a recent study by PrivacyGrade,
Halfbrick,&quot; who makes Fruit
Ninja and Jetpack Joyride,
&quot;is going to still
greater lengths
to inform players about
what data the game
is collecting during the
in-game experience and why.&quot;
And they also did reduce
the amount of permissions
they were collecting, too.
Whether other developers
also followed suit,
it's not clear to us, because
they haven't necessarily
informed us.
So the next part I'll talk
about is using text mining
to infer privacy-related
app behaviors.
Does anyone have any
questions before going on
to the next part?

AUDIENCE: So how many
of the B to A grade
changes happen because
of the FTC fines?
Do you know how many
fines were imposed?
JASON HONG: Yeah, the
way the FTC works,
is they work by using
a very large club,
and so what they
do is they-- this
is just their style of war.
They find one egregious
violator of their policies,
and they whack it
really, really hard,
and then they assume everybody
else is going to follow suit.
So this is what they did
with Brightest Flashlight is
they did a pretty
large fine on them--
I think they also reduced
the fine afterwards
too-- and then assume
that other people see,
oh, we might want to pay
attention to what they're doing
and make sure we don't
do the same thing.
So whether multiple apps
change their behaviors,
again, it's hard for us
to tell cause and effect,
but that's their general style
of the work that they do.
OK, so the next
part I'll talk about
is using text mining for
privacy-related app behaviors,
and the basic idea here is
what I've been alluding to.
Can we infer the
purpose of the custom
written code inside of an app?
And in general, this is a
very, very hard problem,
because if we
could do this, then
we could also find all the
malware out there easily.
We could also do
automated testing easily.
We could also do program
correctness easily.
So, as you can see, these
are all unsolved problems.
So what we're trying to do is
can we just do our best guess
and see how well
we can also do that
using some machine
learning techniques.
And the basic
insight we have here
is that if you look
at compiled Java code,
it actually still retains a
lot of strings inside of it,
which we can use to
infer the purpose.
And so here's a concrete
example of this.
Let's say that an app
uses location data,
and if we decompile the app and
we see a whole bunch of strings
inside of it that say things
like Exit or Photo or Tag,
then you could probably guess
with pretty high accuracy
that it's probably doing
geotagging inside of the app,
because people have probably
not named their variables
things like this or the
method names like this.
And so like good
computer scientists,
or if you're really good
kind of security mindset,
you're probably thinking,
oh, what about the cases
where this doesn't work?
So obfuscated code
is an obvious case.
So it turns out that roughly 10%
of the apps inside of our study
were obfuscated, so we can't
do anything with those.
Deliberate manipulation,
so our working assumption
is that app developers
had no reason
to do deliberate manipulation
in this current study.
In future cases,
they might do it,
but I think it's a safe
assumption that most developers
won't be doing this.
And the other one is, if
you've ever seen code,
you've probably seen really
bad variable names too,
like A1, A2, B1.
And so yes, this does make
our life very painful.
But this technique
still works better
than you might
expect in most cases,
because humans
generally still have
to name their classes
pretty good things,
even if they don't name
their variables good things.
So what we did is we
gathered and decompiled
622 apps, only ones that
were using Find Location
and reading the Contact list.
And we made a
label and clustered
by purposes based on app
description and code,
so the text description on
Google Play as well as the code
that we can inspect.
And here's the different
permissions that we had.
So location permission,
we've actually
expanded the number of
categories that we had, too.
So, for example, for
location permission,
we have Nearby
Places, so find me
nearby restaurants, for example.
Location-based
customization, Turn off
my phone when I go into-- when
I go home, things like that.
Turn off Wi-Fi or
turn on Wi-Fi when
it goes to those places,
traffic information and so on.
On the right side here
is are the categories.
We came up with
Contacts Permission.
So Backup and Synchronization
is a really popular one.
Contact Management, so helping
me manage all my contact lists.
My favorite one is actually
number nine-- Fake Calls
and SMS.
So this is in case
you're in a bad date,
and then you can set it up
so call me in 10 minutes
and then you can try to
get out of the bad date.
There's also a few
apps that we just
couldn't figure out at all.
It was just really
complex code, and we
couldn't figure out what they
were doing with location data.
So not a great sign, but this
is actually a very rare case.
Here's the general
system architecture.
So we take all these apps.
We decompiled them/ we
identified where in the code it
was using sensitive permission,
so location data, contact list.
We do a lot of machines learning
feature extractions there,
and then we train
a classifier, too.
Again, we're just looking
at the custom code,
not for the libraries, because
we're looking at the new part
that we didn't
know how to do yet.
So here's some more details
if you're interested.
So we used Apktool to
decompile to Smali,
which is intermediate code.
This was helpful
for us to identify
which parts of the code
used location or contact
list, because there's actually
a well-known mapping that
says, well, if you're using
this method, that means
you're using location data.
I'll talk about that in
the next step, actually.
Then we used dex2jar
and this other library
JD-Core-Java to decompile
the Java source code.
This is what we used to
extract the features.
For identifying the
permission-related code,
we used a technique similar
to a tool called PScout,
and this was published
at CCS back in 2012
at the security conference.
Basically, again, they
just had a gigantic map
of methods saying that
this method-- if you
call this method, that means
it's going to use contact list.
If you use this
method, it's going
to use location data, and so on.
So we just used
that simple mapping
and just looked for the
methods that were called.
For the granularity,
we used the package
as the granularity of code.
The reason for this is that
the package is just generally
a directory, so a
combination of source code
that is somewhat cohesive.
The reason for that is because
individual Java files tend
to not have enough
semantic information
to choose the
analysis for us, so we
tried to expand a little bit
more to the package level.
For the third step, we
extracted all these features.
We have two
different categories.
The first one is
app-specific features.
So we created feature vector
680 permission-related APIs--
so that's from the mapping
I mentioned in the section--
97 intents and 78
content providers.
And so, again, just to
convey the intuition,
if you see that
it's using contact
list plus the email
activity, it's
probably using it for email.
And if you're using
the contact permission
plus sending SMS
APIs, you can probably
guess what it's
doing as well, too.
AUDIENCE: Does that mean the
dimensionality of the system
is somewhere around 900, and
you have 600 training examples?
JASON HONG: No, it's 680
feature vector, so just--
AUDIENCE: 680.
JASON HONG: No,
actually you're right.
It's 600, 700, 800.
So, yeah, about 800.
AUDIENCE: So you
are vastly sparse.
JASON HONG: Yeah, and
unfortunately it is sparse.
But the next part
actually helps us, too,
which is trying to extract the
strengths from the identifiers.
So in some ways this
part is also sparse, too,
but in terms of the
[INAUDIBLE] validation,
it actually worked pretty well.
There's their good one.
So what we do is we
extract those strings
from the identifiers.
So it turns out that in Java
the package names, classes,
methods, and field
names are kept,
so this include includes
constants and class variables.
Unfortunately, probably
the most valuable ones--
the local variables
and parameter names--
are not reserved, so this is
sort of unfortunate for us.
We used a whole
bunch of heuristics
to split the identifiers
into individual words.
So for example, Find
Restaurant would be split
into &quot;find&quot; and &quot;restaurant.&quot;
We used stemming, dictionaries,
and a whole bunch of heuristics
to do that.
We also use TF-IDF to
score each word, too,
to try to figure out how common
that word is relative to all
the words that were out there
inside the entire data set.
In terms of how well
it actually works,
so maximum entropy
actually works out
the best in terms
of the accuracy,
so just highlighting here,
so 85% for location use,
and 94% for context.
So generally context
was easier for us
to guess what the behaviors
were inside of it.
If we look at the specific
categories for the purposes,
this would have been
better for animation,
but the ones that
are highlighted,
the 78% there for
the F measure--
I should probably explain for
the people who don't know.
F measure is basically sort of
like the harmonic mean of-- I
believe it's harmonic mean
of the precision of recall.
AUDIENCE: Yeah, that's right.
JASON HONG: Yeah.
So basically, higher is
better for all the numbers
here, is the main
thing to keep in mind.
And so you can see the F measure
for searching nearby places is
sort of decent, 78%.
Location customization
and transportation
is much higher, so 95%,
and recordings much lower,
too, at 77%.
And I'll give sort
of some insights
as to why we think
that's the case later on,
like why it sort of
fails in some cases.
For contact lists, it
actually does much better,
as you can see.
The worst case is actually the
contact management, about 81%.
And so, in cases
where this fails,
we found out basically
that really generic words
are the case where it just
does not work very well.
So we had one app that was
searching for nearby places,
and it had top keywords of
local, search, place, and find,
and these are just generic words
that almost all the apps would
use for a lot of their
kinds of functionality,
so this one was misclassified
as geosocial networking.
So in most of the cases where
we saw that where it failed,
it tended to have this problem.
The other problem is, as you
were sort alluding to, is
that when there are cases
where there are just
very few features inside of it,
so it just needs very few APIs
and also had very few keywords
and simple functionality,
we also misclassified
those as well, too.
But as you can also
imagine, the more features
there were, the words,
the much better it did.
We also saw a few odd
design patterns, which
made our life much harder, too.
So we found some
design patterns where
some apps have one object that
periodically gets location
data, and then you have
other parts of the app that
are requesting data from that
one app, that one object.
So basically they're using
a level in direction.
And so in that case,
we actually don't
succeed, because we don't really
know where the location is
going, because we're just doing
simple static analysis right
now.

OK.
So, again, that's just sort of
the high-level view of that.
Does anyone have any questions
about that part of the work?
Or get to the end.
AUDIENCE: Did you take
into account things like
the idea of local is often used
in like a software engineering
context?
JASON HONG: You mean locality?
Or you mean the word--
AUDIENCE: So you were talking
about the word &quot;local,&quot; yeah.
JASON HONG: Yeah, yeah.
So I guess-- I'm
trying to think.
Do people use that inside of
method names or class names?
AUDIENCE: I can imagine.
JASON HONG: Yeah,
that's probably true.
Yeah, I think that probably
would have been-- let me think.
That probably would
have been penalized
based on the TF-IDF if
lots of people do that.
But I could still see that
being a potential problem,
so trying to figure it out.
We did actually try LDA
approaches to start out with,
but that didn't work out
as well as we were hoping.
But now that I
think about it, we
might want to
resurrect that idea
to help us understand if
you see the word &quot;local&quot;
in certain contexts,
it's probably
more likely to mean local
search versus local variables.
That's a good observation.
AUDIENCE: How do the
strings that are not related
to the purpose enter into this?
So the strings that are in
an ad library, for example.
JASON HONG: So we're not
analyzing the libraries.
That is an idea that we're
thinking of doing next,
is to see if we can
automatically categorize
the purpose of these libraries.
But for those
other strings, they
tend to end up being noise
because of the TF-IDF.
There's nothing to
really classify it on.

OK, so I'll wrap up with some
more reflections on privacy
ecosystem-- and
a lot of this was
stuff I've been alluding
to-- is that we should really
consider the entire ecosystem
for privacy here, too.
Most of the research
in the community
has been focusing on
end users, I would say,
the user interfaces for it and
understanding their preferences
and so on.
I'm actually becoming
increasingly skeptical of that
over time as to the end users,
because it just puts too
much burden on the end users.
And I think it also puts-- it
sort of gives, I would say,
developers an easy way out.
It's like, well, we have
this gigantic privacy policy,
and they can just read that.
And I think we all know that's
just really ineffective.
It's also really hard to change
people's awareness, knowledge,
and motivation as well
with respect to privacy.
I have another paper I can
also tell you about this, too,
or using social
psychology techniques
to try to improve these things.
But, in general, it's
just really, really hard
to try to change
people's behavior.
And it's sort of
rational, because reading
this gigantic privacy
policy is a known
cost for unknown benefit.
And so it's-- again, it's
very rational for people not
to really focus too much on it.
Instead, I think we should focus
a lot more on these other parts
of the ecosystem
like the developers,
third-party developers, markets,
OS, and third-party applicants.
So I'll talk more
about the developers
on the next slide and the
third-party developers, too.
The markets is also
an obvious place,
where I'm sure you guys are
also doing a lot on that, too,
and the same with
the operating system.
The challenge there is that
these kinds of solutions
have to be very generic
that worked across lots
of different kinds of
apps, and I'm not sure
that they're going
to work really
well for these really
narrow apps that
tend to collect lots of
data, because it's just
going to be hard to know
that that's what's going on.
The third-party advocates I
think is also pretty promising.
The FTC, for example, I
mentioned is a good one.
I've also met with
&quot;Consumer Reports,&quot; too,
and I think that's also a really
good area for improving things,
because trusted third party
where it can also try to rate
these things.
The problem with FTC
and &quot;Consumer Reports,&quot;
though, is that the current
approach is very manual
and labor intensive.
So for the FTC, they just
have an army of lawyers,
and they-- for
example, the ones who
are looking to try to
find violations of COPPA--
the Children's Online
Privacy Protection Act--
they're just mainly
inspecting lots of apps,
and they can only do
like maybe a dozen or so
per day to try to
understand what's going on.
As you can see, they're
just outnumbered, vastly,
and it's just not
going to work out.
The other problem with the
FTC, which they're currently
trying to rectify, is that--
again, it's all lawyers--
and now they're trying
to get some software
developers to help them
extend the kind of analysis
that they can do.
But that's still sort of a
long-term kind of a problem
they're tyring to address.
I think that helping
developers is the greatest
point of leverage,
and this is really
the main theme of the work
that my team is trying
to do more of moving forward.
So examples would be
better understanding
of third-party libraries.
So this could be-- you can
imagine better summaries
and for developers
to understand, well,
to use this library, here's
the kinds of challenges.
Here's how you might be able
to configure the library so
that it does the right things
so you're not violating
COPPA or other kinds of things.
I
Think better design
patterns for privacy
would also help out a lot.
So you can imagine there's
lots of guidelines,
but there's nothing like
having a lot of design pattern
saying, here's how you
should structure a code,
or if you're
collecting this data,
here's different ways
of dealing with it,
and I think that would
help out a lot, too.
Better APIs is also
something my team's
been looking at right now.
So you can imagine that
if an app was asking you,
are you at home or work,
just those two locations,
and its operating system could
help mediate that versus,
tell me your current
location, and then
I'll infer whether
it's home or work.
So here's a place where I
think the OS could offer
a lot of support, and
if we could figure out
better levels of
abstractions, I think
this would be really good, too.
And so, just roughly, the
current idea we're thinking of
is, what if we had
tiers of abstraction?
So Tier 1 might be,
well, full data,
and this is the most sensitive
data, and Tier 3 or 4 might be,
here's the least sensitive
data from what we can tell,
like you're at home, you're
at work, you're out traveling.
And then you could
also-- if we had
these different tiers of APIs,
we could start categorizing.
Well, now you're actually
collecting lots of data.
We might suggest you use some
of these other APIs instead.
And so this might also help
us infer the privacy grade
as well.
So it's sort of
an abstract idea,
but we're trying to figure out
ways of operationalizing it.
The other one, too, I would say
is I think pretty interesting,
but also not clear how
to operationalize yet,
is if we had better
reusable components.
So one of the things I
find really fascinating
about databases is there's
a set of properties
they have called ACID.
ACID stands for Atomic,
Consistent, Isolated,
and Durable.
And the really beautiful thing
is that you, as a developer,
don't have to know
anything about databases
and these properties, and
you get them all for free.
So if we could identify
useful properties
inside data collection or inside
of how the data is manipulated,
and you can build
it into the most
common kinds of open source
tools that people would use,
then we could also start
nudging people towards privacy
to make it the path
of least resistance.
So, again, the developers
already have a lot of burden
right now, so you
can streamline things
to make it really easy to do.
So another concrete example is
the graphical user interface.
You don't have to know much
about how graphical user
interface widgets work.
You just have to grab.
Here's a [INAUDIBLE],
here's a check box,
here's this other thing.
And it just makes
it really easy,
and then it also
makes the interfaces
more reliable as well.
So it may be the path
of least resistance,
the easy thing to do
for-- the right thing
to do for the vast
majority of cases.
OK, so that's pretty
much it for the talk.
So I want to thank everybody.
So two of the people here,
at least, are on call,
so Shah and Jialiu Kevin Ku is
also an intern here at Google
this summer, so I
would also say you
guys were really smart to hire
him, and he's really fantastic.
He helped us do a lot of
the analysis and updates
to the crawl.
Song, he just graduated, so
you guys were all smart enough
to give him an offer, but he
decided to go to Uber instead.
[INAUDIBLE].
Bharadwaj is also the person who
created the mobile app as well,
too.
And special thanks to
all the funders and also
say Google has been very
generous in the funding,
and I would also like to
thank you guys for that.
All right.
Thank you, and happy to
answer any questions you
guys might have.
AUDIENCE: Yeah, so
you said that one
of the areas that would
be potentially fruitful
would be helping developers,
and you talked earlier about
how developers often
don't understand
what's going on in the
libraries that they're using.
How big is that problem,
because if the developers--
if only a few of them don't
understand the problems, then
helping them to understand
it won't help that much,
if it's case that
most developers do
understand what's going
on, and they're happily
selling your privacy, because
their motivations are mixed.
How about that?
JASON HONG: Yeah, I think
that's a great question.
From what I can tell,
developers have a big idea
that there's data
clashing going on,
but they have no idea to what
extent, how to configure it.
And again, their main
motivation is just to,
let's get it out there and
then, oh, we need to make money.
Let's grab this ad library.
That's the simplest thing we
do, All right, let's do that.
We've seen that kind of
pattern very, very often.
And I would say
there's definitely
long-tail developers.
There's some that can devote
lots of time to understand
this, but the vast
majority, I think can't.
And so if there's better ways
of helping them integrate this,
like I can imagine better
tools for Eclipse maybe.
That might suggest, here's
what's going on with the data,
or maybe suggesting
different libraries,
or even understanding
COPPA as well, too.
Because I think,
again, they sort of
have this vague notion
that something's wrong
but just don't know
what to do about it.
And I think that was
pretty universal across all
the interviews, and I'm not
sure if that's really supported
by the client day to day.
AUDIENCE: Our
experience has been
that a lot of the
questionable behavior we've
seen on behalf of
the [INAUDIBLE]
has been on behalf of, say,
an ad library [INAUDIBLE].
JASON HONG: Yeah.
Yeah, we've also seen some newer
behaviors by some ad libraries
where they're trying to get
more of the developers involved.
And we think-- we're
not entirely sure why.
I think it's probably to
try to shift the blame
from [INAUDIBLE] to developers.
So, for example, they ask
you to get the location data,
and then send it to the library.
But, again, you know
developers, they
might get a sense as to,
well, something's weird,
but we also want
to make money, so
the garden path is,
OK, let's just do it,
without fully understanding
what's going on.
AUDIENCE: I thought you said
you rate the libraries as well.
JASON HONG: We
don't do that yet.
We've been thinking about
that, and that's also
a commonly requested feature
by some of the end users
as well as by the FTC.
We're not sure what
method we would use yet.
We have some ideas,
but we're not sure
how to execute on them yet.
But if you guys
have great ideas--
AUDIENCE: [INAUDIBLE]
to a developer--
JASON HONG: Exactly.
AUDIENCE: --saying this
library and this library both
do the same thing.
JASON HONG: Exactly.
AUDIENCE: Toss
together a couple apps,
and [INAUDIBLE] the
libraries [INAUDIBLE].
AUDIENCE: Two great tastes
that taste good together.
AUDIENCE: Jason,
can you hear me?
JASON HONG: Yes.
AUDIENCE: So I'm
wondering about what's
the sort of full
scope of things you're
hoping developers will do?
So clearly you're trying
to motivate them to maybe
be smart about the ad library
they pick, and you try
to motivate them to
use fewer permissions,
only the ones they
might really need.
What else is it
that you hope they
will do as a result of this?
JASON HONG: Yeah, I'm
hoping that they'll
be able to make good trade-offs,
is really the main thing.
You know, we've also seen
that in some of our studies--
this one's connected by
Jialiu-- that if people also
have a better understanding of
what's going on with the data,
then they tend to be
more comfortable with it.
So even if it's being--
location data or other stuff
is being used for
advertising, as long
as they see the trade off
there, then that's also good.
And so another one
I didn't mention,
because it's partly end
user and partly developer
is, if we can develop
better interfaces that
help convey to people--
but it's not really, again,
the end user part.
I don't want to put
the burden on them
But by having that
transparency, I
think that helps the developer
do better and also can
help with the FTC and &quot;Consumer
Reports&quot; and other kinds
of things too.
So that's part of
my goal, too, is
to get them thinking a
little bit more about this
and make the tools
easier to use.
Again, the garden
path is the safer path
with respect to privacy.
But I also have
emphasized, every time
I've talked to journalists, too,
that the app market is thriving
partly because we have
all these free apps,
and so we don't want to
destroy this ecosystem either.
So we want to make sure
that we can have wins
for everybody inside of this.
I'm not sure if that really
fully answers your question.
AUDIENCE: That's good, yeah.

AUDIENCE: I'd like to say
the [INAUDIBLE] because
even though developers
include those ad
library in their
application, but they
have to just [INAUDIBLE].

But if-- everybody's
naturally working it.
But if we penalize them based
on having those libraries there,
and they [INAUDIBLE]
because it's very hard
to penalize [INAUDIBLE].
JASON HONG: Yeah, that's
a good point, too.
Yeah, I should've added
that to our limitations,
that we're not actually checking
if all the most egregious
functions are invoked,
that they might
be using a less egregious one.
So it is sort of
an approximation.
One thing we've thought
about for developers, too
is that, if you're
using this library,
consider using this other
API, the less egregious one
versus the more egregious one.
But now we have to figure
out how to do that at scale.
We'll figure it out for
individual libraries.
Again, that's a really
good point, too.
AUDIENCE: So sort of
a related question.
Have you thought of
looking at the network
behavior of the apps to see
whether they're actually
sending the data off the
device, or which domains
they're communicating with?
JASON HONG: Yeah, so
we're not doing that yet.
Part of the challenge
there is that we
had to look at doing
dynamic analysis.
What we discovered is running
the Android emulator really
sort of-- I would say cripples.
You only run like one emulator
on your laptop at a time.
We tried running multiple
ones on virtual machines.
Shah was the person
who did that.
And we found that you could
only run like one or two
virtual machines on your device,
on your laptop or a computer
before.
That's pretty much
all you can really do.
And so we figured
it would be really
hard to do that at scale.
We do have monkey apps that--
we created some pretty good ones
that can try to traverse
all the different screens
and capture that data, too.
I think that probably the right
approach for privacy analysis
is that-- from
Google's perspective
is that you probably
want a thin layer that
does shallow analysis
across all the apps,
and then you probably
want deep analysis
where you have human in the loop
across the most popular apps.
And that's probably going
to be the best cost-benefit.
And so, Shah, he did for
his dissertation work
a tool that helped you do
that kind of deep analysis.
So it runs all the
different screens for you.
It tries to capture here's
what data is being sent out,
and it happened because
you clicked on this button.
And then you can
help the analyst
through a lot of
heuristics, too.
So this seems to
be odd behavior.
It's an app that only calls
one phone number, sort
of odd behavior, or it
uses this combination
of permissions, which is also
potentially unusual behavior,
and so by helping to
focus the analyst's
attention at the right places.
So, yeah, we have
thought about that.
I'm also thinking about doing
that for Internet of Things
as well to help understand from
the network analysis what kind
of data is being sent across.
Of course, the challenge
is that once they use SSL,
then that makes it
incredibly difficult to do.
AUDIENCE: Well, you'd be
surprised how few of them
do that.
JASON HONG: Yeah, yeah,
we also know that, too.
Yeah, in fact,
that's one thing--
AUDIENCE: I don't think
that's going to be
a major hindrance to that.
JASON HONG: Yeah, that's
one of the heuristics
that Shah also had
in his work, too.
He's like app doesn't
use SSL properly,
which is easy to find.

AUDIENCE: I wonder if
there isn't another library
that you've missed analyzing
that's right there, which
is the toss libraries encoded
in a language called legalese.

Could these bag of words,
techniques, information
retrieval techniques
also be used
to analyze that at scale
that you can't do if you
individually try to read them?
JASON HONG: It's
funny you mention
that, because that's
one of the projects
we are currently working on.
The basic idea-- I
don't know if you know
what the website kittenwar is.
If you don't what
kittenwar is, you
will love it, if you like cats.
And if you don't like cats,
go to puppywar instead.
And the basic idea
is, it's A/B, right?
So It's like, which
cat is cuter, A or B?
Repeat.
Which cat is cuter?
A or B?
Repeat.
And eventually you can actually
see the winningest cats
and the losingest cats, and
we're applying the same idea
to privacy policies, too, so
we're-- like which thing is
more important now?
The site we care about privacy,
or we will use this data
for these purposes?
And so based off of
that, we're trying
to create a predictive model
as to what kinds of things
tend to go to the top.
We actually-- in our
preliminary results--
we haven't published
this yet, which
is why I didn't
talk about it here.
In our preliminary results, we
tried it on Google's privacy
policy, and things
like &quot;Google really
cares about your privacy
policy, or privacy&quot; just
goes straight to the bottom.
No one cares about this.
And things like Google
might use your call data
for X, that goes
straight to the top.
Location data also goes
straight to the top as well.
And so we are trying to look at
creating the predictive model,
and then we are looking
to augment PrivacyGrade
with those privacy
policies as well, too,
the summaries of
them, and also looking
at how we can put inside
the web browser, Chrome,
and all those kinds of things.
But we're still sort
of in the-- we're maybe
like a third of the way
through the research.
We've tried a few iterations.
We've found things
that don't work,
a lot of things that
don't work, but we
have found a few things
that do seem to work.
And we're also
looking to try to see
if we can add in Elo ratings
and other kinds of things, too.
So because if you have
10 statements, and 10
choose two is
actually not that big,
but if you have 100
statements, it's pretty big.
And so we also have
to try to figure out--
be judicious in terms of
the crowdsourcing of A/B
testing there.
So the short answer is yes.
AUDIENCE: [INAUDIBLE].
AUDIENCE: But most of these
guys aren't that smart
because they don't have
lawyers working for them,
so they most likely might just
copy-paste the known policy
for [INAUDIBLE] their own app.
JASON HONG: Yeah, I'm pretty
sure that happens, too.
In fact, we're
starting-- actually
we haven't done that yet,
but I want to hire someone
to crawl lots of
privacy policies,
and we can sort of see the
genetics of it as well.
I'm pretty sure they are
copied and pasted a lot,
but we also want to just crawl
the privacy policies so that we
can start trying to
see changes over time
and also do a lot more
of the A/B testing.
AUDIENCE: And when you
started grading these apps,
did you receive any complaints
from the app developer,
and how could you give
us this low rating?
AUDIENCE: No, surprisingly no.
The only request for takedown--
everyone requesting take-down
has been very polite.
One requested for
copyright reasons,
because of their
text description.
So it's like, all right, fine.
Another one is, when
we looked at it,
we thought that
there were probably
errors in our analysis,
so we actually--
one thing I didn't mention.
We actually don't put grades on
anti-virus, because they just
use lots of permissions,
and it's pretty clear
that they're not using
for malicious purposes,
but according to our model, we
gave them a pretty low grade,
so we just remove those off the
bat, because of the accuracy.
And then, let's see.
I'm trying to remember
what the other request was.
Yeah, there was
another one, too,
but it wasn't for grade reasons.
But, yeah, we actually
have had a little feedback
from developers, too.
I find that a little
bit surprising,
but I figure they
just, I'll just
take the garden path,
because, like well, all right,
let's just change our app or
rename it or re-offload it
or do something.

AUDIENCE: So Jason, does
PrivacyGrade do anything
like let people who come
and go look at the grades
for all the apps that have
the same functionality
sort of next to each other?
Like I'd need to search for
all the flashlight apps,
and look at them side by side,
because that way I can pick
the one with the best grade.
It would make it easy for
people to make those decisions.
JASON HONG: Right, right.
So right now it only offers
a keyword search rather than
functionality search,
because we're not sure
how we can do the
functionality search.
We have thought about how
to add recommended apps.
And so we might show--
instead of the Google Play
recommendations, we might
show other recommendations.
I did have a summer intern
working on that last summer,
but we've just been
short on resources,
because unfortunately,
almost everyone on that list
is graduating or has graduated.
I am actually looking to hire
three PhD students in this fall
and see how we can
continue things.

Yes, so the short
answer is sort of.
AUDIENCE: OK.
JASON HONG: OK.
So I'll be around for
the rest of the day,
so I'd be happy to chat
with the rest of you guys.
Thank you very much, and if
you have any more feedback,
please feel free
to send it to me.
AUDIENCE: Thank you very much.
[APPLAUSE]
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>