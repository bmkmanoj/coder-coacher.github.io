<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>DocEng 2011: A Study of the Interaction of Paper Substrates on Printed Forensic Imaging | Coder Coacher - Coaching Coders</title><meta content="DocEng 2011: A Study of the Interaction of Paper Substrates on Printed Forensic Imaging - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>DocEng 2011: A Study of the Interaction of Paper Substrates on Printed Forensic Imaging</b></h2><h5 class="post__date">2011-10-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/cYkULBtplrQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so this papers in two parts so it's a
short paper because it's a continuation
of some existing work and the existing
work is on print forensics and the
particularly bit we're doing here is a
study looking at how the substrate
affects this so they said the paper type
we're using and so first of all I'll do
an introduction to the forensics and
then I'll get back to the particular
experience we've done here so this is
joint work with guy Adams who's hardware
engineer and is responsible for the
hardware and also collected all the data
for the suit for these experiments which
was a considerable task and then the
image processing and coding which is was
the responsibility of the two Stevens
and I'm the Steven in the middle so
people may be familiar with printer
forensics where you're trying to did you
get some document you want to know which
printer was printed on but what way what
we're looking at is print forensics
where we want to identify individual
printed items so every single thing you
print if you look at it in enough detail
it's it has its own unique fingerprint
and we can really robustly identify each
and every printed item so we show here
with these high-resolution images if you
really look at this is particularly bad
because it's something very small inkjet
characters only six point text but if
you look at those you can see that each
of them has you know some considerable
differences which are due to the way the
ink splatters and the substrate you're
printing on and this is true of any
print type you know across the board
everything if you look at it in a detail
there's enough there to get a unique
image of it a unique representation of
it conversely if we look at this with a
low resolution scanner everything gets
blurred away so the image on the on the
right here is the same as the image in
the center but it looks you know you
can't get all those all those features
from it so essentially we want to be
able to do that so to be able to do to
be able to identify things uniquely we
need some low cost hardware to do this
as we could put with every printer and
we need some image processing so the
hardware is based on the realization
that any set you know any sense you
by these days the sticking mobile phone
is actually really high resolution and
if we're able to use just one-to-one
optics we will have the resolution we
require so it was just like a much more
if we do this this provides if we will
do it in a contact fashion we don't have
any problems with with depth of field so
so so the the hardware we used is based
on something called a Dyson relay lens
which is a very low-cost way of
achieving one-to-one optics in a contact
image fashion so to show it so this is
the device we've built here which was
built by guy Adams and it uses a simple
low-cost mobile phone sensor and we look
at the objects themselves while these
objects were designed for is for
replicating things and they're used in
the semiconductor industry and basically
all the consist of is a single
refractive surface and a mirror and it
allows to you to replicate an object by
an image as we can see in the top
situation all we add here is we add a a
sensor off to the left or down to the
bottom and introduced some illumination
device so it's a slightly more
complicated situation than usual but
it's still low cost and very simple to
make so based on this we can do these
very high-resolution images so then we
need some really robust image processing
that enables us to get this stuff easily
and those people who came along to the
demo the other day will have seen that
you know it's it's it's it does give us
exceedingly a high confidence robust
imaging and we're able to robustly
identify every single printed item
uniquely as long as we take one the idea
is you take one image of it as you print
it and then store that in a database and
and later later we can compare any
subsequent image of it and identify them
uniquely and the way we do this is by
extracting some information based on a
model-based approach so the idea is we
have a reference model of the thing
to print and what we wanted to identify
is how the thing we actually printed
differs from the reference model so the
reference model just consists of a set
of points that identify that they
represent the outline of the thing
you're going to print so anything can be
used as a as a forensic mark in this
case we're going to use a letter A and
we're going to use the outline of the
letter A to provide a model so that we
can robustly identify it and the model
here we only show hundred points to the
model would actually consist of several
thousand points two thousand points in
the examples I'm showing here and we
also for every point have precompute
it's normal so that we know once we've
located the model what the normal
Direction is so going back to the how
this method works so we identify we have
this model we take an image with our
high resolution imaging device we locate
the instance of the of the of the
forensic mark that we're going to use in
this case it's a letter A and we because
we it's a model this model based
approach gives us a very high confidence
and very well located registration of
our outline on the actual image and from
that we're able to identify how this
particular letter a differs from every
other letter a that was printed and the
way we do this is use the normal
information to extract a border image
around the around the the perimeter of
this particular item and from that we
extract a signature profile which is a
representation of that border the way
that's done is sort of general edge to
proach but it uses all the edge
information that's orthogonal to the to
the printed item so we compute some edge
strengths and then we combine those of
using a windowing function I mean the
details aren't important they are in the
paper but the idea is we come up with
this unique representation which is a
model based signature profile for any
particular representation of the
character now that's still quite a lot
of information but we have and we found
that we can actually reduce that
enormous li and come up with a a simple
coding scheme where we get down to a few
hundred integer values and where we do
that is to
we chunk this this very high-resolution
warping descriptor and we measure
variances that on these chunks so in
this case we have 5050 chunks across our
shape and we measure variances and then
we come up with a mean variance we use
that as a coding parameter to come down
to a very small number of bits at the
end of the day and to show how how this
is effective we here we have two
different signature profiles extracted
from to two captures the same piece of
text the same text night in the same a
terrain and we extract two separate
codings or the difference between those
codings gives us something we call the
shape distortion encoding distance as we
can see that say in this case it comes
out with the value point two two and
well as well see later
that's quite a that's a low value in
comparison to what we get when we
compare two different letter A's so the
sort of experiments we do with this with
a system is we take a large number of
images with one dr sid device and then
another set of images with another dr.
Sid device but where where each you know
for each one we've got you know we have
corresponding pairs of images of the
same character in this case we have 36
instances for most of the experience
were going to show here we collect 40
images of each letter so if you look at
the difference between correct and false
matches here we see that you know the
correct matches all have you know rather
low s de D values and our incorrect
matches have you know well separated
from them and these two distributions
are very well separated which allows us
to to differentiate correct and and
verified instances if we look how this
parameter as we change the number of of
elements that are used to represent s de
D we can plot false matches against the
valid matches and we find that the best
value
to use about 200 elements to describe
the to describe each of our characters
in is that gives us an optimal
separation between correct him for
invalid matches so that's you know
that's the basic technique and as those
who came to the demonstration in today
will have seen this given you know it
gives us a really strong ability to to
uniquely identify printed instances so
all wouldn't do in this and the
experiments here is see how this was
affected by these particular substrates
that documents were printed on so we've
got several examples here we've got
laser printed examples inkjet printed
examples printed on a quite a wide
cross-section of paper types and what we
find is that but that right across the I
mean we found that you know we got
better we got good results right across
this set of paper types they only took
paper types they didn't work very well
where I should ease these the handmade
paper types which have a very a lot of
paper structure which is a
counterintuitive result because you
would expect that the amount you know
that having more random information
would actually give you more more
information in your in your shape
description and yet we found that that
that wasn't the case and that's because
these paper types have a systematic
structure to them and what that this
method is relying on is the random
element rather than the systematic
element another way we can represent the
the difference between our our
populations of correct and incorrect
matches is 2s2 use a modified Zed score
which gives the distance between two
populations and if we look at results in
this case we find that
that apart from the two the two handmade
paper types we're getting good results
with z-scores between five and six and
above and those you know that means that
you know you've got a like a one in a
billion chance of incorrectly
incorrectly getting a false positive or
a false negative boom
rather we also had a look what happens
if you only use the substrate part of
the information and ignore what you've
printed so it's only using that the
substrate and quite interestingly we're
all so we're able without using anything
you print just using the variations in
the substrate underneath you're still
able to differentiate correct and
incorrect matches and the reason so
provided you're using your character to
give you accurate registration we can
still differentiate correct incorrect
matches apart from the case of the
really glossy paper down on the bottom
right hand side because there's no
because it's her glossy it has no
information content
so in conclusion we've shown that you
can get forensic level of authentication
across a wide variety of substrates and
that substrate only forensic
authentication is possible for some of
the subjects enough for most substrate
types so this supports the use of prints
as a forensic marker it's ubiquitous and
cheap and it's part of the exit you know
we print things and we can you know
that's amazing that you can use print to
give you this level of identification of
documents so it's a simple a solution
and it's applicable to any text cliff or
printed mark
have you tested this using different
resolution cameras to sample the
signature and to test the other end so
we're we're done tests what we tried it
with scampers and that's kind of a - low
resolution to do it but one thing we
have done so this this is an area based
sensor so you would have to stop at the
moment you would rely on if it was
attached to a printer you would have to
have the document stationary and and an
either a user would have to offer you
know to scan it or you would have to it
would have to go and be stored in a bin
and sample so what we've also looked at
is using line scans sensors that could
be used as a document was passing
through them past them and that is
that's slightly lower resolution but
it's still like five micron pixels we
don't think we don't think you can get
less than about five micron or ten
micron I was just thinking that this
today we can get five megapixel cameras
cheaply next year it'll be eight mega
pick like no we don't mind going yeah
you can it will sky resolution will not
be an issue it may be that you you know
you have to sensor size sensor size
typically says stays the same it just
gets chopped up into some more and more
pieces there would be an issue of going
you know already you know you need
sufficient sensor size to cover a large
enough piece of printed a large enough
printed item so a larger piece of text
or any other printed mark but it doesn't
matter as long as you you know less than
say five microns then I think you know
things are okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>