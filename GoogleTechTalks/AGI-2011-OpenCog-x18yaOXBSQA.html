<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AGI 2011: OpenCog | Coder Coacher - Coaching Coders</title><meta content="AGI 2011: OpenCog - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>AGI 2011: OpenCog</b></h2><h5 class="post__date">2011-08-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/x18yaOXBSQA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right so Moshe was called off
somewhere so he's left me the task of
introducing myself so that's about it
yeah I'm uh I'm Ben gürsel I'm going to
talk about the OpenCog AGI project and
system which I've helped develop along
with it with a bunch of other people I
was very impressed with how the the Lyda
team put together a really nice hands-on
tutorial that makes it easy for anyone
to get up and running it and play with
it so they could even run through that
during the conference itself and
unfortunately we have not done that for
OpenCog so this this will be the lamer
kind of tutorial where I just tell you a
lot of stuff I mean then the code of
OpenCog is is open-source and it's on
launchpad anyone can download it and
that there are there are simple demos to
run and things to play with but we
haven't we haven't put together a nice
simple to run tutorial system like they
have but I think it's a good thing and
that their talk was was somewhat
inspirational to me in that way I'd like
to see us do something like that for
OpenCog it's it's good to have a way for
people to get up and running quickly
with your system so I'm going to talk
about OpenCog both as an open-source
software framework and primarily as a
design that envisioned for advanced AGI
there's there's a systematic terminology
that we introduced to distinguish the
different aspects of OpenCog which
however is the terminology we have never
used but uh
so in principle OpenCog is an
open-source software framework that
could be used to develop a lot of
different they are projects and there's
a specific AGI design the problem
okay oh now I can see my presentation of
the very slick yeah what we're using
OpenCog for in practice is implementing
a specific design for AGI which I called
cog Prime then we tried to go OpenCog
Prime the importation of cog Prime and
open cog it's supposed to be really
clever with the overlapping verbiage but
then in in practice no one has really
used open cog for anything except
implying this one Adi designed to anyway
so we just use open cog for everything
and I think in principle the framework
could use for a lot of other things I
don't know if it ever will be but it's I
think it's been a good software design
strategy anyway because trying to build
the software infrastructure in a way
that could be used for many many
different a are projects that forced us
to abstract things nicely and to use
good software design the other thing I
want to clarify before getting started
is even though I'm the one who usually
goes around giving speeches about
OpenCog I'm in an extremely small
percentage of the OpenCog code was
written by me I think I I probably came
up with more of the underlying theory
and ideas that anyone else but plenty of
the theoretical ideas were come up with
my my collaborators also and
including Mouser who's the chair of this
conference and then a bunch of other
guys who are listed here mattock lai
who's also here and giving a talk so
it's it's really been a group project
over over many years and if to the
extent it does succeed it's going to
partly be because of that because of
combining the divisions and insights of
a lot of different people so I'm going
to cover a fair number of aspects here
I'll start with our our grand vision of
what we hope the system can eventually
do talk a little bit about the
conception of mind and in general
intelligence under
system and that the longest part will be
about the the cognitive architecture how
is the system actually designed a little
bit about the practical software
architecture aspect to the system then
the environments and tasks what are we
trying to do with the system and I'll
showing some examples of our early
efforts to use it to control robots and
video game characters non player
characters for games then finally what's
the developmental roadmap by which we
want to go from the currently fairly
simple tasks and environments we're
dealing with further and further toward
more advanced AGI so it's a pretty broad
set of topics none of it can be gone
into into too much detail but hopefully
I can give you a sense of what the
OpenCog project is about and then since
it's an open source project with a
moderate-sized developer community if
anyone's interested to become involved
in one way or another it's easy to do so
if there's an IRC channel the mailing
list and the launchpad repository and so
forth also this will be fairly informal
so if you have questions or comments as
I go through feel free to raise your
hand I'll try to pay attention and then
we can have some interruptions or just
fine if not I'll end up going through
quickly and looking up questions at the
end which is which is good too so and
everyone please turn off your
cellphone's what's the grand vision here
so I pretty much agree in the big
picture with my friend Ray Kurzweil and
in that sense I'll put in a plug for his
movie transcendent man which is which is
showing tonight
which features me in it for like four
minutes pretty much all I say in the
movie is once I agree with him that in
the next century we will probably create
Thinking Machines as smart as humans
and I don't think we have any way of
knowing whether it's gonna happen then
and which is not such a profound
statement perhaps but what
differentiates me from some others
interviewed in the movies such as as Ray
who was really quite optimistic I mean
he believes that by 2045 we're going to
create human level thinking machines
though in essence bring a kind of
paradise on earth for everybody which is
a wonderful vision and I hope it happens
and my close friend Hugo de garis who
have worked with on the AI project in
Java and China that I'll mention later
in this talk Hugo believes that we will
create superhuman a eyes I'm roughly the
same timeframe and that they will almost
inevitably kill us all and take over the
universe so I mean my attitude was sort
of in between those two I think there's
a lot of indeterminacy here I'm
reasonably optimistic but not totally
optimistic because it to an extent it
seems like if we really do succeed in
making AG eyes that powerful trying to
predict what will happen it's like
cockroaches trying to predict the
outcome of world war two or World War
three there's something we may end up
being outclassed now when I talk to Ray
about how he thinks human-level AI is
gonna come about by 2045 or before he's
generally quite bullish on brain
simulation and emulating the human brain
and software and he has he has a book
that he's working on now called how the
mind works and how to build one or
something which is not complete yet but
it's largely based on computational
neuroscience ideas and my prototypical
argument with him is that I think that
is an approach that could work
eventually I think it's in principle
viable but I'm not sure that's the best
approach and then he says well okay how
are we really going to do it so the next
five minutes of the talk I'll go through
the the grand vision of how I think we
really could achieve these grand goals
that very is talking about using
computer science based adi not not brain
emulation then after that initial
excursion into a futurist vision will
dig into the into the details of OpenCog
so i'm taking more of a developmental
approach i mean i want to start by
building what I think of as artificial
toddler's not an exact simulation of a
young human child but an AGI with the
wrath of general intelligence if they
say a three or four year old kid and
then work from there make the system
smarter and smarter at a certain point
you have an artificial adult which you
can teach to be a scientist then you can
teach it to be a computer scientist then
it can improve its own source code and
you can go on from there and who knows
where there may be limits that we don't
know yet but at least I think we can get
way beyond the current state of AGI
technology so that's that's the grand
vision which I've been trying to work
toward for a while the OpenCog project
historically started with a project of
the Nova meant a cognition engine that
we developed within my AI consulting
company no ferment a/c then in in 2008
with the help of a grant from the
singularity Institute for AI we open
sourced a certain percentage of the nofa
meant a cognition engine code base into
OpenCog and that that included the the
moses automated program learning system
that moshe looks developed to the PLN
probabilistic logic engine and the basic
OpenCog core system that lets you run
various cognitive processes and let them
interact with each other what we're
working on now among other things and
this is something I'll go into in much
more detail on this talk we're working
on making what I think of as a proto AGI
virtual agent so what I hope to get is
something with some kind of toddler
level intelligence inside of
a video game inside the game world so
being able to communicate in simple
English do simple problem-solving and
learning in a video game world but
that's loosely modeled on the game
Minecraft and we're building that now we
have six programmers in the lab in Hong
Kong Hong Kong Polytechnic University
working on that which I'll talk about a
bit later and if that goes well then we
can step-by-step on the progression I
mean they're something that does toddler
like stuff in the video game world is
it's gonna be really interesting to us
in the sense it'll tell us that that
were on the right track and looking at
all the parts of our system working
together on the other hand a video game
world is pretty limited and I've I'm
regretfully and painfully come to the
conclusion that we probably need to work
with physical robots and now I'll get
more to that issue later on in this talk
but in in Xiamen University in China we
have done simple experiments looking
OpenCog up to the now humanoid robots
and then I'm unsure whether the now
platform is well we'll end up although
it's been interesting to work with but
the general plan is after getting
OpenCog
to do all the basic stuff woman in the
video game world then we start working
on robotics and in parallel and the
basic reason being that there's just so
much more richness in terms of
perception and motoric affordances in
robotics versus a game world that i
wouldn't say that it's a critical
necessity for AGI I think you could make
an AGI with curly a text I oh I think it
in spite of all the difficulties of
working with robots that actually
ultimately will make things easier if
we're trying to make a mind that has
some resemblance to the human mind which
after all the valve to control the body
and we'll dig into these issues a little
later no once you've gotten that far the
next step is more advanced learning and
reasoning which moves you on toward
making artificial scientists
and service robots and useful stuff but
you you see here one of the problems
with AGI development even in this
hypothetical scenario which is like what
we think may happen if everything goes
really really well even in this
hypothetical scenario it's a number of
years before you get something of
practical use directly out of your AGI
research program though there's the
possibility to spin off various interim
applications along the way using bits of
your AI code to do commercially or
pragmatically valuable things but in
terms of the core AGI research leading
to something of practical value if you
take the approach that I'm advocating
and that were found with OpenCog you
can't hear me well connect can everyone
else hear me if you take the approach
that I've been advocating with the
OpenCog which is a developmental
approach if you start off with a eyes
that are like children whether in the
virtual world or in a robot world I mean
if you think about it how much uses is a
young baby not much use right if you
didn't know what's going to grow into
something else you'd quickly throw in
the garbage for being a waste of
resources which some people do I'm not
advocating on the other hand we know
they're going to grow into something
useful and certainly within a GI system
you shouldn't necessarily expect it to
be doing valuable things immediately
because what it's doing is building up
common-sense knowledge about about the
world and about itself eventually I
think assuming the research program goes
well you get to the point where the
common sense knowledge that's gained
from interacting with simple game worlds
or with a robot lab world in the robot
body eventually you get to the point
where that knowledge can be deployed
practically and it's hard to imagine any
practical application of computers that
wouldn't be enhanced by and they are
had some common-sense understanding of
the world I mean just to take one
example I'm doing a bunch of work
applying machine learning to genetics
recognizing patterns in gene sequence
data but this is all about human beings
reading biology papers looking at data
sets and figuring out what machine
learning experiments to run and the AI
then finds patterns that no human could
find but of course if you had an AI
system with some common-sense knowledge
they could figure out what machine
learning experiments to run itself and
even if it didn't immediately cut the
human out of the loop it would it would
drastically decrease the need for human
involvement and be able to do things
that no human can do because we can't
read the millions of biology papers
online but by ourselves so in this
hypothetical timeline I think we can get
toddler like AI think we can then get I
added those specific human expert tasks
well and eventually then you get on to
the goal of a full-on human level which
is not necessarily a system that can
emulate human beings
in exact detail I'm not trying to make
uploaded people but a system that can
basically carry out all the same complex
tasks that people can then be on there
you get even more speculative because
one of the advantages of being an AI is
that you can inspect all your own source
code you can inspect the state of your
knowledge and RAM you can revise your
source code if you want worse human
attempts to modify their own
infrastructure usually lead to blow the
injuries and a lot of problems I mean
we're not we're not made to self modify
our brains and computers are made for
pretty much infinite modify billion on
the software level and even on the
hardware level it's a lot easier to plug
more memory into a computer then into a
human brain alright so this is really
the the grand vision I mean I think it's
the same vision that that the founders
of AI had in the in the 1950s and I
think those guys actually had a lot of
good ideas
but computer hardware was not work need
to be then but by any means worse now
now we have huge amounts of RAM in our
computers we have GPUs supercomputer
cards we have massive distributed
networks of SP multiprocessor machines
so I think we're in a position now to
achieve these these big goals and we
also have much better knowledge of human
cognitive science now the human mind
works quite apart from advances in
neuroscience I mean if if you look at
all the kind of boxes and lines diagrams
of different cognitive architectures
they lie to us or act our OpenCog Josa
box microbe saw I mean one thing you
know this is although there are
differences they all have kind of the
same things with different names with
arrows pointing to each other but thirty
years ago that wasn't so true there
wasn't nearly as much a consensus about
what the main parts of a human mind are
and how they passed information among
each other I mean that we kind of take
it for granted but that's like genuine
scientific progress in terms of
cognitive psychology coming to some
understanding of how the how the mind
works even though it only goes to a
certain level of detail and no further
so that that understanding together with
the massive it advances in hardware is
pretty important so that's the grand
vision and open onto concrete details
one other aspect that I'm not going to
have much time to talk about but I'll
just refer you to previous writings is
the philosophy of mind underlying my
thinking and the thinking of many of the
other participants in OpenCog on what
what what what is a mind that we might
implement it in a computer and it kind
of it can seem a bit of a digression to
think about philosophy when you want to
build stuff on the other hand if you
look in the history of AI you can see
that a lot of mistakes have been made
based on unsatisfactory philosophies of
intelligence and more
Mondays so I think having having a
reasonable philosophy of mind will not
get you to AGI on the other hand having
it bad or overly limited philosophy of
mind can prevent you from from achieving
AGI and I mean without pointing fingers
at particular people I mean if you look
at some of the early work on AI there
was such a strongly logic and rule-based
focus and their philosophy of mind but
they really just look at the mind as a
collection of logic rules and some
abstract higher-level language and I
think if if that's your view of what the
mind is you can have a hard time
building a thinking machine within
within feasible computing resources so I
wrote the book in 2006 which basically
covers my view of philosophy of mind not
everyone involved with OpenCog agrees
with it but large portions of it sort of
underlie the architecture in various
ways that that was called that the
hidden pattern and more of a futurist
slant in the same ideas it was in a book
from last year called cosmas manifesto
and then this last book doesn't actually
exist yet well it's like 800 pages of a
PDF but it's not published it will be
called building better minds and that
hopefully should come out next year
which goes over not only philosophy of
mine but all other particulars of the
OpenCog design and why we think it can
achieve advanced AI so it's a to try to
sum up in one ugly rambling sentence
what the philosophy of mind we started
with is it's all about patterns it's
saying about the mind as an evolving
self-organizing system of patterns
associated with the system that tries to
achieve goals so you you basically view
the mind as a system for recognizing
patterns in itself in the world
including patterns about how it can
achieve its goals in its environment and
that that sounds pretty simple when you
say it that way but of course you can go
through the history of philosophy of
mind and find a lot of very very
different statements also and since it's
not the focus here I won't gone
it's more about that but I do think it's
an important aspect following on from
hold on question yeah
well that's actually that's a question
of the architecture could everyone hear
the question no no I understand the
question I'm just going to repeat it so
they can hear it as I understood his
question is are we aiming to make an AGI
that will come up with its own goals all
on its own or are we aiming to make an
AGI that will always in a sense follow
the goals that were given by its creator
I think that that's actually a question
of the architecture of the particular
system that you build within the OpenCog
framework in the sense what we're doing
now and what I intend to keep doing is
build a system with certain top-level
goals and I'll talk about the goal
architecture shortly where the system
then tries to learn which procedures
will achieve its top level goals given
the context that sees and then it may
learn sub goals of those top level goals
as it goes along and in that sense we
want it to keep trying to fulfill the
goals that you gave it on the other hand
it will be quite simple with an open cog
to build a system without any top level
goals at all but could rewrite its top
level goal content at random for example
I think that's that's not difficult to
do but I'm not sure that it's I'm not
sure that it's desirable and they the
other subtle point there is even if you
specify goals for the system I mean
they're always subject to interpretation
in some way or another so it becomes
quite a simple mother quite a complex
matter I mean if if the goal is to
please Bend for example I mean what
what's been could it create if you go
science-fiction what could you create a
copy of me and entertain that guy right
if the goal was to get the pleasure
button pushed okay well if it just
builds a robot to push the pleasure
button there's a lot of a lot of issues
wrapped up there which we've thought
about a lot but I don't want to go into
too much in this talk is there another
question someone yeah
that's right this women um well if if
the sub-goals were really derived from
the goals and the system was smart
enough then it wouldn't be a problem the
other thing is the system will never be
infinitely intelligent so its sub-goals
not not really imply its top level goals
I'm I think the whole question of goal
system engineering for a system like
this where you can supply goals it's
it's certainly a hard question and I
think it's largely independent of the
specifics of the cognitive architecture
I mean one difference between a system
like this in the human brain is that the
architecture as we'll see shortly does
contain the notion of explicit top-level
goals and then using probability theory
to try to figure out how to achieve them
and that's not really how the human
brain works I mean humans don't on an
individual level really have such fixed
goals is that I mean evolution in this
sense supplies us with goals but then
obviously we can subvert those and in a
lot of ways and become so I think it's
it's a more rigidly goal oriented system
than the human brain but that rigidity
could be toggled by specific choices and
configuring the specific system and I
think these are questions that will get
a much better sense of once we have
proto AGI systems that can like control
robots and game agents that actually do
stuff following their goals and we see
what happens it's kind of the math isn't
there to resolve these things with
rigorous math and it's it seem almost
hard to trust yourself and speculating
but so what is general intelligence so
that this sort of follows out of
philosophy of mind but it's a more more
concrete question so there's a certain
intuitive sense in which humans are
general-purpose
rather than early specialized
intelligences and I think about general
intelligence as the ability to achieve a
variety of complex goals and a variety
of complex environments in practice it
has to be done using limited
computational resources and you'd like
the system to be able to deal with goals
and environments that weren't even
thought of at the time the system is
created and this this suggests that you
can think of intelligence as being about
using perception memory to predict what
actions will help achieve your goals
which is it's not a very original notion
of general intelligence I think a lot of
people would agree with that and that's
sort of an emerging understanding over
the last few decades about what
intelligence systems should be like
there's been attempts to make a more
rigorous definition of general
intelligence I won't go into too much
detail on that I gave a talk on this at
last year's AGR conference in Lugano
Shan leg and markets herder came up with
a definition of general intelligence
which are called universal intelligence
which is roughly speaking it's an
average over all environments of how how
good the system is and achieving goals
in that environment where they weight
environments by their by the universal
distribution which is based on the
compressibility on the Selam enough love
and prior so if if you don't know what
that means i won't i won't take time to
explain it now but that that word that
worries me a bit because i think humans
are incredibly stupid by that kind of
definition like and we're not very good
at solving problems in arbitrary
mathematically compressible environments
so i mean we're not even that good at
solving problems and they are on the
bottom we evolved for when you come down
to it so i I proposed a minor variant of
this last year at AGI 10 just you can
specify any distribution over goals and
over environments it doesn't have to be
the universal distribution and you can
assess intelligence and how good is the
system with achieving goals and
environments drawn from those
distributions and that
that's different cuz the universal
distribution is exponentially decaying
worse arbitrary distributions no don't
have to me and you can also look at
efficient general intelligence like how
intelligent is it as a ratio of the
amount of resources it has and this ties
into Pei Wang's view that intelligence
is about adapting to the environment
under limited resources and whether you
want to consider that part of
intelligence or not it's kind of a
philosophical question I mean if you had
a system using infinite resources that
could solve every problem many of us
would call it intelligent so if someone
say it's not intelligence we are but
that's that to me is just a matter of
verbiage now all this this business of
defining intelligent systems as those
that can achieve goals is a bit funny
when you think about humans you don't
explicitly have any goals I mean you
can't open up a human head and fund the
goal our goals and life changes as we
grow at some points in your life your
goals may be very very ill-defined right
so you end up looking at a goal as
something that wants system uses to
model another system so from I'm a model
you as following certain goals or I'm a
model myself as following certain goals
and then you really need to look at the
intelligence of a system with something
defined relative to a model of that
system as goal oriented another thing
from that 2010 paper is defying the
generality of a systems intelligence
which basically if you look at all the
environments in which a system is
intelligent you can look at the entropy
of that that distribution over
environments and then the so that turns
out to be a kind of orthogonal thing to
how intelligent it is you could have a
system that's very very intelligent with
over her fairly Peaks narrow
distribution of environments a system
less intelligent over a broader
distribution environment so this is a
way of thinking about generality and
intelligence as sort of different
characteristics which come together to
form general intelligence and this is
sort of practically relevant to working
with with AGI systems and I think
intuitively what we tend to mean by an
AGR system is something that can be used
and
a broad spectrum of different
environments and situations hmm so all
this business of defining an intelligent
system relative to certain probability
distributions over goals and
environments that raises the question of
what distributions to look at so one one
thing I've looked at a bit is actually
looking at environments characterized by
a communication where basically the the
the environments are more likely if they
can be more compactly communicated by
the communicators and in some linguistic
group and the that let's you say some
more concrete specific things but all
these distributions and environments and
it kind of makes sense in terms of human
evolution and that we largely evolved
our intelligence to communicate with
each other and that leads that leads by
a line of reasoning I won't go through
in detail here that that leads to a
notion that's important in OpenCog which
is called cognitive synergy so one of
the key ideas in OpenCog gone from
cognitive psychology is that human-like
intelligence relies on several different
kinds of memory which are fairly
distinct from each other yet which all
need to interact with each other and
these types of memory actually
correspond well to different types of
communication so one kind of memory is
what we call declarative or semantic
memory which is often been modeled using
logic but that doesn't have to be it
could be modeled using other words and
this obviously corresponds to over the
linguistic communication there's
procedural memory memory of how to do
something like kicking kick a soccer
ball or walk down the street or or any
any action that's reasonably automated
it could be language generation or
adding numbers and that this correlates
well with with communication by
demonstration you can show someone
there's something I mean if if I wanted
to teach whether serve in tennis I'd be
much more likely to just do it give them
a logical description of the predicate
logic formulas describing a tennis
server or even English English sentences
sensory and sensory motor memory
obviously we store a lot of that just
specific images we have episodic memory
which correlates with communication by
storytelling which is a big part of the
human mind and episodic memory is your
life history that you store then there's
there's what I think of as attentional
an intentional memory which has to do
with what you pay attention to them and
what goals you have and if you look at
intelligence as being goal achievement
relative to probability distributions
they're defined based on communication
within a group it's kind of interesting
because each of these kinds of memory
corresponds to a different kind of
communication in terms of AGI design one
of the decisions we came to in designing
OpenCog is that it made sense to take
each of these kinds of memory and kind
of represent it differently in the
software and that leads you down the
whole complex path because if you
represent each type of memory
differently you ultimately wandah using
different learning algorithms for the
different kinds of memory but then you
have all these memory stores and all
these learning algorithms and you need
them all to work together well which
gives you a really complicated system
and it leads to the notion of cognitive
synergy which is that the learning
algorithms corresponding to the
different types of memory need to work
together well in a way that when one of
them gets stuck the other ones will will
help them out and we've formalized that
mathematically I won't give that here
but that's why the key idea is
underlying the the OpenCog design so for
for a declarative memory we do learning
using something called probabilistic
logic networks which is a bit different
than what Noah will talk about in the
following tutorial but I think
conceptually kind of going in the same
direction in terms of bringing logic and
probability theory together for
procedural learning among other things
we use Moses the the probabilistic
evolutionary learning algorithm
developed by Moshe looks our conference
different species Washington University
then for
for sensory learning we've actually been
hybridizing OpenCog with Destin a system
developed by NMR arrow which is vaguely
similar to Jeff Hawkins hierarchical to
temporal memory for attentional learning
we need something based on artificial
economics called economic attention that
works that Matic lay will will give a
talk about in the main a gr conference
and for episodic learning we haven't
gotten too far with that yet but one of
the things we do as an internal
simulation engine when the system does
run through episodes interesting Wow
keynote randomly insert some slides in
there I'll forgive it that's all right
so moving on to the cognitive
architecture and in particular this this
isn't really an architecture diagram but
it's more of an impressionistic
rendition of what happens inside OpenCog
so yeah this was like a keynote art made
on the flight out of here so forgive me
my bad the visual art skills we have
stores for different kinds of memory
declarative procedural attentional and
so forth there are learning mechanisms
corresponding to each of those at the
bottom you see the agency system
controls for example something we were
working with a few years ago robotic dog
in a virtual world or a now humanoid
robot which is one of the things we're
working with now and there are proxies
that connect to each of these things
there's a game world proxy that
translates signals from OpenCog into
commands the game world can understand
there's a robot proxy which is much more
complicated that deals with the output
of the robot and the servo motors
command and commands and so forth and
actually we're using deston which is in
Amar's a hierarchical spatiotemporal
learning network as part of the proxy to
the robot so that's actually a whole I
think in itself then just recently we
started using something called open Sai
- basically govern the system's
motivations and emotions and do
and that this is based on dosa box work
with microbes I and more foundationally
on Detrick Dorner's sigh cognitive model
of motivation and emotion that the
Joseon introduced me to so there's a
talk I'm given on that in the main AGI
conference just on basically fusing
together the side model of motivation
and cognition action selection so on
with the with the rest of OpenCog so the
previous comments on cognitive synergy
basically have to do with how how all
those kinds of memory inside that circle
at the top interoperate with each other
and the sigh architecture is is its own
thing on its own which I won't talk
about too much now because it's covered
briefly in the talk I'm giving on Friday
in the main a GI conference but that's
that's basically used to select actions
to mediate between perceptions and
actions and it contains a number of
parameters called modulators that help
guide the system's emotional reactions
to the world that's sort of the basic
perception action cognition loop that
even even a simple animal would have so
if you go back to lida there's a
cognitive cycle in lida
and phi has a somewhat similar
cognitive cycle which is its looping
around it's taking interceptions it's
doing actions it's accessing memory and
so forth so we improvised our own
cognitive cycle based on psy a bit
different than what Joshi has done in
microfiber but with some similarities as
well but then on the back end of that we
have a whole bunch of memory stores and
learning mechanisms which are different
than what dietrich norther or Joshua did
and below that indicates of dealing with
the robot we have a whole hierarchical
tempore
memory system because the the data from
the robot is is high-dimensional and
feeding it into open cogs more cognitive
mechanisms doesn't work well so before
digging further in the specific memory
and learning mechanisms I want to
reflect on a few high-level choices that
were made in getting to the point of
that impressionistic diagram I just
showed so think about the different ways
you could implement AGI so one approach
is to pick some complex cognitive
process some core process say this
process is the essence of intelligence
and we may add other processes around
the periphery to help it and so this
approach has the merit of simplicity
certainly I mean for example I would I
would say pay Wang's Norris system fits
that description I in my view they'll
pay me look at it differently I mean he
has his non axiomatic reasoning system
his logic engine and the impression I
get if he thinks that's the crux of it
that logic engine together with the
inference control mechanisms now he may
add on perception processing in front of
that other action control or other even
specialized mathematical reasoning
module but those are kind of add-ons or
interfaces and the crux is the logic
engine and that's that's one approach
which I think tends to be very appealing
people with the computer science
background because it's that's kind of
what you do in computer science is find
a really good algorithm and run with it
and see what it does another approach
that you can take is more emergentist
take a bunch of simple processes and try
to get them all to interact together and
self organize to yield some powerful
intelligence and this approach is
emotionally appealing to me I mean I did
a lot of complex systems research in the
90s and that underlay a lot of the early
neural network especially on a tractor
neural networks not so much feed-forward
learning but unsupervised neural
networks largely came out of this
perspective and
I don't see why that couldn't work I
think it's it's gonna be difficult to
tune the system like that evolution took
a very long time to do so and it doesn't
match that naturally with how current
computer hardware works either but
that's a viable approach to AGI another
approach is to take a bunch of complex
processes and glue them together and
then that then you kind of have the
worst of both worlds because you you
have to implement difficult complex
algorithms and you have to deal with
unpredictable emergent dynamics and
tuning everything to get to work and
that's basically the approach we're
taking an OpenCog and I mean the
different cognitive processes all tend
to be they all tend to use a
probabilistic semantics and some of them
use an economic metaphor for allocating
resources so there there's some
commonality between them but now if this
is the hardest way conceptually then why
do we do it obviously we think that's
the most likely way to work I kind of I
kind of doubt there's only one algorithm
that's going to be good enough to yield
human-level AI given limited
computational resources and I'm afraid
of the complexity of trying to brew
together billions of things and get them
to evolve and self-organized it into the
right thing
if you look at how the human brain works
I think the human brain involves a
number of complex processes each of
which has a lot of internal complexity
and all interacting together in
specified ways I mean if if you look at
like the hippocampus has its own spatial
map and Association recognizing
algorithms and the the parietal cortex
is its own thing and the olfactory
cortex is its own thing I think the
brain has a lot of different regions
with different algorithms in essence
implemented in them and they've all
evolved to work together in various
complex ways and they don't always work
well together and we're not trying to
emulate a brain in OpenCog but I think
the the complexity and how
ingenuity of the brain to me on the
philosophical level it speaks in favor
of making a complex heterogeneous AGI
system now of course there could be a
simple algorithm that works even though
the brain is all complex and mixed up
and tangled up and I don't claim this is
the only way to do it but it's the way
that OpenCog embodies another
foundational question is what's more
critical the cognitive architecture or
the learning if you some people who
build cognitive architectures the basic
idea is you know if you get the right
architecture for representing and
passing information around then the
specific learning algorithm doesn't
matter so much and you can kind of build
a cognitive architecture first and then
figure out smart learning algorithms
later on the other hand some people
espouse away from the machine learning
or reinforcement learning approach they
would say well what the crux of
intelligence is learning needs a good
learning algorithm if you have a good
learning algorithm you can wrap any
common sensical cognitive architecture
around it and another possibility is
that you need to have both of them kind
of at the same time working working
together well and kind of evolve
together which is the approach we've
taken in OpenCog finally there's on the
level of knowledge representation the
question that so much ties in with the
previous ones I mean what's most
important the specific algorithms and
representations you coded or the stuff
that emerges as a system evolves and
again positions on that are all over the
map and I always try to have it both
ways which causes a lot of a lot of
problems in terms of making a very
complex system on the other hand again
when I look at the brain it seems to me
to work both ways like you have specific
cells that represent quite specific
things in the brain on the other hand
you have things you're represented by
complex nonlinear activation patterns
across large brain regions III I
introduced the ugly world glocal to to
describe this kind of knowledge
reference
in global global plus local the brain
seems to do both and they're coupled
together in complex ways and I think
that's necessary and among the things I
think emerge or all these abstracts of
things that you talked about in
philosophy of mind and yourself your
feeling of will your theater of
reflective awareness so these are things
that we don't really try to build into
OpenCog there's not like a self no
there's a freewill agent I think a lot
of these things that are part of our
common sense subjective experience of
the world have to be made to emerge and
with that general introduction I'll dig
a little more deeply into what goes on
inside OpenCog now starting out with
with knowledge representation which
which happens on two levels so there the
first level is explicit knowledge
representation for which we use nodes
and links this sort of weighted labeled
hypergraph and then the second level is
what we think of as implicit knowledge
representation knowledge it's encoded in
the coordinated activity of a large set
of nodes or links so we have something
we call the atom table which is a big
neural semantic hyper graph most of the
nodes and links in there don't have any
English names they're just stuff the
system learned which are given meaning
by their links to other things and their
activity patterns some of them do have
English names some may correspond to
objects we recognize like a table or a
chair a movement like raising your arm
some can correspond in particular
percepts there are different types of
nodes and links and they're all
connected together in this big in RAM
graph database in terms of the software
architecture that I'll dig into and
slightly more detail later we have a
container called the atom space which is
a weighted labeled hyper graph with
these nodes and lengths then we have a
number of different prophecies they're
scheduled by scheduler each of those
prophecies
primarily by grabbing atoms from the
atom space modifying them putting him
back or putting new atoms in the end
space so I mean you could very very
loosely you could view it as a
blackboard architecture with a huge
blackboard because I mean you can have
our current main development machine has
like ninety six gigabytes of RAM and
then we run maybe a dozen different of
these cognitive processes those four
there are just examples and ultimately
we'll need a lot more RAM than that if
you want to approach the human level
intelligence that's just what we could
get on a fairly affordable server right
right now
so the different cognitive processes are
just C++ objects or Python scripts that
access the the atom space and then do
things with it
so each kind of knowledge is represented
somewhat differently I don't expect you
to read all that but it's it's notes for
me anyone so the declarative knowledge
is pretty transparently represented in
terms of nodes and links and there's a
book we published in the Springer in
2008 called probabilistic logic networks
it goes through a sort of probabilistic
term logic that explains how to boil
down general logical relationships and
to weighted hypergraph links in an
elegant sort of way attentional
knowledge every noter link in the atom
table is weighted not only with truth
values but with a short-term importance
and a long-term importance value which
indicate how much attention the system
should pay to that atom so the
short-term importance indicates how much
processor time roughly should be paid to
that piece of knowledge the long-term
importance indicates roughly whether
that piece of knowledge to be kept
around in memory or not then their
equations based on the artificial
economics that spread around these
important values to the system and meta
clay will talk about that in his in his
talk on on Friday procedural knowledge
we have basically little
like program trees in the language that
Moshe developed that can be executed
like little Lisp programs they can also
be converted into the semantic atom
representation if you wanted to do
reasoning reasoning about the program
sensory knowledge is handled in a fairly
specialized way what we're doing for
vision now is using a destined in Amar's
hierarchical temporal memory like
architecture then goals are represented
similarly to declarative knowledge at
this point so a goal is a semantic node
and then their inheritance links between
sub goals and goals and this is all in
the explicit level nodes and links have
various types there's a large list of
node and link types which I won't go
through the whole list there on the
OpenCog wiki in transpo the system can
come up with its own node and link types
as well
I mentioned the truth-value objects
which record probabilistic truth values
associated with links we put some work
into exactly how to do that our default
is not to use a single probability on
the link but to use something like an
interval probability like a lower and
upper bound for the probability or even
more complex kind of imprecise
probabilities so there's a lot of work
that went into how do you manage these
interval or other imprecise
probabilities inside probabilistic logic
which is talked about a lot in the
probabilistic logic networks book and
seems to be useful in terms of helping
the system know how confident it should
be in its various judgments because if
you measure a probability as an interval
like 0.4 two point six versus point one
two point nine they have the same mean
but one has a much greater spread than
the other one so you have less
confidence in that one then short term
and long term importance I mentioned as
atention values telling you how much
processor or memory another links should
get there's also something called very
long term importance that tells you when
you kick the atom out of
Braham do you bother to save it to disk
or not because saving to disk takes a
lot of time in itself so for the
perceptions the robot gets through its
camera or something it doesn't
necessarily save all those to disk but
that would take up too much disk for us
at the present time although ideally it
might be nice to remember though so that
has no very long-term importance you
forget if it doesn't seem useful worse
in some cases you may think of something
interesting you don't have room in your
memory for it but you want to save it to
disk and and you can get it back later
so there's a lot of different node types
and link types there's some that refer
the percepts there could be specific
ones for pixels words characters numbers
some that refer to procedures as little
list programs that talk about some what
we call concept mode is really a kind of
a proto concept no it's just a token
that links and connects to other things
then there are specific nodes for goals
and and and feelings which contain
little procedures in them to evaluate
how well the goal was achieved or how
much how much the feeling is experienced
different kinds of links there's links
representing term logic like inheritance
and similarity and some representing
quantifiers and so forth there's fuzzy
membership links there's Association
links which are more like in a hebbian
neural network and their lengths
representing lists and other constructs
so just to run through a few very simple
examples of representations or a member
link is fuzzy membership so what say my
rabbit maybe point eight a member of my
family or my daughter would probably
argue it's higher than that but it's a
rabbit it's not a human she doesn't have
a birth certificate with her name
hebbian links you can have symmetric and
asymmetric hebbian links that just
measure how often two things have been
associated and that can be generic or it
could be in specific contexts like maybe
dog is associated with pet to a higher
degree in the US and
Shriya because we don't eat dogs often
here and execution links record
executions of procedures which could be
arithmetic procedures or they could be
like a robot kicking the ball type of
procedure
there's complex logic which i won't go
into but we distinguish between
intentional inheritance and extensional
and so you you can have a fairly rich
logical semantics here and this is
something where the basic OpenCog
infrastructure doesn't make any
commitments I mean we've we've
implemented the PLN logic and OpenCog
but you can implement Norah's and
OpenCog or you can implement Noah
Goodman's logic in OpenCog that the
infrastructure would all still work but
we've worked out the interaction between
PLN and Moses and concept creation a
bunch of other cognitive processes in
some detail if you were to shove Norris
as a logic engine into OpenCog that
would be hunky-dory but then the
interactions between that logic engine
and all the other processes would have
to be thought through and the other
process is you might have to be changed
a lot to work well with another logic
engine I mean that that's that's kind of
an open open question I mean that's
that's the weakness of a sort of synergy
focused architecture in the sense that
if all the parts operate together and
then every time you change one part to
some extent you have to think about the
other part something that it wouldn't
cause the software to crash like you
could plug nars or any other inference
engine in there as long as it interacted
with the admin space the right way the
software will operate as software but it
might be really stupid if you haven't
thought about the interactions between
that reasoning engine and the other
parts of the system and you can
represent quantifiers like for all in
there exist and so forth explicitly in
the logical language and I would say
it's an open question an open kaga
research how useful it is we don't know
yet I mean you can represent these
abstract things as nodes and links but
maybe that's not really the right way to
do it
maybe these should be learned more
implicitly through experience as broader
patterns of activity and
the system's breadth is a strength and a
weakness because you could do it a logic
based AI way or he could do it a more
kind of self-organizing learning way or
maybe you could do it both ways and
since we're still working on simpler
applications of the system we haven't
really explored what is the best way to
deal with this kind of abstract
quantified knowledge yet so moving on we
also talk about what we call maps and a
map basically is a distributed pattern
of activation across the system and
either I have some that are mostly
across concepts somebody mostly across
percepts something to mostly across
events or goals and these are sort of
like distributed attractors in the
systems memory and one of the things
I've been playing with recently is the
interaction between these distributed
maps and the more localized semantic
memory like if if the system sees a
battery in the contents of our virtual
world where the agent is a robot that's
always trying to find batteries if the
system is a battery then you have a node
for battery you'll also have a map for
battery which is basically all the nodes
and links that are commonly activated
when it sees a battery and how do those
two interact well what the mat for
battery does affects the links to the
node for battery grows and of course if
you activate the node for battery then
most of the mat for battery will be
activated so this is the glocal kind of
global local aspect of the knowledge
representation which is something we've
just really become begun to explore them
I'm curious to see how that will pan out
on the software level the objects
embodying cognitive algorithms are
called mind agents and as I said you
could in theory you can put any
collection of mind agents and have them
work on the item space through the oven
space API the art of architecting a
cognitive system within the OpenCog
framework is finding a collection of
mind agents that will help the system
for false goals representatives goal
nodes and that will work together rather
than ignoring each other our work
encounter for the
so as I said we have probabilistic logic
networks mind agent and actually a
couple there's a forward trainer in the
backward chainer doing probabilistic
logic inference
there's moses which learns procedures
and as a default that operates
supervised but you can do implicit
supervision so you can have moses setup
to say just find little program treats
representing interesting patterns
statistically significant patterns in
the admin space so then it's it's
supervised learning in a sense but the
supervision is just internally provided
by the fitness function of fun fun
whatever is interesting there's a mind
agent for economic attention that just
spreads short and long-term importance
around and that that's important because
the system scheduler will well not the
scheduler because when a mind agent is
active according to the scheduler the
mind agent will tend to act on those
items have a greater short-term
importance so say when the PLN mine
agent has to do reasoning it will pay
preferential attention to Adams have a
higher short-term importance value and
then how did they get a higher storage
of importance value well the economic
attention allocation agent gave it a
higher short-term importance value their
algorithms for map formation for trying
to recognize one of the attractors in
the system there's algorithms for
forming new concept basically fusing
existing concept nodes into new ones
so probably Moses and PLN are the most
heavyweight algorithms in there in terms
of their lot of lines of code and have
many complex internal algorithms but
they're certainly not certainly not the
only important ones and if you're not
familiar with these I don't have much
time to go into them now but you can
look into them on their own but Moses
kind of it's like genetic programming
but it uses probability theory and sort
of cross over a mutation
so you probably still be modeled what
what are the fittest programs in your
program population and that's quite
powerful and one of the nice things
about it is that it lets you use prior
knowledge which is expressed
probabilistically to bias your program
search so my my talk at the triple AI
conference next week in San Francisco is
specifically on the feedback between
Moses and Pilon and how that works so
Moses and Pia and each each help each
other and make each other smarter which
is allowed because they both use
probabilistic semantics I mean at first
in the late 90s I tried to interface
actually pays NARS system with a genetic
programming system to get inference and
evolutionary learning to work together
and we didn't get so far with that both
for practical reasons to do with our
architecture at the time and because the
semantics or the two systems are pretty
different whereas using a probabilistic
reasoning system and a probabilistic
evolutionary learning system you can get
the two to work together relatively well
because Moses wants the probability
distribution over program space Tobias
it's search so that that's a concrete
example of how you have to do the
mechanics right to get what I called
cognitive synergy to work because if you
want your declarative learning process
pln and your procedure learning process
Moses to work together well then they
need to in some sense speak the same
language so when one gets stuck it can
ask the other for help in the right way
if Moses is having trouble learning
programs to do something say figure out
how to kick a ball through the goal it
can go to PLN and say well based on your
knowledge which program should have a
higher prior probability and then PLN
may do some inference to figure out
which of this program should have a
higher prior probability and Moses can
then use that inside its probabilistic
evolutionary learning so that's that's a
simple example of how the different
agents have to be engineered to work
together well and this is just a
depiction of a from a simple example
finding statistical word associations
using which I use cuz it makes a simple
picture where the gray links were linked
so we're in the FS already the green
ones were learned by PLN I mean it's
it's actually hard to visually depict
something with millions and millions of
nodes and links in SONET another
interesting thing that we're playing
with the implementation of now is
concept blending like you if you have a
journal node than a human node how do
you combine them to make a durable mind
I mean you have choices to make this
durable man have two legs or four legs
and you got to make a choice and how do
you make that choice so there's a whole
theory of this concept blending in
cognitive psychology some people think
this is the main algorithm underlying
human creativity I don't know if that's
true but it's a valuable way to get new
nodes into the system beyond the ones
you get through perception so I I like
to do this system with one of our
graduate students how do you blend in
our robot in the PhD student so that's
one way which is not easy to do in
practice using current technology we can
save that for the for the next version
the the Matt formation process occurs
explicitly as well so say you have a
number of nodes you can't do what used
to be implicit knowledge becomes
explicit knowledge which is yeah
yes well so once that note is formed
when it initially formed it will get a
bit of short-term importance but so yeah
that's the concept most most concepts I
think over complete nonsense actually
hopefully not too many of the month in
this talk but you never know yeah I mean
I think that's an old saying by someone
the best way to have good ideas is to
have a lot of ideas I think I think
that's from Linus Pauling actually of
course within the scope of the human
lifetime that makes sense if you had an
infinite time it's trivial you could
just print out all possible ideas it's
so much true in a system like this I
mean if you want to have the session to
be creative you have to let it generate
a lot of nonsense and then forgetting
becomes very important it's almost as
important as learning and the the
economic attention allocation system
it's important for that because the
basically the things that have the least
amount of money in terms of the
long-term importance currency they will
get kicked out of RAM and I think that's
a big limitation of our current computer
systems is there's not enough RAM
because the human mind has a lot of
nonsense and it has a lot of room to
store that nonsense so the little bit of
our thoughts that are worthwhile can
kind of bubble up to the top and we
don't quite have that much RAM in our
computer systems now so we have to be
over enthusiastic about calling things
but I'm confident the amount of RAM
available will advance but so there's as
I just said there's there's two kinds of
important value they're short and
long-term importance and if if you have
low sort and low long-term importance
you're useless but if you have high
short and long term importance you'll be
used and remembered a high long-term and
low short-term that's like say your
mother's name
I'm glad to remember my mother's name
through this conference even though I
will not need to know it for the next
few days at all it has a high long-term
importance to me a low sort of
importance I'll keep it around if you
have a low low long-term and high
short-term basically it'll be used and
then forgotten look for example where
this teacup is no I'm happy to remember
right now I don't care if I remember
that next week next year I'll probably
totally forget it although maybe not
since I mentioned it now maybe I've
driven it in my memory forever so this
is another thing we're keeping these two
values seemed important and I know a
number of other AI architectures wound
up using two values and in a similar way
and when they basically correspond the
time in space but put very roughly so
talk a little bit about sensory motor
knowledge and this is something new that
I'm beginning into in the last year
because we've been playing with hooking
the OpenCog up to a robot now to do very
simple things with robotics was just
fine
I basically treated the robot the same
way as a video game agent and just kind
of you know when I would give the video
game agent a command step forward okay
we wrote a script in choreograph which
is the nouns language then I was inbuilt
programming language to make it step
forward so then the same step forward
signal could go either to a virtual
world agent or to a physical robot and
for vision we just use OpenCV a standard
computer vision library to to do object
recognition so then okay if our object
recognizer saw a cup open car would just
get the signal cup at these coordinates
right so I mean if you are willing to
completely hack sensation and action
layers in a hard-coded way then you
don't need any fancy sensory motor
processing and your AI system can
interface with the robot the same way it
does with it with a video game agent
however I don't think that's really good
enough I mean I think if if you want to
use a robot the reason is that you want
to make use of the full sensorimotor
richness of the real world and if you if
you want to do that you need a system or
perception and action can dynamically
interact with with cognition and that
turns out to be a huge kind of worms not
surprisingly and the approach we've
taken in the last year I've been
collaborating with the air tomorrow with
it he's he's gonna be here in a couple
hours I guess I think he's I Jason but
he he has developed two systems focus
mainly on visual and auditory perception
one is a commercial system within this
company banal --ax which is called HD RN
the other is called destin which is
developed in the open domain together
with his students from University of
Tennessee Knoxville
so in tomorrow's system is it's vaguely
similar to Jeff Hawkins HTM system from
his company Numenta
but in terms of various technical
particulars I came to believe that in
Amar's system was more functional I mean
it I think it works better on an actual
visual data and the the the equations
that it uses made more sense to me so I
wound up working with Destin and we
we've been doing a bunch of stuff with
this the first thing we did is one of my
students in JAMA and in China we
implemented his destin'd system in CUDA
surround on GPU supercomputers so it
works more scalable now and our next
step were in the middle of connecting
the CUDA version of Destin with with
OpenCog and without going into too much
detail here there's there was a lot of
work in connecting on the one hand which
is a somewhat similarly structured to
Jeff Hawkins knew mantis system or at at
the lowest level the grid cells refer to
many four by four pixels or something
and the higher up you go
each cell refer
to a larger and larger region of the of
the input space and each grid cell in
the hierarchical grid has some pattern
recognition processes in it that try to
recognize patterns what happens in the
Associated region of space-time and then
predict what will happen next there so
Jeff Hawkins called a system
htm' hierarchical temporal memory I
found there was no word or term for an
HTM like architecture so with a de maras
input I came up with the ugly acronym
compositional spatiotemporal deep
learning network which which really is
what it is I mean it's a deep learning
network
it's spatiotemporal and that each node
corresponds to some region of space-time
a little movie and it's it's
compositional and it builds up like it
like a hierarchy so Jeff Hawkins
htm' is one of these in Amar's Destin is
another and there's a bunch of others in
the literature and what we're doing is
building a kind of balloon that were a
semantic hierarchical Network to go in
between Destin and open cogs atom space
so you gotta think about there's a
problem we've been thinking about a lot
like if if I took for example Steve
Omohundro space took a picture of it and
stretched it out so the eyeballs are way
over here and the mouth went way down
here right it's fun to think about just
imagine yeah there you can almost do it
so used in the simplest way Destin would
not recognize that as Steve's face
because each of the grid cells
corresponds to a certain region of the
input now what NMR actually does is
tries to simulate the the movement of
the eye so he'll like moved the picture
around randomly so that each each part
of each cell at different times of
seeing different parts of the image and
then he uses the fact that it does
temporal pattern recognition to have
each cell recognize kind of conditional
probabilities and what is it likely to
see next after it saw the previous thing
and that
that's all quite cool it's also quite
complicated to me then then you you
bring in the fact that the I will tend
to use its knowledge of importance to
move so I'm more likely to say move my
eye around the top of his head or his
eyebrows and just diagonally randomly
across his face so I thought all that
was really interesting but I got scared
of it because it's complicated and I
started working in a simpler way where I
just put the input image in a fixed
position then I used some some pattern
recognition algorithms frequency sub
graph mining to recognize patterns
across the states of the hierarchy over
multiple times and then feed that feed
that into OpenCog so there's many ways
you use these hierarchies basically I'm
using the hierarchies in a simple way
storing in memory what the hierarchy
sees at many different times then doing
pattern recognition across that and
feeling that in the OpenCog works what
he's doing is simulating that the foveal
eye movement and then having the pattern
recognition all done within the the
hierarchical Network and I think those
could actually both work but what he
wants to do is do all of AGI is inside
that hierarchy or coupled together with
other similar hierarchies where so I
want to do is use that as basically a
sensory cortex for the AGI so it's -
somewhat different use and you can look
at doing the same thing with with motor
works I mean you can take a
compositional motor hierarchy here this
diagram is for a tank type robot you can
do the same thing for a now where the
higher up nodes correspond to larger
parts of the robot basically so there's
a note removing your arm a note removing
your elbow and notes removing your
finger and so forth again how do you map
that into a cognitive semantic Network
like open cogs atom space we use some
intermediate hierarchical Network which
contains patterns recognized in the in
the in the motoric network and we
haven't actually done this yet we
started playing with it with the
perceptual thing that this will be the
next step
so ultimately you can look at having
these intermediate networks of patterns
recognized in a hierarchical sensory
network patterns recognized in a
hierarchical motoric network and you
have this kind of glue layer between
perception and cognition which honestly
it's not somewhere I wanted to go I
tried to avoid it but when we started
working with robots you ultimately had
no choice because it just got infeasible
to feed all the raw data into OpenCog as
it was and it seemed that building in
the spaceship to Perl hierarchy like in
Amara and Hawkins do actually makes an
awful lot of sense for processing high
dimensional visual or auditory data but
yet unlike NMR and Jeff Hawkins and
others I'm skeptical that that
representation is good for everything I
don't really want to do language parsing
or differential calculus using a
hierarchical spatiotemporal memory you
probably could mathematically but it
seems awkward to me yet I do want to use
it for visual and auditory recognition
so that's this is current working in
progress but on the vision side it's
going reasonably well now these next
slides no one is supposed to read I just
put them they look confusing but this
there's there's let me let me go over
this they basically show us sixteen by
sixteen table that's from a paper I gave
at a conference a couple years ago but
what they show is I go over sixteen
different mind agents sixteen different
cognitive processes from within OpenCog
and they're along both the road than the
columns matrix and then I show how each
one of them can help the other one to be
smarter so that that's sort of a sixteen
by sixteen cognitive synergy table of
how each part helps each other part and
if you made a similar table for all the
parts of the brain it'd be a lot bigger
and a lot harder to comprehend than this
16 by 16 table but I think that's that's
kind of the way that we should be
in my view not so much about what does
this one algorithm do but about all
these different processes in here how do
they all work together and do they trip
each other offered it or did they they
help each other out and really you could
you could write a paper or more on each
of the entries in this in this table
which would be a lot of research papers
but that's one way of looking at the
magnitude of the problem of building AGI
accordingly this sort of architecture
because each of these interactions
doesn't have to be coded separately they
all kind of come out of the system but
they probably all have to be thought
about yeah a question yeah yeah
yeah um nothing is wrong with the atom
representation the trouble is what
pattern recognition processes are early
doing and it will become very very slow
the thing with the hierarchical
representation is that the pattern
recognition and each hierarchical level
that recognizes things and the level
below it conditions on the things above
it
the representation itself is attuned to
the hierarchical structure of the visual
and the constant world in principle you
could build that hierarchy inside the
atom then it would just be very
specialized and there would be no
problem with that
in practice Destin was a separate code
base that sort of works the other thing
in a more practical level is we
implemented CUDA so we run destiny on
NVIDIA family which is a separate
machine and OpenCog so to talk for just
a couple minutes about this the software
architecture there's a rough analogy you
could draw between the parts of the
OpenCog system and an operating system
like the atom table is sort of like a
file system or a database my agents are
sort of like UNIX demons or processes we
have a scheduler and it schedules the
mind agents most simply you could just
look through all of them more
sophisticated Li you give more time to
the ones it thinks will be useful
there's input/output which we use XML
now for that similar to IO an operating
system there's a simple programming
language that Moshe designed a long time
ago called combo which you can use to
program procedural knowledge there's a
forgetting mind agent which kind of
serves a role of a garbage collector I
mean it's a some reasonable mapping I
mean not in principle you could fork a
version of money
so it would be like OpenCog Linux or
something which there's no point to do
that at this at this moment but there's
a reasonable analogy between what
OpenCog the ISM and what an operating
system does in terms of practically
working with the system right now it
runs on Linux and on OSX people have
tried to get to work on Windows and have
succeeded but no one has ever maintained
the one that was for covers after they
did it initially so it's it's pretty
much being pursued on UNIX based systems
now the core of the code it is C++ with
heavy use of STL and boost templates so
it's fairly elegant but Harry sixty plus
plaus code some of our developers really
hated C++ so we we recently made it
possible the right mind agents in Python
instead so there's a Python API to the
scheduler in the atom space and so on I
would I would like to enable Haskell as
well because it's my favorite language
but right right now it's C++ or Python
which is an improvement over C++ only
and there there's some bits and pieces
in Java but those are accessed remotely
by C++ I mean in in practice you know
downloading the code and running it is
not hard that there's a distribution
called cog bun - which is a bun - Linux
with all the libraries and the right
distros installed on the CD so you can
you can download that cv round in a
virtual machine and you can get OpenCog
- to do something to really make use of
the system for anything significant I've
found that with full-time hires in the
project it takes like two months before
they can learn to do anything useful I
mean I think that's kind of typical for
any large large software system I would
I would like to build simple tutorials
like the lighter guys have but I think
even after we've done that it was so
that people a long time to become useful
not just because the code is complicated
because the ideas are complicated
and this stage AI algorithms are finicky
but I mean we have we have brought in
like new master students and so forth
and in China and after a couple months
of hand-holding they learned to program
stuff in the system for their master's
thesis so it's not it's not hideously
impenetrable here question the mind
agents are hand coded yeah that's right
in in after the singularity they'll be
learned for the moment so so and I'll
move on to something more more
lightweight for the next portion in the
talk so I've talked very loosely about
what the system is I mean it's a big
system just as with Lyda it's pretty
much impossible to explain a complex
software system like this in an hour it
would take a few days actually and in
that vein there if you look online there
are a number of talks from the AG I
summer school we did in gentlemen in
2009 where various OpenCog team members
talked about components of the system
it's like a series of two our lectures
maybe 15 20 hours altogether so if if
you really want to hear Talking Heads
discourse about OpenCog that there's
video food available online that fairly
careful people showing running in the
system during the talks and so on
but I'm going to move on now to
questions of what what do we do with
this system and this is a bit of a
prelude to what I'll talk about tomorrow
night but very briefly when I summarized
the the HEI roadmap workshop the in
Amara I had an University of Tennessee
in 2009 where the workshop gathered
together 12 AGR guys in an effort to
kind of agree on one roadmap toward AGI
well that didn't quite work out as you
might predict given the great and
vehement diversity of opinion within the
AG our community but we did come to some
sort of consensus on what made sense and
what didn't make
people throughout a lot of different
scenarios which were kind of environment
test combinations they thought made
sense for idea the ones I contributed
you see there are preschool learning
because I kind of like the developmental
path other things discussed were general
video game learning I'm going to play a
video game you've never seen before
whether without instructions going
through that the elementary school or
high school curriculum learning to
comprehend scenes and stories which Joe
Shabak talked about very convincingly
Steve Wozniak's coffee test which is
basically can you make a robot that will
go into a random American house and
figure out how to make a cup of coffee
so you can look at each of these in
different ways in terms of how much
capability they require how much
contextual social cultural engagement
they require and another thing we did
was try to break these down into
different competency areas and kind of
look at for each each of these different
hmm wait hold on I'm missing a slide it
was back there there we are
yeah so that was a start I meant to show
before it's actually useful now so when
I was talking about general intelligence
and how the mind is specialized to
particular environments one one thing
you quickly include is that general
intelligence relative to the
environments that we evolved for tends
to involve certain key competencies and
we spent a lot of time at the AGI
roadmap workshop trying to kind of pull
together our knowledge of cognitive
science and see what are these
competencies I mean it's not a
hard-and-fast list but this was a one
stab at it
we knew perception actuation memory
learning reasoning planning attention
motivation emotion modeling self another
social interaction
indication quantitative analysis and
then we can build and create things and
I mean you can merge some of these
together or add a couple other ones if
you want to but these broad competency
areas are kind of what humans do that we
call general intelligence which are the
way that we achieve our goals given our
perceptions in everyday human
environments and you can break each of
those down into a lot of different
things so actuation there's basic
physical skills there's tool years
there's navigation there's
proprioception knowing where your body
is for emotion you have emotional
expression you have understanding of
emotions perceiving of emotions control
of emotions and so forth and getting
back to where I was in the talk if you
have any one of these scenarios you can
then look for each of these competency
areas a learning actuation or whatever
you can see how is that competency area
manifested in that scenario so one way
to generate a suite of tasks for testing
a GIS is pick a sufficiently rich
scenario which could be preschool
elementary school or making coffee go
through all the competencies that people
need to have to be generally
intelligence in the common sense human
world and make tests for that competency
so saying in the concept of a preschool
one way to test learning is can you
learn to build a particular structure
out of blocks and that actually that
also tests the competency of building
and creation and if you learn through
communication it also tests the
competency of and communication in the
context of the Wozniak coffee test you
could run through all those different
competency areas so the preschool
approach is the one that interests me
the most roughly speaking and in
thinking about that I need to think
about all the different competencies
that humans demonstrate an ordinary
journal intelligence and how could you
measure those in the context of a
virtual robotic preschool now one of the
vexing issues that came up at the AGI
roadmap
workshop is a vexing issue for AGI in
general which is how important is
embodiment and I mean you have some guys
like rod Brooks who think a body man is
critical if you're not starting with
robots you're not doing anything that's
really pushing in the right direction to
AGI you have others who think that
robots are a waste of time I mean I was
at a some AI workshop not the AGI
roadmap workshop but another one that
was organized by the onr and the
consensus among those guys with maybe
20% being the exception was it all work
on robots has been wasted and that had
made no contribution to AI which is
fairly strong but a substantial subset
of good old-fashioned AI community
believes that so that's the other
extreme my view is that robots are
pretty convenient and they may not be
necessary but if you really want to make
a system that's human-like intelligence
I think it's gonna be hard to do that
without the system having a body that
can engage with the world not
necessarily exactly like a human being
but then in some vaguely similar way I
don't think that's the only kind of
intelligence I think you could make many
other kinds of general intelligence
maybe one that's a super intelligent
Google search engine or one that's a
mathematical theorem provers way better
than any human could ever be but could
move any type of mathematical theorem
but if you're talking about a human-like
intelligence it's hard to separate that
from the body really on the other hand
that's an annoying conclusion because it
brings you right up against limitations
of our current technology and current
virtual world and game platforms they're
all inadequate I mean it's hard if it's
hard to mediate interactions between
your AI agent and an object in the
current game engine like if if the AI
Ben wanted to pick up the virtual World
Cup there has to be a socket on his hand
and the socket on the cup letting them
connect to each other so that doesn't
let me up like that let me try the
balance in my head
I don't know if I should try this yeah I
mean you need to program in all the
affordances and interactions which is
really limits limits flexibility and the
agent control was done by fixed
pre-programmed animations so you may
have a touch hand ahead animation you
touch your hand your head like this but
if you look at what little kids are
doing in a preschool how much of it is
just due to the multiple affordances of
of movement and in the world an
interaction with objects no I thought of
a way to work around this a few years
ago but haven't had resources to
implement which is if you if you
interface top-of-the-line robot
simulators with virtual worlds basically
you have a massive multiplayer robot
simulator that would work around a lot
of these problems and it's really just a
software engineering issue because there
are open source instances of each of
these but it's a big problem you also
have other problems there's no fluids
powder space or fabrics I mean I I tried
to design a physics engine to work
around this where everything is made of
little tiny balls you can figure out how
can you see me like different kinds of
subsystems by sticking little balls
together that could sort of work robots
also have a big problem if you think
about it look at your own body every
part of your body almost as a sensor and
an actuator you know I mean you can feel
things even inside yourself and all
around yourself and almost every part of
you can do something that's pretty cool
I mean if you look at a robot body on
the other hand like that the now robot
most of it cannot do anything there's
only a few joints that can move most of
it cannot feel anything there's pressure
sensors in the joints right that's it
you could like get a saw and saw a big
hole in its back it would never feel it
right so I think robotics has gone in an
unfortunate direction in that regard you
know I've thought about this like how
could you make different robots which
you got like little ping-pong ball sized
cells each of which has sensors and
actuators and like paints them all
together and something that could walk
around so you both in the case of
virtual worlds and in the case of robots
you come to the conclusion that the
technology
not quite right for AGI and then we have
to work with them anyway and we can
still do cool stuff yeah
the balls are small enough they must be
enough we don't believe in democracy
atoms
well we can discuss that later yeah
because you could know the tog on that
little balls
well yeah don't stop my balls so I think
yeah
well I mean that is
you
don't refer to money in a world that is
earmarked for development of robots or
virtual worlds which it may it may be
easier to nudge some of that to be spent
on stuff useful for AGI than to get it
diverted to it to AGI I mean certainly
in the virtual world context I mean
there's a whole there's a lot of
momentum behind the open sim in terms of
an open-source virtual world so if the
opensim community decide to integrate a
robot simulator with open soon it'll be
pretty nice I mean I think we we do run
into these problems that very concretely
they're not that abstract like having
the robot they can't feel what it's
doing and I become convinced that not
using robots is even worse than using
robots
yeah yeah and then you get into the
question of how bad can the robot be and
still be useful right was there another
question
okay sure don't compress my balls so let
me I'll give myself a break and spend a
few minutes showing the videos I don't
think I'll I don't know
half the people here I've seen this one
this is an old one so this is from
previous work we were doing in the
multiverse virtual world when we we had
the AI using a virtual dog to play fetch
and you can see their hand drawn on the
notes and links some of the nodes and
links in the item space that a dog that
OpenCog that controls the dog originates
when it sees certain things so this was
using Moses
to learn by example so the one moment
has the other woman play fetch and the
dog watches and then there they say what
we're doing is Carl playing fetch then
they try to get the dog to play fetch
and the dog copies and we've this was
done as part of a collaboration with
Electric Sheep company which was aimed
to make virtual cuts for Second Life and
they they wound up dropping that project
and so we we've since wound up dropping
the virtual dog we've done read the
cheesy-looking
but this was a it was a learning
experience in terms of getting Moses to
do stuff where the fitness function
comes from in interaction and in the
world anyway and also in terms of how
annoying it is we had like 75 animations
for that Chihuahua like wagtail Widow
left years so forth it it really made
you wish you had a robot simulator where
they were just just serving other
commands so this one
on YouTube everything must have a video
soundtrack rather so this this just
shows the virtual world that we're
making now after this or a couple others
that actually have the AI in it but
since since the AI can't do much yet
it's easy to show the virtual world with
a human controlling food so we what we
tried to do is build a virtual world in
the unity 3d game engine that is it's
similar to the game Minecraft which is a
game we basically built build stuff with
blocks so we found a Minecraft dish
plugin for unity and basically build the
world out of blocks then we built this
helicopter manipulator that can pick
things up and move it around and so
forth and we we picked the Minecraft
metaphor after a lot of consultation
with different people and consideration
of different options because we we
wanted something that would appeal to
the gaming community potentially in case
you want to do a commercial project on
the AGI for games but also something
with a lot of flexibility and of course
blocks world has a long history and I so
minecraft is basically a blocks world
it's a game that people find fun to
download and play with it's very
flexible you can you can build a lot of
different things with box if some bad
guys coming after you may build a wall
if he's fat you may know the house and
hide inside
and so forth now so far we literally we
just got this Minecraft world working
like a month ago so we haven't done too
much with it yet but this this video
shows a little bit of the open side
system where it shows the system and
that's among other goals that's two
goals one of which is called integrity
and one is energy integrity is kind of
like how safe comfortable on whole
doesn't feel energy is how much Energy's
it have to do stuff and when we set this
up but that's to get batteries on the
other hand it feels safer more integral
in its house so basically with this
configuration what it does is run back
and forth like it sits in this house
because it likes it there because it has
more integrity until it starts to run
out of energy then it runs out and grabs
not the battery to get more energy they
think goes back in sits in this house
again so it's like the animals sitting
in its cave until it gets hungry then it
goes out and kills someone to get food
he goes back in sits in its cave it's a
pretty simple animal level behavior and
it had to learn that batteries given
energy and so forth took a while and
then this this shows the different that
those grass briefly showed that the
fluctuations of the fulfilment of
different goals as its integrity went up
and down and it's and its energy level
once went to and down so it can do basic
pathfinding and it as I said learned
that I need to find a battery to get
energy so that this just shows that
being able to go around I trip to find
the battery they say it's going to
figure out to climb up on top of things
because there's no battery on the ground
there's a little guy that's the guy who
replaced the dog there'll be a big ugly
robot to replace the woman in the dress
so he looks around he can't find it
eventually he's got to come up with the
idea well maybe maybe there is no he has
to figure out to go upstairs but since
he's a Minecraft type world he doesn't
need to use his arms and light upstairs
I think once he figured it out he just
pop upstairs
oh my god he seems close enough to grab
it all right so now he gets to eat but
now are defined in terms of lower level
system parameters in accordance with the
the SCI equations that we got from out
from Detrick donors so that the size
sort of regulates the basic emotions and
actions but no man wants to learn
something like batteries give me food he
goes back to open cogs never in learning
algorithm
so this is a much worse video because it
involves an actual robot and doing
anything with the robot is really
annoying but it can I mean it the robot
is connected to OpenCog not not using
Destin this is using a simpler interface
and it can do basic commands it can find
stuff it can navigate around it can
recognize objects and our goal is to get
imitation learning working with the
robot when we've done that in really
simple cases I got you can like walks
somewhere and say ok I walked to the
wall now you walk to the wall you can
copy simple things but so far we've been
a bit frustrated with just vision
processing issues which is why we're
moving to using Destin as a vision
preprocessor but it's the other thing
you notice here which is kind of
pathetic is how empty the robot's world
is ok the reason you might have worked
with robots is because of the amazing
richness is the physical world then
inevitably you make a robot lab that
does almost nothing in it because
otherwise the robot gets too confused so
obviously we need to get beyond that
level in order to really make use of
robots or anything but playing around
with but still I mean you have to you
have to start simple and then the now
robot has a number of limitations I mean
it looks cool for demos but the the
hands don't pick things up that well and
the cameras are like extremely low res
even compared to the camera in the
iPhone 3 or something on the other hand
is a humanoid robot that did walks it
recognizes speech and it outputs stuff
by Wi-Fi and let alas get started using
OpenCog to control robots which is fun
whether it's walking into the wall oops
so much for him so the last thing I
discussed is that the developmental
roadmap and this is this won't take too
long so
a conceptual way to think about the
developmental roadmap is in terms of
Piaget's theory of cognitive science or
which is it's not really in fashion
among developmental psychologists
anymore I mean they found this a lot
more detailed to child development than
Piazza on the other hand it's very
popular among non developmental
psychologists because it's simple and
the reality is much more complex and
this is sort of an OK approximation he
looked at an infantile level where the
system is just concerned with making
sense of an achieving really simple
goals our robot is that you saw this
doing stuff like that I need the battery
I need to be safe I need the battery
gonna need to be safe when he called the
concrete phase which happens among
toddlers rather than inference you have
a sense of who yourself is and you've
learned mental representations and
operations on them the formal phase I
would argue some adults never get to
based on teaching remedial math well I
mean it has to do with our abstract
reasoning and hypothesizing we can say
do a scientific experiment test test the
hypothesis understand that the variable
may represent different values at
different times and so forth so I when I
taught algebra I had a student who could
never understand variables she thought I
always meant one they always meant to
see always meant three and so forth she
took algebra seven times and failed and
she was in the concrete phase she hadn't
gone on to the formal phase abstraction
and then some have talked about going
beyond that to the reflexive phase where
you can kind of fully and deeply reason
about your whole self as it which Piazza
it didn't do then AG eyes may go even
further and rewrite all their source
code which you'll need and beyond that
the reflexive phase and we had Stefano
guy and I had a paper in the 2006 AGI
workshop trying to map this into an
uncertain reasoning system like like PLN
where an infantile system can do very
simple inferences about the world the
concrete system
has to be able to adapt its reasoning
based on experience otherwise that would
decide horrible combinatorial explosions
and a system at the formal level can
reason about inference control it can
think about thinking which is what's
needed for scientific experiments and
mathematical theorem proving and so
forth you need a certain level of
reflexive abstraction and then when you
get beyond that you get to the level
most humans are not yet which is being
able to reason rigorously about
ourselves and and how we reason so if I
go back to my very wishful development
trajectory for OpenCog how things will
unfold if they go as we all hope and
nothing screws up you can see that in
terms of these PR jet and levels so that
right now we're at the baby level
basically and if things go well we could
be at the concrete level in a few years
what's called a proto AGI mod and then
that would be kind of a virtual toddler
or a robot toddler and then the formal
level would come along being able to do
advanced reasoning and learning and you
have to think about that carefully
because in a sense an automated theorem
prover can already do formal reasoning
but it's doing it in a very narrow AI
sort of way it's not controlling its own
reasoning the way automated theorem
provers work is they work for a few
steps then some human comes in and cuts
things off and kind of directs them
again the next time so to be able to do
formal reasoning in an Ag is way we need
to be able to do inference control of
complex formal reasoning which I think
needs to come out of some kind of
grounded common sense knowledge and then
the reflexive level would come later but
you you could view the hopeful OpenCog
development trajectory as roughly
following the the PIO jetan stages so
concretely what we're working on now we
have a team of I guess five programmers
among graphics guys in Hong Kong this is
a two-year funded project co-funded by
Nova Manta in my company and
Hong Kong government what we're looking
at there is trying to make that little
robot as smart as it can in the
Minecraft type world basically and
develop this into a software toolkit
that can be used to apply OpenCog to any
different video game and the one
possibility there is throwing it out
into the commercial projects in gaming
afterwards but we're in terms of theory
we're hoping to get some interesting
cognitive synergies going there say
between Moses PLN and concept creation
and see how the the synergy concept pans
out and in the in the virtual world
context and this project started six
months ago so we've moved a lot of
cleaning up with a code base we've built
the virtual world and done some simple
things in in JAMA and University we're
working with the robot we hope to open
cog up to the robot we've poured the
destined to CUDA and the plan there in
the next few years is just to basically
take the code we develop in Hong Kong
for the video game agent and plug it
into the robot and see what works and
what doesn't work and then plug in the
division processing infrastructure as
well and we we've cement a proposal to a
few funding sources for a four year
project to make a virtual toddler which
I won't go through in detail now but
that would take more work probably a
team of say 12 people for four years so
that I think when I give this very
ambitious trajectory that this assumes
we have more than a handful of people
working on the project it's more like
what could we do if we had say 12 or 15
dedicated full-time guys working on it
which is it's not a lot compared to what
it takes to build like Microsoft Office
or world of warcraft this is a it's a
lot on the on the standard of a GI
funding and the last topic which I won't
really get to is OpenCog is also being
used within some practical projects even
though it's not a full-fledged Agia it
has a lot of interesting tools in it and
we've been using them within our work
analyzing DNA data and biomedical
informatics we're talking to game
companies about branching out beyond the
prototype and using it since some actual
video games and we're talking to some
hedge fund people about using some of
this for stock prediction on the Hong
Kong exchange as well and this these are
not really AGI applications these are
narrow AI applications of tools
developed in the context of an AGI
system which any one of these projects
winds up being 80% the applications and
20% the AGI is code nevertheless I mean
as well as generating revenue they can
be interesting just in terms of seeing
what your code can do in different
contexts if you're interested in the AI
and genetics connection I had an article
in H+ Magazine on that a while ago which
was called a eyes super flies in the
path to immortality
since we're we're analyzing the genetics
of these flies that have been evolved to
live five times as long as normal flies
using mostly machine learning but now
we're using a little bit of pln
inference in there as well which again
it's it's far short of an AGI
bioscientists but it's it's kind of cool
to see these tools used for for real
stuff now it gives you the satisfaction
of making stuff work whereas working on
a GI I it's it's a long haul and then
the gold the gold is still yours in the
future so in the big picture the hope is
that by putting together the virtual
toddler stuff with narrow AI
applications with the same codebase
eventually that can all come together
with the common sense knowledge and the
the vertical area of specialization
since they're done using the same AR
tools in the same codebase
they should all play together well when
we get further along if we get there
alright so that's that's it any more
questions
you mean why are we so stupid I mean why
artificial intelligence
oh well I mean it's it's amazing that we
can do as well as we can if you could
Pastor like chimpanzees cockroaches and
dogs and they were we're kind of the
were the first intelligence on earth to
evolve with the right intelligence and
until manipulation capability and even
think about these things so I'm not sure
it's been hard on the historical level
AI was founded only like 60 years ago so
in the grand in the grand scheme of
things it may look really fast it's just
hard relative to our lifespan but I mean
the the other issue of course is our
best example is the brain and we don't
know how to measure it very well yeah
you know where the flying cars I mean it
seems like an easier problem I just sit
in traffic to get here yeah</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>