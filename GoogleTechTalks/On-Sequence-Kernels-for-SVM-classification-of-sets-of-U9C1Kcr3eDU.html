<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>On Sequence Kernels for SVM classification of sets of... | Coder Coacher - Coaching Coders</title><meta content="On Sequence Kernels for SVM classification of sets of... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>On Sequence Kernels for SVM classification of sets of...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/U9C1Kcr3eDU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's my pleasure to introduce our guest
this afternoon Holly dowdy he's working
in the french national resource center
cnrs doing basically machine learning
and applying these days to speech and
speaker verification and holidays going
to talk about his recent work with one
of his pc stood whom i have been on the
PhD thesis and i found it very
interesting so Thank You Sammy so before
we get started so I won't get too thanks
Emmy for the invitation and thank all of
you for your presence and all the other
said we are somewhere else so so as I'm
said I work at scenarios but I'm
basically at the basis some ninja
researcher but I lived in RIA 40 years
ago to go to Toulouse where the way that
is kind of close California weather and
but I see work with my India group on
lateral aspect of speech and statistical
modeling so i will present to you hear
some work on sequence caring for svm
classification of chess all vectors
applied to speaker verification and this
work was mainly done by year Jonah Aloha
drew my former PhD student who will just
defend two months ago and also in
collaboration with Francis back who
until a couple of years ago was your
neighbor here at Berkeley he made his
PhD thesis with Michael Jordan aaj be
okay so he happened on some 30 class
pects we need some help here
yeah okay anyway so as I wrote on the
abstract abstract i will use speaker
verification as first motivation to this
work and also as a vehicle to
demonstrate the effectiveness of of the
new method that we have proposed so
speaker verification is a binary
classification task where you have set
of imposter or backs on what we call
background the CEO sequences of
utterance of that are used for training
and they'll have target speaker for
which you injure you have only a little
amount of data and then so you build a
classifier that will score a test
utterance and decision is made whether
the speaker is the target one or an
imposture and the the state-of-the-art
systems are ruled by the generative
approach of gaussian mixture model so
basically the gaussian mixture model is
built using the background data so this
will represent the universal speaker
this is why it's called ubm as unity
universal background model and then
another gem a gem is is learned by
maximum a posteriori adaptation using
the target speaker data and then the
decision is made by a lovely clear log
likelihood ratio okay so and however
when you think about this problem it's a
binary classification tasks so support
vector machine should be in principle
well adapt to this kind of problem
because if we SVM have shown this either
power in binary classification problem
and this is so SVM not only in kind of
speech but they are powerful because
they they have very good clear chicken
foundations even though I know some
people who don't agree and not you
sefino alcova and the learning algorithm
I we'll mastered and valuable and Sammy
here is famous for in his SVM tours and
when applied to different kind of
classification problem but static data
classification problem the results are
in general very competitive and any good
however in large scale problems in
general well speech is not a really
largest with a scale problem at least
call it this way in speech processing
spm are not are not that so far are not
that good why because in speech we deal
with dynamic data and and svm has been
built really for static data so with the
extension to dynamic data is not easy
and also the size of training corpuses
is enormous in speech so as learning
algorithm are at least n squared for svm
so this would be time and memory
consuming and in the particular field
speaker verification SVM's when applied
at the frame level which means that when
you put all the frames all your data in
the same bag and you classify a
classified after doing some christian to
reduce the size it doesn't work so so we
do when you start especially without
okay anyway the problem of speaker
vocational is not to classify frames but
you classify utterance which are
sequences and we don't care about the
ordering of the sequences because we are
in the framework of texts independence
which sequence so our goal actually in
speaker for irrigation want to use SVM's
is to classify sets of vectors so let's
see if we can build the Colonel's
because of course
some some carriers that that will be
adapt to classify sequences i will use
the word sequence because it's easier
but actually what I'm going to
potentially just for sequence of vector
because you don't care about the real
dynamic just about the variable length
and the ungodly will kill colonel that
that that are similar to measure between
sets of vector or sequences so let me
recall some the basics on can so I
assume that people are familiar with
this VM so a kernel is similar to
measure and in a nice property that is
that is helpful in many case from the
article and also practical point of view
is to have is the Mercer property which
means that the current is symmetric and
positive define it and using the
Mercer's theorem with me each it says
that there exists an expansion Phi in a
high-dimensional in fine space generally
unknown called the feature space in
which the care k is a dot product it can
be written as dot product in this
industry shipping and this party is the
one that allows sem to work because it
maps in a linear problem to non-linear
one by mapping the data in this high
dimensional feature space so if
somebody's not very well on this vm just
dropped me and i can spend 20 minutes
which anyway the key for svm is kernels
and our goal is now to build kernels
that measure the synergy between
sequences so in the literature there
exists three families of sequence
cameras they are list there and here I'm
talking of course only about continuous
data I'm not talking about string string
kernels or careless about graph but we
unconsciously just so there are three
families the first one is the mutually
will so called mutual information cannon
which are based on the validity of an a
priori probability distribution of the
the second family is Kara between
probability densities which means that
you have a sequence you estimate poverty
distribution and then your kernel is
Colonel between two probability
distributions and the last family is
combination of vector Kira's which is
basically you combine your frame level
values of Colonel the way the way you
watch and the way it could work okay so
this is I said already but just for the
special issue of speech here the
definition of sequence is volume and set
set of vectors and and we use the same
speaker a sequence from the same seekers
same recording session okay so let me
give a brief overview about all the
kernels that are used in speech speaker
verification so there has been a so the
first one is probably the most known of
sequins are probably the first one now
is the Fisher kernel which has actually
been shown to be an approximation of
material information kernel and it has
two or three years ago there has been a
thesis by somebody from sheffield
university called invention one made
very nice thesis undo the application of
a Fisher Colonel to speaker verification
and the future kernel is here it's a
normalized dot product between future
expansions which take the grade the
likelihood gradient with respect to the
parameters of the ubm so from deal with
respect to yeah we respect to the
parameter of the gaussian mixture model
the ubm and so you can guess especially
that systems that work well gmm based
system that works well in speaker
verification they are kind of
are huge so if you take learned about
all the parameters your expansion is
huge so and the result be thesis was
that it doesn't really give better
result than just clucky classical GMM
and it's a very complete a very
computationally consuming so so so this
is just to show we skip this how future
expansion works go to the other family
of care that i've been using speech well
the first one its gel we use it it
hasn't been done before which anyway so
the probability product its current
between probability density so also
forgot the property product journals
which are oops so on which are here it
just from sequence you do learning to
estimate the two distribution and then
the product product can be choose
between the distribution is given here
and for a gaussian mixture model you can
have an illogical form if the degree of
q is 1 and this is just used for more
robustness when you use you you apply it
and angie ohm has tested this dis kernel
for pickin verification the other a kind
of occur between privacy densities is
the exponential embedding of divergence
so which is alga to do Gaussian kernel
so you take the two distribution you
have diver or distances that should be
symmetric of course and you exponentiate
each and you get a curl and recently
just year ago and last I casp William
Campbell and his colleague had developed
and using just a very simple
approximation of the Kyle diversions
they come up with a kernel called super
vector GMM which is basically you for
each sequence you you estimate using
maximum a posteriori adaptation you
estimate a gaussian mixture model using
the yoga
UBM model and then you just construct
the super vector buy stock in all the
means of the gaussians and then the
Karen just normalize that product but
the problem of course is that every time
you need to estimate a Gaussian water in
training and of course in jest so and
the other kind of kernels the other
family is who is we when you just
compute a function of entire and intra
sequence frames or elements and a simple
example is just the linear combination
taken just the mean overall the the
possible to element by Colonel values
however the problem of such kernel is
that the complexity is t squared for
each candle computation and for
applications like nist once it's
impossible to use such girls so another
famous kernel that actually motivated
our work is so called the generalized
linear discriminant sequence carriage
LDS kernel which is defined this way the
motivations are I'm not going to she's
defined here it just so Phi is a
polynomial next expansion so you expand
all your frames or your acoustic
vectorial you take the mean so using an
expansion of degree of the Greek you you
do the same thing for the other sequence
and then you normalize by the matrix of
second moments and as you see it's the
dot product between these two it's a
normalized that prog between these two
expansions and I remind that this is a
polynomial expedia takes all we take all
the monomers of degree less or equal
thank you so this scale was
motivated by an idea or principle that
you build a discriminative model Adonis
sequence and you scored on the other and
then you do some approximation to make
it symmetric let me I prefer to see it
because since I've been charged in
talking you held the word normalization
all the time so we r CT because it's
necessary because you give the same
importance to all a component in this
unknown feature features / that you
don't control so the advantage of Jesus
kernel is that it and of this explicit
expansion because now you explicitly mad
to the feature space so now you know the
feature space is it it leads to high
efficiency during tests because you just
use afterwards a linear SVM model
however the problem is that you cannot
use high degree expansion and in
practice you cannot go more than three
because it grows geometry p the size of
the feature space so so this was our
question could we consider this family
of kernels and see if we consider in
expansion fee that define a version that
defines the mercer kernel which is can
be probably unknown and see if what can
come up with such far away from such
family of carols so now I I go so to the
new family of cameras that we we have
developed and you see here so we are
just taking again the yielded a scanner
form but now we are considering any
mercy expansion that defines a mercer
kernel or that is defined by mercer
kernel and normalization metric with the
second moment matrix of the expansion
file we add a rigorous a shin chair
because it is necessary from theoretical
point of view also new miracle because
if the dimension of the potentially
unknown expansion is high very hard or
refine it so this matrix want to be
invertible so you add this organization
change to Tremaine
invertible and our goal of course is to
caramelize this expression is to compute
this without necessity of having access
to fee just using the kernel trick
another nice also extension of your
curls is instead of considering the
second moment matrix here if you use the
covariance matrix and why this is
interesting because it gives another
interpretation interpretation to this
family wave of kernels since as you may
know that any colonel defines a distance
and if you compute is easy just to line
of computation the distance associated
with this with this kernel adjust the
empirical mahana Lewis distance between
two gaussians in the feature space so in
other words if you use the covariance
here you're ending up computing a kernel
between probability distributions
Gaussian but in the future space and
this is kind of interesting because well
it depends on who because in in many
algorithms such a honor PCA what're you
can this the apotheosis of Gaussian teen
the future space is used so yeah
yeah sorry I have to repeat your
question they asked me so why do do I
need the regularization term when the
covariance is there just because as I
said is from 30 k pollard also an
American point of view if and as you
will see this epsilon is going to
disappear but here is just too what you
do the generalization with the
covariance matrix and and from a
practical point of view if you use the
stuff as it is sometimes this from the
numerical point of view this disk virus
is not invertible if you have if you
have try to in virtual to really from
numerical point of view covariance
matrix sometimes if you even though for
another you have to add this epsilon of
the diagonal but I see your point okay
so where ya so i will say that with this
we are now having a kernel between
probability distributions in the feature
space still have
okay so this is one of the major
theoretical results so is that we could
prove that we can rewrite these kernels
in a duel form or give give a canalized
form discount so this is virgin girl so
this is when there is no realization
assuming that SV is invertible and
actually even though it's not you can
the expressions hold with the children
in verses and you see that the dual form
involve involves the gray matrix which
is not surprising and the so-called
empirical map which is just start vector
of the value of of each element of the
sequence with the training data in the
case you have the rehabilitation germ
you need to make an apology switch is
kind of weak actually if the opposes is
that all the expansions lie in the span
of of the background data so if your
background data actually I shouldn't
have training if your background data
are your training data so this this
condition always hold because you always
work in the span of the training data
but in our applications not the case the
background data are not are not the
training data are not I mean there there
are just subset of the training data so
thick this condition may not all but if
you have a large corpus and lot of data
so you can assume this condition but
anyway in India in our plication this
epsilon is going to disappear and if you
use the covalence now so the monopolist
sequence kernels you have just to insert
a projection a projection matrix here
so this is the first theoretical result
and here comparison of the complexities
from the origin between the original
form and the dual form so you see that
we have achieved what we wanted so far
is that just the D is the dimension of
the feature space now we replace it by n
which is the remember the size of the
background data so the advantage is that
now we can use any kind of expansion
including in finite one associated with
the care Gaussian kernel for instance
but the problem is that we still have
something that we can not use here in
practice because the size n of bagging
is very huge so so our objective is to
find a an approximate but tractable a
form of this uniform so I'll go to find
an approximate appropriate approximation
so so I said I will reduce if you if you
come back here you see that what what
makes things here n or n squared is
decipher gram matrix and size of this
empirical map here so our goal is to
reduce the size of the empirical map but
still keep a maximum of information and
for many reasons which we choose the
incomplete shallaki decomposition to
achieve this approximation so the
accomplished rescue decomposition fur
acts this way it selects code book from
the origin said to be of background data
and if I is the index of this book so
the approximation is given this way of
the gray matrix and this and the icd
minimize the difference of the trace
between K between the grand matrix and
its approximation and we can easy to
show also that it means eyes do the mean
square and the trace this this choice
here is actually in norm on matrices
defiant and positive it's not a norm for
for matrices but for this particular
class of matrices so it's a good
approximation it's a good criterion for
a proxima shin and also a nice thing is
that at no time we need to do to store
the full grain matrix which is N squared
to compute its approximation there is an
algorithm developed recently by Francis
work and Michael Jordan which allowed to
to compute the icd without storing the
full matrix it's it's a it's an
algorithm linear in n and of course this
stuff you do which of light
so here is a little example what the ICD
does so the blue the blue point are the
training data and this is and the red is
the result of nicd using a Gaussian
kernel so it does the kind of clustering
that spread or you have question respond
so like the same speakers or is that
like male voices
so your your status is long and I have
to repeat it here so the clusters
correspond to yeah so the distribution
our distribution of the blue is by model
and you were wondering what the to do
tues distribution are so I don't know
exactly but I i guess it's should be two
gaussians or something but this is just
a toy example no no this is not voice
data this is just a toy example just to
show what kind of clustering icd does ok
so now so the second theoretical result
is using the icd approximation we have a
new form of the the kernel which is
which resembles to the old one but now
we have reduced the size of the
empirical map and the size of the
normalization matrix and we moved from n
from n square 2m squared where m is the
size of of the codebook and of course
this is a trade of parameter that you
have to to independent you and you know
the efficiency and the autoxidation want
to achieve
okay so so now I move to the application
of of this sequence callous to speaker
verification and see what's going on so
the development corpus is used the the
NIST speaker regulation evaluation data
of 2003-2004 so half of the car Percy's
background data and the other half is
for validation so tune the upper
parameters for the kernels and for svm
and to choose also the decision
threshold and the evaluation is on the
NIST 2005's area and the evaluation is
of course given by mr. as by the
detection distinct the section cost
function which is so this is the
pre-processing I don't think that this
is important and here first result on on
the development data so first you want
to know in practice is what kind we have
defied the family of journals so now
from experimental II have a hint on what
should we choose in all these parameters
so and and the results showed that with
centering so using the covariance matrix
there is no significant game and this is
probably because the data are already
centered from the beginning and the
other important result is that the
regularization degrades and the reason
why is that now we are not using the
exact form we are not using the exact
form of the K but with an approximation
and actually the ICD does already a
regularization so this is why we don't
need anymore the epsilon epsilon chair
so in summary no centering and no
regularization
now what how to to choose and to and and
tune the vector kernel that defines the
map file so our best result were
obtained with Gaussian kernel but using
a good spread parameter and what you was
nice is that the one that works well all
the time is the one which is which have
a vertical motivation that I don't
remember the name of the author of the
paper but in this this paper they all
say that the the good value for all
should be this one with D dimension and
soon as the is the is the variance of
the data it's the mean of the variance
of date so now the size of the codebook
so probably I go fast so yeah I mean if
you are familiar with debt curve you
understand otherwise what you should
look at is the equal error rate and the
DCFS and the lowest they are the better
the system is of course so well right so
on the choice of the core book as I said
so m is trade of parameter that you have
to tune depend because as big as M as
the more m is is high the more your
approximation is good but the more your
but the less your system is efficient so
we tried many values of of em on
development corpus and you see that it
all the score always always the
performance always improve until a
certain point where they become flat and
this is not this show that a certain
product we achieved the good very good
approximation of the gray matrix so in
all the experience i'm going to show
later it's we chose em around 5000
elemental ticket code book so now the
evaluation so here so probably don't
need to look at it so now it's the real
evaluation on this 2005 and here we
compare so our you kernels with the gel
this one and stage of the arch you BMG
mm which is using a tool kit for Liam
James that that is valuable this has
been made a very much University of
Avignon who are or one of the best teams
in France in speaker verification they
are always from the best in evaluations
so and before that I don't have the pic
slide here is that we we check that G&amp;amp;S
gives exactly the same result when you
use any features based on Malibu
sequence kernel with a polynomial degree
of tree so is that it it validates the
theoretical result and and you see that
the the new cars are much better than GL
this one and they are competitive to
state-of-the-art you BM GMA and let me
just check now of course the best system
in in in all misty variations are are
you use use fusion so it's it's all time
any systems and at the end you combine
them so so here you have to again the
three systems basically LDS f SNS and
you BMG mm and the fusion between one
and two doesn't approve which is not
surprising because they spoil the same
information they do almost same thing
but when when we fuse any SVM with you
been
with the gmm it improves and the best
one of course is the one given by the
fusion of our system and the classically
BM GMA so conclusion is that there is a
significant gain in discriminative
generative fusion now now is the
evolution of all the kernels that I have
talked about so far probability product
Colonel GL DS Fisher Colonel FSN SUV m
and the support vector jamin and so you
can see that the best results are given
by the super vector GMM the approach
that just stuck Saul all the means of
GM's in one vector and do Norma is that
product but of course complexity is is
higher than than ours but still it is
the one that gives the best results and
our system come just next and the others
Fisher or our privacy protocol property
prod can truly do give better results so
again diffusion and diffusion is the
best system is obtained by fusing London
tree which is f SNS and super vector
James
so I so how much died I I don't know
when it's okay so in conclusion so we
have made 30 and experimental
exploration of sequence kernels or
cannons between between sets of vectors
I'll see that there is many
possibilities of sets of kernels and we
developed a new family of sequence
kernels that we have experimented on
benchmark misty evaluations and the best
performances was obtained by the
combination of this family of kernels
and the gmm super vector which is a very
recent method that that that shows a
effectiveness in speaker verification so
of course there are several ways to
improve SVM for speaker verification
this season but there is lot a lot to do
one one weakness of all my margin
classification method is that it's not
easy to adapt the classifications to new
data which generative approach do easily
so model adaptation is something that is
that is really of beginning to in large
margin classifier well for speaker
verification space salads so another
thing that could be very poor is to use
twink kernels to add had high level
future to the system because the species
know that high-level feature that pros
OD analytic can can can improve the
performance the other thing is that i
forgot to say that all the result of the
ubm GMM system or obtained using
so-called t norm which is a
normalization of the scores and which
improves a lot GMM with but this this t
norm or these norma genesis of the
scores are based on on the fact that
the scores are probability outputs but
as you know the output arts SVM's are
not probabilities and and the question
is how to handle this course how to to
build a score normalization which is
appropriate for a sphere this will also
in principle improve the results because
an elicitor gmm the old kind of
normalization are necessary to in
state-of-the-art system so I think I I
think I probably go as thank you we have
time for a few questions but let me
first remind you that this this
presentation is videotaped and will be
accessible on the outside google so
please keep your question at the level
where which can be heard outside google
so so no confidential questions and a
and no confidential answer sequences as
sets that means you throw out any
ordering yeah yeah yeah oh sorry yeah
when I when I consider sequel but you
just see language of you said it in the
beginning when I what I call sequences
are cysts of vectors and and we don't
care about the ordering because you are
texting independent independent task but
if your your question I'm going to post
question yourself what do you do when
you have dynamics this is your question
maybe you'll be gained I mean the follow
me on this problem maybe another problem
now another problem I actually this is a
problem that that interests me a lot is
how to hand a really true dynamic for
example how to use these VMs in speaker
in speech recognition and the problem
here is that you don't when if you want
to apply this you are not anymore just
in classification but you are doing
segmentation and this is something that
large margin classifiers cannot do so
far but there are some for example the
Fisher kernel or all the kill that are
based on on generative models on Roger
distribution you can use them to you can
I mean the Fisher Keller for example it
just requires the Grail likelihood of
the of the distribution on your
observation so you can take into account
the dynamics the problem how to segment
and I think there was some paper PhD
then with Mark gays from Cambridge
University in UK where they they apply
the SVM's using Fisher kernel for speech
recognition but just for later
recognition so they escaped the true
problem but if you have any ideas on how
to handle a the dynamics you
thank you very much thank you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>