<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New Techniques for Acquiring, Rendering, and Displaying Human Performances | Coder Coacher - Coaching Coders</title><meta content="New Techniques for Acquiring, Rendering, and Displaying Human Performances - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New Techniques for Acquiring, Rendering, and Displaying Human Performances</b></h2><h5 class="post__date">2008-03-01</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/556FvXHLtAo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I would like to say that Paul has moved
on after Berkeley to go to the
University of Southern California
Institute of creative technologies and
is the associate director of graphics
research over there involved with
photorealistic rendering and
environments and and more recently
virtual actors he was the computer
animation festival chair for siggraph
2007 and is now here over a Google to
tell us about his latest work so without
further ado I Paul de bewick thank you
very much again all right hey thanks
folks so it's very exciting to be here
and another reason that I'm up here in
the bay area's we've been doing a couple
of screenings of the computer animation
festival the electronic Theatre for
siggraph this is the show that
originally premiered in San Diego and
there's going to be a showing tonight if
you are a fan of computer graphics or
know any friends who might be at a San
Francisco State University if you just
go to the San Francisco siggraph website
SF Signori I think the show starts at
seven-thirty and it's going to have sony
SXRD 4k projection off of our hdcam sr
master tape and it's going to look
absolutely gorgeous so if you want to
see a very faithful rendition of the
siggraph computer animation festival the
best computer animations over the last
year that's a great opportunity to check
it out so what I brought to talk about
is some of the computer graphics work
that we've been doing at my group at the
Institute for Creative Technologies kind
of a sampling of a couple of recent
projects and kind of to motivate sort of
how we got here I have a little bit of
some more historical material the take
on computer graphics that my group has
mostly been involved in has been
graphics that tries to make a lot of use
of images and when I was doing my PhD at
berkeley i got interested in trying to
do pretty realistic renditions of things
like the Berkeley campus and it seemed
like photographs were a good way to do
that AI was a very lucky
to be helped out by Professor there
Chris Benton who did kite aerial
photography and eventually when we got
our kite off the ground we were able to
take some aerial photographs of the
Berkeley campus and in particularly of
the Berkeley campanelli which I thought
would be a good focal point for some
image based modeling and rendering of
this and i also got to climb up in the
lantern there and take some photographs
of the campus all the way around as well
and these photographs you can see here
we also found a couple of photographs
from an aerial mapping survey so we had
some parallax from away from the top of
the tower and then using a system that I
developed with CJ Taylor and my advisor
jitendra Malik we were able to build
interactively in not too much effort a
three-dimensional model of the campus
from these 20 photographs here it's not
a terribly detailed model it's kind of a
lot of you know boxy buildings maybe
with a roof on them the campanelli is by
far our best model but the idea of
course is that we're not going to look
at them this way we're going to look at
them with textures applied to them and
projected on so if we take that geometry
based model put the texture maps on
there maybe do a little view dependent
texture mapping depending on where
you're looking at it from it all of a
sudden became a really exciting a thing
for us to take a look at because it
looked really real we had no idea how we
could have made this model look as
realistic any other way and since I was
careful enough to take all the photos in
the same lighting conditions we sort of
are reproducing the appearance of the
reflectance properties all of the
surfaces relatively realistically and
the light transport how the light
bounced between all of the surfaces is
replicated there and I think somebody
over here on the left was pointing out
maybe like a hole filling algorithm
artifact that you that you can see so
anyway there's plenty of little
artifacts all around but it was real
enough that at least back then we could
believe we were really flying around the
campus and I had the chance to make a
short film this was a clip from it that
took some real video of campus as well
this is me standing on top of the
campanile II and doing a little match
move shot back to kind of a fly around
here
in order to create a virtual fly around
of the campus so when I have a chance to
play around with a Google Earth I really
loved seeing that a lot of these same
kind effects are here and they're right
there on software that everybody has and
the idea of that continuing to the point
where everything in Google Earth you
know looks every bit as photo real as
this does right here I think is a really
amazing vision for the future and
obviously Google is in the front seat
for doing that so this was an exciting
project it had some applications in the
movie industry as well for kind of like
doing virtual backgrounds and
reconstructing various kinds of sets for
virtual cinematography what I got
interested in after that was trying to
reconstruct other kinds of environments
so like another place that I tried to do
a reconstruction of was the interior of
st. Peter's Basilica and the idea was we
try to do a little dynamic simulation of
some objects that would be there in the
in the Basilica and I got interested in
trying to record real-world illumination
conditions so I could illuminate new
objects to insert into these image based
model modeled scenes so this is a scene
from a film we did for the siggraph 99
electronic Theatre where the interior of
st. Peter's was modeled from basically
two panoramas one in the nave one near
the altar here some rough geometry built
with the facade system to project onto
and then using high dynamic range
imagery to record the range of light
that was in there and then illuminate
these new computer-generated objects
with the light that was actually there
and the way that that technology really
works is that you would basically try to
use one of the several available
techniques for taking an omnidirectional
photograph and one of the ones that was
particularly successful in early days is
something that was originally used for
purposes of environment mapping which is
to take a photograph of a mirrored ball
which as it turns out gives you a view
of the entire scene all around if you
shoot that as a series of exposures from
underexposed images that can see into
the bright areas of the scene like the
sky and like the
you know any direct light sources that
you would have to longer exposure
photographs that can see the indirect
light coming from the ground in the
trees any shadow detail if you happen to
want that then you've really
scientifically recorded the full range
of illumination and we have some
algorithms that will put that together
into an image that has pixel values that
go not 0 to 255 but 02 whatever they
need to go to a hundred thousand a
million and if you then hook these into
a good computer graphics rendering
algorithm such as at the time we use
Greg Ward's radiant system we could take
these image based lighting environments
wrap them around a CG scene and then
illuminate those objects with that
illumination and get relatively
realistic renderings of things that
otherwise might just look like very
computer-generated objects so these are
just a very simple scene that I modeled
in emacs illuminated by a couple of
different kinds of illumination
environments that we took so this is a I
think funston beach just a little bit
north of here this is the eucalyptus
grove at Berkeley this is Grace
Cathedral in San Francisco which is one
of the prettiest illumination
environments we ever had a chance and
this is a the galleria delia feet see
which is from the one of the middle
scenes in fiat lux so these images of
these lighting environments are posted
on our website and every so often rather
frequently actually people use them to
do some of their own renderings and I
get to see things like this you know
very realistically rendered amplifier
that a fellow in France did it's
illuminated by the light of my kitchen
when i was living in berkeley because i
posted this on here and I've seen quite
a number of objects strangely rendered
into my kitchen which is a cool thing
even more fun is occasionally you see
someone who's gone out and shot their
own light probes for their own crazy
little idea and this is a series of
light probes that surfaced on the
internet a while ago shot by a young
fellow 17 year old a fellow at the time
named Nick Bert key who went around his
parents house had an inexpensive canon
powershot g2 camera and shot a high
dynamic range image series of this
mirrored sphere this is a showing that
you know this is bright enough that you
can see the indirect light coming from
the walls and the floor the light from
the windows is totally blown out so you
wouldn't be able to
accurately light objects using that as
your record of illumination but he shot
it with these bracketed exposures and as
you get to shorter and shorter exposures
you can actually finally see correctly
within range of the sensor what the
illumination was you know the blue light
from the sky and some of the other less
blue light that's bouncing off of the
concrete outside and what he wanted to
do with this he had a little idea he's a
fan of the game half-life 2 and he knew
how to hack the game so you can output
the character models and then load them
up into your own modeling software and
he had the idea that he'd make some of
these half-life two characters basically
come and visit him at his parents house
and he had quite a bit of luck with that
so here these characters are added into
some background plate photography of the
scene and they're lit by the light that
was actually there at the time so with
relatively little effort he could make
it look like these guys were kind of
hanging out and spending the day with in
there is getting kind of comfortable in
the couch air and then maybe they stayed
at state a while watch some TV later and
here is and the exciting thing about
this is that you know you can see kind
of like a little bit of the light from
the TV on this coffee cup which I think
was actually there and then that's
completely consistent with the light
that you see on the character and also
all the indirect illumination and the
soft shadowing that you get from the
characters is basically consistent so
what he's able to do is take this crazy
idea he had in his head and communicate
it visually in a way that the first
thing that you see in the image is not
whether it looks real or not but it's
the idea that he was trying to
communicate and that's really you know
the most exciting thing I see when some
of these technologies get used for
creative purposes and and this technique
has also been used quite a bit for
feature film production the Academy
award-winning visual effects film The
Golden Compass done by Mike Fink and his
team this year used extensive image
based lighting techniques to render
folks like the digital animals and the
polar bears and such and with their
great artistry they got some amazing
results with that as well so one of the
things that you need to create really
compelling computer graphics as you need
to be able to render people and there's
been a lot of exciting work over the
years recently in the area of digital
people and some of our work is
actually played into some of that as
well the first projects we got
interested in in rendering people was
actually the last project I did at UC
Berkeley in 2000 before I went down to
the Institute for Creative Technologies
and we built this device called light
stage one which had the goal of
basically taking a data set of how a
person's face looks lit from light
coming from every direction that light
can come from and with this set of you
know plastic pipes and wood that we got
at Home Depot over the course of about
ten visits as we figured out how to
build this thing we could rotate this
light in a spiral took about a minute to
go from top to bottom but we would get a
data set that would show the person's
face lit from all of these different
illumination directions we just record
it live to a mini DV camera and then
pull the footage off of that we see the
face lit from the front the sides above
and below and even from behind as it
turns out lighting objects from close to
behind is also really important because
you get these kind of rim lighting
effects that cinematographers like to
exploit when they're lighting they're
real characters and the idea was let's
take this face and try to light it with
one of our light probe images that we
got like the Grace Cathedral light probe
so what would it take to illuminate this
face with this illumination environment
well as it turns out there's actually a
very straightforward way to do it which
is that if you take this image here it's
an omnidirectional image you can
resample it to a different coordinate
mapping here this year's like a latitude
longitude mapping and essentially if you
multiply this data set by this data set
there in the same coordinate space there
sampled the same way now you actually
end up lighting the face with that
lighting environment one piece of the
environment at a time so these images of
the face here are now bright and yellow
because there was bright yellow light
coming from the corresponding directions
in the environment these images here are
bright and kind of a cool color because
there's bright cool illumination coming
from the stained glass windows above and
all these other images of the face here
are sort of dim Browns and yellows and
purples because those are the different
colors of the indirect illumination
bouncing up from the floor and coming in
the walls onto this fellow here so now
that we've illuminated his face by that
environment one little piece at a time
we can just take advantage of the
linearity of light and the superposition
principle and simulate him lit by the
entire environment at the same time just
by adding all of those images together
and the result is that you get an image
of his face lit by that lighting
environment without him ever having to
go over to grace cathedral and actually
get lit by the light and it gets nice
things like you know the yellow kind of
rim lighting effect here from the light
bouncing off the altar you can see the
stained glass windows reflecting in his
forehead and in his hair you get all the
right effects of how light hits skin and
is able to and is able to have you know
a specular component a diffuse component
subsurface scattering self shadowing
into reflections and that's just because
it's all there in the original data that
you that you captured so let's see here
so we're going to go for a couple of
slides here that we're not going to be
able to put on the webcast but I wanted
to talk about a chance that we had where
we got to work with Sony Pictures
Imageworks to apply this kind of
technique to some of their digital stunt
double characters for a couple of films
that they did basically through a
collaborator on our first project named
marks agar who worked on a winter work
at Sony image work sometime after this
and then also a visual effects
supervisor Scott stalked I could see our
siggraph paper they thought this could
be a good way to get realistic skin
reflectance for some digital actors that
they wanted to shoot first of all for
the movie spider-man 2 so starting in
about late two thousand two early two
thousand three we did some tests and
then they brought over a couple of the
actors from the film this is alfred
molina who played doc ock in the movie
and we captured a data set on film of
him in one of our new light stages this
is light stage two and it had strobe
lights kind of a semicircle of strobe
lights that go around so this is
actually a long exposure photograph that
makes it look like there's a whole
sphere of light around him but it's
actually just a semicircle of lights we
capture that kind of data and then what
we did for this is we had a rough kind
of cyberware scan 3d geometry of the
face and we projected these images from
the saw
hides from the front as basically
relatable texture maps that you could
then put on the 3d geometry and then get
a relatively realistic rendering of the
face lit by any kind of illumination
environment that you want we had to be a
little bit careful to kind of separate
the specular reflection of the face from
the subsurface scattering component of
the face because when you change your
viewpoint around the specular reflection
actually needs to shift around according
to the surface normals in your viewpoint
so we actually did a color based
separation of those and then
resynthesizer specular component
according to the new viewpoints so they
would shift around on the digital
character it wouldn't just seem
plastered onto the face but it was
relatively successful for them they were
able to use it in about 40 shots of the
film for a totally digital doc ock
character for all of the skin they
augmented it with traditional computer
graphics with some nice cloth
simulations for the rest of the body
they added you know digital sunglasses
so they had to figure out how to make
the light interact with a
computer-generated object that's close
to one of these image-based objects and
they had a full screen digital close up
for his death scene when he's floating
back in the water they thought would be
a little dangerous to film for real and
we also ought tobey maguire for a couple
scenes where he has to stop a train and
has his mask off there more recently we
got to work with this fellow here who
played the new Superman in the Superman
movie that came out in 2006 and with
this they scanned the film at higher
resolution they had the pipeline more
refined and they were able to get some
even better results so when he needs to
throw the space shuttle back into space
that's a digital Superman looking at the
scene there there's a scene toward the
end of the movie where things have
worked out reasonably well and he's
pretty satisfied so he has kind of a
nice satisfied expression flying around
metropolis in this case here they
actually started with a neutral scan of
his face and then animated it using
their animation system to kind of put a
little bit of a pleasant expression on
there that works pretty well if you
needed to really act and talk and go
into extreme expressions that's not
going to work as well because it's too
far away from a neutral pose to look
quite as realistic so some of the work
we're doing now is actually looking at
trying to capture these datasets live of
people in different expressions and
positions there's also one shot where
they came up really really close to him
he's
out in space he's thinking really hard
we have to dramatize this and so the
camera flies in really close to he's
probably thinking right about over in
this area here and they because it's an
image based data set they can you know
design the lighting to have you know a
little bit of a rim light here and a
little bit of warm light coming from
below i'm not sure where that is from
outer space but it's somewhere at this
point and you can see they really can
get a lot of skin detail and a lot of
nice skin reflectance effects that would
otherwise be kind of difficult so we've
had a chance to continue working on this
on the research side and one of the
devices here which we can totally put on
our web presentation is light stage 6
which was a idea to try to extend the
light stage idea from just capturing
faces to capturing the whole human body
and one of the reasons this actually
happened is that my Institute had some
extra space in a satellite facility that
they had to find something to do with
and they said hey Paul you are always
kind of talking about building that big
light stage well as it turns out this
could actually be a useful thing for us
because then we've got a good valid
project to do with the rest of the space
so at that point we had to figure out
how to turn the various talk that we had
into a realizable plan and pretty soon
we had a Maya model that looked like
this which was a somewhat daunting thing
to put together for just one research
group but we were lucky that a fellow
named Sebastian Sylvan is now at
Autodesk had in a head of a virtual
stage facility and Italy had wanted to
work with us you actually had to get
bigger projects together and make bigger
things happen so I worked with him very
closely to actually get the design here
he found all the places that could
source all the parts and within about
two months and a little bit of sore
shoulders we were able to put together
light stage six and the idea of light
stage six being an entire sphere of
lights is that we wanted to very rapidly
be able to capture these datasets we
wanted to very quickly go from being
able to light somebody from this
direction of light to this direction of
light and capture this kind of image
based relighting data set in real time
now if we have some luck here this'll
video will play and we'll see here no
I'm going to pop out of the program
and play it from here and what we've got
is one initial project where we actually
were capturing what amounts to seven
dimensional data sets of people going
through natural motions so this here is
a Bruce lawmen one of our researchers in
our laboratory we have him on a
treadmill here he normally is a
relatively serious fellow and looks like
he's concentrating here he's
concentrating a little harder than usual
because he's trying not to fall off of
this treadmill he's actually paying
attention to some grooves that we cut
underneath the treadmill belts we can
feel where he is left and right and
forward back but the idea is that if we
spun him around for about 45 seconds and
shot him with high-speed cameras under
time multiplexed illumination we would
get a data set of him under all lighting
conditions so if we see him in slow
motion we're going to slow this video
down you can see that what's really
happening is we're very rapidly going
from one lighting direction to another
sometimes it's dark to another to
another and we're interleaving all of
these data sets at frame rate of the
camera we're actually in this experiment
capturing 33 different lighting
conditions every thirtieth of a second
and over a thirtieth of a second these
are all these lighting conditions hereof
Bruce and from that kind of data we can
use that image base tree lighting
process to show them under kind of any
kind of illumination and the reason that
we have him spinning around on the
treadmill if we skip forward you can see
we actually got them from all different
angles as well is so that we could also
have virtual control of his viewpoint so
we had our high-speed camera here at
about chest level and then from a little
bit above we had a borrowed high-speed
camera courtesy of vision research and
then from floor level we had another
borrowed high-speed camera so we really
had three cameras and the idea was we'll
just have him repeat his motion 36 times
as we rotate him 10 degrees over the
course of each one of those and we'd
effectively get this three by 36 light
field of Bruce also for every frame of
his animation and for every one of these
illumination conditions now we wanted to
eventually
take Bruce out of the light stage and
then make it look like he's running
across some place that he's never been
to and complete control of viewpoint and
illumination and part of what we would
need for that is to get a alpha channel
or a mat for him so one of our lighting
conditions we turn off all the light on
Bruce and we just turn on lights on this
piece of gray paper that's behind him
and that gives us this silhouette and
that's exactly the right image that you
need if you want to composite him out of
that environment and then into another
environment the problem was in this case
here that didn't give us a good map for
his feet and we couldn't really think of
a way to take this you know treadmill
that we'd found at the local sports
chalet and get it to you know glow
brightly for a 30th or for a thousandth
of a second 30 times per second and so
what we used for that is we actually
covered the entire turntable and the
belt of the treadmill with
retro-reflective cloth and then put some
ring lights around the camera that were
also time multiplexed in so when we're
shooting the mat frames that actually
glows back toward the the camera as well
and then we get a good map for the
entire body at that point so going back
to the relighting idea here's Bruce
walking forward in the stage but we can
relight it to show him lit from you know
any direction that we want or we can
play that image base tree lighting trick
and show them under the light of grace
cathedral or the feet see gallery and if
we want to change the view point then
essentially what we're going to do is
morph between the different viewpoints
that we have we were an optical flow
between adjacent viewpoints and then we
actually combined the idea of view
interpolation which is one of the
inventions that Lance Williams made at
Apple back in 1993 a very important
paper with another very important idea
which is the light-filled concept which
was developed at Stanford in 1996 and
also some researchers at Microsoft
Research contemporary Gnaeus Lee and if
we basically combine the idea of view
interpolation with light fields such
that the light field quadra linear
interpolation coefficients are also used
to change the displacement vectors that
we get between as we more from one view
to another then we can actually put both
of these things together and then
smoothly from a relatively sparsely
sampled light field actually generate
views that are further
way than we originally captured closer
in than we originally captured and then
any direction all the way around so this
is actually real-time rendering on an
nvidia card a demo done by charles felix
Chabert in our group kind of pushing
into the scene and then doing a slow
rotation around it so finally onto our
problem here is a location that we
thought it would be cool to watch Bruce
running through I shot it as a high
dynamic range omnidirectional image this
time actually using a Canon still camera
and fisheye lenses in a couple of
different directions put that back
together into this high dynamic range
lighting environment then we're going to
drop Bruce into this scene and we'll see
the result that's one of the first
results we got with the technique and
here is so what we did is I animated
kind of a camera pan across the scene
we're matching the viewpoint on Bruce as
the camera pans across we've also
illuminated him with the light from that
environment so hopefully it looks like
you know the color balance and the light
directions are about correct we have
simultaneous questions from Ken and
Lance here let's see if it's the same
question what do we have shadow is very
good and Lance what motion blur ok
different questions both very good let's
see here on shadows I'll show you in a
second what we did with that for motion
blur we did not add motion blur to the
scene in some of our earlier facial time
multiplexed illumination we actually did
use the optical flow vectors to recent
the size the appropriate 180-degree
shutter motion blur we just didn't do
that before this project because we were
doing we're using an nvidia card to do
the rendering and we're trying to make
it more real time for that but we did
get some very nice results in some of
the the facial work that we've got the
shadows if we go a little bit further he
is actually casting a soft shadow he
also has a friend over here just to
prove that this is all virtual and the
shadows here they're not terribly high
resolution shadows but we did is we
actually use the silhouettes that we got
from all around to carve out a basic
volume of him which otherwise is the
first use of any notion of his geometry
that we have in any of these renderings
we get a basic kind of voxel model of
him going along and then we use that to
cast rays from all of our basis
fighting directions to figure out a
shadow map that you get from each basis
illumination condition and then we
essentially do an image-based relighting
combination of those shadow maps to
figure out how much light would be
blocked in one direction verse the other
so you actually get kind of a warm
colored shadow when you're blocking the
skylight and you get a cool colored
shadow when you're blocking the indirect
light from the warm colored building
that's behind us and thinking that you
can't have too much of a good thing we
put a couple of bruises together here
they're not actually into reflecting the
light of each other or self shadowing
each other that's kind of future work at
this point but this was this was enough
to at least a muse Bruce quite a bit and
he actually did all the work on the
compositing and the the mat finding
there this is a reverse time lapse of
building our light stage over course of
four days there we go alright so let's
go back to some slides here and what I
want to talk about is a more recent
project that we've done that's face face
related that was inspired a little bit
by a completely different kind of facial
rendering pipeline that has also shown a
lot of promise which is the idea of if
you're going to try to do a digital
model of an actor maybe the first thing
you should do is take a life cast of the
actors face in plaster and then get that
scan since now it doesn't move and it's
diffused it's a good surface to digitize
you can do that at very very high
resolution this is something that's
commonly done and there's a company XYZ
RGB that does this really really
amazingly well there was a digital face
rendering project that Lance Williams
was involved in that was a test at
Disney I think in like two thousand or
so you're working on this when they
actually did what the first time that
this really high-resolution face casting
process was applied to creating a
digital actor and they got some amazing
results with that the standard problem
here is that it's pretty much not good
for getting like you know a live
performance of an actor since it
requires taking the cast and such as a
bit of inconvenience involved some
people say that it kind of changes the
shape of the face a little bit and the
other problem is you don't easily get
aligned text
maps for the face so you might ask why
not just you know scan the face itself
it really high-resolution maybe you can
do a really fast laser scanner for that
and one of the problems associated with
that is the fact that skin is not as
nice a surface to scan as gray plaster
the problem is of course subsurface
scattering and so if you have a little
laser line on a piece of paper it might
give you a nice sharp line but once it
actually hits skin it's going to diffuse
out and get blurry so if you're trying
to measure the geometry of the
fine-scale skin wrinkling and such based
off of that blurry line you're going to
have some trouble now as it turns out
there actually is some light that
reflects off of skin that does not get
affected by subsurface scattering and
that is the specular reflection of the
skin and this is an image that we found
this is actress hilary swank it's not a
tractive leal it Image because it's
flash right from the front but it
demonstrates this point where if you can
see in her specular reflection of the
light that's where you can actually see
the skin detail of the shape of the
pores and the fine wrinkles and
assumably on her forehead next to where
the specular reflection is assumably she
has a similar kind of skin texture right
there but you don't see it at all that's
because that is the subsurface
scattering and it blurs it all out it's
in the specular reflection that you see
this and in fact it's because you see it
in specular reflections that it's
actually important for rendering digital
characters if we didn't have any
specular reflection you'd never see this
effect you could probably get away
without modeling it but if you want to
try to get that realistic skin look in
the specularity is you need to get that
kind of geometry so our idea was maybe
there's a way that we can photograph
just the specular reflection of
somebody's face and then figure out the
detailed shape of the face just from
that and we thought back to some of the
work we did for our first light stage
paper where he had done a little
experiment using cross polarization to
remove specular reflections for
someone's face this is a holly kim who
is one of our undergrad students working
with us at the time and she's lit by a
single light and photographed by a
camera right in front of her you can see
that we've got both a specular
reflection and the subsurface reflection
of the face here as it turns out if you
put linear polarizers on both the camera
both the light source in the camera and
if there are opposite angles the
specular reflection maintains the
polarization of the light and so it
can't make it through that second
polarizer and it doesn't show up in the
photograph so this is a cross-polarized
image of Holly without any specular
reflection the subsurface light since it
actually gets underneath the surface
scatters around a couple times it gets
depolarized and about half of it will
make it through that second polarizer so
the result is that we can actually
observe only the subsurface scattering
on its own and if you radiometric lee
calibrate your camera's which is
something that fortunately we knew how
to do at the time if you take the
difference between the diffuse only or
the subsurface only image and the
specular image you can get an image of
only the specular reflection just on its
own from just two photographs so our
thought was well let's try to take some
photos of just the specular light on the
face and try to figure out what the
shape of the face is from that the
problem with doing with just a single
light is that you only see specular
reflections for certain areas of the
face I in this case here you know on one
cheek but not the other cheek and what
we really want to know is the specular
illumination coming from the entire face
at the same time so as it turns out we
have devices in our lab that can
illuminate a face from all the
directions that leg can come from at the
same time this is our light stage 5
device which was just for faces
otherwise similar in a lot of ways to
light stage 6 and we asked ourselves
could we cross polarize out the entire
sphere of illumination at the same time
and as it turns out first empirically
and then actually figuring this out
there is a specific pattern of linear
polarizers you can put on every single
light of the stage such that the
specular reflection from every possible
surface normal will end up with the same
orientation of polarization by the time
it gets to a camera in the front and it
looks like this basically there's kind
of a bit of a whirl around the Brewster
angle here and otherwise they're
vertical here horizontal here the it
takes us about an hour or so to get all
of these oriented correctly but the
result is then that we can actually
light somebody from every direction of
light at the same time
and observe them without any specular
reflection whatsoever so here I think
the video projectors brightening this up
a little a little bit extra here but
this is an image of Tom he's the
producer in our group lit from the
entire sphere of light with no specular
reflection whatsoever so if you are for
example making a digital character model
this could be a very useful image to
start with as your diffuse texture map
because it has a very little effects of
you know specularity or variable shading
which is usually a challenge when
creating characters now this is with it
cross-polarized out if we rotate it the
other way and have parallel polarisers
then we can actually bring the specular
reflection back in and here it's
definitely too bright you can see the
specular light comes in but if we take
the difference hopefully this will show
up pretty well we can get an image of
just the specular reflection of the face
from the entire sphere of illumination
at the same time this looks like a black
and white image it's actually was shot
in color and since the specular light
hasn't had a chance to interact with you
know your melanin or your hemoglobin it
doesn't pick up any skin color so this
does correctly look like it has
basically no chromaticity to it and as
you can see we're actually picking up a
lot of the detail of the skin shape and
shading from this specular only channel
if you're again creating a digital
character and you need to have your
specular intensity map this could be a
very good image to use as a start for
that as well so the last part of the
project that we had was the idea of
let's try to do a variant of photometric
stereo this is a computer vision
technique where if you light an object
from different directions analyzing the
diffuse reflection you can figure out
what the surface normal is because
there's going to be only one surface
normal that would explain the different
colors that those different light
directions would produce to the camera
and what we came up with in the specular
case you actually have to use full
spherical positions because the specular
lobe is narrow enough that you might
miss it entirely if you use point
sources but we came up with a technique
that uses for spherical gradient
patterns full sphere a gradient of light
from top to bottom a gradient of light
from front to back a gradient of light
from left to right that essentially if
we just shoot these four images and of
course we really also have to shoot the
images here of the corresponding
patterns on diffuse illumination in
order to compute these images but from
just these images in some very simple
math essentially you take this image /
this image and you scale it so it's
between minus 1 and 1 it reads out the
reflection vector of every one of the
pixels on the face based on just the
specular reflection and then if you just
tilt that half way back toward the
camera then you have an estimate of a
surface normal so the result is that you
can you can also do this of course with
just the diffuse channel with these
patterns here and figure out where the
diffuse lights coming from if you do it
with just the diffuse channel you can
get a normal map that will shade an
image that looks something like this you
see a little bit of detail where you've
got like you know whiskers and such that
darken the dark and the image but you
don't see nearly the kind of detail that
you see from the specular map so this is
actually shaded with a normal map that's
gotten from just the specular component
and you can see it actually picks up all
of these fine wrinkle details on all the
skin poor detail and the final thing
that we needed to do was to figure out a
way to actually apply this kind of map
I'm going to skip over a couple slides
here to some geometry and as it turns
out there's some techniques out there
we'd experimented with them in our group
in about two thousand one where if you
start with a low resolution face scan
that you do get from a laser scanner it
doesn't have skin poor detail in it and
then you know what the surface normal
map should be for that geometry you can
essentially emboss that surface normal
map onto the geometric model and then
put that kind of skin pore and find
wrinkle detail on to your 3d geometry
the nice thing about is that we not only
can get this high res geometry but we
have perfectly aligned texture maps for
the diffuse component the specular
intensity component and such and we can
map those onto the face as well another
thing that we realized and again I
apologize it's a little blown out on the
video projector here but this is
actually a real-time rendering just
using our diffuse map and our specular
maps since we get surface normal maps
for both the specular component and the
diffuse component we can actually render
those two components of the face with
their corresponding surface normal maps
and the diffuse components normal map is
actually going to have less surface
normal variation than the specular
component because of the fact that the
light is scattered and effectively blurs
where the light is reflecting from and
it has less to do with the surface shape
of the skin then what's going on
underneath the skin so as it turns out
if you render with these hybrid normal
maps and you have the smoother normal
map for the diffuse and a sharper normal
map for the specular it actually gives
you a first order approximation to the
correct subsurface scattering behavior
of what the skin is doing and in a just
a local shading model it won't get you
light bleeding in the shadow regions or
the ears glowing when they're lit from
behind but for you know the convex areas
of the face it gives you a very close
approximation to how it will look with
the full subsurface scattering from
full-on illumination conditions you can
also take the model and show it with the
subsurface scattering rendering as well
rendering just from the specular map and
then using a subsurface scattering such
as this is the Jensen in bueller 2002
technique to get very nice renderings as
well one of our datasets we got
interested in trying to get the data of
a person's hand so this is a Hideki
llamadas hand that he put up in our
light stage you can see we got the
details of sort of the you know pretty
fine skin wrinkles and such if we render
it with the hybrid normal maps we can
get a pretty nice running there with a
specular component in this case here the
subsurface scattering rendering was
particularly compelling because it gets
that skin color bleed into the shadows
here and into the shadow here and that
really also helps sell it quite a bit as
well so we've gotten excited about the
fact that with this photometric only
technique that as it turns out it
requires eight photographs for the
spherical illumination conditions and
then just five photographs for doing a
structured light scan at the end in the
small number of photographs we can get
very high resolution geometry and
registered calibrated texture maps for
diffuse and specular in these normal
maps that it seems like a good way to
capture faces and we've started capture
faces in different expressions which we
think we can use to drive digital actor
models and we're also looking at taking
a variant of this and running it in real
time shooting it with even not all that
high speed
ography we think we can shoot this kind
of data set or close to it at frame rate
and capture it for actors performances
so that's some of the directions that
the work is continuing so I think I have
five more minutes and I have one more
thing that I could talk about which is
on a little bit of a different topic but
if we can play the video I'll try to tie
it into what we've been talking about
before this is a project we did in
collaboration with Mark bolas who's at
the USC School of Cinematic Arts and Ian
McDowell from fake space labs which is
near here hadashi Yamada from Sony you
saw his hand just a while ago on Andruw
Jones is a lead author from our research
group and it was a idea to use some
high-speed video projection techniques
that we've been using for doing
real-time structured light scanning of
faces and trying to adapt it into
becoming a 3d display and the kind of 3d
display we showed this at siggraph this
last summer the idea was let's make a 3d
display that doesn't necessarily have a
terribly large image in fact it's a
pretty small image it's about five
inches tall right here people are
peering into it but one that you can see
from any direction all the way around
and does not require 3d glasses so you
can see here this is actually a lot of
our friends from the the software
Department a digital domain who came for
a visit I think we have about 11 people
around the display here and they're all
getting their own individual 3d view of
the scene from all these different
angles and the basic way that this works
is that we have a video projector on top
that projects imagery down onto a
spinning mirror that's at 45 degrees and
the mirror is spinning around the y-axis
so that it kicks the light the image
from the video projector out to all
different directions around it
relatively quickly we spin the mirror
about 15 to 20 frames per second so it
is a little bit strobing that's fast
enough that you can enjoy the 3d the
next versions will be 30 to 40 frames
per second and the video projector is
projecting onto the mirror fast enough
so that in this 15th of a second that it
takes to do a rotation we actually get
an individual image for every degree in
a quarter all the way around the circle
so that's 288 images 15 times per second
you'll find out from that
we actually have to project imagery on
to this mirror at four to five thousand
frames per second we have a question
this actually started its life as a two
thousand lumen op Toma dlp projector and
it got seriously hot wired in order to
make it play these this imagery as
quickly as it does in the technique that
we use for that actually takes advantage
of DVI we decided you know for these
first versions let's not worry about
color we took the color filter wheel out
of the projector let's not worry about
grey levels for the moment let's just go
for binary images show some nice
wireframes and stuff and what we are
doing is we're actually rendering
imagery out of an nvidia graphics card
that's encoded like this normally you
send 24-bit color images over your DVI
what we're doing is we're sending 24 one
bit images with different views of the
scene so this is our model here these
are 24 different views each a degree in
a quarter spaced around and they're all
packed into one 24 bit color image so we
actually send these to the projector we
render them just by setting the bit the
bit pattern to the projector and then we
the projector automatically plays each
image as a 24 frame movie if we set the
refresh rate of the card up to about 180
hurts or even higher than that it
doesn't really start to flake out until
about 240 hertz but then we can actually
get these four to five thousand frame
per second movies and the digital
micromirror device is the DMD TI chip
mirrors have no problem with any of this
when they're showing color they're
usually going around nine thousand flips
per second or more so I have a video
that shows basically how this works here
this is the mirror before it spins up
the mirror has an anisotropic diffuser
on it so when the light hits it it gets
spread out vertically in a little bit
horizontally but this makes it so you
can see it even if you're a little above
or below we don't get vertical parallax
naturally with this device it's a
horizontal parallax only display but it
gives us a chance to get views out to
everyone and this actually does mean
it's a little bit more of a complicated
story figuring out how to project
imagery onto the display we actually are
running a
system vertex shader to render somewhat
multiple center of projection images out
there so that when this anisotropic
defuser kind of rebuild it out into
space you end up seeing correct
perspective and that's you know sort of
what section 3 of the paper is all about
but if we actually get this spinning up
in concert with the video projector here
it goes we get a three-dimensional seen
on there and this is just me shooting
handheld walking around now one of the
nice qualities of this is that since
we're sending completely independent
images out in all directions we have no
problem getting occlusion in this
display some other kinds of volumetric
displays have a nice three-dimensional
image but it's kind of this ghostly all
light is lit up spaces lit up and you
can see things through other things here
when we're looking at the back of the
head we don't see the face anymore
because you know what you see from the
other side really doesn't have anything
to do with what you're seeing from the
back side we're just rendering these out
of the graphics card and the other cool
thing is that since graphics cards are
so fast this is about a two or three
thousand polygon model we can actually
render that at five thousand frames per
second on the graphics card natively so
that can let us make this an interactive
display this is andrew jones with a Paul
Hymas device actually interactively
moving the model around because it's
live off the nvidia card we did one
experiment where we use the tracker to
actually track the just the vertical
position of where the camera was and
then had the nvidia card interactively
adjust the vertical view points so we
could sort of simulate vertical parallax
so this is with a tract camera and we
can make it so you see it from above
when you actually go above there as you
can see so far this is black and white
we made one slightly desperate attempt
at color which was this tent mirror
where we actually had two faces there
one that we kind of split the spectrum
down the middle and we had one that was
sort of orange-ish and one that was kind
of bluish and the idea is that smear
turns around we can do a
thing uh-oh we have a connection
successful hi there I guess I might have
called myself here and there we go and
we're right back to the video thank you
so we spun this thing around and by
doing two channels of light we got at
least a two channel color that we had
now the right way to do this is really
to use a three-chip DLP and just put you
know red green and blue light down onto
the thing we're talking to texas
instruments and they seem kind of
interested in our project so maybe this
year we'll have something like that the
other cool thing was that when we had
the tent mirror for the stuff that was
whitish it actually gave us two displays
of the image for every rotation so this
became a much more stable image going at
30 to 40 frames per second two more
examples one is which we got interested
in trying to show not just wireframe
imagery but maybe photographically
acquired imagery so going back to this
light-filled concept we shot a light
field of this tourist souvenir one of
our folks brought back and we did it
using a victor of Strama coves dithering
algorithm and loaded up the entire
nvidia graphics card memory with
different views of this and we're
actually doing live rebidding of this
image here according to vertical
tracking and then we're putting the
light field back out into space so this
is the real object sitting next to the
virtual version of that and that started
looking pretty cool and we realized that
the fact that we've only got black and
white pixels isn't so bad because the
little bit of blur that you get from the
motion blur and the diffuser kind of
starts to make it look like pretty good
grey levels and we like that so much we
thought what we really need to try to do
is do something that actually like you
know kind of is animated and moves and
we wondered is there some kind of
photographic data set that we can shoot
from all directions of something that's
moving and we realized that actually we
had that kind of data set so calling
Bruce back into service here we got our
light stage 6 data set of Bruce re bend
it onto the display and then we're able
to sort of do it thank you
so not not Princess Leia yet but maybe
we're getting there very cool all right
well that's all I brought today you
thank everyone here there's some
websites with all the videos thank you
very much and if there's time for any
questions I think they're going to do
another talk in here right away but I'm
happy to answer questions as long as
there's time yes you know okay so one of
the members of our computer animation
festival jury was Randal Kleiser who i'm
sure you know eats a film director and
when he showed it to it we showed it to
the whole jury when they were choosing
films he said have you ever thought of
doing you know Princess Leia on this he
says I know Carrie Fisher I mean I'm
sure she'd be into it and at that point
I thought a that would be incredibly
cool be I'm so sure she would not be
into it so but we'll see maybe it'll
maybe all will happen I I hope maybe
someday we can at least demonstrate it
for her that would be cool enough okay
thank you very much
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>