<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Identifying Suspicious URLs: An Application of Large-Scale Online Learning | Coder Coacher - Coaching Coders</title><meta content="Identifying Suspicious URLs: An Application of Large-Scale Online Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Identifying Suspicious URLs: An Application of Large-Scale Online Learning</b></h2><h5 class="post__date">2010-05-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/n3iANHusfcY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Colin Whitaker I'm an
engineer on our anti-phishing team it's
my pleasure to introduce our speaker
today Justin MA
justin is a PhD candidate at UC San
Diego
he is advised by Stefan Savage I Jeff
Volker and Lawrence Lawrence all his
interests include systems and networking
with an emphasis on network security and
machine learning so with that thanks for
the introduction Colin and thanks for
you know inviting me up to Google to
give this talk so today I'm going to
talk about something that hits pretty
close to home for a lot of people in
particular this you know may be
something that you or someone you know
happens upon not necessarily on purpose
so you know they're a bunch of bad
websites out there some may be selling
spam advertised goods such as
counterfeit drugs watches and so on you
know trying to sell you counter for it
fit or illegal things you know through
spam and other nefarious channels or you
might happen upon sites they encourage
you to download postcards or you know
funny like screen savers and whatnot
which in fact are malware masquerading
as legitimate programs and furthermore
you might happen upon phishing sites
which attempt to imitate well sites that
you are familiar with but with a purpose
of stealing your credentials so in this
example down here I'm showing a picture
of the facebook login page and also an
attempt to a phishing attempt to imitate
that facebook login page so there are a
lot of bad things out there and the way
we happened upon them are by visiting
their URLs so in the course of our
normal email browsing or web surfing
will encounter links and the typical
user may have a difficult time
distinguishing between a safe URL animal
one in particular when you take a look
at these few examples
two of them are malicious and two of
them or not benign so one of them might
be pretty easy for a lay user to pick
out right for example the example on the
top has a lot of random jumble of
characters and numbers and such but then
the next two might be a little bit of a
harder distinction to draw from there
and if you find the last one suspicious
I don't blame you but the bottom line is
that we'd like to predict what is safe
to visit without committing to risky
actions so to that end the problem in a
nutshell is that we want to use the
features related to URLs to identify
malicious websites so we're gonna do
this without context and we're that is
regardless of the context in which the
URL appears so it could appear in emails
web searches with embedded within pages
and so on and we'd like to do this
without using the content of the page
now there are different classes of
malicious sites we're just going to
group them all together and treat this
as a binary classification problem that
is distinguishing benign from malicious
sites so when we're faced with a bad
site we would like to give them or
universal stamp of disapproval and the
canonical application I'd like you to
have in mind is that of a reputation
service just as an example so you know a
person visits a URL some it's a URL to
the service which in turn gives you a
rating a prediction that yes I think
this is gonna be safe to visit or unsafe
to visit now how would you do this at
first glance well at a first cut you
might be constructing a black lists say
we compiled a list of known malicious
domain names and you know kind of kept a
little database of them or we might pick
little aspects of the URL things that
we've asked you know attributes that we
know just due to our domain expertise
that might work and train a simple
machine learning based classifier on
them and as it turns out this is the
state of the practice there's various
black listing services out there and
there's
on kind of learning on hand-tuned
features but this comes with some
limitations especially with black lists
you can't learn from the latest the
newest examples and with learning on
hand toon features he can't take it
advantage of the newest features for
classification and in this arms race you
know between spammers fishers and other
because people who construct malicious
websites a fast feedback cycle is
critical
being able for the defenders to be able
to catch up quickly is important and so
to that end we ask is there a more
automated approach to solving this
problem now with that the contributions
of this work we build a URL
classification system that is used by
large webmail providers and in
particular we kind of demonstrate that
you know large scale machine learning
can be practical for this computer
security problem where we our particular
emphasis are using large feature sets
and online learning and I want to
acknowledge the work by the
anti-phishing team who have approached
the problem in this in this in a similar
vein and for making the web safer for a
lot of us so I've just gave me an
overview of the problem and today I'll
talk about the first segment moving
beyond blacklist which presents a
preliminary study of the features that
we use and then I'll talk about
large-scale online learning where we
want to scale up our approach to
millions of examples and features so in
this first segment we ask well do we
have a feature set for classification
that works now for now we're gonna focus
on batch algorithms just because this is
more of a feasibility study than
anything else and we'll work with
datasets that have on the order of 10 to
the 4 examples and features I'll provide
an overview of the system that we use
for classification and then I'll talk
about the features like the various
different features that we considered
before settling on a particular feature
set as well as experimental results
so this is an overview of our system has
three components the first we have to
have training data labeled examples of
no known malicious known bad urls but
then these are just use themselves they
don't provide us any more information
beyond that little text string so we
actually construct a system that
collects Geographic Whois DNS and other
sorts of information based on that in
order to convert that into a canonical
mathematical form which we use for
classification
you know vector notation and so for
those who want who are interested in
seeing kind of concrete machine learning
notation related to these we've got
labeled examples we construct a feature
vector X related to them and then we
want to construct a hypothesis function
that gives us a rating that I think this
is likely to be good I think this is
likely to be bad so I'll quickly go over
you know our sources of label data and
what we use for classification for this
segment of the talk so in this segment
we had these data sets we have two
malicious sources and two benign sources
which we mix and match to construct for
different data sets so the malicious
sources ones the fishing's source the
other is kind of a generic spam sink
which contains links and the benign urls
we draw from the Yahoo web directory as
well as the dmoz open web directory and
when we combine these sets together they
have on the order of 30,000 to 55,000
features as for the algorithms I'm going
to show you results that we had for
logistic regression with Elin
regularization we had similar accuracies
when we used support vector machines
both linear and nonlinear kernels all of
which performs better than naive face
so with that I'll talk about the
different features that we considered
for the AR system now to give you an
overview of feature vector construction
we're given a URL and we'll identify
maybe some lexical features related to
it and then we'll look up information
based on
host itself now what happens from here
is that we convert these sort of values
into real valued features as well as a
host of binary host base and lexical
features but when we're considering
features you know we could start with
something very simple or we could move
on to something that's a little more
comprehensive a little more complex and
so I'm gonna present kind of the five
fundamental feature sets that we
considered in increasing order of
complexity so the first are blacklist
queries so you know we construct a big
vector where for each service that we
require eing we get a yes/no answer or
whether it's a blacklist or not so I
want to emphasize though that this is
not comprehensive as I'll show later on
we actually use these as a subset of our
whole feature set but they don't provide
the entire answer the next set of
features we consider were those that
were kind of manually selected for
previous studies that use machine
learning so in particular they include
whether there's an IP address in the
host name how many dots are there in the
URL right because very long host names
look kind of suspicious to us and we
also look at the Whois registration date
because things that were registered more
recently you have reason to believe are
a little more suspect now to that end we
wanted to consider who is features by
themselves as kind of a fundamental
feature set but not only their dates of
registration update and expiration but
also the registrant that is who the site
was registered to and also their
registrar that is you know who manages
the domain name registration so in this
example you might you might have a set
of domain names that were registered
yesterday by one particular company by a
very rather suspicious company and we
want to be able to use that as a feature
for classification
and then we consider host-based features
which take blacklists and who is as a
subset they incorporate those but
introduce other information as well in
particular features related to the
location of the host on the Internet and
I'll give you a particular example so
Macola was an ISP that used to exist
back in late 2008 and they were
responsible they hosted the command and
control infrastructure for BOTS that
were responsible for sending 40% of
worldwide spam at the time so after
their take down like the volume of spam
dropped dramatically as shown by this
graph that was generated by spam cop
right around November the spam volume
was plummeted dramatically and that
tells us that you know there are there
are bad ISPs out there and so to that
end the features that we use include IP
address related features you know which
autonomous systems which IP prefixes do
these sites appear in features related
to the domain name system the DNS
features as well as geographic features
you know where is it located exactly and
so on to that end we'd like to be able
to use this location information as a
feature to help us distinguish good from
bad sites and finally the last
fundamental set of features that we
consider our lexical features that is
features that help us describe the fact
that malicious urls will look different
from benign URLs so to that end we look
at tokens in the hostname as well as the
path we distinguish between hostname and
path as well as length of the URL and
the number of the dots and so to that
end we want to ask well what does how do
they do so what you're seeing here is a
graph the histogram on the y-axis shows
the different feature sets that we
considered and the x-axis is the error
rate that is the total number of
mistakes made over the over the data set
and the four different colored bars
represent the four different data sets
that I mentioned earlier that is the
result of mixing and matching each of
the two benign sets with each of the two
malicious
that's now off to the right you'll see
the number of features that were present
in the Yahoo fish tank combination
represented by the cyan bars but the
number of features in the other
combinations data sets were similar
now which turn noticing here though is
that the more features that were hat
that we used the better classification
that we're getting all right yeah good
that's correct
oh yeah the question was um is that the
some of the false positives and false
negatives there and that's correct yeah
that's what the error rate represents
but how Faison looks yeah Yeah right
registrants and registrar's we looked at
the it's sort of a bag of words over the
name the tokens and the names of the rip
stores and registrants yeah yeah I was
just curious about there be so many more
lexical features than Whois features
since I would imagine the universe of
who is registrants and addresses and so
forth to be very large yeah and actually
I'll show you some data later on in the
second segment of the talk where you'll
see kind of how the number of features
grows over time and the relative number
of host base versus lexical features so
host base and lexical features are kind
of complementary so what happens if we
combine them right then what we get are
the full feature set with for the idea
who fishtank set on the order of 30,000
features and we get the best
classification were zip rates at least
for this you know for this study which
gives us about 96 to 99 percent accuracy
now you may be wondering though
well you incorporated blacklist as
features I feel like you're getting too
much leverage from that
or you might say yourself well who is
features they're so hard to you know
such a high overhead query operation
right because who is is not structured
like DNS is structured so you know what
have we omit it black lists what have we
omitted who is will we get worse results
and we do but not by much and so this
tells us that although that are useful
black lists and Whois information are
not comprehensive and sort of further
drive the point home I'm gonna show you
a couple of ROC curves comparing black
lists to full features the full features
being the combination of host based and
lexical features now for those of you
who aren't familiar an ROC curve is sort
of a sensitivity plot telling you how
well will a classifier perform if we set
different thresholds for the decision
you know sort of different decision
thresholds so you know classifiers
producer rating a number it's higher it
tells us that it's more likely to be
malicious if it's lower tells us to be
less likely malicious but we have to
make a binary decision saying like yes
we're gonna throw it in the malicious
bin yes we're gonna throw in the benign
bin and to that end the decision
threshold is an important value to set
you know if you said it lower you're
probably gonna get more more likely to
classify things as malicious and if you
said it higher you're more likely to
classify things as benign and so what
this curve show is what happens if we
try different values of the decision
threshold what do we trace out a curve
and see what the results are for
classification so what our he's seeing
here are their ROC curves for the Yahoo
fishtank data set the x-axis shows the
false positive rate and your eyes are
not deceiving you it's going from zero
to one percent whereas the y-axis is the
true positive rate which is going from
zero to 100% so I scaled things a bit on
the x-axis and what you see here is that
for a given false positive rate say 0.1%
false positives we're getting much
better true positive rate when we're
using full features as opposed to the
black lists
so we're able to detect malicious URLs
at least in this particular segment with
only using the URL features and a
diverse feature set helps giving a
snuggie 9 percent accuracy with 30,000
plus features and I go into more detail
about the models and how things turned
out in the kdd 2009 paper but this is
only with datasets that have on the
order of 10 to the 4
you know examples and features what
happens if we scale this up what happens
if we make this more adaptive and that's
the focus of this next segment so as far
as the system itself a lot of its
similar except this time the training
feed is live we have new examples that
come in on a daily basis to that end our
infrastructure has to be able to query
features you know host based features
and so on you know in real time quick
enough so that we can get our features
or vectors in time for classification so
the main question we want to answer is
like well given that the features that
we showed in the previous segment are
promising
how do we scale it to live large-scale
data so I'll give you an overview of
live training feed and then I'll talk
about the challenges that we face in
terms of scale and non-stationarity of
the data non-stationarity being that the
features that characterize some
malicious and benign URLs they'll change
over time in the evaluations I'll talk
about the need for large and fresh
training sets and as well as making the
case for online learning so the live
training feed consists of a feed of
malicious urls about 6,000 to 7,500
coming in daily from a webmail provider
and then we supplement that by picking
out benign urls from the yahoo web
directory this gives us a total of
20,000 urls per day now in this
experiment
we collected data over the course of
four or five months 120 days of data so
this gives us more than two million
examples now the we construct features
the same as before but this time you
know we've got on the order of millions
of features and even by day 100 you know
things will continue to grow beyond that
in particular a lot of these new
features are the results of new binary
features that are being introduced they
and they're the result of enumerated
Koken lexical tokens in the URL as well
as numerating ISPs like there are their
autonomous system numbers as well as
their IP prefixes enumerated straits and
so on 2.9 million features after 100
days and in this graph that you're
seeing on the lower left it's not the
x-axis is the number of days and the
y-axis shows the cumulate the
accumulated number of features that we
encounter over time it's not as if that
these features are useless one example
I'll bring up early on just before day
10 there is a company called dual
management media that's responsible for
registering 27 different sites and so
being able to use the fact that the
registrant is dual management media will
be a useful feature for classification
and I think you had a question earlier
about the relative number of host based
and less cool features and so yeah
lexical features do at number of the
host based features but it's you get a
relative idea of the ratio right there
about 1.8 to 1.1 so millions of examples
millions of features we think that
online learning might be a promising
Avenue for addressing the problem of
training a classifier in this kind of
larger scale environment and that brings
me to kind of the practical challenges
that people face in these industrial
scale applications scale is important
millions perhaps billions of examples
are encountered and then a
non-stationary is a key thing that we
have to address as well because examples
will change
over time because there's this constant
arms race with criminals
it's things are evolving so a pivotal
decision that we have to make is whether
we want to go with batch algorithms for
learning or online algorithms so to give
you an overview of batch and online
learning
well batch algorithms typically consist
of a lot of things that we're familiar
with
support vector machines logistic
regression and so on but the key thing
is that they make multiple passes over
the data to train the classifier and so
incremental updates are not they're
pretty difficult to do in that framework
and so there's potentially and because
you're making multiple passes over the
data there's potentially high memory and
processing overhead contrast that with
online learning which consists is
consists of the family of you know
perceptron style algorithms that make a
single pass over the data seam each
example only once and can make
incremental updates that is updating
classifier one example at a time and
there was a result would incur lower
memory and processing overhead to that
end we think we believe that you know
online learning will help us address the
problems of scale and non-stationarity
in this scenario so with that I'll take
you through the evaluations that we did
over the hundred twenty day data set
making the argument making for the need
for large and fresh training data we'll
compare different online algorithms and
then we'll make the case for continuous
retraining as well as growing the
feature vector that is adding new
features over time so you'll be seeing a
series of graphs that look like this
basically what the x-axis shows the
number of days over which theorem was
conducted the y-axis shows the
cumulative error rate that is the number
of errors made so far in time so this
includes both false positives and false
negatives and here you see two curves
the black curve represents the support
vector machine with linear kernel which
we're going to treat as our canonical
batch algorithm
trained on one day of data day zero and
then fixed and used for classification
for the rest of the for the rest of the
hundred twenty days as you can see the
error rates get steadily worse over time
because it's only trained on one day of
data very early in time compare that
with the purple curve representing the
SVM that's retrained daily so for
example on day 60 the SVM retrained
daily is trained on day 59s data only
one day of data and then tests on day
60s data and so this reprocessing on a
daily basis and you can see that the
error is better what this tells us is
that fresh data helps with
classification but we're only training
on one day's worth of data what happens
if we expanded our training set well
then you get the following so the blue
curve is the kind of multi-day analog to
the black curve here we're training on
two weeks worth of data which is as much
as we could fit in the evaluate the
memory of the evaluation machine and
then after two weeks we fix the
classifier and use it for testing over
time the green curve is is the is the
sort of large data analog of the purple
curve here we're trading on a sliding
window of two weeks of data and we use
that for you know that's the process
that we use for training and
classification in that case the fact
that you can see the blue and green
curves are improved over the previous
curves tells us that having more
training more training data helps now
I'll take an aside so since this segment
is about online learning well we have to
ask well which online algorithm helps in
this case and I'll present you three
examples of online learning algorithms I
sort of represent points in the design
space ranging from simple to more
complex online updates and I'll explain
them briefly so first we take a look at
the perceptron this is a popular
algorithm because it has a convergent
a finite bound on the number of mistakes
that you can make if the data is
separable so if you can imagine that the
URLs the feature vectors are points in a
very high dimensional space then we're
trying to train sort of a plane like
yeah we're trying to train a plane that
cuts between the malicious and the
benign examples and the updates very
simple you just change the direct the
direction of the weight vector to point
you know in the direction away from
positive examples towards negative
examples and yeah it's just as simple as
that the problem is though it's very
coarse which brings us to logistic
regression with stochastic gradient
descent so I don't know if you're
familiar with logistic regression or not
but the idea behind logistic regression
is to train that decision boundary that
hyperplane so that you can maximize the
likelihood that it separates the data
and when you're doing stochastic
gradient descent though you're training
this decision boundary one example at a
time and that brings me to the update
that's in the middle right there now one
thing I'll point out it looks a lot like
the perceptron update except now it has
this little constant in front which is
adjusted based on what I'd like to call
a proportional update so you what
happens in online learning is you you
try to classify an example coming in and
then you're told well it's actually
malicious or it's actually benign and
with a proportional update based on how
severely you miss classify the example
the update will be tuned the magnitude
of the update will be tuned accordingly
so for example let's say that Y of T is
plus 1 that is it's a malicious example
and then our prediction functions which
is Sigma of the inner product of the
weight vector in the example let's say
that that's um let's let's say it's 0
like we think oh we think it's benign
but it actually turned out to be
malicious then Delta will be as maximum
possible value of 1 it'll be a full
update right there whereas if we
predicted that it was more likely to be
say 80% likely to be benign even though
we classify correctly we're still gonna
update our model anyway
but only by a little bit that's what a
proportion will update entails and then
finally the last step in this person is
confidence-weighted learning which is a
recent development came out about 2008
and the idea here is instead of having a
single weight vector that characterizes
our classifier our weight vector will
actually come from a distribution a
Gaussian distribution which has a mean
and a covariance where the covariance
represents our uncertainty about the
estimates of in divet of the individual
feature weights so it's formulated as a
constrained optimization problem I'll go
over this briefly with you where you
want to find so argmin of mu and sigma
that is you want to find new model
parameters you want to find a new
classification vector mu and a new
uncertainty matrix Sigma so that the KL
divergence between the new model and the
old model so you want to minimize the KL
divergence between the new model in the
old model so that the probability of YW
X is greater than 0 0 that probability
is greater than ADA
so what this means is that so YW X is
the classification margin this is a term
in machine learning that tells us that
well if that tells us how correct our
classification is and if it's greater
than 0 it's correct if it's been less
than 0 it's incorrect and the math works
out so that you know if you're if it's a
false positive as or if it's a false
negative correct classification still
means that the margin is greater than 0
and so we want to be able to classify
this newest example X of T correctly
with greater than probability ADA some
confidence value so we set ADA to 90% so
what this is saying is that we want to
change our model as little as possible
so that we can classify this latest
example which you might have
misclassified so we can correctly
classify it with greater than some
confidence and as it turns out you can
express this as a closed-form update
of MU of the weight vector and the
covariance and I just wanted to point
out that the MU update looks a lot like
the perceptron but this time you've got
a matrix in front of the example X and
what this does is it updates features at
different rates now because storing a
full matrix
you know million by million or even
larger is not you can't sort of in
memory we actually represent it as a
diagonal covariance matrix so that
there's an uncertainty value
characterizing each feature weight alone
and so what this will tell us is that if
a feature weight has high covariance if
it has high variance that means our
estimate of that is more uncertain and
so we're gonna update that because of
the the Sigma X multiplication down here
we're gonna update that feature more
quickly than if we had a feature weight
that had low variance that we had higher
certainty of that feature will be
updated a slower rate so that's what the
the confidence-weighted learning
algorithm does any questions so far on
this okay so I'll show you here the
results for the different online
learning algorithms so here are the
results for the perceptron it does okay
about two percent cumulative error over
the course of the experiment but then
here the results of logistic regression
with SGD well we see is that the
proportional update helps improve the
classification results but if we compare
that to confidence-weighted learning we
see then using per feature confidence
really helps and I think it's worth
mentioning I think according to the end
DSS 2010 paper my understanding is that
the Google machine learning algorithm
that you used is sort of a version of is
like a batch version of logistic
regression with stochastic gradient
descent but probably a bunch of with a
bunch of other things had it in as well
and so you can
LRS GD as like that but making only a
single pass of the data as opposed to
making multiple passes over the data
because you could benefit with multiple
passes over a single as opposed to a
single pass so let's take it back to the
batch algorithms that we evaluated
before we saw that fresh data helps we
saw that more data helps but when we
compare that to confidence-weighted
learning we see that online learning
matches batch learning actually in this
case it does better but you know to be
on the safe side we can save for sure
then with online learning you can do at
least as well as batch but why is that
so I'll compare confidence-weighted
learning with the best batch learning
curve where we had a sliding two-week
window of training data so the key
difference here though is that with the
SVM we're only rich retraining once a
day whereas with confidence-weighted
learning we're retraining after every
single example that we encounter
so that classifier is being updated you
know 20,000 times a day so what happens
if we sort of you know level the playing
field what if we had a version of CW
which we trained only once a day well
then this is what you'll get this is the
result the purple curve here you're
saying you were seeing is the result of
confidence-weighted learning where we
only update the model after the end of
each day so you see that it does worse
initially but then eventually it
overtakes patch learning and this is
kind of the fundamental trade-off that
you have to make when you're using
online versus batch learning because I
mean given the same amount of data a
bachelor bachelor nning algorithm is
going to do better because it can make
multiple passes over the data but if
you're in regime where you've got lots
lots of data where it's infeasible to
store it in memory for a batch algorithm
then you you might want to consider
online learning for your purposes even
if you're only updating once a day but
the difference between the
red and the purple curve tells us that
continuous retraining really helps
finally we ask well alright so you're
adding new features over time is it
worth the effort right and what you're
seeing here with a purple curve is this
time it's the online learning algorithm
CW but with a fixed feature vector that
is we only consider the features that we
saw up to day one in the experiment and
then we train over the course of a
hundred twenty days and this is a result
that you get it's and when you compare
it to the original CW result that we
showed it's doing worse and so a growing
growing the feature vector helps with
classification considering new features
but this is for CW which is the more
complicated online learning algorithm do
we get the same benefit when we say you
know grow features for the perceptron
versus not growing the features for the
perceptron well this is the curve
results for the perceptron with the
fixed number of features here's the
curve for the perception with a growing
number of features the improvement is
marginal right but remember that the
perceptron then and confidence-weighted
learning are fundamentally different
algorithms because confidence-weighted
learning has that per feature update
rate and what this tells us is that
growing the features and accounting for
that per feature confidence really helps
with classification so given these
results we wanted to sort of you know
build a proof-of-concept browser plugin
just to see you know how this work and
what you're seeing here let's say that
you're browsing the web so here I'm
browsing Gmail and my toolbar is telling
me well I think this this is a ninety
three point eight percent safe page to
be on and I mouse over a link in my
email that says to continue click here
to login to your authorize.net account
now for those of you who aren't familiar
authorized.net is
it's a company that helps merchants
process back-end transactions for web
Commerce so you can imagine that if you
were to compromise someone's
authorized.net account that you could
get access to you know them you know
whatever money they've accrued from
online commerce so I mouse over that
link and the plugin tells me well this
site is about 0.6 percent safe to visit
probably not a good idea
but I want to get more details and so I
mouse over that prediction bar and it
tells me that well and here's the reason
various different features had different
weights and these were the top ten
malicious and top ten benign features
that led me that were part of the the
prediction right here okay well I
decided to ignore that and I click
through anyway
so the browser lets me visit and this is
the site it kind of looks like a cheap
knockoff of this site
this is the real authorized.net you know
so I have my suspicions I don't go any
further this is on a Thursday I casually
checked it later on it turned out to be
blocked later and the idea here that is
that well
the browser had a blacklist it was tied
to a blacklist but if you had a
predictive you know if you were using a
linear classifier instead which helped
to predict whether it was positive or
negative you probably could have gotten
that warning a little earlier on of
course there are implementation issues
with respect to like employ deploying a
class classifier supposed to deploying a
blacklist but we can definitely talk
about that later so we wanted to tackle
the problem of detecting malicious URLs
it's a relevant real-world problem and
it was a successful application of
online learning in particular what
helped were having lots of fresh
training data as well as having trained
the model continuously and growing the
number of features and in particular
when we compare a confidence-weighted
online learning with the batch learning
it can be as accurate
but more adaptive and use fewer
resources and with that I'll wrap up so
as far as kind of public exposure we've
released a dataset actually I'm an
anonymized 100 120 day subset of our
data so that it can hopefully act as
like a benchmark set for people who are
interested in developing online
algorithms and in terms of industrial
impact there are male providers who have
adopted our approach for classifying
URLs in their email messages and I have
the link up there for our project page
in case you're interested and with that
I'll conclude with some final thoughts
so I work in systems and machine
learning kind of the intersection of the
two and I like working in systems
because it's kind of a source of high
impact large-scale applications you know
things that you know the everyday person
can relate to at the same time machine
learning kind of gives us methodical
approaches to tackle these problems but
when you're tackling these problems you
have to kind of acknowledge the fact
that there are these real-world
constraints that we have to you know
there's limited processing there's
limited memory and we have to deal with
that and to that end I believe that
machine learning is more than just you
know a plug-and-play solution you have
to be aware of you know the constraints
that you're dealing with and so to that
end when you consider the interplay of
systems and machine learning you know
embracing the real-world constraints and
but also embracing the possibilities
that machine learning have to offer you
can come up with kind of interesting
solutions to important problems and the
this work I I believe this work on
detecting malicious URLs is one example
of that so with that I'll take your
questions
so I I saw the - to simplify the the
implementation of your continued or
continuous weighted confidence-weighted
confidence right you know the confidence
weighted algorithm you only use the
diagonal of the the covariance matrix
how does this affect situations where
two features interact very directly like
in an XOR relationship yeah I think you
do lose a little bit I mean the main
reason that you want to use the diagonal
covariance is to save runtime overhead
in memory and you do lose something in
terms of accuracy and I actually I have
an AI stats paper that I'll be
presenting later this month which talks
about you know what happens if we try to
approximate the full covariance
structure during learning for the
further confidence-weighted algorithm
and you actually get some improvement of
course you know you get more processing
and runtime you know more processing and
memory overhead as well but it's true
that there is there are correlations to
exploit in this application it's just
that we'd have to develop methods that
do that scale ibly yeah thank you
are there any questions from the VCS
so I noticed that your system can you go
back to the example with your system
seem to have most of the way coming from
the word logon you seem to have learned
that anything with the word logon in it
is a bad oh sure which one oh yeah this
example yeah I don't know what are they
what are some other examples of features
that your system so other examples of
interesting features were well who is
registration date is actually a very
important feature just because it's it
mimics its it reflects the reality that
malicious folks are trying to
re-register sites constantly because
they get taken down I think that the the
host base features in particular the
Etana the a s numbers and the IP
prefixes were actually pretty important
because they sort of that provided a
level of granularity for class of like
separating good parts of the internet
versus bad parts the internet that was
useful for classification
so yeah who is host base stuff I think
that was that was pretty important yeah
yeah I noticed you're pointing out the
fact that log on gets a very high weight
here but I as we saw in the previous
segment where we're comparing different
feature sets I think you know if you
were to consider that there would be
enough other features that would be
weighted negatively for this</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>