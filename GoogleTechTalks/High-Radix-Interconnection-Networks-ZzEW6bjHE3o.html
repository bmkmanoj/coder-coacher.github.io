<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>High Radix Interconnection Networks | Coder Coacher - Coaching Coders</title><meta content="High Radix Interconnection Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>High Radix Interconnection Networks</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ZzEW6bjHE3o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so it's our heater from j professor bill
atari from sanford villas of willard RN
heinous and professor of engineering and
the chairman of aggressive enough
research to have you had a system
architecture network architecture I
speed signaling problem in
synchronization and most of this can be
found most large Adam us today for his
contribution to the video architecture
and interconnection networks let's
received numerous honors including
the similar pre-award and the ACN
morning Spookies board and he is
published over 170 papers and is an
alternate route xbox digital system
engineering principles and practices of
interconnection networks aside from
academia bill has also played key roles
or founded several horses charging
process early and lots of each existence
and thank you it so much going on is in
that district 9 1194 plus investigation
this talking are we to be podcast on
google videos questions and if you have
sensitive questions beautiful to the end
so without much adieu little bit thanks
I suppose for being here hopefully both
mics releif clearly one is what I'd like
to do is talk to you a little bit about
high rating center connection networks
here's sort of the outline of what we're
going to talk about today but before I
dive into that let me start with a brief
introduction since unlike argent who is
actually one of my former PhD students
many of you may not know sort of who I
am or what my research group does when
Arjun said can you come to Google and
give a talk on what your research is I
said well I could but I can't give one
talk on it which one do you want I gave
him this menu and he ordered the bottom
entree so there's there's other things
we do we do a lot of work the way I like
to describe it as my research group
works at the extremes of computer
architecture we work on the architecture
of very large supercomputers and very
demanding applications for very low
power embedded processors so for example
are efficient embedded computing project
is aimed at being able to develop
programmable hardware which is
competitive with hardwired asics for
applications like cell phone modems
Wi-Fi modems digital TV display
processors and things of that nature we
also doing a lot of work on
bioinformatics architectures where the
applications have computing requirements
that are different enough that actually
fpgas give big advances in performance
by being able to match the computation
we're doing a bunch of work on
supercomputing both of the language
level we've developed a language called
Sequoia which explicitly manages storage
in
hierarchical machine we've targeted that
both to our own merrimack streaming
supercomputer end to the IBM cell as
well as two clusters of workstations and
for interconnection networks at the
embedded side we're doing a lot of work
on on chip networks and other people are
putting multiple cores on a chip
actually a lot of thought needs to go
into the networks to tie them together
and it's a very different problem than
building system area networks what I'm
going to talk about today is our work
largely in collaboration with Cray on
interconnection networks for
supercomputers my group has been working
in their connection at works for a long
time since you know the Mars router the
showing here that I build at bell labs
in 84 I built the tourist riding trip at
Caltech and 85 a bunch of routers build
at MIT and we also built systems around
these routers our MVP chip which
actually had hit a 3d tourist router on
it went into the J machine a very close
router to the J machine router was used
in the crate e3d but reimplemented in
ECL gate arrays our map chip shown there
actually has a a tourist route or none
as well and the imagine chip has routing
on it also then these have found their
way out into a bunch of different
commercial systems in one way or another
there's a lot of distinguished alumni
from the group this is actually just the
academic ones ahead this slide around
there's a bunch in industry as well and
from the shameless Commerce division of
our group you should talk to Larry Page
and have them by every Google employee a
copy of both of these books there the
really valuable one basically sort of
says everything you need to know about
how to build networks the other one
basically says everything you need to
know about how to do the electrical
engineering of big systems so let me die
then into the real meat of this talk
that the introduction done and it's
interesting in the late 80s I wrote a
series of papers explaining to people
that they shouldn't build hypercube
networks they should build a relatively
low dimensional which also implies low
radix torus or mesh networks for their
machines and at the time that was the
right answer but you know time changes
and one of the things that changes a lot
of people don't realize this is that
network routers are on very much the
same kind of a growth
serve as processors are in terms of
Moore's law and here's one way I like to
refer to that that curve which shows I
know you said I'm supposed to use the
mouse arrow but it's hard to do that
when I'm over here is this the laser and
the goods laser works I think the
batteries are dead in any case it shows
as a function of calendar year the
bandwidth off of a router router chip
I've sort of put little boxes around the
routers that I've had a hand in
designing starting with the tourist
routing chip down here what you can see
is that you know over the decade from
sort of 1992 2000 on there were more
than two orders of magnitude increase in
the bandwidth offer router at the same
time the packets that you're sending
didn't really increase in length very
much and the the latency through the
router you know actually went down
because the the devices you're building
a lot of got faster even though you had
a few more pipeline stages so when you
have this increase in bandwidth I think
if you have a router with a certain
number of ports you get an increase in
bandwidth you can either you know put a
certain number of ports on that router
so if i take my bandwidth and I double
it i could make those ports all twice as
wide right but at some point in time
there's big diminishing returns to
basically building you know say a router
with 4-port switch would be more than
enough to build a 3d interconnect know
how it works aha which would be more
than enough to build a 3d interconnect
eventually get my ports wide enough that
I can put a packet broadside into them
it doesn't make sense to make them any
water than that in fact it doesn't make
sense even before you reach that point
because the alternative is I get more
bandwidth on my chip is I could have
more ports of the same bandwidth and
then the question is you know which is
more effective and this is a question
that can be answered quite precisely by
comparing the resulting cost and latency
of the networks that are realized with
high ratings links and low radix links
now the cost is absolutely a no-brainer
because the higher radix router you
build the fewer hops you need to deliver
a packet and the lower cost the
network's going to be because the
network cost goes almost directly with a
number of hops the latency is a little
bit more difficult question
because latency has two components to it
here's the equation for latency which of
course you can find in the book I showed
earlier which has a component due to the
number of hops times the delay through a
router plus the length of the packet
over the bandwidth of the linker
traversing we refer to this part of the
latency as hetero latency the time for
the first bit of the packet to get to
the destination and this part of latency
is serialization latency the time for
the rest of the packet to catch up and
if we sort of express these as a
function of the radix of our router k
we'll see that as we increase K the
header latency goes down we it takes
fewer hops because we're using a higher
base logarithm to decide how many hops
we need and our serialization latency
goes up because as we make higher radix
the channels get skinnier we've got to
squeeze our packet over that the overall
bandwidth per port by the way into our
machine remains unchanged because we can
apply technique called channel slicing
that if you need a certain amount of
bandwidth and you slice your channels
narrower than that you simply then start
commutator your traffic over a number of
skinny ports so this is a completely
independent of the terminal bandwidth
you want out of the network well here's
what happens to latency and I've plotted
both the bandwidth point for 2003 and
2010 technology and what you see happens
is as you increase radix you basically
reduce hetero latency a lot here and
then serialization latency goes up
although by 2010 we have enough
bandwidth that you know the hetero
latency doesn't go up until fairly far
out there so this is sort of the regime
of hetero latency this is a regime where
serialization latency is dominating in
this case the optimal radix is about 44
2003 technology for 2010 technology it's
going to be about 128 ports / router
yeah well tomorrow
these are lies
geography the
yeah actually if you look at the
detailed version of that equation the
backup throw this animation the detailed
version of this equation in the book
there's actually another term which is
the time of flight over the wires and
that term remains relatively constant
regardless of which alternative you
pursue their small differences but
they're not big ones so I basically to
make this talk fit into an hour didn't
go into that level detail but that's a
good question so back back to hear it
turns out that if you decide you want to
optimize latency you can take this
latency equation differentiate it set
that equal to zero and solve for the
value of K that gives you the optimum
latency and in fact that optimal latency
happens when this is true and we refer
to this term is the aspect ratio of the
network because in many ways it's
related to the ratio of bandwidth the
overall bandwidth you have on the router
to the length of the packet you could
think of that ratio as being the aspect
ratio you have to deal with for network
and if you plot aspect ratio on one axis
and optimal radix on the other axis
you'll basically see that it's a
monotonically increasing function and
over time you move up the aspect ratio
curve you also move up the radix curve
so the bottom line to take away from
this is that we've now moved into a
regime we're having a radix of six for
example which is the radix required to
build a three-dimensional torus network
is very suboptimal to give you a much
higher cost much higher latency than a
radix today which is probably you know
somewhere in the range of 64 2 128 which
will give you the optimal weight and
seeing and cost to build an
interconnection network so hopefully you
you buy into that let me now talk about
the problems that that creates so if you
look at sort of a typical router and we
actually in our in our textbook walk
through a typical what we call virtual
channel router and you look at the you
know anatomy of it you know packets come
in to input controllers which are
basically responsible for managing a
bunch of queues in a virtual channel
router these queues are partitioned by
virtual channels
to cause non interference between
different types of traffic that all
scales just fine right because as you
add more ports to the router the number
of input controls grows linearly and
actually turns out a lot of the
complexity of the input controller is
proportional to its bandwidth that
drives the bandwidth you need for the
memories and the amount of storage
capacity needing these memories and that
really doesn't matter whether you have
wide channels or more skinny channels
and you have the same amount of
bandwidth you need the same amount of
logic so the input controller scale just
fine you then take your packet you do
some kind of a routing computation you
decide where's my packet going and in
particular which output port of this
router do I want to use that scales
linearly as well you basically need to
that's again basically the function of
the bandwidth of the router that
determines how many routing decisions
you need per unit time and you can scale
that linearly across the input ports for
simple routing decisions we tend to
duplicate that routing logic in every
input controller the next thing that
happens doesn't scale so well it's you
have to allocate a virtual channel on
your selected output port for your
packet and it turns out that these
allocators wrote quadratically because
they have to take bids from every input
port for every output for it so both the
state that they maintain is quadratic it
goes by the number of input ports times
the number of output ports and the logic
is quadratic after you've allocated your
virtual Channel you then have to
allocate the switch this is also a
function that grows quadratically with
the number of ports and so the switch
allocators that work just fine with six
port tourist routers are infeasible to
build 128 port hi radix routers finally
you go through the switch and building a
switch is building a switch is not not a
lot to do there but we'll see in a
minute actually what we do with that
switch can make the allocation problems
that i just mentioned a lot easier a lot
harder so those are really the problem
areas the virtual channel allocator and
the switch allocator so to baseline
things we decided to start out and look
at what happens if we just build a
straight virtual channel router and
scale it to high ratings for a low radix
network this is the characteristic
latency throughput curve and and what
you see is as we offer the load to the
network we saturate at narrow just under
sixty percent capacity and this is due
to head of line
blocking in the crossbar switch this is
a very well understood phenomena as you
increase the radix the problem gets
worse because if you just work the
statistics out the probability of block
is due to this effect goes up with radix
moreover you see that we're leaving a
lot of bandwidth on the floor here and
for low radix routers a technique that
was used to deal with this was simply to
over provision the switch you say ok I
can only run my switch to about fifty
percent capacity I'll provision my
switch 2x or 3x the bandwidth of the
ports and that will be fine I won't keep
my poor title and that was a reasonable
thing to do and actually still is a
reasonable thing to do because the
expensive resource in these routers are
the links and your job is to make the
links be busy as much as possible and
not to idle an expensive link because
it's waiting on an inexpensive switch so
prove it over provision the switch have
it be idle some of the time keep the
links busy all the time so it turns out
that as you build really big switches
what you'd like to do is not do that
because the switches start consuming
more die area you don't want to have to
over provision them so much the question
is can we you know get better
performance out of this and at the same
time can we do that in a way that makes
our allocation function a lot simpler
and the simple answer to that is yes I
need to just get this to step on to the
next one here's our baseline router
where basic our baseline switch we
visitors have all the input ports in a
cross points which with all the output
ports the way to get rid of the head of
line blocking problem is to separate the
queuing up that's what we'd like to do
is put queuing it to cross points of our
switch so basically if I have a packet
coming in on an input and it loses the
arbitration for an output I have it
wasted the bandwidth of that input port
that cycle instead I can drop it in a
packet buffer and retry the allocation
of the output on the next cycle because
remember these the packets are come in
here actually have to win 22
arbitrations to get to the output they
have to compete among all the input
packets across the virtual channels on
this input port to get access to the
input of the switch and then once say
they got access to the input of the
switch set to compete with anybody else
who wants the same output to get access
to the output of the switch and it's
collisions
in that second arbitration that led the
performance to saturated somewhere
around fifty percent capacity if you
don't have any queuing if I do have
queuing every time we get an input I
make forward progress I don't I don't
even arbitrate for the switch unless my
virtual channel has a slot free in this
intermediate buffer and it decouples
those two allocations so that I can wind
up getting very close to one hundred
percent throughput and in fact here I've
added a curve to the previous graph this
was the low radix router this was the
high radix radder this is with cross
point buffering and what you see is that
you can actually run this thing out to
one hundred percent capacity and it's
giving you very much the delay of an
ideal queuing system which is what it
should do the the problem with this is
that it's prohibitively expensive to
build that the number of buffers you
need in that switch goes quadratically
with a number of ports in fact it's a
number of input ports times the number
of output ports times the number of
virtual channels times enough buffer
space to cover a round-trip delay over a
link so in fact what you see is that
you'll compared to the wire area of the
switch basically you're buffering is
sort of free up to the wire area the
switch because you can sneak these
buffers in underneath the crosspoint
wiring I'm compared to the wiry of the
switch somewhere around you know 50 or
so the buffer area dominates and out of
128 where we want to build these it
starts taking up too much area we can't
economically build a switch of that
variety so how do we fix this problem
what we do is we add a third choice to
our menu of switch organizations this
one is what we call a hierarchical
crossbar we take our large switch that
we want to build say it's 100 let's say
at 64 ports because that divides you but
we take our 64 port switch and we divide
our inputs up into groups of eight and
our outputs up into groups of eight and
for each group of inputs and group of
outputs we put a small switch down and
this switch has no queuing in it but we
put queues at the inputs and outputs of
this small switch so now we've also
decoupled our problem when an input
packet comes in it competes among its
virtual channels for competition to go
across one of these links when it wins
it's guaranteed to land in this buck
it doesn't have to compete for that link
again it then competes for this switch
and if it wins that's which it drops
into one of these output buffers and it
then competes among the other switches
in the column for access to the column
and output to the link this has actually
done two good things for us one is it's
actually dramatically reduced the amount
of buffering required and hence the cost
of building this the other thing that
it's done is it's decomposed our
allocation problem from one big 64 way
allocation problem into several smaller
8-way allocation problems by breaking
down our allocation into selecting a sub
switch and then selecting a port of the
sub switch and selecting the output we
can now build sort of 88 at a time
allocators that actually are feasible
with the techniques we know which we can
run in one cycle and not lengthen the
pipeline of our router too much so the
performance of this oops happened here I
wanted this slide why am I not getting a
graph mice my display shows a nice graph
here which you're not going to see and
I'm not going to try to debug this now
but for whatever weird Microsoft reason
you're not seeing it this basically
shows that the hierarchical network
matches on uniform traffic matches the
performance of the crosspoint buffered
network it turns out that things are not
quite so good on worst case traffic this
is a fully buffered network the network
that basically builds one big cross
points which with queues at every cross
point then these are different ways this
is for a radix 64 different different
ways of factoring it and this is the
baseline switch with no buffering now
with with uniform random traffic
basically the the packets coming in at
an input are evenly distributed over the
eight sub switches in a row say for the
sub switch 8 line here and therefore the
load is is uniformly distributed
everything works well the worst case
traffic basically concentrates all of
the traffic from a group of eight inputs
on to a single group of eight outputs
and what it does then
basically pushes each of those
individual sub switches to a point where
it winds up seeing the head of line
blocking problems that we were seeing
with the original switch the good news
is that this almost never happens the
the worst case traffic patterns a causes
are extremely unlikely in practice and
so in fact we've gone ahead and built
switches doing this and almost all the
time the performance on them is
indistinguishable from the performance
of the fully buffered switch and they're
a small fraction of the cost to build
what's what's even better you even
though I showed before that sort of the
the low radix routers you know looked
like they had better performance because
the less head of line blocking the
problem is that that's less head of line
blocking / top of the network and with
the low radix routers you have more hops
and so in fact over the network they
wind up having substantially lower
performance they both higher latency
because they take more hops their hop
count part of the latency is actually
dramatically higher here there's a lot
of civilization latency and they have
you know lower saturation throughput
because they're having more
opportunities to run into those blocking
problems so high Riddick's is a win all
around so let me not talk about topology
so when we first you know came to this
realization it was actually surprised to
me we were working the numbers and I was
fully expecting to find that tourist
networks were yet again the right answer
but just for thoroughness we popped in
you know Chlo networks as well and we're
running the numbers and the numbers
popped out and showed me that I was
wrong that the tourist networks were not
the right answer that the clone networks
worked and then we sort of torn the
tease that apart and came up with this
notion of aspect ratio and and we're
able to show that these high radix
networks are optimal our initial thought
was we would do clone network since that
was the analysis we were doing to do
this now the problem with a clone
network especially when you're building
a big network which is filling a large
machine room or even several machine
rooms is that the expense of things are
the links and in a clone network you
sort of have twice as many links as you
would think you would need because the
way to think about it is sort of in a
clone network as a butterfly folded back
in itself I probably should have put an
illustration of one of these in here you
basically traverse one set of links up
to a middle stage then you traverse
another set of links back from that
middle
age to your endpoint and so if you were
to compare it to a butterfly network
you're traversing twice as many links as
you know the theoretical minimum that
you would need to discriminate against
and end points now the reason you don't
use a butterfly is what that has a
minimum diameter one I just use a
butterfly to wire my networks up Ezra is
blocking it's a blocking network many
people try this on building parallel
computers in the 1980s BBN and
particular had a machine called the BBN
butterfly and what they found is that
there are traffic patterns have brought
the machine to its knees and the reason
is that there's no path diversity in a
butterfly network there's exactly one
route from every input every output in a
butterfly network and in fact an
adversarial traffic pattern will line up
those routes in such a way that you get
congestion on an intermediate link of
the network that brings the network down
to sort of a square root of its capacity
so if you have you know a 1024 node
network you'd be down to one
thirty-second of the capacity of the
butterfly on worst case traffic patterns
so our review and topology is basically
here's a sort of our constraints we
wanted to get you know the lowest
possible hop count and that's a hop
count for a butterfly the hop count for
close twice that we'd like to have good
performance on both benign and
adversarial traffic pattern so since we
have to handle adversarial traffic
patterns that rules out the butterfly
network they have no path diversity we
can't handle these well but the clone at
works have twice this minimum bandwidth
if we fold the clone ette works we can
exploit some locality because you don't
have to write all the way up to the
middle switch stage you only have to
ride up to a common ancestor The
Thinking Machines cm5 used a similar
thing in their in their fat tree
networks we also looked at these there's
a property of networks called kaylee
graphs that keep mathematicians employed
there horrendously difficult to route in
because they're kind of a bizarre
topology they're not easy to reason
about like a like a clue so we were
playing around with this and I was very
quickly in the context of the crane
machines I was looking at the cost of
these machines and costs were dominated
by the optical links in the machine and
I just couldn't help but think gee we
take these packets and we send them over
one optical Lane
from the cabinet that their source
processor is in to the central cabinet
that's basically full of routers then we
send it over another optical link from
that router cabinet down to the cabinet
that the destination processor is in we
could cut our you know optical link cost
in half if we just threw away one of
those optical links and connected the
source cabinet directly to the
destination cabinet and so we played
around with these ideas for a little
while of course we don't have enough
router ports to connect all the source
cabinets to all the destination cabinets
so how do you do this we came up with
this notion of a topology we call the
flattened butterfly because this is sort
of the way we derived it think of this
as a butterfly that that got rear-ended
and sort of got smooshed down upon
itself here I've drawn sort of a 16 port
radix to butterfly which you normally
think of as having four ranks of
switches and three ranks of wiring
between those switches what we're going
to do is we're going to take all four of
the switches in each of these rows we're
going to combine them in a single router
chip what this means is that all of the
connections within the row basically
half the connections out of each router
go away there now internal connections
in that router and all the connections
to other rows the red lines shown here
are the connections out of this router
to other routers that represent those
other rows now yeah you know have I you
know I already said that a butterfly is
a bad interconnection network is it has
no path diversity have I just gotten
myself back in the same boat as the
butterfly network by flattening and the
answer is no by flattening it I now have
the ability to traverse these dimensions
in any order and to diverse some
dimensions more than once so this
actually gives me complete path
diversity in fact I can emulate a clone
network on the flattened butterfly by
traversing each dimension exactly twice
once on the way up and once on the way
back down but I don't have to by
flattening it I now have the ability to
emulate a clue and get exactly the same
performance as a clue and I need that
non-blocking performance but if I have a
benign traffic pattern I can choose to
go directly to my destination or to take
a minimal path to my destination so I'll
talk about how we're out this later but
but the nice thing is now that by
flattening the butterfly I've eliminated
the
actionable characteristic of the
butterfly it's like a path diversity and
I've retained its desirable
characteristic which is the minimal the
theoretically minimal distance between
ports so how do you package this this
sort of so there's a sketch of how to
package these so there's for those four
ranks of switches in the butterfly now
one thing that we do lose here is that
our effective radix goes down because
I'm packing four ranks of switches into
one high rating so if I for example
radix 64 switch I'm not going to be able
to pax radix 64 in each stage of the
butterfly app do a quarter of that so
the first stage I'll take 16 processors
connect them into the router I'll then
take 16 of those routers and I will
connect them together with a quarter of
the lines out of the router so I
actually need only 15 links to do this
I'll then take 15 links and I will
connect them to other call this
dimension one that was sort of the first
intro router routing in my butterfly
network I'll take another 15 links to
connect them to 15 other cabinets in
this dimension and another 15 links to
connect them to 15 other cabinets in
that dimension if you look at this if
you're a bird flying above the machine
room and you look at this what you see
is this dimension one actually dimension
zero is the 16 processors into one
router dimension one which is 16 of
those routers is all in one of these
cabinets so there's 256 processors in
each of the cabinets dimension 2 is
along one dimension of the machine room
and dimension 3 is along the other
dimension of the machine room so if you
if you package it that way we also did
similar packaging plans for competing
networks which included the khlo and a
hypercube we actually had a 3d Taurus in
here too but it was the most expensive
by far and you compare them what you see
and I if you look at our our paper on
this it actually goes into details about
our cost model with a very detailed cost
model which captures the increase in
cost as you go from a link on a board to
a link over a backplane to an electrical
link over a cable to an optical link
over a fiber
and it's got basically you know cost as
a function of distance and packaging
level if you apply that cost model to
this what you see is you know the
hypercube is absolutely the most
expensive of these the butterfly is
absolutely the cheapest but the flat and
butterfly this flattening which reduces
your effective radix doesn't really cost
you very much as a gap in here where
cost you something is you had to jump to
one more stage earlier than the
butterfly did but you still have that
logarithmic performance and you only
have one traversal of the network and it
winds you fairly big now what does this
do for traffic for performance so if I
have uniform random traffic in this
basically is comparing a bunch of
different routing algorithms I can
basically get you know essentially a
hundred percent throughput out of the
network running valiant is equal to Z
equivalent of emulating the clone
network I'm basically picking a random
intermediate point think of that as a
random middle stage routing to it and
routing to my actual destination it's
what this shows is that when I have
benign traffic I can do twice as well as
the clone at work in terms of
performance delivered per unit cost of
the network that's really because I'm
traversing twice as many of these
expensive links when I have worst case
traffic you'll notice on the axis has
been changed I saturated around fifty
percent because I really can't do much
better than emulating the clone network
what I'll talk about next really is the
difference between this line which uses
basically it emulates the khlo in an
adaptive way with these lines which
basically are variants on valiant
picking a random middle stage routing
there and then in the next little
segment of this talk I'll talk a little
bit about why adaptive routing is just
strictly better than oblivious routing
for doing these things and you really
don't want to do this this is what
happens if you do minimal routing and
minimal routing basically reduces you to
the case as if you had a butterfly right
if you had a butterfly network could
have no choice but to do minimal routing
and there is only one minimal route and
the worst-case traffic pattern is
exactly that one that stresses that
minimal route is basically where
everybody in this cabinet decides to
communicate only with that cabinet you
only have one link between those two
cabinets that link it's completely over
flow the only way to get better
performance is to actually send non
mentally to send to some other cabinet
and then forward on to your destination
cabinet so that brings up the question
of routing how do you route in these
interconnection networks and I should
say that a lot of our understanding of
routing is is due to 22 students that
are both unfortunately no longer in my
group the problem of students is by the
time they get useful they graduate one
is Arjun Singh who's here at Google who
developed a lot of our theory of global
adapter routing and Brian tolls who's
now a DE Shaw who developed our theory
of oblivious routing so there's two
there's two points that I want to get
across about routing and I'll try to do
this by just showing a bunch of examples
one that really was kind of an io / to
me is that adaptive routing avoids this
phenomena we call transient load
imbalance and this is a phenomenal where
if you think about routing at a clone
network for example if you brought
obliviously if you randomly pick a
middle stage of the network you know
that your traffic is going to be
uniformly distributed because I have a
uniform random function picking those
middle stages on average every middle
stage is going to get the same amount of
traffic and everything should be fine
you shouldn't be able to beat that but
that assumes you're integrating over
enough time for the law of large numbers
to work right in an instant you know if
you have n nodes pick n middle stages
what is the expected value of the number
of packets at the middle stage that
receives the most packets will receive
its log n the simple application of
Stirling's formula will show you that
and so because of that in a transient
and the transient of that one round of
sending messages I have a load imbalance
where one middle stage has end as log n
times as much traffic as the other
middle stage a thousand twenty four
nodes I've got you know ten packets on
one guy and the average node has one
packet so this transient load imbalance
leads to greater latency for oblivious
writing where the adaptive routing
avoids at an adaptive routing as I go up
the Khloe I say you know who has bigger
queues well I'm not going to go there
I'll go here and it winds up perfectly
balancing even over very short time
intervals the load on the middle stages
and the load on the channels that's the
first observation the next observation
is really applying some of the work that
Arjun Singh did for his thesis
to these networks is that if we have the
flat and butterfly network what we'd
like to do is when we can get away with
it route it like a butterfly take that
minimal path because that's cheap but
then when we can't get away with it
revert and route it like a clone at work
take you know a non minimal path to
spread the load over our network and the
question is how do we decide when we can
get away with the minimal path and when
we can't and the neat observation that
Arjun headedness thesis is that you can
use local cues as actually a very
accurate proxy of remote congestion and
in doing that distribute the load only
when you need to have a question Greg no
I'm not going to explain that not in the
next 20 minutes so the question was am I
going to explain how the writing I heard
them knows whether the packet it's about
to rout is an adversarial packet or not
and my answer that was no and then he
asked if I could and I said not in the
next 20 minutes I would have to think
about that a little bit longer to try to
try to explain that so here's the first
of those points which is basically the
point about transient load imbalance so
this is a from a paper we have it
supercomputing this year where we've
simulated relatively large khlo networks
and have compared oblivious routing
where we basically randomly pick the in
fact we when we do it in the
conservative way we're using a folded
Khloe randomly pick the nearest common
ancestor between the source and
destination route to that common
ancestor in their route back down or we
do it adaptively where we also route to
the nearest common ancestor but we pick
each output port as we're out up to the
nearest common ancestor based on queue
length what you see here is it it that
low load it doesn't matter because
nobody has you know has q's with very
many packets in them but did you get
near saturation this is really sort of
what determines a cost-effective nature
of your network is what happens out here
because you've got these expensive links
you want to keep them busy all the time
we operate our networks around 19
ninety-five percent capacity is a big
difference in latency it can be two to
one or more between the oblivious and
the adaptive now one thing we found when
we did adaptive allocators is some of
our first adaptive alligators had
horrible performance so here's the
problem you have a 64 port router 64
packets come in and they all say which
of the 64 outputs has the smallest cue
and they all pick that one this is what
we call the greedy allocator you see it
doesn't perform very well the reason it
doesn't perform very well is it was
giving us transient load imbalance
within the router right everybody was
picking the shortest cumin they're
making sure that he was not going to
have the shortest queue on the next
cycle it's not quite as bad as that
because everybody doesn't arrive during
the same cycle but there's enough
arrival there was enough simultaneous
bidding for that one output that it was
causing us load imbalance so he said the
ideal thing we would like to do is build
a sequential allocator and the
sequential allocator assumes it's like
one of these craftsmen who you get to
remodel your kitchen who has this
philosophy it doesn't matter how long it
takes he's going to do it right so this
this craftsman goes down port by port
through your packet go through your
router he goes to port one allocates it
to an output updates the output queuing
state and using that updated state
information allocates port 2 and so on
and you know we actually found very
clever ways of doing this in log time
rather than linear time but it was still
a very slow procedure to have to
basically have the semantics be as if I
updated the state in sequential order
for every packet bidding on an output
within one router chip
right you pick something put the start
on you would allocate that update the
output state pick the next one allocate
that and then you can pick a written
arbitrary order each time the important
thing is before you may each decision
you had updated the state with the
previous decisions so that they weren't
using stale state and all allocating the
same output in overloading it so it
turns out we were motivated extra by
some work that blodgett private card had
done on various probabilistic algorithms
and networks to say well what if instead
of looking at all of the outputs we just
randomly pick a couple of them and you
know adaptively routed between say those
two and it turns out this works
amazingly well sequential are two
ingredient of the sequential and greedy
algorithm where we randomly select two
outputs of the network and then either
apply greedy or sequential to them and
actually it turns out that when you're
only doing it over to it doesn't really
matter which one you do the sequential
are two is slightly better than the
greedy are two but they're both very
acceptable performance ranges and in the
full paper there's actually a involved
comparison as you go to r3 and r4
basically look at more outputs and the
greedy converges to this line and the
sequential converges to that line but it
didn't start out very far away yeah is
they offered mode
this is during the adaptive route up the
club so by definition the the offered
load will accept any output port yeah
related to choosing to random ones I
presume that is highly types of a
certain radius of the local route of
hymns and you make you have a decision
in 872 is a quarter of those it seems to
get your Maserati decision that's made
globally and then the allocation
decision is made in steps so we actually
do this across all 64 at once picking
the is is almost perfectly even though
you have 64 maybe were doing the full
deep thinking about it he would go
sequentially through 64 oh right and
some too gets you that much benefit yes
to to get you almost as much to look at
all 64 it was a very surprising result
so I always wanted to know what
transient load imbalance look like so
actually instrumented our simulator to
take snapshots of what the load looked
like during given cycles this is
actually at a relatively high loading
using oblivious routing and that you
know phenomena that I talked about we're
on average they all have the same load
but you know transiently we've got some
here that are you know holding 16
packets while the average is around 2
and if you use adaptive routing you know
that there actually is still some
transit load imbalance we don't quite
understand why that's happening but you
know most people are actually you know
at the average or just a little bit
above it and we're you know this is the
sort of thing that it was worth digging
into we'll find out what's causing you
know our allocators and this may be the
part of the phenomena of picking too and
not all 64 is it may actually
occasionally send a few more but it
balances pretty well so let me talk a
little bit about a network we built in
collaboration with Cray researcher
called crate research and we're crank
two it's going to be in in their Black
Widow supercomputer and this is a
network that Dennis apps gave a talk at
iska and and gave a lot of details about
I'm going to give it
a brief overview compared to what he
talked about so limited by talking a
little bit about Black Widow it's a
shared memory vector parallel computer
that means it's got a bunch of nodes up
to 32,000 each of which is a pretty
capable vector processor now I forget
the exact gigaflops ratings but it's
many tens and they may even be breaking
into the hundreds and each node can make
a memory reference to the memory
anywhere in the machine so you can do
loads and stores to a global address
space that may land on your note or it
may land on any of the 32,000 nodes in
fact probably one of the main reasons
for the vectors is to hide that memory
latency they can put enough outstanding
memory references out by doing a bunch
of vector loads to hide the and it's
several thousand cycles that it takes to
read a word from the far corner of the
machine the topology is a clue and I
couldn't you know when I was putting
this talk together hastily last night I
couldn't find a picture of the full
close I've only shown the first stage of
it because that's all that's kind of
unique the that the packaging unit is 32
processors this is 32 of those vector
processors each vector processor has
four channels each channel is 18.75
gigabits per second it's 36 and a
quarter links and those four channels
basically go to for duplicate networks
those are for completely separate khlo
networks each of those separate khlo
networks has up to three stages in each
stage has radix 32 and that could see
32,000 the the chip we built for this is
called York for yet another router chip
which also happens to be cray spelled
backwards it's a it's a 64 port chip
each ports 36 and a quarter gig links it
uses table driven routing but lest you
think that we need a table entry per
destination the table driven routing is
really used only to map out bad links in
the network you can you can configure
the network in a very conservative way
around bad links and so what it does is
the tables match ranges of addresses and
say that if you're writing
this range of addresses pick this subset
of the output ports when you're routing
up the club and then when you're out
down the khlo you basically route based
completely on your destination address
so it's adaptive routing up
deterministic routing down there is a
deterministic routing mode going up as
well because it is a cache coherent
machine and certainly the cache
coherence protocol messages have to
remain ordered so if you're sending an
ordered message it uses a hash to select
the upward links and if you're not it
basically adaptively routes to select
the upward links it's got a bunch of
neat fault-tolerance features in a super
computer we have 32,000 nodes you start
working out the fit rates for these
nodes and you know failures happen on a
pretty frequent basis often you know
weekly if not daily and so you'd like to
do is have a failure in the machine you
know ideally not even interrupt
operation but whiff barring that super
computers tend to be protected with
checkpoint restart with checkpoints
having typically on the order of about
an hour so we'd like to do is if you
can't no mask the failure just roll back
to the last checkpoint have a very quick
reconfiguration around it so the first
thing is that you absolutely have to
detect every error you have to detect
the error before it can corrupt state
that you can't that you can't recover
and you'd like to have the rick have it
not corrupt state you can't recover
quickly the quickest restart is we use
link level retries the technique that we
first used on a reliable router about 94
and as since been used on many routers
successfully we're on a router to router
link we check each packet as it comes in
and we explicitly acknowledged each
packet and you rather than asking to
retransmit it typical a particular
packet over again this protocol actually
is a very end of a sliding-window
protocol but not end to end as it is in
tcp on a link level where we basically
say retransmit everything since this
packet and so once we get an error we
basically flag that point throw
everything else that comes over the link
away until the start again here comes
back and the previous router basically
maintains a run
history of everything that its sent that
hasn't been retransmitted just rolls
back to the beginning of that history
and starts retransmitting it this works
extremely well you know these these
optical links are SPECT at sort of an
error in ten to the fifteenth bits
they're actually substantially better
than that depending on the power levels
that they operate on but they still make
a large machine they make errors on a
daily basis and this very effectively
masks those errors no errors of the
optical links squeak through this it
turns out we have a monitor on the links
and if your if a particular link has a
retry rate that is higher than what you
would normally expect you can set the
threshold wherever you want this is
actually you know the mechanisms role in
hardware but the policies are all in
software you basically can degrade the
link so you can find which of the three
bits of that link is the one causing the
problems and you can turn that bit off
so we can run the link three bits wide
or two bits wide or one bit wide or turn
the whole link off and when you turn the
whole link off then that's when we
update the routing table saying don't
use this link and we take advantage of
the fact that there are three parallel
networks and many alternatives for
intermediate nodes that allow you to get
to any destination without using that
link so a lot of clever things in there
the implementation we are follows very
much the subscript chapter I'm a genius
I can go back position going back 15
years
exactly the same thing with air control
on the links he's only been one
I know sure which gave you the same in
other words if the only champions
wouldn't describe then transit errors
inside the church we cover those as
welded the CRC is check in the chip as
well as across the link so we actually
you both of the things that so we both
we check the CRC not just you know from
the end from the transmitter on this
router to the receiver on this router
actually the link level block in the two
routers we also check it from the input
link level block to the output link
level block and a router to make sure
that it hasn't become corrupted
traversing the router and there's also
an end-to-end check so it's sort of belt
and suspenders on checking the the
packets so here's the the the art design
it follows very much the the design of
the the hybrid photo I showed you
previously which was in the the John
Kim's iska 05 paper this is a figure out
of dentists appt cisco 6 paper that this
is kind of a fun project because we've
been doing this work on high radix
routers cray decided that their plans
for the network for the Black Widow
weren't going to work out they needed a
new network and we said well why don't
you guys do this and they went from the
first meeting to talk about it to
working network in 18 months which is
kind of a neat project but in any case
one thing which is sort of shown here
which wasn't shown in my previous block
diagram is a tiled nature of it instead
of taking each input port and putting
them all along the edge like I had in my
picture it's kind of a nice way to draw
it it's not a nice way to build it we've
divided this network into 64 tiles and
each tile has one input port the router
for that input port it's got the
horizontal lines which allow that input
port to compete and get to any of the
eight by eight sub switches in that row
it also has one of the eight by eight
sub switches so it's just a completely
modular design 164th of the whole router
is in this tile including the input port
a sub switch and one of the output ports
one thing different between the the
router that we simulated Frisco five
paper and yar
is that it turns out that when we
actually went to build your art we found
we had enough wiring that we could have
each output tort or each output tile
have its own link to each output
multiplex to these final multiplexers
here rather than having a bus they would
have to compete for for each of them and
by doing that what we were able to do is
that when you win remember it's sort of
three decisions compete you know to you
basically compete among virtual channels
on the input go across a dedicated line
for this input into the AP 8 by 8 switch
complete compete for the eight by eight
switch for one of these ports at that
point there's no competition you're
guaranteed this link to the output MUX
which in our pipeline is actually a
three clock link and then the
arbitration for the output port is all
here what this did is it allowed us to
make that arbitration entirely local
otherwise we would have had more latency
for having to send a bid for that output
port three clocks away win the bid three
clocks back and then oh and now it's
time to transmit six clocks later
instead we can basically just transmit
and avoid that six clock round trip
having to deal with a global arbitration
one aspect of the design is that all of
the arbitration that happens happens
local to a tile the only things that go
between tiles are very heavily pipelined
transmission of data so this is zoom in
on for tile so you can get a closer look
at one so yes I think everything air
have talked about the routing table is
really very simple 88 entry cam that
basically matches addresses and gives
you a subset of output ports it's 90
nanometer ASIC 192 six and a quarter gig
surtees which on tops what in my
knowledge is the next previous record
was the belly o 3003 switch that we
presented at hot chips in two thousand
one that had 144 four gig surgeries on
it this is it an 80 watch if it turns
out this was the estimate they have them
in the lab known it's a lot less than
this it's a usual thing that all of the
library numbers are heavily sand bags
that you'll never be disappointed and
it's a reasonably large die so it's
nearly lunchtime the real reason I came
here today is that Google has
really good restaurant so not not
wanting to delay that let me let me
summarize the talk so first of all you
know why hi radix and it's really
because over the last you know 10 or 15
years we've gotten this two to three but
this late order of magnitude every five
year increase in the bandwidth of our
router chips and if you work the numbers
the right way to use that additional
bandwidth is to have more skinny
channels not to have you know fatter
channels and you can sort of provision
arbitrary terminal bandwidth by using
channel slicing the Black Widow did
4-way channel slicing that's eg arc
having four ports into four parallel
networks and then by having a high radix
you both reduce your cost since there's
fewer hops and your latency it turns out
that once you've decided high radix is
the right thing to do building a high
rate extruder poses a bunch of
implementation challenges both in the
allocators and in avoiding the head of
line blocking problems without having to
over provision an expensive switch we
found is it using the hierarchical
crossbar gives you essentially the
performance of the fully buffered switch
at a small fraction of its cost and you
can get around the problem of the non
scalable allocators at the same time by
breaking this allocation decision down
into a bunch of small decisions rather
than doing it all in one big decision
once you have this high rate extruder
our natural inclination in fact it is
the way that the Black Widow is built is
to use a clone network but it turns out
that if you if you take advantage of the
ability of doing global adaptive routing
you can get twice the performance for a
given cost as a clone Network benign
traffic by building a flattened
butterfly because the flattened
butterfly allows you to have this
minimal route when you have a benign
traffic pattern or at low traffic levels
where you can get away with routing
minimally taking half of the link
bandwidth that a closed you don't have
to go up and down you just go directly
to the destination and then by
monitoring local queues you can detect
when you're in a situation where routing
minimally would cause congestion and at
that point you can then switch and treat
it like a clone Network and get the
performance and cost identical to a clue
an adversarial traffic patterns it's
really the best of both worlds
to rout in that Network we have to do
global adaptive writing that enables use
of the flat and butterfly topology that
global adaptive routing we would have
you know two choices one is route
minimally and that would be awful right
that would give us you know you know a
tiny fraction of peak performance on
adversarial patterns or route
obliviously and that would basically
revert to the close so there would be no
advantage with the flattened butterfly
we might as well just built the clone at
work Global adaptive routing allows us
to switch between these two modes and
get the best of both worlds then the
other thing that we just recently
realized it's the topic of our
supercomputing paper this year is this
phenomenon of transient load imbalance
caused by you know randomized routing
and how adaptive running can get rid of
that now actually offer substantially
lower latency particularly high loads
then you get out of using randomization
schemes I talked a little bit about the
case study of your kit say you know 2.4
terabit per second router on a chip and
actually that includes the fives that
can drive up to about 10 meters of cable
so for you know machines where the
cabinet's aren't too far apart it's
entirely electrically connected which is
huge cost savings compared to even the
least expensive you know xfp type
optical links and it enables you know
creative build 32 k nodes shared memory
computers where the global memory
bandwidth is about one tenth of the
local memory bandwidth so you can
basically reference anywhere in the
machine at about a tenth the bandwidth
you can reference your local memory so I
thought I'd end the talk with just a
little bit of a speculation on what what
should Google do so you guys build big
machines and do it mostly with sort of
stock hardware today you know I would I
would posit that by you know you're
doing an aggressive design you could
build a much more cost-effective and
also much lower latency interconnect
along with high reliability and you know
perhaps by putting the right shims at
the interfaces make it look just like
you were plugging into stock hardware
from the end nodes although you know if
I had the ear of the influential people
around here I certainly wouldn't stop
there I think that using stock pcs for
your processing nodes is also not very
cost-effective either from a dollar or a
watt perspective and you can do much
better by building you know
they multi-core processing chips with
very simple processors optimized for
compute per unit power and then hooking
those together with a very large
interconnect and sort of with the
resources at Google could could bring to
bear you could build a a totally awesome
complete computing platform that should
be able to provide an edge and the
applications that you deploy on it so
any questions yeah is efficient or the
right thing to do to build these high
rate examples why is it if I go out to
people feel chips I can only find 1644
say 10 Gigabit Ethernet ships and sort
of the power arm that Nick is actually
the answer to this is when we use the
word router here or some of us use that
word radish there we're thinking
player 3 or 4 so happy how do i do that
with this saying isn't this really isn't
this really just a crossbar chip in
it isn't actually I think why so I
apologize for using the term router in
the way the inter Connection network
world uses it not in the world the way
the networking world uses it you know
when we built the the avicii tsr we had
this problem so the the tsr itself is a
router and the the oboe chips we built
under the switch in the tsr we referred
to as the fabric router chips to
distinguish them these are fabric router
chips they don't do you know you know
prefix search to do you know you know
forwarding or traffic management or
anything like that they they're very
simple switches from that perspective
they're more than a cross points which
in that they have buffering and and buy
and they have flow control and so by
doing the buffering and flow control
correctly they implement a fabric that
does guaranteed end-to-end delivery of
packets with the appropriate routing
algorithms they do that in a load
balanced way choosing Pat choosing the
paths in a way that do it so it's more
than just a cross points which it's a
but it's a building block of a fabric
that collectively provides a routing
function now you ask the question gee
why why do all the vendors only give you
you know 24 port switches or whatever
and there's a bunch of reasons for that
many of them probably having to do with
what their marketing departments are
telling them people will buy and at what
price points but but one of the reasons
is a feasibility reason because you know
by sort of in some sense it's like a
race car you strip it down by taking the
heating out of it and the sound
insulation all of that to get just what
you need it will go very fast this is a
very stripped down switch in the sense
that we don't need any of the mac layer
we need a minimal amount of buffering
because we have credit-based flow
control on the links we only need enough
buffering to handle a round-trip latency
on the links we don't have to hold a
whole packet even so these buffers are
all very shallow in the inputs and the
buffers at the sub switches are very
shallow they're all provision to handle
a round-trip latency of the next level
of flow control and so by doing that we
can build a very lean chip that actually
winds up being nearly limited by the
crosspoint wiring it's not but it's
nearly
by the crosspoint wiring and where you
try to do a much heavier protocol you
wind up needing much bigger buffer is
much more protocol logic and your tiles
would get a lot bigger and you and you
couldn't build a 64 x 20 gig switch I
think I think that's sort of the
feasibility reason yeah
within the
the latency curves i showed did not
include the reordering time i suspect
though that when you look at the
reordering time you will still have an
advantage because what the adaptive
routing tends to do is it tends to bring
packets into the same time delay and
therefore will probably make the
reordering take substantially less time
right even though you have to wait for
the last you know packet of a message to
arrive before you can release a message
that that mess that packet will tend to
arrive in a narrower window than it
would have if you were obliviously
routing yeah
and then for in cities they
yes the art does not support broadcast
it's strictly point-to-point you could
do it what what broadcast and multicast
break is a lot of the queue management
because you know with a flow controlled
interconnect like this what it would do
is it would prevent you from so think
thing about I have something in an input
port here of this input port say that
wants to go everywhere right what it
means is that before i can even
arbitrate for this input switch the
input line I would have to acquire
credit for every one of these buffers
right and then I could send it it would
pop into all of those buffers and then
before it could win the switch it would
have to require a credit for all of the
output buffers right and and so it's
really that credit management that makes
broadcast or multicast difficult it's
not difficult to do from a routing
perspective or from a you know a data
transport perspective it's difficult to
do from a buffer management perspective
because if one buffer blocks up right
you then cannot advance this packet
you're holding it waiting for everything
to be ready when you can use the direct
cultural thing
share to the longer find evidence no no
there were two separate points that made
about adaptive routing one of them was
that you can basically build a much
lower cost network by taking advantage
of the fact that you can your route
benign traffic directly to its
destination and not through a middle
stage a completely separate point is
assume you have a clone at work so you
have to go through middle stage no
matter what the number of hops is going
to be the same either way for oblivious
or for adaptive routing you will get
lower latency for adaptive routing
because you avoid the transient load
imbalance induced by Bolivia's routing
usually after grabbing simply you're
having a smarter algorithm for choosing
the exit ports instead of the static
cash that's right instead of whatever
your random scheme you have you are
using information about the network
state to choose that output port
whether you do a packet or flow you're
not using it's an oblivious scheme if
you're using hashing you're not using
information about queue queue sizes to
do it by using information about q
sizing you're able to dynamically
balance the load and keep the load very
smooth right so it seems to me that if
you have enough alpha course for the
oblivious has you to choose from right
it's going to be some critical number of
output ports at which the comparison
between adaptive routing and just a
simple hash you know right now it goes
the other way the more the more output
sports you have the larger the maximum
transient load imbalance will get it was
the question I asked them that Baltimore
answered is that if you have n things
picking in things right on average
everything gets picked once but the
worst-case thing gets pic log n times I
think isn't it I think
let me just stop trying to figure out
exactly what you're gonna flip yeah more
looks plenty
it's another question I guess it's lunch
spot they go very much protection</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>