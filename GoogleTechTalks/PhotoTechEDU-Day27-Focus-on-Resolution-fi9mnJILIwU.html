<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day27: Focus on Resolution | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day27: Focus on Resolution - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day27: Focus on Resolution</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/fi9mnJILIwU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome to photo tech lecture
number 27 today we're happy to have Ken
Tarkovsky who's been a Googler now for
about five months
before that he worked at various places
like Adobe and kinoma and for a long
time at Apple where he implemented
QuickTime VR and bunch of stuff like
that and coincidentally ken is probably
the person at Google that I've known the
longest I don't know if he remembers but
we met when I went and gave a lecture at
Dave Patterson's risk machine class at
Berkeley around 1980 or thereabouts
ken take it away Thank You dick today I
wanted to talk a little bit about
several issues related to resolution
from the image acquisition point of view
until the time that it gets into your
camera and you're able to use it now I'm
going to go down some avenues that very
few other pixels have gone through and
you are at least a lot of pixels have
gone through this this direction but you
may be a little bit concerned about the
loss of resolution and how how you could
actually even get decent resolution by
the time you it it gets to your your
computer screen so here's the overview
of the talk I'm going to start talking
about aberrations in the lenses and then
diffraction effects of apertures there
are several different types of image
acquisition devices I will also talk
about the difference between resolve
ability and sampling density and there's
also the what I call the 70% rule which
is a sort of artifact of our common
sampling mechanism and then I'll cover
the relationship between field of view
focal length and pixel density now I've
taken a lot of my diagrams from this
book here called model modern optical
engineering from Warren Smith and I
highly recommend that anyone who is
going to be doing any work in this area
get a copy of this book I understand
that it's it's now in a third edition so
it is probably at least another half
inch
thicker than this no our pixels start
their journey about 93 million miles
away at the Sun whether it's directly
through the light that ends up coming in
and coming into our camera after
bouncing off of something or whether
it's indirectly as a result of being
stored in in some kind of biomass or
wind energy or anything like that and
eventually it goes through the
atmosphere does a little bit of
dispersion interacts with matter and
it's transported through space until it
enters the aperture of the lens and
we're going to start looking at that
light
once it starts coming through the lens
now light is a strange kind of beast
sometimes it's it acts like a particle
sometimes it acts like a wave it's
sometimes hard to pin down and these two
aspects of the dual nature of light are
going to enter into how it affects
resolution the wave aspect of light is
is what we what we can analyze using
wavefront propagation analysis and
particle kinds of nature of light can be
analyzed using geometric optics or ray
tracing analysis and the illustration I
have on the left is a diffraction
pattern made by light as it goes through
an aperture and the rightmost one is
more like a ray tracing analysis of rays
of light as they pass through a lens so
we're going to be looking at both of
these and how they interact in order to
create the type of resolution or degrade
the resolution so first I'll talk about
the particle nature the the nature that
we can analyze with ray tracing the
light comes through the lens it's
actually it's compound lenses several
lens maybe up to 1415 lenses that are
connected in different ways in order to
counteract some of the types of
aberrations that we see the there's a
point spread function which is
inherently related to resolution and
that says how an infinitesimal point
looks when it gets through the entire
lens system the types of aberrations
that occur are spherical aberration coma
a stigmatism chromatic aberrations as
well as distortion barrel distortion
pincushion distortion and we'll look at
each of these in turn and see how they
affect the resolution so spherical
aberration is is how focus varies as as
light goes across the aperture from the
center to the edge of the aperture so as
you can see over here the the Rays in
the center are converging right at this
point whereas the ones that that came
from the edges are converging at that
point you can try to put your you can
try to focus this so that so that it'll
be appropriate for the center rays or
the edge rays but you can't get it in
focus for both of them at the same time
and this is usually something that we
try to correct really well for in lenses
at least across most of the lens may be
at the very edge that will be the place
where it gets extremely blurry but most
of the content that we have will be
located towards the center and so if we
lose a little bit of resolution as we go
off to the edge it really doesn't matter
all that much the next kind of
aberration is called coma this one's a
little bit more difficult to to imagine
but basically as you move across the
aperture the the magnification will
change so maybe in the center you'll see
something that is say one to one in size
and then as you go towards the edge
it'll be more like two to one in size
and magnification as a result you
at a point that that or at least a round
area that will scale anisotropically
it'll scale more in one direction than
the other and you'll end up getting a
point turning into a tier shape like
this or or a comet I believe that most
of the lens manufacturers try to
accommodate this as best as possible
because this is probably one of the most
disturbing kinds of distortion that we
wouldn't be able to deal with very
easily the next type is probably
familiar to those of us who wear glasses
at least the a stigmatism part a
stigmatism and field curvature are
intimately related there they're almost
the same thing they're both different
types of curvature field curvature is
sort of a general curvature where the
focus is not on a plane but on a on a
curved surface instead and a stigmatism
is when the curvature is different in
one direction than another so most
cameras are radially symmetric the
lenses are ground to be round and so you
very rarely have the astigmatism but
field curvature is something that that
you have to deal with and it may be a
little bit difficult to distinguish that
from the spherical aberrations no a
stigmatism and field curvature we can
try to D do deconvolution on I'm not
sure whether we can do any kind of
undoing of the coma spherical aberration
you know might be able to be reversed
using deconvolution but it's best to try
to get rid of them as much as possible
in the lens the next type of aberration
is chromatic aberration and that's where
the the aberrations or even the scale
varies with wavelength the most common
is when the scale varies with wavelength
where the red pixels for example will be
over
out here and the blue pixels the blue
the blue area the blue component of the
color would be over separated somewhat
now those are actually really easy to
correct just by using some kind of
radial remapping and nonlinear remapping
and and it's it's especially apparent in
lenses that have a much wider field of
view something like a like well like a
ten millimeter lens or or especially
something that ends up having well more
barrel distortion which I guess is one
of the next things that I'm going to be
talking about and that is when straight
lines don't map the straight lines
anymore typically the distortion will be
rounded like this this is a barrel
distortion and the pincushion distortion
is the opposite and so we will apply
some kind of a nonlinear pincushion
distortion correction in order to
counteract the barrel distortion so
that's easy enough to correct but it
doesn't really affect resolution it
affects a little bit because you're
changing the pixel sampling density of
particular places but it's it's it's
pretty much infinitesimal the the next
section here we're getting into is the
wave nature of optics and many people
sort of ignore this so I started looking
into you know the number is the plugging
numbers in as to the actual dimensions
of things that people use now this
equation over here on the left is the
equation of the intensity of a point
distributed through space as it goes
through the I mean this is through the
imaging plane as a function of space as
it passes through a circular aperture
now that j1 there is the the Bessel
order bessel function of order one which
sort of looks a lot like a sine wave and
so it's really like sine x over X where
this is going to
like the reconstruction of that
particular point which would normally be
you know that the vessel function would
be zero at this point but because you're
dividing by itself it ends up having a
value of one the first null will come
over here now these things here are are
much this is much larger than than it
actually is it's much it's these ringing
things here are much smaller so the
thing that we're concerned about mostly
is is this now the interesting thing
about that is that as the aperture gets
smaller this this this hump will get
wider which seems a little
counterintuitive it but I believe it's
probably a manifestation of the
uncertainty principle as you try to
squeeze something a little bit closer in
one of its properties it expands in
another now hmm the oh the the center
part here is called the airy disk and
that is that's that's what we're
primarily concerned with as far as
resolution now this will the diffraction
will will limit our resolution and here
are some ways of thinking about
resolution due to diffraction if the if
the pulses are too close then they sort
of merge into one and you get you get
one hump there over here they're far
just barely far enough and you can sort
of resolve it but you can't really see
the peaks separately over here they're
separated far enough that you can see
two individual humps and here they're
even further this is probably one of the
most common mechanisms for determining
resolution due to diffraction and it is
related to the wavelength of the light
and the f-number
so the smaller the f-number the smaller
the distance between let's see the
smaller yes the smaller the f-number a
smaller distance over there the larger
the f-number
that gets and the you know for higher
wavelengths like blue light the the
diffraction the the airy disk gets
larger let's look at some of the
implications for that so a visible light
varies between about 400 and 700
nanometers and at f11 we plug the
numbers in and we get that the area disk
radius would be nine point four microns
and for a blue light it would be five
point four microns but the tip the pixel
size for typical digital SLRs is six
microns which sort of implies that if
you at least if you look at the Rayleigh
criterion where you'd like the distance
to the first null to be approximately
where you'd have your your pixels the
blue light would probably be sampled
adequately enough but the red light is
is severely under sampled I'm sorry
that's the opposite so so what is it
that I'm trying to say here so the at
f11 if you're sampling for if you're
sampling for RGB pixels on a cell phone
or something the pixels are going to be
too close your your your spot is is
going to be is going to be way wider
than the pixel separation so you're in
fact you're wasting those pixels you may
as well have pixels that are much
further apart if you have the bigger
pixels then this is probably pretty well
matched for the digital SLRs there are
some some CMOS imaging devices made by
micron they have 1.75 micron pixels and
these the at f11 at least the the spot
the airy disk would be covering 3 to 4
pixels so you're really wasting those
pixels now f11 happens
pretty magic number in that it is pretty
well matched at least with the with the
blue light here so that you can get
adequate sampling for that the when
you're using a digital SLR if you use
f-16 to try to get more things into
focus then it's it's that then you're
wasting a lot of the pixels and so so
anyway the the diffraction where we're
pretty much right at the diffraction
limit for a resolution if we have
smaller pixels it's not going to work
cell phones I guess typically use a
number of about five six or something
like that so you can get higher
resolution but you won't have that much
depth of field okay now on to image
acquisition devices the quaint old
imaging acquisition device of film has
been around for some time it's still
being used CCD sensors CMOS sensors
there's also mechanisms of acquiring
images by putting the light through a
splitter prisms or maybe having
different layers or having a bear
pattern there's different ways of
capturing the different spectral samples
of light you can use 1d arrays or 2d
arrays or well and and also take a look
at what a typical CCD looks like so film
generally has three different layers
there is a color sensitive grain in each
layer that sensitive to different
different portion of the spectrum and
the green size and the density of the
grains will determine what your
resolution is I couldn't find any
pictures for that but this is your more
typical digital sensor which is in a
Bayer sampling pattern and you don't
really have RGB pixels you've got red
pixels you've got green pixels and
you've got blue pixels and you convert
those to RGB by interpolating them so
for example if you've got a thousand
pixels per millimeter then
then the R&amp;amp;B are a really sample at only
half that density as you go across here
you can see the blues are two pixels
apart the reds are two pixels apart but
what about the greens you might think
well they're two pixels apart but not
really if you turn your head 45 degrees
you can see that there is a pattern
diagonally where the pixels are the
square root of two apart so really for
green your your sampling density for
green is 707 green pixels per millimeter
and this is one of the places where my
70% rule comes from since green is
basically really well tied in well
correlated with intensity of an image
then the Green actually dominates as far
as the highest possible resolution that
you get so that means that that that
you're actually using more pixels to
represent an image once you've converted
from Bayer to RGB than you really need
to so on the bottom so an 8 megapixel
CCD will have 4 megapixels of green 2
megapixels each of red and blue and so
the RGB resolution will be somewhere in
between 2 megapixels and 4 megapixels
and I I claim that it's that it's it's
it's at the 70% point and we'll take a
look at that a little bit later a lot of
high-end imaging devices will use a
prism to split light up into 3 different
directions and we'll have individual
sensors for those 3 this probably gives
you the highest quality because you end
up having in a red and green and blue
pixel for red green and blue sample for
each pixel these images were borrowed
from the vivillon website which makes a
unique kind of sensor that is
actually layered so like film the fobian
image sensor has three different layers
and the lights will go through the
different layers of silicon each one
absorbing particular wavelengths of
light and letting the rest through so
with the fobian image sensor you
actually get RGB pixels as opposed to
the Bayer pattern where you either get
red green or blue pixels so this is a
way of getting much higher resolution
especially if you're if you're limited
by your optics you can squeeze out more
resolution this way now panoramic
cameras use a one-dimensional array
that's a range sort of vertically and
they'll sweep it around 360 degrees
scanning around the scene as it goes
obviously it takes a while to go around
so if there are things that are moving
then they're going to get blurred out if
you rotate the camera too fast for
example if you move if you rotate it so
fast that you'd sample every 90 degrees
you're gonna be aliasing you're gonna be
missing a lot of information if you move
it around too slow then you're going to
be blurring things so you need to have a
good quality lens preferably with a
large aperture that can capture
information the highest the highest
possible resolution information now what
does a CCD look like this is a this is
what's called an interline ccd which is
different than your standard ccd a
standard ccd will have a photodiode that
takes up a good portion of a cell and
that particular type of CCD is one it's
used in still image capture devices like
cameras but the interline is used in
more video kinds of capture devices and
the way the reason why it's designed
like this is that you can expose the
photodiode over a period of time and
then once it's time is up
its exposure time is up the pixels get
transferred to this shield now if you
look at this only half of the area is
actually active or actually less than
half maybe like forty percent or
something like that in a standard CCD
the active portion may take up eighty to
well maybe ninety percent of this but
for this particular thing if this is
nine millimeter nine microns then the
active area here might only be eight by
four so we we may actually be losing
things there may be some details that
fall into this area that we're not
capturing and also we're losing half the
lights so this is what the what the CCD
looks like from the side there's this
transfer gate that transfers of data
into into this area for the light shield
and then there's a shift register or
something that will will transfer it out
now with with video cameras they use
this mechanism to avoid having shutters
that after repeatedly open and close and
this works pretty well so there's some
ways of trying to recover that light one
of the ways is to use lens 'lets over
the top of the pixel to collect the
light so that it comes into this well
because otherwise it's just going to get
lost now this will collect this will
collect more light and it will also keep
the light going through the correct
filter if with without this lens lit we
could have some light coming in from the
side going through this green and
hitting the photodiode if you know if
this if this weren't here if this if we
didn't have light shields here if we had
a full photodiode we could actually get
light coming in from the side so this
will this will collect the lights try to
give you the maximum kind of efficiency
and here's here's the results of using
this lens light I know these curves look
the same but the scale over here is 0.16
and it's up to 0.4 3 on the top so
there's almost three times as much light
being collected just by using that lens
lit and so there's three times as much
light even though we're throwing away
only half of it due to the the pixel
shield in the inner line CCD so this is
actually working out a lot better and
could probably be used for for standard
C CDs now I mentioned a little bit of
color pollution the the inter line CCD
device is rectangular in one dimension
it's half the width and the other
dimension it's almost the full width and
so that's where this this light
pollution would come in you might have
some light coming in here from red and
going into the the green part of the
sensor on the other side so these sort
of things need to be dealt with there
are real factors we've come across that
and some of our work our recent work now
resolution and pixel sampling sampling
density are a little bit different and I
want to talk about the difference
between them over here and talk about
certain ways of determining what the
resolution of a particular image is and
then I want to look at the intimate
relationship between resolution and
focal length because there is a very
intimate relationship now these two
pictures both have the same number of
pixels but they obviously don't have the
same resolution there's a lot more
detail on the right and there is on the
left and the way that I generated this
picture was actually going into
Photoshop taking the right image
reducing it down by I don't know factor
of 4 or something and then bring it back
up one way we can determine the native
resolution is to take the Fourier
transform
now Fourier transform is is sort of Li
rated well it's it's fourfold it has
fourfold symmetry pretty much and so
I've just taken the upper right quadrant
but the energy where it's where it's
brighter indicates where there's more
energy where it's darker there's less
energy and you can see with these two
these two plots the one on the left has
this energy primarily concentrated in
the lower left corner whereas the one on
the right has it spread out a little bit
more but you can see that it tends to
drop off sort of around here so there
there is sort of a radial kind of
component to the to the resolution that
might be due to diffraction it might be
due to the sampling pattern or just the
fact that well that has to has to
propagate through a lot of materials on
the way over now one of the things you
might notice here is that that there are
some lines on the left and to the right
these these over here which are I
believe are probably a residue from the
resampling technique used in Photoshop
so it's leaving some extra data in there
maybe it's due to quantization or other
kinds of other kinds of sloppiness but
ideally that should be pretty much all
that energy should be completely
confined to that one corner yes
what dealer horizontal lights which
gives the high-frequency around so your
your comment was that you think that
those lines are actually a real part of
the image because we generally have more
horizontal and vertical lines and that I
would agree would be correct for for
this image but this image was reduced in
size on the left by one quarter so the
resolution of the horizontal vertical
should have been reduced but you might
have I have reduced the number of pixels
those pixels cannot actually represent
high frequencies like that so those had
to have been introduced by the
resampling process yes
you're right there they're arts vertical
and horizontal elements in the original
peacock image here maybe the legs but
yeah so Lance was saying that he
believes there it's probably due to the
resampling artifacts too well regardless
of those resampling artifacts we we can
we can look at our the FFT and see where
the energy is concentrated and this is
the original FFT that I took without
being cropped the previous one I had
cropped a little bit on the right but
this wasn't cropped and these are the
dimensions that correspond to the full
image and I just by hand said okay well
where does the energy drop off and it
turns to tends to drop off right around
here right around there and I just did
that by eyeball and then I measured how
far it was and it turns out that it's
well 69% 64% which is pretty close to
the 70% that I had suggested as being
the the the native resolution of a bayer
sampled image this image was sample with
a bayer sampling pattern and then
reconstructed so what this is saying is
that you can actually shrink your image
by 70% in each dimension throwing away
half of the pixels and still you won't
lose any detail hard to believe but it's
it's true so that's one way of doing
compression lossless compression another
way to determine resolution is by
resizing an image and that's what I had
done with the right sort of done that a
little bit with the peacock image so the
idea here is to take your image shrink
it expand it subtracted from the
original and amplify the differences to
enhance the contrast so in Photoshop
I've done that with this peacock image
and branded there's going to be some
amount of maybe noise due to the
or I'm not sure where the note where the
differences come but at ninety percent
reduction there is some energy still
left in there but the thing to note is
that the ninety percent and eighty
percent really don't look very different
at all they're pretty much identical so
we could shrink the image down to ninety
percent we could shrink it down to
eighty percent and and it's not we're
not losing any information there if we
start looking at the seventy percent as
you can see the peacock head it gets a
little bit brighter and then going down
a sixty percent and fifty percent there
is more the image gets brighter so that
indicates that residual that's that's
due to the higher frequencies is
becoming more apparent at fifty percent
so this is yet another experiment that
that proves the seventy percent seventy
percent rule for for bayer patterns so i
was looking at what resolution means i
had done some work with with panoramas
over at apple and we started out working
with cylindrical panoramas and people
could compare their panoramas and say oh
mine has three thousand pixels across
yours has 3,500 so yours must be higher
resolution then at one point we
introduced a cubic panoramas and it
wasn't quite all that natural to try to
compare them because they're measuring
different kinds of things so when you do
have a panorama how do you compare the
resolution certainly the number of
pixels doesn't really help any and
pickles per inch is pretty much
meaningless in panorama but pixels per
degree is something that ties the image
into the three space and that gives us a
better idea of what the actual
resolution for a panorama should be
measured in
and I'll go into how I believe that way
but first if we look at a cylindrical or
spherical panorama we can take the
circumference and divide it by 360 and
that'll give us the resolution in pixels
per degree for a fisheye that's 180
degrees across you can take the diameter
and divide that by 180 for a cubic
panorama I do have a formula over here
faced with times pi divided by 360 and
the I will get into how how that formula
was arrived at but here's here's a way
that you can compare these having a
panorama that's cylindrical or spherical
at 24 96 pixels would be equivalent in
in resolution in terms of pixels per
degree for 1249 pixel fisheye image with
180 degrees or a 77 95 pixel cube face
and all of them would be equal to six
point nine pixels per degree now you
might say well there's going to be more
there's gonna be a higher pixel density
in different parts of the image but what
this is actually measuring is the the
minimum pixel density at as determined
by the focal length and so we'll make up
some definitions over here that
panoramic resolution will be the angular
pixel density as determined by the focal
length now the focal length means
different things for different types of
geometry for a plane it is the distance
to the imaging plane for a sphere it is
the radius of the sphere for a cylinder
it's the radius of the cylinder so and
that would be the distance to the
imaging surface at the center of
projection so obviously with a plane you
can take the distance off the side but
that's not the closest one you want to
get the one that is the distance is
perpendicular to the imaging plane
now I've got a few more details at my
website and I'll just show that to you
in just a moment you can look at it a
little bit more here let's see if I can
get the cursor over there mmm let's see
cursor there it is okay so let me just
do this so we determine a standard for
panoramic resolution by defining the
focal length as being the distance to
the imaging surface of the center of
projection and that the panoramic
resolution will be the angular pixel
density expressed in units of pixels per
degree as determined by the focal length
and so here's some formulas for some of
the different kinds of sampling of the
3d space and there's a few calculators
here determining the resolution of a
perspective lens resolution of a fisheye
lens
where's resolution of a given panorama
neither cylindrical or equal rectangular
spherical or cubic there's the field of
view of a window at a picket resolution
size of a window you need the number of
pixels for that you need for a given
angle ur resolution so this this is a
way that you can compare different
representations and one of the things
that I was looking at was what is the
break-even point between a cylindrical
and a cubic projection and turns out at
about one hundred and twenty four point
eight degrees they use exactly the same
number of pixels once you start
exceeding that in a cylindrical panorama
then you're going to be using a lot more
pixels than you really need so anyway
you can take a look at that at your own
leisure
okay so what does it mean what does
focal length really mean it turns out
that there is as I mentioned before
there's this intimate relationship
between the focal length and the
panoramic resolution well I didn't
mention the panoramic resolution but
with the resolution in pixels per degree
so I'm going to claim that focal length
even though it's given in millimeters is
actually millimeters per Radian and I'll
get into the reasons why that is so if
you have a sensor that has a certain
number of pixels per millimeter and your
focal length is a certain number of
millimeters per Radian and you convert
the radians to degree then your focal
length is directly proportional to the
number of pixels per degree you double
the focal length you double the number
of pixels that the angular pixel
resolution of your image and the reason
why I like this result is it gives us an
idea of how the pixels on the imaging
plane relate to rays basically in
3-space just by thinking of the the
focal length as a particular distance to
the imaging plane isn't doesn't quite
give you that that feeling now let me
show you why the focal length is really
why it really should be considered to be
millimeters per Radian here is a
triangle from elementary trigonometry
the tangent is equal to the opposite
over the adjacent so the opposite is H
over 2 the adjacent is f this is the
focal length focal length to the imaging
height so that tells us that the tangent
of PI over 2 is H over 2 F or if you
start getting close to the center when
this angle is smaller then we can
approximate the tangent the angle by the
angle itself so that means that V over 2
is H over 2 F or if you work it out your
focal length is approximately equal to
the height divided by
the angle as the height approaches zero
so that yields directly the fact that
the focal length is so many millimeters
per Radian now radians are actually sort
of Nan units they can they can sort of
come and go as they will they aren't
really natural but but this derivation
shows that it does actually sort of fall
out and tells us the intimate
relationship here between the focal
length and the pixel density so in
conclusion I have taken you on a little
bit of a journey showing how your pixels
got to where they were by going through
the lens having different types of
aberrations and the lens the the
diffraction and the wave nature of the
light and then that the pixel sampling
density is not necessarily the same
thing as resolution even though
sometimes we will call them the same
thing resolved ability might be a better
word than resolution - to distinguish
between those two you probably shouldn't
really use more pixels than you actually
need for an image otherwise you're
wasting the bytes it may give you some
better quality images on some devices
but you can actually you can always
expand them analytically or
computationally to generate more pixels
you don't really have any more detail by
having those pixels so you may as well
toss out those additional pixels the 70%
rule sort of fell out of the Bayer
sampling notion and it's been proven at
least experimentally here that it
actually does hold up and the final
thing is that that the focal length is
basically the resolution or at least the
pixel sampling density so if it got any
questions Jeff
disability kicks intensity but higher
populated often be too smart and abused
when in requisition you will often one
compromise between
that's true the comment was that when
you increase the focal length then
you're also decreasing the field of view
if your imaging device is the same size
and so yes if you if you hit if you
double the focal length and you probably
need to capture twice as many images in
order to in order to capture with full
360-degree panorama that's right any
other questions
they are instructions depend a lot on
the beirut instruction however that
that's true so so probably the the
comment was that the the seventy percent
rule the seventy percent conclusion that
came from the the bayer sampling pattern
is a function of the type of
reconstruction that you use so yes
typically one point four is probably the
best possible case that you can do i
doubt whether you could increase the
resolution any more than that i don't
think it's i don't think it's possible
due to the Nyquist sampling theorem and
typically your reconstruction is a lot
worse than that
dick maintains that it's something more
like one point eight instead of one
point four so you do lose a fair amount
that's right
any other comments Corrections maybe yes
the reason that it's kind of humor in
the upper right hand corner is because
it is a square sampling grid and those
frequencies are now sent for that route
to instead of good point
Lance's comment was that the reason that
the spectrum didn't show very much
energy in the upper right hand corner
was that the diagonal frequencies are
actually being sampled at at a pixel
sampling density of one point four
pixels instead of one pixel and so
they're inherently going to have less
resolution in that direction so make
sure you get one of these if you're
going to be doing any any work in optics
I've really enjoyed it I'd be interested
in seeing what what Edition 3 looks like
of the modern optical engineering book</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>