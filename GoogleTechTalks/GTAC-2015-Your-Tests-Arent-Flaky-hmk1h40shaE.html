<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Your Tests Aren't Flaky | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Your Tests Aren't Flaky - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Your Tests Aren't Flaky</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hmk1h40shaE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">&amp;gt;&amp;gt;Yvette Nameth: And this is Alister Scott,
talking about a very sensitive subject.
&amp;gt;&amp;gt;Alister Scott: Thank you.
Hello, there.
I'm going to be using the &quot;F&quot; word a lot in
this talk.
Like, a lot.
I apologize in advance if I offend.
You all know the &quot;F&quot; word, don't you?
It's also known as the &quot;F&quot; bomb.
We test engineers use it all the time.
Yes, that's one.
Flaky.
So last year, I went to GTAC in Kirkland.
And like a lot of other people there, I was
very shocked by the language.
Every second talk was dropping the highly
offensive &quot;F&quot; word.
Over lunch, we were jokingly calling it Flaky
Testcon 14.
There were lots of talks about how to reduce
it, ways to minimize it, and what to do when
you have a flaky test.
Flaky tests are something that most of us
experience almost every day in our lives.
They cause us no end of despair.
And we should be afraid, very afraid.
The concept of flaky tests has the potential
to bring our testing profession down.
There's been lots of solutions proposed around
test design and test resilience.
But we continue to suffer from this problem.
The solutions don't work.
As the &quot;F&quot; word gets dropped more and more,
people start to doubt whether any tests are
reliable and if testing's worth it at all.
What if testers are flaky, like their tests?
We need to kill the term &quot;flaky.&quot;
So I got back to work after the conference,
and I was sitting at my desk, and he overheard
a developer dismiss a failed build as, &quot;Oh,
that's just some flaky tests.&quot;
I was shocked.
I was -- I was shocked and offended.
The &quot;F&quot; word, an excuse to ignore testing.
People were talking about our work and now
using the &quot;F&quot; word.
So I swore to myself from that moment forward
that I would not dismiss any issues as flakiness.
I would stop calling our tests flaky.
But if our tests aren't at fall, what is?
Our applications.
You see, flakiness implies that it's a test
problem and that no one needs to worry about
it from an application development point of
view.
But what if it's not the test's problem?
What if it's our app's problem?
Developers have started to use the term &quot;flaky&quot;
to describe any failing test.
Imagine how useful a flaky smoke alarm would
be.
It would go off very often.
And people would be saying, &quot;That smoke alarm,
it's just flaky.
Let's just ignore it.&quot;
You may as well not have a smoke alarm.
You may as well ignore all testing.
Why have testers?
Why have test engineers?
And like that, our profession became under
threat.
So how do we get to the bottom of flaky?
Well, imagine you had a toaster that was sitting
directly underneath your smoke detector and
it was on a very high setting.
Is your smoke alarm still flaky?
Or is the thing that you're measuring not
quite right?
We need to get to the bottom of it.
We need to look at the whole story and stop
using the &quot;F&quot; word.
Let's look at our applications.
So not too long after this happened, we had
another issue with inconsistent or flaky tests
at my work.
So we had a whole bunch of acceptance tests,
and they gave us really good confidence that
our app was ready to release to production.
And we do that very frequently.
And we'd never release our app unless acceptance
tests had passed.
We had about 500 tests, and we ran them in
parallel in about 50 groups, each having about
ten tests.
And it meant that the entire suite could run
in as little as ten minutes for the entire
suite, which was pretty sweet.
And so we wanted to get a build released.
And five of our 50 groups failed on the first
run.
So what we did was, we reran those five groups.
And I think then one group just failed.
So we reran that one group.
And it passed.
Our build was green, and we released it to
production.
We didn't ask why our tests had failed in
the first place.
So shortly after release to production, we
started seeing our customers were losing their
sessions and crashing.
But all our tests had passed.
We had a closer look and found that we had
an obscure location bug where the display
mode between desktop and mobile was being
incorrectly cached and displayed for the wrong
user's session.
This was killing the user's session and it
was affecting our sales.
It wasn't immediately obvious, as it wasn't
happening for every customer.
It was only happening for about 10%.
Five groups out of 50 is also 10%.
Our tests had found this bug, but we'd completely
ignored them.
We'd ignored them until it didn't happen anymore.
But our tests knew that we had an issue.
Trust the tests.
Don't rerun blindly until you get the response
that you want.
Find the issue and fix it.
Kill any possibility for flakiness.
We had another situation with an app that
I was working on.
And we were running some tests in parallel,
because running tests in parallel is very
efficient and replicates how our users use
our systems.
We encountered some flakiness or inconsistency
in our test results.
Our app happened to have a feature that we
didn't know about where subsequent login to
the first would destroy the original session
in some cases.
So if two of our tests happened to run in
parallel at exactly the same time, the second
test would make the first one fail.
Spending time trying to make our tests resilient
or reliable would be fruitless.
It was built into our application design.
Our app should have been testable, and it
shouldn't have killed the subsequent sessions.
That wasn't meant to happen.
So rather than going on a flakiness hunt for
our flaky test, we refused to believe in it,
and we discovered application quirks that
were disguised as test flakiness.
Our faith in testing was restored.
Our tests had revealed the true story.
Thank goodness it wasn't a flaky monster under
our bed.
We had shone our torch, our trusted test,
on it and realized it was just some dusty
bits of our application design.
There was nothing to be afraid of.
We had a single-page internal Web app that
I was working on, and it used dialogues mon
dialogues, with popups and asynchronous server
calls everywhere.
Our tests would dismiss a dialogue, and it
would miraculously reopen when a callback
was later received.
It would make our tests very flaky.
What we could have done was make our test
resilient as to accept -- as to expect a random
popup at any moment and dismiss it.
And I have seen that done.
But why should we write such complicated tests?
What's the real issue?
Why should we have highly resilient tests
for such a clunky application?
Shouldn't we look at the reason behind the
flake?
We decided to work as a team to make our app
more testable, to show spinners for AJAX calls,
to make dialogues not miraculously reappear,
and to provide hooks saying you could know
when asynchronous stuff was happening via
JavaScript.
We did this.
And guess what?
Our tests were suddenly consistently passing.
Flakiness, zero.
Application testability, one.
I worked on another app.
And it was to order pizza.
It was a multistep flow.
And every step relied on some state from a
previous step.
If we were to write tests that included every
step and any combination of steps, there would
be lots of possibilities for flakiness and
inconsistencies.
Instead, we chose to write a test-specific
controller which was embedded as part of our
application to instantly display any page
in our app with any state that you required.
So there was no navigating of pages and no
constraints around time or data.
If you can set up a single URL with everything
that you need to test on functionality, this
means your tests can be focused on testing
and avoiding flakiness by avoiding navigation
and layers are set up, and options.
We built testability well and truly into our
app.
The integration of testing into system development
saved us from flakiness and complicated after-the-fact
test writing.
Don't be afraid to add testability-specific
features to your apps that don't serve a functional
purpose.
I recently had to get tires, new tires on
my car, and I realized that a lot of tires
have testability features.
They're call tread indicators.
They don't serve any functional purpose.
A tire doesn't need a tread indicator to operate.
But it does make it possible to quickly and
easily test the tread of a tire without any
complicated measurement instruments.
Why don't you consider adding -- investing
in similar testability features for your apps.
These are two of the most annoying messages
and popups I see on Internet sites that are
used.
They are particular to sites that don't cater
well for browser navigation.
So tests for these sites can be flaky, as
it's really hard to handle these popups and
test automation.
They also cause a lot of fear in our users.
Has it taken my money?
Have I double-booked?
How long should I look at this screen before
I refresh?
[ Laughter ]
So there's a lot of user panic.
But what if we built our apps so they didn't
have these problems?
Our tests wouldn't need to cater for them,
and we'd have a better user experience.
How?
Well, we design and build your app with testability
in mind.
An integrated team of developers and testers
throughout the whole process is ideal.
A testable app is a usable app.
And usable apps aren't flaky.
So you might be asking what can I do to help
kill flaky?
We all have a roll to play in restoring faith
in our profession and pushing flakiness into
extinction.
Number one, don't blindly rerun tests, if
you roll a dice enough number of times, it
will give you the number that you want.
But is that the same thing that happens if
a real user rolled a dice?
No.
It's not realistic.
We need to look at why it failed the first
time, why you didn't roll the six straight-away.
There's a reason you didn't roll a six.
It's a dice after all.
Maybe you need to use a different approach
all together.
Number two, use flaky tests as insights into
your apps.
Flaky tests are not useless.
They're telling you something.
You just need to work out what that something
is.
Be the test whisperer and decode it.
You'll be rewarded with a secret about your
app or an area for enhancement.
And, number three, build testability into
your apps.
Flakiness comes from after-the-fact testing.
We are not an afterthought.
Testers and test engineers should be part
of application design teams.
We need to consider and build testing and
testability into our application designs.
Efficiency, effectiveness, and testing confidence
comes from this strong base.
I'd like to finish with a final message to
flaky tests.
What I do have are a very particular set of
skills, skills I have acquired over a very
long testing career.
Skills that make me a nightmare for flaky
tests like you.
I will look for you.
I will find you.
And I will kill you.
[ Laughter ]
Cheers.
[ Applause ]
&amp;gt;&amp;gt;Yvette Nameth: Thank you, Alister.
You have a lot of questions coming your way
so it's a good thing that we've got a lot
of time.
So first one: Isn't it a question of probabilities?
A bug that manifests in a flaky result is
less likely to be seen in production than
a test that always fails.
How do you convince a team member that fixing
flakiness is a priority?
&amp;gt;&amp;gt;Alister Scott: I guess I sort of disagree
because I have seen a lot of flaky -- like,
doing a lot of research into these flaky tests
which I have sort of demonstrated today -- like,
I have actually shown that a lot of these
things do actually come through in production,
we just don't realize it.
Like, so my example of where we had a quirk
in our application where, like, multiple log-ins,
the second one would kill the first, no one
had any idea about that.
It was happening in production.
We just had no idea about it.
Our flaky -- our test was flaky because of
it, but it was an application issue all along
so it was actually in production.
The question's not there anymore.
That's my...
&amp;gt;&amp;gt;Yvette Nameth: How do you convince the team
members that fixing the flakiness is a priority
is the other part.
&amp;gt;&amp;gt;Alister Scott: Sure.
I think a good way to do that is to make flakiness
very prominent.
And so ways I've done that before is make
sure that everyone can see that we've got
a flaky build.
So having things like build lots and build
monitors and things like that, making it really
clear that our build is failing and it is
red, we can't push that.
And then people will start seeing that and
say, Why is it red?
Okay, let's have a look into it.
And then sort of dismissing that, hey, it
says &quot;flaky test&quot; and really starting to delve
into it and looking at it.
And people often say it will cost a lot of
time and money to do this.
But not doing it is also going to cost a lot
of time and money and prod issues and things
like that.
So, yeah.
&amp;gt;&amp;gt;Yvette Nameth: Our team finds that tests
fail due to hardware, OS, network, and job
management problems often enough that it is
not worth investigating flakiness below a
certain threshold.
Do you agree with this strategy?
&amp;gt;&amp;gt;Alister Scott: No.
I think that --
[ Laughter ]
[ Applause ]
&amp;gt;&amp;gt;&amp;gt; Yes!
Yes!
&amp;gt;&amp;gt;Alister Scott: So any one of those things
like can cause a flakiness in our test.
So if you are finding those, we need to actually
make a testability feature that we can get
rid of that flakiness.
So if we've got like -- if we've got an issue
with, like, network, we need to say, hey,
how can we make our app, like, better so it
can run on, like, flaky networks.
Or how can we make our app better so it can
run in the test mode where it doesn't need
to use the network.
Or how can we actually improve our app to
mitigate any of these, like, potential reasons
for flakiness?
If you can -- like, an example, like, we had
all these issues around flakiness around time.
And then so, like, how about we just, like,
make our apps so that we've got a mode where
it doesn't sort of worry about time.
We can test everything else and then we can
test time separately.
Instead of time affecting every single test
that we've got, we didn't worry about time.
And then we could just test in this little
thing over here.
So I guess this is worth looking at any of
these issues that people say, hey, that's
the reason for flakiness and then just trying
to work out how can we use testability to
solve those.
&amp;gt;&amp;gt;Yvette Nameth: The challenge with flaky
tests is that they are very difficult to reproduce.
Do you have a suggestion for pinning these
down?
&amp;gt;&amp;gt;Alister Scott: That's a very good question.
So what I've found to work out what it is
-- because, as I said, I've spent a lot of
time doing this.
Things like logging and screen shots and recording
videos of tests running, these are really
good things.
A lot of CI tools do this anyway.
But there are really good techniques to actually,
like, basically, be that test research, trying
to work out exactly why it's happening.
Oftentimes, like, you can run it locally in
a dev environment and it passes every time
and then every time you run it in CI.
But when it is running in parallel, it will
fail.
So if you can put things like -- turn logging
on, put in, like, debug statements, if you
can capture screen shots every time it fails
or you have a mode that every time it runs,
it does a screen shot on everything it does,
all these different things you can do, you
can use all that information to realize, hey,
this is why it's flaky.
And that's what we did for, like, the issue
where we had the subsequent log-ons.
We're, like, we really need to actually start
looking at the screen shots and the logs and
trying to work out exactly why this is failing.
Because every time we ran one of those tests
locally just as a single test, it would pass.
There would be no way it would fail.
But when we would ran it in parallel, we would
fail.
&amp;gt;&amp;gt;Yvette Nameth: I'm going to add one thing
to that because I have the power of the microphone.
I would also say production monitoring.
So you actually realize this is a real problem
in production.
This isn't just a flaky test.
Actually 10% of users could be correlated
back to this thing in production.
&amp;gt;&amp;gt;Alister Scott: Exactly, yeah.
Like the example I gave where 10% of our users
were completely affecting our sales, like,
that was production monitoring.
It's, like, really important.
&amp;gt;&amp;gt;Yvette Nameth: Okay.
Back to questions.
What about diminishing returns of test stability?
How do we justify X weeks of software engineers'
time fixing something that only affects tests
versus stabilizing the tests themselves?
&amp;gt;&amp;gt;Alister Scott: That's a good question as
well.
So, Yvette, I think you talked this morning
about Google Maps and spending two weeks on
the sort of &quot;flaky test&quot; --
&amp;gt;&amp;gt;Yvette Nameth: Unfortunately I did.
&amp;gt;&amp;gt;Alister Scott: I mean, we already spend
-- like, we spend a huge amount of money and
investment on flaky tests.
We do that already.
So by reinvesting that effort in testability,
I've seen that the benefits are much more
substantial.
So it's not about -- it's not about spending
more money on testability.
It's about spending less money on flakiness,
spending more money on testability -- diverting
that money to testability.
I've seen it in all the examples I've presented.
I have seen it in a team how greatly -- much
-- the team is so much more confident if they
can have consistent green builds and being
able to release to production consistently
without any flaky tests.
It's like a priceless thing to have.
So...
&amp;gt;&amp;gt;Yvette Nameth: Is it or is it not a general
best practice to ship internal testability
hooks as part of the shipping software package?
If we do, aren't we exposing internal tools
to customers that increases the size of the
shipped software but provide no use to the
customer?
&amp;gt;&amp;gt;Alister Scott: So all the testability stuff
that I mentioned in my talk, we basically
through config we disabled it in production
builds.
And I think we've even in staging builds,
we actually disabled it as well because we
wanted dev to be very much like production.
So any sort of testability feature like I
talked about the pizza app, so any ability
to basically, like, jump straight into a customizing
a pizza or in setting up all this different
data, there was no way a user could access
that in a production or staging environment.
So we're very -- and we had a lot of config
tests.
So we're very -- everything was heavily driven
by config, and we came up with a collective
data to have actually like unit tests for
config.
So you could actually -- every time you change
config, you would actually write a unit test
for it.
So we had a lot of confidence, I guess, that
our production config didn't include any of
our testing stuff because I don't think it's
very good -- like from a size point of view
and also from a, like, risk management point
of view, it's not very good to actually, like,
ship testability features to customers.
&amp;gt;&amp;gt;Yvette Nameth: How do you know that you're
validating real-world conditions when you
test a specific controller?
&amp;gt;&amp;gt;Alister Scott: Yeah, so with the test controller,
like, it was full stack.
So it did use services.
It did use a database.
It just was really, like -- I guess like a
setup, like a quick mechanism.
So it was, basically, a way of setting up
state in your application and redirecting
to a certain point so that you could actually
see that.
Saying that, it is a valid question because,
like, they weren't the only tests that we
had.
So I think we mentioned we had about 500 tests,
and about 450 of those approximately would
have used the test controller.
So they are very targeted.
They would just be -- the way the test would
work, it would say, hey, I call this test
controller, I get to where I want, and I do
some accessions.
The remaining 50 are the sort of -- like,
the top of our test pyramid.
They are our end-to-end real-user test.
They, basically, go through from the start
of, like, specifying who you are to order
pizza, selecting your address, going through,
choosing Hawaiian pizza, doing all sorts of
stuff to it and saying, Hey, it needs to be
delivered to my door, and being able to see
a pizza tracker.
We had a handful of those.
We limited them.
There was nowhere near as many.
But they didn't use any of the test testabilities
sort of features because they were representative.
And the benefit of those -- as I said before,
our testability stuff was disabled in production.
Because our end-to-end tests didn't use any
of that, we could actually run those in production,
which we did.
We actually run them in dev, test, staging,
and we actually ran them in production.
Every time we did a production release, we'd
run those.
They would order three weeks in advance so
we could have time to cancel them.
But, yeah, so, we could actually use those
tests.
So it wasn't just -- we weren't just using
testability features.
We were actually, like, complementing it with
real end-to-end testing.
&amp;gt;&amp;gt;Yvette Nameth: Okay.
What about heisenbugs?
Instrumentation changes outcome and prevents
reproduction.
What do you do when your actual instrumentation
and test hooks cause something to change?
&amp;gt;&amp;gt;Alister Scott: I guess we didn't see that
whole amount.
Any of these testability, like, hooks or changes
were, like, completely reproducible in a development
environment.
So it was very easy to run our app locally
in debug mode and actually use one of these
testability features and see exactly what
was happening.
And just running it in debug, you could actually
see the values of everything.
So, I guess, being able to do that, we could
actually iron out whether there were any issues
in that.
But the benefits far outweighed any issues
that we ever had with that.
I have never heard of heisenbugs, but I will
look that one up.
&amp;gt;&amp;gt;Yvette Nameth: Was it difficult to convince
people to undertake this effort?
&amp;gt;&amp;gt;Alister Scott: Yeah, it was.
Particularly -- particularly developers because,
as I said -- like with the question we had
before, like it is a fair bit of effort.
But it is about, I guess, changing your focus.
It was hard to convince people.
But as soon as we actually made some gains
and, like, just taking, like, a single bug
that you have in production and then saying
how can we test this smartly, introducing
a testability feature for it, and actually
doing that.
But doing small examples and proving and showing
how they can be effective, you soon get buy-in.
And then basically, as I said before, as soon
as you have consistent builds passing, everyone
thinks it is the best thing in the world because
they are no longer questioning the quality.
&amp;gt;&amp;gt;Yvette Nameth: I'm going to throw in my
own follow-up question which is: How do you
measure flakiness and sort of follow that
along?
You're talking about, like, you know, showing
proof of concept and then iterating on that.
How do you actually measure flakiness?
&amp;gt;&amp;gt;Alister Scott: I guess the only way you
can really measure it is by every failed build
that you have, looking at it, and then it's
asking whether it's a legitimate fail.
And so, firstly, I guess reducing how many
failed builds you have.
And if you are having builds that have failed,
just look at them and say, hey, is this a
legitimate bug?
And how easily you can determine, how quickly
you can determine that, hey, this is a legitimate
bug we need to fix.
As soon as you start realizing that every
build that fails is a legitimate bug, I guess
that's when you have got confidence that you
have eliminated flakiness.
&amp;gt;&amp;gt;Yvette Nameth: One last question.
What helps you to inculcate -- you guys are
teaching me new words, thank you -- a culture
of testable design within the developer community?
What do you do when they want to cut corners
to release urgent features?
&amp;gt;&amp;gt;Alister Scott: Again, a really good question.
I think it sort of goes back to what I said,
that it is really, like, a long-term view.
And it's just really, like, showing, like,
demonstrating the ability of, like, the effectiveness
of doing this and -- in small amounts and
then sort of gaining momentum over time.
And, like, ultimately, like, every developer
I have worked with has actually been sort
of involved in this.
Ultimately, they love it because it gives
them really good confidence to make changes,
which every developer wants.
It gives them really good confidence to do
a release, and it reduces how many calls they
get afterhours because something's crashed.
So, yeah, everyone I've been involved with
has actually loved this process.
But it's just a matter of, like, changing
that culture over time.
And really the only way to deliver, like,
fast software sustainably is to have, like,
very reliable non-flaky tests.
&amp;gt;&amp;gt;Yvette Nameth: Thank you, Alister.
&amp;gt;&amp;gt;Alister Scott: Thank you.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>