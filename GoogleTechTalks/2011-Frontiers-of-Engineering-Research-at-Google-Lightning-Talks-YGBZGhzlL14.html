<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>2011 Frontiers of Engineering: Research at Google Lightning Talks | Coder Coacher - Coaching Coders</title><meta content="2011 Frontiers of Engineering: Research at Google Lightning Talks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>2011 Frontiers of Engineering: Research at Google Lightning Talks</b></h2><h5 class="post__date">2011-09-28</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YGBZGhzlL14" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">to kick things off I'd like to invite
Franz och who is one of the architects
of google translate so over to France
Thank You amar ok so I want to talk
about google translate just a quick
question for the audience who used
google translate before many hands go
anyone that didn't use it still a few
hands go up ok nobody in your family
you'll sit ok so but it seems that the
majority of people has used it before
negative ones that haven't probably will
after the talk so what Google Translate
does is it's a service that you can use
on the web translator google.com and
various other places and you can put in
some text in some language and translate
in 63 different languages and so here
you see from czech to english and what
you can also do it's integrated in
search so if you are looking for some
information and the best answer for that
information would be written in a
different language then you can just go
to search type in your query in your
favorite language in English and then
when you do translated search it's a
chest which language this might be an
interesting you might get an interesting
answer and then you basically get search
results in that language where
everything gets translated to English so
the title would be translated into
English and the snippet would be
translated into English and when you
click through to that search result
local the web page of that restaurant
for example would be translated into
English another use and we have it
actually in many different products at
Google integrated now that you use that
we have is for example for YouTube so
Obama puts up his weekly address to the
nation on YouTube and it comes with
caption information that's in English
and then someone can just say oh I don't
know English I want to translate that
into Arabic and then basically the
captions for
that will be translated into Arabic and
and you can wait if I'm Arabic speaker
you can you can see what is set there
and we have it integrated in chrome and
and various other products so the idea
is that whenever you would run into the
language barrier that you don't
understand some text or they will find
one to answer a question and the answer
for that question would be on a
different language website whenever you
run into the language barrier the goal
here is right technology will help you
understand what's there so the approach
we are doing is it's different from what
you probably might think of how this
could be done so machine translation is
a very old field people originally have
used a lot of rule based approaches
where you have linguistic experts at
write down how to analyze a language and
how to transform it into a different
language and how to formulate then other
words there in the sentences and the way
we are doing that is is just the Google
approach of using very large amounts of
data massive amounts of data from web
from books and other material and then
basically with fancy magic machine
learning we learn from that how to mimic
what humans do when they do translation
and so as a result for most of those
systems that we've built so 63 different
languages now nobody in the team
actually speaking most of those
languages but it's still possible than
with with machine learning to basically
build systems that way so this shows
over time how many languages we had so
we basically edit most recently very
small languages at least small with
respect to representation on the web
very large number of speakers we added a
bunch of Indian languages bengali in the
tamil Gujarati and those languages so
now we already we have actually over
ninety-nine percent of the languages
covered that you find on the web with
respect to the material in which they
are printed you so
one important point for for the way we
are doing machine translation is
basically data the more data we feed the
system the better the translation
quality gets so here you have a plot
where you see basically for a bunch of
different languages on for 1 million
words on the left to 10 million words on
the right how translation quality
improves on the vertical axis you see a
metric which reflects translation
quality and in that metric roughly
basically if you have point2 in that
metric that means you have a reasonable
translation quality that we are
basically willing to expose a system
like that to our users and obviously you
want to have the metric is bound by one
point 0 so whenever as large number as
possible so with 1 million words right
very few languages are good enough 1
million words it's not too much a book
probably something like 200 thousand
words but with 10 million words right
many languages already cross that line
and then if you go 200 million words a
translation quality becomes even better
and then if you go to a billion words
basically all language is a well above
this point to line and and again so
basically data is a key in order to make
this approach work and work well so so
as data be used typically data that red
comes from the web or from books to
learn the translation but there's
basically other kind of data which is
the users themselves in some sense can
collaborate in order to make a system
bedroom and that's actually active
research to use that kind of data but
the idea is that by users using the
system and interacting with the system
in this case basically users can see a
translation and if it doesn't like the
translation you can click on the words
and get a list of alternatives and then
say oh this one is actually the correct
one and then by that edit that
translation but also provide a signal
point that helps them to improve the
system closure so if later on a
different user would would see it's less
likely to get a similar mistake and so
some of the developments
the futures are something that we are
working on currently and I think very
very exciting is basically then to
combine this with speech recognition and
Texas beach and speech-to-speech
translation if you own an android phone
you can download the google translate
android app where then you have google
translate integrated in there in this
app and you can actually then speak into
that app and for english to spanish you
can also have a conversation than with a
Spanish speaker back and forth
communicating and basically one person
would speak into that and the other
person can respond and always basically
gets mediated with speech recognition
and translation and so this is one of
the this is we have this out as an alpha
version so obviously a lot more work
needs to be done to make their quality
really good and and what it requires is
really good speech recognition quality
really good translation quality really
good text to speech and that's a good
way to hand over to Vincent you will
talk about speech recognition okay
thanks France and so my name is Vincent
van you become a member of the speech
technology team here at Google and I
thought I'd try to share with you a
single piece of insight that we gain
from working on speech technology at
Google scale a lot of the machine
learning that we do and really uses
humans as the benchmark the golden
standard both in machine translation or
in speech recognition human performance
is what we're striving for and as a
consequence if we you ever considered to
you know how would we do super human
speech recognition or super human
machine translation we could simply not
get there because humans are the
standard so the first step might be to
first get rid of all the humans you
might wonder why I'm talking about super
human speech recognition right now and
we all know speech recognition right now
is not at the level of human performance
the problem for us researchers is that
it's getting dangerously close and i'll
share with you
experiments so I work mostly on a google
voice search it's an application that
you can have on your phone it's
integrated in android it's also
available on iphones you can just speak
a search query and the result pops out
people love it it's a great user
experience a great UX and we want to
make it completely magical and
transparent so that everybody can just
speak into their phone so imagine that
you're talking to voice search
reassuring you query we record that
query and then we played back we played
back either to yourself or to the speech
recognition engine that's in your phone
or that's in the network there is no
question that you will do a much better
job at returning the right search result
then the speech recognition system might
do listening to yourself speaking you
will have no problem speech recognition
is not yet your level now if we change
the experiment a little bit and we
replace you with another person another
human being somebody who might not know
you who might not have the same cultural
context the same not live in the same
place or not not really have the same
background then that becomes a tacit and
the knave human will not have any
significant edge over the speech
recognition system why is that roughly
really quickly speech recognition as two
parts there is an acoustic model and a
language model the acoustic model is
what translates sounds into phonemes and
that is not nearly as good as humans
right now the other piece is the
language model and the language model is
what really predicts words and their
pronunciations and how people might say
them that model we train on really large
amounts of data so for the voice search
system at Google we train it on 240
billion words and those come from
billions of users see huge amount of
knowledge that's embedded into the
system so any any human being that might
be listening to that audio might not
have this vast amount of knowledge at
their fingertips if if a person is not
from Canada they might not know how to
spell Saskatchewan
if a person is not from New York they
might not know how to pronounce
schenectady new york and i'm probably
pronouncing it wrong myself so this is
this is where the system has an edge and
and it's a huge problem for us humans
are really what we measure against so it
puts a upper bounds they put a upper
bound on the performance of our system
and there is also the issue of privacy
we definitely don't want to have humans
listen to things that we might actually
want machines to transcribe and do a
good job at transcribing and there is
the issue of scale we receive right now
about two years of data two years worth
of data every single day and that in 27
languages we can't even imagine trying
to transcribe all that data and so how
do we leverage it if we don't have
humans annotating it that's the that
that's the fundamental issue that we're
facing so Fred jellineck is a famous
researcher in speech recognition he used
to say that every time you fire the
linguist performance would improve so
I'm just trying to take it a little step
beyond the key to make it work is this
very simple idea in speech recognition
as I say we have those two models we
have the acoustic model and the language
model and they work in tandem they
combine together to produce your results
and they sort of compensate for each
other's deficiencies right so the
classical setup that we have is we have
the truth that's determined by a human
transcriber we have our system and we
compare that system to the human to
evaluate how well we're doing and to try
to push the system towards the
performance of a human transcriber we
don't have a human in the loop we're out
of luck we don't have anything that we
can measure against we can't improve we
can cheat we can cheat by taking parts
of a system and artificially weakening
it and making it harder on that piece of
the system to work so what's going to
happen
now it's system that's worse than the
best system ever we can have and we're
trying to push the system and push the
rest so if for example here I'm making
their language model worse and we're
going to try to make the acoustic model
compensate for this deficiency so as a
result hopefully the acoustic model is
going to get better now we can switch
the roles make it harder for the
acoustic model to work the language
model will improve now we have a better
acoustic model a better language model
we can combine them together we have a
better system a role we can iterate and
keep going so you'll notice there's been
no human involvement whether in training
or in evaluation or in any of the steps
that we do to actually run the machine
running pipeline this is in general in
the car in the around in an idea around
and supervised training and surprise
training itself is not something new
it's something that researchers have
been doing for a long time what's really
new it really exciting to us is that
we're not only training in a completely
unsupervised fashion we have a complete
development cycle that doesn't involve
any human supervision any human
interaction and can and as a result it
can scale to a much larger amount of
data and larger much larger systems the
hoop there is that by removing this
asymptote by removing the ceiling that
we can never reach and we'll finally
maybe be able to reach human role
performance and with that I'll just give
the floor to remco who is told about OCR
thank you
okay googles mission is to organize the
world's information and make it
universally accessible and useful and
that includes of course books but
unfortunately OCR is not available for
all languages or it's not good enough
but we feel very in a good position to
basically build our own system now OCR
hasn't been heavily researched as it has
been in the past but other areas are and
for example you want to take advantage
of the developments in speech
recognition and in machine translation
and of course you have a massive amount
of data that we also want to use our
initial goal is to focus on google books
but ultimately we want to be able to do
Universal LCR meaning that you give it
any image with text them will recognize
the text and the goal of the team is
basically to create the world's best oh
she are for all scripts and languages
now that sounds easy right well maybe
not actually LCR is a non-trivial task
there's many of course the scripting
languages but also fonts and styles and
sizes some languages have really large
number of characters other languages you
know go from right to left and you can
even have left to right within the same
sentence then there's languages that
have characters that look very similar
and of course I'm not even talking about
the layout problem in fact what I'm
talking about here and what we're
focusing on is just recognizing an image
that has a single line of text so what's
the approach well first of all we're
starting from scratch we're not taking
an existing system and adapting it or
trying to improve it we're starting from
the ground up and the basic idea is to
use multiple models including hidden
Markov models language models and
template based models and combine it in
the log linear space now Vincent didn't
mention it but his acoustic models are
hidden hidden Markov models and he also
mentioned that he's using a language
model and in fact there's a lot of teams
out there that simply use their speech
recognizer to do OCR now we want to be
able to make different design decisions
for our system so we're not but it
doesn't mean that we cannot take
advantage of the technologies that the
speech team is developing so for example
the technique that Vince just talked
about in terms of doing the unsupervised
training we can use that to and for
example on the language model site the
machine sensation guys they used it
massively distributed language models
and we want to be able to use that too
and of course the whole system is going
to be data-driven now talking about data
we're actually a very nice position for
OCR because you can generate our own
training data and they can't do that for
speech for example we can print text on
paper and it's kanabec it and voila we
have training data so that's very nice
but in fact what you can also do is just
simply create images from tax directly
and then the store it a little bit to
make it look like it was scanned and
then of course the sky's the limit in
there but you can do but that day
they're still not going to be as good as
data from actual books but if you can
use the synthetic data to create a
system that is good enough then you can
take it from there and do unsupervised
training with it so the idea is
basically you take text create synthetic
data train your system and you know it
will pop out the model and then assuming
that the model is good enough use that
model and scan for real books to improve
the system so that's very nice so how
have you been doing well we've been
making good progress you're still a very
young team this slide shows the
character error rate as a function of
time and the timeline here is starting
in june of this year the task here is
recognizing text lines from a variety of
books which means there's a variety of
font sizes and fans etc there's two
lines one for chinese and one for
english not as a nice curve going down
but of course you know the cynical view
is that if you start up with arrow is
high enough then it's easy enough to
make progress nervous you know is very
true but what is nice or for english
we're now actually low enough that we've
sort of reached the state of the art
performance at least for this test set
but as you can see for chinese we still
have a long way to go now i want to show
an example this is ty and the top line
is an image from an actual type book and
then there's two lines below it that our
recognition results 1 which is called
other is from a commercial OCR system
and the lower one is from google now
find me the difference it's actually not
that easy uh certainly if you don't know
ty but that maybe you found them there
are five differences where the other
system and our system differ and let's
zoom in on one difference so here it is
now you know you can sort of see that
that little book that little circle in
the middle of the arc is sort of
attached to the arc differently and you
can sort of imagine that if you look at
the image part that if you get a little
bit more noise that it will be
impossible to tell the difference now if
you actually look at these four cases or
five cases where there's a difference
than four out of the five we do better
now of course you know this is very
carefully selected example but
fortunately you know there's more of
these kind of examples what does also
examples where you completely fall apart
but I didn't think that that would
impress you but the thing is all kidding
aside there's actually a much bigger
story here and that is that this tie
system we build in approximately 12
hours from scratch so I think that's
very nice so what did we do well first
we downloaded 25 megabytes of Thai texts
from wikisource and then you simply fed
it into the training pipeline and out
pop the model that gave the result that
I just showed the training pipeline
involves training a language model from
the text training or generating
artificial data from the text then train
the optical models from that synthetic
data and then finally tuning the system
automatically on some hell that data so
you know there's and that's basically it
there's no real human involvement at all
I mean you basically at the diner today
then you feed it back in and then that's
it now this does not give you a
particularly good system it's up optimal
it has very small language model it has
very small optical models and there's
actually be used very few funds because
I didn't want to spend the time
searching for for a lot of Thai phones
but that's actually very important to
have a good variety of phones because
the system needs to see sort of a
critical mass of variety to be able to
generalize well we didn't use any
unsupervised training and we don't
actually have any transcribed data
because he of course that requires
humans and so we don't actually know how
well the system works but you know you
saw the example and it's not too bad so
what we currently working on
well synthetic training data we just
really got it to work but now there's so
many things you can do and we're still
exploring that realm we're working
unsupervised and discriminative training
discover the feature extraction we have
somebody work on the belief networks and
you got some promising results and we
want to scale to more languages
currently working on five languages
Arabic English Hindi Russian and Chinese
but we want to skill to 25 languages and
that will basically ensure that we
cannot optimize for any particular
language because there's just too many
of them so the only short of you remain
sort of language in independent so to
summarize OCR is non-trivial problem and
it has not really been solved at least
nothing in general we can take advantage
of the latest technologies coming out of
speech and machine translation we can
generate our own training data and then
use that in conjunction with
unsupervised data to actually take
advantage of the huge amounts of scan
books that we have you know we're still
a young project but if you're making
great progress and for English we've
sort of reached state-of-the-art
performance but there's a lot of work to
do so that finishes my talk now next is
going to be some cool demos from my
colleagues j &amp;amp; duck so and they need a
little time to set up so please stand by
thank you all right I'm gigantic I'm
going to be talking about machine
perception and particularly with
emphasis on what large data sets can do
for us so let me just define what i mean
by machine perception here when we talk
about machine perception as a high level
i am talking about audio visual analysis
so we do computer vision machine hearing
video content analysis and some cool
work on learning models with inspiration
from actual biological visual models at
a high level the technical problems
spanning the space of machine perception
can be divided into three sort of main
buckets the first one which we call
semantic interpretation is basically
coming up with knowledge the way humans
understand knowledge right so when you
are given an image it's a describing
image in a way that the human can
understand that would be semantic into
rotation the other ones are when you
have an image video or a piece of media
we'd like to find stuff that is similar
to it and you see more demos of this
later from Doug and the third one is
when you have lots of data can you
actually create new images video audio
that would sound cool or look good in a
given context for remainder of the talk
and actually going to just focus on
semantic interpretation so we will be
taking some case studies which are
around problems that require us to
generate human level knowledge but
before I do that let me first pitch what
what scale I am talking about every time
I say large scale right here are
examples of some public numbers on what
we mean by square so YouTube gets about
48 hours of new video uploaded to it
every minute so for the 10 minute that I
am going to talk there's 20 days worth
of video right let me emphasize on that
more in a week that's equivalent of
240,000 full-length feature films right
in about a day we get eight years of
video right so you know you do the math
and pitch these numbers with relative to
other large numbers that we would have
known like they get more more video on
youtube in a month then the three major
media houses in the US have published in
the last 60 years right and so we are
talking about writing algorithms that
are going to work on this scale and
hopefully my knowledge that we otherwise
could not do right and this fits in very
well with our mission which is organize
all of words information and make it
universally accessible my favorite quote
from there is that most of the words
data lives in pixels today right how
much information is there in pixels that
is a function of how well we can do our
job in parsing those pixels okay so i
will be talking about three case studies
along with related demos about two main
themes right one is what i call
information fusion is when you have data
sitting in separate corpora and if
get smart about how you integrate
information across these corpora you can
extract a whole lot of knowledge and we
take case studies of celebrity
recognition in place recognition from
there and then part two I will talk
about how you can leverage crowds
tagging stuff to learn knowledge also so
going to the first piece let us type now
try this crazy experiment I go through
all the web pages that are out there and
you know pick someone up from the
natural language processing line and say
give me a good named entity parser right
so parser that will tell me this text
phrase looks like it might be a person
named right and I run that over all the
web pages I have access to and I start
counting how many times the text phrase
came up and was you know was called that
it might be a person named after i am
done counting right i have some measure
of reliability something that occurred
50 million times is probably likely so
then i take these phrases and i go to
image search and I query image search
with those phrases right and I get let's
say a few thousand results back like the
phrase might not have been a person
named all the images that come back
might not be from this person but I have
some some notion of relevant knowledge
that is building up here right then I
can take this this giant list of people
names and images that came back and do
some fancy machine learning that would
filter out noise and you know get me
good discriminative model creation in
the end I can learn automatically models
for all of these people without ever
having to specify that this is a person
and this is how they look like right and
later when a new video comes up on
YouTube I can run these models and
without having to enter any text I can
basically say I know the people here
because they appear so frequently on the
web and I have these images that they
appear with and I can reliably identify
that this is the person in this is how
they look like and so here's an example
of an actual system so what I'm showing
is the last piece here where assume this
was the video uploaded
on youtube and this is actually a
collection of youtube videos that i put
together where the system is run but
you'll be said when they look at the
record enter a child how to read I know
the human being and fish can coexist
peacefully I think we ought to raise the
agent with juveniles can have a gun you
should go back to yatim this upon the
shore sure to event to it I was going to
the park to play with my friends I drove
my ball there my mom sent me to the
store I take my ball with me all
changing experience I suicune amazing
beautiful invigorate what's different
this is martin scorsese the top of his
game i can remember the fact that these
are people is completely automatically
discovered to kick ass course a similar
you on screen image and if you read
carefully the name said James Bond well
that's a side effect of data right right
so if this is not cool enough we can
take it a step further and now that we
have a reliable system that can tag this
on video people talk in a video right
and so every time I have a good belief
that it is it was a James Bond speaking
right I see lips moving I see the head
track correctly and then i can record
the audio and say let me try and find
patterns in the audio that might be
typical of James Bond right and using
this system i can now completely
automatically bootstrapper training
process for how people sound like right
so i started from what was written
written web pages on text i went to
image search I went to video and now I
can build a model for how people sound
like and they actually did that and then
you can you know it's an independent
source of information the errors are
uncorrelated and then you can come back
and Q's both the models ensure that the
end product is better than either one of
the two individually right so it's an
example of how powerful this concept of
information fusion can be when you're
sitting on multiple types of data data
that have plausible relationships
between themselves and the concept can
actually be extended it is not specific
to people or celebrities we can do it
for places so using the same principle
we built a landmark recognition engine
for more than fifty thousand nine works
in the world you know covering more than
100 countries again completely
automatically no one specifies what the
landmark might look like what we look
for is to restore you know web pages
that are that seem like they are talking
about tourist tourism related stuff
right so we look at that we look at
pictures that are uploaded to us and
marked as public which are from these
places the GPS tags we basically put
them together for all the text phrases
that we mined from the tourists sounding
articles we follow the same principle
the query image search and gather you
know a few thousand images and we have
these photo sharing websites like our
website panorama or picasa public albums
right so we put these sources together
then run some fancy image matching to
say you know for all these images I
really know that this part of the image
is common with that part of the image
and even we can do that reliably even
when there is 3d angle rotation so after
doing all of that I can filter out what
is you know what are noisy hits and come
up with true possible models for what
these famous places are right and so
using the same principle very easily we
went we just switched our matching model
from matching faces two matching places
right and using the same principle we
automatically built this very large
scale landmark recognition engine so
that was information fusion another very
powerful source of data that we are on
is crowdsourcing right so on a property
like YouTube let me repeat Frances
question how many of you have used
YouTube yes so you know people give us a
lot of information on YouTube people tag
videos they do all kinds of activity
is they leave comments right you can
imagine that that body of work put
together should you know should give us
a lot of information to create some true
knowledge out of it and so we attempted
to do that in the first level using
crowd-sourced tags as a source of
information so for you know if a tag
like car crash has come up 5,000 times
on YouTube videos it probably means
something and given the fact that it
that the tag has come from different
people it means it talks about a concept
that people seem to agree on right so we
do the following simple experiment we
look for all tags that have some minimum
number of videos with that tag we
extract some general sets of you know
computer vision based features both
audio and visual from from the video and
then we set up a machine learning
problem to predict the tag given those
features right and so up it is you know
a pretty simple straightforward thing to
do and we then come in and say that now
that I am looking at a really really
large space of tags let me see if I can
recover some real-world compositionality
from here and so we do the following
experiment we ask these classifiers to
train against the tag verse is not the
tag and then say how many of these
classifiers were better than a certain
measure of accuracy right and so we call
them you know the good classifiers
versus the not good classifiers so let's
say we tried with just ten thousand
classifiers and let's say a thousand of
them turned out to be good right so what
we do is we take those thousand
classifiers and say I'm now going to add
them to my knowledge base which means i
am going to take the results of those
classifiers now as features and i'm
going to ask the same question again can
we now learn the other classifiers
correctly given that we have the
original features and these thousand
classifiers so if i ask this question it
turns out that layer over layer we can
learn twenty to thirty percent more
classifiers using this process and we
can learn this only because concepts
depend on others
right and we actually trained up a five
layer model and we went to layer 5 and
inspected you know why is it that some
tags were only learnt in their five and
we inspected some music genres to test
out and it turned out that the iteration
five music genres where what was
considered by the key PDS complex or
more evolved music genres and the
sources of inspiration for those genres
happen to have good correlation with our
dependence tree in this layered model
and there's another example of you know
a place where we recover some
information that is close to how we
think of set of the world and another
peculiarity of human learning is that as
we learn more it gets easier to learn we
need fewer examples so we showed here
that you know after you train all these
tags if you take this collective tag
space as a feature space then which 10%
of training data you can learn a better
classifier then you could with the
original low level feature space right
and so we are basically moving up in
representation from low-level audio
visual features to these semantic
features right and again learnt
completely automatically humans were
telling us that these are dimensions of
knowledge that we care about by the fact
that we take them frequently right and
the fact that we go into that space we
can do this on a completely unrelated
problem so this graph basically shows a
binary classification performance on a
problem that was not part of the tag
learning set up that we originally
started with and here's a little demo so
this is basically some training dump of
intermediate results and I believe I'm
on the tag car crash right now so you
see things like
world if I
and let's see sure animation is easy
it's two airplanes Oh awesome well trust
me an airplanes work I've seen that all
right I'm not going to try
but what we can also do after we get
this basic track classifiers is to say
that the very fact that we created this
dependency tree and we exploited you
know this inherent correlation between
tags we can say that not all tags are
independent right which means there is
some structure to the space of tags and
what we can do is once we have the
classifiers for these tags we can
measure the correlation between these
classifiers is a data-driven way of
measuring how correlated the original
tag spaces are right and we can
essentially use this process to cluster
tags and automatically come up with
higher-level concepts right and so we
have done this in several iterations if
you go to youtube.com slash browse you
are going to see a taxonomy of videos
that you can browse by and many of those
categories were automatically learnt
bottom-up through this clustering of
tags and you know here are a few
snapshots of what that looks like so in
summary I basically talked about two
ways of exploring information when you
have very very large data sets in order
to create knowledge one was information
fusion where you go across multiple
modalities of data and second is
crowdsourcing where you use humans
giving you independent knowledge to
create a very large knowledge base and
that's about it thank you
hi my name is Douglas Eck and if you
haven't figured out there's a theme here
I'm also from google I've got the last
ten minutes here for you get to talk
about a little bit of music and more
specifically let me bring in some full
screen sorry about that we're going to
look at the problem the specific problem
of starting off with an audio waveform
and moving through acoustic features
into some space where we can pull out
some some musical meaning so the slide
that I put up here shows you know some
waveform some WAV file being pushed
through some features that I'll describe
in a minute and then from those features
we do some some machine learning and
come up with the probability that this
original segment was reggae or that it's
good for jogging or that it contained a
kazoo and if you follow the theme of
what the previous speakers have have
mentioned it probably goes without
saying that we we do this with lots and
lots of audio data and that we train
over lots of words so we're looking at
large scale learning what I want to do
for you now as opposed to walk you
through a specific blueprint of the
system I want to try to give you some
intuitions of what it means to move from
raw audio through some acoustic features
and towards some learn features so I'm
going to play some audio demos for you
the main goal here actually if you want
to take home the research message is
we're doing the best we can to pull up
musically related features from audio in
a way that's tractable and we're using
some nice machine learning techniques
luckily Sammy Benjo for those of you who
saw Sammy's talk this morning what was
this morning right yeah we're working
with Sammy and using some of the same
techniques at different parts of our
pipelines so you've already seen some of
the machine learning techniques I don't
need to talk about that but what I want
to talk about here first my first demo
is just to talk about some simple audio
feature extraction and there are two
pretty pictures up here the one on the
left and the one on the right and most
of these features are drawn from the
speech recognition community in fact so
we have spectra let's see if this leaves
your works is that do anything said of
laser there it is we have a spectral
content here we have something
cap stroke content or MF CCS for those
of you do signal processing and then we
have some autocorrelation features so a
column is is are the features and time'
valves from left to right so the fun
part here is to listen to these things
so let's see if we can make this work
I'm in full screen mode sorry
alright where's my video let's turn just
sitting behind here and quit I think
it's because I switched we're having
demo problems today I thinks because I
switched video modes here and pull up
pull this up a second time it was I
played these over and over all day
compulsively and making sure they'd work
of course I have yeah there it is okay
so I won't spend too much time on this
but it's hard to see on the on the
screen but there's a little red line
that's moving from left to right okay
and what you're hearing is the track
Pink Floyd money and can you hear that
okay so as we're moving through time
what you might say is that we've we've
managed to take a hard problem which is
a audio signal processing and reduce it
to an even harder problem which is image
signal processing ok and what I wanted
to point out in this demo is I'm going
to skip forward a little bit and I want
you to focus on on this bottom part here
these bands roughly correlate with with
with rhythm with the base tempo and then
harmonics of the tempo so as these bands
move up the track gets slower and slower
and as these bands would move down the
track gets faster but we would predict
here if we were using these as training
features is that something happens here
radically with the rhythm of the song
and that it gets quite a bit faster I'm
going to jump forward here and so what
we have is a saxophone solo happening
which incidentally shows up consistently
in these these features up here which
correspond to tambor and as soon as this
sax solo finishes we're going to have a
quite a bit faster tempo shift here
the point being we we can pull out of
even using sort of standard speech
recognition features and adding some
simple rhythmic features we can pull out
quite a bit of structure from audio that
uncovers the underlying dinner lying
musical analyzing underlying musicality
in the signal and to really help i think
is to see here's a walls okay you'll
notice that just play a little bit of
this this is a PM you might have to
bring the volume up on this little bit a
might need to hit play thank you okay
one thing we see is there's no stable
tempo at the bottom there's very
difficult to tap your feet tap your
fingers with a rubato piano piece and so
our bottom features here look very very
different there's none of these bandings
that we see and there's also much much
less full spectral content and different
looking cultural content is it kind of
reset that we're okay okay so where do
we go from here here I think things get
fun I try to give you a basic idea that
we have some underlying feature spaces
that we can extract from audio that will
uncover a lot of the regularities that
aren't there in the waveform itself it
should go without saying well let me
continue on this so where I want to go
from here now is to talk briefly about
sparse ification that is we want to move
from a dense feature space into a sparse
one for various reasons having to do
with machine learning and 1h use linear
models so now what we'll do is use a
very simple k-means clustering algorithm
so we're going to look for k-means in a
multimodal space and we're going to take
as our space one of our acoustic
features and it turns out i would take a
histogram over these cluster indices it
gives us a nice Barth's code for doing
learning downstream but it's I think
really fun to listen to the audio
samples near a cluster so this is an
even more fun demo if you ask me so what
we're going to do is have a look at
three so okay it takes some explaining
imagine we take 20,000 or 30,000 tracks
of audio and we take some acoustic
features in this case some MF CCS
and we run k-means and extracting
something like 3,000 different means and
then we find one of the means which is a
which is a cluster in this in the space
of KF of MF CCS and we ask ourselves
what does it sound like to listen to
that cluster so we simply dive back down
into our audio and going back through 5
second segments in all 30,000 songs we
pull out the closest match to one of our
centroids and I've listened to hundreds
of these and pulled out some ones that
sound fun so what will here are some
some centroids if you will some tiny
little musical words that seem to sound
like maybe some choral singing some
piano and some strange electronic sounds
so that's coral
piano a third centroid that clustered
some electronic sounds and I'm noise
sounds
okay the point of that demo is to show
you that inside of even relatively
simple features there's an underlying
space that's quite complex and gives
rise to a vocabulary of thousands of
relatively interesting sonic words if
you will so then the question is how do
we get from these these sort of acoustic
elements to something that sounds more
like music and to do that we bring our
slide back up to do that they really
don't like jumping back and forth but
there's not much I can do about it let's
just do oh I did quit didn't I well
that's terrible and so um what we'll do
here is take these acoustic features and
use supervised machine learning so we're
going to take these this feature space
having been sparse ified and we're going
to move it through a machine learning
algorithm that will try to learn some of
the regularities and take these
underlying acoustic sounds and map them
on to musical classes okay I'm not going
to talk about the machine learning again
because Sam you talked about a lot of
this this morning instead I'm going to
rely on a demo so what I'm going to do
is ask you to think about the
vocabularies that were available to us
in this sparse the sparse representation
that we just heard and you know they
have a very they have a lot of surface
similarity in terms of acoustics but
they don't have the ability to without
some machine learning and some remapping
to actually cover the acoustic variance
of an interesting class so for example
let me play reggae for you when what now
we're going to take is a model trained
on on positive examples of reggae
mapping these acoustic features on to a
classifier and then we've taken out of
sample data and pulled the nearest you
know sort of the hottest the best match
for rigging so of course I am
attention if you will to the the fact I
hope you'll agree that these all sound
like reggae but the actual acoustic
similarity becomes weaker and weaker as
we move towards having a kind of musical
similarity that's based on something as
high level as a genre okay that's that's
enough I want to make sure I have time
for last slide here so where are we
going with this I think let's see in
terms of research in our group we're
spending a lot of time looking at new
audio features so the examples I gave
you were relatively simple features that
is I have five minutes ok good perfect
awesome those five minutes over so we're
looking at new audio features and
specifically we're looking very closely
at what realistic auditory pathway
models can do for us so if you look at
these bottom plots looking at paying
less attention to spectral based signal
processing models that are popular in
speech recognition and looking more
closely at realistic cochlear models
that use filter banks and stabilization
and also in following using vector
quantization as far as features as I
just mentioned and we've numerically
shown that that that i'll perform some
of these other features but it's a
relatively complex story to tell so in
in a ten minute over if you talk I
didn't want to fall into the trying to
explain what these paths many models are
doing and we've also been looking at a
very very hard problem how can we pull
more long time Kayla structure into our
research for those of you that might
think of speech recognition by analogy
we are looking very closely a very short
time scale structure to make predictions
about things like genre and instrument
and artists but it amounts to taking
music and cutting it up into 500
millisecond chunks putting those chunks
in a bag and shaking it up okay so we're
not paying enough attention
long time scale structure and it turns
out to be really hard to pay attention a
long time scale structure because we
don't have anything like a language
model we don't we don't know really
enough about how music fits together in
long time scale to build tractable
models that don't explode as you move
through sort of all possible
combinations of short time scale
segments so this is a really hard
problem it's a it's a deep problem how
do we do music how do kids handle music
how do they track things in time how to
adults do music and how can we get
machines to do the same so finally and
the last demo that I'll do is to ask to
begin with why care about audio so some
of you may be asking that question
behind this as a desire to build tools
for music recommendation a music
discovery and I'll give you a demo of
that in a moment and you could ask
yourself why not just use collaborative
filtering why why is music special and
as it turns out I think you can make a
convincing case that the way people use
music is special we listen to the same
tracks over and over again we have
favorite artists we have favorite songs
and we tend to recombine those songs and
sequences whether it be listening to a
whole album or building a playlist and
the space of sequences of songs is
immense they're an order of magnitude
more songs out there than movies already
and then when you start sequencing
tracks into playlists and into sub bits
of albums and into dinner party music
and in the music for jogging the space
is so immense that if you want to use a
standard kind of collaborative filtering
approach where you say John like sex
Mary like sex Mary likes why John
probably like why you find that you have
a problem that you can write down in the
same way as a standard sort of books
collaborative filtering problem but that
the sparse the sparsity of the data
becomes a problem furthermore and
related if you think about what it means
to use a particular track I guess this
is a concrete example think of the track
tax man from the album revolver that
might show up on a playlist because you
want it for jogging that might show up
on a playlist because it's dinner party
music maybe you're listening to a
Beatles fest maybe you're listening to
the album revolver if we have two or
three bits of information if we're going
to do good play listing and offer good
discovery we'd better manage to be
sensitive to that okay so my last demo
and I will get in make it in on time I'm
going to give you a last demo of the
work that we're doing that highlights
the audio component of the work we are
going to talk about making a playlist ok
and we're going to talk about making a
play
lists from a seed so the seed artist in
this case is you too and the first thing
I'll play for you is a few seconds of
the similar artists that would be
brought up and some randomly chosen
tracks popular tracks by those artists
to say this is kind of what a standard
collaborative filter would do if you
said I like you to give me some more
music I'll make it so we start with you
two
this was done on a it relatively small
music push in the Smiths
you can argue about that you too comes
to new order or joy division or
something like that in any case a
sequence of sort of kind of artist level
similarity based upon the original seed
track the era of the track now we're
going to talk about something I think is
more interesting what if we started with
two different songs by the same artist
and we wanted to make a playlist from
there okay our metadata for on the song
level is pretty sparse so we're more or
less going to rely on artist level
metadata for looking at similarity we're
going to be thinking about the
difference in the way the songs sound so
what I'll play for you first is a a
playlist that was made based upon a very
laid back track and I'll compare that to
a playlist that was made based on a
really upbeat track that's paying
attention to the sonic similarities
between these songs and other songs and
then I'm done
the kiss my humor what I'm feeling okay
so that's the laid-back one and here's
the upbeat one same artist similarity
and in fact those were drawn from the
same music collection these were drawn
from a music election that we were using
for testing for google music beta for
google and it's the play listing
functionality for that was built by us
so um I'm out of time and I hope that I
gave you guys a few fun music demos to
listen to the take home is that I think
we can we're moving towards trying to
have some uncanny interesting play
listing and discovery tools built around
audio you can see I left a blog posting
that you can look at if you want more
citations and I think now I can stop
being between you and drinks and dinner
thanks so will now have the speakers
come up for questions I Mike mcalpine
from Princeton I'm not sure that this is
something that Google would be
interested in but I'm just interested in
it which is can you ever unmix perfectly
unmix a song like for example so a song
from the 60s where the tracks are not
widely available the individual tracks
not perfectly but source separations
nice problem and there's been a lot of
success and doing so our separation for
music it's expensive to do a lot of
people are spending time on it so the
answer is yes but not perfectly not a
very interesting answer right
hi Tim bar codes from exxonmobil I
watched jeopardy earlier this year and
saw Watson beat the humans quite
convincingly and read the book about I
think it talked about Google Translate
there what is your opinion of that
jeopardy in Watson is that just a
one-off thing is that the wave of the
future was that child's play compared to
what you can do I mean what what what
did you think I I read shortly after the
Watson performance an article that where
people actually took the questions they
were asked to that were opposed to
Watson and put them in the Google search
box and if you consider a correct answer
to be an answer that shows up on the
google top first page then we did about
as well as Watson according to that
website another the real trick is how do
we make it the first answer and how do
we make that answer semantically
relevant something that you can actually
process and like you can actually use
for something something more actionable
and that's a really interesting area of
research providing disqus kinds of
answers beyond just search things ask
yeah def just google then ok i will
follow hi young one from texas A&amp;amp;M it's
amazing so many experts in front of us
we were just searching online and search
for topics and you're in front of I
think we would be Watson yeah the
question first is for the google
translate actually i happen to use it to
translate some of like in my paragraph
into certain different language and try
to fit through one problem i had was
when i ran into technical you know words
for example let me use the word twin you
know tween in mature science understand
that as a special interface between two
grains we
looks very much as similar to each other
so we caught tween interface but when I
put that you know particular words into
a whole paragraph then the translate
comes out is a twin baby so you know
into other words you know into other
language so I just want to see you know
that particular words has to be set in
the thing so describe a different
meaning of that so what's the google it
that right after that into the
dictionary no but so in general to take
into account the context of words in
order to make a good translation that's
that's a key challenge in translation
and if you look at where technology is
at this point then if the context is
very local then with those Engram models
and very large amount of data we can
actually make typically very good
decisions but if the context kind of is
more long distance that it would be
necessary to kind of figure out that
this is now something which is a
specific domain right in a specific sub
domain that word is used very very
differently then then this is a harder
problem and and so this is an active
research area to take into account to
make long-distance inferences to to
infer from the context of a column say
this is for technical people this is a
for like a standard you know just common
language this is a for maybe medical
application you know maybe you have some
sub columns people can select and then
we can define the word it right way so
this is active researcher in similar
ways here the idea would be for many of
those things that you're trying to do
this to a lot of things there
unsupervised that we wouldn't require
these kind of explicit annotation that
right because then we'd have to have all
our training data annotated by domain
but but to do that in unsupervised
fashion that we basically can infer that
all these in all these areas this term
gets translated like this and in all
these areas it gets translated like that
and but machine translation is a hard
problem and I think basically with a lot
of data and applying this data intensive
approach thought of progress has been
made but there's still a lot of progress
to be made in the future actually one
last
no I take many I think many people also
have a similar experience when you're
typing any new this is for all of you
actually went when i type Google I
search for something when I tapping
words in different sequence all tapi and
actually the same question but ask in a
different way it turns out the answers
are very different all the results are
very different do you have any comments
or suggestions so you know what's the
most effective way we can find you know
describe you know shorter sentences or
maybe few words or you know I the answer
is it shouldn't be your problem it
should be ours we should be doing a
better job at understanding what you're
asking for all right and you shouldn't
have to play tricks with the system to
actually make it work better I should be
completely natural so yeah it's it's a
tough home thanks a lot my question Sam
via car from Cummins I have interest in
optical character recognition as it
pertains to scanning old texts so for
ancient texts how good does it have to
be before you then would represent it as
this is what was written that's a good
question I suppose that you know if
there's no relevant files available
anymore then it might be hard but but
you can of course you know start with
with scant books old scam books and
annotated by humans and use that as your
seat and go from there basically so in
fact someone is working on like ancient
Greek to try to do OCR for that sort of
way I mean might not be able to do it
automatically that's fine too but when
you speech they don't start from text or
anything and that either they do a big
data collection effort where they have
speech annotated by humans then I go
from there and we can do the same thing
so in terms technology I don't think
there's any problem we should be able to
do it let's advance your question
oh well I think that if we actually
going to show it to you then it has to
be really good but it was just for
searching and you know you wouldn't know
right and you just won't find a term
that you're looking for I can I don't
know any numbers I mean if that's what
you're looking for in terms of like
accuracy I'm a function so I guess it's
just for different applications there
would be different quality threshold so
i guess i can answer that for machine
translation and it's something where we
went through different iterations so
machine translation people that tried
machine translation ten years ago on the
web and said oh this is all pretty bad
and and so we're unhappy with the
results and throw it not not to be very
useful and then over time basically
machine translation improved and with
those data intensive techniques that now
very many people use machine translation
and are very happy with it and but what
we also see now for languages for which
there was no machine translation
available before that when we make it
now available actually quite low quality
still gets large amount of usage and
because there is a fundamental need
there to get access to that information
so where we over time we basically have
this quality bar where we said we only
want to make it available with that
quality barring what kind of arbitrarily
set which are separate this is what we
feel it's good enough and over time
we're actually lowering that because we
noticed people want machine translation
for getting access to information and
data and right there you have no way to
get to that information all you have
chromium empty then then you would take
that so I think also here it will depend
on various different things and depends
on the application where it might not be
useful for some applications but it
would be very useful for search or other
things I don't think there's a clear
answer there
there is so you download the Google
Goggles application on Android and you
can do it exactly that you take a
picture and it basically gets sent to
the server get the text gets ocr'd
recognized it gets translated and you
see back the information and it has all
kinds of other image kind of things
especially good for wine bottles to
figure out what wine is you can look
like a wine expert in the rest are you
secretly do that but there's also this
translate integration I think it's
another one for translation i think the
exciting thing now for normal text
translation on the web it gets very
heavily used and basically people are
using that on a daily basis but then
these other kind of places where you run
into a language barrier that's when you
communicate to people when you talk to
people or then when you are right you're
just faced with a document which you
don't even have online to put in if you
go to japan and you don't know japanese
you have a hard time putting the text
into google translate so then those
other kind of technologies they all
combined together and then at some point
the language barrier will be much less
of an issue right so these are first i
wanna say these are just amazing results
the performance you showed just now a
few years ago would be deemed impossible
by AI researchers and now you're just
showing it work so it's fantastic I
think the singularity is near yes that's
the conclusion the question I have is
taking taking it to the next level and
by that I mean synthesis so everything
is showing is about analysis taking
stuff that humans have generated and
understanding it can you take this and
now generate new content for for example
can you generate new songs new music
that is similar to existing music but
different can you generate new images
that are similar to existing ones but
more beautiful or more interesting and
so forth so move
from analysis to generating content is
that I think there are some similarities
some principles that could be used and
maybe that's the next thing so in my
talk if you remember I talked about
three you know big areas we work in the
first was semantic interpretation which
is what I focused on the other two were
large-scale matching and synthesis
synthesis was indeed the third area and
you know the kind of use cases you
describe are you know are pretty high
level synthesis is actually required
from ground up right so any place where
you collect imagery where the collection
process has problems right you know you
could imagine synthesis coming in there
to complete the process for you right so
there are quite a few places where you
know we actually work on synthesis in
that context um I guess I just had a
it's it's amazing what you guys are
doing but most of what what is focused
on is trying to basically make the
perfect human right you're reading
flawed data because it's put on by
people who are relatively flawed and
trying to get the right answer every
time well there's some cases like for
marketing surveys and stuff maybe people
would want to go online and say okay
what does a person going to think of
this product that you don't want there's
no right answer is there work being done
on that of like trying to design more
like a human that would be flawed but in
nature rather than giving a perfect
answer because people make unraveled
assistance we don't have to work really
hard on being flawed actually I could
comment to that an indirect way some of
the things that our group is trying to
do there's no right answer in the first
place so a number of you know a number
of queries it's possible to come back
with a nice ranking of what you think
the right answer should be not always
but for example even take the simple
idea of building a good playlist you
know I think I know what have some ideas
but I mean it's not really clear I we're
I'm very interested in carrying a lot
about what people what people want in
the first place whether whether rational
or not and trying to model some of that
I guess I rationality if you want if you
and put it that way
so we're also doing things like
sentiment analysis for reviews or so
we're then you basically see right what
what what people would think about
restaurant and the food you know in a
certain place and that aggregates kind
of the rationality of irrationality that
people have if if you look very
different restaurants you might have
been a very different opinion but but
there you see kind of the average
opinion that the web is having or people
are having their so in that respect
things like that get model too and are
then exposed back to the to the world if
gaining over LOL research i have a
question about OCR you mentioned you are
using language models and you're doing
OCR for books we spend hundreds of years
I wonder if you build language models
for different periods of time because
language has evolved over 500 years
mouse and keyboard five a hundred years
ago meant a very animal probably a piano
not computer parts today so I wonder if
you model language change in remain
right which I mean that's definitely
possible and you know generally we
probably have like a two-pass approach
where you have some maybe language or
time period independent model that you
first tried to do a first pass to get
some idea and them in the second pass
you might use like a in the area
specific model or you know book specific
model or whatever it is to do a every
scoring and get a better result so what
you're talking about and in certain
languages the spelling complete changes
and stuff with that so you would
definitely need to do that I just wonder
if wish resolution do need to model
language change do I twitch resolution
do need to model language change do need
to change to build a new model every for
every year in the past every decade
that's a good question but I suppose she
would do it on a you know on a limb
supervised way so you know you wouldn't
decide in advance like how many chunks
there would be that who's just depend on
language yeah I was just interested what
your findings are so it's all more
general answer there is and probably we
didn't go to that too much so for this
specific case i guess it's also a
research question that's still open to
be figured out but one thing that that
kind of also occurs as a pattern for a
lot of the work that
we do it seems that simple models with
very large amounts of data often to a
very good job and and finer grained
distinctions and complexities in the
models are sometimes necessary the same
thing basically that you refer to for
OCR for its painting a very long time
frame for machine translation you have a
similar issue where contemporary right
there all these different domains and
and so ideally you would want to
fragment your training data in all these
different domains and it just turns out
to be very very hard to do that one of
the best things and in sometimes it's
great for from system development seems
to be at this point given where the
models are at this point to just throw
all the data together and build very
long history models where you have in
grams of a long long size me a very
large amount of data you can still
remember then or estimate those reliably
and based on that you have a very simple
model and if you have a five-word
history then this often also this
emigrates the specific context and it's
just very hard unfortunately or
fortunately depending on your
perspective from as a system developer
you want to have it simple but but from
a right fundamental you would expect
that there must be information in that
in that finer grain distinction luckily
it works quite well with those very
simple models and that's a general theme
I think that also you see for all the
model that we are doing there in if you
look at the core of it it's very simple
but very large amounts of data then help
to book very thank you hi Jeff Coppola
Sandia National Labs my question
concerns machine learning probably
everybody on the panel is too young to
remember back in the late 80s there's a
book that came out women who love too
much and then within a few weeks there
was a joke that there was another book
coming out women who love too much and
those woman's phone numbers fully
cross-reference and indexed what I worry
about in looking at how effective these
machines are at learning is that it
seems like it'd be a piece of cake to
put together an algorithm to find people
who are rarely exploited and let's just
take a small example for instance
blackmail if you
troll the web and look for terminology
and patterns of exchanges that indicated
some criteria for being able to
blackmail somebody let's say it's
infidelity or you know cheating on your
taxes or what have you it just seemed
like based on the way the wealth of data
that you have the robustness of the
algorithms and the extent to which we're
putting all kinds of information on the
internet it just seems like both
inevitable and and and quite straight
forward to be able to develop algorithms
that would allow you know unethical
people too badly exploit and efficiently
exploit a lot of people on the internet
I just wanted to get your comment on
that we're not allowed to discuss
features before they're released no I
didn't mean to short-circuit I think
it's a valid question I should have made
the joke sure that some other people
have something meaningful to say about
this before we move on to the next
question so it is certainly true if you
look at the how technology their
advances and what's possible and to some
extent what we can do here at Google now
which sometimes right other people with
just a hard time replicating because of
the computational resources that that
just would not be available in other
places then you kind of can look a
little bit into the future of what would
be possible right most law will most
likely in in one form or another
continue to make it easier for quite
running algorithms which now run on a
thousand machines or ten thousand
machines there in a few years you can
just run it on your on your mega desktop
that you'll have then so so this is
certainly a challenge which society will
have to deal with a lot more things will
be possible and and and we run into
those things so for google goggles for
example that recognition could be very
fancy face recognition in there but then
the question is why would that be a good
thing or not so there was a decision
made that we wouldn't provide this very
fancy face recognition there because it
would allow you to do things which
probably would allow random people to do
things
which might be problematic so this kind
of privacy and security aspects will
have to evolve over time and a lot of
meaningful discussions are to be had
there in society has to deal with the
new possibilities that right not only
the good people who will have but also
the bad people so I'm old enough to say
that I was entering grad school at the
time when people had enough data to look
at network traffic and see that it was
actually a fractal and self-similar as
opposed to being Poisson late which was
a early predictions internet that the
data would be modeled that way so you
guys are analyzing unprecedented amount
of data so are we about to see some of
the normal conventions or theories about
what happens in machine learning or
information theory or dimension
reduction machine learning applications
sort of overturned simply because they
haven't had enough data to look at one
of the things that we've discovered
along the way is that a lot of the
machine learning techniques that have
been successfully tried in the past I've
really been solving the problem of not
enough data and when you start trying to
scale those techniques and trying to
apply them to a lot more data then they
don't work anymore they don't really
provide any edge because a lot of the
work was really about compensating and
making things more robust to lack of
information and over and over we see
very successful algorithms that were
developed in the past really become your
relevance and much simpler algorithm if
this scale really take over so language
modeling we have the old classic
language in graham language model it
works extremely well it scales extremely
well you can throw a lot more data at it
and it works even better every time you
add data at it it's very stupid people
really hate that model because it's too
simplistic but it's a very simple and it
scales and so we see a lot of those
patterns emerge over and over where yes
there is a lot to be learned and a lot
of things that we thought were true
before don't really apply anymore and as
in a simple generalization of that
pattern
which I think both people at Google and
in academia are also discovering is this
notion of distribution drift that no
classic machine learning tells us that
models with fewer parameters are always
better and models that retain less
information or better like the VC
dimension thing but then it turns out
the way Vincent described if you build
these models that are essentially
smartly memorizing the interim models
are essentially learning a lot about
your world but in a sort of a smart way
those models tend to have much better
robustness when your world changes which
happens pretty often that's one common
pattern I see across many different
areas where this old SN is you know
starting to be viewed differently
because this this notion of changing
word was not commonly analyzing machine
learning before so my name is Eric booth
art from Washington University won quite
a lot of your models are informed by
data that's being explicitly put in by
the human users you know kind of labels
and so forth are you at all looking at
stuff that's not explicitly put in for
instance in with videoconferencing you
have facial expression they or even
people are diameter things that could
give you physiologic or a mode of
information about the content that you
know is essentially not entered by that
person or that group of people or that
at large corpus of information well so I
guess a lot of the annotation which is
learnt from is in some sense is given by
the person but not as an explicit label
but by by dealing with the system so
spelling Corrections or the example that
Alfred yesterday gave where the system
learns from from Clarys and and clicks
then it's not that the user annotate oh
I have here a spelling mistake but it
quick
so currently the video your laptop is
not monitored when you type in the
search queries but I mean far in the in
the place recognition example I gave we
get a lot of data with GPS tagged images
that's something not you just don't put
in the camera puts in for them and you
know we can leverage it once you get it
in our pipeline and certainly for music
I mean certainly would be nice if your
phone came with you and and decided you
in the car figured out where you are
like you so the GPS stuff and on the
other side on the data analysis i think
it's interesting to look for features
that came from the generation of the
content as well like for music
expressive timing and dynamics things
that are so explicit in the signal
higher level emotive things okay Chris
last Empress vaio I was wondering well
some of the OCR techniques be able to a
debt improve to the point they can be
used for forensic applications like
fraudulent art or handwriting
identification or the types of forensic
biology that's very possible but that's
certain that our focus right now it
might be hard I don't know I haven't
really looked into that I suppose it's
possible at some point okay so we'll
have to stop because we're about ten
minutes overtime thank you all so much
for listening thanks for the questions
and I hope you have a better idea of
know about some of the fascinating
problems that we work on at Google so
will now be heading to trader wicks all
the panel members will be joining us
there so you can also catch up with them
one-on-one if you have any questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>