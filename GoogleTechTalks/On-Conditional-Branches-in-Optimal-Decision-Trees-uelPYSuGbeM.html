<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>On Conditional Branches in Optimal Decision Trees | Coder Coacher - Coaching Coders</title><meta content="On Conditional Branches in Optimal Decision Trees - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>On Conditional Branches in Optimal Decision Trees</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uelPYSuGbeM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">who's giving a talk on optimal decision
trees and how we can improve them this
is dr. Michael Baer who is a PhD from
Stanford in electrical engineering
graduated in 2003 and he's now working
at electronics for imaging up in foster
city where he's recently started in the
color group and so without further ado
Michael
it's on ah there we go there's a little
louder than I expected okay so i'll have
to speak up and to make sure that I
don't deafen you all anyway as as David
said I'm here to talk about decision
trees so I I'm here to talk about fast
hard coded decision trees and what I
mean by hard coded decision trees is the
narrowing of intervals to a desired
number of intervals such as illustrated
here where I have one two three
intervals and I have this code which
will narrow the intervals to the one
outs that I want when this code is
compiled it will produce assembly code
that will look somewhat like this that
will compare an item that will go on if
an item is greater than or equal to it
or in branch otherwise and it will
continue to do that until it gets the
desired interval at which point it does
whatever it needs to do so first I
should ask why do we care about this
problem we care about this problem
because it is a problem that occurs in a
number of situations the most obvious is
explicitly when you have if their son
else's or implicitly when you have a
case statement when you have a case
statement it has to somehow decide which
part of the case statement it gets to as
to make the decision and one way of
doing that is a decision trees of course
you can also use a jump table but in
some cases decision trees are better to
use and in those kit in some cases you
don't really in if in ifs and else's you
don't have a choice it's especially nice
if you know the probabilities of the
outcomes that you want to have that way
the less likely outcomes can take a
little more time and the more likely
outcomes can take a little less time and
what you want to minimize is your
expected amount of time that you're
going to take and for example if you
know that your distribution output is a
bell curve for example if you have a
number of grades that you want to
calculate this is a good thing to do and
one of the reason that it's good to do
in the case of grades is because in
grades you have to
in terms of academic grades you have to
figure out what the score the person got
was and out of how many that is well
that's of division some processors don't
have a division function or instruction
and another's division is just very slow
if division is slow or if you have to
emulate division it might be worthwhile
to have a quotient search and that's the
way a lot of computers do emulate
division is they try to come up with a
series of ifs and else's such that it
can figure out what the quotient is and
if you know the probability of the
outcome the probability of your quotient
then you can tailor that to that
particular distribution finally a
particular application that I'm
especially interested in is that a fast
data decompression if you want to have
fast compression decompression one way
of doing that is Huffman coding and in
huffman coding when you try to
decompress the Huffman code if it's
compressed in the quickest manner
possible what you'll want to do is to
figure out first how long the current
code word is based on the bits that
appear ahead of you in one way of doing
that is to use a lookup table or jump
table but in some instances it's more
efficient to use a decision tree so I
want to ask now ask you if you can think
of any other examples that you've
encountered in your day-to-day work here
well maybe I cook
a classifier yeah if it's a classifier
in terms of some miracle classifier then
that's a good example any other well
I've probably given you enough over here
so let's get on and state the obvious
which is that if you do things faster on
computers you get happier users from
faster performance and sometimes you
save money if you have a server farm and
you have a loop that's running over and
over again if you tighten that loop if
that loop involves a decision tree then
you have to spend much money much less
money on hardware which makes people
happy at least the company's happy so I
originally was going to title my talk on
conditional branches in optimal decision
trees but I decided to name it in a more
cheeky fashion improving optimal
decision trees in that there are some
optimal decision trees that aren't
really optimal and that can be improved
and I'm going to show how to do this as
David said my name is michael baron i do
work for electronics for imaging we're a
company that makes enterprise print
solutions is so very fast very big very
expensive printers and devices that go
into printers but of course this is a
problem that is a little more Universal
than that so again a simple example is a
case which statement how is that going
to be implemented in practice well in
practice you might just want to divide
and conquer if you assume everything has
the same probability then you will want
to have the solution space and have it
again until you get what you want so
that's good and if I express that in
terms of a decision tree and time is of
course vertical in this particular
problem then everything happens at the
same time assuming that every
instruction takes about the same amount
of time but the fact is that in real
computers every instruction does not
take the amount same amount of time and
what I want maybe this but what I get
may look a little more like that in this
particular case that's not too big a
difference but in more complex problems
I might want a tree that looks like this
and so it's pretty balanced it's good
for binomial distribution because the
center occurrences are the most likely
but if I'd have if I don't keep in mind
how my Hardware looks in terms of time
what I might get is a tree that looks
like this now why is that the case why
aren't branches fixed time well simple
solution simple explanation is
pipelining salts and I'm going to have
to talk about computer architecture a
bit maybe I should ask you how many
people already know about this so I see
about two-thirds so for the one-third
that remains I'm going to explain it
hopefully the rest of you can follow
along and humor me so in modern machines
tens of instructions or maybe scores of
instructions take place simultaneously
from the time that the instruction is
originally fetched from the code to the
time it's retired to the time that it's
done everything that needs to do so that
works well for most instructions but one
question is well in the case of branches
what actually happens so here time is
along the horizontal axis and I have a
series of instructions that take place
one after another some instructions take
place at the same time some instructions
you have to wait for part of the
previous instruction to be over before
you can start for one reason or another
but the instruction that I'm most
worried about here is the orange
instruction the branch instruction in
the branch instruction I'm only going to
know what the outcome of the branches
here what the outcome of the comparison
is that predicates my branch so if I
only know what my next instruction is
going to be here then that's trouble
because if I were to
do this in the most straightforward
manner possible I just wait for the
outcome and then start a new instruction
so I be basically costing myself lots of
cycles maybe 15 20 cycles which might
correspond from between 15 and maybe 50
instructions that would be very bad and
so because I don't especially in a
decision tree where you're going to have
branches occur over and over again so
what would be better is if I could
somehow divine my solution ahead of time
and the way this is done is basically a
guess it's called speculative execution
but of course that's just a fancy name
of saying guessing and if i'm going to
guess and not wait for the instructions
to actually calculate what my outcome is
then of course sometimes i'm going to be
wrong and when I'm wrong I'm just as
slow as when I waited because I'm going
to have to wait execute a bunch of
instructions and didn't realize that I
shouldn't have execute all those
instructions and I need to go back to
the state that my microprocessor was in
beforehand but if my prediction rate is
about half or even three quarters or in
the case of some applications may be
ninety-five percent then on average I'm
going to be much much faster because I
can just say well I'm going to assume
that I just for example fetch the next
instruction and if I fetch the next
instruction and the instruction turns
out to be the one that I should have
fetched then that takes barely any time
at all so but because a correct guess is
much quicker to execute than an
incorrect guess that results in a
symmetry in that if I'm going to have a
branch then for one particular case my
branch is going to take maybe two cycles
and for another case it might take say
11 cycles so if I ignore that encoding
my decision tree if I assume that every
instruction takes the same amount of
time I'm going to wind up with that
asymmetric outcome that I had in one of
the previous slides a much better way of
approaching
is by saying well i know i have in this
symmetry let me just fix that asymmetry
and come up with an optimal tree based
on that fixed asymmetry and that's nice
it can result in good trees but you can
do that one better and the way of doing
it better is to control the asymmetry
sometimes these symmetry goes this way
and that might correspond to a greater
than or equal to function but if i
change that if i rearrange my code and
change the greater than or equal to 2 a
less than then i'll have an equivalent
set of instructions that will have a
different asymmetry so let me just make
sure I and is there anyone who's lost
now because I want to make sure that you
all are understanding this ok good so
the most simple type of prediction is
stag prediction and stag prediction
happens on processors like that which
you'd find in an iPod or in a cell phone
which are all of the arm family actually
and similar type of processors also have
static prediction in a static prediction
you follow a simple roll and usually the
role is as follows if I'm going to
branch to something that's ahead in
terms of the code space then I'm
assuming that that branch isn't going to
be taken because that might just be a
special condition if I branch back then
I'll assume that the branch is taken
because it's a loop now in this problem
i'm going to assume that all branches
are branches ahead and so i can just
assume that the branch is going to be
assumed untaken and code accordingly so
the inputs to this problem are the
probability of each outcome events each
outcome interval and the time cost
asymmetry that the processor presents to
me and as an output I want the decision
tree and the decision tree can be
expressed in terms of the splitting
points which is the number that I say is
this greater than or equal to that
number or is that less than that number
so if my inputs are 1 through n then
that's going to be 2 through
because one is trivial because
everything is greater than or equal to 1
all the inputs are and I'm going to have
a bias the asymmetry which I have to
physically code into my assembly
language or whatever language that I
have informally speaking that can be put
as follows of course formal formulations
are necessary but not always that
interesting the input again is the
probabilities and the asymmetry and the
output is the binary tree with an
assignment of dia symmetries which are
represented by B and that should
minimize the expected number of time
that it takes to actually get this done
so before I look at solving this problem
I want to bring up some problems that
are related to this problem some
problems that are also decision tree
problems but take a slightly different
tack so one of these problems is called
sometimes called the car problem and
what that problem is instead of saying
is this less than or is this greater
than or equal to I say is this in a
given set or is it not in this given set
so this is a very generalized form of
this problem that is applicable to
certain problems it's actually one of
the ways of thinking about it and I
won't fully explain this is morris code
actually operates like this because you
can assign any code word to any given
any given letter and in order to
optimally assign them given a certain
probability you would solve this using
the car problem and because it's a
relaxed version of this problem if you
solve this problem the solution this
problem will be slightly quicker or
around this or exactly the same time as
the problem that i'm considering now so
it's a good lower bound for performance
a good upper bound prefer prefer
performance is the case one that I
talked about if you assume that the
symmetry is fixed and that is what i'll
call the aight aight problem after the
person who
into that particular problem and other
problems are related to this current
static prediction problem according to
this grid if the cup costs are the same
then the static problem becomes what's
called the who Tucker problem and the
car problem becomes the Huffman problem
now I mentioned Huffman coding before
it's the same problem but before I was
talking about well this is a good way to
implement Huffman coding now I'm talking
about the actual Huffman problem and
finding of the optimal solution so this
is related in huffman coding is related
for with Huffman coding laid with
hochman coding in two different ways and
that's why I'm interested in it because
that's what my doctoral thesis was on um
but this is actually applicable to a
wide variety of problems so I already
said before then I want to minimize the
expected cost the expected time in
cycles and that is equal to the weighted
by the probability of each outcome how
long it takes to get to each outcome
which is determined by what type of
asymmetry I have which is B and the
amount of time each asymmetry takes and
how many steps I have to get on the way
to the solution so as I said before if I
relax the problem I can come with a
lower bound for the performance of the
problem if I constrain the problem I can
come with an upper bound so these lower
and upper bounds can actually be easily
computed far more easily compute than
actually solving the problem at hand and
they've been looked at in other papers
and these are what they are they're
relatively close to one another of
course it depends on your various values
so if I say well I'm not sure if I
really want to use a decision tree I
have this other method which is a lot
which may be better maybe worse you can
figure out whether it's better or worse
than this range in which this value
which you'll have to calculate lies so
everyone's with me with that as well
correct
okay so I built you all up and it turns
out that the algorithm for solving this
is extremely straightforward this is not
exactly a stroke of genius to come up
with the idea that it's a dynamic
programming algorithm that takes
advantage of the fact that after you
make the first decision all decisions
after that have to be optimally made and
you can show via an exchange argument
that you can build the problem up as a
dynamic programming problem that starts
that goes with the size of the sub tree
and builds it up until you get the size
of the sub tree that you need and so
this is the formulation of it as with
many mathematical formulations it looks
a lot worse than it actually is so I'm
going to go through an example to
illustrate how it really works and the
example has input as follows with
probability point three point two point
two and point 3 with an asymmetry of
three verses one I want to come up with
the optimal decision tree for that so
you start out if you start out knowing
that your solution is one then of course
your cost is going to be zero so if you
know you're so for these single certain
cases you have the zero cost and this is
the number for the item that you have
but overhear the item course the S the
last thing here corresponds to the
splitting point so if I have items 1 and
2 i'm going to try to figure out whether
the item is greater than or equal to 2
if it is then it's too if not then it's
one so these are really straightforward
in terms of how to calculate the total
probabilities and thus playing point and
also relatively straightforward is the
cost because I'm going to say well for
the least likely item 2 i'm going to
have that take three cycles whereas the
more likely item point 3 is going to
take one cycle so point 3 plus point 2
times 2 or x 3 actually because of this
is point 9
and I continue to calculate that for
each particular value and then for these
values I have to figure out which of my
two choices is best so I go to my two
possible choices for each of these and
calculate some orally until at the end I
get what my optimal performance is which
is 3.6 I'm not tell you what units I i'm
using perhaps i'm using cycles if it's 3
2 1 i'm probably using groups of two or
three cycles but the idea here is that
you get the optimal solution and you
know the splitting point is here and
then from there you can calculate all
the other splitting points and note that
your tree is going to look like this an
upside down version of what you get by
Sri splitting so it's relatively
straightforward approach and it takes
over n cube time and O of N squared
space because you have to store this
triangle which is over N squared space
and for each entry in the triangle you
have to calculate all Ovan possible
selections so that's somewhat slow if n
gets large enough it will be very slow
so can I come up with the faster way of
doing this well one way of solving
similar problems including I ties
problem is was no bikes knuth and I'll
call it the Quadrangle inequality truth
isn't actually the one who called it
this but he did know a certain property
which meant that you could actually make
this from 0 n cube time to O of N
squared time and if you can do that in
this case that would be great but you
can't and the actual example that I used
is a counterexample to this and I won't
go through this but if you want to write
down the numbers and try it yourself you
can see that this the X newest approach
doesn't actually work an even better
approach would be a Huffman like
approach because that would be 0 of n
log N and this was found by who and
Tucker it actually kluth is the one who
showed that it was Ovan log n time but
again there's a
counterexample to this so not quite sure
how I can get better performance than
this but it doesn't really matter that
much because n is going to be relatively
small if it were large then I would
probably want to use a jump table or a
lookup table or some other method other
than a decision tree so for small
decision trees Ovan cubed isn't too much
of a deterrent for me so another example
is if you have a binomial distribution
that has this asymmetry then this is the
optimal tree and the number of cycles
that it takes to get to a decision on
average is 12.9 eight if I were to
assume a fixed asymmetry then I could
only do as well as 15.1 which is
fourteen percent worse so obviously much
better to do it according to this
formulation a more complex example is in
terms of Huffman coding if you have a
bunch of words that are distributed
according to zipps law as words often
are and you want to come up with a
Huffman table for it you'll have the
following probabilities that the lengths
of each of the code words are these
particular numbers and if you want to
code for this have a decision tree to
see how long each code word is then the
decision tree will look like this and if
i were to solve this in the most
straightforward fashion then i want to
get this tree I'd get a slightly
different tree that's actually
three-point-one percent worse so this
speeds up Huffman coding by this
particular step of Huffman coding by
three-point-one percent which is nice so
that's good for ipods but for Pentiums
things are a little more complex and
depending on how my time is I don't know
how deeply I'll get into this got
another 15 minutes so hopefully you can
indulge me and I'll get into it
processors such as the pentium 4 use
dynamic branch prediction which means
that they have a slightly more complex
way of predicting whether branches are
taken or untaken what they'll do is
they'll use the results of previous
branches to calculate that which makes
things much more complex but for most
applications it makes things anywhere
from a little bit faster to a lot faster
so that's kind of nice but in this
particular problem it doesn't do this it
actually makes things slower and the
reason is if you know which branches are
going to be more likely taken in which
are going to be less likely taken then
of course you'll always want to assume
that you take the more likely route and
you don't need any dynamic modeling and
even though the dynamic modeling can get
rather complex it turns out that in
order to model this particular problem
because you're assuming that each
decision tree is going to be independent
from each other decision that you're
going to make if you make multiple
decisions at a time you assume that
they're all independent and because each
particular step of the decision tree is
fully determined by whatever steps
precede it then you can actually model
this quite simply as a single Markov
chain that you want to find the
stationary distribution of and in Intel
Architecture is that Markov chain is
relatively simple this these tees and
ends over the arrows represent that if a
branch is taken you move and you're in
this state you move to this state so if
it's taken take and take and you're
eventually stuck in this state where you
always assume this T in the circle which
is that every other branch will be taken
if you're right you go back to the step
if you're wrong then you go over here so
if a lot of branches are taken you're
going to assume that the next branch is
taken and if a lot of branches are
untaken you're going to assume that the
next branch is also untaken so it turns
out to be a pretty good method for
predicting branches
but as I said in this case it makes
things a little worse instead of having
the probability of the less likely
outcome be equal to the probability of
misprediction it turns out that the
probability of misprediction will be
slightly greater than the probability of
the less likely outcome and you can do
that through analysis of the Markov
chain and if you do this you find that
the main two methods a 3 &amp;amp; a 2 a 2 which
is used by the Pentium not actually the
Pentium itself the Pentium to pentium 3
pentium 4 pentium pro the pentium was
actually a little broken so if you were
to try to optimize for a pentium you
would actually do something slightly
different than this but they eventually
fixed it and so this is what you do for
modern pentium and other processors
maybe mips because this is in Hennessy
and Parsons book so one would think that
the MIPS processor would use this
technique the curves differ but it gives
you a function for how long things are
going to be expected to take and taking
advantage of this you can come up with a
general formulation instead of one of
two possibilities there might be one of
K possibilities in the in dynamic
branching itself they're only going to
be one of two possibilities but there
are other things that you might want to
do such as conditional instructions or
other types of models that are more
complex in order to get all these
complex models I want to come up with a
very general dynamic programming
approach and in that dynamic programming
approach you basically say well what is
the cost given that the probability of
one way branch is P Prime the
probability of another way branch is P
double Prime and I'm considering the
tree between I and J inclusive with I
with splitting point s using method k so
I'm going to have K possible methods you
can have one asymmetry another asymmetry
a different method combine it all and
you get a general formulation like this
and that's going to take Oh of
M to the N cubed time where m is the
number of possible case the possible
splitting methods which if K is small
enough is basically o of N squared just
as in the previous approach so what I've
done here is to use the actual behavior
of conditional branches to impact
decision tree design to make sure that
my decision tree truly takes the least
amount of expected time and in some
sense this results in better than
optimal trees because like I showed
before if you use previous methods to
find optimal trees they won't truly be
optimal in terms of taking the least
amount of expected time and
unfortunately I've shown that the most
straightforward fast methods of finding
decision trees don't apply here but
because n is usually pretty small slow
fish slow methods are efficient enough
for most purposes so finally I want to
end by saying this is what I've done
what I haven't really done is go into
enough real-world applications and
that's one of the reasons I'm here is I
want to ask if any of you had any real
world applications if you want to you
can come see me after the talk or come
talk to me because I think that this is
kind of something interesting to look at
but it's even more interesting if I can
show you know here's a problem that I
had I use this technique and suddenly my
solution is a lot better than it would
have otherwise been but for those of you
who don't have problems like this this
concludes my talk
well yeah I'll repeat the question the
question was how does this coward this
compare with a jump table can you
analyze a jump table in a similar
fashion and why should we sort of
pointed out was that in the case of a
jump table you would have to analyze
that in a different way see how fast a
jump table was and then either solve
this problem or use the entropy balance
that I showed before to come up with in
an estimation of how long the non jump
table approach is going to take and if
the jump table approach is faster use a
jump table if this approach is faster
than you don't in some cases a jump
table isn't really applicable because if
you've got say the estimation of a
division then you really have to either
do the division and use the quotient to
figure out what interval you're in or
you have to use a method like this so
you know I did emphasize the cases
statements and ifs and bends but of
course you can do things that are
slightly more complex
so the Seas cost function basically
kind of a suit is constant I would just
believe if you be organized your badges
that console changes okay so so you're
saying that I'm sort of assuming that my
costs are constant and it is true that
in the more complex processors they
won't be and that's why I had that final
see of K function and where each K is to
determine well if I guess I should
shouldn't have I shouldn't have stopped
this let me let me go to the slide so I
can show you what I mean well I mean
presumably you know the behavior your
processor you're saying well how do I
know the cost I have to speculate in
some cases that may be true if I don't
know the behavior of my processor which
you know in many cases perhaps say AMD
or intel didn't tell me and the
processors to knew that someone else
didn't reverse engineering didn't
reverse-engineer it and figure out how
well it performs then yeah these numbers
might be a little hard to come by but
when I can come by these numbers then I
can take into account not only the
probabilities involved but also the
positions for example one thing you
might say for the division problem is
well it's a lot easier to shift a number
than it is to multiply it it takes a lot
less time so if my eye I J&amp;amp;S are such
that i can do the comparison with just a
shift instead of a multiply then my cost
is going to be much lower
to how fast graphics I'm not sure what
you're asking could you sit could you
say that again you know in the case of
the division that sees that cause you
know that I think simply comparison is
much faster than emulation of the
difference function but in more general
case when you get to the way to the cost
functions are you saying that I don't
know what my probabilities are or that I
don't know what my cost functions are
yeah I mean in you know if you want to
optimize this for a family of processors
for example then it's a little harder
especially or if you want optimize this
for a particular processor that you
don't know the performance of then that
can be problematic and yes I am making
the assumption that i know my processor
well enough that I can actually do
something like this which might be an
improper assumption in some cases but in
most cases you have some idea based on
reverse engineered versions of previous
versions of the processor what
performance is like and you might not
actually get your optimal performance
but you might get a better performance
than if you didn't take that into
account at all
because if then else you know a decision
tree that what fun it would be smaller
than any update it is slower behavior so
it will only work the small amount yeah
you just repeated the point that this is
best for smaller problems if the problem
is big enough then you should use a
lookup table in some instances you can't
quite use a lookup table but you use
something equivalent like a division
like a hardware division or an emulated
division and that's actually going to be
faster than the method that I've
proposed up here and of course I don't
deny that and I say it explicitly and
you know one example of that is that in
this Huffman problem this is based on a
paper of fast Huffman coding that was
put out about nine years ago and in that
paper they compared using a lookup table
with using a decision tree the decision
tree that looks almost exactly like the
decision trees that I show up here and I
contacted the author and he said well I
formulated that decision tree using the
who Tucker algorithm which means it
won't be as fast as it needs to be in
some cases some cases the decision tree
was faster than the lookup table in
other cases the lookup table was faster
than the decision tree so obviously you
want to solve the problem according to
the way that you want to do it optimally
and another thing that I didn't mention
is that if n is large enough then you
have to worry about problems like code
caching and if that starts to be a
problem then in some cases all bets are
off because this model just doesn't
conform to that
any other questions
well thank you Michael
yes thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>