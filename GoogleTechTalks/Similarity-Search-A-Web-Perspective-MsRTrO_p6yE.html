<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Similarity Search: A Web Perspective | Coder Coacher - Coaching Coders</title><meta content="Similarity Search: A Web Perspective - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Similarity Search: A Web Perspective</b></h2><h5 class="post__date">2007-10-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MsRTrO_p6yE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you very much thanks for inviting
me let's start so let's start with
definition of similarity search problem
in similarity search problem you have an
input it's an algorithmic problem and an
input is a huge data set of objects and
say in this example we have pictures and
then we have some time for pre process
them so we need to build some data
structure to keep our objects and once
we get a query a query is another object
and what we have we have similarity
function or distance function that for
any two objects provide distance or
similarity how how similar they are and
our task is quickly to find an object in
the data set which is the most similar
to the query object so can you solve
this I hope so so let's your governor
and okay so we find the most similar
object in the data set that's the
problem and the problem is to to find a
data structure that is small ideally
linear in such a way that the search is
fast ideally something like logarithmic
not like comparing the new picture with
every picture in the data set so the
outline for today talk has four parts
first I quickly overview what are
applications of similarity search within
web technologies then I quickly show you
what what is known in theory and then I
will argue that what the model will
study in theory is not exactly what we
need for practical application and web
and then we put some new definitions and
new open problems mostly theoretical and
then I show you a couple of new
algorithms that's the the plan for today
please take the first place yes ok so
applications
so that's the least so usual application
say advertisement is the following thing
you have a lot of ads in your database
and every advertisement is characterized
by by keywords by a set of people who
already clicked on that by the website
its ranking and there are plenty of
characteristics and there is a user with
Google Google cookie and so you know a
lot of things about the user like what
was his previous search queries what is
he visiting what is he writing in gmail
I don't know you know a lot of about
user some of information you probably do
not like to use you say we keep privacy
but some other things you consider to be
useful for make more personalized
advertisement it called behavioral
targeting when you target your
advertisement to to user that is likely
by his previous behavior is likely to be
interested so you have a data set of all
advertisement there is a user with his
profile there is a similarity function
or relevance function that say how
similar the user profile is for every ad
and the question is in what data
structure should we keep our
advertisements in order to very quickly
identify the most relevant ads to a
given user so user with a query ads our
objects in the data set another example
is various kind of best match search so
usual web search is empty search you
have query words and you search for
usually four pages that contain all of
them but if you put hundreds of them or
you put some say example or some
restrictions if you are searching for
Caro apartment or boyfriend/girlfriend
know you you set a lot of restrictions
constraints and most probably there is
not a single object that satisfy all of
them so you're searching for object that
satisfy most of them and that's called
best match search so the number of
satisfied constraints is a similarity
function between query and object so
it's there are plenty of different
problems and you probably should address
them
differently but on very high abstract
level they all are like similarity
search so that's a typical example of
how a similarity is how you can define
similarities so this is a picture about
recommendation papers to scientists so
there is a scientist and we would like
to recommend him new papers to read
please take take places there are a lot
of free seats so first idea is to use
keyword machine like this scientist is
interested in similarity search this
paper contains these words so let's
recommend it it's called content-based
recommendations another idea is like use
social structure and using social
structure means that you look at friends
of this person and if one of his friends
like something like his co-author
already cited this paper then it's
likely this paper to be recommended to
initial user this thing is ma tricky
assume there is a paper I am very big
fan of maybe i'm just an author of this
and and then we are trying to identify
the similar objects to what i already
like so in particular here there is a
paper that is often cause cited together
with my paper so once somebody site my
paper here already also decide another
paper then there is a good idea to
recommend the second paper for me to
have a look so the general idea that
there were a lot of chains in this
networks that can connect the the query
object and one of the objects in the
data set and the larger number of this
change the shorter they are and if we
have bates the heavier they are the more
likely this object to be similar or
relevant to a query in particular here
to user that was very brief description
of application so what's known in theory
so in theory we have this abstract
problem and an absolute problem we have
some domain domain is the set of all
possible objects we have n objects which
is data set or database we have a query
object and our task is to find the
object in the data set that has the
smallest distance now the highest
similarity to query object and there are
and there are plenty of particular model
so model means the the data model what
is the object and the similarity model
the answer to the question what's the
function of distance so the typical
example is like Euclidean space with
Euclidean distance or Euclidean space
with the scalar product similarity so
that's a typical models or four strings
if you are care not about this set of
words but about the sequences then you
can use something like edit distance or
leverage 10 distance of the same name
like what is the minimal number of
operations you need to transform one
string to another string that's the
measure of similarity okay so what is
known in theory about that there are
several algorithms there are few of them
are here so and you see that that's once
you have an application and you see on
that look at that any little bit lost so
you don't know what to use and for
example the geometric near access new
neighbor accessory was the first of the
second paper by sergey brin so he also
participated in this competition in
nearest neighbor algorithms so I think
that if you are working with some
practical problem instead of looking on
all these papers that use a lot of
specific choices a lot of heuristics and
some elements from which they build
their own algorithm it's better to be
aware of of key ideas instead of all
these detailed algorithms look at the
ideas and reuse ideas not the algorithms
itself in practice so that's my personal
choice of
for great ideas so I will briefly
describe you very briefly what was there
about and if you are more interested
then you're welcome to my tutorial I
have on my website tutorial with full
descriptions of all these methods so the
branch and bound is idea to keep all
objects in a tree and on every internal
node you have something like say
bounding ball that cover all objects
within this within this subtree so if
you have a root and you have say two
children then you have a ball that cover
all points that are in the left branch
you have a ball that cover all points
okay they may intersect the balls cover
points in the right branch and when you
have a query and you probably have a
search radius so this is query and you
are searching for similar objects within
this range if this small ball doesn't
intersect say left branch then you skip
the whole branch and go on lid to the
right branch if if it intersect both you
need to go to both so there are in
general there is no guarantees that you
will not visit every every object during
this tree navigation probably you will
visit everything but sometimes if you
have clever heuristic for partitioning
it can decrease amount of computation
the second idea is to use walks and
walks is the following idea you start
from some object in the data set and
trying to move to another object which
is more similar to query and then you
look around and if you find an object
which is even more similar you move
there and so you do several steps until
you arrive to such an object from which
you can cannot do any immediate step to
improve your situation and then
sometimes you can click conclude that
that's the nearest neighbor the third
idea is to go to another space so if
you'll have plenty of objects in some
very wired space maybe 1 million
dimensional space and
it's really hard to work there or do
something like branch-and-bound there so
you're probably first to move map all
this object to some much better space
maybe even discrete space or whatever so
a fashion is a particular idea of
mapping and then you are try to identify
neighbors in this new space and if the
mapping does not affect too much the the
notion of being similar then you
probably find the problem is in the new
space the last idea which appears I
think like five years ago in the context
of similarity search in Britain you is
the idea to have the several like maps
of your data set on every every scale so
you start from from one ball that
covering all points and this is your
world map and then you are trying to
cover all points by both twice Leslie
radios so it's like country level and so
you see a picture that data set is
covered by all these countries and then
if you zoom in that there is a third
level of balls of one-quarter reduce
that again cover whole thing and you can
think that these are states or counties
and so on and so you have a day for
every scale you have the covering of
balls that cover all the data set and
then you your search is like jumping
down so you identify the most
interesting region on every scale so you
identify the right country then you look
for right state then you look for Wright
County and then you look for the most
similar city so that's the idea so the
data strategies like the several layers
of this balls
okay so now we can move to thinking
whether the problem was stated correctly
or not so i showed you before this
picture of of chains that produce
different contribution to overall
similarity so we can try to model that
with graph theory instead of euclidean
space and euclidean distance something
like that we have several types of nodes
we have probably waited edges we have
restrictions on degrees because every
scientist hero that most hundred or 200
papers not like millions of them but if
you have citations so every paper cites
a 20 or 30 papers but every paper can be
cited by thousands so there is different
restrictions on out degrees and in
degrees and then we have this similarity
chart that the overview all
contributions and then we have some
summing up function maybe some
coefficients so this is much more much
more mathematical model just the same
intuition by much more mathematical and
if you like mathematical problem this
light presents you an open problem so I
assume you have a beeper Ted graph say
between people and movies we have n
people we have em movies every person
like no more than K movies so it's
binary either i like it or not like or
you can think that people and advertise
manner who clicked on what whatever you
want so and we can define that person
person similarity is just number of
joint movies we like both and the MA
tricky thing is person movie similarity
and let me explain
so if there is a person and he already
like like three movies then we look at
other people who agree with this person
that they also like this movies like the
reason ok there was person a and this is
person B who shared two movies in common
and like one movie more and then we
decide to recommend this movie to
initial person so we say that probably
he will be interested and we can
formalize that by having here at 33 step
chain between a and this movie acts that
we would like to recommend and the large
number of three step change between X
and ay we say the most similar they are
that's the very very clean mathematical
definition of similarity in Bieber that
graphs between points in the first then
the second part and then ok so to
complete this statement so the queries
in you pairs and so you see animation so
there was a beber that graph and the
query is a new person so the query is
like a set of another set of K movies
that somebody comes and say I like this
this and that and the task is to find
person a movie for two step so for to
stop person for three step similar movie
with maximum number of these chains and
I have clear constraints so I allow any
polynomial pre-processing time for
bipartite graph and for query I allow
only paulina from k from degree and from
Logan Logan and this is an open problem
so I think it's some somewhat relevant
to these practical problems I announced
to the first slide and it's very
interesting from mathematical side
because it's very simply formulated
so let's now move to basic assumptions
we have in theory and sometimes they're
really broken in these web applications
so first thing we have we have trendle
inequality usually because we say
distance we assume to angle inequality
and another assumption which I'm not
going too deep that they assume that
probably representational dimension is
very high like we have millions of
features but there is something called
intrinsic dimension and it is small and
the current candidate which is assumed
to be the best best formalization of
intrinsic dimension called doubling
dimension and the usual assumption that
it is much smaller than log n where n is
number of points but in practice we can
we for many data sets not for all of
them for pictures we do not have that
but for for movie preferences I think we
have this and I call it separation
effect and the separation effect is the
phenomenon that all distances are say
between one half and one so let me give
an example so let's define that the
similarity between people by number of
joint friends so just the my similarity
to Marsha's number of joint friends so
we have at least one joint friend alipur
at but maybe somebody else and so so how
is similar we have this similarity but
then the distance is something opposite
mean so the distance is the this small
it is the the most similar way so the
distance would be below one half so we
will be less than one-half similar is
say I have hundreds of friends and 70 of
them are shared with Moshe but clearly
it's not the case so for most of us if
you look any two people here in this
audience and you intersect the the
friendship to be less than fifty percent
I think so and the and then so you have
almost for many many applications you
have all distances in this range and
this broke most of algorithms because in
particular doubling dimension for this
case is very close to maximal possible
so for any data set Dublin dimension is
at most log in so
be more than log n so in here it would
be log n / 2 so it's almost as worse as
possible and the branch and bound
algorithms they cannot use trendle
inequality so they all based exclusively
on to angle inequality and trendle
inequality produce no no new knowledge
if you take two edges you know that both
of them are at least one half so you
know that third one is at least one but
you know that okay at most one but you
know that it is at most one by by this
separation effect so it doesn't produce
any new knowledge and you cannot do too
much so the next thing to to maybe to
change in the model is the notion of
success so there is an exact algorithm
that produced the most similar most
relevant nearest neighbor but sometimes
we speak about approximate algorithms
which means that if the best best
candidate is within one QA meter and we
find something which is say two
kilometers or maybe one at 15 kilometers
then it's still okay we are still find
something reasonably close but with this
separation effect it again it has it is
very meaningless to measure that because
what we have in separation ethic we can
just pick a random object if a
similarity okay it has distance R say
one even worst case one but the best has
11 half so the random object has the
approximation factor 2 to the best
optimal solution but it has a factor too
but it has no relation to what we are
searching for so maybe we should ready
find the notion of alder they and what I
suggest is approximate similarity
instead of distance and the example
would be so assume you have something
like that similarity is 1 minus distance
for and you have something like
separation effect then assume the best
the best thing you have is twenty
percent similar which is a huge overlap
of keywords say bit
in keyword of interest of the person and
keywords in some scientific paper and
clearly if you find something that it is
one dot one percent similar that's a bad
approximation for that so we say that it
is has only 200 factor of approximation
but if you look for the distances it has
eighty percent distance and it has 99
dot nine and we can say that this is
very good approximation for that but it
is not so what I suggest is to you to
study approximate algorithms for
similarity so if you would like to use
the usual model like Euclidean model
then similarities just color product so
can we find an object that has say at
most two times smaller scalar product
with query than the best possible choice
and this is again a northern problem so
polynomial prep process and end sub
linear search for approximate algorithm
for Euclidean space and cosine
similarity is an open problem it is done
for for for Euclidean distance so for
occlusion distance for Clinton space we
can do sub linear search with polymer
polynomial data structure approximate
algorithm but here it's not clear
whether we can or we can not so that's
another open problem and ok the last
thing I would like to revise its dynamic
aspects so in classical text books like
cormen leiserson rivest and we say that
dynamic version of the problem is the
problem when we have a search problem
like sorting that when we have
insertions and deletions and how to
handle them quickly but in the web we
have other dynamic aspects with if you
have these networks like hyperlinks or
friendship graphs in the facebook or
myspace or higher or citation graph it's
changing and even if one edge is changed
it affects a lot of similarities if we
consider this three-step chains and one
person say okay like one movie then all
similarities i changed so you need to
rebuild the whole data structure
or not I don't know and I think that
Wade's are changing because we can say
that wait for in use on digg is number
of votes entities increases ever every
second and the same with the hyperlink
popularity in blogs so if you would like
to identify the most popular blog post
of today it gained you backlinks every
every every 10 minutes so you the
information coming very quickly and
another thing that the similarity
function is actually not as clear and
the main experts can change the
coefficients every day and say okay we
make another training on another
training data set and now have new
coefficients like relevance formula for
web search and so it's not the picture
is not stable so you need to have a data
structure that can keep some some deal
with all that so that was the third part
and now we have some time for the new
results
so for new results we need one new
notion and that's the most at three key
and important slide on the presentation
so i hope you will get it so this light
is about definition so the definition is
as follows you have a data set and i
would like to define the how nice this
data set is so if it is very nice i can
do quickly quick i can do five
similarity search if it is bad i cannot
do so I here i present how to measure
the niceness of goodness of the set to
be searched in so i take all objects I
take object p and sort all up other
objects by the similarity to be so if we
have people in this room and we have
this number of joint friend similarity
so I take myself put it in the left and
then so sort all of you by number of
joint friends to me so the the person
number one is the person who has the
largest number of john friends and so on
so in this picture the person arc has
rank for because he is fourth in this
list and this person I s has ranked 11
as far as I remember and okay so we
added this is the definition of rank so
the rank of R with respect to P is the
position of our is in the sort at least
sorted by similarity to pee
and what I would like to have I would
like to have a triangle inequality for
ranks not for similarity functions also
the separation fxdd the the similarity
for values has not such a such a big
value but but trendle inequality for
rank is very very powerful so i would
like to bound the third-ranked so to
compute the third-ranked i need to
resort everything with respect to R and
compute the the position of s and what I
would like to have is the following
theme which I call disorder inequality
so the third-ranked should be no more
than some of two previous ranks times
some multiplicative factor d
so you have two ranks like as I said
four and 11 and you would like that the
third-ranked would be no more than d x 4
plus 11 and as you mean that in the data
set of endpoints where n is very large
you have that this is satisfied for
moderately small D we have a good result
we have a result which is data structure
together with naveen de y'all and
hinrich schÃ¼tze we have data structure
of size d n log N and search time is d
log N and the algorithm we use is a kind
of this random walk so it's one of these
for ideas in work in new model so we are
trying to simulate the the how people
travel in unusual world so i assume i
would like to travel from Moscow to
mountain view so first I do I go to the
airport and check all possible Airlines
and choose the airline which bring
brings me the most Claude must closest
place to mountain view so I do the
greedy airplane choosing airplane choice
and okay where I go I go here right and
then once I right here I try to look not
only these large gems but a little bit
smaller so I look for ok so I arrived to
Los Angeles because probably there is no
flight to San Francisco and then I look
for for for trains and takes a Amtrak
and arrived to san francisco or palo
alto or where it goes and then when I
arrived here I take one of the buses so
I look for all buses and I take the bus
that brings me as close to Google
headquarters as possible and then I need
to walk so this is probably the closest
the public transport station so if I
search the all public transportation for
the nearest neighbor for the
headquarters that's the way how I
on them starting in Moscow so the data
structure we suggested is the set of all
airplanes the set of all bus routes and
train routes so that's the transport so
here it is so we have some new constant
D prime which is located written here
and what we do so this is for every
object p we should choose several buses
or trains and several airplanes so we
have these balls and these are not usual
ball not not Euclidean balls on you kill
in a ball you have a ball of radius R
that means okay all points that within
range are coming in here it is a
combinatorial balls means that if it is
p and half means that I'll have most
similar objects to pee so I do not
define explicitly the radios I just say
that if I ever right bb7 that means the
seven must close up closest objects to
pee so I right or heaven and imagination
these balls and I choose D prime
pointers okay here should be D prime to
to this ball d prime random pointers to
this ball and two that ball so small
animation so I take couple of objects
here these are my buses I take couple of
objects in the huge next ball so and
they might fall down in the small ball
as well it's possible so here or here
and then I take random points and a huge
ball so that means that if you would
like to estimate complexity we have
endpoints we have log n levels and we
have this D prime pointers on every
level that's the size of data structure
and navigation is very simple this
search we simply start from some point
we'll look for pointers of first level
and move to the best end point so we
need to check this point this point this
point and the initial point compute the
similarity to query and choose the best
so sometimes the best is the point we
are already in and that means that we
do not use airplanes because we need
just go to the train so we are already
in the right country probably lets the
search and okay that was the first
result so the good thing about this
result is that it does not require any
arithmetic about distance values so it's
purely combinatorial we it's based only
on respective order by similarity and
that means that it can be implemented
for any data model so whatever your
objects are whatever your similarity
function is you can and it's really easy
to implement I hope so let's move to the
second result and so here we use this
triangle inequality assumption so we
cannot do it for any data set we can do
it only for data set that has this
trendle inequality for ranks for small
constant D that's my general idea that
the problem cannot be solved in general
for any data model without any
assumptions so we need to put some
assumptions that are true in practice
and so the second idea is to use
probabilistic assumptions so when you
when you something about probability of
word occurrences in the natural
languages and the illustration I have
here is called zip flaw and it says that
the word number K by frequency by sorted
by frequency has probability of
occurrence proportional to 1 over K so
these are not probably not these
probabilities but they are proportional
to that and this is my model of random
text and random queries so I have these
M words maybe in English are maybe not
in this because this tip flow of power
laws that you can find them even outside
of natural languages so they therefore
many data models they they are true and
to generate a document I simply
independently
take every term with these probabilities
unfortunately find these texts they are
not independent I know that but my
current result works only for
independence or my proof works probably
algorithm will still work for for for
real data but the proof of correctness
work only for independence assumption
and the similarity i define very very
simple way by the size of intersection
so given to all 22 texts Paxman a bag of
words so I do not use any sequence the
order i defined the similarities simply
by a number of common terms so i do not
introduce any penalty for frequent words
and then there is a very funny theorem
proved with the benny hoffman and duke
no vodka that we can look of two type of
overlaps so assume we have a query
then and okay we'll say searching for
maximal overlap and maximal overlap is
say three things three words so say if
contain this word maybe this word and
this world that's the best possible okay
maybe this one oh ok so the best our lab
produce you for but then you can also
look for the maximal prefix overlap and
prefix overlap is overlapped by most
frequent terms in the query so maybe
it's three so there is a document that
has this please and this but there is no
document that contains this this this
and this so the maximal prefix overlap
is three on this example and the any
overlap maximal is for but the wonderful
thing you can prove that with very high
probability the the size of maximal
prefix overlap is within constant factor
is something like 1 plus epsilon times
the maximal any overlap so if you if you
search for a document that has the
maximum possible prefix overlap we will
find a good approximation very good
approximation for for any maximal
overlap but then the last observation is
the prefix match is much easier to
search so if you sort all your documents
in the data set by the frequency of
their terms and just will look for all
this and another thing that the the size
of overlap is very stable so it's always
within some very small additive factor
from this magic level so we still need
probably to understand why it's happen
what what's at significant features of
our model that produce that but it's I
think it's interesting so I have some
questions to audience so I hope you have
questions to me but I have to you as
well so what are the main challenges in
in similarity search you deal with I
expect that for recommendation system so
for advertising or for best matches you
have something and you try some
rhythm so what are the problems what are
the real problems you face because I'm
interested to learn them to think about
them and to to involve other members of
theory community to work on them and
then another interesting question is
what are you using so probably it's a
secret but ok the high-level idea
probably is not a cigarette so what so
you remember the slide with this huge
amount of algorithms so it's interesting
what what Google is using and the third
thing is data sets so if there are any
data sets that can be made public or
provided to a specific research group
under some MDA we will be very
interested to test algorithms we
thinking about on your data sets ok so I
have some sponsored links like Google
Health so you can visit my homepage and
my special page about similarity search
these are two papers and this is
overview in pictures of all the talk
thanks a lot ready for questions yeah do
have any incremental complexities also
instance if you have a site to get
proper session cases on your bedside
periodically so I would like to know so
after each update which kind of K
process you can be done such a thing as
for queries please because typical some
problems from incremental combustibles
of you are simple as an additional
problem like transitive closure which is
paranormal is it can be done in lock
spaces in your model results so there's
plenty of research incremental complex
to the have incremental compress results
poses sorry do have incremental
complexity results for this I do not
know so that's that's like one of
dynamic aspects and we can think about
that so you mean there are results for
for insertions and deletions so if you
have new objects you can insert them to
some of these datasets data structures
and delete but if the objects are
changing then you cannot do well even if
you insert an update so the note that
after institute of teaching source on
how much the process we should do so i
think they have good results for that
for amortized complexity so there might
be some particular insertion that
require a lot but in average yeah they
do technique similar to be trees but
some are ties complex just so it's not
incremental complexion yeah I think so
okay
yeah you can answer my questions if you
don't have any questions you can show
you them
I guess I don't understand everything so
let's go team again thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>