<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Free Tests Are Better Than Free Bananas | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Free Tests Are Better Than Free Bananas - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Free Tests Are Better Than Free Bananas</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/ExXATBlygl0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Our next speaker is Celal Ziftci.
Some of you might remember him from last year
if you were at GTAC last year.
So he doesn't need an introduction, but Celal
is now -- he now works at Google in the New
York office.
And his focus has been developer productivity
tools for a while.
Celal is going to talk about using data mining
and machine learning to improve your monitoring
for your applications in production in almost
real time.
&amp;gt;&amp;gt;Celal Ziftci: Thanks, Sonal.
Can you hear me?
So hi, everybody.
As Sonal mentioned, I was a speaker last year
at GTAC last year.
I was a Ph.D. student then.
I finally finished my Ph.D. And here I am
this year, working at Google.
Thank you.
So I am an engineering working in the display
ads engineering productivity team in New York.
I just completed my first year at Google.
And the title of my talk is free tests are
better than free bananas.
Obviously, this is subjective.
So for some of you, it might be better to
have free bananas.
But in Google, we have free bananas already,
so I assume free tests are better.
[ Laughter ]
But, yeah, like, in reality, what I'm going
to talk about is using -- how we use data
mining and machine learning to automatically
generate test cases so that we can monitor
production systems.
So I'm going to go over quickly how we release
software.
I'm pretty sure most of you know about this,
and we -- a couple presentations already went
over this.
But I'll quickly reiterate.
So we have developers, monkeys, again.
And they develop software, basically.
And then we send the change list that we have
for review.
Obviously, we test our software.
And, finally, we release it to production.
So the left side is deployment -- development.
The right side is production.
So as our systems are running, obviously,
we monitor them in real time continuously.
And typically, what we monitor them for is
crashes, how much is the memory usage, CPU,
QPS, basically, the usual suspects that you
would expect.
If there's anything out of ordinary compared
to the previous release, we will immediately
inform a developer, and then they will look
at what's going on and maybe go to the previous
version, and so on.
So, obviously, we test before releasing.
And the types of testing we do are unit integration
system, all sorts of testing that you can
think about.
But interestingly, we also test after releases,
after we go to production.
So you saw some of this in the Netflix talk,
I believe.
And this is kind of a similar approach, but
on a very different level.
We already do what you saw in the Netflix
talk, like canarying and other things like
the monitoring for memory CPU crashes.
But we also do some very interesting testing
after we go to production as well which I'm
going to talk about now.
So you might ask, so we already tested before
going to production, unit testing, system
testing, integration testing.
Why are you testing after production?
And what are you exactly testing; right?
So first of all, our systems are running in
production, and they are continuously producing
logs.
So logs might mean different things for different
people.
The simplest way to think about them is, it's
just a text file you keep appending it.
Bug logs, information logs, error logs.
Or you can output some other information about
your system, what is your memory usage every
20 minutes and so on.
So these are just examples.
And for -- and why do we collect this?
Obviously, to improve our systems, to see
if there's anything out of ordinary.
And for my presentation, I'm just going to
use this simple log message that we log, which
is a dropped banana.
It's a banana on a tree and it drops.
So I really want to log them.
And what I'm logging about this dropped banana
is, every banana in my system has an ID.
Every banana comes from a tree.
And that tree has an ID as well.
And, obviously, bananas have colors.
And I also keep information about whether
it's ripe or not.
Some interesting things here.
Obviously, there are some expectations that
you might already have, for example, when
you look at this log.
For example, the ID, it's something that's
generated by the system.
It's probably a nonnegative.
(indiscernible) ID.
Obviously, banana that's already dropped should
have been in a tree, and so on.
So I will cover these going forward.
But, yeah, this is basically the simplest
thing, the simplest long in our system.
So why are we testing?
So we have a system, which is the one that
I'm focusing on right now on the left side.
And this basically interacts with a bunch
of other systems that exist inside or outside
Google.
So let's say our system typically interacts
with these two systems, but then those systems,
in turn, interact with some other systems
somewhere else.
Now, obviously, when we do integration testing
or system testing, what we try to do is we
try to bring all of these systems together
and actually run tests almost identical to
what would happen in production.
But this may not be feasible all the time;
right?
These are big systems.
They are distributed.
And we may not be able to understand what's
actually happening in production.
The environment is different in testing versus
production.
So what, then, ends up happening is -- and,
by the way, when we are interacting with these
systems, we are obviously producing logs,
even in our testing environment, we are logging
these logs.
And when we test, let's say we are only testing
-- bringing up the dashed part of this environment
and just running this part as a whole.
And we are basically leaving out the boxes
outside the dashed box.
Now, this basically means we are not actually
testing exactly how things would happen in
production.
And, obviously, we don't know what happens
in production.
Things might be different, and we are missing
some systems in our testing.
That is exactly why we actually want to test
things in production.
Now, how do we do this?
Again, our systems are producing logs.
And we have an unlucky developer which we
ask to implement some more test cases.
So I specifically call them assertions here
to distinguish between where we test.
So the -- I call tests the ones that we implement
before we go to production, and assertions
the ones that we have after we go to production.
So an unlucky developer sits down, manually
implements these assertions.
And we have an assertion framework that basically
takes the assertions as the logs are coming,
you just assert them on the logs.
And if anything that is not expected happens,
then we inform a developer.
Now, the assertion framework is pretty typical
from what you would expect from a testing
framework.
Think about jUnit or PyUnit or any other testing
framework that you have, with one big difference,
though.
When you test before you go to production,
you have a closed system.
You bring it up.
You run everything through it.
You run your test cases, the data is output,
and you're done.
But in this case, our systems are running
24/7, and they're continuously producing logs.
So, basically, this is, in a nutshell, a testing
framework that is running in production continuously,
because logs are being produced continuously.
So this is the high-level overview of how
we do this.
Now, let's go through some machinery, some
examples, just to understand how exactly things
are working.
Again, our DroppedBanana example, there's
a banana ID there.
So say -- this is a kind of good expectation.
The ID is always positive; right?
So, basically, we take this, and we write
a test case.
It says assert the banana ID.
Given a dropped banana, if the ID is greater
than zero, you pass, otherwise you fail.
And you say the ID should be positive.
Since this is continuously running, let's
just go through a couple iterations.
Say we are given this dropped banana and the
ID is 10.
Nothing is out of the ordinary.
It runs through, everything is fine.
Another one comes in, this time, the ID is
19.
It's fine.
Whatever.
We just keep going.
Finally, we get something with a negative
ID.
So this might happen because we are already
in production, who knows who is using our
tool, and they are doing their own schedule
of release.
Maybe they made a change that caused this.
And, basically, at the end of the day, we
log this message, and this is not good, because
the ID is negative.
So because of this, our framework immediately
notifies the developer, and the developer
is given some information about what is the
log that caused the failure, which test case
failed.
Obviously, this is not our only test case;
right?
There are some more.
And where did it fail?
Which line?
And, basically, they debug, just they would
do in testing for production.
And then, obviously, there are different paths.
They can roll back the release or they can
try to figure out what exactly happened.
Maybe some upstream system is -- is having
some problems.
And they immediately figure out what's happening
and inform the right people.
Now, let's look at -- So this was just one
example of a simple assertion that we might
have.
So I'm going to talk about a bit more of those.
So this was the one that we had, banana ID
is greater than zero.
We might also want to have something like
color is either green or yellow.
So, obviously, we expect bananas to be green
or yellow.
And, finally, we -- this is a very logical
thing to expect.
If the banana is ripe, it should be yellow.
If it's not, if it's green, then it's not
ripe.
Another type of expectation we have might
be the banana with the banana ID must have
been on the tree with the tree ID.
Otherwise, there was something in our system,
about our assumptions, something is wrong,
so we have to check it.
Now, these are two different kinds of assertions.
This first set is what we call stateless assertions.
So the reason we call these stateless is because
the only thing to assert these on the given
log is the log itself.
Because I'm not using any information other
than what exists in the log itself.
Well, obviously, the logic is there.
But the log is enough, basically.
In the second one, though, we call them contextual
assertions because we need some context to
be able to do the assertion.
So in this case, yes, we are given the banana.
And we know the banana ID.
But we must have some other system where we
can check whether this banana was on that
tree or not.
So, basically, we need some external piece
of information to run this assertion on the
test.
Now, given all of this, these two types of
assertions, implementing them is very time-consuming.
So most of you are developers, people in software.
You know that we spend a lot of time in -- writing
test cases before we go to production.
We write unit tests.
We do system testing, integration testing.
We already spend so much time on it.
And as you know, like, research says that
maintenance is the biggest cost of software
systems.
So, basically, you have to keep maintaining
those test cases as well.
You cannot just change your code.
And this is exactly, again, back to the same
problem.
These are simply test cases that you sit down
and implement manually.
And as your system changes, you will have
to update them as well.
So therefore, it's very time consuming, and
developers are not very happy about these.
Now, given this information, how developers
are not happy, we went back and looked at
this problem again.
And, specifically, I want to show you something
about the first type of assertions that we
talked about.
So these are the stateless assertions that
I talked about.
If you think about these, these are basically
invariants; right?
So what is an invariant?
An invariant is a logical assertion that holds
at some point in your system.
Invariants come up in a lot of different fields
in software engineering, contract-based programming,
for example, you have contracts in your system
software at the beginning of a function, you
make sure that that contract is bound, like,
whatever comes into your function adheres
to that contract.
Another thing -- example is, you can have
defensive programming.
So, typically, people check these little invariants
at the beginning of a function call so that
they don't get data that they don't expect.
So invariants are basically everyone already
in software engineering.
And once we realize these invariants -- these
are invariants, we ask the question, can we
actually just mine the logs themselves, and
to find these invariants, and once we find
those invariants, can we just automatically
generate the stateless assertions?
Why do we want to do this?
Because we like our developers.
We don't want them to sit down and write test
cases again and again.
And the answer is yes.
So I'm just going to briefly talk about how
this works.
So our system is generating logs continuously.
And we take those logs and we perform some
data mining and machine learning on them and
then come out invariants.
So invariants are basically those logical
pieces of information, like banana ID is greater
than zero.
And after we have that information, we generate
assertions, which are simply a form of test
cases, and generating them is basically a
matter of getting some template and just running
the invariants through that template and generating
the actual executable test case itself.
And this basically summarizes exactly what
we do.
And today I am going to talk about two different
data mining and machine learning techniques
that we use.
And something that's very interesting here
is that these logs that we use are the exact
same logs that are actually being produced
right now; right?
We just use them to generate the assertions,
and then we run the assertions on the incoming
logs in the future.
So, basically, we just use the existing data
to do this.
Okay.
The first one I'm going to talk about is Daikon.
So Daikon is a system implemented by professor
Michael Ernst and his group.
He's a professor in University of Washington.
Two of his students are actually here.
And this system is basically open source.
You can just go use it yourself today.
And the one thing about Daikon is, it basically
instruments your system, and then it finds
invariants on your source code.
So it is specifically mostly targeted for
source code.
So what you do is you just run your program
with a bunch of test cases, personally, and
then you -- Daikon finds invariants on your
source code.
It says things like, in this function, this
was always greater than zero.
Or in this class, this field is always between
five and ten, and such.
So it basically finds those for you automatically.
But it's not exactly geared for data mining
and machine learning on data.
So it's basically finding things for your
source code.
But what we wanted to do is to use this on
data.
So -- data being the logs.
So what we did is we implemented an API on
top of it so that we can programmatically
keep -- instead of running software, we just
keep pushing data to it.
And then it finds invariants for us in the
data.
So, basically, we get a bunch of logs.
We push the logs through the API through Daikon.
And I will, basically, give you a high-level
overview of how Daikon works logically.
So at the beginning, basically, before you
push anything, Daikon basically just has a
bunch of hypotheses in its mind about what
your data is, what your source code or your
data looks -- will look like.
And then you push some of these logs through
Daikon, and some of those hypotheses don't
hold anymore.
For example, let's say you have some field,
and it's -- Daikon thinks that it's always
greater than ten.
And it sees some data, and that doesn't hold
anymore, so it just eliminates them.
It's as simple as that.
And then you keep pushing more data to it.
It eliminates some more of those in the hypothesis
space.
And, finally, you just keep pushing more logs
and, finally, in the end you are left with
these four invariants that it finds.
The important thing here is, this is kind
of a -- instead of a constructive way of doing
things, this is a top-down way of doing things.
So it has a huge space of hypotheses, of invariants
at the beginning.
And as you push more logs, it will actually
start eliminating them, and it will have less
and less invariants over time.
And the more logs you have, the better, because
it will observe more data so its guesses will
be more correct.
I'm mentioning this because this is basically
a dynamic analysis, what we call a dynamic
analysis on your software.
And in dynamic analysis, there's a problem
in the exploration space.
If you don't have enough data, it will find
things that actually don't make sense.
So if you have a more logs, then it's better.
And we have a lot of logs, so that's good.
All right.
So let's look at some examples on what these
four might be.
So daikon might actually find that banana
ID is greater than zero because it looks at
millions of records and it actually observes
that it's always positive.
Another thing it might observe is that color
is either green or yellow all the time.
There are other things that it might observe.
For example, banana ID is less than 71.
So this clearly is the case in the logs we
push to Daikon, but it's not something that
we're very interested in.
It's not useful to us because, as I mentioned,
this is dynamic analysis.
If you actually kept pushing more and more
data, in the ideal case, we would have logs
that represent the space that we have, log
space that we have, but that's basically not
possible.
So basically we look at these and say, okay,
so banana ID is less than 71.
We don't care about it so we're just going
to eliminate it.
Similarly, it may find -- it may be the case
that in the logs that we pushed, banana ID
is always three times tree ID plus four.
It happens.
Like, I can tell you we found some equations
like this that are just insane, and we were
just amazed that it can find those things,
but they are just not useful to us.
So what we do is basically, we do two things.
First we write a simple filter.
We filter these things automatically.
The first thing we do is we filter things
like ID.
We don't filter the lower bounds because nonnegative
is a good thing, but we might actually filter
out upper bounds.
Or we wrote some filters that just ignore
things like time stamps, the usual things
you wouldn't care about.
So we automatically filter them out.
And then the rest we basically run to a developer
and say what do you think about these?
Do they make sense?
And at that point it's basically a shopping
cart for him.
He can say yes, this makes sense; no, this
doesn't make sense.
He picks whatever he wants and tells us these
are the ones that are actually logical.
So, yeah, in a nutshell this is what Daikon
does, and this is how we're using it in Google.
And this is already implemented, available.
And with that I'm going to go to the second
tool that we use.
Now, the second tool that we use is Association
Rule Learning.
This is basically an algorithm that is very
common in data mining, and it is basically
-- it was basically amended to analyze database
transactions.
There's a very famous example where they find
that when males shop, they find that whenever
they buy diapers, they also buy beer.
So they --
[ Laughter ]
They rearrange their shopping mall so those
two things are close to each other, and so
on.
But, yeah, it's just a simple example of how
this thing works.
So I'm going to still run through an example
to show you how this works.
So again, we have logs.
We push them through this Association rule
learning algorithm.
And this basically takes this as tabular data.
It's like a database, basically, and immediately
when you look at this, if you look at rows
one to four, you realize that whenever is
ripe is true, color is yellow; right?
So basically Association Rule Learning can
actually find this and it can output that
whenever ripe is true, this implies color
is yellow.
Obviously, this may find the reverse.
Whenever color is yellow, is ripe is true,
which is true or yes.
But we just don't care about this rule.
What we care about is the other way around.
So we simply basically eliminate this.
We ignore it.
Or it may find something like whenever the
banana ID is not null, tree ID is also not
null.
This might be the case.
It is probably true, but we are not interested
in this.
It's not useful to us.
We simply basically eliminate that.
Now, there's one important point here.
In Daikon case, Daikon is very suitable for
continuously streaming data, because all it
does is it keeps a bunch of hypotheses in
memory and then it just filters out, eliminates
the ones that don't hold anymore.
But in this case, Association rule learning
most of the algorithms that are known for
Association Rule Learning basically work in
batch.
So in our case basically we had to look at
the literature and there are some algorithms
that are coming up recently that can actually
handle streaming data as well.
They have some different characteristics,
but they can still find, given enough data,
these kind of good rules.
So we are basically looking at those.
And finally, just to sum up, this is basically
how our assertion framework was running in
production.
We basically take this unhappy developer and
we replace him with our tool where we can
just mine invariants from logs and just generate
assertions for them.
And the important point here is obviously,
as I mentioned before, we're doing this only
for stateless assertions, not contextual ones,
because for contextual ones, the search space
is just huge, so we're not even attempting
to do that.
Another important thing here is even though
we're only doing this for stateless assertions,
there are two benefits for developers.
One is for developers this is now just a shopping
exercise.
They just look at the ones we find and pick
the ones they like.
Second, earlier they had to sit down and manually
implement all this stuff.
But now we're actually showing them what we
find in the data.
So sometimes they don't even know things exist
in the data, because some developer implements
the system, the invariant is there, it's not
documented, and he puts a test case for it
as far as he can tell, but then when we show
them these things that we can mine, they go
like, oh, yeah, that actually makes sense.
I actually never thought about it, you know.
So basically those are the two benefits that
we can provide to our developers.
Hence, free tests are better than free bananas.
And I'm going to open it to questions.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Thank you, Celal.
More monkeys.
I'm pretty sure everybody here will be soon
replaced by monkeys, whether or not we migrate
to machine learning.
I'm pretty sure we're close to that.
Any questions?
&amp;gt;&amp;gt;&amp;gt; Hi, so two questions.
One, I don't recall, do you have any examples
of a product being built using either Daikon
or ARL from the beginning to see does it drive
bugs down?
Does it enable better code quality throughout?
Any data to show you should use it early and
often or do you wait until you kind of go
to prod first?
&amp;gt;&amp;gt;Celal Ziftci: I don't know of a product
-- So there's a product called Agitator that
uses Daikon.
They use it for, I think something else, but
there's research that uses, like -- I mean,
Daikon is one example of finding likely invariants;
right?
So there's research that actually supports
find these invariants is actually good for
you because whenever you find something is
missing, you'll write test cases for it.
Basically it implicitly increases your code
coverage, anyway.
Test coverage.
&amp;gt;&amp;gt;&amp;gt; That just brought up a different question.
So we've had a few presentations so far of
different tools.
Have you tried taking this approach and merging
it with other approaches that have already
been shown earlier today to see if together
they can -- how they impact quality and if
you meet all --
&amp;gt;&amp;gt;Celal Ziftci: Do you have any specific question
about a product?
Because we actually do other things.
Like, for example, Netflix explained how they
do canarying and other things, and we actually
do that as well but this is basically another
layer of testing apart from that.
Because those are basically -- those are like
a black box approach onto testing and production,
like monitoring production.
You just look at memory, you look at CPU.
But I mean if CPU is up, like lots of things
might help (indiscernible); right?
But this is more like a white box approach.
We actually write a test case for it.
And then whenever something fails, you immediately
know which event it failed on, which log,
which line of the test case that the (indiscernible)
failed on, and so on.
&amp;gt;&amp;gt;&amp;gt; And the API you said you wrote on top
of Daikon, is that available for all of us
to use?
&amp;gt;&amp;gt;Celal Ziftci: Not open source yet.
&amp;gt;&amp;gt;&amp;gt; Not yet.
&amp;gt;&amp;gt;Celal Ziftci: But I'm sure you'll work with
professor (saying name) and make it open source.
&amp;gt;&amp;gt;&amp;gt; And the other question, is it customizable?
Does the source code need to be written in
a certain way so that Daikon and Aero can
interpret them to find the invariants?
&amp;gt;&amp;gt;Celal Ziftci: In the original implementation,
Daikon has front-ends for different languages
like C++, Java.
But the output format that it has is basically
language agnostic.
It just outputs its output.
That's actually what we do as well.
We just create the output it expects, input
it expects so we can just run it through Daikon.
&amp;gt;&amp;gt;&amp;gt; I'm curious, what would happen if all
of a sudden there are red bananas?
&amp;gt;&amp;gt;Celal Ziftci: Red bananas.
So if you have a test case for it, obviously
we would flag it.
You just asking for color; right?
In our example.
&amp;gt;&amp;gt;&amp;gt; Well, let's say right now we live in a
world with just green and yellow bananas --
&amp;gt;&amp;gt;Celal Ziftci: Yeah.
&amp;gt;&amp;gt;&amp;gt; -- and all of a sudden red bananas started
popping up in real life.
&amp;gt;&amp;gt;Celal Ziftci: Yes.
So what would happen is like -- I think I
showed you an invariant that said banana is
either yellow or green.
So we would have a corresponding test case
in our test framework that asserts that, and
that would immediately get flagged and somebody
would look at it, why it happened.
And then basically what would happen is they
would try to understand why we got such a
banana and then try to figure out the system
upstream on where this thing is coming from.
So I think the most important thing about
the whole invariant approach is that if you
have them everywhere in your system -- so
this is like gatekeeping; right?
If you do the similar thing upstream, you
will actually immediately realize something
is wrong upstream as well.
So as long as you put these invariant checks
everywhere in your system in between these
boundaries, like communication boundaries
between systems, whenever something fails,
all the teams that are going through this
whole failure route will be monitoring and
they will be notified.
&amp;gt;&amp;gt;&amp;gt; So they can manually modify that invariance
if it's actually valid.
&amp;gt;&amp;gt;Celal Ziftci: Yeah, exactly.
So I just showed an example on how to automate
it but like I mentioned, it's not actually
automated.
So the (indiscernible) still look at them,
and sometimes they might say, yeah, like you
found that it's either green or yellow, but
it might also be blue, for example.
So they might actually go and change the invariant
or the test case itself.
&amp;gt;&amp;gt;&amp;gt; So the machine learning algorithms are
kind of sensitive to garbage in, garbage out;
right?
&amp;gt;&amp;gt; Celal Ziftci: Yes.
&amp;gt;&amp;gt;&amp;gt; So are there certain standards for the
logging or, like, frameworks that you automatically
log certain objects or certain method calls
or something to support this?
&amp;gt;&amp;gt;Celal Ziftci: So in Google, we use protocol
buffers.
So that thing I showed you is protocol buffers.
And protocol buffers is open source, by the
way.
It's from Google and lots of people actually
use it today.
That's what we log.
So this basically works on any protocol buffer.
&amp;gt;&amp;gt;&amp;gt; So that would be between systems, I guess?
Where the logs tend to be.
&amp;gt;&amp;gt;Celal Ziftci: I basically talked only about
part of what this tool can do that I built,
and this is just the aspect that we use it
for generating test cases.
But it can do other things.
So I can give you one more example.
So just as I showed you in the testing section,
we basically bring up our systems and run
integration in system test; right?
And during those tests, we still generate
logs.
So one thing that we did is we ran this same
algorithm, machine algorithms, on test data
as well and also in production and then we
compared both of them.
And there are some interesting things that
we found.
Like, for example, this is one way of doing
test coverage; right?
So you look at your invariants and you realize,
oh, like the invariants we find on this field
is different on production and test.
So we should write more tests because we don't
have coverage for that.
&amp;gt;&amp;gt;&amp;gt; That's beautiful.
&amp;gt;&amp;gt;Celal Ziftci: We also have that but I didn't
talk about that part.
&amp;gt;&amp;gt;&amp;gt; Cool.
&amp;gt;&amp;gt;&amp;gt; My question is how many -- Do you have
stats on how many errors you actually catch
that are useful and how often does this actually
find things that you wouldn't have found --
&amp;gt;&amp;gt;Celal Ziftci: Yeah.
&amp;gt;&amp;gt;&amp;gt; -- through all the other quality means
that you have?
&amp;gt;&amp;gt;Celal Ziftci: So I can tell you for one
product, for example, right now we have about
25 assertions that we continuously run in
production.
And so far we caught two production problems
and we immediately fixed them.
Sometimes systems are updated.
One time they had to change the test cases
because something changed in the system so
they had to update the test case.
But, yeah, we've been running this for about
six months now, and, yeah, we caught production
problems twice.
&amp;gt;&amp;gt;&amp;gt; And one more question, is do they -- do
developers often learn something about the
code from the assertions?
If they say the system says this is an assertion
and you're like oh, really --
&amp;gt;&amp;gt;Celal Ziftci: Yeah.
&amp;gt;&amp;gt;&amp;gt; -- I wouldn't have thought that, that
might be a problem.
&amp;gt;&amp;gt;Celal Ziftci: Definitely.
Because, I mean, software companies have a
lot of developers and they come and go; right?
And somebody writes some part of the code
and you look at the invariants and then you
realize, oh, I didn't actually realize this
was the case.
And then they actually learn something from
it.
So definitely, yes.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Great.
Thank you very much, Celal.</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>