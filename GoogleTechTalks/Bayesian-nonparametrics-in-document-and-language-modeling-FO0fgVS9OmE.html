<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bayesian nonparametrics in document and language modeling | Coder Coacher - Coaching Coders</title><meta content="Bayesian nonparametrics in document and language modeling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bayesian nonparametrics in document and language modeling</b></h2><h5 class="post__date">2008-09-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FO0fgVS9OmE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's players they have a kind of the top
number of papers every year and and Mike
Jordans group is one of the other big
players so you eye has a very impressive
pedigree in terms of nipson machine
learning us done a lot of great work in
nonparametric Bayesian learning in
particular he made some very nice
connection between traditional language
modeling techniques and their amazing
interpretations is made a significant
contribution in the hierarchical douche
like process realizing that if you make
the parent model in itself atomic and
get this very nice sharing so that's a
really amazing intellectual contribution
amongst other things that he's done so
he's going to tell us about some of the
work he's done in document that language
modeling but he has tons of various
things that he's done which you can find
all the papers on the web pages and
everything can he'll be around this
afternoon with him okay okay thanks okay
so uh thanks M so that's a actually
really nice summary of this talks
Salvy this is kalfa i guess the title as
the file says it's based in on projects
in document and language modeling
there's gonna be some of the projects
which have been working on applying some
of the base in our countries ideas in
document and language modeling so I
think both they had recorded a process
as well as the language modeling what
will appear that's that's a nice mention
over here so I presume that most people
here may or may not know much about this
is nonparametric so I can spend a bit of
time going through some intuitions of
Jewishly processes first before I go
into the rest of the top okay so the
first bit is really just can't review -
so here's an overview so I'll start off
with telling you but what artificially
processes and basically infinite mixture
models and then how about hierarchy
would relate processes topic models
modeling and language model
okay so so let's start off with very
simple a problem of clustering right so
it's kind one of the basic problems of
machine learning you're given a you're
given a data and set of data points and
you want to figure out which are the
subsets of your data points which are
similar to each other and different from
other subsets so those are the clusters
of data points and this is of course the
basic technique that's been used that is
used all over the place in machine
learning data mining statistics right
and of course is a relief rich history
of you know with many algorithms and
models applied proposed for clustering
okay so there are kind of two
algorithmic questions I guess for
clustering the first one is where the
classes if you are given a data point
you are given a data set where are the
clusters so in this case you might think
that okay there might be like five
clusters here here here here and here
for example right and the second issue
which is kind of a harder issue is how
many clusters are there so if I give it
if I if I give give you a data set how
do we determine the number of classes in
in this in this data set so for example
in this case maybe I think that that's
five clusters but if I if if in the
future you see more and more data points
and you see that there's lots of data
points in here and then there's lots of
data points in here and there's some
sort of margin in between the two tell
you might think that maybe it's not five
clusters maybe it's six clusters so this
is a question of model selection how do
you select for the right size of your
clustering model to explain your data so
they the approach we should take for
clustering is mixture models so this is
a probabilistic approach to clustering
so given a data set say X 1 + 2 X n so
the probabilistic approach says that you
you start off with some description of
how is it that the data could be
generated based on so
parameters so in the case of of mixture
models you assume that the way that the
data is generated is is the following
you for every data point you pick a
cluster from some distribution of the
clusters and then given the cluster
which the data point belongs to you then
generate the actual data point itself
based on some distribution that
describes the cluster so in the case of
Gaussian clusters for example the the
parameters each cluster is described in
terms of its mean and its covariance so
those would be the parameters of your
mixture model and given this generative
process you can then write down was the
probability of your data set given your
parameters so that's in this case like
this where the PI's are your mixing
proportions or the prior probabilities
of your data point being assigned to
each cluster and this P of X i given so
z is a cluster indicator variable which
tells you which cluster data point x i
belongs to and theta that is still
prominent your cluster parameters and P
of X i given as i and theta is the
probability of the that data point given
the cluster so and the typical approach
for mixture modeling to is to find the
parameters which which maximize the
probability of your data
yeah which maximizes a problem
probability of the data so this is
called the maximum line you can approach
and I think you probably a lot of people
here know about like some like here fish
so I won't really go into deep it so and
I guess I should say that yeah
so once you've found a
maximum-likelihood set of parameters
that count tells you where are the
clusters so the and the second question
we should like to us is you know hum how
do we determine the number of clusters
if you just do maximum likelihood what
happens is that as you increase the
number of clusters in your mixture model
you are increasing the number of
parameters that's used to describe the
data so you are giving a model more and
more flexibility in terms of describing
the data and of course the probability
of in of the data would always increase
as you increase the number of clusters
right so this will lead to overfitting
if you just say you just want to find
okay so the number of clusters which
maximizes the probability of the data so
people have dealt with this for example
by using cross-validation or using some
self external measure of the fitness of
the mixture model so for example if you
use your mixture model - if you have a
second stage in which after you've
clustered your data you might want to do
some sort of discrimination task that
you might want to optimize for the error
after you've plus their data so in this
toy I'll be taking a Bayesian approach
and what the Bayesian approach says is
that you actually want to account for
the parameters of your mixture model by
introducing a prior which regular I
system okay so the product you can think
of it as every time you introduce new
parameters into a mixture model you want
to pay a cost for the provender and
because you have to pay a cost of the
parameter as you increase the number of
parameters or you or your number of
clusters at some point you you will
that's this trade-off between paying
lots for your parameters and describing
the data better and at some point the
trade-off will balance out and it's that
point in which you have the optimal
amount of clusters according to the
Bayesian approach and given a prior you
want to compute a posterior distribution
over the parameters and your latent
variables right so this is given by
Bayes rule and if you compute this
posterior distribution in a proper
manner you're not really fitting
anything - you are fitting anything to
your data right so and what this means
is that
in the sense the Bayesian approach does
not really over fit your data at all if
you've done it if you do it properly of
course and since the Bayesian approach
doesn't really over fit to your data
there's really theoretically no reason
to use small models anymore because you
like your model to be as rich as
possible to describe any data set that
you could encounter okay but so if you
do with small models you may have a
problem of underfitting right so you
don't you want to avoid that by using
large models and is an approach that's
not all fit hopefully okay so so which
means that we can consider mixture
models with basically really large
number of clusters so these are what's
called infinite mixture models and it's
been a proposed by in the machine
learning community by rational and
conference person it's in statistics is
people have looked into this for a
longer period of time and it's got a
richer history I guess
and the the idea of an infinite mixture
model is that you define a finite
mixture model given here and then you
say that let's just assume that the
number of the number of clusters in our
mixture model is really large so we take
K the number of clusters to go to
infinity then the next question is what
does this infinitely mean actually mean
I'll come to that later but I'll kind of
describe the disk the mixture model is
given here just to familiarize you with
the notation I use so pi here is a
mixing proportion for your mixture model
and usually we give a prior over mixing
proportions which is a Jewishly
distribution and this is usually because
the delays have a very nice conjugate
prior for mixing proportions
basically it allows you to do
computations in a very
intractable tractable manner and being
basin basically you do I want to
introduce priors for all your parameters
so theta K is the parameter for cluster
K and let's assume that they're drawn
iid from a prior over parameters given
by H and then for each data point X i we
have a corresponding date latent
variable z i which is its cluster
indicator variable it tells you which
cluster this data point x i belong to
and this is drawn from a multinomial
distribution with parameters pi or isn't
and then X i given zak i and thick and
the cluster parameter theta is drawn
from some distribution f parameterize by
theta so this is the notation i use i
guess and this is a corresponding
graphical model for the mixture model
okay so coming back to here what does
this infinite limit mean so that's kind
of a few ways of looking in well what
does this infinite mishima mean
so philosophically what this thing
actually says is that it's basically
encode encoding a prior belief that the
data that we see in the real world is
often coming from very complex processes
that cannot really be described by very
simple finite mixture models right it's
complex process requires a complex model
basically algorithmically this infinite
limit can be seen as basically infinite
limit of algorithms for posterior
computations for the mixture model okay
and then so I think both Redford and Cal
they came about this idea of an infinite
mixture model from this algorithmic
perspective but what of the model is of
what is the infinite limit of the model
and it turns out that this is a
basically a stochastic process called
the duration a process which has been
studied in the statistics literature for
for the last basically 35 years now I
guess maybe 40 years so next few slides
I'll tell you what artificially
processes so what's the duration a
process so it's basically a distribution
over distributions okay and so I can
write i additionally process in the
following way
so G is a random distribution that is
traditionally a process distributed okay
and it has two parameters at the
jewishnet process it has a concentration
parameter alpha and the base
distribution age the basic distribution
you can think of it as the mean of G and
alpha is some soft inverse variance for
G so you know this as a Gaussian you
parameterize it with a mean and variance
in this case a dirty trick process you
prioritize it with a base distribution
which is basically the mean and alpha
which is the inverse variance and what
is meant by a distribution over
distributions what's meant is that if
you draw a sample from a duration a
process then the sample itself is a
distribution so G is a distribution okay
and in particular it turns out that a
draw from a duration a process is always
going to be a discrete distribution and
a discrete distribution is simply a
distribution which consists of a number
of atoms and you can always write it in
the following ways it's going to be an
infinite sum of weighted atoms so Delta
Theta K is an atom located at as theta K
in a primitive in a probability space
and PI K is the weights of the
corresponding atom of course since G is
a this distribution the weights has to
be non-negative and sum to 1 so the
Pisan non-negative and sum to 1 and I
haven't really told you what are the
distributions corresponding to so G is
random right so which means that both pi
and theta are random and I haven't
really told you what Arta
distributions corresponding to theta and
PI so the phosphate is very straight for
it all each of each atom in your in your
Jewishly process simply iid draws from
the base distribution H here well the
pies coming from a stick breaking
process is what's called stick breaking
process and have the I basic idea is
very simple is imagine that you have a
stick of length one so for some reason
that the duration a process literature
has a lot of metaphors like stick
breaking constructions and Chinese
restaurant processes and stuff like that
so you see the Chinese restaurant
process later here I'll tell you about
this deep freakin cut from process
so therefore the stick breaking
construction start off with a stick of
length one and you break it at some
point
see here so the length of this bit of
the stick now is gonna be your PI one
okay and then you take what's left of
this stick and you break it at some
point again here and the length of this
segment now is going to be PI 2 and then
you can't break it infinitely often down
this way and that's gonna be PI 3 PI 4
PI 5 PI 6 and so forth and if you start
off with a stick of length 1 the sum of
the PI's has to be 1 by definition
basically and yeah so so that's
basically the stick breaking
construction so that gives you a way of
a constructive a way to construct a
sample from a duration a process you
draw your PI's using this degree in
construction and for every atom you draw
it iid from your basis distribution H ok
something like you some constructive
definition
so how do we use this in a mixture model
so if you have your G which is drawn
from a tradition a process we know that
it's a itself is a distribution so we
can draw samples from it so let's draw a
bunch of samples Phii from G then for
every data points X i we are going to
say that X is going to be drawn from
some distribution which is parametrized
by fine I now since G is a random
distribute it's a discrete distribution
like this that means that Phi I is going
to take on values theta K with
probability PI K right that's the
definition of a discrete distribution
basically and if you if you now say that
let's just say the data item I is going
to be assigned to cluster K if Phi I
equals to take some value theta K right
then you can you see that this the
original process mixture model is
basically a mixture model with the
infinite number of components each
component corresponding to one of the
atoms in your original process and the
PI's correspond to your mixing
proportions and theta K corresponds to
your parameters of your mixing purport
to you the parameters of your clusters
so this is a kind of a very nice way of
describing and a mixture component
mixture model with an infinite number of
components basically and it turns out
that it's equivalent to this infinite
limit that I've come described here okay
of a infinitely more finite models
okay so that's a a basically a
traditionally process and a traditional
process basically one of the
cornerstones of Basel not prometrics is
one of the basic models that everybody
uses and some I guess philosophy
off-base and not from metrics firstly
I've now told you that basically real
data is often very complicated and
really nonparametric model is suitable
for describing this on real data so you
do want to move to nonparametric models
a second reason for using nonparametric
models is that if you use parametric
models you often have to contend with
this issue of model selection so in the
case of mixture models how many mixture
components are there and stuff right it
turns out that model selection is a very
hot question if you look into them into
the literature and nonparametric models
basically sites that this issue at
basically little extra cost I have to
say two caveats I guess firstly that
non-private refroze are not never really
truly non-primary right often what
people do is that they start off with
some perimetry model and they fit and
they fit it to the data or they and then
they find that okay perhaps it doesn't
it doesn't really fit the data very well
so then you could kind of like make it
richer by not from non permanent rising
certain aspects of your parametric model
that does not fit the data well and this
this often results in models which can
basically be seen as semi-parametric
because it has certain components which
are perimetry and certain compost
components which are not primary okay
and a second note caveat is that because
nonparametric models are kind of very
rich models sometimes if you're not
careful you may end up with models which
are basically not consistent and you
have to be careful that in your
applications of non furniture of
nonparametric models that they can work
well for the problem that you look at
so move on to hierarchical additional
processes now right so for the
originally processes I've just motivated
it as you know clustering off of one
single set of data points so for the
hierarchical duration a process the
corresponding basic problem is still
clustering but your clustering multiple
groups of data points but in a but such
that the groups of data points are
somehow related to each other so here
for example we have three groups of data
points and say you might think that say
three clusters here maybe three clusters
here and two clusters here with some
outliers and what I mean by related is
that the clusters in the different
groups of data can be shared across the
different groups of data so if you so
you might fit mixture models to this
three groups of data and you find that
say this cluster is basically the same
as that cluster it's basically the same
as that cluster okay so basically we
have multiple groups of data we want to
cluster each group using a duration a
processing show model because we also
want the model to discover for us the
number of clusters
that's exhibited in each group but at
the same time we want our clusters to be
shared across this multiple groups of
data so just to motivate for you some
examples of how this clustering of
related groups of data appear in
applications the first one is in topic
models so if you're familiar with latent
the original allocation which is a very
popular more a topic model model for
describing documents in terms of sets of
topics you take a document alright and
you wish is basically a sequence of
words and you treat it as a set of words
in which to order
doesn't matter and then you model the
documents with a mixture model where
each components of the mixture is a
topic so here say I have three topics
and I've kind of highlighted words in
different colors corresponding to the
possible topics that the words could
belong to okay and of course you never
have one documents you often have lots
of documents so you have a corpus of
documents and you want to describe each
document within self topics but the
topics are shared across documents right
so now you have for every document you
have a group of words you want to
cluster the words together such that the
clusters or the topics are shared across
your groups of words you know mine are
documents and you want to also discover
the number of topics so that's precisely
the the problem which I've just
described here so this is in document
modeling in genetics this solve problems
has also occurred in so for example you
might have multiple populations of
individuals and you might want to
cluster the individuals into different
clusters based on their genotypes and of
course the populations are never the
same so different populations might have
different proportions of the different
types of of the different genotypes in
your in your in the human race Angus so
again we have for each population we
want to plus the individuals into
different genotypes the genotypes are
shared across multiple populations and
we also want to determine the number of
genotypes another example of this
problem occurs in visual modeling so
imagine that you have multiple it from
multiple images if extracted certain
descriptors from each image now you
might want to class this the cluster
that descriptors within each dimension
to the different objects in the image
and objects could again appear in
different images and different images
could consist of different sets of
objects so this is again
this problem of clustering of related
groups of data issue group corresponding
to an image now so how do we solve this
problem of sharing of two of the
clusters across this grouped data so in
in the statistic literature sharing in
group data is often addressed using this
notion of a hierarchical Bayesian model
so the simplest the simplest example of
a hierarchical Bayesian model is that
let's imagine that you have two groups
of data it is Group one and this is
group two and you might want to figure
out estimate the mean of the data points
in each group so this is going to be the
mean for Group one I mean from good for
group two and if you believe that these
two groups are related in the sense that
the two means are somehow related to
each other the way you do it is to say
that both means are random and they come
from say a Gaussian with a common mean a
global mean and this global mean mu
naught is itself a random variable and
because mu naught is random it would
induce dependencies across the means for
disk for the two groups right and it's
precisely this dependency which add
which describes the sharing of the
information across these two groups of
data in the case of the hierarchical
drift a process is basically a
hierarchical Bayesian model that shares
clusters across the different groups of
data so so
as I say so we want to plaster you know
the data points in each group using a
drill a process mixture so here is a
simple example in which you have two for
group one you might have a a draw from a
duration a process g1 which can have
gives you a duration a process make sure
to model Group one and then for group
two you might have a separate duration a
process due to which did which gives you
a duration the process make sure for
group two right so you might make this
into a hierarchical Bayesian model by
saying that the common parameters of the
two trishla processes so the base
distribution and concentration parameter
you can induce shape sharing of
information across these two groups by
letting this both of this be be random
variables but the problem with this with
this approach is that if our base
distribution is a smooth distribution
then we will never get clusters shared
between the two groups and we can see
this because recall that in the
dirichlet process the atoms in the in
the durational process are drawn from
iid from the basis distribution right so
we have a set of clusters here which
I've drawn iid from H you have another
set of clusters here which are drawn iid
from the same age but if H is a smooth
distribution then any if you draw a but
a bunch of samples from H here and a
bunch of samples from Asia these two
sets of samples will never overlap with
each other right because that's that's
basically the definition of a smooth
distribution it doesn't have atoms
basically it's non-atomic and if the
atoms don't line up and we know that the
atoms in the dirichlet process
correspond to clusters in our mixture
model then the classes will never be
shared because the atoms are not share
so the but the the basic idea here is is
that if we make the base distribution
discreet then the atoms will be shy
and this is basically what we do in a
hierarchical drilling process we simply
say that the common base distribution of
our to dirigida processes so both G 1
and G 2 are not gonna be drawn from a
drill a process the same duration a
process with a common base distribution
distribution of genome and we again
place a triplet process on top of genome
and because Gina is a draw from a
triplet process and we know draws from a
traditional process are discrete
distributions the the atoms in G 1 and G
2 will be precisely the set of atoms in
Jinan and that's how clusters are shared
across the two groups so visually what
happens is to draw a sample from this
model right do not G 1 and G 2 we draw G
from G not first that gives us a set of
atoms and we know that the self atoms in
G 1 drawn from G not so it has to be one
of the atoms in G not so the server
atoms here is precisely the same as the
same atoms here which is also precisely
to same atoms here so we have exactly
the same set of clusters which are shed
in the two groups but the mixing
proportions which corresponds to the
weights will be different now in the two
groups it's okay that's basically the a
high record reached a process in general
hierarchical dirty process is basically
a set of Duraflame processes which are
linked together in a hierarchical manner
right so you have a tree structure over
multiple Jewishly processes and each to
reach each racial a process has a base
distribution given by its parents in the
tree basically and turns out that there
are analogs of the stick breaking
construction which I can't prove Lee
described in the Jewishly process
there's analog for the stick breaking
construction for the hierarchical tree
clear process I have already told you
what Chinese restaurant processes yet
but there's also analogs for that in the
hierarchical Jerusha process
so that's a hierarchy additional
processes and it's kind of used it seems
to be used across quite a number of
different domains by now so tell you
actually an application of the
hierarchical Trisha process to topic
modeling which actually is one of the
motivations for when I started looking
into Iraq your own traditional processes
so we call that the I the problem we
want to solve in topic modeling is that
we have multiple documents and we want
to describe each documents in terms of a
set of topics basically a mixture of
topics and the most popular model for
for documents is basically probabilistic
LSA LSI as well as latent really a
location and the the model which we look
at is an extension you can think of it
as a nonparametric extension of LD or
latent to recently ramification
so this models each document as a finite
mixture model okay where the topics are
the classes are shared across the
documents so each document has this own
mixes make a mixing proportion we're
going to give each of the mix
proportions at really prayin again so
it's given by this and we have a number
of topics and each topic has a set of
parameters theta which is also given a
duration layer prior and then that the
generative model for LD a is that for
every word we're gonna first pick a
topic that's given by Zack GI and then
given a topic we're gonna draw the word
itself given the topic
and this is a very successful model with
lots of different extensions of it and
the particular extension which we look
at here is that we want a non-primary
extension in which we have an infinite
number of topics skip this
so the HDP can basically be directly
applied to produce a non-primary out
here with an infinite number of topics
so here each document has its own
duration a process so for document J we
have a draw from iteration a process
given with a base distribution of Jinan
and then for every word in the document
now we're going to first draw a topic
from Gigi and then we're gonna draw the
word given the topic so this is quite
similar to the to the Audi a-p-d
the difference is that the GSR draws
from directly processes so they have an
infinite number of atoms so we actually
have an infinite number of topics here
and now to induce sharing of the topics
across our corpus we would simply say
that the base distribution G naught is
itself gonna be random and be drawn from
some traditional process some global
traditional process and the base
distribution for the for this de
richelieu process is basically the same
as the prior over the topic parameters
in the LDA so so we have you can compare
the nonparametric Lda against a
Promotora howdy in terms of perplexity
so here I've run the HDP topic model on
a corpus that this is an obscure person
no there's not nips capacitor the
nematode copacetic
and it produces a certain perplexity so
it is given by the red line for the four
Lda is a parametric model so you need to
tell it the number of topics that it
that it uses to model or come at the
corpus so we vary the number of topics
in the x-axis here and you can see that
as you vary the number of topics if Cal
produces a u-shaped curve in which the
perplexity is lowest in this regime
right and you can see that the HDP is
basically doing just as well as the best
Lda model but we don't have to do any
model selection for it so it's kind of
in fact more efficient if you take model
selection into account you
from yeah DJ's just for that pretty good
I could see that gives me what I'm not
seeing is to work each topic the topics
are shared across all documents so and
that draws there is just so is something
question you abbreviating something here
right okay
the Feed the Future ads and things in
the sense across all documents so
effects fix just the the Detroit the
probability its selecting the particular
one property word varies from talking to
topics that Ford in to GJ yes that's
right man so it's not the DJ act I'm
drawing from TJ right is just is the mix
is the topic index I see
so am i confusing yeah so so the v-j
here it's actually not the index but
it's actually the parameters of the
topics itself so I recall that further
so here's the funny thing with the
digital process is that in a typical
mixture model you think of the mixing
proportions and the cluster parameters
as separate things for you you first
pick your cluster according to the
mixing proportions and then you draw
your and then you generate your data
according to the parameters given from
the corresponding cluster in the
durational process that found like they
are tied together so recorded in if you
have a draw for a duration a process
this is this
it is this infinite sum right of Delta
functions here right right so this is a
government from you directly process it
has weights and has atoms they are the
atoms corresponds to the parameters of
the clusters and the weights correspond
to a mixing proportions so if you draw a
fee from G here right fee is gonna take
on one of the values one of the values
of theta K so it equals to theta 1 with
probability PI 1 equals of theta 2 with
probability PI 2 theta 3 with
probability paltry and so forth so the
PI's are the missing proportions but the
Thetas is corresponds to the parameters
of the clusters so when it takes on the
value of the parameter of the cluster we
can then so that's precisely what that's
okay what happens here
based in tribution it looks like this is
dublicate
yeah so that's actually oh that's simply
the vocabulary size we're kind of like
assuming that we have a finite
vocabulary and the beta's that you can
the pseudo counts for the words for the
web types across the contest usually
people just take the bait has to be all
the same time foot up you so it's Carol
I roll into one so Phi J is gonna take
on value say it okay and then what that
means is that what x ji is gonna be
assigned to topically and phi ji itself
is gonna be the parameter for fo
topically
that's gonna be used to generate the
particular word X
that was actually an infinite sum here
because this is a so that this the
richly is a finite dimensional
distribution but still a smooth
distribution so you can still have an
infinite number of different atoms drawn
from the smooth distribution so in the
language modeling case later on it will
turn out to be finite okay so I guess we
should move on quickly we can also
generalize this hierarchical really
process topic modeling to the case where
we have multiple collections of
documents and every one topics to be
shared across documents within each
collection and also across collections
but of course we're under sharing
written is collection to be stronger
than across collections and the
hierarchical duration a process can
basically be generalized to this case in
a basically a straightforward manner so
you would have a different duration a
process for each collection if a global
original process which ties together all
the collections and then you have a
separate division a process again for
each documents in each collection so
I'll skip that basically it does well
actually it's kind of interesting to see
some of the topics which are could be
shared across different collections so
this is in the lips corpus and you can
see that for example the topics which
are shared between vision and signal
processing tend to be actually this
first topic a bit funny but if you look
at the second topic is kind of like
image processing type of words and the
topic which is shared between vision and
neuroscience tend to be about visual
like the visual context and sign with
them so it's kind of nice and as I say
so the HTTP has been applied um well I
guess there's two things here the first
thing is that there are also other
nonprofit or extensions of Oda so here
I've listed two of them so it is nested
Chinese restaurant process and touching
touching collocation the HTTP has also
been used in genetics so this is a
haplotype inference problem by Erik
Singh it's also been used in time series
modeling where we have a generalization
of the hidden Markov model where you
have a infinite number of states in your
hidden Markov model and this also can
used envision so the the visual modeling
example that I gave you the image well I
just cropped up from Eric's artist paper
right so I want to move on to language
modeling in the last 15 minutes of the
talk so the so this is kind of
interesting because there are two ideas
in the hierarchy really process this
hierarchical models and then the second
ones of course nonparametric models and
it turns out that both ideas hours can
also be used in language modeling so the
language models which I'll do is here is
Engram language models so basically
model a sentence the using you wanna
build a distribution over sentences
where the probability of a sentence is
simply a product over the probability of
each word given is its context which is
a finite which is a fixed length of
words which appears the right before the
word of interest would I so we have
predict what I even words I a minus n
plus 1 okay what is 1 and of course I
think everybody here knows that if you
have a for most of natural languages we
have a large vocabulary and you cannot
estimate the parameters
using naive mana I think I think maybe
Google so treat this as a Tweel's like
so if maximum likelihood estimation
doesn't work I tensor that naively
regularizing of remedies don't work as
well and the reason is because most of
the parameters in fact we will have no
associated theta at all so and then
basically we just simply reflect our
prior which may not be very good if you
have a naive prior and basically there's
a cafe really really rich literature in
on the problem of smoothing which is a
way of dealing with this sparsity of
language models of theta assists there
is language models and smoothing is
basically enough the basic idea is that
you want to interpolate are you into
average between larger and smaller
models so that the larger models are
able to give you the expressiveness and
the smaller models you can count alike
back off to the smaller models when you
do not have enough data to ask them in
your large models well
so and it's a really rich literature and
so I guess Chernenko bad did a really
nice review of the state of the art of
smoothing algorithms I guess ten years
ago and they I found that interplay that
can easily and modify can is a name
other am the best smoothing algorithms
daddy had so and so in in this part of
talk I would like to look into a
Bayesian approach for smoothing and the
reason for this is that this idea of
smoothing is really similar to this idea
of hierarchical Bayesian modeling we
should have just described because you
can think of each context in your
language model is a different group of
data and then you want to share
information across different groups and
people and you can use a hiraku
amounting a for doing that and
specifically the hierarchical model
which will build is based on the tree of
suffixes so what this means is that we
are assume that more the more recent
words in the context are more important
which is reasonable and the second idea
in this is that we'll be using
generalization of triphala processes
called Pitman your processes as the
priors in in states so we'll be using
hierarchical Pitman your processes and
the reason for this is that the few
manual processes produce power law type
of behavior which are much better models
for languages than the traditional
processes are and the nice results here
is that firstly we do produce
state-of-the-art language model results
and secondly in fact we can show that
interpolator conditioning is can be
interpreted as approximate inference in
this hierarchical pitman your language
so this is kind of nice because it gives
us a nice explanation for why is it that
interpreter can easily it does so well
yeah so what's our hierarchical model do
we do so I say that we'll be using
suffix trees so so here's a little
example so we might have different
contexts a hidden Markov right so that's
our context and a predictable model so
for different contexts you we have a
vector of probabilities for the for each
for the words which could occur after
that context so G of U of W is simply
the probability that what W occurs after
context of you
so this is G hidden Markov model so of
course these things are probability like
this so they have to be each entry has
to be non-negative and they have a sum
to one okay so we we have lots of
different contexts right because for
every pair of words here for offer every
sequence of words we have a different
probability vector and the hierarchical
model which we place on this set of
probability vectors is basically the
following whereas you cut as you go up
the hierarchy you drop the the initial
work from each context so in this case G
hidden Markov and G stationary Markov
post have the same parent which is G
amato and so forth and basically you can
think of G of Markov as the mean for
mean probability vector in a prior for
both of these probability vectors and
basically in this hierarchical model the
day that is sharing of information
across the different context on the tree
and the structure of this sharing is
precisely the same structure of sharing
that we see in the traditional smoothing
algorithms so the next question is what
are the distributions that's going to be
placed on each of these ages so what's
the probability of what's the
distribution over G of hidden Markov
given that it's mean is G of knockoff so
what is probability of G of U given G of
parents of you so the standard
distribution that people use on
probability vectors is of course the
traditional distribution and this
produces a hierarchical duration a
process language model and mcian and
peter has proposed that like about 14
years ago now but it turns out that this
is a bad model for language for
languages so yesterday an example would
have come varying training set sizes and
different context lengths and this is in
the politican a surname and the numbers
are the perplexities so lower is better
and this is modified kinesin a so this
numbers are pretty good and the
perplexity for the hierarchical duration
language model was is a lot worse so
this is kind of a bit unfortunate
because I think this idea of
hierarchical Bayesian modeling is really
is a really well-suited to language
models but you know because it didn't do
as well as interpreter can easily as a
lot of the other smoothing algorithms
people can't I gave up on
on the Bayesian approach to language
modeling for like ten years I guess and
it turns out that the problem here is
that the digital distribution does not
produce power law tough behavior which
which fits in with languages very well
and will be extending the duration a
process to pitman your processes so to
see how is it that the pitman your
process extends the duration a process
we'll be talking about the Chinese
restaurant process so the this Chinese
version process is basically a different
perspective on on the DeRusha process
and it gives us a distribution of
partitions so it's a Chinese restaurant
so yes we imagine that we have okay so
now here's a different metaphor okay so
we have multiple tables in our Chinese
restaurant okay we assume there's
infinite number of tables and customers
come in one at a time and they sit
around tables so the first customer sits
at the first table the second customer
could choose between sitting with the
first customer or at a new table and
then the third customer could choose
between sitting with the first customer
the second customer or the new table and
so forth and all the customers come in
and see our own tables like this and
basically the first customer sits at the
first table and subsequent customers
will sit at table K with probability
proportional to the number of customers
sitting around table K at a time and
will sit at the new table with
probability proportional to a parameter
alpha so what this producers is that if
you have like nine customers
if you have nine customers this produces
far as a partition of nine nine
customers into multiple tables right so
if you think of customers as data points
and tables as clusters this produces was
a distribution of a clustering of data
points and now for each table we can
associate a parameter for the cluster
which we can draw from a basis
distribution H and then the
corresponding data point X I would be
simply drawn from some distribution get
traumatized by the cluster parameters so
this gives us a basically intuition a
process mixture model okay so this is
for a duration a process what's the
difference in a given your process the
difference is simply that when a new
customer comes in that the customer will
sit at table key with probability now
gonna be proportional to the number of
customers sitting around table key
subtract away a discount factor this is
a fixed parameter that's between zero
and one and the new customer will sit at
a new table with probability
proportional to alpha plus T key so we
have K tables and we subtract
probability mass of D from each table
and we're gonna add all those
probability masses to the probability of
the customer sitting at the new table
and this is of course a generalized
generalization of the jewishnet process
which is the case when the equals to
zero
and why does this work the reason is
because so for the duration a process
you know we have the probability of
sitting at a table K is proportional to
the number of customers sitting around
that table right so the it has this rich
gets richer effect so this tends to
produce for us a few tables with lots
and lots of customers so you can think
of this as there are a few word types
which are really common in examples of
of a language so these are basically the
common words in the languages and things
like stop words in that article language
they are very common right but for the
people in your process it has this
second effect in which if we see lots of
words coming if we see lots of different
tables then the probability of the
customer sitting and a new table has
also increases and what this what's up
and this corresponds to the effect we're
in lots of languages we have lots of
words which all of them all of which
occur really rarely but there's lots and
lots of them so basically there's two
effects here one is that there's a small
number of common web types and the
second affects that we have a large
number of graduates and this in fact
produces a power load up of behavior
which is of course suitable for
languages so just to show you some
crafts
so here I've plotted the number of
unique words we see as we see as a
function of the number of web tokens
sent for the duration a processed
discount tapers off after well but for
the payment process increases a lot
faster then doesn't really process you
could actually assume that in it yes so
some of these experience so rights so it
is definitely possible to extend this to
the case where you could generate an
infinite number words from so you just
have to make sure that your based
distribution itself can generate an
infinite number foods it says that we
haven't done that we haven't tried that
yet but it's not the next things that we
want to do in the experiments we
actually we actually do to two things we
actually one we try cross validation 250
and the second one is actually we
introduced prior for D and then we just
do posterior something for D as well and
they work about the same since a cross
validation worked a bit pattern in our
experiments I think that's because even
though this is a more suitable model
dendritic processes it's still not the
perfect more language model for cuffs
right so I should wrap up okay so
basically it does pretty well it does
just as well as modify kinesin and in
interpolate the kinase and again in fact
the numbers look very similar and it's I
think the reason is because you can
basically treat the kinesin that a
method says very good approximate
inference algorithms for this
hierarchical payment your language
we've also looked into domain adaptation
with this with this language model now
extended it to a graphical payment your
process we can talk about this and this
plot can shows you the perplexity as you
increase your training set within your
domain of interest as well as some
general to me and in both axes that
products still decreases as you increase
the amount of data and it seems to work
pretty well against a simple baseline so
this is still working progress so
conclusion so I think based on not Pro
metrics is quite nice as some really
nice applications in both document and
language modeling and that's in fact
recent work that tries to much post
ideas together and I think the graphical
kid when your process framework may
actually provide a nice framework to
explore some of this ideas merging
document and language modeling and of
course I've only talked about some of
the work which I've done myself and
that's based on permit races a careful
growing field now and it's growing
pretty fast I guess in both machine
learning and statistics in statistics
people are interested in various
processes which go beyond directly and
pitman your processes well in machine
learning seems that people are
interested more in like things which go
beyond clustering and care like they are
structurally quite different so the
thing with the models that the statuses
people have been looking at
that there are different priors we
should be used but they are still used
for clustering and in machine learning
people are interested in using this
motor entry models to code to do
structure learning beyond figuring out
the clusters it's pretty efficient so
for the language modeling case is
actually very efficient because someday
I guess it is linear in the training set
so it's still not quite Google skills
but it's is linear and the nice thing
with the language modeling case is that
you could show that the posterior is in
fact convex it's not concave so it's a
unimodal posterior so you could get
today get samples from the troop from
the whole posterior very efficiently so
you need only a very few number of
samples from CMC for the document
modeling case it is well I mean it's the
same as in an Audi a collapsed skip
sampling seems to be very efficient and
this thing uses collapsed skip something
as well in fact I think the non profit
rate models tend to be more efficient
than a naive implementation of a
parametric large parametric model and
the reason is because it scales
according to the number of clusters or
the number of topics which are actually
used by the data while in the parametric
case the scaling is actually it scales
linearly in the number of potential
clusters which could be used to explain
the data which could potentially be much
larger than the number that's actually
used
the data
questions okay thanks alright so you
were you're going to get some said about
this law but rather one plus D then you
had another curve this is how fast you
new word survive
but give me the day but somebody or
others law for new keeps yeah so dude
she doesn't tie in oh I'm not actually I
don't know but it's long thought just
says that the number of total vocabulary
items you expect to see after consuming
and there's a relationship between I
guess this thing is that this plot that
I'm showing is actually a keeps like
this and I guess it's law says that the
the most popular word is significantly
more popular than the second-most
popular with and so forth and this thing
actually appears in the people in your
process too but I guess they are tied
because they use the same parameter D to
describe both yeah so I'm not sure
whether you could be couple them to
describe both the effects separately
that's tenth of the big hundred because
with sensor that the people and your
processes there's a really nice process
but you know it has two parameters but
nobody has come up has been able to come
up with a three parameter version that
actually has all the nice properties of
pinyon and traditional processes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>