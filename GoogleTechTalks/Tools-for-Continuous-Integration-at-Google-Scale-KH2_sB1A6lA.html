<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Tools for Continuous Integration at Google Scale | Coder Coacher - Coaching Coders</title><meta content="Tools for Continuous Integration at Google Scale - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Tools for Continuous Integration at Google Scale</b></h2><h5 class="post__date">2012-08-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KH2_sB1A6lA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is John mokou and I flew in for
this talk from San Francisco from the
Mountain View office and never been to
New York before
so unlike Ari I have not don't have to
work for material because New York
provided it for me
I was coming from the airport in the
shuttle there were five people on the
shuttle and I was the last stop of
course I'm going to be the last one and
number four the driver pulls up and he
said 112th Street the guy looks at him
he goes I don't live here
the driver goes this is where you told
me you go and they spent 10 minutes
arguing and the guys where he would
actually lived was like two blocks that
way I was like wow welcome to New York
so I learned a little bit about New York
um maybe I'll be back I don't know um
okay they really do with mean it when
they say that you know people aren't
exactly friendly here to each other
okay I just thought they they both
survived and yeah no yeah I was the
problem maybe that was a bad example
anyway so I want to talk a little bit
about the size of the system that we are
working on we have to test every day we
have about 15,000 developers in about 40
different offices they're pretty much
all submitting to exactly one branch now
the the projects all the Google projects
that you know and love about they're
only the ones that are open sourced like
Android and Chrome and maybe something
was open-source that I don't mention
those don't use this particular
integration system they stay with the
open source tooling but everybody else
all the other products are using this
system for integration testing so we
have about 4,000 more than 4,000 active
projects about 50 500 submissions per
day on the average and again I already
talked about it all being on one branch
and all submissions are at head and we
build everything from source there's no
you know partial builds or prepackaged
anything every every build goes all the
way to the bottom of the source tree and
builds it we have during you know 20
plus CLS per minute and with peaks that
are over 60 sales per minute
so about one a second coming into the
main repository during peak times 50% of
our code files change every month which
is kind of funny and we run over 75
million test cases per day so that's
just kind of a little bit of an idea of
the problem that we're facing so I want
to talk about a little bit how that
works how continuous integration systems
normally work and how the one at Google
works that's a little bit different so
first of all let's talks take a side
trip a little bit what what does it
continuous integration systems supposed
to do well two main functions that it
does right one is to provide real-time
information to build monitors when you
have only one branch when there's a
build break you want to know as fast as
possible and as accurately as possible
so you want to be able to say this guy
broke the build with this CEO right and
at some companies with a regular
traditional build which I'll go into in
a second if several seals are being
tested together how do you know which
one broke the build and that becomes
either a tribal knowledge or something
you have to have somebody smart there to
figure it out so you also need to be
able to handle flaky tests test that the
I'm Pig questions at the end thank you
so yeah you have to be able to handle
flaky tests when they happen because
it's just a fact of life that sometimes
tests are going to fail when they're not
really failing OS CL I'm sorry
CL sorry change list it's a single set
of files being submitted together by one
developer I'm using slang I apologize
I'll try not to do that but but CL is
one that this is going to keep coming
out so then there's a second function
for release engineers they want to be
able to take a green bill the bill
that's been tested and they want to be
able to ship it they want to put it on
the web they want to do whatever with it
so they need to be able to identify a
recently green build they need to show
all the testing that ran against that
particular change to make sure it's good
and make sure all the tests pass and all
that they also want the tooling to
automatically select one of those green
builds and use it and again they have to
handle the flaky tests because if a test
fails at a particular CL I'm going to
keep using that I hopefully will be ok
if it fails at a particular CL and it
actually Phil then they need to know
that so they can rerun it and get a
passing result and finally the last
function is you want your developers to
be able to develop safely in other words
you want them to know ahead of time if
they're going to break the build so you
want to be able to sync to the last
green change list the one that was green
before you want to identify whether the
change you're just about to make is
going to break the build right you want
to build that reliably and you were able
to submit and say yeah I didn't break
the build and know that you didn't and
again they have to handle flaky tests
too because for them to be able to say
I'm going to submit it if it test fails
and it's not really failing they can't
submit so they want to be able to know
about those flaky tests so how does a
standard continuous build system work
here's a nice little diagram okay so the
changes come in the first change the
second change the third change okay and
the standard continuous build basically
runs in a loop it starts it runs all the
tests when it's finished it says ah hi
it's either passed or failed and then it
runs all the tests again it syncs to the
new head so in this trivial example you
have three changes the first one gets
tested okay and if it broke the build
you'd be able to tell that it did maybe
it's the only one that's included but
then change two and change three come in
and they get tested together and now you
have a break of test one how do you know
which CL changed two or change three
broke that a particular test and the
answer is you don't unless you're smart
unless you have somebody who's really
good at isolating what the fault is or
you have tribal knowledge that oh if
this breaks Joe's Joe submission must be
the one so so what the Google continuous
build system does instead is we start
testing simultaneously every sale that
comes in at the second it's submitted we
monitor the source control system as
soon as that's change is submitted we
pick it up when we start testing it
immediately or as fast as we possibly
can and we test all the different seals
can concurrently so change one comes in
we start testing it change two comes in
even though change one isn't done we
start testing that one and then change
three comes in and we start testing that
one so we can tell that when test one
breaks change to broken not change two
or three but change to precisely that
changes the one that broke it that has a
big
because it makes the build monitors job
really easy this this change it broke
this thing and you know you can go to
someone and say hey with confidence you
broke it you have to have to roll it
back or fix it now you notice here that
we didn't run tests to a change - and we
didn't run tests one at change three you
might ask why we didn't do that well our
testing system uses the fine-grained
dependencies you may have been here for
previous talk by Michael Barr nothing
about the build system it uses the
dependency tree that's in the build
system to figure out which tests to
trigger so it only triggers tests that
depend on the code that changes this is
really important for Google because if
you can imagine some of the leaf
projects say Google+
they don't want to be running the tests
for ads some ads project when they
submit they only want to run the tests
for Google+ so only those things that
are affected by their change you're
going to get tested and only if you have
a change like in the core library
somewhere those poor guys are always in
trouble but if you have a change in the
core library somewhere then you can
break a lot of things and then a lot of
tests have to run but normally the
normal submissions only have to run the
test from the area where the code is
being submitted and so here's a display
it's kind of weird um and I have to gray
out the names of the test might BP said
no you can't see that so what this is is
a bunch of names of tests over here and
we did something funny we made the time
go in the reverse order so the older
older changes were over there and the
newer changes are over here and you can
see the little green checks are hey the
tests passed and across the top you have
all your change list going these are
older towards newer and you can see this
test was Green Green Green Green Green
Green Green up which one broke it this
guy and so you call the author of that
sea change you say hey your test your
change broke the build and they have to
roll it back out or they have to fix it
and we have an automated roll back so if
you hit submit a CL that's going to
break the build and it breaks it you can
roll it back take it back and it'll
negate the change and keep going now
sometimes and we allow people to make a
quick fix but if it's going to be like
home line
then roll it back and you'll we submit
the change after you figure out why that
particular test broke now you can see
this lovely little red scattered through
here you can imagine what that might be
those are all flaky tests and their
tests that are failing but it's not
hermetic when we say hermetic it just
means that it doesn't only depend on the
code that was being submitted it depends
on some other variable that's not
related to the code and this code is
perfectly fine or by some definition
perfectly fine it might not be even
might have might have leaky problems in
the code under tests as well as flaky
test issues this test is not showing you
a real failure and that's a real problem
for us at the scale we're running I'll
talk a little more about that problem
later so what are the benefits of this
integration system that we have um it
lets you identify the failures sooner
because it picks up the change as soon
as you submit it it tests every change
so that by itself so that I can
immediately tell who broke the build
okay and it avoids this sort of divide
and conquer or tribal knowledge you know
the 20 seals that are getting big
together which one broke the bill and
that's always a problem to figure out
and the more changes that are in it and
the more complex changes that are in it
it's hard harder to do it um
the use of the fine grained dependencies
help to reduce the computing cost
because we only test the things that are
dependent on the code that changed it
helps to keep the the build green by
reducing the time to fix breaks because
you're going to find out as soon as you
possibly can that you broke it
google has a lot of tests hundreds of
thousands of tests and some of them take
a long time to run we actually put a
time limit on them we tell them no you
can't take more than fifteen minutes for
the test to run and we can retry it a
couple of times there's all overhead and
but we guarantee that after you submit a
change that you'll have a result pass or
fail within 90 minutes of a change going
into the system and we're stacking them
up so all the changes that are coming in
the mini a minute are all being stacked
up and none of them are more than 90
minutes behind the head okay this has
been accepted enthusiastically by our
product teams they love
okay they're dicted to it we try and
take it away they're like no you can't
we need it it enables the teams to ship
with amazingly fast iteration time
there's teams in Google Google Plus is
one of them that developer submits to
change its live on the web within 36
hours of them submitting a change and
it's all part of this continuous
integration and there's other kinds of
testing that they do but they have a
pump and they're shipping releases out
onto the web every 36 hours of that
product now some of the other products
are a little bit behind in that they're
not chipping quite so often but all the
Google products collectively are moving
in that very fast direction we want to
be able to take developer code and get
it out to our customers as fast as we
can so well now here's the costs well
gee it requires an enormous I'm using
the word enormous I wasn't allowed to
put a number there but it's enormous
investment in compute resources it does
help a little bit to be Google we have a
lot of computers and it grows in
proportion to how fast developers submit
we hired developers what is the first
thing they want to do when you want to
write code okay the test time the
average test time is growing the
different variants that people want to
do is growing I want a debug built in an
optimized build and I want to use Val
grind that I want to use all these other
tools that take compute resources when
you run valgrind on an application it
runs about ten times slower than it does
without Val grind and that means that
the duty cycle on those computers that
are sitting out there take 10 times
longer on those as well and branches
sadly I told a little lie we actually
have two branches of main development
one for the very chorused of the core
because we found that the overhead of
those guys submitting the chorus of the
core stuff is too high on our compute
systems we actually have to so it's not
quite true that it's only one but we
release the chorused of the core stuff
four times a day onto the main branch
and it just goes and runs all of the you
know 500,000 tests that it's going to
trigger one runs and the other thing
that might not occur to people right
away is that the dependency map
instantly changing so before we can tell
what tests are to run on a given change
we have to compute that dependency map
so we have a live dependency map it's in
memory and every time there's a change
we rebuild that map before we ask the
question what test should I run okay and
that takes time so after the change is
submitted we pick it up and we run the
dependency update part of the system and
then we say ok what what test should we
run and then we start you know filling
them out so um let's see ok now I switch
gears a little talk about the other half
of the system is for free submit if
you're going to keep the bill green you
need developers to be able to test in
our experience the best way for them to
do that is to be able to do exactly the
same thing that we're going to do with
their change after they submit it then
we can make a good prediction of yeah
this is ok it's gonna it's going to pass
so it meant we made this testing system
completely available before submit ok it
uses the same fine-grained dependency
mechanism and it recalculates any
dependency changes that you made in your
workspace so if you go and change the
build files and you change the
dependencies you have a new file that
needs to be compiled whatever it is we
keep track of those new dependencies um
we use the same pool of computing
resources that we used for the for the
changes after submission but we give a
higher priority to pre submit because
some developers waiting we want to get
their CL into the build we can't wait
for them so we allow them to use the
same compute resource and we give it a
higher priority this avoids build breaks
basically because we let the developers
and in fact some teams many teams have
instituted a practice where they are not
allowed to submit until pat pre submit
the presubmit tool says yeah it's good
and it captures the content of their
change and tested in isolation against a
known good starting result so you got
your starting position you're going to
make your change you test it exactly the
same way you would after submission and
you know you have good confidence that
it's going to pass and we always test
against head so even though your
workspace might be a little behind it
can't be up to date because somebody
just submitted something a second ago we
we test it against head at the time when
you start and
this identifies problems this tool
identifies problems with missing files
because when you're on your workspace
you do a pre submit run we have to
capture the workspace content and ship
it off to our servers to be able to run
the build okay and we capture it right
at the beginning any changes they make
don't show up in the test results it's
kind of a mix a double-edged sword
because they could actually break
something after starting the pre submit
run but if they forgot to put a file in
the change list and it's in their
workspace guess what it won't pass
because we only captured what was in the
change and not what was in their
workspace or their file system so it's
completely divorced from the file system
that developer has and I can't tell you
how many problems that just avoids
because people are always forgetting to
add new files or they're forgetting to
you know put the files it together into
the change it integrates with the
submission tool there's a way to say pre
submit my my change and if it passes go
ahead and submit it and so it's sort of
integrated in that way it's also
integrated with the code of view tool we
have a very strong code review policy
here at Google everybody reviews peer
reviews their code and this pre submit
will actually put the results onto the
code review thread so everybody who is
looking at that code can see the results
of testing it so it's pretty cool but if
we're going to have all these people
submitting on one branch you've got to
have it so this is an example result
from the pre submit tool oh my VP missed
the names oh it's okay um so here's the
pre submit tool you can see it groups
things for you it tries to help you to
point to what might be a problem here so
we skip 223 tests skip tests or tests
that are like too big if they take too
long to run or if they're marked local
which they can only be run in the corp
side they get skip they go here still
passing we ran 1366 tests on this
particular change and all of them were
passing before and passing after the
change that's a good sign one test is
newly passing hey maybe they fix
something with this change that's a good
thing and one test was newly failing and
this little mark means it's a timeout
it's probably likely that that's a plate
and then one test is still running on
that particular change and when it's
done you'll get the final result so this
is just the UI that the developers see
when they say okay I'm done pre
submitting my change what happened oh
well yeah mostly you can ignore it you
fix one thing and maybe you broke
something maybe need to look at that
so here's just a little overview of the
system architecture sort of how it works
how its put together okay we have this
dependency service that constantly is
monitoring for change lists that are
coming into the system and updating that
dependency graph so that it's up to date
and it actually it can actually take a
parameter of what change list you want
the dependency graph for and it rolls it
forwards and backwards one of my
engineers wants to do a multi you know
so you can have like many versions of
the tree in memory at the same time but
we haven't gotten that fancy just like
one version of the tree and we know how
to roll it forwards and backwards by
applying the changes forwards and
backwards so we analyze each change and
we figure out sort of the the content of
the change and we may write a little
packet that says here if you have to
apply this change or take it away here's
what changes you need to make it's like
a patch patch to the dependency graph
and we store those and we can roll
forward and backwards on them and then
the change whilst at the same time are
going into a different server that's
supposed to find the affected test for
this particular change so it'll go
through and ask the dependency server
hey what tests were affected and if the
dependency server isn't done updating
the graph it'll say wait wait wait a
minute
it'll hold it make it wait they'll pause
until it gets the answer and one of the
other things is we allow a concept of
projects that group a bunch of tests
together that are relevant for a
particular team like the Google+ team or
the ads team they all have their
projects that define what tests are
going to run and what tests are relevant
to them what tests they care about not
breaking for those things they're
allowed to associate flags with them
that are for different variants we want
to debug bill to that and we want to
optimize bill to this and we want a Val
grind build of that all those project
flags can mean that you take the
effective test the test you know you
want to run and now you multiply it by
every project that has that test
in it and it's project settings so if
one is optimized and one is debug and
one is Val grind one test might get
three things that you need to execute
the debug version the optimized version
and the Val grind version all have to
run so even though you know here you
might have some number of tests this is
going to basically multiplies the
cross-product of this another thing that
happens because as a consequence of this
these project this project concept is
these projects have patterns and then
say every test under this directory
that's what I want to run as part of my
project ok and so we have to update
those Maps as well because you add a new
test to a particular directory and it's
going to show up in somebody's test list
because we and we have to know that
because it's part of the pattern it
matches the pattern that they said
everything in this directory should be
tested as part of my project so we're
updating those at every seal as well ok
and then we put them into a big queue of
tests that are waiting to run and then
we ship them off to the build system
which is what Michael Barton talked
about the last time it the the test
system is the biggest consumer of the
build system because all of our test
actions execute on their built on their
computer arm so we send them all the
tests and we say go run them we monitor
we make sure it's working and then
eventually we get back results there's a
pub/sub that gives you give us us the
results and we get that pass/fail and we
put it into the database and we display
the nice little picture about what
happened pass this thing work it didn't
work it failed it didn't fail all that
good stuff so just a quick you know high
level view of kind of how the system is
put together so well now I'm going to
talk about flaky tests I know I
mentioned them a few times along the way
um some people calm sporadic but they're
just a test basically as the system
assumes it assumes this kind of a system
that tests pass or fail reliably given
the code that the only input that
decides whether the test passes or fails
is the code if that's the way it was my
job would be a lot easier ok it would be
a lot easier unfortunately a lot of
tests don't have that property and
they're called flaky flaky tests um
and I've been at several companies
working on build systems Google's my
third and they're just a fact of life a
certain amount of flaky tests is a fact
of life because the test could have
races in it there's all kinds of reasons
I have all the reasons here the machines
could fail the environment in a
different test could impact each other
we found some tests that fail when the
resources get overloaded they like
timeout because when you're running 10
builds on one machine and and they're
all like you're running the worst-case
test on every one of those 10 things
that you're running on that one machine
you can crunch it and then the test
might start timing out because they just
because they take longer and if you're
right near the edge of a timeout you
might get a timeout those sorts of
things so we've we've started to work
with it I'll talk about a little bit on
the next next slide but there are all
kinds of reasons why these tests can be
flaky and they're not all possible to be
fixed and certainly not all possible to
be fixed at one time because what do
they say about once one change a minute
unchanged a second in the peak right so
the rate at which new flaky things are
being introduced you have to in it
they're hard to fix so the rate they're
taken out you know it's kind of hard to
get to a state where there aren't any um
and not to mention the all the other
reasons the infrastructure and all that
that can cause problems so she's
laughing it's true
okay so again having these flaky tests
means that you really can't find changes
that are breaking the build reliably
anymore all of a sudden you got that
like I showed you in that other picture
where those those nice little red spots
that are showing everywhere um actually
that project that I showed you up with
the red spots they're actually actually
declared a sort of mini code yellow it's
a google term for sort of a problem
they're having a problem with flaky
tests and they're as a team they're all
trying to attack their particular
project and making sure that the flaky
tests in that project or are eliminated
or fixed it's an addendum and you also
have trouble finding green builds for
the releases because if you have these
flaky tests showing up it makes it you
don't get a green result and then how
can you release because you don't know
whether it's right or not
you could be a real failure that only
failed at one CL and maybe got fixed in
the next one but likely it's not it
wastes work for the build monitors
because you're monitoring the build you
see that if it's failed now you have to
go bug somebody or do you you go bug the
developer oh yeah I know about that it's
flaky you know okay
ways to compute resources and we're
wasting tons of resources running tests
that are sometimes not producing a good
signal for us and inappropriately
failing pre submits this is probably the
biggest one right if we tell people you
can't submit until you get a green and
sometimes they can't get one easily it
means you're wasting their time there
they're having to run the tests a few
times or rerun the test it wastes their
time and that's and that's the developer
time is at a premium right that's that's
the most that's the thing you want to
optimize for almost even more so in the
computers solutions to this problem we
do all of them we fix them it's hard
requires developer time if I go to some
fast-paced group like Google+ and I say
hey I want you to spend time fixing your
tests instead of writing new features
for your customers their VP is going to
look at me and say what why more
machines deflate them I don't want to
know um you know it's it's that
developer time is really hard to get
especially when they're competing with
the ricean resources they're going to
have to build the next version of their
tool the customers are using and that
we're making money with I mean it's it's
a hard sell and in some cases it's not
the right choice there are some flaky
tests that are you know infrequently
flaky and that are hard really hard to
figure out what's going on that it might
be better not to fix because that
developer time involved might be more
expensive than then the just running
them again um hiding them we can we have
a system for retrying flaky tests when
they happen so you just we have this
thing you mark the test flaky in the
build file and it'll run it three times
Oh first time fell if you run it again
okay and if it ever passes it says yeah
that's good we think we're trying to
identify infrastructure flakes so we're
trying to we're trying to eliminate all
the sources of flakiness that are coming
from our system because they do come
from their overloaded machines and and
different reasons why the infrastructure
and show up and and cause flakes and
we're trying to get rid of those or at
least report them right so hey look we
know we flaked sorry and we're trying to
use metrics we collect metrics about
which exact test runs were flaky test
because at night when we have a few
spare computers
um at night we go and we every failure
that occurred during the day we run it
to try at that same exact change to
figure out whether it was real or not
and we marked them after the fact saying
yeah that was flaky yeah that was like
you know no that one wasn't flaky so we
can actually discern and then when we
see a test that flaky a lot now we know
Hall yeah when this one fails maybe it's
not really a real problem when we give
that those statistics back to the users
so that they can use their judgment and
say yeah that test yeah
it's probably and we track them and
provide the metrics as I just said so I
got a display on the next slide here
that talks about again the gray box um
so here's a list of tests over here and
over here it's telling you how flaky
these tests were and the percentages
that might be a little misleading this
percentage huh yeah well the percentage
like you might be a little misleading
that's a percentage of the number of
times this test failed how many times
was it a legitimate failure that was
caused by a breaking change and how many
times was it a failure that was just
flaky so it's a hundred percent flaky if
every time it failed it was a flaky
failure and not a real failure so like
you see that second one there it says of
one recent fail this is over an entire
month so in an entire month that test
failed exactly once and it was a flaky
fail the one time it did so it's not as
bad as it might seem from 100% in fact
people have said maybe we shouldn't do
that metric we should try a different
one but that's the way it's that's the
way it is now so you can see here they
have you can see some of the more
important ones like here this test
failed 177 times this one failed 500
flakes 572 times in a month okay so you
can see that's uh if you were a manager
testing manager some of those the one
that's flaked you know 572 times in a
month I might want to have somebody look
at that one I might get my device go fix
that
but the one that failed once in a month
I'm gonna say look cover that one over
because I you know whatever that is it's
gonna be hard hard to figure out so
again it's just a matter of realism
that's realistic you have this problem
with these tests that that they don't
always accurately reflect the state of
the code so it's a pretty significant
problem for us okay even Google can't
buy enough computers that's that's the
next section so the sources of growth
and text execution time as I said more
developers always come in raw hiring
people there's this thing inside Google
called percent and it's how many people
in the company are you newer are you
senior to basically okay and I've been
here about a year and I think my number
is like I don't know eighty-five percent
that means that that fifteen percent of
the Googlers that are here today are
newer than me and it used to grow a lot
maybe the company we're starting to slow
down in terms of you know the
percentages but it's still alive um
anyway so Mort F we tell people to write
tests we want you to write the test I
want you to write the tests all the time
every day write them longer running
tests well as that systems get bigger
the tests that test them get more
complicated and they take longer to run
and the tests are very good now you
weren't you test on multi-threaded
server it could just blow out a machine
I mean you can take the whole thing over
I mean you're in any question about that
right so you know that that's a really
important problem to because the the
test that you do have are requiring more
compute resources harder predict how
much compute resources are going to take
you want to give them as little as
possible because you want to pack them
into as many a few computers as you can
but they're trying to take as much as
they can get we don't have really have a
good system for adjudicating that at
this moment and it's hard it's a hard
problem so I did a little work to
examine what the growth trends are for
this trying to figure out okay well how
fast is it growing how much how many
computers we're gonna have to buy next
year to keep up with this thing so and
also to look for any sisters that we
might need to do to stay ahead of that
so here's a graph
the light blue underneath is the number
of the daily number of submissions for
every day over the last year how many
submissions were made to the codebase
notices some that's another thing my VP
had me gray out was the unit's so it's
just show you the trend um and I think
maybe the VP's shouldn't may have don't
tell them this but you see this here
that's 4th of July and that's Christmas
ok just so you know and it's pretty much
standard every year we're at the July
Christmas Google happens to give a long
4th of July holiday so you know there's
a big dip there and a big dip at
Christmas um but you can see we're
linear in growing uh you know and it's
gone up maybe 20 30 percent in the last
year that's pretty good considering how
many people we hired maybe it's almost
close to the same number I just said my
percentage maybe does the same a bright
or not but so that's the growth in sales
per day and it's linear thankfully right
it could be could be nonlinear at least
as far as we can see from this here the
oh the black line is the 14 day rolling
average and the Gold Line is a linear
fit to the to the line and this is the
number of test seconds that we spend for
every change that comes in ok this line
is also is more wavy but it's actually
less of a band right the blue with the
blue underneath is the actual number on
every day and the black line is a
two-week rolling average and the Gold
Line is linear fit so the number of test
seconds it was a little less than double
so we're spending twice as much to test
the CL a change and the changes are
coming in say 20 or 30 percent faster so
if you have two variables and and two
factors and you multiply them together
what kind of a curve are we going to get
okay that's exercise for the reader see
if you're awake stone it's a quadratic
and this blue line is the total
execution time in our execution system
the black line is a two-week rolling
average and the Gold Line is a 2d
polynomial fit to the rolling average
parts yeah to the data so it's quadratic
so anything eventually we're not going
to be able to buy enough computers
that's the moral of that story
I mean we're Google we might go out
there are ways okay but we'll hit the
knee sooner or later we'll be screwed
need a bigger planet well Moore's law
helps right I mean but it's not going to
even that I don't think I Moore's law as
a way to tread water you know with this
kind of a problem so but it isn't going
to get you there either so the
conclusions of that part are you know
given that it's quadratic again
ultimately we can't buy enough computers
to run every test at every effective CL
we're very sad our customers are sad
we're sad we'd like to be able to do it
we're going to our VP the saying G just
keep doubling our resources and we'll be
able to do it and they're already saying
well okay sorry um in fact we figured
out at one point that if we wanted to
double the resources here we'd have to
take every spare computer in every
datacenter Google has and that's when we
got that look for every B so you're
going to do what okay right now we
haven't done enough to provide incentive
for teams to optimize their use of the
shared resources for testing so we have
a number we've hidden this all from them
for as far as the developers and vuelven
managers are concerned you submit a CL
and it tests it for you within 90
minutes ooh we like that but you know
how heavy the tests are whether the
tests are adding enough value whether
they're one of these things we've given
for free I don't know why we did it I
wasn't here then but they they came up
with this system called sharding that
allows a test to be divided into as many
separate parts as you want up to 50 and
each one gets its own execution engine
okay and what happens when you divide
the tests up under more execution it
just takes less time to run oh yes I can
get my answer faster well guess what
there's a lot of overhead built in to
each text action setting it up and
uploading the data and doing all this
wonderful stuff before you can actually
run the test and what we found is that
as you go up with a number of shards you
get a tiny benefit hi
benefit like seconds on your execution
time profile but you double the number
of resources that the system is using to
run that test and no one is looking at
it because why I mean I had to develop
for the other day her tests timed out
and her response was oh damn it double
the number shards you know and that's
exactly what she did
this doesn't matter to her she doesn't
care I'm doubled on four shards yeah Oh
doesn't I'm not any more happy um but it
cost twice as much computer resources to
do that so we're trying to provide
incentives and and try and show people
their resource consumption so some of
the things we're thinking about and
again there comes as I've said before
even if you give them all the data and
in charge them with sort of monitoring
their resource profile resource
consumption there comes a point where it
doesn't make sense to do it because the
engineering time investment would be too
expensive and it's it'd be better to buy
the computer at some point I'm not
saying it's it's the zero right now we
have zero it's some number bigger than
zero and less than infinity I don't know
what it is but it's somewhere so we're
talking about is enforcing perhaps
execution quotas for teens so a team
would have so much budget to be able to
anything they want they want on valve
Ryan to ten times the cost okay fit it
in your budget somehow and the the VP's
can go in a room that I don't have to be
in and they can argue about who gets
more quota oh it's Google+ no its ads no
it's sorry um you know so I don't have
to care they can come out with a number
you know here this guy gets 5% more than
that guy happy
I'll do it it's good I would definitely
recommend that for the people who are in
the build system you know just try and
factor yourself out of those kinds of
discussions okay and then you have a
smarter scheduling so we're definitely
figuring out that right now our
scheduling engine is pretty stupid every
effective test run it pretty simple
we're going to have to have something
smarter that's able to scale back the
number of tests that are running to meet
with the computer resource that we
actually have okay so you have this many
computers you can get them all busy and
then as soon as they're busy you have a
backlog and now managing that backlog
and doing it in a smart way
is our next generation system that we're
thinking about working on some
engineering problem miners love it they
absolutely love it right it's like
running water and a beaver I mean okay
so I'm gonna keep myself from laughing
sorry
so we're thinking about smarter
scheduling the periodic green builds so
every we don't want to so you will have
to do some culprit finding in such a
system because the computer resources
aren't there to run it at every Co so
you gotta like space it out and get
periodic greens and we'll be skipping
over something else we're gonna try and
minimize that will we use whatever
available resource we have well pin it
down before we start cutting back but we
have to be able to cut back if we want
to keep up with head and guarantee the
90 minutes the other option we could
have just said is look you'll get your
results at night you know and a lot of
the teams like the fast pace teams don't
want that answer they don't want to have
to wait until night to get their answer
from a CL they submitted at noon they
want that answer right away and so we've
decided for us that getting that answer
more quickly means that we're not going
to have exactly precise culprit finding
and we'll have to implement some kind of
wonderful algorithm that when there is a
transition edge can go back and do the
culprit finding and figure out that's
the CL that broke it again a wonderful
engineering problem and the last one is
the flake hiding being able to use
resources to be able to hide those
flakes because they're going to happen
and you have to sort of paper them over
to get accurate results for people for
the releasing and for all that stuff so
alright I hope nobody fell asleep I mean
that one new your comment and she's
going to hold it to me okay um I put
some links here for other resources more
information about some of the stuff
that's already been put out publicly the
different presentations and talks you
can go look at those um and I guess now
we're ready for questions are we yeah
okay so
oh is that on no is there a switch on
there now try it yeah no oh good and
again ah there it is everybody oh we're
good so you mentioned earlier with
regards to pre commit that certain teams
were mandating that that be run before
any change huh is there anything you can
elaborate on as for why that's not
becoming the standard um I think it's
the culture at Google to have very
highly autonomous teams and teams make a
lot of their own decisions locally that
are right for them and what's right for
a team that's trying to ship every hour
or every four hours or every 36 hours is
not the same answer for a team that
might only be shipping once a month we
have teams who routinely break the build
and leave it broken for a while because
it's not that they're placing a premium
on getting that new release next month
or next week or two weeks from now
instead of every a few hours and for
those teams you know they can make a
trade off I mean we encourage people to
run them but the highly autonomous team
model really just sort of it's a Google
cultural thing I've only been here a
year but I could tell you for sure we
don't want to tell teams what to do for
the most part as much as we could
possibly do it we just stay out of the
way and give advice Google yeah if
you're trying to tell them what to do
you have to duck I've tried to tell I
had to tell them that they weren't going
to get every test at every CL and that
was a very unpopular message and people
came and like beat the crap out of me so
yeah they don't they don't like to hear
bad news or be told that they have to do
something thank you mm-hmm so I I would
like to know what type what are the
types of tests that you do do you do
load test stress in base your that's a
good that's a good question in this
system we limit any tests to 15 minutes
and we constrain it so that you should
not be talking it's not strictly
enforced and maybe it should be should
not be talking to any production
infrastructure because that's something
that can cause test flakes right if
you're talking to some server that's up
in Google somewhere and that server goes
down for maintenance or gets updated or
whatever your test starts failing so we
this system is not designed for mostly
not designed for load testing or stress
testing because those typically take
longer
then what you're going to be able to do
in 15 minutes some tests some teams are
trying to squeeze some of those tests in
and that's good and we have a separate
system for integration test called
guitar that's meant for longer running
tests or tests that are more complex or
tests that have to bring up servers in
production um do you have to pass those
tests before you release to production
so again as I said in the talk the this
test is a quick test at every change
list and then we have the integration
test take longer and most teams again we
don't dictate or mandate anything most
teams require that both the integration
tests the longer running integration
test and the quick running tap tests are
all green before they submit before they
you know push out a change or one more
question do you have how do you end all
schema changes that a database changes
all you handle that ah as I said most of
this the testing in here is very fast
unit testing and limited to fifteen
minutes in duration and all the testing
we have starts with you know you can
give it can data but there aren't any
real servers involved and so if the
schema change happens you have to submit
if anything needs to be submitted you
submit all the schema changes and then
you build either an in-memory database
mock or a real database using the schema
on the fly on demand for that particular
test and if it's too big to do that with
it's excluded from our system and you
have to run it on this other integration
testing sis framework that we use huh
guitar yeah actually have two of them
one's called guitar one's called sitar I
don't know why it's this musical
instrument thing but that's what it is
well I can't give a talk on that because
I'm not the dude who does that but I can
get you the dude who does maybe Ari can
be talk
Kurt into coming out here it could be a
good part of this series in fact you
should I was going to add a slide from
from Kurt from the guitar system in here
select couple of sides but since I'm not
the guy you might ask too many questions
I would know so the and how many testing
and rumors do you have that are distinct
or gates that are you know you have to
pass CI tests you have to pass
functional tests you have to pass load
test so many of these together well
again I can't answer that in the general
case we provide and this is again the
way we do it at Google we provide all
the frameworks and systems for this and
it's up to teams to pick
choose from the palette which systems
are going to work for them and which
gates they want to have in front of
their submission so I can tell you for
particular teams you know I could say a
little bit but it varies some teams say
yes everything's got to be the top
filled it's got to be this we even have
some manual testers still I think
they're in India somewhere but they come
and they actually you know bring up the
servers may they try them out to make
sure it works so so yeah
teams have those guys in the loop to
some teams do so like the translation
for example I think that's one of the
teams because you translate something no
computer in the worlds can tell you
whether it's right they put people in
front of it and they say is it right
that's right so pump is older we've got
a scenario where you can break a build
with a failed test but that actually
doesn't detect that the CL and question
broke the build but it detects is the
seal detected the broken build that's
correct so you can have a CL 5 back that
suddenly exposed a parameter which this
is the first test that happens to
actually stumble on it well it's the
probe you'll tree in the forest that
crashes if nobody hears it that's some
what did it make a noise that's somewhat
unlikely um because of the way we're
using the fine grained dependencies we
know every file source file that
contributes to our build and to our test
and we put dependencies on the data
dependencies on the even the data files
that are input to the test
so it's hard it's not possible but it's
hard for a change to come in to
something that doesn't trigger the right
test for that we over run tests I mean
in the sense that we're conservative any
thing that depends directly or
indirectly through any chain on a file
when that file is submitted
it runs the tests that are relevant to
it so well we the case the kinds of case
I was thinking it was the core
functionality would say an exception
condition say a resource overflow if
that isn't part isn't created and
generated and forced as part of a
pre-existing test the first person to
come around with a CL that actually uses
the facility oops yes there's all kinds
of defects that aren't tested for that
can slip through and they can be caught
either by tests later or by real
production code problems or any number
of other things we have a pretty strong
regression test
feedback loop in most teams you know
that when something breaks they had a
regression test for it and hopefully it
won't break again just want to make sure
in this to the presentation correct
and do you include any performance this
to measure time if some some
functionality the trade performance
again this framework is not being used
today for any performance testing it's
only for correctness and the performance
testing is either on the guitar
framework or we have some ad hoc some
teams even developed performance tests
as part of their own team infrastructure
that you know we're sort of looking at
and saying yeah do we want to pull that
in or not that's sort of their various
years from Google board here for example
I come with the ads group and I know
coming here from the maps of my company
you have specific questions on how
Pacific group moving Google music school
I'm happy dance or behalf of my
engineering that I can yeah it's harder
for me to answer how anybody uses my
tool because at Google all I do is I put
it out there and people either use it or
they don't it's kind of the model the
way Google is it's kind of googly yes I
didn't know that was an addict until I
came here but then when you run the
tests how'd you get the results of the
feedback back to you do you get it off
to focus has to finished running or if
you get a file you're than the tests do
you get certification meeting well
believe it or not the build system is
fully integrated with the test system
Wow
isn't that amazing thing and they we
have a pub sub channel coming directly
from the build when we invoke it so the
pub sub it's streaming results to us as
soon as they're found a failing test or
a failing build we get that information
instantaneously when it happens because
we're right on the pub sub channel it
takes delivery time but you know it's
like nothing in computers is really
instantaneous but you know it takes
delivery time we get that in right away
now we do mark the test as failing
immediately as soon as we get the result
incremental e as we go through the build
but we don't compute the project status
until we have all the tests that are
included in that project so we roll them
up into a green or a red for the project
and that only happens when all the tests
for that project are done but when you
do
the posts limit game you have 20
projects that have all triggered tests
for this particular CL and if any of
them are done they get calculated and
get a green or red
the second all the tests for that
particular project are done so we try
and stream it as fast as we can and be
as close to the real bleeding edge as we
can so from an individual developers
point of view the speed with which you
can move is very much driven by that
feedback so I'm really concentrating
here on the preset
yeah that's right so you said that the
quality service guarantees you have a 90
that's kind of on their proposal men so
what's the what's the sort of average
pre submit feedback hug if something's
failing is it going to come back within
a few seconds well again you're bonded
in that case by the time it takes to run
the longest test that you triggered so
though in the pre submit case you won't
get any if you if you fighting a test
about learn about it until all the tests
run no you'll learn about the individual
tests as soon as it finishes but you
won't get a complete picture of the
change until you get the last test which
is the longest one it'll take however
long it takes
typical times are in the twenty to
thirty minute range for a pre submit for
a large project you know and developers
complain about that and I'd say look you
know it's your longest test that's
you're waiting for not not my system in
fact we have a guarantee of not only the
end-to-end time but we have a guarantee
of overhead taken by our system and on
pre submit we guarantee that we're going
to have less add less than two minutes
of overhead to your pre submit run and
that might seem like a lot but it's
necessary for all the functions that
we're trying to do compute the
dependency grid and scheduling the test
so but we in post submit we're allowed
to have fifteen minutes so that shows
you we're really trying to put an
emphasis on pre submit getting those
results as fast as we can
try to work constantly every quarter in
my okay ours which is Google speak for
my objectives for the quarter every
quarter is decrease this latency by so
much and we're constantly trying to
crank the latency down and tighten our
response time so we can get quicker and
quicker results to the users one of
these are focusing on now is changing
the way we do flaky tests instead of
doing them one after the other after the
other which means you have to run that
long test three times if it flakes up
we're trying to doing simultaneously so
we can run
probabilistically we know this test
fails lakely 76% of times so we run
three copies of the test at the same
time that means if any of them pass
we're happy and we don't have to wait
for the three in a row thing which just
drags out the test time
so hi I'm curious what's the rationale
that you have only one branch um oh no
this is a wonderful philosophical
question I could spend a lot of time
answering it let me give you my two
cents okay um I worked at my previous
company they had 80 branches okay and
that's how they chose to do it is the
Maverick that 80 branches okay and a
developer over here would submit a
change and it was a chord change maybe
and it would take three weeks for it to
integrate into the main line and three
weeks for it to come down on to a side
branch so if I'm a team that's a leaf
team trying to build a product with a
core team that needs to deliver me a
feature okay I have to wait six weeks to
get that feature and guess what it
always has a bug in it always they
didn't consider my use case properly
there's some weird thing maybe pass all
their unit tests but when I go to
actually use it in my albums like wait
you look I need another parameter for
this because I need to know whatever and
then it takes another six weeks to get
them the feature the reason why Google
chose the single branch and I think it's
a good choice is they can put all the
resources on testing the one branch
instead of dividing the resources up
between testing the many branches and
they can get the integration to happen
like this I have a guy who's delivering
me a core feature so I can do my leaf
thing I get it as soon as he submits as
soon as he submits I can sync and I can
build with it and that makes a huge
difference for us it allows us to that
it eliminates that integrations like
also the Mavericks had an army of
release engineers managing the
integration constantly worrying about
branch to branch merge and merge
conflicts and we don't have that so I
assume that this is one single target
view that do you have a different time I
remember I kind of lied but differently
have to be nice but for example if you
build the Chrome browser that you're
going to test in different client
platform right as I said in the
beginning of the talk the open source
projects are not using this build system
but like Google+ and Google web search
are both using this system and when they
submit things that are only related to
them it only affects their area it
doesn't trigger any other tests from
other areas so yeah we're taking
advantage of that but they all depend on
the same core stuff like the server code
and all the other stuff that Google
maintains and that stuff is good it
could be changing and they might need to
rely on those changes and we get them to
them right away so well I be surprised
because I just went to lunch with those
guys but a couple weeks ago so it's so
of to question the first question is
with so many submissions coming in on
how do you how do you compute the the
dependency tree all the time I suppose
is really expensive to compute that tree
yes it is I mean you had let's say you
two different submissions how are you
going to compute those two and those who
have okay so the way we do it right is
we do it sequentially in the order of
submission of the changes right so
there's a linear line of changes they go
1 2 3 4 we go along that line always in
exact sequence we don't try and do out
of order and we keep each one we
generate from the that particular change
of Delta and we look at the contents of
the submission submission in this
directory cannot affect the dependencies
of a distant directory so we limit what
we actually require a - hopefully a
minimal set sometimes we have bugs and
our algorithms that we get a bad patch
and then a dependency can be wrong and
we have to regenerate our tree from
scratch we have a utility function that
goes and regenerates it the open a bug
we fix the problem so it's a tricky
problem but doing it sequentially and
producing patches then allows us to roll
the tree forward and backwards we know
how to apply those patches in either
direction so you can ask for any change
list across the entire line and we can
give it to you by rolling one patching
back and patching forward that's that's
how we're doing it now it is very
expensive to compute and hence the the
15 minutes for post submit a lot of that
time is taken up by this dependency
calculation another part of it is taken
up by the pattern matching when you say
everything in this directory is should
be tested by this project both of those
maps have to be kept up to date yeah so
by single threading it and doing
the patching strategy that we have it
seems to be we're able to keep up and
also some smart strategies about exactly
what we need to wreak worry when
something changes helps to on the other
question is I know a Google Google has
many offices around the world
ah for this system um probably you do we
have this does this is only applied to
one office of it now our office I
actually can't I know it's global and
and I got to tell you the only reason
we're able to do it globally is because
we have the best network in the world I
don't think that's a secret right right
but okay where do you sit down when you
do Google web search where it's not fast
okay so you know I rest my case
okay there's lots of people here all the
Google folks who are willing to answer
questions come up here and then any of
us are happy to answer so I still have a
question about the branching or the
single or the two branches how do you
deal for example if you have a hotfix
you let's say you release something to
production and you realize a big problem
now I need to fix it but now you have
all these commits and you your let's say
your test fails now because the race
question it's a great quality you and
all that you write again you let again
the answer is I lied again we also use
branch at hey see you catch me all the
time it's not good do you branch in the
code we when we would make releases of
each package we branch the code for the
release on to a release branch then we
build from that branch we branch from a
known good CL we build from that release
branch the packages for the product we
ship them on to the web and if there has
to be a hotfix or a patch it goes on to
that patch branch that was created
specifically for that release we don't
do a lot of testing on those branches
and in fact it causes problems for us a
little bit when you're far along ahead
and you have to do a hotfix you might
have to merge it into the patch branch
and you don't have good facility for
testing there and it's a problem we
don't we don't support really testing on
those patch branches usually what
happens
being a web application is very easy
right we don't have as much of that
problem with some of the other companies
like MathWorks has a huge problem with
that but Google doesn't because most of
the time when it gets to those Harry
merges we just move forward move the
whole to take a new cut at a different
CL get all the changes that were rolled
in and release again so generally and if
it's been long enough we don't do hot
fixes we just do a new release so but we
still have that have to have that
capability and that's those relief
branches do so what do you mean you're
not running all the tests at every
change list there's got to be another I
mean I guess if it was me at a small cup
I might say okay I'll just make my own
Jenkins Hudson thing and run just my own
code in my own branch so I know if I
break something I can point it to some
particular change list like does your
accounting system help at all like you
say okay I have one hour of compute time
and you limit it that way and so people
have to scale back their tests can you
get because it just seems like be losing
the being able lose the pinpoint losing
the ability to pinpoint a particular
change and say this change broke thing
yeah it just like it's one of my one of
the people on my team compares that
ability to like crack okay but you know
it is it's like crack I mean you get a
hit of that and you just want it right
it's good stuff I mean uh sorry maybe
that's the wrong analogy but you know I
it's it's like they want they want to be
able to have that ability and people are
really not happy that we're talking
about taking it away and I'm hoping I
can leverage that into a little bit of
motivation for teams to cut back on
their resource consumption look if you
want to run a fewer tests at every CL I
can do it okay but you'd have to be
fewer and you have to work with me on it
and that's it that is the first message
that's been resonating with my product
area teams when I go to them and I say
look this is it this is what we have
this is the growth curve put up the
growth curve like oh my god yeah you
know and and you have to work with me
and yeah that's going to be effective
and it may be effective to lower the
coefficient just enough that we'll be
able to do it for longer but when I see
quadratic I know we're get it's going to
be an end eventually
and it just has to do with this power
growing and scaling so I have two
questions the first question is um
you could probably cut down on the
amount of time you're spending running
testing some sort of like calm threshold
say like it somebody does a check-in and
maybe two tests fail then you say
alright this is bad as long as you're
not flaky test you don't run the rest of
the test you just kind of kick it back
to them yep
you know there's all kinds of strategies
we're considering certainly we've talked
about different strategies like that one
of the things we're trying to understand
is how much complexity we're going to
own right we don't want to own the
system that's very complex because the
more complex it is the more trouble it's
going to have for us to maintain it so
we're always trying to push ourselves
especially with a system like this you
really have to be quite simple because
if there's if it's really too
complicated but people understand and to
work on it'll be not right so so we've
considered some solutions like that and
we at the end of the day look at it and
say you know maybe that's a little too
complicated we'd like to have something
simpler and again a little bit of
pushback on the t's may lower the
coefficient enough that will be good for
another couple years I don't know and
also pressure on the VPS to say hey give
us more machines actually we're now
getting big enough before this we've
just been piece milling machines you
know going and asking for some and we
get some and and now with this growth
curve we've actually had to do six-month
planning for the first time you know
like most big teams around Google they
have to do long-range planning to say
here's what we're going to need next
year and have the data center people
build it out and that's what we're
starting to get to with this system is
we've learned from some of the bigger
teams that we have to do that we have to
go resource planning and we have to say
look six months from now we want to
double the size of what we're doing
where can we put it and they can say
well you're nuts but if you could get
the VP to approve it might go here now
that kind of thing and my second
question is so you have a 3,000 people
like say in this office and you've
basically replicated it so that whatever
test is going to be run when they check
in sky gets run also on their local
pretty much the same way so why don't
you just flip it and trust what their
local did if their local is that is
technically identical the problem there
has to do with the CL submission rate
because it takes 20 minutes to test and
during 20 minutes do you know how many
sales get submitted a lot okay and we're
taking a risk with that presubmit result
saying that enough
broke it in between the time when they
started the presubmit and the time when
it finished its just in that 20 minutes
there might be you know several thousand
CLS that got submitted and any one of
them could have broken their test or
could have interacted poorly with their
change so the answer is we can't yet
test fast enough to be able to do that
because it requires us to keep up with
the pace of submissions yes could we see
from the precip yep out a set of CLS
right yes there's that complexity thing
I was talking about yes we could
possibly figure out from the dependency
graph whether a steel that was submitted
in the between could have potentially
interfered with this one and if it if
there weren't any then we could say but
I'm guessing with yeah that that could
be another that could be another thing
we try we try and tackle I hadn't
occurred to me but that's definitely
true we could figure that out
sure okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>