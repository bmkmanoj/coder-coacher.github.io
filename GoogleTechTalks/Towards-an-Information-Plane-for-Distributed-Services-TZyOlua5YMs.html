<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Towards an Information Plane for Distributed Services | Coder Coacher - Coaching Coders</title><meta content="Towards an Information Plane for Distributed Services - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Towards an Information Plane for Distributed Services</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/TZyOlua5YMs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">since I knew about to the work of
thomond
seven years and met a lot of his
students but I never had a chance to
meet him in person even though i read
probably over this year's more of his
papers and I read the actually other
literature but tom is a very well
recognized authority in internet
measurement and he has done one of the
best work in the field in identifying
internet on very fine granularities
properties of various portions of the
internet and what also impresses me is
that he and his team are able to
synthesize all the state of the art that
was achieved before there are many teams
that just do their own story and don't
pay attention to what's going outside
but his team is not so likely out yeah
actually you are looking at the overview
of course all the best that the team's
Thomson team has done but also at the
best measurement community have achieved
over these years yeah so fair enough
thank you very much for that kind
introduction so what I wanted to do
today was talk a little bit about some
work that we're doing the my group is
more broadly interested beyond just
network measurement on issues of how to
build a planetary scale or global scale
distributed systems and kind of the this
effort that I'm going to describe here
is really try to merge those two
together so can we take the output of
the network measurement community which
has been making a lot of progress has
its own conference does a lot of really
great work and actually bring it in some
way produce useful data that would be
good for optimizing distributed systems
so that's the goal here is to say rather
than necessarily think of coming up with
great innovative new ideas but rather
can we actually produce data in a way
that will be useful for for distributed
systems and this is work with a cast of
thousands difference of these people
have been doing different parts of this
work arun is at
UMass and some of the Yatton actually is
now working for google but most of this
has happened at University of Washington
so actually what I'm going to talk about
is free related problems and they're
related because of the following kind of
common theme that goes throughout all of
them so the three problems are ones of
how do I know what what location what
physical location and IP addresses it
and we're not going to answer that
question we're going to say well what
region is an IP address in the second
one is can I predict what the properties
of a path are without measuring them
directly so can I know what the likely
behavior of a path will be before before
actually observing it and that's useful
just for a number of different
applications I'll talk about some of
those and then the final piece and it's
very very recent that that's why you can
tell it's not been published yet is on
real-time black hole or brown out
diagnosis they can I know things about
what's happening with respected
connectivity the common idea behind all
of these is a in addition to to
understanding topology that is using
topologies all of the information
sources that we have about the internet
and integrating and synthesizing them
together to be able to get better idea
of what's going on to be able to produce
useful things for distributed systems
and all of this work is in work in
progress even the parts that have been
published their ones that we're still
continue to refine and improve so one of
the key observations about this and I
realized that being here at Google this
is going to seem obvious to you but for
the rest of the community isn't many
people think that the Internet's too
large to measure particularly measure in
any kind of real-time sense and get
useful online data about it but clearly
you do just for web pages but even if
you just look at the topology of the
network it's actually you know pretty
big if you take every link in every
router and every every end hosts but if
you actually aggregated it at a at a
fine at a kind of coarser grain level
which is essentially what what pieces
what at what level is is they're useful
performance data
what levels they're useful data from a
point of view of a distributed system
the internet is actually quite small
there are only roughly a hundred
thousand pops that is a pop sir points
of presence that has essentially route
independently routable entities in in
different ISPs and only about a couple
hundred thousand different prefixes that
are independently routable so you have
relatively small number of things that
you have to actually go and measure and
that's what makes a online measurement
traffic ille so okay so for geolocation
the question that we wanted to ask was
could we come up with an automated way
of doing geograph of identifying
geographic locations of IP addresses and
you can think of the existing databases
commercial databases that I think Google
uses some of them that that exist out
there and they're somewhat hand-tuned as
well as some kind of mixture of
automated and in hand to process our
question could we produce a our goal
really was one of saying well if we're
going to start building the next
generation of distributed services in
the academic community we should have an
infrastructure that said here I can you
give me an IP address I'll tell you
where it is roughly so what our goal was
to try to get it accurate to a major
metropolitan area that allows you did
you targeted advertising and it'd be a
web service and that the key thing again
is going to be delivered all possible
information sources I probably don't
don't have to argue why you want to know
where things are I will only say that
for our perspective one of the reasons
for us to get into it was it was
actually a key piece of technology for
us to be able to do some of the other
work that I'm going to talk about that
is understanding where things are end up
helping us kind of narrow down some of
the possibilities that we might might
consider for example in doing alias
resolution if I if I have if I'm
observing different parts of a router
knowing that those different parts might
possibly be in the same geographic area
allows me to narrow my search down
significantly okay so here's the problem
setting I have a number of different
vantage points all of the work that
we're going to talk about here leverages
planetlab so think of several hundred
programmable vantage points around the
world in this case this particular part
of the
is only going to focus on the u.s. just
so we're in a defined region we have a
set of vantage points that i can make
measurements between those vantage
points that can make measurements two
endpoints and those different kinds of
measurements can either be paying that
is end hosts measure end-to-end
measurements just latency round trip
times or they can be traced routes that
as I can understand what the what the
sequence of topology is between them
that is what sequence of steps what
routers they go through on the way and I
can also make measurements between
landmarks and then what I'm really
interested in is locating the red dot
right that is some arbitrary point some
arbitrary prefix that is not a landmark
but one that I want to know where it is
there is a lot of related work and I'm
not going to talk about any of it but
but what we are going to sorry I'm going
to talk about one piece of that related
work which is called constraint-based
geolocation and explain why that doesn't
work and then talk about our approach
which is called apology based
geolocation and and that's the outline
just for this first part of the doc so
if I'm trying to locate something in the
middle here you could think okay what I
might do is I might ping it and
integrate over all of the different
circles that that the kind of round trip
times from these various places so if I
ping it from various places I can
isolate where this red dot is going to
be based on kind of the the intersection
of those and that works really pretty
well I don't know where in this region
it might be but I do know that that
based on the round-trip time that it's
going to be somewhere in there and I
might as well just pick the middle and
if i don't care if i only need to know
whether something is in arkansas then
that actually works pretty well the
problem comes if that red dot is
somewhere outside of the convex hull of
the of the information sources that
you're using so if I'm in Florida and I
don't happen to have a measurement note
in Florida which maybe I don't then the
world is really a lot worse off so if
you look at what's going to happen from
my measurement point in Kansas it's
going to be pretty large right half the
u.s. even the one in Houston is going to
be you know only narrow it down a little
bit and the one in Atlanta is going to
basically say okay that that's going to
define my my region and
if I take the center point of that
region well actually I'm going to find
out that it's actually I'm going to
think that everything is and everything
that was in Florida is actually in
Atlanta which isn't really what I wanted
so the question is going to be how do I
get better reliability given they have
all this information but I might not
have vantage points in all the right
places that I want so if i evaluate this
kind of constraint based approach the
skin published research in the
measurement community what you'll see is
a kind of systematically that error goes
up as a kind of linear function with how
far I am from a vantage point so closer
I am the better I can I can get the
estimate the farther away i am the worse
off I am and so if I'm close then you
basically if you're close then ok that
says you're within you know that you're
close to the vantage point therefore
you've got a fairly tight bound if
you're farther away if you're far away
from a vantage point then you know all
bets are off so can we do better can we
do well for the case where you're not
anywhere close to a vantage point and
the real problem here is that these
distant points are unconstrained they
could be anywhere and just based on the
rank on round trip time you're not going
to know any of it so the two the hard
part here is going to be to use or the
two parts of this equation are going to
be to use the topology of the network to
be able to constrain the locations of IP
addresses and then and then essentially
to run a constraint solver that's the
easy part is to run the constraint
solver on that so here's a couple
examples of of ones where you can see
the kind of behavior of the system for
example if I have a path or various
paths between my systems well any of
those round trips can take wherever Z if
they're going to deep may actually take
some circuitous route and if I have a
security route again I'm going to have a
much larger window in which to place
something if I happen to know as a
consequence I kind of because in this
example I'm not going to get a very
close relationship to t but if I can
place our that is some router that all
of these nodes go through then I can
limit my limit limit the bandwidth
significantly in this case
I the intersection of the router may
well actually be fairly well placed and
then the the the placement of the node
beyond that is just then a
straightforward a region around that
router even if I don't know anything
more than just where that router is and
you can see how that's going to be a
much narrower bound on it than other
ones we're going to add some techniques
to this over time but that's the basic
idea ok one of the key things about this
you might think that I could do this
just you know the simple case that I
just did which is solve a given
constraint by itself and then go on to
move to a different constraint and what
I'm going to argue here is that I need
to be able to solve all the constraints
simultaneously that as I have to
actually treat it like it to contain
solve a rather than it it or process so
in this case i have x that knows where r
is and z that if it's trying to place I
guess that's trying to place our right
I'm not actually going to be able to
place our very well right this the
region that this R is going to be in is
now you know fairly large but maybe it's
ok and if I do the same thing for s I
think that's what we're doing here i can
again place s and now if i add to that
the notion that i can bound the
relationship between snr because I can
do / hop information then I can end up
with a bound if I add that to the bounds
that I had before whoops sorry then I
end up with some fairly narrow
positioning for our that is the ability
to use the information that Z and Y
we're close to s and that s was close to
are added to the information that W is
close to R and X is closed star allows
me to get a fairly narrow point on the
information about our so the final point
that I was going to make about this I
think this is the final point again
these aren't my slides is that it's very
important to be able to cluster
information about different routers
together and this is going to be this
common theme that you want to be
operating at the right granularity so in
this case if I if I don't know that snr
are close together to each other for
example that
that there are both interfaces on the
same router or if I can't know that
they're part of the same pop then then
my information that i have about placing
s and placing are going to be much
aren't going to give me as much useful
information with respect to t in this
case if i only can see in my in my
traces x going to t through our and and
why going to teeth or s then i'm really
not going to have a great understanding
of where this the combination is so if
you kind of look at it from this
perspective the outcome of this is going
to be a fairly large region because i
have to say that it's where s can move
and where r can move separately whereas
if I can somehow say that both of these
are the same router or connected
together then the region is and much
much smaller okay so now I actually
forget what this says another issue that
you can have is that you can get length
like estimate hop lengths as well so the
basic model of the approach is one where
you have known locations of landmarks
you're going to have our tee tees from
these landmarks to constrain feasible
locations for all of the intermediate
hops between you and the router the
router and the targets and then you're
going to din run a constraint solving
problem on the whole thing in order to
in order to eat yield the result and
what you can find is that this is
actually remarkably effective right that
is being able to over constrain the
problem knowing the topology the network
allows you to be able to solve the
problem from a much greater degree which
is so this was our CDF so that's a
cumulative distribution function of
essentially any point on here says
there's a 20-percent likelihood that i
have found with the old scheme a twenty
percent of the route of the kind of the
positions that i find in the old scheme
are within 2 250 kilometers and you know
some portion and remember the old these
points by the way are within the
continental US so or actually it's
remarkable that there are so many that
are so far
it's right you know the thousand
kilometers is you know larger than Texas
so so you know you have some large
section of them the median number is
about eight hundred or so kilometers not
actually very very specific whereas if
you have this kind of topology based
constraint then you can get a median
error that's a much tighter and does
better on the kind of worst cases now
the second part of this work was this
try to it okay what other pieces of
information can we have that will help
us locate this information and there are
lots of different information sources
and we're only going to use a few of
them here but it's an indication of how
you might be able to integrate topology
information with other information
sources and in this case you could use
some available database that you could
go by there are other pieces of
information for example DNS locations in
many routers have dns information and
there were named information that
encodes some information about the
location of the router it's often wrong
that is sometimes you'll see a router
with a name you know that's in that's
actually physically located in New York
City that happens to be you know happens
to have a name that had been moved there
from some router that had been in Topeka
or something like that so the this
information that you can get out of DNS
is not always accurate and one of the
things that we had done in previous
study previous uses of geolocation
information was to just manually go in
and fix the information that we could
get rid of so the question here was
could we come up with safe ways of using
the information that would allow us to
use it without within it with an
automated technique so in this case for
example I might have a hint that a
particular router is in Boston say
because it's got a name that uses BOS
and its name and then another one that
has LA and it's the you know LA in its
name and and that I have some
information that I have oh and say
another one that I think might be in
Detroit and I have a landmark in San
Francisco clearly Sam just goes a long
way from all of these but I can
potentially no information about
clusters of things that would that would
help me for example if I know that these
things are are hinted to be in Chicago
well and these things are all cluster
than I could say that that also is a
hint for a cluster for Chicago
and that therefore if it's the case that
that these are all really close together
because of their hop there there Layton
sees and therefore this has to be
constrained to be close to Chicago then
this probably can't be a hint for Boston
right so that begin is probably wrong
and I essentially do some kind of
waiting mechanism in order to do this
constraint solving simultaneously
similarly once you've had you placed one
of them then you can verify that hint
some other hint that you have is
inconsistent with the other hints that
you have for example clearly Los Angeles
is not five milliseconds away from
Chicago and so we'll have to drop that
hen off and not use it and then speed of
light times round trip time limits the
path length and then that gives you some
bound for saying that the rest of the
world is then consistent with that okay
so here's an example of where this
really pays out basically if you're if
you're traipsing downs and this is why
we started introducing this information
if you're traipsing down some part of
the internet topology and you don't have
any kind of cross links between things
but rather you're just it just looks
like a part of a part of a treat writer
at a tree limb that just by itself in
this case the only way that you can get
from plant lab to Alabama am is through
this router in Atlanta and it's just how
all the packets are routed through
through Alabama A&amp;amp;M is coming at it from
from this router and atlanta is in
general a bunch of the internet has this
property particularly in the third world
then the kinds of solutions that I
talked about originally with respect to
placing routers really wouldn't tell you
much right because you're just seeing
this long chain and say okay out this a
location that happens to be in northern
Alabama I really could be anywhere
within a five-state region so are there
things that you can use for with respect
to hints and one of them would be that
you know I have a hint that this is in
Birmingham because of the label of the
of the router which would be BR HM that
says okay maybe that's probably
Birmingham right I don't know for sure
but it's you know consistent and then I
can if I believe that hint then I could
or verify that Hannah at least
consistent with all the other data then
I can then place a lab ma am at least
within Alabama
and then if I have other information for
example the location of the University
of Alabama Huntsville and then I know
that the IP address for Alabama
Huntsville is in fact a certain a
certain location because I know where it
is on the map then I can use that
information similarly to be able to
bound the router that's an upstream from
Huntsville and then as a consequence of
that where that that Alabama am has to
be somewhere in northern northern
Alabama that's the idea anyway and if
you take what is essentially the worst
possible case for what we do which is
university data sets and the problem
with universities is they tend to only
be routed down these done these long
chains then you actually do much much
better using these topology based
information with passive hints and if
you can think of doing were driving or
adding additional information that seems
quite plausible you could do a lot
better than this and if you could kind
of consider this compared to where we
started this kind of median error is you
know only about is you know within a
metropolitan region it's it's only about
50 km 50 60 kilometers and i should say
this is all for locations that are far
away from we've restricted this to only
those locations that are far away from a
landmark so if you're within if you're
close to a landmark then of course
you're going to get a fairly close tie
in right essentially it's how do I
simulate the behavior of a landmark if I
don't have one okay so this kind of
summary here is that these purely delay
based techniques those purely paying
based techniques without an
understanding of the underlying
structure of the network don't help you
too much the understanding that apology
in the network at the right granularity
helps you a lot that is you have to
actually understand what which routers
go with each other because you can't
actually measure every interface to
every router and place them well unless
you actually understand the constraints
between some of those routers with each
other and then if you can add hints into
this is Amanda can actually end up
improving the improving the case where
the network structure is insufficient
for bounding okay so I'm going to move
on to another case study that's similar
this can our goal
here was to be able to produce data
that's useful for distributed systems
and here what we're going to do is to
try to predict path performance and this
is a couple papers that you can read up
on this and what I mean by path
performance I mean all different kinds
of path behavior and again on predicting
it rather than measuring it directly I'm
using other measurements to the system
to be able to tell what's going on and
here's the motivate motivating example
here is BitTorrent if you look at how
BitTorrent works today I go to a tracker
the tracker returned so in order to go
find a bittorrent swarm I go to a place
that's going to tell me all of the other
people who are current or some subset of
the people who are currently downloading
the file that i'm downloading for
bittorrent so the client tracks this
contacts this thing called the tracker
and that then provides a list of peers
for the for for me to go get and that
sounds great and today the default
behavior is to return a random number of
these peers so random selection they
could be anywhere in the world but
clearly what I want in terms of what I'm
turning it back from the tracker is some
more directed set wait i'd like to have
ones that are close to me in some
meaningful way so out of the total
number of people who are downloading a
swarm I'd like to somehow know that
these ones here are likely to give me a
high bandwidth connection or likely to
be low latency and those other ones over
there might be random so you don't want
to only base the results the returns
from the tracker only on locality again
what that would do would be to tend to
create little clumps of of clusters and
not and essentially disconnected pieces
you know you have a swarm for for north
america and the swarm for europe so you
want some amount of randomness to be
thrown in as well but you do want a
favor the notion that i might be able to
download from people who are local right
if I know for example that somebody else
at the University of Washington has
already downloaded some something that
I'm interested in some already
downloaded the Linux distribution it
makes no sense at all for me to go off
to MIT to get that distribution rather
than going to em going there that's the
idea that is how can we build better
swarming systems using this now what
that requires me to do is from the
trackers perspective predict between any
two points in the network what the good
performance is right and I have to do it
without any measure I have no active
measurement between though
two points right I'm just trying to
return something to somebody say here's
the ones that you want to use there are
existing techniques that will give me
latency predictions they're essentially
treat the internet as a black box and
really cool systems I mean just you
really amazingly weird that they work at
all so just to give you an example of
what GMP does it embeds the Internet in
a hyper dimensional cube that is you
know n-dimensional seven dimensional you
know it's like how did that work and
then tries to basically point fit a
closest fit the measurements that's
making between the between the various
landmarks and end posts to be able to
locate end points and landmarks within
this n-dimensional scheme and then from
that then takes the euclidean distance
from it this makes has no rational basis
for how Internet routing works but it's
remarkably good because basically per
fitting works really well in a lot of
different cases the real problem with it
isn't that it doesn't work it doesn't
work great but it works pretty well and
actually you can find a serious actually
uses one of these versions which is
called Vivaldi which is a little bit
simpler than than full GMP but basically
based on the same idea actually azureus
which is the most popular bittorrent
client uses it in order to be able to
judge who they should be connecting to
hers they should favor for giving giving
a bit torrent BitTorrent communication
with because you want to go to people
who are close to you because they'll
likely have better bandwidth the real
problem with it is that it's not it's
not extensible other bandwidth it only
gives you latency predictions it doesn't
give you bandwidth predictions yet
and with me
it's in
so essentially if a number of parties
that are coping could given
yeah so
it doesn't matter where you are when you
separate
yeah so so let me see if I can translate
what you'd say clearly latency is
unrelated to ban to access link
bandwidth except you know if I'm in
general you know depending on where i am
sampling view I could be very close to
you where I could be very far away and I
don't know what your access link
bandwidth is as a consequence of that so
one thing that is true is that and it's
a kind of separate thing that that that
we've been looking at is there is a wide
diversity of different bandwidths that
different peers will have and as you
might imagine a large large number of
people have got dsl or cable modems or
whatever and then some number of people
who have just very very high bandwidth
and so in order to actually do really
well with in a bit torrent system you
would like to connect the people who
have very high bandwidth in general
particularly if you're high bandwidth
yourself you really want to go find the
people who are high bandwidth and that's
almost independent of latency in that
sense if you're trying to do fast
downloads particularly if you can do if
you get amortize it over a longer period
of time the people who have a lot of
bandwidth actually add a lot into the
system in this case we're actually
asking this of the tracker that as we're
saying okay if the tracker is going to
try to organize who's talking to whom
and that's really the trackers goal how
can it do that strategically in order to
improve the performance of the system
and in general even if I'm just talking
between dsl modem or dsl customers I
want to connect the people who are close
together just in latency because they'll
be able to transfer faster and get
better better TCP throughput than ones
who are talking further away ok so the
structural approach here is to use
measurement to the internet structure to
predict the end it so the two parts of
this i should i should point out the
structural approach is one that's going
to use the structure the internet to be
able to make these predictions and then
what we're going to do is to use
measurements of the internet structure
to predict the route that the packets
are going to take between these two two
points a to b and then compose the
proper measure and compose the
properties of those of those links in
order to on that predicted route in
order to get the performance and i'll
show you this in a second and this is a
the work here is called I plane and the
idea is going to be that's going to
predict path properties on the internet
and then predict multiple metrics and
then use it on distributed services so
going to have three four parts to this
one is that I have to be able to know
what the internet is going to look like
a lot of that same structure of what I
had in the previous part of the talk I
have to be able to predict routing that
is if I don't know what paths somebody's
going to take between two points it's
going to actually be pretty difficult
for me to understand what their
bandwidth is going to be right or what
their latency is going to be or what
their what their loss rates going to be
it just if I don't know where they're
going then it's going to be very
difficult so I have to be able to
predict the predict the actual routing
between these two two end points despite
the fact i could have no measurements i
have no ability to do Trace routes
between them and then and then finally I
have to be able to measure these
properties of the sorry measure the
properties of the links in the core and
then and then finally the one of the one
of the big challenges here is I have to
be able to measure the actual edge and
access link bandwidth that is I also
want to know what the bandwidth is for
the at the edge of the network so I'm
going to use a bunch of information that
we have in order to build a structure to
atlas of the internet again i'm going to
use planetlab also there a set of public
traceroute servers that ISPs host around
the world and so we're going to use that
information to build an atlas of routes
and then and then what we're going to do
is from these various traits are
observers plus planetlab do Trace routes
to various prefixes again we're going to
try to aggregate at that level cluster
these interfaces into pops and then
repeat this repeated lather and rinse
okay so that's the structured structured
Atlas is really one of okay measure from
a lot of different vantage points the
two pretty much every prefix in the
world and you think okay that's too many
addresses to go do but if you're
aggregating at the prefix level there's
only a couple hundred thousand of those
and then try to try to cluster those
into into pops so okay there's only a
hundred thousand of those and then
measure the inner pop links okay there's
only a couple of there's only a few
hundred thousand of those so it's
actually from a measurement perspective
actually a relatively tragical problem
okay so i can build the structure dat
list and then the next problem that I
have is how do i do path prediction and
here again we're going to steal sorry
for the wash out here but we're going to
steal some ideas from the rest of the
internet measurement community so I have
a set of different measurements that I
made it mate and apologize for the
washouts but figure these
these things are just various cities
that I have so Seattle up there might be
a vantage point I have another vantage
point a trio I can measure from Seattle
and another one in Chicago matters from
Seattle to all prefixes so I have
measurements from seattle to portland i
have measurements from portland from
seattle paris and measurements from
Chicago to Portland Chicago to Paris I
have once Rio de Paris Rhea to Portland
whatever so I have a whole mass set of
things and they all intersect and and
and combined in different ways and you
can see that if this pink one is the
real path between Seattle and Portland
somewhere in my database I have I
probably have something that closely
approximates the actual path that's
being taken by this I may not know what
that path is and I know may not where to
find it in the database but somewhere in
there I probably have something that's
pretty close and so the goal here is
really one of to try to identify this by
trying to find places in in my database
where I can find intersections of paths
so that i can use path segments say this
one from chicago into my source and this
one from Seattle off to my destination
that to happen to go through the same
router through the same pub and and
thereby give me a prediction now I have
to also be able to exclude ones where
there might be intersections that are
farther away or less likely than the
other ones and that's going to be a key
problem for the for making the system
work so I have to use the ones that seem
more likely to be useful and the way
that we're going to do that again I'm
going to point you to the paper for
details is to use the ones that Internet
routing will more likely use so we
understand a lot about how Internet
routing work what the policies are that
ISPs use and then use essentially model
those into the algorithms in order to
say okay this this intersection is more
likely to be one that if the ISPs are
acting rationally they're likely used so
for example early exit or short as they
ask path those are the ones that you
would like to use for these intersection
points versus things that are farther
away okay the second point that I would
make about this is that it's very
important to be clustering interfaces so
so for example in this case it might
well be that the that the the vantage
point going to Paris
or the destination actually goes through
a router that's not actually the same
router as the one going in the other
direction and that's actually it's very
common for it not to be the same
interface but it actually might just be
a different router in the same pop or a
different router in the same
neighborhood and so what you'd like to
be able to do is actually cluster them
together and it'll actually do you a lot
of good with respect to the accuracy of
your system to be able to cluster
information thats related together but
it means that you don't have to have
every minute you don't have to find a
measurement that goes through that
router rather you can find one that goes
through any of those two routers and so
there's ability to cluster into Pops's
actually ends up being essential to the
to the success of the system and then
we're going to do this using a lot of
the techniques that I talked about
earlier which is essentially discover
locations based on names and inferred
locations and then and then run a set of
algorithms that are that are standard in
the literature of which we did did some
a few years ago to to identify routers
that are close together and then and
then essentially look for routers that
are that are very close in latency and
also very close and hop count from from
the other routers and the same in the
same cluster so here's an example
actually you can I think harsha is this
on line in try it out I'll give you a
web page at the end this is a Harsha
gave this talk at imc which was in Rio
and did a traceroute as an example this
is the actual path you take from Rio to
was it at embassy in input no that's not
Belgium you're going to notice Romania
right over me okay I was thinking no
that's not belt your european geography
really needs some help you okay so the
so from Rio to a tool location in in
Romania and then here's what the
predicted path is it's not exactly the
same but in terms of overall latency
it's pretty close right that is what
this does is to use some paths from from
Rio basically from a vantage point Rio
to where his hotel was was very firm
managed by in the US the where the hotel
was ended up looking like like it was
likely to be the one that was useful and
then from
one over the other side and it's not
going to be exact but it's going to be a
usually gives a pretty close behavior
one of the key things here is that how
often does this path prediction work and
which is how often is it the case that
the a s path is what you would expect
given that I have no other information
about it I don't have any any
measurements from the source I don't
have any measurements from the
destination I just have you know bgp
reach ability information and other
trace route information how often am i
able to actually predict the path that
that you that path from using this
algorithm where we actually know what
the path is in some cases we can know we
can know because we can do direct
measurements and in that case then
you're getting the actual path the exact
path about seventy percent of the time
and then some some increasing amounts of
the some portions of the path some of
the rest of the time so the so in many
cases you're actually able to predict
the EAS path which is which I'll show
you in a second is actually ends up
being essential so okay given that i can
predict the path then it becomes
actually a pretty straightforward thing
to go predict add to end properties i
have to go and measure along every every
inner inner pop link in my network of
which there only half a million or so so
that's pretty easy and then and then
combine them in some rational way so in
this case take products of loss rates
which isn't you know literally valid but
you know is close enough sums of link
Layton sees and minimums of bandwidth
and then the one final point oh sorry I
should say I then have to measure
lengths in the coroner be able to do
that that's actually pretty
straightforward and then the final point
is how do I actually estimate edge
access link bandwidth and this is
something that's that you know if I was
at Google it'd be pretty easy because
you get a bunch of bandwidth you get a
bunch of customers at each of these
prefixes actually contacting you in our
case because we're University and we
want to be able to publish our data we
have to use day we have to use data
that's available so what we did was well
okay there's this really popular system
out there that's got six million people
who are all open to
having a having a torrent that's going
to tell them here connect connect to
somebody who's now joining joining us
warm so we passively join a bunch of a
bit torrent swarms and then and then
essentially look at the packet pair
distances between pairs of packets that
are that are returned from downloads and
that gives us an estimate of the upload
banner that doesn't actually tell us
what the download bandwidth is but but
it gives us an estimate of the upload
bandwidth to from different sources and
because it's just a passive monitoring
it actually doesn't raise any alarms we
actually don't download kon well in some
places we do download content but in in
this particular example we're not
downloading any content at all from the
actual BitTorrent peers we're just
connecting to them downloading you know
block or two and then so okay that's
good enough for us to get our
measurement and then we go on from there
yeah oh
oh so now what we're doing is we're
trying to estimate the in this case the
access link bandwidth so that just just
the little part that the end host is
doing and and we're going to do that by
looking at the distance between packets
that are being retrieved right to
effectively manage measuring the fresh
of the link landed
which is ah so if you're measuring it
yeah if we were measuring the end end
behavior of the of the of the connection
then then what you said would be true we
would be measuring the effective the
effective bandwidth would be some
fraction of the total access link
bandwidth however if you look at the
inner packet distance or the minimum
inter pack of distance that you observe
between packets TCP congestion control
will do this thing called act clocking
which in general will cause it to send
back to back packets on regular on a
regular basis those back to back packets
will tend to traverse through the
network together that is they will tend
not to be interspersed with other
traffic that's going to other
connections is a consequence it gives
you a relatively clean read as to what
the actual access link bandwidth
yes and just the natural behavior does
because of the way it does that o'clock
so whenever I increase my position
window sufficiently to be able to send
another packet i will send those two
packets back-to-back I'll send them as
fast as I can and if the access link
bandwidth is in fact the you know the
limiting factor then those will actually
go out together right and now we have to
do some kind of signal you know
everything I so one of the things that's
very clear about this is that everything
that we do has noise in it I mean
everything that we do is not perfect
it's not a it's not a clean signal so
you have to be able whether it's you
know the the label on the dns name for
the router or or in this case you're
having to do signal processing or some
kind of ability to throw out noisy data
and that's really one of the essential
pieces of this that makes it a lot
harder than it looks just for my kind of
logical side but you can actually it's a
well-known technique when the network
measurement community to be able to use
just a regular TCP transfer one way it
needs to be transferred to be able to
estimate the bottleneck bandwidth that
that went through and it's because of
the because of the dynamics because TCP
is not doing pacing of its packets is
not evenly spacing its packets is
actually back and it's actually bursting
them that allows you to be able to do
that yeah sorry did you have okay
that delayed acknowledgments yes it
turns out that it's easier to do it on
the forward path because you've got
actual large packets as it's a little
bit more difficult to do yeah so it's
possible that you could also you play
tricks with TCP to be able to trigger
packets coming back and that would be
another way to do it but we just
passively listen to what people are
going to send us we're not actually
doing anything other than connect to you
you have something let's go listen to it
and so here's a an example of the kind
of data that you can get out of this
which is how often is it the case
because we've got a large number
actually we've got a very large number
of hosts we haven't connected everybody
who's does bit toward but it's pretty
easy to go connect to a lot of people
how often is it the case that different
prefixes with different addresses within
the same prefix give you the same
measured data and the answer to that is
this cumulative Duchenne function which
is about you know two-thirds of the time
you will see in this data that's coming
back from this the signal as being
within about twenty percent of each
other which is to say that you're
getting a pretty clean signal for those
in some cases clearly one prefix vs. and
other prefix may have completely
different bandwidths sorry one address
within a prefix versus another one I my
desktop for example has 100 megabits on
it and there are other prefixes other
addresses within the University of
Washington the dump haven't in close to
that so so that's not going to tell you
in all cases what the bandwidth is but
but one of it sorry one of the ideas
here is that then you can use a
measurement of one prefix to be able to
predict measurements of other prefixes
and it's again it's not perfect but it
would be better than nothing ok so we
measured links to the edge we measured
properties of links in the core we can
predict routes therefore we can compose
properties of links in the core to be
able to produce these end end behavior
and then the question is whether the
combination of these work we have
measure this is a kind of an example of
how accurate how accurate these are four
paths between planetlab nodes where we
actually know what the data is because
we can have we can control both sides
there is clearly room for improvement if
if you look at
cases where we actually have the correct
AAS path that it was we're predicting it
correctly then we actually do pretty
well the red line is we're for all paths
and then where we don't actually know
the S path we we sometimes actually
still get pretty close but but it's
certainly in terms of absolute error
much worse off so one of the challenges
that we have is how do we improve Ras
path prediction and that's certainly a
future work oh sorry this is a loss rate
and this is latency so we're actually a
lot closer with respect to lost weight
but you don't see very many losses
anyways so that's part of the reason for
Roy screw yeah this is planet led the
planet right
yeah this is only for the yet then I
believe it's measuring Pat it's because
we want to have known pads and we're
comparing the known paths we're taking
planet led to planetlab measurements it
throwing out those end points from our
data set building the predictions using
all the other planet lab nodes and all
the other information we would have in
our system and then and then using the
prediction from there oh where the nodes
in planetlab uh they are everywhere I
could pull up that but essentially
there's about 300 350 sites around the
world covers five continents largely the
u.s. largely universities in the US but
increasing numbers in in Asia and and
elsewhere a lot in Europe so it has some
distribution it's not represented you
shouldn't think of it as being
representative you should necessarily
think that these are representative data
it's more what this is really my goal is
to say look if we can predict the aaas
path we actually do pretty well and and
better than the black box techniques
even for latency and recall that the
black box techniques like Vivaldi and
GMP don't have any any indication of
what what loss rates would be so it's
actually in that sense a step forward it
actually doesn't cost very much to
measure and now the question is how well
does this do for improving performance
here is a bittorrent swarm using this
information so if the tracker returned
two cases you could have the tracker
returns random people the tracker
returns this is the case where it's
using Vivaldi so returns people who are
close based on late purely just on
latency so I would like to give people
who were who are nearby each other i'll
return more of those than others and to
be clear and then can i return the pier
is based on the predicted tcp throughput
that is a combination of latency and
bandwidth for my transfer time and the
red line is that which is to say that
the swarm actually completes faster and
you get better response time as a
consequence of this this is between
planetlab nodes so it's a little bit of
a constructed case
but it's an example of something that
you might be able to do and then I have
a couple other examples but I'll
probably skip by them they're in the
paper here's a kind of one example if
you look at at a kind of another case
study for it if you look at the problem
of skype one of the challenges skype has
but is it's having to predict for
setting up a phone call between any two
points predict for that phone call which
intermediary point it's going to go
through now skype almost always goes
through an intermediary point because
just because of the consequences of nats
on both ends often you'll have to
connect to somebody else in order to be
able to get through your firewall or
through nap as a consequence it has to
predict pick that intermediary point
well is if it picks one with a high loss
rate on either end it's not going to
work very well if it picks one with high
latency that's not going to work as well
so the question we had was kind of in
terms of lot just in terms of trying to
improve loss rate could we find one that
would that would work better than the
ones that you would pick randomly or the
ones that you would pick close to the
source or close to the destination and
that's what that's supposed to do anyway
so these traces that we have for I plane
are all available on our website and
happy to talk to people about it
afterwards and then finally I just
wanted to talk a little bit about global
reach ability and this really gets at
one of the kind of you know most
interesting things one of my students
went and got this this quote out of
Clark's paper I apologize for spelling
Dave's name wrong there I didn't
proofread it I guess which is that the
design of the internet was really are
targeted for the following thing which
is that the only failure that you
observe is one where there's a partition
which is if there is connectivity you
should be able to get through on the
other hand what you actually see on
nanog one of the most pop Tanana gives
this operator email list that tells you
what actually is happening on the
internet and then you know one of the
most popular queries on that is is can
you reach me those are people basically
continuously they don't know whether the
rest of the internet can reach them and
they don't know what to do about it they
have no really no way of knowing what
whether it's happening what you can do
so please go ping me so that I can know
what's what's going on so the
question is kind of how often is this
global reach ability violated that is
how often are there these black holes or
reach ability problems in the internet
and can we use these vantage points to
be able multiple vantage points and
multiple information sources to be able
to do it and this is simply one where we
take the data that we collected through
for I plane and then use it and
basically just process it analyze it for
how often it's the case that these
things that we're doing two and the
prefixes are in fact having problems and
now we're going to state that there's a
problem where I can't get to the
destination and I can't get to the a s
that's in front of the destination so if
I have both an AS problem and I can I
can't reach the end destination that I'm
going to call that a black hole and what
was interesting about this is that in
many cases actually a really remarkable
number of cases you're seeing problems
and over a four-month period almost all
prefixes had some kind of problem in
them which is I think pretty remarkable
and there are a bunch of problems
questions that you could ask about this
and our system that's that we're
currently building to do this is called
Hubble's you might imagine for trying to
understand black holes it's like we've
done a better job of naming the system
than we have and actually building it
but but you can see you know essentially
questions that you might want to answer
would be how long do these things last
you know how often is how many of them
are happening how persistent are they
and those kinds of things so for example
if we measure from say 30 planetlab
nodes spread around the world to a
particular prefix every half every 15 I
think it's every 15 minutes right every
15 minutes how often how long to the
outages that we observe last how often
do those things last and the red line is
that cumulative distribution of that
which is to say that you're seeing them
that they you know most of them get
resolved right away some of them last
longer some of them last a very long
time so you know hours and and then
finally one of the things that you might
be able to do with this kind of data is
how often is it the case that it's in
that you're seeing kind of complete
connectivity loss versus one where parts
of the network are able to see through
the black hole and other parts of the
network are not able to see through so
how often is it that the that have got a
single home day s that that just simply
goes offline and everybody behind it is
is dead for a while well not much you
can do about that case but what's really
Merkel here is we were able to see lots
of cases where multihomed sites were
losing connectivity for some large
section of the network now multihoming
is a technique which enterprises will
very popular use for improving their
internet connectivity but if the a if
their isp upstream from them doesn't
announce when they have a problem then
their connectivity to you know half the
network is not kind of is not going to
help them very much so the this kind of
thing is actually something where we
were seeing a really large number of
these cases were ones where the
connectivity was in fact there could be
could have been established but but
wasn't as a consequence of of
potentially routing policies or ISP
behavior and in many ways you could
think of this as kind of Byzantine
behavior by the ISP but one which one
which potentially routing protocols
could deal with or overlays might be
able to deal with so the summary here is
de pollo that you know internet
measurement has a lot to offer
distributed systems we can build these
systems that actually give useful
predictions that you can start to
optimize distributed systems based on
them start to assume that every piece of
information you might think of having it
from the internet is stuff that is
actually available and can be
efficiently measured we don't have to
wait necessary for the internet to be
re-engineered for disclosing all this
information you don't have to wait for
isps to suddenly wake up to think that
it's in fact than their interest to tell
us information about their underlying
network but rather this is information
you can find and then can actually
optimize on so that's where i'll stop
and take questions if there are any and
otherwise thanks very much
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>