<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Bay Area Discrete Math Day XII: Hierarchial Dirichlet... | Coder Coacher - Coaching Coders</title><meta content="Bay Area Discrete Math Day XII: Hierarchial Dirichlet... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Bay Area Discrete Math Day XII: Hierarchial Dirichlet...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/2e1TrYT7owA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">pleasure welcome Michael Jordan from the
University of California at Berkeley
we'll be talking to us about
hierarchical dershlit processes okay
thanks so I'm really going to give a
talk that's part tutorial in part
research probably little more to troll
or anything else because I have a
feeling that most of the audience is not
in the same area that I'm in and the
topic I have in mind is the field of
random measures so a lot of you know
about randomness and measures and I
don't know how many of you work with
infinite dimensional objects that are
random in particular random measures and
so part of this will be just what are
those objects mathematically what can
use them for and and particularly how
they would roll the plane statistics
which is my my home field so let me
start with a slide that gives you a
little flavor of some of the kind of
problems that we face an hour in our
work and the field faces as a whole so
one of these is kind of a trivial
problem and another one's not so trivial
but it's pretty well known what the
answer is the third is more of the kind
of problem we really face in real life
and what my group tries to do is to
bring all kind of three these levels
together so the first problem is one I
actually gave it a randomized algorithms
class Cup a few years ago it's an
undergraduate level problem so take the
integers 1 through n and consider the
ensemble of random permutation so all
all permutations have equal probability
and now I asked what's the probability
that energy I and J appear in the same
cycle all right so i'll let you puzzle
about that for a minute some of you
hopefully all know the answer my
undergrads were able to solve the
problem it takes about two or three
pages of sort of just a little bit of
calculations and an interesting answer
comes out so if you whether you know the
answer now you might want to think about
it a bit and I'll get the answer later
and think pretty good how its scales
within okay so if you notice about
permutations you know kind of about
login and so on you might think about
scales so here's a little less
undergraduate kind of problem this is a
real problem that's been solved but it's
not a non-trivial problem so consider
probability distributions which are
exchangeable ie for some permutation tie
the probability of x1 through xn is
equal to the probability the / muted
values of the random variables and that
that's true for all n so that's called
infinite exchangeability suppose that's
true
so that sort of seems that things you
know doesn't matter what the order is
and the question is what does that have
to do with independent identically
distributed variables what's often
called bag-of-words malls that's where
at Google we should say that kind of
word people the whole processing my
document at Google is based on a
treating as a bag of words if you
scramble the words it doesn't matter and
so what probability structure is implied
by adopting a bag of words model the
third problem is the following let's
suppose you have a very good clustering
algorithm spectral clustering or k-means
or whatever you like but let's suppose
that I have a problem where I want to do
this with multiple groups of data not
just one group I'll give you an example
in bioinformatics later so you run a
clustering algorithm on each of several
groups but I believe in some way these
groups are related and so what I would
like to do is to solve the clustering
problem jointly across the groups i want
to share clusters among groups so how
you but how would you solve that kind of
problem so that's more of a research
kind of problem it's not a math problem
it's a figure out what to do kind of
problem though kind of we really face in
real life and so part of the talk then
is going to be about what are these kind
of three levels of thinking have to do
with these are whether these specific
problems have to do with each other all
right so let me answer the second
problem me being definity Saudi finetti
and the 30s proved the following theorem
so exchangeability as I'm as I said is
the invariance to permutation of the
Joint Distribution of financing of
infinite sequences of random variables
so if you have infinitely exchangeable
sequence then for any finite sub
sequence p of x 1 through n has a
representation so if it only if as a
marginal probability i an integral under
some random measure and well let's just
let's call this a parameter so some
distribution on a parameter of
conditionally iid data ie conditional on
that parameter you have independent
choice of X from a distribution indexed
by theta ok so these you call this
conditionally iid independent
identically distributed okay conditional
on theta and then you put a measure on
theta make it be random and then you add
up over all Thetas all right so one
direction this theorem is really obvious
and trivial that if you have this kind
of representation then these things are
exchangeable because there's symmetry
here it's just kind of obvious but the
other direction is what definitive
they proved that if you have this
symmetry then there must exist some
theta and and a distribution on theta
two together such that this thing can be
representing this way all right so it's
a very important theorem it's sort of
the core of these bayesian statistics
because if you believe your data are
exchangeable that said nothing about a
particular statistical model it just
said that you know there's some in
variance in the world that you can often
assert about some real real situation if
you assert that invariance then there
must exist this mathematical structure
underline the data so a bayesian would
call that thing a parameter and call
that a prior on the parameter and say
given the parameter that there is a
statistical model in from once you get
the data okay then you must have that if
you have exchangeability okay so that's
the definitive theorem now this theorem
wouldn't be true if theta were just a
classical parameter ranging over
euclidean vector spaces of the kind you
see in elementary classes okay there
wouldn't be true you need a rich or
structure to make this thinner be true
in particular theta really needs to
itself be a whole measure okay and think
that makes sense and now i'm going to
replace theta with g just to make this a
little clearer so you're mine doesn't
think parameter so i'm going to draw
something a measure that itself has a
probability distribution and haven't
gotten that measure then i'm going to
draw a data from that measure repeatedly
ok that's that's the level at which the
theorem is true ok and so this is a rich
object here this is just any old measure
you get data from that measure so is
what we call nonparametric it's just
anything ok and it says there must exist
a distribution on that measure so you
have a random measure all right that's
what that's kind of where in mathematics
the idea of a random measure really
comes from is the d phonetic theorem ok
so we'll have to talk a little bit about
what you mean by random measures and
have this talk not be four hours I'm
going to concentrate on a simple version
of a random measure something called the
derision a process alright so now let's
go to the third problem and then we'll
try to tie all these things back in
together later so important data
analysis problem is how to do clustering
and there are many many ways to do it
you've probably all seen different parts
of this slide
many different fields have developed
many kinds of clustering algorithms and
so on so forth and let me just kind of
let you look at that slide for a second
I won't go through the issues underline
them what we're going to do today is
talk about a method it's a little
different something that maybe you've
not seen it's not none of the above and
this method is based on something called
the chinese restaurant process which is
a kind of piece of probability in
combinatorics so let's describe it as a
as a stochastic process so it's a random
process in which in customer sit down in
a Chinese restaurant with an entrant
number of tables okay so the first
customer sits the first table
deterministically so we're going to
describe this in some ordering in
customers are going to come in the
second customer let's suppose this is
the first customer the second customer
arrives and sits at that table with
probability half or in general some
parameter alpha and moves to a new table
with probability a half third customer
comes in and joins a table proportional
a number of people already at the table
that's the general rule so it's one of
these kind of rich get richer
reinforcement sorts of schemes and there
emerges the biggest table and so and so
forth and you can prove that after n
people have come in you have roughly log
in tables occupied and moreover as I'll
think I say on the next slide you can
prove that this process is exchangeable
actually I don't have that on this next
slide I will prove that in a minute that
the process is exchangeable that even
though I described it with a particular
ordering their customers came in 13 and
in a certain order that the partition
that you induce on the customers is it
is invariant to ordering I who's
together with who at what table or who's
together with who is in marrying to the
ordering so we'll talk about that in a
second anyway back to clustering how
would you use this to develop a
clustering algorithm well the idea is
now we're going to have parameters and
data and distribution so we'll be
bayesian about this and so when you
arrive that the first customer arrives
at a table draws a parameter that could
be associate with that table from some
distribution so if we're doing galaxy
and mixture clustering for example so we
have these little you know round or lip
soy de things in space the way it arise
this is that when the first customer
sits at this table they draw parameters
from a distribution on parameters like
means and covariances for the Gaussian
case and everybody sits at that table
inherits that parameter so everybody
that's a cluster has a certain mean and
a certain covariance and then all the
customers are like data points that are
coming from the Gaussian probability
with that mean in that covariance some
other table whenever someone sits there
they choose a new mean and covariance
for that table so that's some other
cluster okay and so as people start to
fill the tables you start to get means
and covariances filling the space and
then data points selected from that
those those those Gaussian distributions
okay so now we have a fully specified
probability distribution on everything
we could care about tables a number of
tables assignment of data to tables
parameters everything okay so if you're
familiar with finite mixture modeling
which is widely used in statistics it's
where you pick the number of tables off
priori k all right and then you have a
likelihood and you do some statistics
with that here we don't have that the
number of tables is open in fact it
grows as it's a function the number of
data points let's log in ok but anyway
we have a we have a joint measure on
everything in the world and we can do
bayesian statistics and infer things
like the posterior distribution of the
number of tables that are occupied given
some data so i think the next slide is
just got a little picture on on it
showing you doing this with some fake
data there it is so the little blue
points are data points so I've now
sampled from underlying mixture of
gaussians and then this is the posterior
mode the most the the configuration of
everything in site that has the most
probability to plot something I did that
alright so that's after a certain number
of data points after more data points
and so on as a more and more data points
I start to get more and more clusters
that's the posterior mode okay so this
is a nonparametric the number of
clusters is not being fixed out priore
okay so that should be somewhat
unfamiliar to you how can you do that
sort of thing and we'll talk a little
more detail about how you do it so let
me return to exchange ability so as a
prior on the partition of the data the
chinese restaurant process is
exchangeable okay even though i said it
in an ordering it's actually invariant
its ordering all right moreover the
prior on the parameter vectors
associated the tables which is also
exchangeable that sort of follows from
the first point they're closer related
the prior is just you know once I have a
table I have a prior
I have a parameter for that table we can
write this out mathematically here's the
parameters associated different tables
so after I've assigned parameter vectors
to the first i minus 1 tables the
probability the parameter for the I've
table provides for the I data point
these are the previous i minus 1 data
points now the I data point comes in
it'll all sit in one of the previous
tables with this mass or it will come
via draw from a base distribution of
parameters that you know for the
Gaussian case for example this could be
means in covariances okay so that's just
a way of writing the kind of dynamics I
described above as a as a growing
probability measure and this thing is
called the Paul you pull your urn model
and so the description there in terms of
the Paul you earn is just that I have
colors in an urn and I grabbed one of
the balls in the urn certain color and
it's either one of the colors that's
previously there or it's a brand new
color and the probability measure on
colors is G ok once I've grabbed that I
put it back in the urn with a second
ball of that same color ok so that's
another way of saying the chinese
restaurant process that's called the
pull your own model ok so anyway it's
exchangeable and you could just simply
take this thing and multiply out all
these sequence of conditionals you'll
just get a product of things by the
chain rule of probability theory and
you'll get the joint probability over I
one through I in ok just take the
product of a bunch of these and you can
do that on the back of your envelope you
can do a little combinatory and you'll
find out that that object is invariant
to order and that's your proof of
exchange ability ok so that's an easy
little proof to show that this is
exchangeable just a little bit of kind
of products and sums kind of calculation
all right now now that you prove that
that thing is exchangeable you can do
all kinds of interesting things are in a
particular you can do statistical
inference and we'll talk a little bit
about this later and one of the previous
talks someone mentioned heat bath kind
of things and glower dynamics you could
do gulaab or dynamics with this because
of exchange ability and all sorts of
other things so let's return to the
problem I posed earlier in the talk
what's the probability now that
integers I and J appear together in the
same sequence on random on the outside
love random permutations okay so anybody
want to shout out an answer what they
think the answer is 1 over log in went
over in some constant one
the one thing it won't have someone else
I said that anyone want to guess there
are all the log in clusters or glycan
the permutations that's the number of
choice cycles and a permutation login so
kind of equal it would be like 1 over
log in all right so why is it one-half
it actually is one-half something where
they don't appear the same cycle you
just transpose them and then you get one
that does so there's a projection yeah
that's sort of a I want to repeat as if
proof if I wouldn't give credit for that
in a class it's kind of you just
transpose things around and didn't quite
work but anyway there is a proof that
kind of exchange ability is the
underlying idea so instead of just
calculating things out which you can
also do and that takes a couple of pages
a little thought and this is not much
thought but a little thought shows that
cycles in permutations are isomorphic to
tables in the Chinese restaurant and
that's in fact why that it's a Chinese
restaurant cuz they have a round tables
and people sitting around the table
there's actually that mathematical
structure you want to capture so it's
it's round tables so the Chinese
restaurant is exchangeable as we
basically proved just by showing you how
to prove it so the probability I and J
appeared in the same cycle is the same
as the problem that one and two appear
you just exchange and probably the one
and two are the same table well probably
the guy first guy said the first table
deterministically and the second guy
joined him with probability a half and
move to a new table probability house so
the answer is actually half and that's
the proof and there's lots of other
things you can prove in combinatorics
and probability just by referring to the
Chinese restaurant it's quite a little
generator of mathematical structure at
the interface of combo tourism
probability okay so that was kind of a
little bit of background and so I'm
going to now move into some more rich
stochastic processes and inference what
can we do with these things and then the
main important the talk was a hierarchal
additional a process but I don't really
want to spend a lot of time with it it's
my research agenda but I want to give
you more of an idea of the flavor of the
field okay so I'm going to need a little
notation for the talk and instead of
writing out lots of symbols i like to
use graphs to express probability
distributions so hopefully a lot of you
have seen graphical models before this
is just a way of writing out
multivariate probabilities
shins so if I have random variables or
vectors 1 through n in this case 6 I can
associate them with with vertices of a
graph and then the edges defined joint
or short conditional probability
distributions locally so the joint
probability of X 1 through 6 is just the
probability of well X 1 has no parents
so it's just a marginal X 1 X 2 has the
parent X 1 so i have a conditional there
and x6 it's got parents x 2 and x 5 so
there's that conditional chain together
the conditionals and let's use that to
define the joint probability associate
with the graph and each one of those
little probabilities can be indexed by
some parameter and so this is a
parameterized family of distributions so
the undirected case of course is the
easy models or Markov random fields or
whatever a little bit of shorthand that
will find very useful is that instead of
drawing this kind of graph which happens
all the time because of the definity
theorem will short the shorthand is that
that you put a box around something that
means you replicate whatever's inside n
times and expand out the graph like that
it's just a little bit of shorthand and
then glower dynamics or gibbs sampling
is one way to compute on these graphical
models you take one of the nodes at a
time and you form the you take all the
conditional probabilities associated
with that node its parents and it's
children okay a little local set of
probabilities in the graph multiply them
out sample from that resulting object
and that is the new state of that node
and then do that repeatedly at all the
nodes in the graph and that's the
Glauber dynamics for a directive
graphical model okay and that converges
under the usual assumptions to a
stationary distribution which is the
joint probabilities division associate
with a graph okay so when I say
inference later in the talk that's the
kind of thing I mean that's a simple
version of it there's lots more
complicated ones but that's one way to
do it all right you also all need to
know what the duration a distribution is
I hope that you do I don't know what
this audience is but additionally
distribution is a it's kind of it's an
exponential family distribution on the
simplex so it's kind of the gal scene of
the simplex if you will it's the kind of
little bump sort of object it's got
parameters so the probability of pie now
is a set of non-negative numbers that
add to one so numbers on the simplex all
right we'd put a dist
abune on them with parameters alpha by
taking each one of those things and
raise you to the alpha power the minus
one is just a convention to make it look
more elegant and this is just a
normalizer and so this kind of has a
bump shape at the dericious a
distribution a special case of that
where the simplex is just the image the
interval from 0 to 1 is the beta
distribution so I've just got two of the
pie values now and depending on your
particular choice of parameters there's
a bump with alpha bigger than one but
when alpha is smaller than one you get
things that go like this they go the
other direction and will be particularly
interests in that case so in alpha is
equal to 1 you get the uniform
distribution and then as alpha grows
this thing tilts and you can go either
with positive values of a bitter than
one or smaller than one you have all of
these distributions so this is alpha is
increasing you start to go up and up
like this putting more and more mass
towards the left the wristlet
distributions aggregate if I take PI I
and pie i plus 1 I take those two values
and I add them I still get a delay
distribution so it has a consistency
property under aggregation and it has a
conjugacy property which is that if I
take a random variable Z a multi noma
random variable that has k possible
values that can states it can be in and
the probability of those states is given
by pi k where pi k is a drill a variable
then i can talk about the posterior
distribution of pi given i've observed
one of those k states okay and all it
ends up doing is that you add one to the
Alpha value for the guy that you
observed and everybody else stays the
same ok ok so do rich lay and
multinomial go together very nicely you
just kind of add counts and you get de
richly distribution alpha value Spina
piling up and that that bump moves
around as alpha changes right ok so just
so that was a little bit of elementary
background and so let's now move on to
sort of building them some more
structure ok so I another topic I hope
you know you've seen somewhere before is
a what's called a fine
mixture model it's a a bayesian approach
or generative approach to clustering so
the probability of this with RK clusters
in the world i know that opera re then
the probability of some data point x
given some parameters is that i have one
of k possibilities i choose with
probability pi sub K one of the K
possibilities having chosen that I have
some parameter and I get my data from
the distribution index by that parameter
and i sum over all the possibilities
that's called a finite mixture
distribution okay so it's this thing if
these were all gaussians this would be a
mixture of gaussians or they could be
whatever other distribution all right so
there's just an example under the
Gaussian case all right now it's going
to be useful to make this more general
to introduce an explicit variable Z
which indexes which of the the
components was chosen for any particular
data point X ok so I can kind of expand
this thing out as the probability of
that indicator variable being equal to
one and then having chosen that one then
I condition on that and I get my data
from that particular component ok so by
sum over K it just collapses it just i
rewrite back to this and I get this
exact same marginal distribution ok so
it's just a way another way of kind of
seen what's just making more explicit
what's happening in mixed remark all
right now here's the transition and you
need to be able to make which is that
finite mixture model can also be written
in a slightly different seeming more
complicated way but the one that we can
generalize which is now we're going to
define a measure G was just a sum of
atoms so Delta is the Dirac Delta at
locations V sub K with weights PI sub K
or the paisa k is some 21 ok so that's
just a little picket fence of heights
given by PI K and it's a finite number
of pickets in my picket fence ok that's
a measure ok and so now I could define
the process of the tray of obtaining a
sample from a finite mixture model as
follows first of all choose from my
picket fence one of the particular
pickets it's just a measure to picket
fence I can choose from
measure I just get one of the particular
pickets having chosen that one now take
data from the distribution index by that
particular picket now let's do this over
and over I'll get different one of the
things chosen and I get my data from
them I just a way of indexing a bunch of
distributions ok and the pie values are
the probability of choosing each one of
those pickets right so that's a nudged
another way of saying what we mean by a
finite mixture model and here's now this
graphical representation of a finite an
extra model which is that I have is
underlined little picket fence gee I
pick repeatedly from that G a particular
one of the pickets and then that's a
that's the index into a distribution in
particular maybe that's the location
parameter so it's a mean and I get data
from a distribution with that it does
mean and that's how my data rise hey
just another it's a drawing to suggest
what's happening ok and now if we want
to be Bayesian about this we have to put
priors on everything in sight and
particular on the parameters pie and
theta the probability picking the
pickets as well as the parameters you
get associate with each one of these ok
at a particular the first thing you
might want to do to be bayesian about
this is to use the dericious a
distribution for these proportions pi
you would take some concentration
parameter / k kind of these equal
parameters and that's that's how you get
pie all right
okay and so now we would write out as a
bayesian this fuller model the last
three lines of the model we had before
and now we put some distribution on the
the the fee values let's call it G not
you guys don't care too much what it is
just some prior that I basel ii would
put on parameters for some model and
then pi is the one you may be care more
about it's the dershlits is called the
symmetric diversely distribution okay so
now we have a fully specified
probability model that has everything in
sight being random in particular now g
is now random it's now a random use your
first random measure okay because the pi
values themselves that defines you here
are gotten randomly from the original a
distribution every time i sample i get a
new picket set of heights of my picket
fence and the locations fee are coming
from some distribution to think of the
galaxy and maybe as a generator of the
atoms so every time i sample a bunch of
fee values and then the pies I get a new
picket fence it's now a random measure
and then I sample the mixture model the
rest of the story is the same as before
so that's a bayesian version of a
mixture mob a finite mixture model
written as a random measure okay and
let's since I not spent in a lot of
times those statistics here let's just
wave our hands and say we can do
bayesian inference with MC MC or the
gibbs sampler the glabra dinosaur you
want just take that kind of graph take
one note at a time and sample from
conditional probabilities associate with
the table converge and so on so forth
alright so now let's go on to the harder
problem which is how do we pick K how do
we know we don't know the number of
clusters are priori how do we handle
that and so well now we're going to try
to return to the idea the Chinese
restaurant now from a more sophisticated
perspective alright so the first thing
you would think of trying to do and then
you run into trouble it's hard to do
this mathematically is you would say
well what I really want to do is I want
to take G not to be a finite sum K
equals 1 to K capital K I want to give
myself an infinite number of atoms I
want an infinite random measure their I
wouldn't have to pick K I would just
have all of them having different
heights and maybe they tail off in some
way and I would get around the whole
problem of having to pick K okay well ok
let's go back to our probability
specification we have ad originally
distribution it's very symmetric and we
might say think you've taken taking k to
infinity ok but these things have to
some
on and you're going to run into trouble
there you're not going to be able to
find such a distribution that the
symmetric diversely distributions k goes
to infinity does not approach a
distribution okay you're going to run
into trouble doing that all right so
there's another way of doing this which
is better easier to understand and leads
to a mathematically sensible object that
we can then generalize so the way to do
this is to do something called a stick
breaking construction and the stick
breaking instruction is straightforward
you take a stick that goes from 0 to 1
that's 0 over here and here's one over
here and you first of all break off a
piece of the stick break that off and
that amount is beta sub 1 and that's
sampled from a beta distribution with
parameter 1 comma alpha not it's the
same alpha not as before it's the
concentration parameter hey you remember
the beta distribution was a distribution
on the interval from 0 to 1 and it kind
of peeked up to the left depending on
the value of alpha that's the
distribution we're going to use so we'll
break off a certain amount and call that
beta 1 and then set PI 1 equal to beta 1
all right and now we'll take the rest of
the stick 1 minus beta and break off
another amount beta 2 again sampled
independently from the from the beta
distribution so that's some other random
amount okay and that part right here is
pi/2 so it's beta 2 x 1 minus beta 1
that's this amount and so on so the pi
values are these things between the
intervals here and there gotten by
breaking off independent beta
distributions from beta variables from
this stick all right so it's clear that
these pi values if you sum them from
cake will 1 to infinity that's got sum
to 1 because there's only one though
it's only to stick that goes from 0 to 1
okay so that's the way to get a random
distribution on the integers from 1 to
infinity and that's the way to get it
okay so here's in fact the proof that
this sums to 1 1 minus the sum from 1 to
K is equal to that and you could do a
little bit of work there and an easily
show that this thing converges all right
so now if we use that to define our
random measure going up from Katie or
equal to 1 to infinity this has a clean
definition as a random measure okay
because these weights sum to 1 so we
have now
an infinite picket fence of the decays
in some way and so in our graphical
model representation we can conceptually
think of generating a random measure by
getting a bunch of picket locations a
bunch of atoms from fee from g not those
are the fees and then the heights of
them are giving them the stick breaking
process we put them together and we get
this infinite picket fence now having
gotten that we do the same stuff as
before we repeatedly pick one of the the
pickets and according to its height and
then having picked it we use a
distribution with that as its location
parameter and we get data ok all right
so if you're understand what I've said
you've understood quite a bit this is a
nonparametric bayesian statistics
something you will not see it on the
undergraduate curriculum or even most
graduate curriculum including at
Stanford and Berkeley but sort of too
bad you should lots of more
combinatorial stand and probability come
out of thinking about these objects and
jim pitman I should mention Jim's name
is one of the propolis in the Bay Area
work a lot on this I think he's probably
talking this in this particular seminar
at some point in particular you can show
under an appropriate definition of
convergence that originally of that
converges to x in the following sense
it's called size by a sampling which is
that if you take the if you throw darts
uniformly at random at 01 sequentially
and you take the sized Maya sorry the
stick breaking intervals that we had
defined before and whenever you hit a
new sub interval you set pi i equal to
the corresponding weight and increment I
so I just I'm reordering in a random way
whatever intervals I had before right
that's called a sized by a sampling of
intervals right and you can go back and
forth between these objects the stick
breaking representation and something
called the poss owned originally
distribution by size bias sampling so
I'm sort of not I'm doing a bad job of
explaining that because I don't really
want to spend time with it but I just
want to give you a flavor that there's a
lot of interesting comment or behind
definitions okay so that's a little bit
of probability and combinatorics and
statisticians we want to use these
things now to given some data to infer
things and so I those were the
definitions of the prior how do we
actually now observe data and go back
and for all the parameters that's what a
statistician goes backwards give it a
model and fir things from from the data
all right and so particular if I sample
a point theta from some infinite random
measure how do I get a new probability
distribution from the prior condition on
theta in turn making it I into an
opposed to a posterior or suppose I do
this for a whole lot of Santa of datas
how do i get a posterior distribution
conditional on all of the data so far
okay and so to do this you need to
actually get into a little bit more
serious combinatory our sorry stochastic
processes and so I need one more actor
in my little drama here which is what's
called the de richly process so you
haven't seen that yet you've seen a
whole bunch of other things we haven't
seen the actual de Risley process so i
hope that most of you have seen a
Gaussian process probably you have so we
want to put a distribution on a whole
function space we want talk about what a
random function might be that's and one
example of that is the Gaussian process
and so one let's just think about
functions on the real line and so we
want to make get random functions of the
real line one way to do that is to
specify Gaussian distributions for
finite locations along the rail line so
pick K locations make the values at
those locations be random according to a
Gaussian distribution and have a
multivariate Gaussian on them okay and
then do that for all arbitrary
collections of locations and then insist
that they be consistent with each other
so if I have K locations and I add one
more I get k plus 1 and that if I
marginalize out the Gaussian
distribution on k plus 1 dimensions down
2k I get the same answer as I had before
all these specifications are consistent
with each other right so coma gourav
said if you proved if you could do that
consistently for all finite sub
collections you have defined a
probability measure on the infinite
objects I e on functions in this
particular case and that's called a
Gaussian process so you can do that for
all kinds of other district
not just Gaussian distributions you can
do if there anything else where you can
get consistency among different sizes of
objects so the discrete world you talk
about things like gives measures and so
on but in the continuous world you have
all these other distributions to play
with in particular Gaussian
distributions and de Risley so remember
I talked about earlier if I take a
Jewish lay on a partition of size K and
I collapse two of the cells to get a
partition of size K minus one that
originally behaves like it should it
adds the probabilities among those cells
I get consistency so you can now define
a drill a process on partitions all
right so here is the mathematical
definition hopefully I've now motivated
well enough that you can actually
understand it for a little bit quickly I
have some underline measure space let's
call that Omega that I'm putting
measures on I have some base measure
which I'm calling G naught I think of
kind of a Gaussian something like that
and now take a partition of that space
a1 through a are ok and now the Gaussian
say or whatever G naught assigns a
measure to each one of those cells in my
partition multiply each one of those
things by a number alpha not a
concentration and I get a set of numbers
that's what these are use those as the
Alpha parameters to a drill a
distribution right and then sample from
the drill a and after I've done that
I've got now our new numbers which are
kind of fudged up versions of those old
ones have a little bump the Alpha define
kind the center of the bump I'm getting
numbers kind of around that on the
simplex so these things live on the
simplex and they are sampled from a
douche later distribution gotten from an
underlying partition all right now do
that for another partition and ask that
the probabilities are getting on this be
consistent among all choices of
partitions and because of the drill a
property you can do that all right and
now Cole maguro tells you that you have
a consistent set of probabilities you
must have a measure on in this case
measures ok so that is the definition of
a random measure in the full
mathematical sense of the term it's a
measure on measures gotten by showing
you get consistent assignments of
measures to partitions ok random
measures
mm okay having to find it originally
process in that way it's actually very
easy now to see what the posterior
dershlit process would be ie I take a
random measure and then get a point out
of that and say now conditional on
having seen that point what's the
posterior measure what's the new measure
you get having seen that one point and
way to think about it is that you take
say I've observed theta 1 from a random
measure I take a little tiny cell around
that theta 1 and remember how I update
the rich lady distributions I add one to
the Alpha for the thing I observed and
everybody else stays the same okay and
for that so for that cell i'm adding 1
and that's true for any cell that
contains theta 1 i add one so I'm really
adding a mass of size 1 even a very
small cell gets a new massive size plus
1 to it so a new little spike is going
up at that location that's what's
happening all right so after I've
observed a data point at theta 1 in my
posterior I have a spike at that
location and then i have the derision a
process everywhere else now as I had
more and more data I get spikes a rising
up glow throughout the space ok so the
derision a process is actually discreet
with probability 1 and and you may now
have understood why that's the case so
let me check my time I don't want to so
there are we've now defined three kinds
of objects a stick breaking measure a
Chinese restaurant process and adder ish
late process right and you may have
liked one of them and not like the other
two and the the nice factors these are
all kind of the same idea ok so the
stick breaking thing is this random bet
bunch of picket fence if you take a
partition of the space one language they
picket fence lies and you ask how many
pickets are fallen in any given cell
it's tricia lee distributed so that is a
jewish Lee process ok and moreover
afternoon having gotten a data point
sample of those pickets and you ask
what's the new distribution on picket
fences all right that is also the
derision a posterior update and it's the
chinese restaurant process i have a
certain probability of choosing one of
the previous pickets and if I get that
then I add more string to that picket I
add one to that otherwise I get a new
picket okay so these actually ideas are
all closely related so mm let me let me
draw a picture that shows you what's
happening here at this point so a
durational a process mixture model can
be thought in three different ways and
this is my favorite way of thinking
about it which is that there's
underlined measure on parameters G not
there's a concentration parameter that
sort of says has a bayesian how much do
you believe in the underlying measure
and then G is a random measure you get
it by repeatedly sampling from g not
with this stick breaking kind of process
and then you get one of those guys you
get a data point and you get a data from
that cluster ok so that's called it
that's an often called an infinite
mixture mall or a nonparametric mix drum
wall or a judicially process mixture
model ok so I'm going to skip through a
few slides at this point so this is part
of a longer talk which you can find on
my website that was a tutorial in this
material and so there's some material
that I kind of left in because I thought
you'd want to see it as it as it blazedd
by but I don't want to really explain a
detail here so what I really want to do
next is to explain two things one is a
little bit about how you do inference
with these things and then how you can
use it for various kinds of interesting
applications so the first one is how do
you do inference and we've already kind
of waved my hands at explaining this the
you can do MC MC with this kind of model
so this is now an infinite object we
have these infinite random measures but
because of exchangeability you can
actually still do inference and the
basic scheme is the following imagine
you have the chinese restaurant process
situation and i'm trying to decide for a
given data point which table it should
be sitting at which allocation which
cluster does it belong to at the current
point in time so to do the gibbs sampler
of the global dynamics what you do is
you take that data point you pull it out
of the system and you look at the
configuration of all the other variables
and you say where what where should I
now reassign what tables should that
data point go to I was the conditional
probability of that data point given the
configuration of all the other data
points
and that's a hard thing to try to
calculate until you realize you have
exchangeability and by exchangeability i
can take that data point right at the I
table and move it to the end of all the
data points I can move it to the end at
the end slot all the other data points
have already arrived and SAT it all the
tables and I'm just asking what's the
last data point do well it joins a table
proportional a number of people already
the table I know that that's the
conditional I don't have to calculate it
anything it's not hard okay and so then
I can now figure out what table it
should sit at according to that rule and
according to the likelihood all right so
that took about 20 years for the
statistical field to realize it's not
that hard but it took a long time the
original a process were defined in the
early 70s and it wasn't until the 90s
that people actually realize that you
could do glob or dynamics on them and do
inference on these infinite objects all
right and when they did then this sort
of exploded as a feel that became the
keystone of GaN parametric bayesian
statistics okay so it's very simple idea
but it took a long time the one of the
main topics I work on in real life is
variational inference it's an
alternative method to MC MC it's not
random it's not flipping coins it's
based on a deterministic optimization
framework kind of a relaxation of an
intractable problem and to attractable
one things that should be familiar for
other reasons and you can do that here
as well based on the stick breaking
representation so I'm going to not show
you how to do that I'm gonna skip again
over a few slides that if I were giving
a longer talk I would go into some
detail so this is just how you do the
collab or dynamics and I'm going to skip
over that and I should also add they're
a bunch of alternative 0 this is a whole
little discipline of its own trying to
figure out how to do fast mixing mcmc
algorithms for richly process mixtures
and then the way that you get the
deterministic algorithms is that you
truncate rishtey processes and you can
base optimization algorithms on that so
I'm skipping some slides on that I did
actually write out the end of the day
the equation is that you actually need
to run on the computer to do inference
under a diversity process mixture it's
really just a very simple little fire
lines of matlab to do this for with the
variational algorithm and i think the
next slide actually shows fitting data
using variational inference to a
gaussian process mixture
okay all right so let me give an example
that'll sort of bring a little bit more
home to you why we do this kind of why
do we care about these sort of things so
I could easily give document kind of
examples and I since i'm at google today
i thought i might do that but i decided
not to here's another example that I
quite like which is haplotype modeling
and some of you may have heard about
that from kind of theoretical computer
science point of view I think of this as
a good statistical problem it's just a
mixture model problem so the problem is
the following if we have some genomic
region and we have em binary markers
these are called snips or single
nucleotide polymorphisms in locations in
which you and I differ or you know all
of us differ and a particular pattern of
markers along a chromosome is called a
haplotype and I'd like to figure out
what happen is are there in the world in
the population as a whole because I'll
tell me something bout the founders of
the human population and then our
various relationships among each other
and the problem is that you and when you
ask say in real life in biology a
haplotype so here's a haplotype capital
a there are two possibilities you can
either a capital A or a little a saw the
capital a here a capital B here and a
little see here so that's one haplotype
here's the second chromosome y cap
chromosomes come in pairs here's the
second chromosome on that chromosome I
have the other allele not the capital
labour the delay and here I have the
other Leland here similarly so I have
two haplotypes here for a particular
individual each one of us for each pair
of chromosome have these haplotypes I'd
like to figure out for that individual
what are their halfway so I could look
at that diagram I'd be done but the
problem is we actually did the
biological assay you lose the ordering
so you find out that that person has a
capital A and a little a unordered and a
capital B and a little beat unordered
and similarly but you don't you lost
which who goes with what okay so to go
from data like this is which is what the
assay gives you to go backwards to this
diagram that's called haplotype phasing
and that's one of the major problems in
modern competition all statistical
biology or genetics is to go in that
direction all right now if I only had
one such person this is a very ill posed
problem I couldn't do it all right but
if I have many such people and I start
to see some patterns
weighs two stitches together that are
consistent with each other I might be
able to infer the underlying haplotypes
and that's what people try to do can I
ask about time I've now Mike Mike
watches those about 15 minutes thank you
okay so this is really a clustering
problem and so let me try to convince
you that it is by just writing it out as
a mixture model so here is the actual
probability of a genotype here's a
genotype this unordered set of alleles
over here so how as probable let's just
try this try to add a model for that the
probability of a genotype is the
following when I made another human
being I first of all got a haplotype
from their father I got a haplotype from
their mother all right and just by
reaching the bag of haplotypes that are
sitting in the population randomly chose
a random father and a random mother all
right and then conditioned on that pair
of haplotypes I got a genotype and this
probability is just the probability of
the assay all right given I know the
haplotypes what's the probability if
given i had that what's probably getting
that it's just the UH nor during process
okay all right now I don't get to see
what the haplotypes are so I have to sum
over all possible choices of haplotypes
to get the probability of a genotype
okay all right so that literally is the
model I mean that is the biology it's
not just the mall it's the biology and
so however what method you want to use
in this problem you really are solving a
mixture modeling problem it's just a
mixture model okay now I really could
have written this is probably the H 1
comma h2 and kind of group them together
and separating out from the in that way
is called hardy-weinberg equilibrium and
it's assumed here and it's assumed there
lots of genetics it's it's a fine
assumption oak at the first order here
okay so anyway it's a mixture model
problem but the real part of the problem
isn't the usual one of finding the
probability model for the components the
pies and the fees it's really how big is
the bag of haplotypes what's k if you
could solve that you'd know a lot about
human populations of genetics how many
haplotypes are there among us in the
room how many are there in the world you
know what was a 10,000 years ago that
there was a crisis in the population it
dropped down to 50 people in Africa and
all of our haplotypes are those 50
haplotypes are a little bit scrambled up
York what happened and what's our
diseased propensity is because of
knowing because of that so it's a real
important problem and it really is the
problem of deciding what's the
cardinality of H okay so I got involved
in this problem by just me and a student
hearing about this problem and realizing
this is exactly the kind of problem that
ad originally process really nails this
is exactly what her surprise are aimed
for it's a mixture model I don't know
the number of components that's random
and I have a lot of properly structure
from Mendel's laws and so on sitting
there a below me so when we did that we
implemented this we took some data and
we did originally process mixtures and
we compared it to the state-of-the-art
methodology which is something called
the phase program by Matthew Stevens
it's a gibbs sampler kind of thing on a
different probability model and these
are different kind of error measures in
different regions of the human
chromosome and let's focus on this error
measure which is the one that people
tend to care about the most let me not
worry about explaining the other two and
our so are you want have small numbers
here and so here's our method here's the
state of the art and we're a little
worse on one region of the chromosome
and we're better on all together three
so there minute this is a hard thing to
beat this is a really good piece of work
and many people have written haplotype
systems that don't refer they don't
perform as well as phase so with just
doing nothing other than implementing
the delay process we have a
state-of-the-art haplotype bayesian
system okay so now the last part of the
talk is kind of going to move a little
bit of just tutorial mode and try to say
what more can you do with this and it's
also some of the theoretical problems
that arise out of all out of this area
and this is the last problem I had on my
very first slide which is now we have I
hope you have a more sophisticated idea
what clustering is about then you may
have had before you're now talk about
clustering where I have infinite numbers
of components or unknown number of
components and so on so now we have that
all done let's now try to solve a more
interesting problems now we have in the
room in fact multiple ethnic groups all
right and we all share certain
haplotypes in common the bag of
haplotypes we have or in many relations
we all originally came out of Africa
probably so you know my haplotypes and
yours have a lot in common but we have a
little bit of differences as well
okay so if you just take data from one
ethnic group and you do this kind of
clustering you'll get certain clusters
and you do it for a different ethnic
people get suttin clusters too but there
should be some similarities and we'd
like to find out what those are okay and
so you really want to solve multiple
linked clustering problems okay and
that's non-trivial in particular I don't
know of any algorithm really that does
that or anybody has even worked on that
but it's really important and so I can
that's one example but you can think of
lots of other examples in particular at
Google you try to do this sort of thing
all the time I want to take a bunch of
documents in some area and want to find
one of the clusters in those documents
what are the words the defeat the topics
are usually called then I want to do
that on another set of documents or
maybe a set of images all right but I
believe there's some relationships among
these clusters if I have a bunch of
documents talking about the sky is blue
and so on stuff I have a bunch of images
with blue skies I want to find the
relationship so clustering prompts often
want to be solved conjointly with other
clustering problems all right and I
never in these cases know how many
clusters there are in each of these
domains okay so really that's the
problem we want to try to solve okay so
last part of the last few minutes then
is what is a hierarchical brush late
process kind of mathematically and then
how to use it to solve problems and so
let me just say a little bit about why
you why was one want to be a bayesian
anyway you often probably if you're not
statisticians you've heard that there's
all these debates going on and so forth
and as priors is where priors good or
bad and so on and my point of view on it
is that priors are usually kind of bad
their heart set priors you'd usually
don't know what they should be try to
push them under the rug but there's
another very good reason for being a
bayesian which is you can be
hierarchical you can build hierarchical
models so you can group things together
and so let me show you a picture that
will kind of help bring that home maybe
well I'm suppose I'm trying to figure
out how tall people are in various
cities around the country so I have New
York San Francisco Los Angeles all right
a theta is the underlying height of
people in that city and then I get a
bunch of data which is the actual
observed heights of a few people all
right and having gotten some data I
could do maximum likelihood estimation
to go backwards and assign the mean
height for a given city okay so i could
get maximum like that estimates here
they are for each of the cities right
but suppose that I just happen only have
five people in New York and I have
thousands of people in San Francisco and
thousands of
people in Los Angeles all right well
that estimate based on five people in
York is probably a very noisy very not
trustworthy I'd really like to use some
of the data over here to kind of leak
over to there and help me out now
that'll buy it's me a little bit because
maybe people in San Francisco or a
little bit taller than people in New
York because the weather is better all
right so i might get a little bias but
with all that extra data somehow should
get a better estimate because of
variance being smaller anyway this is
the kind of argument that's called
shrinkage and statistics you want to
share statistical strength to make
estimates be better and you don't want
to do maximum likelihood so for a
bayesian this is very natural all you do
is you put a mean height of ever all
people in the world up there at the top
at some unknown parameter and then the
heights of people in various cities or
gotten randomly from the mean height of
everybody in the world there is a
linkage showing us you put it in
explicitly and now i have a graphical
model in which i observed data here and
probability things propping it up and
down a tree and come down to here and
help me with these estimates all the
data collaborate to help me with my
estimates alright so that's classical
hierarchical bayesian parametric
statistics and it's the main reason in
my view for being bayesian okay it
solves a lot of heart of statistical
problems naturally okay so now that we
have hierarchical modeling in hand let's
go back to multiple clustering problems
how did would you do that if you have a
bunch of clustering problems where you
have these probabilities of being in
certain classes the class indicators and
you have a whole bunch of those things
so what do you share alright and you
don't know the number of clusters in
each of these cases so how do you match
these things up how do you handle this
alright so the first thing you would try
then is that you would have a duration a
process that's what this lower part of
the diagram represents and then you have
the outer plate v I've multiple the
original a process is not one for each
group that's what you're using our new
tool that we and the way we're going to
link them is still going to have an
underlying base measure which is the
same for all these groups okay they're
all come from the same underlying base
measure right and that fails completely
and this is kind of one of the main
message is the talk that's something
that works for parametric good
old-fashioned statistics completely
fails in the nonparametric situation and
the reason is the following that if I
have some based measures everybody is
sharing so say you're running the
program and you're running my new
program you sample for that base measure
you get a picket fence and then you
analyze your data with that he samples
from that same day
measure and gets a different picket
fence with probability one of those
pickets overlap each other because these
are continuous underlying measures so
there's no sharing your clusters and
your clusters have nothing to do with
you today we get no shrinkage all right
all right so you're a bunch of smart
people so how would you solve that
problem well you got to use Chinese
restaurants now y'all start thinking
bout dinner already you got to think
about how the Chinese restaurants help
me with this
alright so what you really like to do is
to have the base measure not be
continuous if I didn't have continuous
atoms to worry about in the beginning if
it was just a discrete underlying base
measure then you get some of those atoms
and you get someone able to have
something in common but if I already
built in the thing being discreet what
if I knew where the albums were all
prorated what am i doing I already know
the answer all right and so you'll face
that problem again again in statistics
if you could make it you can make it
simpler this way it would be great but
that would building too much and so the
answer is do that but now make it be
random make it be a random discreet
underlying measure all right all right
so if you understood that you understand
what our idea was and we call it
hierarchy literally processes and all
you do is you go recursive you'd be like
computer scientists all right you say
that the base measure for a derisively
distribution is G naught which I've been
using is fixed before is now itself a
draw from underlined originally
distribution drilling process so this
thing is now itself discreet because it
comes from this kind of object and it's
random so I don't have to know where it
is our priori and then I pass that in as
the base measure to an underlying set of
de richly processes there Albert's
buddies shares everything now ok so the
picture i think it's probably will give
you the idea i now have my hierarchy
taking it to another level which is what
Bayesian methods do they say to sell
underlying base measure which is the
universal generator of all atoms and
then I repeatedly get a picket fence
from that I pre to get atoms from that
and that's the base measure which is
shared among all the children all right
and so all these atoms are get shared by
everybody in sight ok so the rest of the
talk you know there's a bunch of slides
but I don't want to I'm out of time
basically and I don't want to get into
them anyway because it's late in the day
but I hope you get a little flavor of
what you would do it at this point you
would set up Chinese restaurant process
you'd set up sampling algorithms you set
up all the machinery and you just work
with this kind of probability model and
you would solve all kinds of interesting
problems so you could do a stick
breaking you could do the chinese
restaurant process and it now becomes a
franchise what it is you have multiple
Chinese restaurants and you have dishes
in the restaurants that get shared among
the restaurants according to the
popularity of the dishes not just the
tables in the restaurants and you get
things being out building up into a
hierarchy and you work out all the
probability theory and you do various
applications so
me I think I'm also going to skip
through all the apples you just
basically try to finish up here so
there's a topic model which is the kind
of thing people here do at Google you
can solve some interesting problems with
that in particular let me show everyone
picture they'll give you a flavor of
this yes you can have Chinese
restaurants now arranged in a tree you
can have a Chinese restaurant of this
thing and one at this thing you can have
infinite this is the first night you go
to his restaurant and you pick a table
and that tells you where to go to the
next night either here here or here and
the next night you go to this thing
there's another infinite set of rest of
tables you pick one there and you go
down a certain branch so this is now a
random branching tree and you get a
probability measure on its infinite but
that's perfectly fine all right and now
you can do cool things like you can take
in all right here's maybe a more
interesting one to you you can do these
topic models infinitely branching trees
you can build at every node of one of
those trees a list of the most probable
words that have documents that are
following paths in this tree so at the
very top it turned out that when we did
this on the jcm abstracts over the last
ten years that the words which occurred
most frequently in all articles worthy
of and and so on all right and those
became the topic at the highest level
this branching tree and in the next
level down the most useful words were
things like algorithm time and a little
letter in which is what algorithm is
used often in there in there in there
papers queries class complexity programs
language rules networks network routing
and so on so this looks like the
curriculum of a computer science
department and then you go to the next
level and it becomes a little more
precise and more precise all right so
this is what you this was a nice example
of probability theory in action we
didn't have any tunable parameters here
we plug that we gave it the data of the
jcm and it has now a random probability
measure under it infers all of the all
that structure all those words all the
probabilities associated with that
alright so that's a nice example I think
of this kind of thing in action and then
the last slide is just a as a
statistician you worried about will this
really work can I prove
given some data that this thing will as
I get infinite amount of data that this
will proved and converge the right
answer and the answer is no you cannot
there as there is counter examples to
that you cannot prove that incomplete
generality you do not have consistency
in the nonparametric world and here's an
example where it actually you lose it
but there are now some positive results
for certain kinds of neighborhoods you
can't get it for weak neighborhood you
can get it for Hellinger and Colbeck
leave our neighborhoods and this is
actually an ongoing problem in
statistics is to try to characterize
things like consistency and weak
convergence for these random measures
okay so sorry to give kind of a highly
technical talk at the end of the day but
I hope you found that interesting to see
this this these air this this area it's
basic as a statistician I find this an
exciting area I tend to be a bayesian
statistician and most Bayesian
literature has been very parametric you
make a lot of really strong assumptions
and then you show that it works but but
but this is a way of sort of not having
to make those kind of strong assumptions
and and be nonparametric and then I hope
to try to convey a little bit reason
this exciting mathematically there's a
lot of combinatorial sand graph theory
and probability all being mixed together
with infinite objects here and there are
quite a number of open problems some of
which I've alluded to and there's many
others ok so I gave a tutorial on this
this past year and there's a long
bibliography with literature references
and so on so forth so you're interested
please have a look kind of done you'll
see the tutorial ok thanks
I see tired faces
all right well if you have questions you
can ask this week here that dinner ok
anyway dinner is at six thirty the
directions are up there so at some point
we will all migrated over let's thank
all the speakers for today</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>