<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Team Cornell and the 2007 Urban Challenge: Research, Results and Next Steps | Coder Coacher - Coaching Coders</title><meta content="Team Cornell and the 2007 Urban Challenge: Research, Results and Next Steps - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Team Cornell and the 2007 Urban Challenge: Research, Results and Next Steps</b></h2><h5 class="post__date">2008-01-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8v12wDeHsXI" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay we get started my great pleasure to
introduce
locker here I've known in compute for
about 15 years were together at the at
Xerox for a while where he at the luxury
of being a having a joint appointment in
the Cornell in Xerox i was at park for a
number of years after i think in the
early 2000 he actually left then went to
the dark side to pick up a businessman
and and help story comical teen agent
market and then since then he's
contacted Cornell or he has a John
appointment as a person business as well
as the president computer science
probably that right and lately he was a
you know cope with his co broker mark
can tell here he was leading the team
conan at the dolphin chatters about this
hopefully explain first of all how they
got this car to be so nice and clean and
neat all other cars I'd like these
sensors all over the place which were
crazy looking and secondly pretty
hopefully tell what happened in the last
mission where they somehow lost some of
the added edge so was that further ado
great thanks Luke glad to be here today
so I'm happy to take questions along the
way so if people want it and stop and
interrupt me that's great I spend half
my time teaching in a business school
and in the business school things are
very interactive it's hard to get three
words out of your mouth so I hope to
answer the couple things Luke mentioned
here so let me just give a little quick
overview about the the team so we had a
fairly small team working on the urban
challenge about thirteen students eight
of whom really took it on as their
full-time activity and two faculty who
were involved as the team co-leads mark
Campbell and mechanical and aerospace
engineering who's up here too and myself
we were one of the ten teams that had
the track a funding from darpa so we got
a million dollars and when you're paying
students a million dollars actually goes
pretty far in terms of salaries didn't
go so far in terms of the hardware that
we were buying however i think the
vehicle in the end is probably about
half a million dollars worth of hardware
we were one of the six to finish the
competition is Luke mentioned we were
not one of the top three prize winners
which were carnegie and stanford and
virginia tech there were 11 teams
selected for the final race based on
semi-finals that were run just
beforehand DARPA had indicated that they
were hoping to run twenty teams in the
actual race there were only 11 that they
felt were qualified enough to run the
race and in fact fairly quickly in the
first mission 5 of the teams were
eliminated so within about an hour into
the race it was down down to six and
there were about 75 teams that received
site visits from DARPA and June and July
be glad you weren't the DARPA program
manager who had to go around and visit
in 75 places and evaluate evaluate their
vehicles that was pretty painful so I
want to just say a few things about what
what we view is distinguishing
characteristics of of our approach and
what we've done it Cornell so one of the
things is that we really did both the
design and the development of this
vehicle to use it as a subsequent
research platform we didn't want the
view that we didn't want the urban
challenge to be the end of life for this
vehicle and that somewhat is reflected
in some of these design aesthetics as
Luke was pointing out the thing doesn't
look like it's just got a bunch of
sensors bolted on the outside and that's
partly because we believe this thing
will have about a five-year life is a
useful research platform and if all the
sensors are stuck on there like an
erector set and you live in places like
upstate New York or it's raining or
snowing a lot of the time very quickly
your sensors will have fallen off of
your car so they're they're pretty well
integrated into the vehicle and hidden
from view also the team just brought a
real sort of engineering elegance kind
of outlook and that that's reflected not
just in the clean appearance of the
vehicle but also in the fact that it
their goal as a team among the students
was really to make this vehicle drive
like it was a human and in fact I think
that probably the the biggest reward for
the students on the team was hanging
around in the lounge of the sleazy
ambassador hotel want one night talking
to the race talking to the chase card
drivers and having the chase card
drivers say you know your car drives
more like a human than any other car in
the race and for the students that was
probably the biggest reward that they
could have gotten because that was one
of their their big goals I guess another
distinguishing characteristic I didn't
write up here but I should be sure to
mention is that mark and I really were
faculty advisors to this team there
wasn't a single technical decision
that we made the students really drove
all of the technical decisions on the
team and a couple of those technical
decisions I'll just sort of give you a
little heads up at the beginning and
then we can look for them as I go
through the talk but one thing that we
did that no I think no other team did or
certainly no other team that placed well
in the race is all of the actuation was
done in house for our vehicle most of
the other teams used repurposed systems
that were human driver assistance for
people who can't operate a vehicle and I
think that in the end that was by and
large a pretty good decision although it
did lead to this problem Luke was
alluding to at the beginning and then
the other thing that we did was we did
all of our own pose estimation and
that's a place where I think with
twenty-twenty hindsight we should have
shelled out the money and bought one of
these apply next systems which most of
the other teams that did well did and
you know you're you're making all of
these decisions along the way and at the
time it wasn't clear that the Atlantic's
was really going to deliver they did um
you know so I think at the time it was a
reasonable decision but with hindsight
certainly wouldn't do that again another
big distinguishing characteristic from a
technical point of view and therefore
what I'll focus on in the technical part
of the talk of the most is that we did a
lot of stuff with identification and
tracking of objects which most of the
other teams didn't do so just a sort of
quick thing about the vehicle platform
again it's Luke mentioned this this
thing is you know very clean looking
this has a full array of sensors on it
like every other vehicle you just don't
see them because so there IBO sensors
embedded in the lidar is embedded in the
front of the vehicle here just sort of
some pictures of the steering break and
transmission actuation they were all
done to sort of the National Highway
Traffic Safety specs so the steering
angle is what will is the amount you can
steer and roll and roll this vehicle
over at 35 miles an hour so it can turn
the wheel pretty fast there were there
was basically a data center in the back
17 dual-core machines I don't think we
quite win the prize for that I think MIT
had a few more cores in their vehicle
than we did but it was pretty close and
then in order to drive all of that we
had a
a big secondary power system that could
drive all of the computers and other
things in the vehicle and this was this
proved to be more important than we
hoped it would be in the qualifying
events leading up to the race DARPA had
some trouble with their emergency safety
stop and one of the modes of the
emergency safety stop was not just to
tell your software to stop but was to
shut the engine of your car off and of
course when the engine of your car is
gone so as your alternator and socio air
conditioning and then after that
happened to us due to this malfunction
of the e-stop system the the race of the
the qualifier officials spent the better
part of an hour scratching their head
trying to figure out what to do about it
and our car was sitting there with all
the computers running in the desert in
in in in victorville luckily it was
november instead of august or i think it
wouldn't have lasted so the power system
held up and actually the computers held
up fine with without the air
conditioning for that hour the use of
mobile processors was pretty key there
because these things generate a lot less
heat and deal better with bad
environments like that just a sort of
quick overview also of the sensor system
more not the details because we don't
have time to go through it but just to
give you a sense if there's a lot of
lidar Zaun here also millimetre
millimetre wave radars some vision
systems very important there's a very
high quality inertial navigation unit
and and then two different GPS systems
and then we also for odometry of the
vehicle we use the stock wheel encoders
that are part of the vehicle stability
and anti-lock braking system so we
didn't have to add any kind of extra
encoders in for that and in fact in
general this is pretty much just a stock
Tahoe with some stuff added to it so
it's human drivable it would almost pass
inspection I think the only thing at
least in New York State if you have the
airbags disabled you can't pass
inspection but that's about about it in
terms of changes to the vehicle platform
and so this just sort of gives you some
sense that there's very good light our
coverage out over the front of the
vehicle a little less sell out of the
back and I guess the contrast didn't
quite show up here the radars are very
thin so we get straight out front and
then sort of to the
sides for merging and a little bit out
of the back so one of the big issues
just from sort of the system vehicle
platform point of view that that was
really important and just sort of
getting it out of the way so we didn't
have to worry about it was something
that had bitten the Cornell team back in
2005 when I was much less involved with
things that year that time and most of
our students were veterans from that
program which is that with all of those
sensors the problem is you have you can
easily end up with a bunch of sort of
non standard interfaces to the sensors
and so that had been a disaster really
in 05 and what what we did is a design
this time around was to use a standard
custom design microcontroller to control
all of the sensors put that
microcontroller as close as possible to
the sensor sort of short wiring
harnesses and then very quickly just
take that data and distributed out over
ethernet so all of the data was
distributed using these Ethernet ready
microcontrollers using UDP multicast and
there were time stamps that were
synchronized generate generated by the
micro so we knew exactly when the data
from all these sensors was gathered and
that was pretty critical for doing any
kind of reasonable integration of sensor
data and one of the things that was sort
of interesting at a workshop that we had
it nips recently where a number of the I
guess all six teams that completed all
participated in the workshop is pretty
much every team ended up doing something
along these lines we all we weren't
communicating with each other during the
development because it was a competition
but after you know once we got to
Victorville everyone started talking
about what they were doing because it
was sort of too late to do anything else
but everybody really pretty much ended
up with some sort of UDP multicast based
distribution of their data so so I
mentioned pose estimation telling where
the vehicle is is one of the things that
we did ourselves and I think there is
one place where there's still something
pretty interesting about this the really
key issue compared to say commercial
things like a plan X and that's the use
of vision as part of the pose estimator
um so a really key issue in in doing
pose estimation for driving and for
autonomous control as
to avoid sort of big jumps in your
estimated post so those of you who know
much about GPS know that as you pick up
and drop satellites suddenly you may
think you're a meter or two laterally
from where you just thought you were a
second ago and similarly there's the
same kind of characteristic with the
vision based matching right I mean we're
sort of trying to fit lane lines two
things here and every once in a while it
will sort of jump and decide that maybe
that piece of you know sidewalk or
something over there is the lane and the
lanes will jump laterally to the side
and then jump back again so the big
issue for that for the data here is
being able to reject these sort of big
jumps and when you have multiple sources
you know including several GPS multiple
GPS units and vision that you can sort
of correlate with each other actually
rejecting these big jumps proves to be
something that's not that difficult yeah
so with all of this basically there's an
underlying probabilistic model and
there's just a sort of likelihood ratio
test of you know how likely is it that
this is actually a correct measurement
given that these other sensors are
reporting and so but rather than just
taking all of those and just putting
them together into one estimate some
estimates can be actively rejected and
then when you will sort of if you watch
the output of the system it'll sort of
tell you you know I'm rejecting the sub
10 trio right now because I don't
believe its signal yeah Indonesian
happens that copy is that
which of those properties and here at
these bitches they inside those Johnston
yeah so the particle filtering here was
used just for estimating probabilities
of what Lane the vehicle was in and they
are basically we had a whole bunch of
particles each estimating probability
distribution over the the local road
network so I'll probably slip into urban
challenge lingo so the RND f is the is
is the road network definition file and
so the particle filtering wasn't being
used for integrating the the different
sensors but just given that we had a
pose estimate and now you want to map
the pose information onto the lanes
they're essentially a bunch of particles
that are each voting for different
possible hypotheses about where you
where you are in the map so I don't want
to go too much into the technical detail
or I won't get through but basically oh
sorry what is particle filtering so this
is a this is a scheme for handling
ambiguity of hypotheses in probabilistic
modeling so for example if you're doing
something like just having say a simple
Gaussian model right you'd sort of
average everything together into into
one estimate the particle filter allows
you to have multiple hypotheses that
you're maintaining it once and then pick
the highest probability hypothesis
rather than so you can sort of think of
a set of gaussians for example would be
a simple example of that and rather than
treating those is say some mixture you
actually want to allow each of those to
exist as separate hypotheses and then
pick the most likely one when you
actually have to make decision is the
problem with maintaining ambiguity is
that's a great idea except in the end
this vehicle has to do something it
can't say well I think maybe I'm in this
Lane and maybe I'm in that line so and
and so one of the things about this is
that it's quite accurate in gps
blackouts and so Isaac Miller who's the
one PhD student who's been involved in
this project has a paper coming out in
the robotics and automation conference
looking at exactly how accurate this is
doing fairly long GPS blackouts and this
just sort of shows a little test course
where it stays right on the sort of same
track that you get without without the
when the GPS is turned off
so as I said the main sort of technical
part of things that i think is
innovative and what we ended up doing is
is this detection and tracking and so we
use a combination of lidar which is
laser ranging radar and vision and i
should say a little bit about the lidar
data so lidar has come in in in various
shapes and forms and my sort of
characterization of the state of the art
at the time that we were going into the
race was that you either had lied ours
that were sort of one scan line or a
very small number of scan lines but
we're but had pretty good range or you
had lied ours that might give you more
scan lines but were much shorter range
and that's starting to converge now but
at least when we were doing in the
design you could either have this very
you know sort of view of the world where
you're sort of cutting the world with
one plane and you can tell the distances
to all the objects in that plane but
that's a very impoverished view of the
world because it's only one plane or you
could have a whole bunch of planes
intersecting the world getting distances
on a bunch of planes but your distance
estimates would be both noisier and not
as far away from the vehicle and so we
have a mixture of lidar units some of
which are single scan line or small
number of scan lines but can see out 150
to 200 meters and then others of which
see much less far so that data needs to
be integrated together in detection and
tracking now we also used vision for
detection and tracking but it's one of
the things that we turned off in the
race so there's a bunch of things that
the car does that weren't on for race
day and that's pretty much true of every
team is that they ended up having parts
of their system that they decided a
little too risky now that we understand
what's really likely to happen in the
race will turn this bit off yeah well a
lot of you use a liner you have
shoes where one cars being with blind
yeah so the question is is with a lot of
teams using lidar were there problems
with the the lidar is blinding each
other so all the teams did some degree
of testing of pointing light ours at
each other some teams that had multiple
full vehicles could do a better job of
that because they actually pointed their
two cars at each other um the lidar zar
fairly good at not having that be
problematic but that said there was a
complete disaster on that so the day
before the race in the morning they had
a practice run and they lined up all 11
vehicles for the race facing a big metal
bleacher that was empty because there
was no audience there and essentially
everybody sensing systems died and
nobody really know you know there's a
lot of active sensing on these things
there's the millimeter range radars
there the light ours so but you know
people were bouncing huge amounts of IR
off of this large very reflective
surface and it was coming back so what
they did in the actual race was they put
the cars out two or three at a time and
that came from trying this dry one so
there clearly is some evidence
suggesting that there's some kind of
interference among these things but it's
not interference that anybody saw in
sort of normal operating conditions but
when you took 11 of these vehicles and
pointed them all at a highly not only
reflective but you know strangely a
variegated surface so things were
bouncing off in all directions it
definitely caused issues so there is
there there and this is an issue with
active sensing in general that there's
always the opportunity for for
interference and one of the things I
think is a big lesson for us at a high
level out of this is that you really
want to be integrating a lot of
different sensor sources that have
different failure modes so vision may
have too many false positives and
negatives we're going to try to work on
that to get it down to where it's usable
but it's not going to be as susceptible
to interference of this form but clearly
the active sensors are really good when
you can use them because they they get a
lot more out of the environment than
passive sensing like vision so you had a
so the vision we were doing was to D I
think there's some interesting
opportunities to do more 3d vision with
this valid ein sensor that I mentioned
or alluded to which is it's a 64
scanline lidar unit and you actually get
quite good intensity information back in
addition to range off of the off of the
off of the lasers and so you get range
data that's perfectly aligned with your
intensity data which is really nice it's
still only 64 scan lines high and it's
all 360 degrees around but but I think
there's a real opportunity to look look
at some of that data but we were doing
2d vision and two different approaches
we done some target detection work of
our own and we also were using a
commercially provided system and
basically just there was not an
operating point that that was reasonable
given the kind of environment we were in
and on light you know and and and of
course the ideal goal one would go for
is what we achieved with the with the
pose estimator which is that when you
have some reasonable characterization of
the kinds of errors that the units are
the different sensors are giving you you
can actually reject them as being bad we
never got to that stage with the
obstacle obstacle detection basement
very do you have any people on your team
working
security in the sense of
someone actively no reply to me about
surroundings by erecting
yeah so the question is about just you
know given that these things can be
actively interfered with or is anyone in
our in fact so i should say you know our
team is sort of now history and i think
that's true of all of the teams but we
do have a smaller group that's working
on research growing out of this i think
most of the people who spent two years
of their life working full time on this
wanted a break but but so we're not
actively looking at interference with
these units at this point because or you
know with people trying to disrupt the
behavior our view is that when you're in
a friendly environment things are still
hard enough at the moment and so those
are the problems we're working on but i
think when you get into sort of hostile
environments where someone's trying to
interfere with the vehicle that that's
yet another set of research challenges
but even in a friendly environment where
there's a lot of clutter you know sort
of more clutter than the urban challenge
it's it's a problem these days so
specific I've always had traffic lights
for humans so in a city that was set up
for autonomous vehicles what would you
put in as the cheapest most reliable
system on every street corner they had
the beloved well so I mean so so so
ramens sort of question was about
passive versus active sensing and then
this issue of traffic lights and and
what would you do to sense traffic
lights sort of chief Blaine reliably
what I what I might have done before is
is a mixture of IR and vision sensing
but the problem is traffic lights are
getting replaced with these Oh LEDs that
don't actually admit that much heat
they're too efficient so the IR is not
going to work as well as a secondary
sensor so I think that probably the
right thing is to put in it errs of some
kind and there you know I think there
are a lot of solutions to different
sorts of emitter is for traffic lights
you know literally spread spectrum radio
signal and you know there's a lot of
things you can do that don't don't
require sensing and that's probably the
right thing to do if you had a lot of
autonomous vehicles so I do want to move
on so one more
mistake for people that there's this
much any machine that has this much
power and alternate Omni
being attached to it to assume that
there is such a thing as friendly
environment
not talking horse out of just talking
this is individuals oh yeah I Everglades
yeah absolutely I mean for these things
to be released in the wild a lot would
have to be done and that would include
sort of safety and security systems the
way this car is driven at the moment
it's basically a huge user assistance
device right there is a human driver
sitting in the driver's seat and he or
she can take control of the vehicle
instead they basically have a sort of
Deadman switch to kill it but they also
can just you know grab that we ought to
put their foot on the brake the way we
did the automation it doesn't preclude a
human driver from from from just taking
over now of course when DARPA ran the
race they didn't want any humans
anywhere near the car but then they were
pretty careful I don't know if anyone
saw any of the stuff from victorville
but there the the you know k-rails the
big concrete barriers that are used on
like freeway construction projects that
kind of stuff isolating off all of the
places where the autonomous vehicles
were driving where there was any chance
of pedestrians so you know like DARPA
had DARPA officials out on the course
and they were all surrounded by K
barriers so I think that to take these
things and put them in places where
there could be any reasonable chance of
them driving autonomously with people
around you have to be a lot more careful
absolutely um wait so oh yeah okay so
this this was on the on the tracking
stuff so so I'm going to just sort of
skip through this a little faster than
I'd intended cuz I want to get to
showing some some videos from various
things but basically the lidar data
itself really needs to be segmented
before it's worth very much for object
detection and then we have techniques
for determining how many objects there
are and then sort of solving the kind of
data Association problem of mi updating
some estimate of an object I previously
had are initializing a new one and then
as we estimate the track objects we not
only estimate tracks r objects but we
estimate certain metadata about them
like is this thing vehicle like is it
stopped is it moving its
and then as i said before maintaining
stable track ids over time is something
that that we did so this just shows that
the raw lidar hits which are these sort
of individual points here this is just
from a four scanline lidar the IBO unit
looking out of the front of the car so
that colors here the heights of the
which of the four scan lines and then
we've sort of drawn boxes around things
that that are clusters of points that
lie together and we have a simple
criterion for deciding whether a cluster
stable which is based on just clustering
together points using two different
sorts of separation thresholds of half a
meter and a meter and if they're the
same if you get the same cluster it's
stable and if you get different clusters
it's unstable and these are chosen based
on the fact that we knew that vehicles
had to be about a meter apart all right
so DARPA actually had constraints that
said vehicles shouldn't get closer than
one meter from anything and then even if
you and so this picture just illustrates
the out of all these clusters these
little couple red boxes here are the
only stable ones so you get a huge data
reduction just by doing some simple sort
of stability criterion on the clusters
but when you've got these clusters and
you want to start matching them over
time just the issue of what are stable
measurements from the clusters isn't
completely non-trivial one so you might
think ok I've got a box i'll take the
center of the box or the or the or the
bounding box itself or use the center of
mass of the set of points the problem is
these are very unstable things to take
out of the data because as you're moving
with respect to something else you start
seeing very very different points and
things like the center of mass and the
bounding box change size radically and
so the particular techniques we
developed our to use the occluding
contours where you sort of see the edges
of the of the object is disappearing and
the closest point and those proved to be
pretty stable but it means that there
are various things that we can see that
we won't get estimates from because
we're assuming we have both including
contours so if you see an object that's
partly hidden from view we're not going
to be able to estimate things about what
it's about what it's doing very well so
the other thing it was really critical
in getting targets out of the lidar data
is to have
reasonable estimate of where the ground
is in the lidar data you're getting
these these clusters of points back and
you know people are great at looking at
the lidar data and saying yep this is an
object note that's background but not so
easy in the in the raw data itself in
terms of algorithms and so what we did
is to have a model of what the height of
the ground is everywhere and then things
that were above the ground by more than
a certain height or obstacles that we
were interested in and the velodyne
because it gives us so this actually
shows these dark blue lines here are the
are the actual velodyne data so you can
sort of see what the 64 scan lines going
out would look like from the vehicle and
then you can sort of sort of grow out
from those locations by minimizing
because you're looking for the lowest
stuff in the environment by minimizing
height and then that gives you a ground
model but there are always regions where
you're not getting any hits back at all
for whatever reasons either because
you're off the top of the lidar or
because of the way the vehicles pitched
with respect to things so here there was
something higher further away that did
get a few hits but a region in the
middle where there were no hits and then
you can take these and project them
forward over time because you know the
Eagle motion of the vehicle and get a
pretty good ground model over time so in
doing the tracking of the objects we we
represent each object in its own
coordinate frame and then store the
observed lidar data points and other
data of things like the radars give us
back point information also and then we
keep a 2d rigidbody transform that says
how that object is is changing over time
and that's relative and then ground
speed is absolute because we really care
how fast that vehicles going and not
just its motion relative and then
heading and then we just use common
filtering to sort of standard thing for
this kind of scenario for predicting the
point locations forward and then when we
get the new data we just update the
coordinate frame and our velocity
estimates and one of the key things is
we throw all the data away on each frame
so once we've decided which new data
points belong to an object those become
the new data for the for the for the
model we again use particle filtering
to represent alternative hypotheses but
here it's a very small number of
particles so basically the problem that
you have is that if you have several
objects that are near each other there's
a data Association problem you know
which object at which time corresponds
to which one at the other time we need
some small amount of ambiguity to be
able to represent those hypotheses and
so we use a small number in the urban
challenge we used for particles to
represent so each particle is a
hypothesis about all of the objects in
the world and which objects they came
from at the previous time and then and
then we use four of those to maintain
some amount of ambiguity in in the
tracking so so we when we do sort of
integration and fusion of data we do
this all at the object tracking level so
this is all at the level of objects and
not trying to do things like integrate
the radar and lidar data together
globally we only do this for the for the
things that we've identified as being
trackable objects and to start new
tracks things after meet certain
requirements so as I said I think I
mentioned before the lidar we need if
we're going to start a track with lidar
data we need to see to including
contours we need to see sort of both
edges of the object and not have it be
partly hidden and emerging from behind
something else on the other hand for
radar if we've got a hit we start a we
start a track in the urban challenge
there were often about 50 simultaneous
targets so this is just a picture of one
of the things at the qualifying event
where there were two lanes of traffic
moving around so each of these are
tracked targets green means we think
it's moving red means going to get
stopped this is our ego vehicle um and I
should have a just a video of this so
this one of the guys on the team called
this ford taurus frogger so you can see
these are a bunch of ford tauruses
they're driving by the front of our
vehicle as we're looking here you can
see that there's a que barrier directly
in front of us so right along the side
of this there's a que barrier and
basically you have to sit here and wait
until there's enough of a gap to pull
out and we're supposed to turn left
across the oncoming traffic and then out
so here we decided there's enough room
we're moving along so again in
video here it's small but you can sort
of you know there's not much here so you
can see the car going around and you
know we're going pretty close to these
other vehicles these are not wide Lane's
given them the width of the vehicle that
we're driving so coming around the top
here now the next thing we're gonna have
to do is make a left hand turn again
across oncoming traffic we don't have a
stop sign here we slow down we wait you
can see a vehicle sort of appeared out
of nowhere there and it's because it had
been included by another car make the
turn across and so the way the what this
was probably the hardest thing in the
whole urban challenge this was the area
a qualifiers and so what you had to do
is make as many laps here as you could
in 30 minutes and if you cut somebody
off if you pulled out too soon and they
had that there were human drivers and
all those ford tauruses and they had to
jam their brakes on they honked and you
had to get as many honk free laps as
possible so and every team got two runs
I think on this and and on the second so
this was actually from our second day
running on this and then Tony tether the
DARPA director was there basically
saying you're in you're out as is this
the teams were running around this
course so it was a kind of interesting
so in addition to so the object tracking
there is really important for being able
to tell when there's enough time to pull
in in front of other vehicles but one of
the other things that our system does is
it actually maintains identifier is on
those objects across time to allow us to
try to do things like reason about the
vehicles over longer time periods and so
here again we use sort of a standard
kind of approach of doing maximum
likelihood estimation from frame to
frame again using these sort of stable
measures the closest point and inclusion
bearings and so again here that's the
same kind of picture from before but in
this little movie I'll show and I'm not
sure probably the screen resolutions not
going to be quite good enough but we'll
see so can you yeah I think maybe so you
see there's a 116 on that car up ahead
of us so you know as we drive around and
again here this gives you a little sense
of what things look like
victorville I mean it's sort of a
suburban environment in some ways it's
the darpa suburban challenge I mean this
is a residential area by the way we
didn't see this area until right before
the race our car spent five days driving
through here and we'd never been in it
during the qualifiers I mean we saw a
video like this of it off of our vehicle
but we didn't see the place so you can
see we're following along here and that
idea is still staying 116 throughout
time the vehicle behind us actually
changed IDs once during this sort of
minute and a half run and we have a lot
less sensing out of the back of the
vehicle so the ability to do this stable
identification and you know knowing
things about the fact that you're
tracking the same vehicle over time can
be very useful and in fact I'll show you
in a second scenarios at 4-way stop
signs knowing which vehicles or which
can be can be quite important oh so so
as I said the sort of in addition to
actually doing tracking we also for
higher-level planning want to know
things like you know is this a car is it
stopped or not and so here we use just
the simple to state hidden Markov models
on certain data like you know width of
the point clusters and speed whether
something's included or not we use
geometric reasoning for that and then
Lane probabilities we have to we have to
assign Lane probabilities for the other
vehicles for example if there's a
vehicle coming at you and you think it's
in your lane your behavior is very
different than if there's a vehicle
coming at you and you think it's in the
opposing light and those aren't very far
away from each other right your lane in
the opposing Lane is it a few meter
lateral shift and yet your behavior is
going to be completely different so and
so it's it's really important to have a
reasonable estimate of what's going on
with respect to the map now there's a
sort of there's a there's there's a way
to get this fatally wrong and though and
the way the way to get this fatally
wrong is to therefore do all your
reasoning with respect to the map since
you need to know where other vehicles
are with respect to the map let's just
throw everything into the map and the
problem is the maps not necessarily
accurate your estimate of where you are
on the map isn't necessarily accurate
and so if
you're going to only reason about other
vehicles with respect to the map when
you're really perceiving them with
respect to yourself and then mapping
them into the into the map frame that's
a bad idea because if something's coming
at you at some point you don't really
care where it is on the map if it's
close enough to you you want to be
taking evasive action and so so there's
a real challenge in the representation
for these kinds of systems is
simultaneously having some
representation with respect to a map
like this and some representation that's
just local with respect to the vehicle
and being able to and now you've got two
representations and you want to do
something coherent that's consistent but
but we and most other teams that did
reasonably in the race ended up dealing
with this by having both a map based
representation and a local vehicle
centric one so so this if my little
mouse will appear so this shows a area c
of the qualifiers and then i'll get to
some video from the race itself in a
minute but so this is again at the
qualifiers were pulling up to an
intersection here you can see there one
two three cars already parked these
little L shapes are the actual lidar
hits and then these are the and and at a
four-way stop you have to reason about
the fact that these guys are all here
first so you have to wait DARPA counts
to five and then let's this cargo you
can see green and that one flicked
purple for a minute which meant that we
knew it was occluded because of course
it disappeared now this one goes you can
watch this one little flip purple and
then back to read I guess the purple red
distinctions not so clear here so we
realize that this vehicle still there
it's not something that appeared because
it was occluded then this one goes and
it's time for us to go and you can see
here these tracks sort of splitting up
in some weird ways there's some blind
points out of the side and rear of the
vehicle and we're not so good at
tracking an object consistently through
time as it goes past there and so then
the vehicle goes on so in many ways
these tests of the merging in you know
the sort of video Frogger and the
intersections were actually harder than
what happened in the competition itself
so DARPA set up a set of qualifiers that
were quite rigorous and I think it's
partly because you know in many ways
this
ivenn was was a public relations event
as much as anything else and I don't
mean that in the negative sense I mean I
think everyone who participated in this
thing regardless of how they did feels
that it was an unbelievable opportunity
to be able to participate in it but it
really was geared toward sort of the
public learning more about these kinds
of vehicles so I think darp I wanted to
be pretty darn sure that the vehicles
behaved well before the race day and so
they made the qualifiers even tougher
than the race itself um so just sort of
a comment about tracking versus
occupancy I believe we're the only team
that did tracking as opposed to sort of
reasoning about the world by just sort
of looking at what's out there around
the vehicle and doing that both vehicle
centric and with respect to a map and no
currently we have that kind of at the
level required for this intersection
precedence but of course you could do
intersection precedents also by just
sort of drawing a box that each stop
wine and saying well if there's
something in if if that box was full
before I got there and it's still full
now there's you know somebody else is
there with precedence over me so you
don't need to do vehicle tracking to
solve that kind of problem but you know
the problem is is you get sort of you
know more complicated behaviors it's
good to actually have a notion of an
object over time and identity but that's
not to say we've solved that problem i
mean you know as you get longer time
periods and you get the kind of change
in shape that happens as things move
with respect to each other that the
tracking has problems also but this is
definitely a research direction that
we're very interested in of using
tracking the reason about the world
around us and I'll also mention in our
little fender bender with MIT in the
final race that this sort of tracking I
think is really crucial to preventing
those kinds of problems so so just a
little bit about I've been focusing on
the perception stuff largely because I'm
a perception guy let me just say a
little bit about the decision-making
stuff so we had a three level
architecture one sort of which we call
behavioral which is kind of macro level
planning right this is sort of you know
Google Maps right it's a map and find me
a route from A to B then the tactical
sort of local planning is deciding
things like when should I change lanes
when should I pass somebody
you know when should I merge into an
intersection and then the operational
layer is the sort of planned execution
generating paths and obstacle avoidance
and almost every team had some
multi-level at least two and probably
three level set of AI that kind of made
sure that at least this low level path
planning was separated from the higher
level sort of reasoning about what the
vehicle would be and should be doing so
this is really a state machine here and
then this in the way we implemented it
is a gigantic non-linear optimizer so
the operational layer does constrain
nonlinear optimization it starts out
with some base path that comes from the
map some set of constraints that come
from things like sensed lane boundaries
or lane boundaries from the map itself
some set of target paths about where
we're trying to get to starting and
heading positions things about obstacles
and then it tries to interpolate a
smooth path through there just by
setting up a big a big nonlinear
optimization problem and this is really
the route of the vehicles ability to
drive more like a human because it takes
all these constraints and throws them
together in one of the set one of the
very strong sets of constraints is about
smoothness of the final path and people
tend to drive very smooth paths and this
is done just using a an off-the-shelf
nonlinear solver called loko and this
planning is done in fact almost all the
high-level planning is done at about a
10 hertz rate oh so this sort of shows a
picture for the path planning
constraints of this and I'll show
another little movie here this is an
area that again from the qualifiers that
they called the gauntlet because they're
a bunch of cars parked on both sides and
you have to sort of swerve through them
so this show is a slightly different
view that we've been seeing before these
are sort of convex hulls around sets of
points sensed in the environment this is
where the path planner is trying to go
these are the lane boundary constraints
here and here and you can see here that
they sort of dart in and that's because
there's an obstacle here and so the the
path planner is now going to try to find
a good non linear path given this
heading of the vehicle here the desired
heading coming out of the path they are
and the set of constraints around the
boundary and it just sort of keeps
planning that forward at a planning
horizon that depends on the speed at
which the vehicle
moving so again here this is a little
video of this so you can see here we're
approaching a car it's on the right
which is going to be there there
suddenly the constraint appears and you
can see the car very smoothly sort of
steers around and then it's going to
steer back it looks like we cross the
yellow line there because of where the
camera is but in fact the vehicles date
on its side now there's some barrels in
the middle we're going to swerve a
little to the right now we're going to
swerve a little to the left again and
you can see the constraints keep coming
up here but you get this very smooth
path even though we have these discrete
events right these constraints suddenly
pop in and appear but you don't jerk the
wheel around because you're doing this
nice nonlinear optimization to find a
smooth path through here and so it gives
so this was one of the places where we
definitely had issues when we went
through this the first time we thought
this was just blocked because of the way
these cars were parked and that we
couldn't get through and so to get
through the second time around we we
reduced our clearances a lot by how
about how close we were willing so we
pass within about 20 centimeters of some
of these parked cars at you know 20
miles an hour but in in in in the words
of someone from DARPA well I can drive a
Chevy Tahoe down that at 20 miles an
hour so so i guess i guess rai can now
too but but that raises some issues that
we'll see in the final race and some of
the things that happened there so the
last part of the high level stuff is the
higher level planning so the tactical
planner has different it's a big state
machine it's got different so it's got
sort of Road reasoning intersection
reasoning zones which are sort of
parking lots and big places where you
don't quite know what's going on and
blockages the whole design for this was
being able to recover from places you
didn't know where you were because the
vehicle might start up in strange
conditions and this in fact was a big
problem for stuff that happened in the
2005 challenge but the idea is that the
vehicle sort of has these oxidants
around it and it has little sort of
software monitors that are responsible
for telling what's going on in different
places and so if you're in say in a stay
in Lane behavior
there's a 4-1 forward monitor here which
is going to monitor the distance to the
nearest vehicle in front of you so that
you're staying in lane trying to drive
the speed you want to but you have to
keep a safe distance from the vehicle in
front and so this is a nice sort of
example of the stay in Lane and change
lane behaviors this is from the actual
final event or this is a place to lane
in each direction road known as phantom
and we're coming up behind a chase
vehicle and an autonomous vehicle here
this university of central florida and
you can see we changed lanes as we saw
that vehicle out there we're going about
35 miles an hour they're going about 15
the speed here is about 35 you can see
that we're catching up there's our chase
vehicle behind us right now we only see
the rear most vehicle and in another
stare that's the that's the vehicle in
front so this is the chase vehicle and
the chain the lane change behavior wants
to change back to the right lane but it
realizes that this gap here is not a
reasonable gap to pull into so it keeps
going and then as soon as it sees a big
enough gap then it's going to do a
smooth lane change back and you know the
low-level operational layer that we just
talked about is what makes that be
smooth right that the high-level planner
here that was saying change lanes to get
around this slow-moving vehicle change
back just has these discrete pass stay
in Lane return to lane and the smooth
path planner is responsible for making
that smooth yeah so so the issue the
question was how do we deal with
opposing traffic during that kind of
maneuver we only pass moving vehicles
when we have another traffic lane in our
direction in the map we do pass stopped
vehicles even if we have to pull into an
opposing lame but we don't pass moving
vehicles and that was part of the DARPA
rules was that you didn't do a moving
pass you didn't pass a moving vehicle
going on the wrong side of the road the
sensing horizon here is with the with
the radars is far enough though that at
35 miles an hour if we had good radar
coverage you'd probably be fine
but our radar coverage is not so great
we have only five of them and only one
is only five on the front bumper and
there they're looking more sideways for
merging than they are forward so so in
the final event there were three
missions with a total of about 56 miles
we completed that in five hours and 53
minutes of autonomous driving and about
half of the time was spent in the third
mission so as Luke mentioned we looked
like we were doing great in the first
mission pretty good in the second
mission and so pathetically in the third
mission I wanted to hide under a rock
well he didn't say that but all that's
how I felt on that day was like no not
after all that good performance earlier
but but what happened was we had a sort
of intermittent problem and I must say
this race relate to me was about two
things it was about software it was
about testing and not software testing
but testing testing like testing the
whole integrated system and you know I
think we did pretty well on the software
side I think we didn't do that great on
the testing side and we had a sort of
intermittent throttle problem that we
had seen occur but it usually didn't
occur until several hours of autonomous
driving which makes it fairly hard to
debug we thought we'd fixed it we
obviously hadn't and when this throttle
problem happens the vehicle the AI will
be commanding the vehicle to go at some
speed but the throttle actuator will
only run at the engine idle speed which
in the Tahoe is about five miles an hour
unless you happen to have it happen when
you're saying second gear and then you
go faster right because it's just the
idle speed but if you're in first gear
at idle it goes about five miles an hour
um and so that's what sort of killed us
at the end there but I must say I mean
really you know having completed the
race to me is a great thing you know
whether we did it that fast or not I
don't know this is still annoying but it
is what it is test more and I was sort
of the you know the test junkie of the
group anyway I mean I was sort of my job
was to keep beating everybody up and
test more but still wasn't enough so the
race there were hundreds of interactions
with other vehicles right i mean we went
through on the order of
you know a hundred 4-way stop signs most
of which had other vehicles present some
of them are pretty interesting which
I'll touch on here so as a traffic jam
in the first mission which was caused by
university of central florida which is
one of the teams that was in but was
disqualified pretty soon after this
mishap where they stopped at an
intersection for about 10 minutes and
there was another interesting one with a
stunt driver a human being going the
wrong way down a one-way road that was
kind of cool we dealt pretty well with
that and there there's our collision
with MIT so the traffic jam is kind of
an interesting one because i think it
points to this need to do much better
planning ahead and and to have a
perceptual system that can that can
enable that so here we're pulling up
behind a whole bunch of stopped cars and
in fact that vehicle is still moving
when we pulled up behind it this just
again shows this sort of view that we've
been saying all along as we pulled out
to start to pass oops somebody else came
along then we sort of pulled way over to
the side of the road here so if you look
and see what's going on here we can't
get back into our lane after we passed
so we're just going to pull way over and
try to let other people be able to get
by a little bend the pen lehigh team
does in fact come by while we're there
and then eventually UCF decides to go
and and we pull out so in this traffic
jam I think the thing that's sort of
interesting here is we made very local
decisions we decided to pass a stopped
vehicle and then by the time we pulled
out the gap that we could have pulled
back into closed because the stopped
vehicle moved forward and this shows
that you know if we'd been doing better
tracking and reasoning about behaviors
over time the fact you know if you pull
up to a line of cars and the last
vehicle still moving as you pull up
you're not going to pass that last car
right I mean unless you're a really type
a driver but I mean the point is you're
not you're not going to get anywhere
there was a whole bunch of stopped cars
and this last guy stopped far enough to
leave you a little gap and then he sort
of rolled forward later and you know so
that kind of reasoning means we needed
to be able to say that car that is
stopped now we
all moving he's not the problem passing
him is probably not a good idea if we
want to get out of here let's make a
u-turn and replant and the problem is
once we started passing it's pretty hard
to back up now when we're in this one
lane place with cars on the other side
and it wasn't wide enough to make a
u-turn so we were sort of stuck so the
ability to reason further ahead is
important so here's the the wrong way
car there was this traffic driver as I
mentioned two who sort of got lost and
was going the wrong way he was moving up
the hill as we first came down and I'm
not quite sure what would have happened
if he'd still been moving when we got
near him probably we would have stopped
and pulled his far over to the right as
possible that's what we should have done
but by the time we got there he was
stopped and so we just passed him as a
static obstacle and we didn't actually
slow down and stop because of him but we
should have if he'd still been moving
when we got there so this is just a so
DARPA put some dirt tracks in various
places to connect various parts of the
course together you can see the earth
and berms on both sides this just sort
of cuts further down he calls to see why
cameras don't always work so well there
that guy is parked on the side and so
here this is ego moving this way
following that vehicle and there was the
parked vehicle and if that vehicle had
been moving instead of parked or higher
level reasoning would have said hey
there's someone coming at you in your
lane stop and get as far over as you can
but we didn't get close enough he
stopped moving beforehand and and we
kept going so so I think but the maybe
most interesting interaction was MIT and
us had a little bit of a fender bender
in the third race we were the two
vehicles out on the course the longest
and we were having an issue at a stop
sign where we have a sort of stop and go
behavior because there was a k rail that
was kind of far out into the lane and we
thought that the lane was blocked so MIT
tried to pass us and first they tried to
pass us where there was a two-lane
segment but then it narrowed down to a
single lane then it came to an
intersection but they sort of kept
trying to two inch their way around and
by the time they came alongside us we
had no reasonable estimate of their
speed and so
we sort of you know in our stop and go
behavior happened to hit one of our go
moments just as they were beside us
trying to pull into the same Lane and
the two cars just drove right into each
other now the reason I think this is
interesting in terms of future research
questions is that the if we'd been able
to reason about them over time like if
MIT had tracked our car over the two
minutes or so that they were trying to
pass us they would have realized we
weren't stopped we were stopping and
going and they would have given us a
much wider berth and not just pulled
right in front of what they thought was
a stopped obstacle similarly if we knew
that this vehicle coming from behind was
the same vehicle that was coming from
behind us and was trying to pass us
reasoning about that behavior over time
we would have been much more
conservative about what we were doing
and I think if you know other than the
bozos on the freeway who you see who
seemed to be swerving in and out paying
only attention to the one car
immediately in front of them most of us
drive with some degree of awareness of
the vehicles around us and what those
vehicles are doing over time and that's
a very important part of both good
defensive driving and reasonable
offensive driving and that's not
something that these vehicles have the
capacity to do right now so this just
sort of shows in lidar data our view of
the world in mi T's view of the world
and you can see that we have this K rail
very close to the vehicle to the right
here and then there's MIT just to our
left and we don't have any good velocity
estimate of them here and oh you know
the world looks nice and clear out in
front and so we just start moving that
way because their stuff on both sides
but they're there they came like this
and they're turning in front of us and
we don't understand that trajectory so
what so this may be a little too washed
out to sea will see this is a mixture of
some video from we don't have a good
video of this happening other than from
the tent where DARPA was projecting
their video but you can see we're sort
of stopped here stopping going there's
MIT coming around us they turn in front
of us and smack the two cars go so this
is just showing again sort of what looks
likes happening from the point of view
of so that's them come and so you know
we saw them they saw us it's not a
question of seeing
a question of not understanding enough
about the behavior of multiple moving
objects in the environment and so we're
in the process of analyzing in a lot of
detail exactly what happened here at MIT
and us are jointly going to write a
paper describing this incident so
there'll be you know it's like there's
always an academic opportunity out of
anything although at the moment it
happened I think both of us were oh no
but and so both vehicles drove away you
know they separated the vehicle both of
our recovery teams went out separated
the vehicles and there we are pulling
away so there was no actual damage that
prevented them from completing the race
which was a good thing so no my main
goal going into this was to finish the
race and not to get not to destroy
anyone else's car ah uh-huh or god
forbid in humans but I was pretty sure
dark what was going to set things up so
that there wouldn't be much risk to
humans and and so we you know we came
close to that but but not completely so
just to finish up here just a couple
things about some lessons learned as I
said this is largely about software and
systems testing this race the accurate
distribution and time stamping of data
was critical the multiple sensing
modalities and a good ground model are
really crucial to being able to make
these things work reliably and I think
that should be a combination of active
and passive sensing although in the end
we and almost every other team ended up
relying either largely or completely on
the active sensors in the actual race
the only passive sensing we did in the
race was for Road finding not for
vehicle finding this was sort of an
interesting one to me I didn't really
believe when we went into this that this
constrained nonlinear optimizer was
going to be ready enough and mature
enough for doing the planning of the
path of the path planning and it
produces really nice smooth paths in
fact I invite anyone who just happens to
be in ithaca new york to come drive in
our car it's act it's actually not a
nauseating experience that the car
drives quite smoothly and then you know
I think this sort of track metadata and
going beyond occupancy models toward
behaviors is a really important
direction for future research I think
another very important direction which I
didn't touch on too much is just this
sort of deterministic state machine base
high level reasoning that we were doing
is very delicate this is clearly not the
way to go for for more complicated kinds
of environments so here's just sort of a
list of the people involved in the roles
they had and and and our main sponsors
and I'm happy to take more questions
religions so the question was suppose
say during a collision or in fact any
other time some fraction of the machines
went down can we recover most of the
system is stateless so the only bit of
state is actually stored in the very
highest level planning module which is
keeping track of what goals we've
achieved in the mission so far and so
the system was pretty robust two things
dying and then starting up again it
might take us some time to recover and
if machines crashed at a time when you
know the vehicle was in the middle of
doing something it might do something
bad in the interim but essentially the
way we designed it was that the very
lowest level controllers if they stopped
getting commands from the high level
they shot the vehicle down so that if
the crash got you to a state where you
weren't commanding the vehicle it's so
several teams had problems where they
just kept doing their last commanded
control I don't know if some of you saw
there's a I think there's even a video
of it on YouTube but Georgia Tech had a
problem that was really sad in that area
a where they waited and waited for the
vehicles to go by finally they got a gap
they pulled out but instead of turning
they just smashed into the K rail and
sort of the acceleration to try to pull
into ten mile an hour and all the
sensors are in the front of these
vehicles so it was it was it was it was
a very sad thing but what had happened
is something had crashed and they were
just maintaining their last commanded
their last command so so if you have
some low-level failsafe that shuts
things down if it's not getting commands
and and then a stateless system design
pretty much you could shut off any
computers you wanted to in the car you
know modulo the fact that if you're
going at 30 miles an hour it's going to
take the car a while to stop and you're
stopping blind right it's just applying
maximum braking force but other than
that
not not really a problem for various
reasons we didn't actually persistently
write the little bit of state out so if
the highest level module it crashed we
would have started the mission over but
that was a very small very well debugged
piece of code so and it at least ran for
six hours we've never seen that piece of
code actually crashed I guess the
computer was on kind of crashed 24 clock
drift and I mean basically how do you
keep the time so so the question was how
do we keep time pretty much everything
is clocked off of one central clock so
there's an there's there's a this
Northrop Grumman Lytton and inertial
navigation unit in the system that also
generates a very high accuracy clock
pulse and that clock pulse is
distributed to all the microcontrollers
and all the data is time stamped with
that and so the data is then all just
distributed out on this on the network
but it's all got consistent timestamps
and basically everything you can sort of
look at the system as being an
event-driven system where the events are
the data and since the data is all time
stamped with very high degree of
synchronization that's what we end up
synchronizing on so if the IMU is clock
pulse were to die that's definitely a
single point of failure it's one that
we've seen happen actually because we
how much dirty laundry should i air we
made a modification that was going to
have no effect whatsoever about a week
before the qualifiers which caused the
IMU to stop generating his clock pulse
it was fixed and we also had a spare IM
you but we didn't want to go into the
race without a spare IM you so but that
is definitely a single point of failure
is is that pulsing organization
yeah yeah
so day get him humans managed to grab
his passive sensors
it seems like most of these teams and
give it up
is there any chance of actually
just so so so so dicks question for
those who didn't hear it is you know
humans drive with just passive sensors
it seems like most of the teams have
given up on passive sensing and vision
and or using lidar and radar is there
any hope for vision is that a reasonably
so I I have a big hope for vision I mean
that's a piece of why I'm interested in
some of the research following on from
this but maybe not quite in the way that
the computer vision community has tended
to look at vision in it um I think that
there's a real opportunity here for
using active and passive sensing
together and I'm much more interested in
that step than trying to solve this
level of complexity problem with purely
passive sensors I think that the degree
of complexity of these environments even
in the urban challenge which is a lot
simpler than you know even just trying
to drive out of the parking lot here
much less say on 101 at rush hour the
complexity there is high enough that
we've got to use all the sensors we have
available to us rather than just
restricting ourselves division but you
know one of the bad things about lidar
is with just a point cloud it's very
hard to do any sort of identification so
several times we thought k-rails were
cars and with vision you would never
make that mistake okay rail the point
cluster maybe car-sized but there's
nothing else about it that looks car
like and so I think integrating these
different sensing modalities is really
really important so I guess one last one
last question is that are any of the
clear is taking their expertise to
important commercial markets like toys
yeah so the question is as any of this
expertise being taken to important
commercial markets like toys I think
right now the main commercial markets
Dino of that are aware of this or the
auto companies who have really changed
their view of the role of automation in
cars in the last few years um I would
hope so i'm not aware for example of toy
toy companies doing this which doesn't
mean they're not it's just I'm I'm not
aware of any you know I know one thing
DARPA was trying to do with the very
wide scale publicity and running this
largely as a media event was to look for
a broad range of possible spin outs of
the technology but I think you know
autonomous toys is a great it is a great
possible environment for looking at
these I mean some of the techniques that
we developed and others are going to
have a hard time there because we all
depended on Gooden Urschel navigation
and you know it's pretty important to be
able to project your representation of
the environment forward over time in an
accurate way and if you don't know how
you if you don't know what your ego
motion was that's hard on the other hand
that's a place where computer vision
might actually save us Bob bowls and
colleagues at essar I just down the road
here have done some very nice work on
visual e based visually based IM use so
basically visually based an original
motion not name or not inertial but non
inertial motion visually based motion
estimation over time and that you could
put in a in a toy alright well thanks
not happy to answer other questions
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>