<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 14: Exposing Digital Forgeries from... | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 14: Exposing Digital Forgeries from... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 14: Exposing Digital Forgeries from...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/lv95oQEkZ0w" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome to photo tech again I think
this is day 14 if I remember right and
today we have dr. honey Farid from
Dartmouth computer science department to
talk to us about a new approach to
digital image forensics honey take it
away I'm amazed as many people here this
early geez I'm not sabbatical so like
nine o'clock seems really early to me
now so at Dartmouth we've been thinking
for the last seven years or so about how
you detect tampering and images and one
way you might think about this is the
way most people have thought about it
the past which is watermarking so you
have a trusted camera you insert a
watermark you extracted signature and
you look for deviations of that as
evidence of tampering and that's you
know a reasonable thing to do to the
extent that you believe watermarking
actually will work but I can tell you
having done this for a few years now the
vast majority of images you don't have
that luxury things are appearing in a
court of law or in the media and the web
and you don't have this luxury of saying
you know what was this thing I do I have
a trusted sensor and so we've been
thinking about how do you do forensics
in the absence of watermarking and of
signatures and so what I'd like to do
today is just give you a little bit of
idea of the scope of the issues around
digital tampering like where is it a
compacting us as a society I'll give you
a brief overview of sort of how we've
been thinking about this problem and
just very quickly highlight a number of
techniques and then near the end of the
talk will dive into some details and as
you do some some real technical work
okay so where is digital tampering
affecting us about the obvious place i
know i love the slack so there's always
like about a three-second pause and then
people see it it is it possible to bring
the lights down in the front just a
little bit just to get a higher contrast
okay thanks so it's clear that digital
tampering is affecting us in the media
I'm just in this last week the New York
Times published a doctored photograph
newspaper and Ohio published you know
close to 100 doctor photographs this
year alone that they discovered and you
know pretty much it every week this is
happening this is at T an image that USA
Today published just like this this is
actually what they publish the
the photograph that the photographer
actually took and this is a true story
when this image showed up the White
House wrote to the USA Today or call
maybe and complained that they that the
USA Today made Condoleezza Rice look
demonic this is this is the word that
they use they were very upset and USA
Today very quickly retracted and issued
an apology but you know this is a very
simple type of manipulation
photographers are doing this all the
time right there's not shocking i love
this example because the looking about
how few pixels were actually changed and
what a dramatic impact it had on the
image people probably saw this one this
was a photograph published by reuters or
late last year this is the image that
was published it showed the remnants of
an israeli bombing at a lebanese town
this was the photograph that was
actually taken by the photographer and
you can see here oops red button okay
you can see here these little curlicues
repeated over and over again so he did a
cloning job he just added more smoke
people accused him unfortunately was a
Lebanese photographer so there was sort
of the whole you know Lebanese Israeli
thing they accused him of trying to
exaggerate the damage of the bombing I
don't have to buy it I think he just
thought this looked weird and just
thought it would look better with a
little bit of smoke nevertheless on this
de Ruyter I me people are still talking
about this photograph and Reuters are
still reeling from the scandal of
publishing a doctored photograph and i
can tell you i was recently visited the
associated press they're also completely
freaked out by this both these
organizations deal with on the order of
10,000 photographs a day so you can
think about the scope of the issues and
seventy-five percent of the
photographers are freelance they're just
you know don't don't know who they are
they're just young people sending
photograph so both the reuters an AP are
clearly dealing with this issue in a
very serious way this is the cover of
Newsweek that appeared when Martha
Stewart was coming out of prison the so
the headline reads after prison she's
thinner wealthier and ready for prime
time so the editors at Newsweek thought
or hypothesized that when Martha Stewart
is going to come out of prison she'd be
thin because the prison food is bad and
so they wanted a picture of a thin
Martha Stewart but she's in jails and
I'm a picture of her so what do they do
but they took her head
and they took a models body and they put
the two in the cover and they put they
published it so this is this is not
Martha Stewart if that means her head
but if somebody else's body this is the
cover of Newsweek and this is not like
Star magazine right i mean this is
amazing and so when when people complain
Newsweek said look what's the problem
and they said go to page likes four and
then three point font in the bottom
right hand corner it says head shot by
and they give the photographer's name
and that's their disclosure that this is
not actually a photograph of Martha
Stewart but it's in fact just her head
in somebody else's body who we don't
know and by the way Time magazine just
published a cover with Ronald Reagan on
it with a fake tier news and time we
have been doing this forever and they
finally just came out and said look our
covers are strictly conceptual get over
it right there are not photographed
anymore the conceptual which I love they
finally admitted it and we can move on
so the media is clearly on a weekly
daily basis struggling with this i mean
the vast majority of images you are
seeing on a daily basis in the media are
being manipulated in some ways from but
the profound to the trivial no doubt
about it it's happening our own back
door this is a picture of professor
Hwang woo-suk who was being touted as a
possible Nobel laureate because of his
groundbreaking work and stem cell
research published several very
groundbreaking papers on in science and
slowly the results are done ravel and it
turned out so he's doing cloning and it
turned out that the cloning he was doing
that was not in the lab it was in
Photoshop what he was doing is he was
cloning images of cells to make it look
like he'd actually cloned cells but it
was a Photoshop cloning which was a bit
of a problem it turned out and the the
papers got retracted he resigned it was
a disaster and I can tell you it is not
just him I was recently talking with
Mike rosner who's the editor-in-chief of
the journal of cell biology and he says
that twenty percent of manuscripts that
he publishes figures have to be remade
because of inappropriate manipulations
in the images that's twenty percent and
he said one percent are downright
fraudulent they're just the fake
completely fake so take that one journal
and i'll multiply that by you know what
we're publishing and you've got a major
problem out there
and I've talked with science and nature
and pnas and folks at the office of
research integrity and it is there are
there's another major scandal that will
happen in the next few years and slowly
the scientific publishing community has
to start getting a grips on this on this
problem ok so it's happening in media
it's happening in science it's happening
in politics i love this last election oh
so much fun so this was ned lamont of
connecticut he was running against
joseph lieberman and one of Lieberman's
i guess people who wanted him in office
published this image on the website this
is actually published on our website ok
so fine this is probably not a true
image and I don't think very many people
are fooled by it but it was a
fascinating example of tampering because
what you see more and more in political
campaigns are the creation of digital
images linking people in a both negative
and a positive light and even though
even if people know that the image is
fake these images stay with you in fact
there's really interesting work in the
psychology literature that says even if
you show images to people and you tell
them to their fake you can actually
start to implant those memories in them
they start to forget that they're fake
images they're like they're like false
memories so okay so nobody's going to be
confused by this but nevertheless this
image which it appeared when John Kerry
was running for the the primary actually
appeared so it showed carry and Fonda
sharing a stage an anti-war rally and a
head like a hole headline the caption
the whole bit was a really good forgery
made the rounds front front page
everybody was talking about this when it
came out and then later came out there
was actually a fort forgery it was a
composite of these two images they were
they never at the end anti-war rally and
I heard this on NPR just before the
election there was an undecided voter
and the interviewer was asking them who
you gonna vote for you said why can't
vote for Kerry and the interview so why
not I said well I can't get that image
of kerry and found an anti-war rally out
of my head and the interview said well
you know it's a fake image he's like i
know but i can't get the image out of my
head and I mean that was fascinating
right I mean he knew it was fake but it
still bugged him it really really
troubled him even though it was a fake
image and you are seeing more and more
political campaigns are doing this
of games it's happening all the time now
which i think is a very interesting turn
okay so now it's in the media it's in
the science it's in politics it is
certainly in the law in a very profound
way and let me just give you one example
of that in 1996 Congress passed what's
called the child pornography Prevention
Act and what they said in part was they
were trying to update the current laws
for child pornography and they wrote
that we are going to ban a range of
sexual explicit images sometimes called
virtual porn that appear to depict
minors but were produced by means other
than using real children such as the use
such as using youthful adults or
computer imaging technology so forget
about youthful adults that we don't care
about that it's this part we care about
so what they said is that
computer-generated images Shrek The
Incredibles that kind of stuff is
illegal even though no child was
involved in the creation no actual child
is depicted we're going to make that
image illegal in addition to child
pornography interesting idea by the way
I'm sort of on the fence of whether this
is a good idea or not the free speech
coalition sued by the way I love this
the free speech coalition anybody know
what that is it's the porn industry it's
like this big conglomerate of all this
porn industry they create this thing
called the free speech coalition which I
love I love that I think larry flynt is
in there somewhere I'm sure or was so
they sued and thus US Supreme Court
heard the case in 02 and Ashcroft at the
time argued the following well justice
argued the reason we are making computer
graphics images illegal is that we
believe that if somebody looks at that
image they are going to commit a
subsequent criminal act ie sexual abuse
and the supreme court said virtual child
porn is not intrinsically related to the
abuse of children the government asserts
that images can lead to actual instances
the Supreme Court said no we're not
buying it and they struck down the CPP a
so now we are in a state of affairs
where virtual child porn is protected by
the First Amendment really interesting
well how does that affect anybody well
people are now this happens routinely
our being icky are being caught with
child pornography they are being
arrested and then their say they're not
real the virtual so what happens while
the burden of proof now shifts the
prosecution has to prove that the images
are real and how do you do that
and the courts have actually also ruled
in fact it was a federal ruling just
four months ago that said not only get
to prove that they're real but we don't
actually believe because CG technology
is so good that people can actually look
at the image and tell them apart so if
you don't have a mechanism by which you
can prove that that image is actually a
real photograph you can't show to the
jury cuz it's prejudicial and now you
got a mess right how do you how do you
how do you prosecute these times so and
this is just one corner of the place
where digital imaging is affecting the
law I mean virtually every crime now has
some kind of digital technology around
it civil cases criminal cases
malpractice cases I do a lot of expert
witness testimony these days and
constantly the courtroom talking about
photographs video images as evidence and
people it is on their mind that you can
manipulate digital media and if that's
on your mind you can raise the specter
of doubt about authenticity and it is
just pervasive in the law okay so it's
really it's sort of everyone I think we
also live in a really fascinating time
where photography is something very
different than it has been in the past
than it was ten years ago and we live in
an age where really you know we don't
really believe the images we see anymore
we know that Photoshop can do things
that they never used to be able to do
and there is this doubt about
authenticity of digital media so as I
said in the beginning of the talk we've
been developing techniques to detect
tampering so let me now just so to give
you a one slide overview of a number of
different techniques we've developed and
then we'll dive into some details of
some cool things well I think they're
cool at least okay so some of the
techniques we've developed are
unbelievably simple and boneheaded so
here's a really simple idea image number
one taken by Canon PSA 510 image number
two taken by an olympus see to 2020 z so
almost every digital camera sadly saves
in jpg format i'm trying to tell people
not to do that but they do and so when
you save an image in the jpeg format you
have some options which is how do you
compress the images in fact you have a
lot of options so the way manufacturers
decide on the actual compression is with
what's called the jpeg quantization
table so in a jpeg quantization table is
eight by eight numbers and there's
actually three of these okay and you
trade them off the smaller the number
the higher the quality of the images but
also the lower compression the higher
the numbers the more compression but the
more artifacts you get and you've got 64
by three degrees of freedom and every
manufacturer does something a little bit
different well not everyone that there's
you know there's some reasonable bounds
so typically these numbers up in the
corner here which correspond to the low
frequencies are a little bit smaller
than these numbers but it turns out
we've been looking at close to 500
cameras now that these are somewhat
unique to each camera there's
equivalence classes and then often
there's several cameras in one but
here's a really simple idea just grab
the quantization table and compare it to
what you think the camera came out of
but here's the really cool part
photoshop's since version 0.1 has not
changed their jpeg quantization table it
is exactly the same and cs3 uses the
same one they have not changed it in the
last is that EWW she's one of my former
students and I remember you doing that
in class too she's blushing okay so so a
really simple forensics too is if
somebody claims and we actually did this
in a case so that's who the police
department claimed that the images came
directly out of the camera grab the
quantization table it was a Photoshop
conization table I'm like well you got a
problem guys because there's no way this
came out of the camera so really simple
idea don't know what they did of course
in Photoshop but just really simple way
to do a sort of a first pass forensics
okay what is the most common type of
manipulations you see in tampering and
probably the most common one is this
cloning right you've got something in
the image and you want to get rid of it
right so for example I want to get rid
of that little driftwood there and so
what do you do wow you grabbed a bunch
of sand from somewhere else and you copy
it okay and you leave the red and the
yellow blocks off so this is actually
the output of the algorithm so we have
we've developed a technique that
actually looks for this type of cloning
and so the idea is really simple right
if you grab one chunk of image and you
copy and paste that you've got these
virtually identical regions in the image
and what's hard about this is a
computational problem you don't know
we're in the image or how big they are
and so the trick of this algorithm ake
this efficient when you got a 12
megapixel image and that's where a lot
of the algorithmic issues came up but
very nice simple technique for detecting
tampering here's another one very common
for manipulation this is the original
image this is the doctor you take this
person you isolate them you rescale them
to make them larger so what happens when
you rescale well you have to create
pixels you have to interpolate so if I
resize if I rotate there's a number of
pixels on the off integer lattice that
have to be interpolated buy photoshop
and that means that there's a subset of
pixels in here that are very
specifically correlated to their
neighbors because of this interpolation
artifacts and we can detect that and we
can detect if something has been resized
resampled skewed or whatever here's
another one that I really like it's
based on color filter array so you know
probably that cameras don't actually
capture well except for the pho be on
camera does not capture full-blown RGB a
capita has a single sensor and it
captures a subset of the pixels you need
and then it does addy mosaicing or
interpolation so in any color image a
subset of the pixels in the red channel
the green channel the blue channel have
been interpolated from the original
Bayer pattern or color filter array and
that means when you introduce something
into an image such as this that does not
have those patterns you can actually
detect deviations of the color filter
array interpolation ok this is a really
cool I'm to it actually exploits the
fact that the optics on cameras are
imperfect we know that there's all kinds
of optical aberrations in an image the
type of thing you will often see is this
some chromatic aberration which is a
slight splitting of the red and green
blue channels relative to each other
because of Snell's law and so you get
this sort of color aberration and so
then the idea is that you can actually
model these aberrations it turns out you
can model them very nicely with a low
order model and you can look for
consistency's or inconsistencies it
turns out we can start doing ballistics
with this too because each camera does
something slightly different in their
lens train to minimize our aberrations
and so they manifest themselves very
differently ok so lighting in shadow
this is my favorite forgery so this
is of course Brad Pitt and Angelina
Jolie when they were still rumored to
have a relationship and everybody was
desperate to get an image they offered
half a million dollars for an image of
them together you believe that that's
amazing so this image is really
interesting so let's take a look at it
so here they are outside on the beach
presumably walking together and let's
look at Angelina Jolie so she's got a
shadow here on her feet she's gotta drop
shadow on her chin and her face is
equally illuminated and you see a slight
lighting gradient her leg so all
evidence suggests that the Sun is
somewhere in this direction over here
right okay so let's think about that for
a second so son is here now let's do the
Brad Pitt's face all right we got a
problem right because for Brad Pitt the
Sun is what over here I mean this is
like 210 degrees off all right this is
amazing but the best part about this is
you don't notice like when you just put
it up like that your brain doesn't
really care right doesn't seem to really
care doesn't bother you does it bother
you really when I saw this it was not at
all obvious to me I I just didn't by the
way there's their psychological that
maybe you have a better brain that I do
there's evidence in the psychology
literature that in fact the human brain
is largely insensitive to these types of
manipulations it doesn't care about
inconsistent shadows inconsistent
lighting it seems to basically ignore
them they I think they painted a shadow
I actually think they drew it in because
they knew it was wrong yes I saw that
I'm really bad at if I think I have seen
that like if the light is here where
with the shadow be right yeah there's
actually competition on the web right
now you can win a house in London by
figuring out where a shadow from a
telephone pole is so some people are
having trouble selling their house so
they did is they took a picture of the
house there's a telephone pole in the
front and the shadow cast by the
telephone pole and they removed the
shadow and they said guess where the
shadow is and you win the house and it
costs like 60 bucks to get in or a
hundred bucks at 60 pounds and they're
limiting it to 25,000 people they're
gonna raise something like three point
six million dollars this is brilliant I
love this idea and if you can figure out
where the shadow is maybe you should go
you'll probably be good at it it's 60 a
summer house in the UK not bad so we've
been developed so here
it is obvious right and especially after
I pointed out it's obvious but often
it's actually much more subtle than this
and we've actually been developing
techniques that can estimate where the
illuminating light source in the world
is relative and this is one of the
techniques that I'll talk about in a
little bit of detail this is actually
the photograph of these TV hosts from
American Idol it was actually given to
this image was given doing about the AP
and I spend a lot of time talking with
the folks at AP because you know I got
ten thousand images coming in a day and
often what happens is there's a few
images they're a little worried about so
they send me images and we get to play
with them in a forensic setting and they
were a little worried about this one
does this one bugging you too yeah
doesn't it this one that she's not very
good fox news actually made this and
they gave it to the AP what a surprise
so yeah when we saw it it was bugging us
to it's something something's clearly
wrong but here's a really cool thing so
this is actually a blow-up of their eyes
this guy this guy and her and look at
the little white specularity 'he's which
of course our reflection of the light
source that's in the world so these two
have classic studio lighting to diffuse
light sources like this and now you see
something very different here in here so
the eyes are completely fantastic right
because they reflect the world that that
person was looking at when they were
photographed and in high resolution
images not only can you actually
estimate where the light was from that
specularity and i'll talk a little bit
about that you can add you the high
enough resolution image you can actually
recover an image of what they were
looking at that's very cool actually I
won't talk about that however okay so
okay so you got that sort of a flavor
for the types of stuff we do we go we do
everything from sort of pixels
statistical type of stuff to more
geometric lighting and optical and
there's sort of a whole bunch of
techniques and the general philosophy is
none of these techniques is perfect they
all have countermeasures they will only
be applicable in certain times but the
idea is that as you begin to create a
web of these techniques each one makes
it harder and harder and harder to
create a forgery because you've got to
sort of make sure you don't disturb any
of these things and there's more I'm not
actually giving all of them and we
continue to develop new one so let's
talk about the lighting stuff because I
think that one's really cool all right
so we need to start by making some
simplifying assumptions and we're going
to try to relax them
in a minute so here's what I'm going to
start off by assuming I'm going to start
off by assuming that my scene is
illuminated by a single light source
that it is infinitely far away and that
it's a point light source so think the
Sun basically okay and I'm going to
assume that my surface that's being
illuminated is Lamberson and has
constant reflectance so by lamberson I
mean that the amount of light that's
reflected in all directions is the same
so it's not like something shiny and by
constant flood since I mean it has the
same color everywhere okay so very very
stringent constraints right on both the
surface and on the light source well
we're going to relax a lot of it but
let's just start there okay and by the
way classic assumptions and graphics and
vision and nothing's shocking here so
with those assumptions i can write the
intensity which is I at a point on my
object right there as the following it
is a product of the reflectance function
its color a dot product between the
surface normal and the light direction
and an ambient term so let's do this one
first so at this point think about a
sphere you've got a surface normal which
is perpendicular to the to the sphere
you've got a vector that points towards
the light source and I'm going to take
an inner product between those and
remember that the inner product is
proportional to the cosine of the angle
between them so all this is saying is
that if the surface is facing the light
source it will be very bright and it
will fall off as a cosine as you go away
from that that's it okay and you see
that here bright here towards the light
falls off and of course the ambient is
just for all the sort of reflected light
that comes back and hits anyway okay so
in general what we will have here is red
is is it's things that we're going to be
we want to measure that are going to be
knowns blue are things that we want to
be able to estimate their unknowns okay
so this vector is a three vector of
course because we're in 3-space this
vectors at three vector the direction to
the light source so this thing's
annoying are like how the hell am I ever
gonna figure that out who this is being
published all right I will turn out to
swear you know the video where I just
made fun of yous gonna be on the web so
you may want to turn the phone off
I'm so sorry here so this R is annoying
because I'm never gonna be able to
estimate this so but it turns out we can
get rid of it and here's how I don't
actually care about the magnitude of
this vector what I care about is the
direction so what I'm going to do is I'm
going to say that i'm going to write up
proportionality instead of an aquatic
wyverns and say i'm only going to
estimate this up to an unknown scale
factor okay and then the reflectance
term goes away all right now here's the
good news it's linear if we it's linear
in the unknowns right these are just a
bunch of the values we know we don't
know these we don't know this its linear
that's great that's the good news the
bad news is that i have to come up with
3d surface normals from a single image
really bad news because i'm not gonna be
able to do that unless the world is
peppered with Lamberson constant
reflectance spheres everywhere which I
would love to make my job a lot easier
but okay it's not going to happen okay
so these guys knew Lisa that learn very
clever group I know one published this
really nice paper so let me describe it
what they said is well basically what I
just said and then they said well we're
not going to be able to get a 3d surface
normal from a single image so what do we
do what we do is we make the following
observation on the occluding contour of
the sphere right around here the Z
component of the surface normal is 0
while in the proper coordinate system so
assume that this is the Z coordinate of
my world coordinate system well then
this point here at the including
boundary has a zero for that term great
okay so let's look at our equation well
what happens is if this is 0 here this
doesn't matter it doesn't contribute
anymore so what I now reduce my equation
2 is something that is still linear in
my unknowns now 3 and I can estimate NX
and NY I can estimate the x and the y
component of this because it's just
right I got the including boundary I fit
a little patch there and I take the
orthogonal thing so I can estimate 2d
surface normals that's easy so the good
news now is linear I can estimate these
two things in an actual image the bad
news is I lost something I lost one
degree of freedom so what we're going to
be able to do with this technique is
tell you that the light is here but I
won't be able to tell you along which
arc in this direction right there's a
there's a degree of ambiguity okay so we
can do this but we can't do this and the
second technique I'll tell you is how do
you get this how do you get the Z
component right so let's start with
these two and then we'll worry about the
third component later all right million
Eklund we didn't do this is in our work
this is still million Eklund so how do
you do this well its linear there's not
hard so here's your unknown two
components of the light direction in the
ambient term here's your surface normal
so we're going to do this times this is
that so that's the equation I had in the
previous slide all right well I can
estimate those surface normals at a
whole bunch of places around my object
right and as long as I come up with
three linearly independent versions of
these I can solve this right so we do is
you walk around your including boundary
you grab a bunch of surface normals you
grab the corresponding intensity you set
up a system of linear equations
quadratic error function differentiates
that equal to zero solve and you've got
your classic least squares estimation
okay so all you gotta do is invert this
matrix and you're done right closed-form
analytic solution fast simple and
remarkably effective yes that's a good
question you really have to finish it
and it's a good question how reliable is
the intensity measurement at the
occluding boundary right so there's
something I its a cheat I put it under
the rug so something is weird here I
don't actually know the intensity of the
including boundary do I because it's
it's actually on top I don't see it okay
so here's what we've discovered it turns
out you it turns out you can just grab
the intensity right here just along the
inside boundary and it actually seems to
be work fine and what's amazing is if
you look at it you can sort of see why
right i mean imagine this image grabbing
just this inside its black here right
here where is it brightest that's the
direction to the light source and that's
actually why it works and what we're
gonna do is we're gonna actually add
some regularization to stabilize this
estimation I'll show you that in a
second but it's surprisingly stable I
suppose it depends on your skin if it's
shiny skin we have trouble so we often
we did a forgery we did an examination
of somebody recently it was a basketball
player who was really sweaty and
actually we had a lot of trouble with it
but we've also done a lot of
friends exams people's faces and it
actually works just fine we don't have
trouble with it did you have a question
otherwise the normal of the edge would
be anything at all ah yeah so we are
going to assume it doesn't have to be
closed and what all we need actually
need is that locally it'sit's actually
continuous so that we can actually
differentiate it and get the surface
normal out normal has nothing to do with
the shape I didn't get that last part
the last problem incoherent but sorry
that's why yes you have unclothed
surface then you can make any normal oh
because the including contour will be I
yeah you're absolutely right you're
right that's right we had maybe i should
say maybe we want to assume like some
kind of convexity to sort of make it
clean or something like that right but
for example for typical like a person
standing here right I can take the
occluding contour of the arms legs the
torso I'm in the face anything like that
right all of those things work just fine
right as long as I can get 2d surface
normals off of the off of this occlusion
it's fine
the other side damn it hi yes there is
there's actually non-linearity in there
so the problem is this is a cosine and
it can go negative and there actually is
a non-linearity there and we have a
cheat to fix that so we don't have to
worry about it and I can show you the
paper for how we do it it's actually not
hard basically if you think about only
the hemisphere that's facing towards the
light then you've got a linearity if you
just essentially ignore this part then
you don't to worry about it but yes
there is a non-linearity in there yes
because
wait are you saying that you'll have
higher error when the say it again when
the direction yeah yeah what you mean
the actual light is not infinitely far
away it's actually close now ok mission
using the pixels
no I don't think so and here's why I
think you're thinking about derivative
in the wrong space we're not taking
luminance derivatives we're taking state
we're taking derivatives of the shape
here so if i want the surface normal
right what i do is i take an x and y
derivative at this point we don't even
really take derivative we do is we fit
we get famous on the staging a little
issue experience at this name so you're
you're worried about our ability to
estimate surface normals at this point
is that what your concern is yeah such
that your measurements of all right I
don't actually think that's the case
about but let's talk about it afterwards
cuz I may be misunderstanding you okay
so system of linear equations you can
solve an analytic form and you're done
and that basically was new lease
necklace paper and it is surprisingly
effective even when you don't have an
infinite light source even when it's not
a point light source and even when you
don't have Lamberson were in constant
reflectance it actually works
surprisingly well not great but good so
here's a couple things we did to try to
help it along and this is now where our
work came in so one thing that was
bugging us was this constant reflectance
over the entire surface because that's a
really strong constraint right the
entire surface the interesting has to
have exactly the same color and we
wanted to replace it with a local
assumption of reflectance that is
locally over a little patch that the
color doesn't change but it may change
as you move along the contour so what
we're going to do in the in the original
version here's what you had you've got
surface normals along your occluding
contour you've got a single directional
light source and you just do a single
estimate right so what we're going to do
is we're going to assume that the
reflectance over this little green area
here is constant still membership and
but it could be different than this
little patch right here and notice that
my light direction vectors now are both
pointing in the same direction because
the light source is infinitely far away
but their magnitudes are a little bit
different this one's a little bit
shorter than this one but why is that
well remember that the reflectance term
folds into the magnitude of the light
source so it's possible that even if the
light is in the same direction these
guys will change magnitude
such a embodying the difference in
reflectance so how do you set this up
well it actually surprisingly easy
you've now got a series of individual
light source estimates for each little
patch each one has a block matrix here
with the surface normals over that patch
and you have the intensities that those
things so you can just set up a bigger
system of linear equations and solve it
and fine but this is not a good idea and
the reason is let me back up one slide
is that we are essentially treating this
estimate in this estimate is completely
independent right I mean if you go to
this slide we're estimating each
component of the light source assuming
that they have nothing to do with each
other and of course they have something
to do with each other they're all
pointing in the same direction so we
should we should we should actually
condition it so that these things
roughly have the same direction in fact
if you just use this you get actually
very unstable results and the reason is
that these block matrices are almost
each rank deficient because you're only
over this little patch with the surface
normals don't change very much so here's
what we do here's our least squares
estimator from the previous slide and we
add a regularization a smoothness
whatever term you like to call and what
it is is it says we're going to penalize
the difference between the neighboring
light source directions okay so li li
plus minus 1 i'll i minus 2 and so on
and so forth and this is some parameter
that we can adjust and the good news is
this stays linear so we can write this
part here as this matrix times the
unknown and the whole thing still has
the least square solution where that's a
pseudo inverse so still has a clean
analytic solution and we can actually
relax the global assumption for the
local assumption of constant reflectance
and this helps a lot in a forensic
setting and the other thing we've done
which I won't describe is we've made
this work for a local light source so
not just the Sun but actually if you're
in the room and there's a spotlight
we've actually actually been able to
make this work for local light sources a
little tricky but it actually seems to
work okay so let me just show you how
effective this is so we wanted to test
this outdoors because that's basically
where we have the best approximation to
the infinite light source single light
source and I had this really cool idea
of using a gps-enabled camera and you
know all this sort of triangulation to
figure out where the Sun was on any
given moment give relative your position
and I
to my student chemo Johnson and he
looked at me like i was nuts and he
can't he literally left my office came
back like 20 minutes later and in his
hand he was holding a piece of cardboard
and a stick man if it's probably you'd
be embarrassed and proud at the same
time and so what we did is we put a
sundial in the world and the shadow cast
by the stick on the piece of cardboard
tells us where the Sun is it's really
smart isn't it I was pretty impressed so
we've got you know reasonable
calibration as to where the Sun is I
mean within maybe a degree or two so
here are the surfaces we manually select
them and this is where your
non-linearity comes in what we do is you
have to know roughly where the light is
within a hemisphere and you pick
surfaces that are basically roughly in
that direction we've actually been able
to relax that we have a paper we just
submitted that actually removes all of
these assumptions that you have to make
and actually deals with it I'm in a much
more elegant way than this this hack but
this is how we do it is how we did it
originally these white ones are the
individual estimates and then the blue
after the regularization the whole
minimization in the blue is simply the
vector average of all of those and this
is one of the best situations you've got
these you know lamberson pipes with you
know lots of surface and almost a
beautiful example it's about two degrees
error here's another example this is
about the average about 8 degrees so
this is typically what we find is
averages between eight and 12 degrees
are roughly what we can estimate on the
light source direction remember by the
way we'll only get two components of the
light source right we only literally
this is the estimate right here it's in
plain we don't know anything about the Z
yet okay and this is the worst case
scenario it's these parking meters that
are slightly specular and we grabbed a
bunch of surface that is actually almost
orthogonal to the light source so you
get slight instabilities there because
of the direction because that
non-linearity and so they hear the error
this is one of the worst cases is about
12 or 13 degrees okay so here's an
example we're going to use that real
image we don't I mean we know this is
the 4g but we don't know where the
lights were but here we used on her arm
her back here and for him we used his
back
and his arm for the surface normals and
we get a difference of about 40 degrees
and we actually played with different
parts of the image and pretty
consistently within 10 degrees or so we
get these two measurements so when the
light is wrong by 40 degrees or so you
know even though when it's visually not
very obviously by the way this was a
really cruddy jpg image so this is
actually works on pretty cruddy images
and I think the reason by the way is
that your only you're looking for the
bright spot basically I mean that's
essentially what it comes down to it's
just not that complicated just you know
the formulation is a little involved but
but it turns out actually not to be a
hard estimate to make all right okay so
how do we get that third component the Z
well the problem is that you where do
you get 3d surface normals from I mean
that's that's essentially what the
question is and that turned out to be
hard so this is where this idea of the
using the specularity came in so you'll
remember this example and now so here's
the idea the eye is lovely it's
basically a sphere or attached two
spheres embedded in one another and so
the idea we had is that if we could if
we could if we could see the image of
the light in the eye and we could
localize it and if we can get a 3d model
of the eye we would have 3d surface
normals and if we had 3d surface normals
we should be able to figure out where
the light is in three space okay so
let's talk about how to do that so
here's the basic idea here's obviously
simulated I there's a little white
specularity right there and here's the
basic imaging geometry you've got a
camera you've got a single light source
you've got a point on the eye and you
got the surface normal at that point
this is all in three space i'm showing
you a section of course and in a perfect
reflective surface the angle between the
light and the surface normal theta i
will be the same as the angle between
the surface normal and the camera okay
and sorry what I mean by that is that
the angle at which you will see the
specularity is at this orientation for a
perfect reflective surface and obviously
for a non-perfect you got a little
sloppy so what we know then is that
there's a relationship between Al the
rection to the light source which is
what we want the surface normal which we
can hopefully estimate the view
direction which hopefully we can
estimate and that relationship was a
little bit of algebra is basically what
you see on the bottom so you can see
that if I know V and I know n it's
trivial to solve for the directional
light source inner product in a product
a little bit of arithmetic the whole
thing is over okay so the question is
how do you get n the 3d surface normal
and how do you figure out the direction
between what the eye is what essentially
what direction the camera is looking at
the eye is looking at okay so let's talk
about the 3d surface normal first
there's get you quite a bit of
literature out there about modeling eyes
so here's a basic cross section cross
section of the eye that you see here
turns out this is not our construction
there's these nice references for how
you you can model this you can basically
model an eye of an adult human with to
embedded sphere so again showing you a
cross-section these are actually spheres
so here's one sphere in dark gray one
sphere and light gray they are offset by
distance D the big sphere has radius r1
the little sphere has radius r2 and so
three degrees of freedom for your model
turns out human adult's with normalize
basically within a very small error can
be modeled with these coefficients women
are about one percent smaller than men
in terms of the these dimensions here
very small difference yes oh god damn it
yeah that's hard you know what I went so
when somebody asked me there's I said we
hadn't done the experiment I thought
it's not going to matter because the
contact is right up against it it's a
disaster it really matters but it's not
it's what really matters is the position
of the specularity it actually changes
it so it doesn't actually change
somewhat we don't know how much it
changes the geometry but it moves that
position of that little guy and glasses
do it even worse so at she's a problem
with your contacts and lens and glasses
and I don't have and then we don't know
right glasses with ZZ but I don't use it
for glasses we don't we're actually
doing the experiments with context now
to see what the sensitivity is so but it
does have an impact it's a great
question all right you done or you could
say I'm you know all right
oh man I see this talk at pho beyond
they weren't as smart as you guys so
you'll be happy to hear sorry give
me that convert that back consent form
back I'm gonna stay on my stance script
alright so what's hard about this well
here's one of the things that's hard so
you have an idea you get 3d surface
normals if you can model this with two
spheres of course then you can get 3d
surface normals that's not hard but
here's the thing that's gonna be a
little tricky the 3d surface normals are
with respect to the eye right so that's
in the world coordinate system and I
haven't told you how we're going to do
it but if we get a view direction it's
at you can be with respect to the camera
coordinate system and of course that
doesn't work right so now you've got
these vectors and completely different
coordinate systems and you want to of
course combine them and so now you got
another problem which is not only to
estimate this then you have to have to
make this which having shown you how to
do yet you then have to relate them in
the same coordinate system so that's a
whole nother problem okay so chemo
Johnson who did all of this work is
really deserves the credit for this
because he basically figured out how to
do this and it was it turned out to be a
really hard problem and you can dig up
the paper on our web page we just
published it but here's basically how it
works i'm not going to go through the
details of it because it's you know
would take about two hours or so so
you've got a coordinate system in the
world and you've got a coordinate system
in the image and you want to relate the
two so it's a classic camera calibration
problem and you can write that is the
following its this matrix H which is a
product of an intrinsic matrix which has
things like focal length scale factors
and all that and the next trinsic which
relates the rigid body transformation
between the two coordinate systems okay
so that's what we want to estimate and
here's just to give you a top-level view
of how this is going to work how we do
it we know that the limbus here should
be just about a perfect circle circle so
imagine somebody is sort of looking in
this direction and the camera is over
there what's going to happen is that
that limbus is going to be imaged as an
ellipse and the deviation of that
ellipse from a circle
is basically tells you where is that
camera over the view direction that's
basically the idea so we do is we set up
unfortunate nonlinear minimization
that's this right here which says here's
the the points on the limit on the image
limbus that's the little X i we know
that if you apply some transformation
matrix H to a perfect circle in the
world coordinate system you should get
to that tell me what that is and that
turns out to be a nonlinear minimization
yes you want me to go back to the model
don't you yeah so there's okay so yeah
so it turns out that what I said and the
slide is a bit of a lie yeah II have to
deal with the fact that there's a3d
there and that actually is in the paper
but it just tends to be really hairy to
explain which is why I sort of brush it
under the rug but yes you have to
actually worry about that and we
actually deal with it okay i know that's
a bit of a cop-out but so this turns out
to be a nonlinear minimization this is
that te but it turns out to converge
very quickly because we can compute
derivatives and we can do a conjugate
gradient descent it's actually pretty
efficient so there's actually four
iterations of the minimization and very
quickly converges and seems to be very
very stable actually we've not had
problems with this although I thought we
would okay so that means we can get the
view direction the calibration and the
3d surface normals and of course I
haven't given you all the details but
you know there's there's there's a lot
of issues and the details of that but
you know you can look at the paper for
that this is our little slightly creepy
simulated world we have a camera with
one light source that's always affixed
to the camera and another light source
that moves between these four positions
we have eyes in one of four spatial
locations against the wall and in one of
three locations in Z so we can actually
play with where the eyes are of course
we have perfect knowledge of everything
in this next little simulated world and
let me just give you an idea of how
accurate this is in simulation so here's
a couple of examples of the eyes we took
there's always two light sources one
directly on the camera and then one in
one of these other positions so we've
annotated the specularity with
red dots the circle is both sort of
semi-automatically extra the ellipse
rather semi-automatically extracted from
the image and the average error in
estimating the direction to the light
source is only in 3d by the way is only
three degrees and the max error is about
seven degrees and just for point of
reference the radius of the of the eye
here is about 25 pixels so the diameter
is 50 so it's about you know 50 by 50
pixels so reasonably small not tiny but
reasonably small okay so not not
unrealistic okay so this is in
simulation you're getting about three
degrees max there about seven degrees
standard deviation about one now in in
real world so this is a chemo in our lab
he's sitting in various positions
looking in various directions we had two
light sources studio lighting on either
side of him and calibration was only so
so we actually did just measuring tapes
because we couldn't figure out how to
get really accurate so these
measurements are probably off by a few
degrees but in the real images we are
off by about nine degrees worst error
was 16 degrees and here the radius of
the eyes are only 9 pixels so 18 by 18
right pixels these are tiny in the image
and we did this intentionally because in
a forensic setting you've got to worry
about what happens when you've only got
you know 20 by 20 pixels that you're
examining so this is a pretty realistic
setting I think he actually went too
small I think he get it done better but
these errors most of the increase in the
error is due to this resolution
basically nevertheless we can actually
pretty reliably modular a couple of
caveats right contact lenses glasses
things like that but to the extent that
you can localize a light source you can
estimate its 3d location pretty
accurately yes
yeah we're working on that we actually
been thinking about because can you
exploit the symmetries in the face
basically to figure out where that
person he was asking about can you
figure out the view direction from his
his face so for example if you look at
him you can get an idea of where the
camera is we the thing is we don't think
gonna be able to do it very accurately
because we think the symmetries are not
as you can be very reliable but actually
we're working on we're at you working on
another idea that in addition to getting
a 3d model for the eyes getting 3d
models for faces faces are actually not
that different and we back to you
there's a really nice database of 3d
laser scanners at people faces and we're
trying to find places on people's faces
that are largely consistent things like
the forehead the cheeks you know things
like this and be able to extract surface
normals from that and 3d surface normals
men do measurements in that because
obviously this is a very specialized
technique right it's when you can see
the specularity in the eye okay so we'd
like to be able to do that and other
settings and we're working on that right
now the other thing we've done we've
just submitted a paper on this is taking
the original idea that I showed you with
the light source direction and
generalizing it to complex lighting
environment so in an environment like
this if I ask you what is the direction
to the light source it's it's sort of a
silly question right there is no light
source and yet I am being illuminated so
it's obviously a far more complicated
lighting environment than light source
and ambient term so what do you do with
those environments and we actually have
a paper we just submitted on essentially
estimating environment Maps this
actually comes from computer graphics
technology so the idea is that imagine
you take a sphere Lamberson sphere and
you put in the middle of the room and
you essentially ask well what does that
sphere look like you know what's the
color of it what's the shading on it and
how do you estimate the parameters of
that and what we do is we use spherical
harmonics to estimate the the lighting
environment for complex lighting so now
we can do lighting forensics and a far
more general setting we don't have to
rely on a single light source being
present oh yeah these are the results
for the American Idol so we estimate the
direction to the light source it came
out exactly as you expected this is
actually a hemisphere for each of the
eyes these are little probability
distributions on the sphere showing
where the light source is most likely so
you see these two are basically the same
and actually these two are quite
different this guy was that you probably
taken
flash you notice it's almost direct on
facing the camera and these were
obviously taking with studio lighting up
and off to the sides okay so how we
doing on time five minutes great okay so
I started off by trying to sort of
sketch out the scope of the issue of
tampering it's happening everywhere more
and more and more I mean it's pretty
much now on a daily basis I get an email
from somebody somewhere worrying about
some kind of forensics and it's it's
utterly fascinating in my opinion but
really sort of it's been interesting few
years and we've been thinking a lot
about forensics I've talked mostly about
images and our techniques fall into
several categories statistical geometric
optical and again I think you need to
have a suite of tools no single
technique is going to solve all your
problems we're just getting started with
audio forensics I have a new student way
Hong Wang is doing video forensics and
we're just getting into document
forensic scan documents which of course
have very different type of statistical
properties and images and of course the
question is one of countermeasures and
yes every technique we described has a
countermeasure and those countermeasures
have countermeasures and so this is the
so you know you think about it as spam
and anti spam and virus antivirus this
is an arms race we continue to do this
and in the end what will happen is we'll
make it harder to make forgeries it will
always be possible to create a forgery I
don't have any illusion about that but I
do think we will start to raise the bar
and make it more difficult and there are
papers and source code for a lot of the
things on the web page if you'd like and
we can take a few questions if anybody
has any thanks
okay my question is suppose you you have
a large a lot of images you want to
yourself special gallacher or you know
yeah well so the obvious I mean so you
know people like what is it Getty you
know use the the visible watermark right
so so one approach would be watermarking
right so that's that's sort of the
obvious approach I think the problem
with the watermarking technology is that
all evidence suggests that there are no
secure watermarks you can always rip the
watermark out and that doesn't seem
surprising there's actually some very
nice work by Jessica Friedrich at SUNY
Binghamton who has shown or has begun to
show that images taken from the same
camera have very specific noise patterns
that seemed to be unique to that camera
and I don't mean to make and model I
mean like the serial number of that
camera and that's not surprising because
see CDs and sea monsters are actually
pretty variable and so in some ways her
work suggests that images from the same
camera already have a watermark and it's
in the noise pattern and she's done some
really nice work with ballistics and
forensics using underlying noise pattern
so there you don't to add anything right
you can just do that you said something
about adding a non-linearity or a gamma
we actually have some work and in
showing that when you introduce gamma or
point-wise luminance nonlinearities you
actually introduce them we really
interesting artifacts in the Fourier
space and so that is a possibility we
can talk about afterwards if you like
but I mean the obvious way to protect
images it's typically the invisible
robust watermark it doesn't make it
impossible but it makes it difficult and
then they're actually pretty easy to
search for that's the benefit
yeah methods that you just mentioned
sufficient in five to get recovery of
actual response curves ah no they're
very they're actually very crude we
model the nonlinearities with a simple
gamma a one parameter gamma a 2
parameter sigmoid and the errors are far
too high to do calibration they're good
for forensics but they're not good for
actual you know high quality calibration
know how much our jury's buying it
that's a great question so I I don't
know I've worked a couple dozen cases
probably in the last few years and I've
only lost one they're buying it I mean
they're by both sides of the argument
they're buying that you can tamper with
things but they're also buying the fact
that you know one side of the aisle is
always exaggerating the technology and
you know people are doing things are
saying well here are these images and
their computer generated there's no way
in how their computer generated right we
know that there are limits to cg
technology for example we know that
there are limits stuff it's to photoshop
so photoshop can't take a picture of
somebody in profile and generate an
image like this right now juries don't
know this but if you explain to them why
you can't go from this to this they're
like oh well okay fine right so they are
buying and the courts are buying it I
think they should be suspicious and they
should be slow to buy it though I mean
anytime technology answer is a court of
law we should be very very careful
because the stakes are very very high
but I think there is going to be room
for it and it seems to be happening you
know in cameras take pictures
Thanks a sharpened image and stuff like
that yeah yeah so this is becoming an
increasing concern to us look I repeat
the question so you were asking about
when cameras take a picture they do on
board processing sharpening denoising
I'll cut white balance correction all
kinds of things and we are seeing more
and more cameras doing a lot of work at
the sensor level or on chip level and so
what we are getting is something that is
already heavily heavily processed for
the lighting stuff the nonlinearities we
actually done the sensitivity don't seem
to matter so put some gamma sigmoid
sharpening noise doesn't seem to bother
us other techniques it's actually a
problem so but what we're also seeing is
that cameras are trying to do even more
sophisticated processing so the canon
has a camera now that will take an image
of a person and do what's called the
Photoshop diet which it will thin that
it will it will resize in this dimension
to make the person look thinner this is
just it's amazing um so this is
happening on board right not even in
Photoshop and so so what's coming out of
the camera already has been manipulated
and that is going to make our life very
very difficult so the hope there is that
well k so this thinning will hurt
statistical techniques but probably the
lighting stuff will still work more or
less maybe surface normals will be
disrupted a little bit but you know so
that's the hope of that you just you
sort of build up enough techniques and
so we have you know we have software now
and we have maybe a dozen techniques and
any given image I can maybe run four or
five of them and the other ones just
simply don't apply and that's you know
as you build more techniques you just
have more tools and you're in your tool
kit
stage itself Congress or Ralph it just
averted it alright so the question is
what's easier straight jpg out of the
camera or raw to jpg I don't know the
answer to that my guess is that so the
thing that we let me answer since I
don't know the answer that let me answer
another question the thing we have the
most difficulty with our low-resolution
images something that's been jpg
compressed below fifty percent and has
been rescaled down the 200 x 200 I mean
you know what we're done for right
there's just nothing there so I don't I
suspect there probably won't be a
difference between captured and raw and
save the jpg and JPEG directly in I
don't think they'll be a big difference
yeah well somebody's camera somebody's
doing always expect some photographic
software to have it's so so that the
interpolation I was talking about was
this the color filter array
interpolation so there the jpg is sort
of independent of that right because
what happens is you go from raw a CFA
you create RGB and then you jpg compress
and the hope by the way that the JPEG
compression doesn't destroy the cfa
artifacts right the interpolation so so
that technique is very very good when
you have high quality jpeg or tiff or
raw images but sort of starts to suffer
very quickly when you add a lot of jpeg
and i don't actually think that answered
your question but I didn't really
understand the question fully okay good
eighty percent can can argue that okay
one more then
more than people say in JK what's the
alternative yeah that's a good question
well I think jpeg2000 a little bit
better i think the wavelet compression
is better than the DCT I think what
really makes jpg nasty nasty is that
block dct transform and i think the the
jpeg 2000 with the wavelet transform is
actually better it's got different
artifacts but they're better my
preference is uncompressed if I know
it's not very realistic I only shoot in
raw to be honest I don't shoot in jpeg I
shoot in raw but I think jpeg2000 is
better and I'm actually amazed that it
still doesn't seem to have caught on as
far as I can tell it's just not it's
been around for it it's 2000 right
jpeg2000 so you know seven years later
it doesn't seem to be really catching on
I I think that would have to be an
improvement okay thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>