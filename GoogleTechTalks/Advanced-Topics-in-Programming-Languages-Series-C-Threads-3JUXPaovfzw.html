<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Advanced Topics in Programming Languages Series: C++ Threads | Coder Coacher - Coaching Coders</title><meta content="Advanced Topics in Programming Languages Series: C++ Threads - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Advanced Topics in Programming Languages Series: C++ Threads</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3JUXPaovfzw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi folks welcome to the latest in the
talk series on advanced topics and
programming languages as I start every
single one of these talk series of these
talks in the talk series with I would
like to just make a quick plea for some
for for people who are interested in
various topics in programming languages
to come and give talks I see a lot of
people who have given talks but I see a
lot of people who might know know a lot
about a lot of topics and have not yet
given talks so please come up to me and
tell me I would love to give a talk and
we could set something up today we have
a talk on a topic which should be near
and dear to the heart of every Googler
who writes C++ programming because this
is a this is coming down the pike really
quickly and everybody needs to know
about it it's from Lawrence he's given
several talks on on C++ standards he's
very involved in the C++ standards
process and today he's going to talk to
us about multi-threading in C++ and and
the changes that the standard has in
store for us with respect to it Lawrence
can you hear me good I'm gonna start off
with the introduction but my real reason
for starting with this slide is to let
you know that when one of these types of
slides appears that means I'm about to
make a significant topic shift so if
you've been sort of sitting on a
question one of those topics shifts
would be the time to ask it because I'm
going to move on to something completely
different the first thing I want to talk
about is where this all fits in there's
quite a number of changes to C++ coming
with the next standard and one of the
major goals is to extend C++ into
concurrency right now it's a serial
language and that's no longer acceptable
but an important feature of C++ and one
of the major reasons for its success is
it lives within a computational
environment you can include the system
headers all this kind of thing so C++ is
connected to the rest of the world it's
not an isolated language and we want to
preserve that feature as we move forward
into concurrency and finally C++ gets a
fair bit of its power from being able to
let good library writers write good
libraries and we are going to be unable
to provide the right parallel solution
so what we want to do is make sure that
what we provide enables library writers
to do the right thing for their
application or for their industry and
move forward from there so what I'm
going to show you today is for the most
part a start not an end and for this
talk what I'm trying to do is basically
outline the primary features I can't go
into all the details but I can give you
an overview of where things are heading
and not all of this is stable yet
there's active debate within the
committee about what to do some of the
syntax hasn't been chosen some of the
semantics hasn't been chosen and because
of all that debate I would like to get
from you feedback on
where you think we're doing things right
where you think we're doing things wrong
and so forth finally as this is dealing
with the C++ standard I have a set of
disclaimers here which means basically
that you should take what I say today as
a hint not as an answer so everybody has
heard the story about multi-core
processing and how it's coming and we're
not going to get single cores that are
going to go any faster so all the chips
are being multi-core and we're gonna
have to start dealing with
multi-threading and that's part of the
story of why we need to add threads the
other part of this story is that it's
now a connected world we have the each
computer talks to the internet and some
of the ways you write programming is
better in a concurrent world even if
you're still on one core and so we're
looking at those kinds of issues the
other issue which applies less to
Google's specific applications but does
apply it's in other application domains
is some programs require very large
machine resources you know hundreds or
thousands of processors and those you
can't do without some form of
concurrency so people are getting by
today and what do they get out of a C++
standard on all this stuff why isn't
POSIX good enough well for one thing is
it's not just POSIX it's also Windows
it's also embedded systems it's also
mainframes there's a wide variety of
machines out there and while the model
is dominated by windows and POSIX it's
not solely windows and POSIX so we'd
like to create a portable expression of
programs the other thing that having
this in the C++ standard does is it
creates a bigger community to talk about
parallel computing concurrent
programming and so forth
so some of the solutions that people
create if they create it in a standard
environment it'll spread to a wider
community and
community can work on it and that's a
big leverage on the on the work that all
of us are going to be doing the next big
approach we're taking is to standardize
on the current computing environment so
we're not going to try and invent a
totally new model of parallel processing
in a C++ community the world is littered
with parallel computing languages that
died and so we are going to standardize
on a model that is well supported by the
current operating systems and we have
strong belief that those will be around
for quite a while
so we have one C++ thread will be one
ôs thread it's heavyweight
independently scheduled preemptive etc
so that cost model is exactly the same
as what you're used to with POSIX or
Windows threads
the second major bullet point is its
shared memory the variables of the
threads are all shared among all the
other very all the other threads there's
no isolation of memory and this too is
in common with POSIX and Windows threads
the big point we also need to make is
that we are not trying to compete with
other standards there are standards for
message passing we're not trying to
displace that there's OpenMP which is a
standard for sort of hierarchical loop
based programming we're not trying to
replace that we're not trying to replace
automatic parallelization and compilers
so we're really trying to sort of
provide a standard access to the
environment
and we have a different we have a sort
of a split view of what we're doing on
this and this new is a little fuzzy but
to two sides the change is necessary to
the core language to make this all work
and change is necessary to the standard
library that give you a library
interface on some things now
this separation isn't entirely clean
because sometimes they overlap some
things that look like a library
interface are in fact the core language
change so that it's a bit fuzzy there
but overall you can think of this split
and the core language were really
concentrated on what is memory what are
variables and when two threads touch of
a variable at the same time what happens
what does that mean that's the kind of
core level changes and this core
language change area is where I'm going
to be spending most of this talk for two
reasons one is I know more about it and
the second is that this area of the
language is more baked right now the
second area is in the standard library
and this focuses on how threads are
created and synchronized and how we deal
with what happens when a thread stops
how do we make a thread stop and so fan
layers beyond that
yes
that's I want to know if directly it
reading us drinks and what they is
well the string is an abstraction put
put out by the library so there's
actually a slightly subtle answer here
but it's important the answer is that if
you are talking to the same variable
then you must do the synchronization the
standard library by design is not going
to try and synchronize a variable on the
other hand if as is common with many
implementations of string they have
reference counted objects then the
standard library is ensuring that any
object shared between two variables is
properly synchronized so you are
responsible for the shallow
synchronization on the variable but any
deep synchronization is the
responsibility of the library
I'm sorry the question was whether or
not things like string are synchronized
in the library or not and so the answer
is halfway in between yes the question
was will there be any thread local
storage and I asked you to wait a bit so
my first topic is what is memory the
traditional notion of shared memory that
everybody thinks about is that you write
a value to memory and it's instantly
visible to all the other threads this is
sort of what we thought about in in the
past what people tend to think about
when they think about shared memory but
it doesn't work among other things it
implies faster-than-light communication
and we've pretty much ruled that out
over the last century or so it doesn't
match the current hardware because they
have speed of light limitations as well
and in addition current modern compilers
even in the serial case will apply
optimizations that make that not true so
for all of those reasons you're not
going to get an instant shared memory so
there will be some lag between when you
write a variable in one thread and when
somebody else gets to see it and we're
sort of nominally terminus message
shared memory so
you have two threads and they want to
communicate one thread doing its rights
those rights are sort of held up in
limbo until you write to an atomic
variable or write or acquire a lock and
then those get communicated until
another thread accessing that same place
picks those rights up so two threads are
mediated through special variables it's
detailed I'll get to that a little bit
later so the mechanism here is acquire
and release so a variable when it writes
to a thread when it writes to a variable
says I release all of my rights to this
variable and then somebody coming along
later says okay I want to do an acquire
from that variable and all of the rights
that the one thread did are now visible
in the second thread okay and so we have
a store release and a load acquire which
is sort of the base mechanism of how
this stuff happens so typically you
release things when you do a store and
you acquire things when you do a load
there's also yes
please does that mean the economic
driver has to acquire the signal
personally
I'll get to that in a minute
modern machines also have the notion of
a memory fence and the proposal right
now Foote C++ does not have memory
fences in them and the reason for this
is partly the definitional process
partly that there are some proposals
that are in the early stages we don't
know quite what's going to happen but
the major thing here is to take away is
that the current machines have fences
but the current proposal does not and we
don't know how that's going to settle
out partly we keep talking about them
because in terms of implementation when
we say this c++ language construct
implies these certain barriers on modern
machines these certain fences on modern
machines we're trying to get an estimate
of the cost the the main problem is
there might be we might inhibit future
architectures if they if we put fences
into the language so because we are
communicating through shared memory the
sequencing becomes an issue so within
each thread we have a sequence of
operations and then we write to shared
memory and now we want to ask questions
about how to thread sequence our reads
and writes between threads ordered but
in order to solve that problem we have
to first back up to what happens in the
single thread case and the old model for
sequencing was the sequence points in
the old C and C++ standards and those
are not well-founded
nobody really quite knows what they mean
various other problems there so the
committee has done a significant amount
of work and sequence points are now gone
and there are a couple of ordering
lations the first one in a sequence
before where there's a strong sequencing
between operations
then the next one is indeterminately
sequinsed where there's sort of a week
it has to be either entirely before or
entirely after but I can't tell you
which and the key idea here the key
point in the standard is that if you do
a couple of writes or a read and a write
to a variable and those operations are
not sequenced you now have undefined
behavior okay
so we don't try and make it defined and
it's up to you to do the right thing
you and your compiler vendor if they
give you warnings so but that's
undefined behavior if you don't properly
pay attention to your sequencing so
that's the sequential case now we get
into the parallel case and the main
relationship we have here between
operations is sequenced before and that
comes from the serial case by adding to
the serial case the acquire and release
operations on variables which then get
picked up in another thread so if you
have intraoral you zip over to an atomic
variable pick that up in another thread
and now you have those operations are
now sequenced before each other across
that connection through memory okay and
put all of that together and you have it
happens before a relationship between
two memory operations and different
threads and here's where the hard part
comes in so data races if you have a
right to a regular variable not atomic
but a regular variable and some other
thread either reads or writes to that
same variable without this happens
before relationship between those two
then you have a race condition and your
program
as undefined behavior now I want to be
clear here undefined behavior means yes
you just did order a thousand hogs to be
delivered to your front door on Tuesday
okay really the standard can't say
anything about what's going on because
you're suddenly getting random word
tearing wild pointers all that kind of
stuff so the standard really can't say
what's going on so you get undefined
behavior so the question is won't you
also be preventing useful
non-deterministic programs from
happening the answer is no that's not
true but you can't use regular variables
to make that happen
you have to use atomic variables to make
non-deterministic programs happen in
that last slide I sort of slid by this a
memory location and just waving their
hands like that is not quite good enough
for the standard so we have an actual
concrete definition and that is a non
bit field primitive data object or a
sequence of adjacent bit fields with
some weasel wording so the idea behind
all of this is to avoid having to do
atomic read-modify-write operations to
access bit fields so we're going to
continue with the current compiler
notion of reading read the bit field and
all the bits around it change the part
that has your bit field and write it
back out and if there's any collision
between two threads that's a data race
and you have a problem so if you if your
bit fields really do need to be atomic
then you have to go to extra work to
separate them in memory and for for
people that are working in binary worlds
this will also change the layout of your
structures in memory could increase the
amount of memory
use could change compatibility with
existing libraries so be careful in that
location now this model of memory and
data races and so forth does have an
effect on optimization it's not entirely
free there are some speculative reads
and writes that current optimizing
compilers have been known to do that
will no longer be legal and the reason
is that when you're compiling some
function you have no idea if it's in a
multi-threaded application or not you
might not know that it's in a context
where the code can't be moved around or
the rights can't be dropped and so forth
we believe that those speculative reads
and writes will not be a major importer
impact on program performance we think
it's in the low single digit range but
of course we can't guarantee that for
all programs there is however one case
where we had to have a special rule and
that is the compiler can assume that a
loop will terminate there are a
substantial number of optimizations that
involve pulling code out of loops in
front of the loop or behind the loop and
if the loop never terminates you can't
really do that kind of stuff so we made
the rule that you can assume that the
loops that the compiler can assume the
loops terminate for the most case this
is nearly always true and in the cases
where the loops don't terminate they
typically have shared memory operations
or locks or i/o or some other thing that
would solve the problem as well this
really only affects tight loops that the
compiler can't determine terminate yes
what is a loop that's cyclic control
flow arc yes so the in this case the
compiler just does not have to determine
that it's going to terminate and this
allows a lot of the current
optimizations to continue to go through
so the quote that the question was the
definition of Lu doesn't include control
flow by exceptions and if you create a
loop via control flow that is not
currently covered by the definition
through changing single thread
so the the question was that if you
perform some of these optimizations
you're not going to be chain across
loops you're not going to be changing
single thread semantics and that's
correct
okay so we talked about memory and some
of the interactions there and we said
that we're going to communicate reads
and writes through Atomics and locks and
now we're going to talk about the
Atomics so we have to say what is atomic
and what we have is a set of atomic
operations on a single variable so the
operation on that one variable is atomic
and those operations will appear to
execute sequentially on the view on that
variable is sequential to all threads so
they will see the same order of events
in the system for that one atomic
variable so if thread a writes two
values to an atomic variable thread B
will see those two writes in exactly the
same order now it may not see both of
them because there's a certain
resolution problem but it will it will
appear as those that there's an inherent
sequential order
we do not use the volatile keyword to
indicate this Java does use the volatile
keyword C and C++ had a longer history
than Java and we've been using the
volatile keyword for many years and we
chose not to change its meaning it still
has the old device register meaning that
it's always had and it does not indicate
atomicity but you can have a volatile
atomic variable which says that there
might be some external agent that
changes this variable in addition to
some other thread so atomic is a notion
between threads and volatile is a notion
to the environment we also have a set of
requirements on atomic variables the
first big one is static initialization
if you're using these things to
synchronize threads that may pop into
existence
you don't really want to worry about the
threads starting before you've had a
chance to initialize the atomic variable
so one of our key criteria here was we
want to make sure that atomic variables
are statically initialized we also want
a reasonable implementation on current
and future hardware so we didn't want to
create some type of atomic variable that
was inherently expensive on current
hardware they're all going to be a bit
more expensive than regular variables
but we don't want to have huge costs
we'd also like to enable relative
novices to write working code now I want
to warn you here that relative novices
is a very small group the experts are
the people that publish in principles of
distributed computing and find errors in
other papers on that same journal so
that's where the experts are
around them are people that sort of can
do a lot of it but aren't really experts
and incidentally I fall in that category
so most C++ programmers are not in the
category where we expect them to be
using Atomics our final crate you are
one of our criteria is to enable those
people that write the principles of
distributed computing papers to write
very efficient lock-free code we want to
give them all of the tools to make the
hardware sing because while that kind of
code doesn't have to be written very
often when it does have to be written
it's performance critical and we also
wanted to leverage those very bright
people that know how to do this kind of
stuff
so we want them to be able to write
lock-free libraries and give the rest of
the world higher-level data structures
that do the right thing where the users
of those data structures don't have to
worry about the gory details
yes the question was is the atomic a
property of the variable or the way you
access the variable it's a property the
variable yeah I'll get to that in a
second
so the operations on an atomic variable
have a couple of attributes one is
acquire which we talked about I want to
get other memory rights release I want
to give my memory rights you might also
do both an acquire and a release at the
same time and then finally you want to
do relaxed which is there's no acquire
and no release implied and that's where
you get the non non deterministic
behavior so you're not necessarily
synchronizing any of the rest of memory
but you still do get that sequential
view of that one variable and the other
memory ordering we get in here is fully
ordered where we get extra ordering
semantics beyond what arises from simply
acquire and release the problem in all
of these atomic variables is that too
little ordering on your variables will
break programs but too much ordering
slows the Machine down substantially and
you need to find the happy balance as it
turns out most programmers when they
write this code should be conservative
and put in too much ordering because
then you're more likely to get correct
code and then when you find you have a
measured performance problem you might
want to go back in and relax things a
bit and use weak or ordering constraints
but once you start down that road you
need to take the attitude that you're
writing public code and get lots of code
and so forth to make sure that it's
working and part of the reason for this
is that people tend to think of a world
in a consistent fashion and once you
start getting into weaker memory models
you don't get that consistency and in
particular if you look at this code
example two threads independently write
two independent variables and then two
other threads read those variables and
do they get a consistent view that is is
there a system-wide total store order on
x and y some hardware does provide this
other hardware does not and systems that
don't provide it can be faster but
people have a harder time programming
without it so what have we done what
we've chosen to do is for the fully
ordered atomic operations we have made
them sequentially consistent because
this is where people seem to be most
comfortable in terms of getting correct
code it's a little bit less efficient
slightly weaker models don't seem to
have a good formalism we sort of wave
our hands and can say it's sort of like
this but we don't really have a good
formalism and we're trying to make sure
that the language has a good formalism
because a good formalism helps
programmers and compiler writers resolve
any of their differences and then we
have weaker models that you can get to
through explicit programming
so I'm gonna switch gears a little bit
now that we understand the background
for the Atomics I'm going to sort of
build up what they look like in code yes
yes so if you use the operations that
are syntactically most convenient they
will be ordered now if you start writing
more verbose code you can weaken that
it applies to all variables so if you
are using only the fully ordered
operations you will have a sequentially
consistent view of all the atomic
variables but once you start getting
away from the fully ordered operations
things will get weaker and they won't
look sequentially consistent okay one of
the problems that the C++ language has
to deal with is a wide variety of
hardware and in fact we have some older
hardware and some embedded hardware that
may not have all the full hardware
support that some of us have become
useful used to so the standard is built
around one primitive atomic data type
that you really have to have hardware
support for and this is the atomic flag
and it has test and set and like
semantics once you've got that you can
build up the rest of the environment it
will not necessarily be the fastest
thing you ever saw but it should be
sufficient and at this level of
programming it's really down in the
basics and you you want to stay away
from this as much as possible past that
we have a few basic atomic types a
boolean a set of integers and a void
pointer and you can build these from
that flag that I showed you earlier and
the proposal before the standard shows
how to do that
and but we expect people to go a little
bit further than that but those basic
operations have a set of c-level
operations which look like function
calls as I showed you in some of the
earlier examples and as you can see here
on the screen you can use these same
operations from both C and C++ in C they
look like type generic macros in C++
that look like overloaded
actions but core thing is you can write
the same syntax and the same things will
happen and those operations in crew
include the order and constraints that
we talked about earlier but we also have
a C++ level to the operations and here
those atomic types look like classes and
we have a small number of member
functions and a small number of
operators and those are the fully
ordered operations we talked about
earlier so when you write do an
assignment you'll get an atomic when you
do a compare and swap
that'll be strong when you do a plus
equals operation that increment is
atomic and it's strong that is fully
ordered but there's a problem here and
that is if you look at a C++ class it
comes with sort of this default
assignment operator and that default
assignment operator is wrong for atomic
operations because the compiler is going
to generate a planeload and a plane
store and it won't work so the simple
answer is of course write your own
assignment operator and make that atomic
the problem is that if you write your
own assignment operator under the
current language rules that is no longer
a pod datatype and it's no longer
guaranteed to be compatible with C and
remember one of our goals is to be
compatible with C so we couldn't do that
either and we can't really stop that
assignment operator in C and
particularly C 90 however we've made
substantial progress in the current
language with a bunch of new papers that
allow us to address this problem head-on
and we can from the C++ Committee side
now prohibit the assignment operator
without violating compatibility with
scene and I expect that if the C
committee adopts this work they may well
just prohibit the assignment operator as
well so one of the things I just said
right there is prohibit the assignment
operator and the question is why do that
instead of make the assignment atomic
and the answer is one we didn't want to
interfere with with the Seaview and the
second thing is that people would see
that simple assignment operator and
think that the reading and the writing
together were atomic and given current
Hardware we can't make that happen we
can make the read atomic we can make the
write atomic but we can't do both
together atomically and so we invalidate
the assignment operation so that users
have to explicitly see the read and then
do the write separately question
well I just say no operator
well let me be clear on which assignment
operator I'm talking about when I talk
about assignment this that assignment
operator I was talking about the
assignment from atomic to atomic we
still report the support and assignment
from an integer to an atomic right and
so all of these previous operators that
you saw here they're all taking in
values so three so you can read from a
atomic variable you can write to an
atomic variable with an integer what you
can't do is simultaneously read from
atomic variable and write to an atomic
variable in an atomic assignment
operation
I'm prohibiting and atomic where the
both the left hand and the right hand
are atomic variables it's a question
know that dot all works those are MOT so
basically what we're looking for is
operations that have only one L value
right if the operation has only one L
value then we can make it work the
traditional c plus plus assignment
operator that's automatically created
for you has two l values one on the left
and one on the right and that's what we
can't handle it's one it's one atomic
variable and we can take advantage of so
let me repeat the question doesn't plus
plus a have that problem when we have to
read from it and write to it and the
answer is it's one l value and we can
use fetch and add hardware to make that
operation work there is there is a
change here that's a bit subtle for the
plus equals operation as you would
normally define it in c++ it would
return a reference to its left hand side
for these atomic operations they do not
return a reference because then you
might read from it again and get bad
results so for these atomic operations
what they return is a value which is
surprisingly enough what c does so what
we've kind of sort of come back a little
bit so you had a question
okay the question is is there an
equivalence between a + equals 4 and and
a equals a plus 4 the answer is no the a
equals a plus 4 is two separate atomic
operations there's a read a regular add
on values and then a write so there's
very much a difference between those two
they're there they're different
operations so what question in the back
can can I do a race I'm not sure I
understand the question
oh so can I do a raise
there's no atomic operation over a raise
but you can do an atomic you can have an
array of atomic variables question
all the differences between what is sort
of the expected medicine
did you ever discuss not supporting
operator
so the question is with the differences
between sort of traditional C++ and
semantics here did we ever discuss not
having any operators the answer was yes
but nobody wanted really to do that
particularly among the committee
whenever we wrote code we almost never
wrote out big function calls we did plus
equals and and so for in a lot of cases
we just thought that it would be too
much trouble your point is valid though
so in addition to the C++ view of these
basic types we also have a atomic
template which gives you a uniform way
to name the types so the other types
were atomic underscore int if you wanted
an atomic underscore char there was it
was atomic underscore char and those are
two different names and it really hard
to manage from within templates without
writing a bunch of specializations so
forth so what we have now is an atomic
template you can write atomic left angle
and right angle and you get all those
same semantics and now you can put
Atomics inside of templates and make
those things go forward there are
however some semantic restrictions on
what you can put inside of this atomic
template what types you can put in there
the primary ones being it must be
bitwise copyable because we have to rely
on those semantics with the underlying
hardware and they must be bitwise
comparable the compare-and-swap
operation is a bitwise comparable
operation and that template has
specializations into the basic types and
we also suggest how to implement this so
that there are specializations on to the
hardware for arbitrary types so one of
the things in C++ standard library is
this notion of a pair so if I have a
pair of two ends
I'd like to be able to put that into an
atomic template and have everything work
and with this formalization it will work
so any small type like a gnat you can
wrap with atomic and things should work
out pretty well if however you wrap the
entire circus with an atomic we cannot
implement that in hardware so what we
will doing be doing is putting in some
locks around it and stopping the whole
world while all of these things go on
and so while it works we recommend you
don't do that so use atomic variables
with care the other thing about Atomics
is there are certain properties of
freedom that keep getting labeled with
these Atomics so I want to go through
those because they do affect the
interpretation of C++ the first and
foremost principle is whether or not an
atomic operation is lock-free and this
is important because if it has a lock
and you crash it in the middle of the
optimal crash the program in the middle
of the operation or crash the thread in
the middle of the operation you now have
somebody holding on to a lock and you
could get a chain of failures because
that lock isn't released lock free means
that there is always somebody making
progress even in the presence of crashes
okay the next stronger capacity is
weight free and what that means is that
every operation will complete complete
in a bounded amount of time so you can
get lock free but there may be
arbitrarily long to make some of those
operations finish they will finish but
it may take arbitrary amounts of time
weight free means they won't take
arbitrary amounts of time and address
free means that the atomicity of the
opera' of the operation does not depend
upon the address
okay this is particularly important when
you have two processes sharing memory at
two different addresses so you have
physical memory at two different virtual
address spaces and so this is this is
where that property comes in there are
no support in hardware for lock-free
Atomics on big types so those big types
must necessarily be implemented with
locks and those locks cause problems
when you get to signals so you don't
want your atomic variables to be have
locks in them if you are also using
those variables with a signal because
they'll come in at arbitrary times so we
give a means to be able to test whether
a certain atomic variable is implemented
in a lock free manner or not
so well we can't guarantee that the
atomic variable you want to use to
communicate with your signal is lock
free upfront in the language we can
guarantee that we'll give you that
ability to test it and for and because
certain processors implement lock free
stuff through dynamic libraries that are
dependent upon the individual processor
we can't even know at compile time
whether or not certain types are going
to be lock free so it's a dynamic test
for that we also have pre processor
based static tests that will tell you
whether or not a the basic types are
always lock free or never lock free and
if they're if they're not always lock
free and not never like free you have to
fall back to the dynamic tests
the second layer of freedom that I
talked about was weight free and again
you need hardware support to make these
things work but that support is
substantially less common and when
people write this kind of code they end
up writing fairly processor specific
code and it's difficult to write
portable programs in this realm we also
found that few of the people that were
wanting to write with these atomic
variables cared some did but most didn't
so we're going to leave this property
unspecified in the standard we won't say
whether or not it's it's it's weight
free and the third freedom there was
address free Atomics inherent in this is
that there are two different addresses
for the same variable and that's outside
of the C++ standard C++ standard has no
way to say that but we recognize that
people in fact do write C++ code that
shares memory between processes and does
so at different addresses so we're gonna
try and help out here what we're going
to say is if an operation is lock-free
it must also be address free that's the
intent that we will put into the
standard we can't make it a firm
requirement in the standard because we
have no way within the standard to test
that but that's what we're going to say
we don't believe that that'll be a
problem for any of the C++ compiler and
system vendors but that should then give
people writing multi-process programs a
leg up and it should also help people
that are mapping the same file into two
different places in the virtual address
space a leg up
yes the question was have we thought
about incorporating transactional memory
the answer is yes the committee has
thought about that to the extent that
we're pretty much agreed that while it's
an interesting area of research it's
still an area of research not yet stable
enough for us to put into a C++ language
standard yeah yeah so I want to I want
to point that out some people are
starting to get nervous that I'm not
wrapping up this is a 90 minute talk so
if it appears I'm not wrapping up its
there it's that that's there for a
reason
entire entire structure classroom entire
structure and you could easily imagine
that being released
the actual mechanism
right but the the atomicity we have over
entire striked is basically to load and
store it and you can do a compare and
swap they're very limited operations yes
so the next thing is that we have these
variables and in this multi-threaded
world what happens to the variables
we've got three major areas the first is
thread-local storage and question
earlier we are introducing thread-local
storage we also have to address the
dynamic initialization of static
duration variables ie global variables
and function local statics and we also
have to address the destruction of those
variables the first thing we're going to
do probably the easiest thing is adopt
thread-local storage there's lots of
existing practice for this already for
pod structures simple C types and we do
this the same way those compilers did we
introduced a new storage duration and a
new storage class keyword each variable
is unique to its thread each but each
variable is at is accessible from all
the other threads so if you take the
address of your thread local storage and
you pass that address to some other
thread it can read and write to your
variable and a consequence of this is
addresses of thread-local storage are
not constant those are created for each
thread
we are also extending this to allow
dynamic initialization and destructors
for variables in thread-local storage
and we've defined this very carefully to
permit dynamic or lazy initialization of
these variables because you what you
don't want to have to happen is when you
start up a thread it to have to
instantly run around
initializing thousands of thread local
variables that it may never reference in
the entire lifetime of the thread
so we're very much trying to be able to
set it up so that you only have to
initialize the variables at the
intersection of the set of variables in
the set of threads so we want a sparse
use of memory and initialization the
question it was do you initialize it
when you take the address of it well the
what the standard says is that it will
be initialized before first use of that
variable and so it gives the compiler a
little freedom to push things a little
further ahead
so what's permitted for instance is if
you use a thread local variable in the
middle of a loop it might move that
initialization out of the loop and do it
at the top of the function so so there's
there's a little leeway in where the
compiler vendors can can start that
initialization we can we can sort of do
this all in the compiler but OS support
would provide us a little more
efficiency if I have a global variable
that requires dynamical initialization
say it has a constructor this gets
tricky because what if I have two
threads running and they both want to
see this thing initialized or they both
try to initialize that without
synchronization we potentially have data
races with synchronization we
potentially have deadlock
and so we're going to break this apart
into two problems one is for function
local static variables and one is for
the namespace type variables and the
reason is function locals already have
this notion of lazy initialization and
we're going to capitalize on that the
first thing we're changing is that the
current standard of course doesn't say
anything about synchronizing these
initializations we have decided to
standardize on making sure that those
things are synchronized so two threads
who decide to initialize the same thread
local variable thread local static will
be synchronized
but while the initializer is running
they will not be holding a lock okay you
may still other threads may be blocked
waiting for that to happen but there
will be no new lock introduced and held
for that duration and this has made
possible for by an new algorithm
developed by Mike burrows here at Google
and Google has released this code into
the wild under a under a friendly
license and basically what this
algorithm buys us is we get the on
average a cost of one additional member
non-atomic memory load to make all of
this work it's it's really a quite
subtle algorithm for non-local Static
duration variables that is say global
variables we've gotten a little bit
tricky here the current language doesn't
let you gives you sort of in
deterministic access to variables that
are defined in another translation unit
so the only thing that's that's sure is
what you access within your own
translation unit what we have done is
change the current definition where it's
either zero initialized or fully
initialized - you don't know what you
get so what was sort
indeterminate is now undefined so all
you can really rely on while you're
initializing the global variables in
your translation unit is other variables
in your translation unit what that
allows us to do is concurrently
initialize the variables in two
different translation units because as
it turns out concurrent initialization
and some applications will help
substantially
so the question is whether or not vector
relies on variables from some other
translation unit and the answer is the
implementer of vector has to make sure
that they don't do that so it yeah so
it'll have to be part of the contract
between you and your library vendor
about what they do in terms of their
implementation
destruction has much the same property
so we also say that when you destruct
you can only rely on those same set of
variables the the extra complexity here
is what happens to function local
variables that were initialized while
initializing a global variable and the
answer to that is we interleave them
properly and save it up for destruction
the question is are the Constructors run
in the same threads as are the
destructors run the same threads as the
Constructors and the answer is you don't
know okay and I'll get to that reason in
a second when I when I get to
termination yes
now
because if anyone's taking these threads
division very much understand
and they depend on that thing
right so this model implies that you
aren't going to be storing addresses and
global objects that cross those module
boundaries so we had a lot of discussion
over over this and this is sort of the
working compromise between basically
forcing sequential initialization of all
of the variables which means that when
your application starts up you have this
long period of being able to only
exploit one thread versus basically
being eliminating global variables
because of the synchronization problems
so it's it's not an ideal compromise I'd
be really happy to sit down with people
and if there's a better idea or you
think that this isn't going to work
please sit down with me we'll go through
it and try and make sure what happens
because we certainly don't want to
release a standard that can't work but
we understand that what happened this
model here will invalidate some current
codes we don't think it'll be too hard
because what we've made undefined was
previously indeterminate
and also common causes mysterious bugs
so now we're going to move into the sort
of a library phase and this should go
quicker the model that we have for
threads is a fork and a joint and you
fork a function to be executed and then
later you come through and join and
because of the magic of c++ operator / n
/ n we can make at make the joint looks
sort of like a function call this notion
is very standard about what's from
what's being supported in the current
operating system world that view is
common we're going to stick to it we are
also going to support functor like
objects so that you aren't totally stuck
trying to communicate through a narrow
POSIX only passing in your function
pointer type of interface talk to me so
the question is why are those extra
parentheses there talk to me afterwards
and I'll explain to you a business Ben's
arnis and C++ syntax those extra
parentheses have nothing to do with
threads the next thing is the scheduling
of these threads as it turns out
scheduling is a very touchy issue and
things are dicey so we are probably in
the stander you're going to only supply
two mechanisms for scheduling threads
one is a yield which says now would be a
good time to switch to somebody else and
the other is asleep which is I want to
go away for a while
those are adequate for a lot of
applications they won't be adequate for
all applications for those other
applications we're going to give you a
handle on the operating system thread so
you can ask us for your platform
specific handle to a thread and then you
can go off and do communicate with your
operating system detailed scheduling
issues we will also have
a query to allow you to find out what
the hardware concurrency is on your
system so this sort of gives you a
measure of maybe what's the most number
of threads you should fork off to be to
be helpful it's a very vague number
because hardware is actually quite
different for synchronization the base
concept we have is a mutex which is
similar to what POSIX and Windows
provide we're going to have a notion of
sort of exclusive you know write or lock
and then we're going to have a reader
writer layer on that and there are sort
of four notions of locks in here and
mostly those are still the same
exclusive you know single thread lock
versus reader/writer lock the difference
between these layers is sort of your
right to change from being a reader to
being a writer without having to give up
the lock so that's what these
convertible upgradeable types of things
are so that is you can convert from
being having only one person only one
reader can have convertible access and
when he wants to become the writer he
converts
so I have to sit down and look at the
api's every time I try to answer this
question and I don't have those api's
there so it's a it's subtle in the
transition states and and and I can't
remember exactly which one happens here
but but it's all centered around being
able to convert from read access sheriff
then you hold the read lock and being
able to get to the right lock without
releasing it
yeah that sounds sounds similar so the
the observation was convertible as
single reader to multiple reader and
upgradeable is I am the the golden
reader who can go invert two writer now
so that's a mutex on top of a mutex we
have a lock and a lock is basically the
state of owning a mutex of controlling a
mutex so a lock is typically done as a
function local object it's a local
variable in your function that provides
both the acquire of the mutex and the
release of the mutex and these are
coordinated with the mutexes this is
fairly common in some of the C++
libraries to make sure that you get
mutex acquires and releases paired up
properly you're not required to use
these things we also have condition
variables earlier versions of Windows
did not provide condition variables
later versions of Windows do everybody
who's confirmed on Windows and UNIX
pretty much agrees that the Windows
events are much harder to use than POSIX
condition variables and so we're
standardizing on that approach what
condition variables allow you to do is
upon grabbing a mutex you've discovered
the state isn't quite right for you to
release the mutex and have somebody
notify you when the state is something
that's appropriate to you and once you
have these then you can write your code
in the monitor pattern which has a fair
amount of literature behind it
and so here's an example of the
conditions and where they get used in
particular they represent extreme states
so for a buffer we have conditions where
the buffer is full and are not full and
not empty so we represent the extremes
and then we can notify threads when we
reach 16 states the next thing the the
question is are the locks non reentrant
the answer is that's unclear right now
the committee hasn't fully decided
whether or not to provide non reenter
lakhs only reentrant locks only or two
locks with the option so we haven't
quite settled on that yet so the next
topic we're going to talk to you about
it's thread termination the first issue
is voluntary termination so if you run
off the end of the function that was
used to start the thread you've
terminated that thread but there's a lot
of applications where the natural thing
to do with a thread is to put it in a
loop waiting on some external event and
you want to shut it down some way so the
application that doesn't terminate of
its own accord it wants to be told to
terminate and for that we need some way
to signal to the entire application and
all of its threads that it's time to
quit and the committee has strong
opposition to asynchronous termination
because asynchronous stuff going on in
C++ is just nearly impossible to code
properly so the Committee has strongly
voted against any form of asynchronous
termination so now we're left back to
synchronous termination
the only way to really make this happen
is if in one thread I signal to another
thread that it's time to terminate and
that second thread perceives that as an
exception so let's take a little
digression here and ask well what
happens if a thread has an exception
that it doesn't handle the exception
runs off the top well do you call STD
terminate as you would if an exception
popped out of main do you propagate the
exception to the to some thread trying
to do it join do the juju just ignore
the exception and the answer to that is
we haven't fully decided yet
there's various issues either way you do
it in any way you do it you're gonna
make somebody unhappy but we have sort
of decided that what we need very
definitely is a way to manually
propagate so if we assume in the
simplest case we just call terminate
then we're going to need a way to
manually appropriate an exception from
one thread to the other and that in fact
requires new language facilities because
right now we have no way to sort of
store and forward an exception
so what happens we're back to
cancellation we've already sort of
mentioned that cancellation will be
perceived as an exception in the second
thread that's being told to cancel the
question is will that thread be ready
for it
when will it see that exception in the
case where you're not ready for it we
have an API to say I'm not ready for any
cancellations now and so any
cancellations will just sort of be noted
and will not actually appear as an
exception and then when you become ready
to deal with the cancellation you can
then pick up that exception within that
range of things where you're not quite
ready for one you couldn't you can
explicitly test for whether or not a
cancellation is pending and you can
simply ignore things but in the normal
case you will get a exception but you
won't get an exception at arbitrary
points in the program you will get an
exception only at cancellation points
places where we know it's safe to cancel
you and those are based on this list
here just the synchronization points
where you're likely to be blocking or
doing some other act like that so what
comes out of all of these things is
potentially instead of what you were
expecting an exception saying you've
been cancelled there is however a
problem in all of this none of those
points included i/o operations so if you
are blocking on an i/o the thread is
blocking on Io you now have no way to
cancel it and if the i/o it's waiting on
doesn't show up it's going to be hanging
there forever so we know this is a
problem partly it's a weaknesses in the
current operating system because many of
the current operating systems don't
guarantee that you will wake up from an
i/o operation if you cancel a thread so
the C++ committee
trying to work out in conjunction with
some other committees like POSIX what we
can do to make sure that we have some
way to cancel threads that are blocked
on i/o and we don't have a resolution on
this yet and then going beyond what
we're putting into the standard we want
to try and able libraries to be to
handle the heavy lifting so most of what
I've shown you here today is primarily
the lowest layer that most programmers
will never see that they will work
through libraries that a higher layer
and the question is how do you get that
higher layer the first task is probably
things like thread pools and thread
groups that the C++ Standards Committee
will define in terms of a technical
report there's still an open debate
about how much of that will go into the
next standard and how much will be
deferred to the actual technical report
but the issues for deferring these
things add up among them being we aren't
quite ready we don't know how much we
want to force early
we still need specification work etc and
the standard is looming very quickly
what would we count as success well one
of the things is can we build libraries
and the answer to that is we believe
that as of today that we can build all
of the facilities that we're trying to
implement in the second technical port
using the facilities that have been
proposed for the next C++ standard so we
have sort of evidence that at least the
first round of libraries can be
implemented with what's being defined in
the standard and we believe then that
you can pick up and define your own set
of higher-level libraries beyond that
one of those examples is futures that
we're contemplating putting in a
technical report and basically what a
future says is go off and compute this
function and later I will come back and
ask for the return value the fork/join
model that the standard is proposing
doesn't provide for return values it's
all void so what happens if you want
those return values futures is a means
to happen that what happens if the
thread exits with an exception you want
to be able to get that exception at the
time you asked for the return value and
the advantage here is that now you have
a simple mechanism to take sequential
code and using the futures turn it into
parallel code and for those interested
in reading the actual papers those are
the paper numbers but we have a big hole
right now in the standard and that is
the proposed standard does not have a
mechanism for lambda for the purposes of
parallel and programming what we really
need is basically an anonymous nested
function something like out of Pascal
there's a group
we're late but we're trying to get this
done in time for the standard so if we
can put that in then those lambdas
provide you an extra layer of
functionality and capacity that really
helps you write good libraries so that
you can write a parallel for loop
entirely in library code
so the conclusions the basics are on
track most of the core language stuff we
could probably go with what we have
today we are still fine-tuning it maybe
trying to loosen the model in certain
places so that we can support more
loosely coupled hardware and the basic
model for launching and dispatching
threads and synchronizing all of that is
there we're still working on detailed
syntax and detailed semantics but we're
basically on track there some of the
features need work like this destruction
we talked about to make sure that we're
right there making sure the the
exception propagation is is solid and
getting these cancellation lambda the
higher-level issues in that's where
we're working where we've got the
biggest work ahead of us and then of
course the real value to all of this is
not when we're done with the standard
but when you pick up the standard and
write the libraries write the the
facilities build on top of what's in the
standard and you know put it all on open
source and so forth
okay so the question is when do these
atomic variable dynamic allocation of
atomic variables and locks and when do
those get handled first the atomic
variables have what are called trivial
constructors so they they come into
existence basically either completely
undefined because you failed to provide
an initial value or they have a defined
initial value so there's no possible
race on creating them once you have
their name and can reference them
they're already initialized okay
operations and so so the question is
what happens if you have in an arena
allocator and you're basically reusing
the same same memory right the the
atomic variable
the operations the atomicity comes not
with the initialization but with the
first atomic operation on it so anything
you do before that first atomic
operation is is is prior history
sequential history so that first atomic
operation the reason that first atomic
operation is good is that the
initialization is well defined and the
initialization is done before the atomic
variable comes into existence so the
normal semantics is the variables that
are dynamically initialized are zero
initialized before that variables name
is released and because they're 0
initialized we have a good state at the
start of the atomic variable okay now
for mutexes the story is different
because most of the mutex is from the
operating system require that you had do
some initialization operation on the
mutex and for that it has to be done by
one thread and you have to make sure
that you don't let that the name of that
mutex out until after it's initialized
so you don't want to pass the address on
until after you've initialized it now
for Destruction the atomic variables
also have trivial destructors so
basically they are never destructed
they're always valid they'll be valid
till the end of the address space but
the mutex is from the operating system
they also often have that same trouble
if you have to call a specific function
that says ok this mutex is now over with
and you have the same problem there that
you had with the initialization make
sure that you've taken away the address
new Tech's from all the other threads
before you bring it down the question is
can you have reference counted objects
that delete themselves the answer is yes
you can but you have to be careful to
make sure that you the reference count
is such that you don't let anybody else
in so so you have to set up set things
up in the right order and rare reference
counting objects in an atomic world are
not trivial so yeah the mutex can be
handled because you can you can undo the
mutex while the reference while still
preventing access with an atomic
variable so you can use an atomic
variable to stop people from trying to
do a double operation on the mutex
question
okay so yeah the the question I'm going
to expand your question a little bit and
the question is is there prototyping
information of efforts underway for any
of this kind of stuff the answer is that
there for the standard in general
there's a fair amount of prototyping
effort going on within GCC to prove
certain higher-level features are in
fact implementable for the next standard
there hasn't been any prototype work for
a lot of the lower-level stuff here
directly for this standard we the atomic
stuff has been justified in part because
a lot of the prior work out there is
very very similar to what we're
standardizing so for instance the atomic
operations there's two lines of
reasoning that say it's implementable
one is the paper that proposes it has
the bare minimum of implementation in it
and then it then it points at the GCC
and Intel underscore under sink
primitives that say the rest of it has
already been implemented by GCC and
Intel compilers for this alternate
syntax now the one place where we have a
real weakness here is in the
implementation of the concurrent
variable initialization and destruction
there is there is no implementation of
that yet
yes the question is maybe we can get
that going and yeah I would like to get
that going and every time I sit down to
start on it and I get interrupted and so
if somebody out there has some spare
cycles and want instead and help work on
this happy to hear from you
oh okay so what the question is what
does n 2169 mean so the C++ standard
works in terms of working papers so
people write a paper describing a need
and a proposal and so forth and then
that gets a document number and you know
we started n1 and we're now at and 2169
and so that's the paper that has on some
certain topic this and 2169 is a paper
that's describing all of the features
that are being tracked for the next C++
standard so all of the proposals for the
next standard it's tracking all of them
and giving you sort of an an overview of
where everything is right now and a
subset of that is all the threads
dealing all of the papers dealing with
multi-threading and unfortunately
there's no overview paper for threads
you get an overview for the entire
standard or individual thread topics but
not for the whole for the thread domain
any more questions well thank you all
for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>