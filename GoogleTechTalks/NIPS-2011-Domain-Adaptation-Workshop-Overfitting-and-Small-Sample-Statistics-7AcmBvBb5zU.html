<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Domain Adaptation Workshop: Overfitting and Small Sample Statistics | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Domain Adaptation Workshop: Overfitting and Small Sample Statistics - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Domain Adaptation Workshop: Overfitting and Small Sample Statistics</b></h2><h5 class="post__date">2012-02-02</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7AcmBvBb5zU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so whatever talk today about the main of
the patient small sample statistics and
this is the work return home with Sean
and Dean median give me a little bit of
motivation
let me just give you enough motivation
is too well only the worst environment
is that stuff about that
the motivation of why what we're trying
to do here so if you put the most of the
machine learning algorithms most of the
assumption that you're making is that
the distribution of training samples
name is institution what that sounds
that's the typical assumption but in
many real-world settings that particular
assumption fails to Paul it's one of the
central problems in practical domains
yes yes that's what should be dancing at
issue so what I'm going to do this stop
is I'm gonna concentrate on one
particular try to do this one particular
idea very simple idea and the idea is
that what we're going to do is we're
gonna make for an assumption let us say
that our trainees cells but it consists
of a small number of sample domain
sample domain but we're going to have
lots and lots of examples in each domain
and the goal that we're going to try to
do is we're gonna try to generalize the
new domains so how does that let me give
you just an intuition of of how we
initially looked at that problem suppose
I give you Mami's having the character
states at the standard data set and I
contains ten different classes and I try
to classify that discriminates tues
versus all the other hand even digits
good boy is a source domain I can drain
my favorite classic black and what
happens here is that I can get accuracy
they are one person so I can do really
well now what I'm going to do is I'm
going to test my model discriminative
model in that second two verses knots in
the target
I'm gonna call it are you don't know is
that images of nice never appeared in
this sets of images so any any ideas and
he guesses what will happen seems like
lastly I should be able to accomplish
that by material some minds to look
something like through yeah so what
happens here is that these are just
areas other brokers these were majestic
regressions are here and what you're
seeing here is you really add features
this was a 2000 features you didn't
believe that work style features and
they typically instead of their results
here what happens here is that if you
look at the best though generalization
on the source domain so i get if i show
you images of tools and I try to
discriminate with your new images of
these guys this is what happens and you
know that that thing here goes up there
but less than one person so I can do
really well but notice what happens to
the air thank you don't so what happens
here is you have a quick drop in the
performance of the moment
these features and notice how big the
gap between these two these two models
little least given domains and knowshon
also tried using this type of how with
no messy am using various gonna
logistical in Russian there is future
choices you see that problem over and
over again and it's up there is a baby
which I didn't say well you're testing
if you training on this particular task
in the testing in this task well you
expected you know section air about the
hotel we have log into here right yeah
whatever think is too or 50 50 let's say
that the two worth of value fifty-fifty
from the crafty friend would classify
everything in the coop no this was bout
so there's the bounce data set so you
had I think in this case he had one
thousand villages of tools and one
thousand images of peace yeah Richter
your reply to test her so lower is
better or so so so high in fat so high
is the attacking its accuracy I am a
blocker so high static oh ah so uh so we
look at this problem said well is there
anything we can do here
and what we're going to do is will they
make the following assumption the better
assuming that the heavy distributional
delegates have a small number settled I
make so we make an assumption that these
small number of delays I independently
drawn for some distribution of the
domains our main focus on the next slide
so for example in the previous example
we could just say well suppose that we
have eight no domains in our training
set the mains of the 42 versus 02 vs
want to fit two verses 3 and then when
the goal is to how well you can do all
new domain like that so really the idea
here if you look at this problem you
would say well what you want to do you
want pick up features that a
discriminative of two right if you if
you want applied to a minimum of that
form now you could argue that maybe it's
better to build giant at home if you
build generative models of tools versus
other do this you can be discrimination
that way and that certainly is one
solution and you know in in my view it's
not obvious which is better in some
cases particularly show you some results
when using high dimensional images and
now it's a little hard to do generative
so in this that's what we're gonna take
up otherwise money approach and if you
think about this problem it's much like
a supervised learning problem except for
your several points among several
domains okay so here's a definition you
have a distribution of the domains
conditional a particular domain you have
a distribution of the Inca County pairs
defined this way what we're going to do
an S work in particular we want to try
to find a way connected w so as to
minimize this was just the square dance
notice here with any expectation
respective domains as well as a patient
with respect to me and what the climate
assume is that our training set is going
to contain and non domains / t 1 per
diem where each domain these suction is
that each domain is sample may come now
as a sign form just for clarity of
presentation what what will make your
assumption is that typical in practice
making sumption that n is small number
of domains is small but we can get lots
and lots of examples initially so in
particular you're going to make an
assumption that these expectations so
these are correlations expectations of
facts with respect to Y why is the
target
as well as covariances within each
domain you can estimate the very right
so I'm just gonna make an assumption
that we can have those expectations and
practice them get empirical estimates of
those expectations okay this is for
which is quieter preservation you look
at the paper there's some some
description of how we deal with can
estimate these things okay so we go the
do you go with this come about this work
is try to avoid overfitting as we had
features so between the fine features
such a way that when we add in those
features we can try to improve the arrow
in the domains now if you think about VD
way of selecting features in video we're
selecting features we basically look at
your source domains the mix that you
have we try to decrease the training sat
there and that performs quite cool as
we've seen in the previous slide so I'm
just selecting features based on the
trailer your needs that doesn't look
like much so what can we do well let's
look at the optimal weights what's the
optimal weight back to that little bit
I'm also going to make an assumption
that we have normalized to just sort of
expected value of x y squared is going
to be equal to my floor lxi in this case
the optimal weight has this one that
just comes out of the standard squared
loss think about fitting to be in
fashion lines so that's the way that's
the optimal thing now what we want to do
is what fine features in such a way that
the corresponding estimate WI is closer
to the truth than to zero because zero
in this case would correspond to a non
protection so one way in a natural way
of estimating these ways you to just
look at the the average vehicle average
that's just as well for every single
domain we have an estimate of our
expectations and then we are visually
what this essentially says is that what
we're going to do is we're going to take
all of our mains throw everything
together and just estimate me that's
what it says so no distinction between
various domains so if you think about
that two examples when we were looking
at two verses warrants to their
suspicions force you just put everything
together into one big data set
investigate that's your original set of
results that you that's that's the
original settings there's also not in
fact if you actually pick the maximum
one that would be equivalent to DVD
selection now and that's so that's not
your estimate the matter if you look if
it's centrally appearance you saying
that the things should be converging to
the
one of us will accept this song koopa
there is here now what we tried to do is
we try to do something very simple and
very intuitive what we're going to try
to do is when i try to take into account
a pic of X so the empirical dance is
just going to be computed this way and
we want to try to take into account
these Indian dance when we're going to
be selecting those features it simply
covariance basically sets the
variability of the weights across
multiple buildings so what we want in a
sense is without this value to be high
because that becomes more predictable we
have high correlations with me targets
but what we also might have is one hand
empirical managed to be small so we want
it select itches that or not barring a
walk across multiple domains so if you
have a feature that's very good for some
domains but not very predictive for
other domains that values will be half
so that's just a very simple observation
here somehow the question is how can we
combine these two things so one thing
you can do is if you make an assumption
that these estimates before lower
distribution you can use didn t test and
it is statistics has visible that's a
standard t-test and what you're
effectively doing here is you testing
the null hypothesis well the population
so if you look at these two jumps again
if this fellow is very high and you
would say that's a good feature but at
the same time yours what to control the
variance like if the glass is very high
that would drive the T statistics 20 so
that's basically that's just being
tuition of how can we select those
features now you may ask the question
well what if you is actually made up for
all institutions right so that could be
their locks at least examine if you look
at the paper there is a recite but a
number of which is shown that is the
Buster and what comes out of it is is
the theory that essentially says that we
can accurately fast exponential number
of features with fairly high confidence
so this let's just look at it in a
little bit what it senses is the
following
we have to make an assumption that the
expected value of x and y conditional d
and this time he is symmetric so that's
one of the assumptions I think that
between get rid of let's leave we have
to make that assumption so it might be
truth my default now suppose that F is
the set of features whose size satisfies
this expression here so notice the size
is exponentially n then for all X I in
that set we have the probability greater
than Y minus Delta this expression here
where we're not making any assumptions
on the parts x and y
exists of the first movies
again if you look at this expression
here if you're taking sample variance
into account again sample variances is
small then you have that estimate here
so if you look at the paper that is a
proof of the day in in the paper will be
going details but let me show you how we
actually use this in practice in
practice the other again has to be
fairly simple algorithm what we believe
the following first of all instead of
searching over explanation many subsets
what we're going to do is we're gonna do
breeding selection features justice kind
of people do that and what we're going
to do is we're going for each paycheck
side with a computing become in advance
so the mean is just what you would
compute in a static setting and never
completely unbiased estimate of the
dance in this case that will give you
some idea of how the ability of that
feature across multiple domains and then
we're going to choose the future because
T statistics and that will be the future
that we're going to be choosing if you
get the greedy selection you
teeccino the house we use a lot which is
the estimate of clear w this case and
then we get started updates or just to
do it so that's that's be the stem right
and then you know it's typically the
case if we have fun at number of samples
in each domain we can estimate these
expectations just baked some critical
estimates so we will show you what
happens in practice if you use that
procedure so it's the same data set and
in this case you're looking at the tiny
domain when you're testing to go to
success so again the blue curve shows
the degree where you're selecting future
space on the t statistic so you're
taking a paper advances into town and
record shows an IV you still see that
you know you can add up to about 10
features here and on the curse that
dropping down so there's obviously needs
to be some form of a stopping ornaments
in fact to see design mobile because we
have a green card attorneys here this is
just another example where we testing
two versus three same-same degradation
happens but we consistently observed but
when we can add many more features when
we looking
supposed to just agree selection here
and these curves just basically show the
performance on the source domain so
generalization performance on the source
code so you can see that the blue curve
is slightly below the red line which
essentially will expect that because
you're not selecting features that
maximized in training error on the
source and then you picking some
pictures of the more robust and finally
just to show you some results on AC for
a dataset this is the data set that
contains 10 different objects these are
different objects you have and it's
fairly complicated data set so don't get
that day is that building a genetic
model for these for these images is
quite hard and I'm again if you look at
the same statistics here look for the
youngest is that if you're looking at
very big at some point it just looks
almost too runny there is the TPP's able
to
it's like movable number in terms of the
teachers will reliably select up to 50
inches of the 50 of 11 pitches and this
is just showing you that when the energy
are different you're attending classes
reference we can come up with you do
much better and
they</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>