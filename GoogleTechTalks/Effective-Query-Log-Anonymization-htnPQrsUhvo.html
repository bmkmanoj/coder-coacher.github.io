<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Effective Query Log Anonymization | Coder Coacher - Coaching Coders</title><meta content="Effective Query Log Anonymization - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Effective Query Log Anonymization</b></h2><h5 class="post__date">2009-01-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/htnPQrsUhvo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">today I am going to talk to you about
effective anonymization for query logs
and this is joint work with my colleague
Nabil Adam at Rutgers and our students
showing hey and you and honk so you all
know what query logs are obvious they
don't need to talk about that and I
guess I'm preaching to the choir when I
say that you know query logs are among
the most valuable data that a search
engine you know collect and provides
great insight into human behavior and
again you all know you know there are
several applications of query logs you
know where the suggestion is just one
example i put in rutgers and i got a
whole bunch of things you know over
there so you are responsible for part of
this so you know about it the the
question that i want to focus on today
is basically the anonymization of query
logs or your question of privacy with
respect to query logs so why do we even
care about preserving privacy more
obvious most of you ready logs contain
lots of personal and sensitive
information turns out you know most of
us do search for topics related to us
and you know that's not a great surprise
you will search for you know about you
including your own name or you will
search for diseases you may have
personal information so on and so forth
so when you look at it you know when you
look at the data within a query log that
is you know if you look across all of
the queries there's a lot of data about
you that tells you know that makes it
valuable in some sense but apart from
being valuable there's also sensitive
data you know I don't want people to
know that I have XYZ disease and and so
on and you can link across this and
figure this out however query logs are
also widely used as I said they are
really valuable and you want to be able
to analyze them and come up with
interesting information it's useful for
researchers it's useful for law
enforcement and and so on and so forth
so we want to publish them you know
perhaps you want to release them even if
you don't want to release them you want
to keep them you know archived in a
reasonably safe way so to speak so one
example all of you probably know about
the AOL query log incident which
happened in 2006 when one of the couple
of the researchers
eventually released these logs to
academia and they were so to speak
anonymized by the moving you know that
the direct identification information
but that clearly was not sufficient
right so I'm sure all of you know you
know within three hours a new New York
Times reporter was able to figure out
that such and such queries were you know
referred to a particular person in a
particular town and then effectively get
a whole lot of information about that
person so that clearly shows you know
just doing something about it is not
good enough the bigger problem is
adversaries have may have a lot of
background information this was just a
reporter trying to hunt for things a
typical adversity may have even more
information you know partial information
about you and can easily tie this in to
sort of identify the set of queries you
have and come up with more information
about you so that's a genetic problem in
fact at AOL you know as you might know
the researchers were you know
effectively fired for for the sax it's a
big deal and as a result of that even
from google now be in academia have a
lot more trouble to get useful data from
you right with respect to this so I'm
trying to help myself over here by
hopefully coming up with some models
that might allow you to share data with
me so that I can do some interesting
things but but anyway you may make you
know you may pose the question what the
heck maybe we don't care about academia
we have all of the brilliant people
already in house you know we can do all
of the analysis what if we just keep it
all to ourselves you know why do we even
care if I never release it outside of
the company maybe it's safe and secure
guess what no that's not true one that
is always the fear of hacking and so on
again you may say our security systems
are really really good you know we can
protect our data very well but then you
have the problem of law enforcement
remember the case of Yahoo in China
where they actually where they had to
find out that particular person and
report back to the Chinese government
who then had you know pretty much put
him in jail for the kinds of things he
was searching for publishing now of
course that referred to publishing but
you can apply the same thing over and I
had and you cannot battle that unless
you can say oh for technical reasons or
for whatever reasons we only have these
versions of the query logs and you have
to show blah blah you know certain
requirements for revealing certain kinds
of things so so the case I'm trying to
make is it's pretty clear that you would
like to protect these query logs if you
can you know it's not just oh let me
keep it in my safe secure storage and
forget about it so again the question is
how can you effectively anonymize query
logs there has been significant work on
looking at micro data or relational data
anonymization know Roberto has
contributed to that as well but query
logs you know we may hope to get and use
some of that work in this context but
query logs are different in many
important aspects and I will talk more
about that in in a few minutes related
work there's been a lot of interest
recently in looking at how to anonymize
query logs kumar at all came up with
with a method for anonymizing query logs
why I token-based hashing you basically
take the query split it into tokens hash
all of the tokens and that is what you
store the problem is you still have CDs
leaks possible even when the order of
the tokens is hidden so so they have
some solutions but they themselves point
out certain problems even with the the
solutions over there there are suggest
that we can simply remove queries that
are uniquely identifying you know just
get rid of that the problem there is can
you easily figure out you know what
queries are are uniquely identifying
just in frequency may not be such a
great way to figure this out but even so
you know the solution proposed there is
we can split all of the queries into you
know multiple secret shares and only
when we have you know at least the same
queries repeated at least say T times
then you could pretty much put all of
the shares together to decode and find
out what the actual query is might work
it's a different question and this and
make use of something like this in in
any case and again as I said in
frequency is not always you know the
only way of identifying you know what
what really is sensitive so to speak
there's other work by by Jones at all
again at cikm 2007 so as you can see
there is a lot of work in the last year
or
so which has been looking at at this
topic and there they use classifiers to
map a sequence of queries into entra
demographic information and again they
can just show that you can identify
candidate users much better using query
logs you know then then just by random
chance so there they don't really give
that many solutions on what can you do
about it one thing I want to go back to
since I'm coming over from the
relational data perspective and I'm
going to use that perspective in this
work I want to briefly talk about k
anonymity which is one of the first
formal models proposed for for
protecting relational or microdata
traditionally taught microdata so to
speak or micro detour release so some
ratty and Sweeney proposed k anonymity
and idea behind k anonymity is is very
simple a record is supposed to be k
anonymous if it's indistinguishable from
k minus 1 other records with respect to
the quasi identifiers and I will just
talk about what was if identify herbs
are in in just a minute but the basic
idea is hey you know you're sensitive
information it can't be linked to less
than K other people so to speak okay so
everything you basically have a
equivalence class of size K and you can
never go back you know beyond that k
that's the basic idea it doesn't work
perfectly all the times in fact there
have been better extensions of this but
this was essentially the first it's
widely used and you know there are some
very easy and intuitive properties that
you can understand from this so again
for example you may try to identify a
man from some release data all you have
is his gender and birth date now
supposing you find out that there in the
release data set there are K people
matching that gentle and birth date then
the data is K anonymous and of course
this is only true if that is true of all
people over all combinations of quasi
identifiers no not not not otherwise
again when i'm talking about relational
data and k anonymization there are three
types of attributes the key attributes
which can basically uniquely identify an
individual and obviously are always
removed before release so name address
and so on and cell phone to at times
then you have the quasi identifiers
which are basically the set of
attributes which
potentially reveal who you are so
potentially this can be included along
with other information to uniquely
identify you so and then these
attributes you want to somehow change so
you may suppress them generalize the
values whatever to give you some form of
protection so an example is the zip code
word data and gender together which
actually is sufficient to uniquely
identify you know about eighty-seven
percent of the US population so so
sweeney came up with this metric in fact
she was able to find out exactly who the
mayor of i think well let me see either
boston or whatever around the
massachusetts eda she was able to
uniquely identify people in that area
from you know identify their sensitive
data in the health records using just
these three feeds and this was public
information and of course sensitive
attributes have that sensitive
information which you don't want to be
so just as a quick example you may have
your hospital patient data the date of
both sexes the code disease and you're
assuming hey I didn't put in any
identifying information I got rid of the
key attributes the SS end the healthcare
ID number and so on but as I said it's
not sufficient because when you link
this with additional data by the way if
you know km on amity this is pretty
obvious to you you've seen this before
but for the rest of you if you link this
with other data such as the voter
registration data guess what you can
actually link the quote by the Quadi
identifies the gate of both the the size
and actually this yeah those three are
sufficient to link across both and
uniquely identify that you individual
and all of a sudden you know hey there's
alway nearly matches this record and he
has appetite us which clearly a
sensitive information you would not
typically want to reveal so that's the
problem and K anonymity helps with this
because you basically transform this
data set into a corresponding data set
where you can no longer identify that
person unique
I need to stay with them all right okay
let me stick over here and point using
this maybe that would work yep all right
so so that our key anonymized data set
over here essentially you would have if
you try to match using Andres date of
birth sex and zip code now guess what
the sex still stays the same but they've
generalized the zip code the last two
digits have been taken off and same
thing with the date of birth which has
been generalized or bucket arrested in
two years and now you have more than one
person who is going to match and in fact
it will be at least K people who match
that so you have those equivalence
classes of size K so you can no longer
know you know hey was it Andre or it
could be K minus one other people
hopefully you know any of the other
people now there are problems with this
there have been better models I will
talk about that briefly at the end but I
will just give you one one idea for now
you know one problem that that could
happen is that all of these people in
these equivalence class have the same
value for the sensitive attribute so I
have K minus one of the people great but
all of them end up having hepatitis so
then guess what I I really haven't
protected myself at all right basically
now I know and they belongs to the set
of K people but he clearly has hepatitis
with probability 1 protection whatsoever
so that's clearly a problem that's a
limitation of K anonymity and there have
been better models which actually deal
with this as well using including L
diversity and so on but nevertheless
this is actually a decent model you know
at least it's a starting point and it's
very clear and intuitive you know the
kind of protection you can you can get
with this one thing is once you
understand k anonymity this is just a
model you need techniques to come up
with the right generalization or
suppression and so on so there are many
ways to sort of change the data set
right you can change this in lots of
different ways and you could get many
different data sets which would all
satisfy k anonymity okay so which one is
good you know which one is better so on
and so forth that depends on the metrics
you want to optimize so again Roberto in
fact had had worked on this problem
where you try and minimize the
difference between the original data set
and the final data set right so
essentially hopefully that leads to less
loss of utility so the data is still as
valuable you know it has as much
information as before hopefully again
that's not guaranteed because they're
the metric is number of differences
maybe you're just getting rid of you
know very useful difference so to speak
so that's not guaranteed but you know at
least it gives you again a starting
point so to speak now let me switch over
to query logs here is one typical you
know example of query logs now again if
you've seen the AOL data sets at all
this will look very familiar to you this
is the the data the schema that they
have but I would assume this is typical
of search engines of course you may be
collecting a bit more data and so on but
you are bound to collect you know the
actual queries the time at which it was
you know post the URL that was clicked
as a result of the query and the rank of
the URL you know was it the first one
the second one third one and so on and
again there may be more data you're
collecting in fact I would be very
interested in finding out what extra you
know what additional data you are
collecting that's one of the reasons why
I'm here your feedback would hopefully
you know further educate me on that but
this is fairly typical this is the kind
of data you would have with a query log
immediately you're going to notice hey
guess what query log looks very
different from relational data right
guess what the data type is different
you have arbitrary words you know
unformatted there's no structure it's
basically unstructured data and they
contain and precise information so
different queries may mean the same
thing you know again there are different
ways people will be no pose a query they
want the same form of information people
may misspell that again leads to the
same thing you know lots of different
things can happen it's not precise like
relational data I know I put it as a
table but you know it's very different
from your standard relational data over
there and again unlike microdata there
are no explicit Kwazii identifier
earlier i was able to say you have your
birth date you have your zip code you
have your gender together this can be
identified eighty-seven percent with the
query log what can I say you know what
is really really you sometimes it's the
entire query
which may be really fine you sometimes
it may be parts of it right so again
there's no clear structure you know
which which we can exploit and the quasi
identifiers are not very clearly laid
out or anything of that sort then you
have duplicates you have lots of
duplicates in the query logs and in
terms of using it you know it's
sensitive to the distribution of
distinct queries and and so on so the
uses that you have a query logs are also
pretty different you know the kinds of
things you would like to to keep are
fairly different from relational data so
even though we may be able to transfer
ideas over there you know it's a sketchy
transformation into some extent you
really have to think out of the box it's
not a one-to-one you know mapping that I
can just take this and you know put
something there which is good for me
right so I have something to talk about
so so all right let me talk about our
you know the first proposed definition
that we have for giving something
similar you know again to K anonymity in
query logs and then you can see how this
is tailored to the query log environment
so instead of key anonymity we actually
proposed something called ask a delta
anonymity and we say a query log ql will
satisfy this k delta anonymity if you
have for every user you you basically
have K minus one other users who are
Delta similar to that user based on
their queries in ql and you will
understand why delta similar in just a
minute but briefly let me just say if I
just said similar you know exactly the
same the query log would probably be
very useless because all you have is K
copies of every query you have you know
just if you make a copies of the entire
log you have K copies but that doesn't
really protect you in any sense you
basically have the same information as
before and I can infer the same things
as before which is why we have this
delta similarity kind of definition now
when we anonymize user and in fact the
queries we basically first obtain their
similarity and and look for the most
similar users based on the similarity
and that's just an effort to sort of
minimize the amount of transformation
you have over here as well so the
obvious question is you know how do you
measure similarity now so you know if we
can measure similarity
most of the steps sort of become clear
at that point so again when we look at
queries you know I may issue the query
gladiators and I may mean the movie
gladiators or I'm aiming the history of
gladiators it's not implicit from the
word right same thing you know if I
search for sun is at the company or the
star in the solar system no you know
just looking at the text as a computer I
cannot you know figure out what is your
intent right at that point but
interestingly and this is there's been a
lot of work in web mining and one of the
interesting things they found out was
that for a typical query log you can use
the click to URL to actually represent
the the query which was post you know
that gives you much better insight into
what the query really meant so for
example if I the cliq GRL was triple w
dot suncom you know that is a pretty
good indication that the user really
meant Sun Microsystems not the star
quite when I he searched for son you
know son leading to this leads to sun
microsystems so clip URL gives you a
much better view of the the users intent
in that query as opposed to just the
words so when looking for query
similarity we in fact ignore the words
completely okay we don't care per se
what the specific words were because
again users may have you know ten
different ways of writing that query
when they mean the same concept so they
may be going to the same point in any
case so if you look at it in that sense
queries form of bipartite graph you know
this is not our work this is something
that has been observed in fact in the
web mining community you have the
bipartite graph with you know the actual
query text on the left hand side the
cliq URLs on the right hand side and
clearly you have you know it's a
many-to-many relationship you know well
actually one too many definitely but at
times many as well so now if I have such
a bipartite graph I can actually measure
the distance using something known as
the jacquard coefficient so if you work
with sex at all which I assume most of
you have probably all of you you know
that the jacquard coefficient is
basically the best way well the best
known way of measuring set similarity
right and it's fairly simple simple if I
want to measure how similar two sets are
I just look at
number of common items and divided by
the total number of items across both
the sets and that gives me a measure of
the similarity and that actually is a
metric you know by according to the
mathematical definition as well and I
can use that to measure basically if I
look at the left hand side over there if
i look at the creddies I should have
mark that the queries and the cliq URLs
for each query for each text you may
have a bunch of clicked URLs and now if
i want to see similarity between to
query text i can basically look at the
click to urls as a set and use the
jacquard coefficient to measure the
similarity across time once you can do
that we actually intend to go ahead and
actually cluster the queries so again
working on the queries you know rock
queries would be very difficult you have
well your google so you have lots and
lots of users but let me just say
millions of users and you know in that
sense billions it probably is worse in
terms of queries you know if I start
just started just looking at at queries
in the raw form I would be dead in the
water so to speak so so we are going to
cluster the queries in some sense and
here we don't propose a clustering
algorithm we actually use something
that's out there we are not tied to this
approach you could use any clustering
algorithm but most of them look at query
similarity using something similar and
then one a trade of clustering algorithm
the one the beef ramen burger we'll go
ahead and then a trade of li cluster the
queries into groups so that's good
enough we actually go ahead and use that
to cluster the queries but we could
replace that with something else you
know potentially that that's not really
what we're focusing on so once you have
that once you have your queries now look
at users so each user has a set of
queries now of course instead of looking
at the rock queries I am looking at the
query clusters by the way right so I'm
looking at Oh user one has five queries
each belonging to say cluster one
cluster to you know whatever clusters it
may be but I'm no longer looking at the
rock queries even though I've written it
as q1 q2 I really mean the cluster of
the queries over there and this just
simplifies you know it's it's a
tremendous computational task in that
sense right so this simplifies the
following computation a lot more so now
we will again use the jacquard
coefficient guess what you know you have
users who have queries these are set so
perhaps we could use the jacquard
coefficient so in this case you know
Alice has for queries q 154 and bob has
a again five queries q 1 to 2 and five
six and seven so the Dakar coefficient
tells us that the similarity between
Alice and Bob is two x 7 and then the
distance is you know one minus the the
similarity right so so we can measure
the similarity between users except that
when we are looking at users and queries
they have not just set there really
multisets right you can have repeated
queries so to speak so if I look at
these two sets you know in the first set
basically I have to queue once whereas I
have just one Q 1 in the second set the
basic jacker coefficient will tell me
that the two are identical but they are
not exactly identical right and I can
construct you know much worse situations
the multiset thing really matters you
know if I have hundred copies of
something that's similar to another user
who has 99 copies of it but it's not
that similar to a third user who has
only one copy of it right so so the
number the number of queries really does
matter so so in that case what can you
do well guess what you can actually go
ahead and well this is just saying okay
that's a problem what you can actually
do is you can normalize the queries also
you know the absolute number is not that
big a deal over here right so supposing
i ask i have 100 queries in total and i
asked 10 of each type in off 10
different types and you have tongue
queries and you ask one of each type as
long as the types match you know the two
users are similar so again it's really
the proportion of queries that we are
interested in in certain cases of course
you can filter out you know outliers in
both case but typically it's the
proportion of queries that I'm
interested in as opposed to the wrong
number so what i can do is basically
normalize the queries for each user
basically we'll have a clustering vector
so we've already clustered the queries
now for each user you have a clustering
vector which gives the number of queries
in that cluster and you can normalize
this to give basically a proportion of
the
of queries in that particular cluster
and now guess what I can actually take
these two clustering vectors for the two
users and I just compute the distance
and here the way we do it is to take
actually the absolute difference between
the clustering vectors and just some
across the absolute differences to
measure the actual difference so in the
same case as before the same case over
here Alice's queries and Bob queries
over there you can actually figure out
basically I have two queries out of six
of q1 so i have 1 by 3 anything for you
2 1 by 3 and then one of you p and q 4
so 1 by 6 that's my normalized
clustering vector for for Alice and then
for Bob same thing follows over there I
one query 41 and so on so it's fairly
obvious and if you go ahead and multiply
it by half then you why but effectively
you end up with the distance being half
between you know point five between
Alice and Bob and the similarity is also
point five so now why the half basically
if you think about it think of to users
who are completely independent okay so I
have all of my queries in this cluster
which is totally independent from that
guy who has that other cluster in that
case when you basically take the
absolute difference it will sum up to
two and we wanted to keep the range of
distance between zero and one so that's
why half and we can still prove that
it's a metric and it has all of the
properties that that you want a metric
to have so so this just limits the range
to which i think is fairly good yes not
necessarily that's so we could use
whatever we wanted this is just one
thing we went with but you can
absolutely use cosine distance so we
don't depend on the similarity function
either so you know just like the
clustering you can use whatever
similarity function you wanted
absolutely so in fact one of the things
we want to test is try it out with
different things but again you want to
do it on real data to see you know how
well it really does so that's a
trade-off but absolutely you can use
whatever you wish over there
alright so now I'm sort of ready to
define what Delta similarity of users is
forget all of the math over here you
don't forget the formal definitions I
just talked about it so to speak well
maybe I should have put something better
than if I was going to say forget about
it but anyway i'm lazy so i will talk
about it so to speak so when you're
talking about Delta similarity of users
I said Delta right so what does this
Delta me remember you have for each
user's you have a clustering vector
which gives you the proportion of
queries in each cluster right so
basically if you think of each cluster
you know like a circle with a certain
radius you have users with proportions
of of queries in those clusters I have a
bunch of circles for the first guy I
have a bunch of circles for the second
guy and assume that the radius is
proportional you know to the number of
queries they have to the proportion so
to speak basically with Delta similarity
I'll even skip this for now I'll just go
ahead we want that the proportion of the
queries in the clusters be exactly the
same ok so the proportion of queries in
the clusters for the two users has to be
exactly the same well for them to be
Delta similar so I have to have ten
queries of maybe I have ten queries of
this type tank berries of that type you
know so on I have ten queries of you
know hundred queries and total split it
into 10 queries of 10 you have tank
where you split into one of each type as
long as our proportions match you know
we are good so to speak if our
proportions don't match that's no good
so that that is easy that's just the
basic distance you know we are just
looking and making sure the basic
distance is zero the Delta part comes
because of the query clusters remember I
said these were clusters now if within
the cluster you look at queries the you
can have divergent queries right so I'm
clustering all of the queries into
clusters if I look at all of the queries
within each cluster I could measure the
distance between the queries and compute
the max diameter the diameter of the max
radius of that and all we are saying is
that the diameter has to be within Delta
so when we start clustering we didn't
put any constraints on how small the
cluster has to be home
the cluster has to be here is where we
pretend the constraint we are saying the
proportion of your queries has to be
exactly the same first and secondly when
I look at all of my queries and all of
your queries within any cluster you know
they have to be reasonably close even
though they are assigned to the same
cluster because there is no guarantee
you know you may have a very sparse
cluster and a very short cluster we
don't really want that you know that
that would defeat the purpose of making
them similar so the Delta just allows
you to say oh all right you know each
cluster has to be of a certain diameter
you know it has to be small enough so to
speak and I said remember I said the
distance is zero so that proportion has
to be exactly equal now you may think
that is a you know a limiting condition
so then you have a generalized
definition relax definition as well
instead of Delta similarity we can say
that epsilon similarity and guess what
the epsilon actually allows you to
increase the distance from zero to
epsilon okay and you pick your epsilon
the reason we typically shy away from
introducing new variables is that
fueling these variables is a tough task
right I'm sure all of you or most of you
know that in any case we want to limit
it as much as possible but anyway we
thought it's good to have that relax
definition perhaps you may or may not
use it depending on particular situation
so anyway the standard similarity
proportion has to be exactly the same
the way the query variance has to be
within Delta in this case you also have
a further in a way of having it be
within Epsilon okay the distance of the
proportions itself all right so after
all of this definition stuff I can
actually tell you what you can do to
anonymize queries now it's actually
fairly simple guess what what what you
can do given the query log take a user
and again there are different ways of
doing this but i will give you one take
a user and then go ahead and group at
least K closest to users based on the
distance matrix so basically if I was
able to compute the distance between all
of the users I could go ahead and group
basically in some sense and clustering
the users right it's effectively the
same thing and plastering the users so
that all of the groups are are at least
K but instead of clustering
what I'm telling you is something that
comes out of the condensation approach
another technique that was done it
actually IBM PJ Watson you pick a user
and then essentially you pick the
closest users once you're over K you can
stop and then go with the next userpic
the following k users keep doing that
and you know once you're done with that
you can assign the last few users
wherever okay so to their closest
clusters once you have your user
grouping you can select a user as a
representative for each cluster and
again you know if you think about
clustering you know hemorrhoids I can
you know that's my representative but I
could do k-means where I have so called
a virtual user who's representing that
group as well but for now I will just
assume we pick one representative user
you know who has the highest similarity
to all of the other users in that crew
okay and now it's simple basically I'm
going to try and make all of the goo
users within the group as similar as
possible to each other or actually
exactly similar in some sense nearly
their queries are different so there
will be some natural variation I will
talk about that but in terms of the
proportions and the clusters they have
to be exactly the same so i will add
queries or aunt or suppress queries for
other users in the group to make all of
the users in every group satisfy the K
Delta anonymity definition that I have
before so if i have the same proportion
clearly the distance is going to be zero
and then I just have to ensure that the
queries are not divergent you know
they're less divergent and then Delta
and guess what I won't again go into
this but you can come up with the exact
numbers given to users of how many
queries exactly the number of queries
that you need to add varies that you
need to suppress to make the proportions
exactly equal so you have the clustering
vectors you have the numbers you can
actually compute the new number that NY
I for example over there sense for the
new number of queries i would add and
and i can figure out this exact same
number and i can compute a closed form
equation for the new number there and
the new number here for adding or
suppressing so then it's easy the actual
algorithm for K Delta anonymity I first
cluster the queries then I calculate the
similarity using the distance matrix and
just what I told you we are going
group the users pic I userpic the
closest k minus 1 users but all of these
groups together and then you know any
groups which are smaller you want to
throw away if you've got a group which
is smaller than K that's of no use to
you so then you want to reassign those
users to some of the existing clusters
and then add and suppress queries for
each user's in the group to make them
similar to the representative users at
the end of the day you will basically
have a bunch of clusters which are all
satisfying this definitions basically
they're composed of users who are very
similar to each other okay so exactly
similar so to speak so one quick example
this should be fairly obvious let's
assume case three we have 10 users with
those queries I'm sorry that's just the
similarity vector then users and that's
the similarity between the users you
know every pair of users and now for
let's say I randomly pick to user 1 and
then obviously I want to pick the two
closest to use the Stewart who are you
five and you sakes so then I end up
putting those together in the first
group same thing for the second guy
let's I randomly picked him second and
then i will pick you three and you for
because those are the closest to him and
then same thing for the third guy so for
d3 i will add you know actually g3 is
composed of the last three remaining
closest ones and then what happens is i
have one user remaining i have basically
you 10 who's still remaining right I've
grouped into three groups of three and
now the last user can be added to any
one of the three groups clearly you want
to add him to the group which is the
closest to that user but that's going to
happen and once you have that done as I
said you are going to add in suppress
queries and again several methods exist
depending on the order of addition of
suppression you know first you add
refers to suppress I mean sorry then you
suppress or vice versa whether you do
only one or you do both you know things
change based on that but typically the
most accurate method is to fix you know
decide on a final number of queries and
then you can calculate the number of
queries for each cluster that you have
to add or or suppress and one problem
that may happen you know with that
equation that we have you'll end up
getting a number like 3 point 33 you
know 9.8 and guess what I mean I don't
know how to add 9.8 queries but i can
add either mine or 10 so they won't end
up being exactly the same no that's
that's just a natural progression of
this in some sense so quick example
again and this should be fairly obvious
to you we have over to users with those
sets and now I've basically come up with
their clustering vectors 1 by 3 1 by 6
blah blah and now with those equations i
can compute and find out that to make
the second guy scene is the first guide
the proportions and required 3.33 I told
you 3.33 right so I need to add either
three or four queries the q1 have that
as the final number for q2 q3 and q4 so
then I will add you know I already have
you do the same since it's almost the
same I'm sorry I have to stick here
right and then essentially do the same
for for the remaining and I'll suppress
q5 q6 since it makes more sense to
suppress them as opposed to adding them
you can look up the math in any case I
won't go into this but you know there's
a clear deterministic way of doing this
you know figuring out what would be the
the minimal number that's true I said
adding queries actually no we're not
adding query clusters we are adding
queries from a particular clusters when
I say add X queries I will randomly
sample from that cluster and then add
these queries into the group okay so I'm
going to do a random sampling based on
the quays I've already seen but yes it
is clusters we all of this is clusters
not queries they are right there so all
right all right so I we came up with one
more relaxed definition and adding one
more parameter into the mix this one
reason for this instead of K Delta
anonymity we have something known as
if I epsilon anonymity I'll just tell
you why the addition of the new
parameter so to speak one problem with
what I just described to you is the fact
that you know for large case you may not
have enough users or you may have the
problem where the user distances are
very high okay so basically you're
clusters are pretty heterogeneous
difficult in areas and you have to add a
whole lot of of queries so you have to
add very divergent queries and your
transform-set will be very different if
you followed any of the literature on k
anonymity normally the standard
definition of k anonymity assumes that
there is a single k okay everyone has
the same ki pic 100 you know everyone
picks hundred i need at least 99 other
people in my group but there are better
definitions which allow you to
personalize this definition so you can
say oh I want five people I am fine with
k equal to five and you may say you know
I'm more paranoid I want k equal to 50
and someone else may say k equal to 1
you know I don't care about privacy at
all so there are ways of personalizing
this level of cake now we didn't go to
that extent but we did add this new
parameter M which basically tells you
this is the minimum level of anonymity
that I absolutely want okay so so
basically you can say oh the absolute
minimum level of anonymity guaranteed is
that you will have em people M minus 1
people close enough to you and then all
of the remem remaining you know I can
still give a preferred parameter of K so
let us say I set Emmy quill 25 I need at
least four people who are exactly you
know the same as me with my earlier
definition and if K is then that would
mean I need at least five more people
who are within epsilon distance of guess
i'm going to use both basically the
exact match and the relaxed match and i
will say i need a minimum number of
exact matches and then you know some
number of the relaxed matches and that
allows us to to increase the utility a
whole lot because guess what you can do
a lot of subgrouping with then users so
here's an example again no details but
if by chance i ended up using user one
as my representative user with this i
would have to add queries for all of
those remaining ones which basically
destroys you know
utility in this data set so instead of
that with with distance 0 what I can do
here is have three representative users
and basically grew subgroup then so my
minimum k here i keep i'm too used to
being near the blackboard so anyway I
have three representative users so now
my case three I at least have three
people who are exactly the same as me
but I have 10 people who are within
epsilon of me ok so the case then the M
is 3 and that allows you to say exact
match for X many but relaxed match for
at least this and clearly you can see
you know how this is bound to help out
with utility right and now I don't need
to do as much and I can still give you
you know a relaxed guarantee on on
privacy in each subgroup is zero the
distances between each of the three
groups are within Epsilon ok so again I
won't go into the equations we can talk
about that offline if you wish but in
any case even in this case I can
actually come up with closed form
equations for the number of queries you
need to add or and or suppress ok yeah I
would say when I'm allowed to do both
addition and suppression so depending on
the order so i can i can indeed come up
with with with this how am i doing on
time well close to finishing all right
let me go ahead then and just tell you a
little bit more clearly as you would
expect you know k delta and all MIT is a
special case of km delta epsilon
anonymity and those equations will
indeed reduced to the right cases when k
is equal to m and when epsilon is equal
to 0 so you can check that but but that
actually works out and this is indeed a
special case of that we did some
experiments to see you know how well
things work out so to make you feel good
i decided to tell you what the most
frequent clicked URL was no surprises
here google was the most frequently
click URL you know out in our
experimental data and again i would like
to gather a lot more data and find out
you know different ways what are you
doing with it which was one of the
purposes of of coming here and I'm
talking to you so it would be nice to
have something like
soon afterwards but we try it out on on
a whole bunch of data these are just
general statistics you know how many
URLs are clicked by only one user to use
those three users clearly that number
keeps keeps decreasing where if you are
clicked by more than thousand users and
so on jungle solistes where is the
reader from it's coming from somewhere
let's just put it that way not a good
answer right but I can talk about it
afterwards the case so I would like to
get more data from you how's that but
it's real data in any case what we
effectively figured out well maybe I can
say a bit more it's coming from one of
your competitors anyway but I'm not
allowed to see more so in terms of
similarity what we figured out again
when we did the experiments you know the
first figure on the Left sort of shows
you that the original similarity was
pretty low but after the entire process
the final similarity is significantly
higher so again it's not exactly one
because the number of queries you add
and subtract and suppress you know it's
not an integer sometimes it's a real you
change it so so it won't be exactly the
same but it increases a whole lot okay
so we can make it much better now you'll
ask why was the original similarity solo
it was also somewhat sparse data anyway
we also checked the origin the original
similarity with different sizes of of K
and average similarity even when varying
k and we have some interesting results
in that I'll talk about that in just a
bit we checked how many queries you have
to to add and suppress and as you would
expect when the number of users
increases number of queries you have to
add and subtract as an absolute number
clearly increases but interestingly our
average number of user sorry number of
queries per user decreases so with
larger groups you know as you would
expect number of queries you have to add
or subtract decreases larger groups are
better in this sense what about k the
number of queries per user again changes
with respect to K and one very
interesting result that we got which can
be seen from this graph on the right and
also the prior graph over there is that
you know interestingly for four
different
groups there is a sweet spot of case so
to speak so you know differing case
actually give you different values and
for for each group you could identify a
que where the similarity is the best so
to speak so for larger case you know you
have more problems to get it to a
certain similarity for smaller case
sometimes it's not as good as well you
know you can do better in some sense so
that's something we're still figuring
out you know if there's some way to
incorporate that into the anonymization
process now in terms of pre sampling and
doing something which would help out
with this and sort of making it fully
personalized in terms of the computation
time as you would expect well in terms
of absolute time you can see that for a
thousand users we still are in order of
seconds and this is you know standard
workstation so to speak so so it's not
that bad but you can see that in terms
of the time for grouping users that's
increasing very much right well it's
quadratic you should expect it we are
basically seeing the the similarity
between every pair of users again if you
actually were able to do a better
clustering this time should decrease in
in terms of a more efficient clustering
of users right now we are just doing
clustering of queries not refuses but
and we are computing the similarity or
similarity between users but you could
do a more efficient clustering of of
users and that could go down the actual
time for grouping users or adding and
subtracting queries is is you know
negligible so to speak so it's in
milliseconds again and and so on so it's
not a big deal with respect to K again
as K increases the time for grouping
users increases but the time for adding
and suppress the suppressing queries
decreases because you can actually find
closer users very easily but it takes
more time to group that you should again
expect nothing really really surprising
so general conclusions that we had in
overall the original similarity actually
increases when we enlarge the user group
but no matter what you know the final
and on my similarity is very close to
one so it performs quite well over there
and as I said you know wild absolute
number of queries added in suppress
increases total number in a per-user it
as number of users and pleases and there
is I shouldn't say optimal that's that's
a dangerous comment but there is a sweet
spot for K with respect to the groups
and I think that's one interesting thing
observation that we came up with that we
would like to see this is somewhat still
by the way a work in progress there are
some things that we would like to do but
I still thought it would be interesting
to talk about over here today the the
time for computing similarity is more
cost expensive when compared to the
other costs but again if we can do
clustering that hopefully should be
better but we would like to see what
else we can do and as K increases I
talked about this time for grouping
increases but adding in suppressing
decreases so overall conclusion we've
basically taken a first step towards
providing an model a formal model for
query log anonymization based on k
anonymity which is very well understood
in the literature and we've given a
relaxed model as well with more flexible
personal preferences so to speak so we
basically come up with a bunch of models
and we give you some ways of doing this
and experimental results show that the
overall computation time is reasonable
and gives you pretty high I privacy in
this sense but again this is more in the
sense of a framework this is not
necessarily the best way to do things
okay I mean in terms of the technique
you can definitely do better but our
models will still hold in any case so we
are defining a model based on K
anonymity and this environment and then
you know clearly you want to do more
things so cluster not just queries but
users instead of selecting a user you
know maybe a virtual user would be
better in terms of minimizing the number
of added of suppressed queries you can
do better in terms of grouping and one
thing you should expect you know there
are like I bet there are lots of
applications of query logs right if you
give one general technique for
anonymization that probably will not
work as well again here our metric was
just what is the difference in terms of
the transformation but if you could
tailor it to your applications to an
extent you know it's actually worth it
you you may not just want to keep one
anonymized version of a query log you
may want to tailor it applications and
keep five different versions which give
you the same privacy but can do better
in terms of utility okay remember I told
you
it's just a model there are several
versions which would still meet that
that definition but how different uses
right so perhaps we could do the same
and tailor it more towards specific
applications so as of now we don't know
how well it does for specific
applications so we can in for some
things you know in terms of the numbers
of collocations and so on that's still
the same so any application using that
will likely still do good but we would
like to explore you know what happens
for specific classes of applications and
are there ways we can do better in terms
of utility for this and the last thing
is you know again this is based on K
anonymity which has some known
limitations you know there are problems
with this I think some of those are not
as big a problem in this environment
because in terms of sensitive attributes
for example you know that doesn't
directly affect us over here the domain
is quite different but clearly you could
use you know other models and think of
how you could put this together and you
know build something of that sort within
this framework but you are going to have
to build things up from the ground up
and in that sense because the
environment is very different you can't
just apply you know whatever you know in
the relational data directly to this you
can take some inspiration from there so
that's it I'm done questions yes
that is right right right right that's a
very interesting question okay I'm sorry
the question was supposed query logs
actually have more information things
like location and correctly that kind of
information which doesn't have just a
zero and similarity but may have you
know that does have some definition of
closeness right that's true even
addresses have a measure of closeness
you know there's a whole I'm actually I
should put it this way i'm also working
on location privacy we're in terms of
the queries you can generalize the
queries so that you know you cannot be
identified within a particular region
and and so on but again my view on that
still has been somewhat different we are
looking more from the mobile subscribers
perspective and we are keeping that
separate from the query logs but you
raise an interesting point that you know
for someone like google really you're
getting both so it would be an
interesting thing to see if you can
incorporate both my gut feeling is you
you should be able to do it okay because
when we are measuring similarity sure
right now I sort of talk in a sort of
categorical sense but you know there are
ways of incorporating distance functions
which take both numeric and categorical
attributes so you should be able to do
the same you know there are ways and we
are not dependent on this you know
definition of distance so we can
incorporate any other definition of
distance into the mix as well it will be
a bit difficult to view this from an
intuitive nature when that happens but
it can definitely be done and I think
it's worth exploring so yeah if that
sort of answers your question all right
other questions
that's true and and my view on this
that's an interesting question too again
because it's very difficult within this
environment to sort of formally define
this I will answer that in two ways the
only thing that we are really
guaranteeing here is that afterwards you
are going to have groups of some sort
which are very similar right so in terms
of the number of queries or the kinds of
queries they are fairly similar now you
would still need to do things like you
know the word tagging kinds of things
where you can get rid of things such as
ssense and so on and that can be used in
coordination with this so you can you
know this doesn't replace something that
you already have it would seem to get
rid of things the nice thing is that the
only that in terms of the numbers know
what now I think with things of that
sort you would still have to use the the
filtering approaches as as a
pre-processing step in any case so you
still want to search you know any nine
digit number you want to get rid off in
any case because you're right i mean if
i see a nine-digit number I'm again I'm
not looking at the queries anyway right
i'm really looking just at the click
urls and as soon as they are similar I
just ensure that there are enough of
them so of course if there are unique
URLs from that respect the decision
gives Vice unique URLs that will be
suppressed or that will go away to an
extent but I don't think this can
entirely replace other techniques to an
extent but it definitely gives you a
better sense of privacy because now
someone looking at a query log cannot
directly just say that oh these bunch of
queries are exactly yours you know you
have a much larger set and there's more
fuzziness involved basically it still
doesn't give you exactly what you want
but I think you can use it in in
coordination with other techniques it
then it's necessarily yours that's true
which I think you would have to use this
in addition to something else on top as
well
but it's a good way to keep it for
archival missus yes right oh absolutely
i entirely agree on okay his point again
was that you know instead of anonymizing
the whole set at in one go you can break
it up into many parts and basically no
yes so basically one users queries you
can also split up instead of keeping all
of them together that's that's true but
nothing precludes you from doing that so
I I just said that we are doing this on
one data set but absolutely you know you
may have split up your original data set
into many groups of this kind and you
may want to do that in any case so you
can do that absolutely I think that
might make sense could create some
problems as well you know because now
you have same problem like you know what
Robert was saying supposing I put a lot
of essence together so now I know that
your SSN belongs to this set you know
and that could still exist even with the
bigger approach so to speak so I think
that that problem would exist
nevertheless but but in terms of
increasing utility yes you can split
across users and within users as well
you can spread up there the query groups
you know where is no problem and
everything still works
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>