<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Random Sampling from a Search Engine's Index | Coder Coacher - Coaching Coders</title><meta content="Random Sampling from a Search Engine's Index - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Random Sampling from a Search Engine's Index</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/b7PH78fnsmw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a pleasure to introduce to you zoom
bar yourself who's here from from google
actually he was at technion until two
weeks ago so I say no blood
it's going to tell us about how to
randomly samples from a search engine
this paper won the best paper award d
2006 world wide web conference so it
should be interesting talk this is an
externally visible talk it's going to be
put on video at google.com so please
make sure that you exercise your
judgement when asking questions are
making comments thank you thank you
Shiva and this is joint work with my
student maxumcorp a bitch so I'm going
to talk about random sampling from a
search engines index before I talk about
our contributions in this work let me
start with a brief description of what
is the problem we were trying to solve
so all of you are familiar with with how
a search engine is built and looks like
and this is a very high level picture of
a search engine the main data structure
is the index containing posting lists
where each term is associated with one
posting list and the posting list
consists of pointers to documents on the
web that contain that term and with the
note throughout this talk this set or
the corpus of documents that are pointed
by the index by D we call it the corpus
of the search engine or the corpus of
index documents or sometimes we just
simply call it the index although we
just mean the corpus of the documents
now standard users do not have direct
access to the index they cannot get any
data directly from the index the only
way they have the only possible way for
them to get information from the index
is via the public interface of the
search engine okay so users can submit
queries and get back results
what we would like to do is to create a
sampler which is a computer program that
interacts with the public interface of
the search engine so it can send queries
like users and get back results after
submitting queries and getting back
results somehow magically the sampler
should produce a random document X from
D note that for internal users it's very
easy to generate these random documents
because they have this privileged full
access to the index however for external
users it will be very hard to sample
documents from deep it's okay to use the
other string okay
I would be step here
and that's a good question actually our
sampler can work with arbitrary target
distributions over D but for the purpose
of this talk let's think about the
target distribution as being the uniform
on that turns out to be the most
difficult case okay it will be okay
now one technical restriction that
search engines all search engines have
that when a user submits a query to the
search engine if there are many many
matches the search engine does not
return all matches it just returns the
top-k matches where k is a cut-off
parameter depending on the search engine
in google and yahoo it's k equals 1000
in msn it's 250 and this is a severe
limitation that our sampler will have to
deal with why do we need search engine
samples at all well it turns out that if
you have a search engine sampler this
could be very handy in creating
objective benchmarks for search engines
and we can use them to evaluate the
quality of search engines for example
you can test the freshness of the search
engine what fraction of the documents
indeed in the corpus of the search
engines are actually alive you can test
topical bias for example you can compare
and which covers which of the search
engines covers better the dot il or dot
edu domain you can
why why is it happening
it's going to be very confusing if this
stock continues like that
and yeah
so people at the remote offices please
stop pushing buttons because we are
getting confused here you can test spam
you can check if you have a good spam
classifier you can try to see what
fraction of the pages in the corpus are
spam you can check the security
resilience of the search engine whether
it what fraction of the pages are
contaminated by viruses or worms and
probably the most interesting part for
you guys you can check relative sizes of
search engine you can check which search
engine is larger Yahoo or Google
so it turns out that sighs words between
search engines are almost as old as
search engines themselves and the latest
rounds in the search engine size wars
erupted last August after you announced
that they index over 20 billion
documents at the same time google
reported on its homepage that it indexes
only 8 billion documents but shortly
after it came out with its own
announcement that its index is three
times larger than the competition's so
this clearly doesn't add up and it calls
for objective methods to measure search
engine size we can see from this that we
cannot just rely on search engine
reports there are sometimes misleading
why do we care about size
search engine size first of all is just
a measure of how comprehensive the
crawler of the search engine at is a
good crawler is one which is able to
fetch and cover the most documents
possible and in search engine size could
be could indicate whether the crawling
is doing a good job it also indicates
the ability to answer narrow topic
queries queries for which there are very
few results like navigational queries
either the search engine has the right
result or it doesn't a large search
engine has the ability to answer a wide
variety of such narrow topic
navigational queries probably the real
reason why people care about search
engines is just a matter of prestige
search engines like to boast these size
numbers because they think people may be
impressed the question of measure
measuring size of search engines from
the outside was started quite a bit from
1998 in one method to do that you is
similar to what we are doing here is to
sample pages and uniformly at random
from the search engines index how do you
use such samples to estimate size and
there are two ways two possible ways to
do that if you really want to know the
exact absolute size of the corpus the
only way I know to do that is the
following you just generate samples
independent samples one by one until you
hit a collision a collision meaning that
you got the same document twice by a
simple probabilistic argument called the
birthday paradox it's easy to show that
you expect this this first collision to
happen after about root and steps where
n is the real size of the search engine
so if you got
the collision after K steps you can
estimate the size of the search engine
as K squared the problem with this
approach that as n is very large also k
the number of samples unit is very very
large and it may be prohibitive to send
so many queries to the search engine
just to estimate it science so the
approach that was taken so far is to use
relative to is to estimate relative size
rather than absolute size suppose now
you want to find what is the relative
size of google versus yahoo what is the
ratio between the size of google versus
the size of yahoo you will not know the
absolute size it but at least you know
which is bigger and by how much to do
this estimation you don't need that many
samples you need very few samples from
google and test then whether the how
many of them are also in the yahoo index
and vice versa okay and from these two
numbers you can estimate the ratio
that's the approach we use in our paper
there are other approaches for measuring
size some are just based on anecdotal
queries just you can sink the articles
really news articles where they say we
check these 20 queries and it looked
like that the Google results were much
better than the young results and from
this they somehow conclude that Google
is bigger or better than yahoo of course
this kind of studies do not have any
theoretical statistical guarantees
somewhat more principled approach is not
just use anecdotal queries but rather
queries taken from query logs you track
all the queries submitted by users over
a span of three days in some
organization and then you submit these
queries to each one of the search
engines and somehow process the results
and conclude about a relative sizes
again there is no guarantee about what
distribution of queries in these queer
user query logs and therefore it's hard
to conclude anything about the relative
sizes from these experiments a different
approach is to use random samples not
from a search engine but rather from the
whole web if you could somehow sample
pages from the hallway and then test
what fraction of them belongs to google
yahoo and so forth you can get accurate
estimates of their sizes and there were
several studies that address this
problem however this problem is much
harder than just sampling from a search
engine and therefore the methods known
suffer from certain biases which means
that also the estimates they produce are
not very accurate
and the main previous work before ours
was due to Krishna baharat and Andrew
brother a Googler and the Yahoo a person
nowadays and while they were both a dick
I think and they invented the following
method to generate random samples from
the search engine so in their method
there is a pre-processing step where
they crawl a large corpus they crawl the
out directory at the time and from the
large corpus they create a lexicon of
terms this is simply all the terms that
occurred in documents in that corpus
they attached to each term frequency
estimate of the frequency of that term
this is simply the frequency of this
term in the document in the corpus and
the thought here is that since the
corpus itself somehow represents the
whole web also these terms and their
frequencies represent the frequency of
terms over the whole web now what do
they do with this lexical yes how did
they choose the terms
I don't really remember exactly they
just two up I think they took all the
tokens or possible tokens
okay now given a lexicon that was
created in this free processing step the
sampler works as follows it somehow
chooses two terms t1 and t2 from the
lexicon these are not chosen
independently they use the frequencies
to choose these two terms and then they
submit a conjunctive query to the search
engine get me all documents that have
both t1 and t2 okay they get back from
the search engine the top K matches and
now they just pick a random match from
these top K and return it as the sample
so now the question is why does this
give a random sample from the corpus d
well you can prove that other two
assumptions first if all queries return
exactly the same number of results and
moreover all of them return lists or at
most K results and secondly that all
documents are of the same length then
the samples are indeed uniform well you
can see right away what the problem is
that these two assumptions do not hold
in reality which means that the samples
have bias they are not really uniform
the major bias is towards long documents
long documents are much more likely to
be sampled using this approach the
reason is that they just match many more
pairs of terms so it's much easier to
find terms that will match long
documents
second problem is bias towards pages
that have high static rank or high
PageRank even though some the the choice
of the terms the they used was meant to
return more less than K results
sometimes these terms returned more than
K results in that case they just choose
among the top kek meaning that matches
that are beyond the top K have zero
probability to be selected so documents
that have low static rank have very
little probability to be selected and
therefore this method favors highly
ranked documents so what are our
contributions we created two new search
engine samplers one is what we call a
pool based sampler it's based on a pool
of queries which is very similar to the
lexicon of the rotten brother we could
actually prove that other plausible
assumptions this sampler generates near
uniform samples and we can quantify the
distance from uniformity we propose
another sampler which we call the random
walk sampler a this sampler runs a
random walk on a virtual graph the nodes
of the graphs are exactly the documents
indeed the documents indexed by the
search engine and you can show that if
you run the random walk for sufficiently
many steps it will converge to the
uniform distribution so you just run the
random walk for many steps and stop and
that's your sample the main advantage of
this sampler just a second is that it
doesn't need this pre-processing step it
doesn't need an explicit lexicon or
query pool so it's easier to to run
however turns out that it's a less
efficient it requires more queries to
the search engine in order to generate
each sample yes
you were asking I have to repeat all
your questions for this video so the
question was under what measure is the
this uniform distribution is I just mean
that each document in D is equally
likely to be selected it has won over
the size of the probability to be
selected by the sample and the
probability is over the choices made
internally in the sampler the sampler
will toast coins choose random queries
in some way so it's over that measure if
I understand your question correctly
okay in due to lack of time and focusing
this talk only on the pool base sampler
be happy to talk to any of you later
offline about the other sampler tube so
let me start with the simple obstruction
of what a search engine is and we view
it as a bipartite graph so let's look at
this small search engine has only ten
documents in its index and now let's fix
a pool of queries this is just a
selection of queries that you can submit
to the search engine in this case we
selected for possible queries news BBC
Google and maps so the nodes the of the
graph on one side it's the documents on
the other side it's the queries in our
pool now edges are defined by how the
search engine replies to queries for
example when we submit the query news to
this search engine it returns the first
four documents and therefore we connect
the node corresponding to news with the
four nodes corresponding to these news
sites okay similarly when you submit the
query bbc2 that search engine you return
these three documents and therefore we
connect them by an edge and so forth so
we call this the queries documents rough
for a query q we denote by results of q
the set of documents returned by the
search engine when you
and cue as a query results of news are
four are the first four documents as an
example cardinality of Q is just the
number of results returned queries of X
for a document X queries of X is all the
queries in our pool which will return X
as one of the results for example
queries of dub dub dub CNN com is just
news because the single query here that
returns cnn.com as a result is news
however queries of news BBC co do tuk
has two items both news and BBC we call
the degree of the document just the
number of queries that return that
document as the result so the degree of
CNN com is one but the degree of BBC co
uk is due ok now we note the following
observation it's relatively easy to
compute cardinalities and degrees
cardinalities of queries and degrees of
documents so how would you compute the
cardinality of a given query q you just
simply submit Q to the search engine you
get back the results you do not rely on
the number the reported number of
results because this is not oriole
notoriously then you just count number
of results and that's the cardinality
how do you compute the degree of a
document this is somewhat more tricky if
you assume the pool consists say just of
terms or phrases queries then you could
do that suppose our pool is just terms
like what in like the example we had and
so we just enumerate over all terms in
the document we fetch the document
enumerate all terms in the document and
see which one's of them belong to belong
to our pool and that would be exactly
the degree of the document and I'm
making certain technical assumptions
about how the search engine works and
that I know that
given the document whether it will be
returned by the search engine or not but
I push these technical details right now
to decide if any of you wants to talk
about them I'll be happy to okay so from
now on we assume that given a query we
can compute its cardinality and given a
document we can compute its degree
rather easily how do we use these facts
to sample pages uniformly from D so it
looks like that sampling pages from d at
least directly is hard it's a hard
problem we don't know how to do it
directly however it will be easier for
us to sample documents from the
according to a different distribution
not necessarily the uniform one okay
actually we will show that we can sample
documents proportionally to their degree
so this is a non-uniform distribution
it's biased documents that have high
degree will have high probability to be
selected documents with low degree will
have low probability the probability to
be selected is exactly the degree of the
document over the sum of the degrees of
all documents in the corpus so I will
show you in a few slides how we can
sample the documents from d according to
this distribution rather than the
uniform one okay this is an example of
the degree distribution under this
distribution CNN will have one over 13
probability to be selected while news
BBC co uk will have two over 13
now we need samples from the uniform
distribution but we will have samples
from a different distribution that the
grid distribution so it's a natural
question to ask whether we can somehow
use these bias samples the samples from
the degree distribution to simulate
sampling from the uniform distribution
well since I'm here you know that the
answer is yes and actually there is a
it's a well-studied arrogant statistics
called Monte Carlo simulation which
addresses exactly this question how to
take samples from one distribution and
simulate something from another
distribution in our work we use for such
methods and in this talk I will focus on
the simplest one which is called
rejection sampling yes
how do I find that I have the poor
remember we are in the pool based
sampler meaning I have the pool
explicitly and I'll talk in the end how
do we create the pool it's very similar
to how Bharat and broader created their
lexicon okay so we have it available
okay so what is a Monte Carlo simulator
this is an abstract description of a
Monte Carlo simulator it has some target
distribution pie this is a distribution
we want to sample from but it's hard to
do it directly in our case pie is the
uniform distribution over D on the other
hand we have P which is the trial
distribution this is a distribution on
the same space which is easy to sample
from in our case p will be the degree
distribution over documents a Monte
Carlo simulator needs to be able to
compute compute what we call bias
weights for each instance X in our case
for each document X we need to be able
to compute the bias in p relative to pi
meaning we need basically to compute the
ratio between the probability of X under
pie and its probability under t and
these hats are intentional you'll
understand in a second what they mean
and I will show you that we can in our
case we can compute these bias weights
in fact in our case the bias weight of a
document X will be just 1 over the
degree of that document and i already
mentioned to you that we know how to
compute the grease therefore we can also
compute bias weights now using these
bias weights amount a carlos emulator
works as follows remember it wants to
generate samples from pie but it has
samples from t so it will use a piece
sampler example the generate samples
from p it will repeatedly invoke that
sampler to generate samples x1 x2 and so
forth for each one it will compute these
bias weights W of X 1 W of X
and so forth and then feed the samples
and the bias weights into the Monte
Carlo simulator the Montecarlo simulator
is just an algorithm it's a simple and
we'll see one example of such an
algorithm in a second which somehow
processes the samples in the bios
weights and then generates a single
sample from PI note that you may need
multiple samples from P to generate a
single sample from PI what are these
bias weights in order to define them I
need to define what is a normal eyes
form of a distribution a distribution is
just in our case we talk about discrete
distribution over finite domain so these
are just functions from the domain to
the interval 0 1 and they sum to 1 over
the domain an unknown alized form of a
distribution is the same as a
distribution but it's multiplied by some
normalization constant which could be
unknown okay so pie hat of X is just pi
of x times the normalization constant
zip I and now that it's the same
constant for all exit independent of X
to make this definition more a concrete
let's consider an example for example
suppose pi is the uniform distribution
like in our case so the simplest
unnormalized form of pi is the following
for each X we assign the value one
constant it's always one and note that
indeed pie hat of x equals pi of x pi of
x is 1 over the size of the times the
normalization constant size of the so in
this case the normalization constant is
the size of d and the unnormalized form
is the constant function one
what about the degree distribution in
this case and uh normalized form of the
degree distribution is simply the degree
of X ok and the normalization constant
in this case is the sum of the degrees
over all documents in the corpus the
bias wait if you remember is just the
ratio between the unnormalized form of x
under pie and the unnormalized weight of
X under P so we don't really need to
know PI of X and P of X with it's enough
to know pie hat of X and P hat of X and
these are easier to compute at least in
our case for X pie hat of X is just 1 so
we know it right away for a pee hat of X
is just the degree of x which also we
argued it's easy to compute so in order
to find these bias weights we don't need
a normalization constant that are
unknown the size of the and the sum of
the degrees
okay i have about 30 minutes so any
think i have time let me quickly show
you one such multicolor simulator it's
the simplest one known do to John phone
ointment it's called rejection sampling
this simulator needs to know what we
call an envelope constant and it has to
be given a priori it's a constant that
is guaranteed to bound the bias wait for
all X's so you need some upper bound on
the bias weight of all X's in our case
it's very easy to find this bias where
it is constant because if you remember
the bias weight in our case is 1 over
the degree this is an N value which is
always at most one we so we just use an
envelope constant C equals 1 now the
rejection sampling algorithm works as
follows it repeatedly generates in an
infinite loop samples from the trial
distribution P for each sample it
invokes an acceptance rejection
procedure which decides whether to
accept the sample or not if the sample
is accepted it is output as the sample
from PI and the procedure stops
otherwise it's just tossed away and the
new sample is generated so it continues
until one of the samples is accepted how
do you determine whether to accept a
sample or not that's easy you just toss
a coin whose it's not a fair coin it's
not half half but rather it has
different probabilities for tails and
head the probability for heads is the
bias wait remember we know how to
compute the bios weight over the
envelope constant see now that the
envelope constant guarantees that this
is a proper probability it's a value
between 0 and 1 and and now if the time
if the coin comes up heads we just
accept the sample
as I said our in our case C equals one
and the acceptance probabilities just
one over the degree and it may look
somewhat abstract but it's easy to see
what happens in our situation in our
situation p is the degree distribution
which is non-uniform pi is the uniform
distribution so what basically the
acceptance process rejection procedure
is doing is smoothing the degree
distribution samples that have a Apriori
high probability to be selected because
they have high degree will be accepted
with low probability on the other hand
samples that have low probability to be
selected a priori because they have low
degree will have high probability to be
accepted so overall you get that all
documents will have the same probability
to be selected it's it's a rather simple
a probabilistic argument okay so how do
we use rejection sampling to create our
sampler so as I told you we have some
way to create samples from the degree
distribution and I'll show you in a
second how we do that this has to
interact with the search engine somehow
and then it generates samples from the
degree distribution it feeds them into a
rejection sampling procedure the
rejection sampling together with these
bias waves and then the rejection
sampling procedure just outputs a
uniform sample from D so it's very very
easy so the main question to be asked is
how do we sample from the degree
distribution if we do that we are done
okay so let's see how we can sample
documents but now by their degree not
uniform so here is a naive way to do
that we just select a random query from
our pool we said this is an explicit
pool we have it so we can set select one
of these squares at random suppose we
selected BBC and now we choose one of it
the results we submit that query to the
search engine get back the results and
so and choose one of the results at
random
say in this case we chose news BBC co uk
it's very similar to the bar at broader
approach by the way and then you just
output that as the sample well you could
see that indeed documents with high
degree have higher probability to be
selected simply because they match many
more queries however what we get here is
not exactly the degree distribution
let's compare for example two documents
that have the same degree CNN com and
and the wikipedia page of BBC both have
degree one but I'm claiming that not it
not have the same probability to be
selected under this process can anybody
see why yeah you do
exactly so you could see that CNN shares
its resource the query with more results
there are four results for news but only
three for BBC therefore the probability
of CNN to be selected is 1 over 4 first
to select news and then 1 over 4 to
select CNN among the results of news so
this is a total of one over 16 however
the Wikipedia page has won over for
probability to select DBC but then just
1 over 3 to select the Wikipedia page so
it's 1 over 12 so it has higher
probability to be selected so we see two
documents with the same degree but with
different probabilities to be selected
which means that this process does not
generate documents according to the
degree distribution how can we fix that
if we could somehow sample queer is not
uniformly but rather proportionally to
their cardinality meaning that queries
with higher cardinality will have higher
probability to be selected then we will
be fine so we will choose the query news
with higher probability than the query
BBC and that will fix the mismatch
between CNN com and the Wikipedia page
ok
so here's how you do that is you choose
news with probability proportional to
its cordon allottee and then you select
a random document from its result set
and this is an analysis I will not go
into that just a one-line analysis that
shows you that this is good enough if
you can do that you can sample documents
by their degree yes no I'm not assuming
that I queried all the queries in the
pool that something usually pools are
very very large in our experiment for
example day we had 650 million queries
in our pool so you cannot expect to
submit all of them to the search engine
and you indeed found the main problem
that will have to tackle is how do you
know cardinalities of queries okay yes
yes
I think it's modeled yes what about
conjunctive queries like news and BBC is
it modeled by our setting or algorithm I
don't see the the real difference here
but there are other reasons not to use
conjunctive queries and that's why we
don't use them yes April
are you going to
I didn't hear the last
vengeance
so the question is when we when we use
the samples to estimate relative sizes
of search engines do we use the same
queries and with the we do to search
engines so I think the answer is not no
we don't use exactly the same queries
yes yes
right
call person
oh these specific choices
okay so the question is whether we have
complete freedom in choosing the
normalization constant sore this
specific choice was important in any way
so I don't think it was important in any
way you just choose constants for which
it's easy to compute the unnormalized
form for the uniform distribution it
doesn't matter because you choose a
normalization constant that will give
you a constant function if that it
didn't have to be the size of date would
be other things that depend on the size
of the like to the side twice the size
of the earth so forth so I don't think
it's very important there is no one
choice which is good other questions
before I go on ok
so now how do we sample documents by
degree even what I told you it's easy we
somehow sample queries now from the pool
proportionally to their cardinality and
we'll talk about in a second how we do
that we submit the query to the search
engine get back results and pick one of
the results uniformly at random and by
the analysis shown in the previous slide
this really gives you samples from the
degree distribution so the main question
now left is how do we sample queries by
their cardinality as Shiva noted we
don't know the cardinalities of all
queries in the pool apriori in fact it
seems hard to sample queries from the
pool of proportionality to their
cardinalities on the other hand it's
easy to sample queries uniformly does
this ring a bell so it's exactly the
same scenario we faced before of Monte
Carlo simulation we have one
distribution we want to sample from but
it's hard we have another distribution
on the same space which is easy to
sample from so we can use Monte Carlo
simulation again now we use Monte Carlo
simulation in the reverse way we want to
say we want to have bias samples bias by
their cardinality we have uniform
samples but you could do that as well
with Monte Carlo simulation you could
use it in the reverse direction ok so
the target distribution now is the
cardinality distribution over queries
the trial distribution is the uniform
distribution over queries this slide I
won't go into the details shows you what
are the corresponding bias weights and
envelope constants but just believe me
that it works
okay so this is the full picture of how
the sampler looks like first your sample
queries uniformly from the pool you
submit them to the search engine you
don't know their cardinalities which use
you use in the bias weights you feed
that into rejection sampling and the
output now is our queries some according
to the cardinality distribution not the
uniform one you feed these queries into
the degree distribution sampler which
can now select random results from their
result sets feed that again into a
rejection sampling procedure which will
finally output a uniform sample from the
document corpus d now that rejection
sampling is used twice once to select
queries and then second to select
documents yes
search engines constant as your
when you're comparing to search engines
with different constants of the bias and
residence so there was a question I just
ignored basically the cutoff constant so
the question is how could it be i'm just
summarizing what you said and indeed
this is a technical detail that I've
swept under the rug but actually it's
not a technical data it's something very
important and you know that it's a good
question because I have a slide about
that it's exactly the next slide so
let's see so the thing that I ignored is
indeed this cut off constant I assume
that when you submit a query you get
back all matches and that's something
that does not happen moreover as you
mentioned different search engines have
different cutoff values so what do we do
with these queries which we call
overflowing queries queries that have
more than K matches okay if we don't do
anything will suffer like the baraat
broader sampler from bias towards highly
ranked documents documents that tend to
be ranked low we just have zero or close
to zero probability to select them and
that's not good so what are solutions
first when you create the query pool you
try somehow to create pull of queries
that do not overflow for example if you
take phrase queries it's better than
taking conjunctions conjunctions will be
more likely to return to overflow or
even single terms are very likely to
overflow also conjunctions are likely to
overflow but phrase queries if the
phrase is long enough are less likely to
overflow in our experiment we took
phrases of length 5 which rarely
overflow okay but you still have
overflowing queries you cannot eliminate
that at all what happens if you happen
to choose an overflowing queries so we
just skip it we toss it away we don't
use it at all
that creates some problems in our
analysis okay and the problem is that
the bias waits we compute are no longer
accurate because if you remember that
bias weight in our case was won over the
degree of a document the degree assumed
that that you can just look at a
document and know which queries it
matches but you will not know from the
document itself which of these queries
are overflowing and which are not now we
are using only the non overflowing
queries meaning that the degrees the
actual degrees are different from what
we compute so now what we have is only
approximate bias weights these are not
real bias weights which can totally
spoil what rejection sampling and the
other Monte Carlo methods do in
principle so actually a big chunk of our
paper was devoted to this question what
happens when you feed Monte Carlo
simulators with bias weights which are
not accurate and we analyze the what
happens theoretically it turns out that
if the bias weights we use are close
enough to the real ones to the ones we
would have you wanted then also the
target the output distribution of the
sampler is not very far from the target
distribution and it depends on what we
call the overflow probability of the
query pool this is just the fraction of
queries in the pool that have more than
K results that are overflowing okay so
if your pool was indeed constructed in
such a way that it has few overflowing
queries it also means that your samples
very will be very close to uniform they
will not be exactly uniform but very
close to uniform okay and there are
there are actually your accurate
mathematical ways to quantify this
distance
any questions okay how do we create a
query pool I already told you that we
use phrase queries where do we take the
phrases from like Bharat and brother we
just crawl a large corpus we didn't call
the yahoo directory but rather the open
directory project and from that we
created the pool of phrases of length 5
what do I mean by that for example here
this is what you do if you wanted to
create phrases of length a pool of
phrases of length 3 if a phrase like to
be or not to be occurs in some document
in the corpus so you'll put the
following phrases in the pool to be or
be or not or the overlapping phrases so
it's quite simple and one important
point is know that since we create the
pool from some corpus and this is a
relatively small corpus is just a few
millions of pages there is a possibility
that there are documents on the web that
are not covered by this pool of phrases
meaning that none of the phrases in the
pool or that they do not contain any of
the phrases in the pool if you have a
document like that know that our sample
will never will have zero probability to
get to that document I'm not sure
according to your eyes and I think you
didn't understand what i meant let's
let's consider a simple example suppose
my poor head only 10 phrases it's a very
small pool and now suppose I have a
document none of its phrases is one of
the 10 that would mean that if i use my
sampler with this pool of 10 phrases
that i will never get to that document
because there is no query in my pool
which will return that document as a
result so there are documents that have
zero probability to be selected now even
if you create a very large pool of
phrases like what we did 650 million
there may still be documents that are
not covered by the pool for instance our
pool consisted primarily of English
phrases documents that are written only
in Chinese have zero probability to be
selected and so that brings the issue of
what we call the recoil of the sample
the sample doesn't generate samples
really uniformly from all of the but
from a subset of D and we want these
substitue be as large as possible we
call it the recall of the sampler if the
recall is one then we know the subset
covers all of the if the recall is less
than one then it covers the fraction of
the a we were able to empirically
estimate the recall of our sampler using
phrases of length 5 and it turned out to
be something like eighty percent of
English pages in the web ok I will not I
will skip the random walk sampler there
is one slide and I'll go directly to the
results of our experiments so this is an
experiment we ran in order to estimate
the bias in our sampler relative to the
bharat and broader sampler what we did
here is we created our own small search
engine we use the ODP the open directory
project corpus to create a search engine
and then we ran the three simple
samplers against the search engine why
did we have to do that because for the
that small search engine we have ground
truth we know everything about it so we
can compare the samples to what is the
reality and this experiment shows bias
towards long documents we took all the
documents in the index of that small
search engine order them by size by
length and then split them into 10-day
styles then we ran the three samplers
against the search engine and found what
fraction of the samples fell in each one
of the deciles if the samples are
completely uniform you expect about ten
percent of the samples to be in each one
of the designs it should be independent
of the size of
so what you could see here that indeed
barat embroider the burrata and brothers
sampler has very strong bias towards
long documents in fact more than fifty
percent of its samples fell in the top
decide on the other hand our two
samplers had essentially no bias towards
long documents the random walk sampler
since it will it's slightly less
efficient or it's not like it's less
efficient than the pool bass sampler had
some bias to a negative bias towards
Shore documents okay then we ran just
the samplers against the three major
search engines and try to find
interesting data and that's what this is
an experiment from late April early May
this year and we compared the relative
sizes of VL google and msn and according
to this experiment yahoo is about thirty
percent larger than google and google is
about thirty percent larger than msn
and we use the sample to check the
distribution of domain names in the
three search engines there is some bias
in yahoo towards commercial com domains
and and you could see also interesting
things in this your lower tail and this
is an estimate of the fraction of dead
pages in the index and again Google is
not doing that great relative to the
competitors this experiment we try to
compare the cached copy of the document
relative to what you actually get when
you just fetch the document from the web
what you expect the cache document to be
as similar as possible to the current
version of the page so the way we did
that how did we do that we looked what
fraction em in okay we checked for each
a value for each value P between zero
and one for each pre sent between zero
and one hundred percent what fraction of
the samples had the peep recent change
between the cached copy and the current
copy so you can see that about sixty
percent of the samples didn't change at
all so in sixty percent of the cases
they both all of search engines were
were fine the cached copy was equal to
the current copy on the web as a you
increase the number of changes you could
see that again Google is not doing that
great because more
confused a little bit here
yeah you know sorry I'm just confused a
little bit by the graph but the graph
also i think means that google was not
doing great in terms of freshness yes
we always so we checked only text after
removing HTML we didn't want just
technical changes to excuse me sorry so
the question is why did we check just
changes in text and not just okay bye
happens okay why does the amount of
change matters and not only whether
change or not so a if you find documents
that change just just by a bit okay
maybe just one word changed and and
Google didn't catch it that's not too
bad however if half of the page changed
but Google didn't catch it that's worse
so that's what we were trying to capture
here but I'm I can't really explain it
well right now yes
what you want is that all
the live version
change
right so I meant
exactly exactly so now it will be hard
for me to repeat all of that but the
ideal thing is that for a search engine
is to have one hundred percent of the
samples at the zero-percent point here
meaning that all documents match exactly
their current version and the steeper
the curve is means that the search
engine is doing better meaning that they
and I'm sorry I'm getting confused by
this slide so just excuse me let's take
the offline okay sorry yes master
p
pick up that
okay you'll repeat that later okay
that's the last experiment trying to
find out what fraction of the documents
in the coppice are dynamic and we did
just a simple test checking whether
there is either CGI or ? in the URL and
you can see that google has a much
higher fraction of dynamic pages in its
index than than the competitors okay to
conclude we presented in our work to new
search engine samplers one is the pool
based sampler which was on which I
focused in this talk and the other is a
random walk sampler and for both
samplers we were able to prove that they
are guaranteed to produce near uniform
samples under plausible assumptions and
we also ran experiments that showed that
these samples have little or negligible
bias that's it thank you
yes question
sighs
yeah so the question was whether we
compare the result the estimates of
relative sizes we got from this
technique against relative sizes that
you could have gotten from sampling from
the whole web so we just don't have
anymore the software that we use to
sample from the whole web and it's quite
tricky to create it from scratch so we
just didn't do that experiment yes
you are L in terms of fact that if there
that content is an example
yeah so the question was what is the
definition of a page is it just a URL so
yeah we just think of pages as URL so if
you have two different URLs with the
same content it what they will consider
two pages yes what was the overflow
probability and I think it was something
like ten ten to fifteen percent yes
do we have any statistical guarantees
about the confidence of the samples yes
that's the main part of that paper of
the statistical guarantees that's when I
said that we were able to prove its near
uniform that's what I meant yes
your size from the same
so the question was whether we can use
our technique to estimate the dupe rate
of a search engine and then adjust this
decides the relative sizes numbers
accordingly possibly we haven't thought
about that yes
figure out the power
so the question is how did we figure out
that the coverage of our pool was about
80% we did it again using our local
small search engine because there we had
complete knowledge
strong
the question is whether we can conclude
anything from that about others about
search engines in general or about other
search engines if you think that this
corpus is representative of the web
maybe but yeah it's hard to conclude
anything and really theoretical or
guaranteed about other search engines
the question whether it could bias our
relative size estimates yes it could it
could mean that our if our recall is
very very low yes these these relative
size estimates will just refer to the
part with sample from if that part is
very small then their numbers do not
mean much that could be maybe you can
take the other questions if there are
any offline and thank you again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>