<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Online Spectral... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Online Spectral... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Sparse Representation &amp; Low-rank Approximation Workshop: Online Spectral...</b></h2><h5 class="post__date">2012-02-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/T6hG6M-e8s0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so this is just the work of the
moon my advisor jungle and our goal is
pretty simple so give it a stream of
centigrade we want to try a bit half the
size mala which explains this data how a
reasonable model representation to this
is just a dynamical system in that local
system consists of two parts there's a
state space where state is just a
concise summary of past experience that
we can use to predict future
observations and there's the actual
dynamics of the model which is just a
recursive rule for updating state with a
new observation so this work is about
trying to learn model of an animal
system and trying to learn a model of an
amicable system directly from
observation to actually great difficult
all the required solving temporal in
structural credit assignment problems
which often lead to a search space will
host of Babylon so what I'll be talking
about today is on a very general low
rank approach to trying to solve this
problem and I'm going to provide some
tricks so that we can actually try to
learn a technical system massive amounts
of data and specifically i'll be talking
about spectral learning algorithms to
learn a particular type of nampa system
model called a predictive state
representation so a predictive state
representation is very general nonlinear
dynamical system model we includes many
other models that you make me familiar
with like in market bubbles and omni
peace and one of the nice things about
predict estate representations is
although it's a very general model
they're actually relatively easy to
learn using fast statistically
consistent spectral methods and all talk
a little bit about the general strategy
for for learning analysis in this way so
the basic idea is that let's say we have
some trace from our dynamical system
which consists of some seen lots of
observations we can take the sequence of
observations and we can divide it into
two parts and we can think about the
first part as the past in the second
part is the future and now given a lot
of drinks it's from an amiable system
you can split them all up and we get a
whole bunch of past sequencing and
future sequences what we're going to try
to do to learn a state space is just try
to predict the future sequences from the
past while forcing this prediction to a
bottom line so the basic idea is that we
can think about state as a predictive
compression of the past now one way to
solve this problem is we can create the
covariance matrix of past and future
observations and if we can if you think
of the bottleneck is a rank constraint
on this covariance matrix let me get a
spectral learning operating than the
system model yr spectral method is going
well in contrast to many other methods
you may be familiar with like maximum
likelihood of Bayesian inference
expecting learning algorithms have no
local optima and this results in a huge
game and computational efficiency you do
have to pay for this was a slight loss
of statistical efficiency but in
practice of the game and computational
efficiency makes the learning out of
them much better a better in terms of
learning more accurately so I don't
really have time to go into the details
of the spectral approaches to learn
Kosar's but i'll give you a very high
level view of how our running out Pete
works so first we compute
variance matrix of house and future
sequences of observations we then factor
this covariance matrix by singular value
decomposition but we can recover the PSR
parameters just using some linear
algebra in this little earning a very
nice because it's a statistically
consistent money algorithm it turns out
we can also do interesting things like
extend the learning albums infinite
future spaces listen vultures and
kernels basically all the formulas that
you use to in the finite dimensional
feature space algorithm carry over into
the infinite dimensional algorithms and
we could just rewrite all exploded less
in terms of grant matrices I let me use
a kernel city of that your composition
instead of any cost ok so let's get to
the I guess interesting part for this
workshop so all of these learning
algorithms which I've talked about for
learning critical state representations
have to perform a singular value
decomposition of a covariance matrix or
in the criminal case a gram matrix and
this is actually a huge bottleneck so if
we have a lot of futures or a lot of
data points we might not be able to
store the grand matrix or the covariance
matrix of memory much less be able to
compute a single value decomposition of
it so for example let's say I want to
learn a model of a video and I have one
hour of video at 24 frames per second
each frame consists of 300 by 300 pixels
and then our features of the past and
future visa sequences observations they
might consist of all pixels into second
windows now the ground matrix would have
like 10 to 10 entries and the covariance
matrix would help filling up 10 to 12
countries so trying to learn it out the
system from either of these are state
space to deliver these matrices to be
impossible
so how do we make learning tractable
well leverage two techniques and we use
online learning we use random
projections and either one of these
techniques is actually knew but the
combination with special learning
algorithms for predictive state
representations and dynamical systems is
it actually makes a huge difference in
practice on trying to learn these
radical systems from a large amount of
data so the key insight for online
learning is that we have this covariance
matrix we have to factor it well instead
of building the covariance matrix
explicitly we just store a factor
covariance matrix you know each human
observation one rank one update of this
covariance matrix or the factory
covariance matrix and the predictor stay
representation parameters can be updated
accordingly by just using some linear
algebra now what this means is that we
might have a system which has like a
hundred thousand features but then maybe
the dimensionality and dynamical system
is only like ten dimensions when
something like this happens then we can
easily store the factor covariance
matrix in a small amount of space in the
computer and the updates are just found
in time for example so it suddenly
becomes a practical problem now one
problem here is that if we're interested
in learning models for infinite
dimensional future spaces there's no
rank one update for Carlesimo valuing
compositions but what we can do is
instead of using HIPAA dimensional
feature spaces we go projects down to
finite dimensional future spaces and use
a huge number of features and then use
our online learning algorithms
ok so we apply these techniques to
several different robotics problems
we're trying to learn dynamical system
models of sensors on the slot car racing
around the track or damping system
models of video from a robot which is
moving around the room in different
directions and these problems actually
become tractable now you know other
methods couldn't even attempt to solve
some of these problems in the past
through the large amount of data alright
so in summary the focus of this work is
just spectral running our rooms for
predictive state representation models
of partially observable normally I have
assistants and we show how to update
parameters of the estimated Piazon model
giving new data we show how to use
random projections to approximate kernel
based learning algorithms in a
combination of online learning random
projections means that we can learn
order with orders of magnitude larger
data sets and this then means that we
can tackle robotics bombs that are too
complex for previous methods so thank
you very much do you have any questions
about this
the November closer</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>