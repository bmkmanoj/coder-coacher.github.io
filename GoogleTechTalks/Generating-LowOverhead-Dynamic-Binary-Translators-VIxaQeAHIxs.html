<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generating Low-Overhead Dynamic Binary Translators | Coder Coacher - Coaching Coders</title><meta content="Generating Low-Overhead Dynamic Binary Translators - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generating Low-Overhead Dynamic Binary Translators</b></h2><h5 class="post__date">2010-06-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/VIxaQeAHIxs" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome everybody good morning I'm
really happy to see at least a couple of
people although it's quite early in the
morning my name is Matias pyre and I'm
going to talk about how to generate low
overheads dynamic binary translators
this is joint work this professor Thomas
grows and the laboratory for software
technology at ETH Zurich so what's it
all about binary translation or short
BTW is a technique well known for late
transformations binary instrumentation
debugging profiling and to add or extend
a lot of features on the fly that were
not present in the original program or
an original code but there are a couple
of problems the flexibility of dynamic
software binary translation and cures at
least some runtime overhead and can slow
down the whole program and all that and
the complexity of all the
transformations can be a challenge so
what we try is to offer a high level
interface at compile time which will
then be compiled down into effective
translation tables these translation
tables are then used in a low overhead
binary translator to keep the overall
over a translation overhead and runtime
overhead as low as possible so that we
can have like a high level interface
with low overhead enabling a lot of
these late transformations and to be
really flexible after short motivation i
will give you introduction setting the
scene how binary translation actually
work what the different binary
translation techniques are i will
discuss design and implementation of a
table-based binary translator talk about
the translator techniques and all that
and the generation of these translation
tables I will then discuss some
optimizations which enable a fast binary
translator in a table based optimization
and at the end I will conclude so what
actually is
binary translation by new translation in
a nutshell is if you have like an
original program on the left hand side
you apply some transformation and you've
got an instrumental program on the right
hand side which will then be run
actually on real hardware there is
static translation for one which
analyzes the original program checks all
basic blocks translates these basic
blocks and in the end emits an
instrumented program which will then be
loaded by loader and executed at one
time so all the instrumentation is
already present in the program manager
started and everything is done
statically the only runtime overhead
that we have is due to the adit adit
instructions or changed instructions but
we see a little cloud over there there
are some drawbacks to static
optimization or static translation what
about self or self-modifying code if the
code reads itself and modifies itself or
if you have a jit compiler we will never
never be able to instrument these kind
of programs that akali because they
change itself and the whole instructions
and everything will not be known at at
translation time on the other hand what
about dynamically lot of shared
libraries if you lot these libraries we
add new code to the program which has
not been translated before that and if
you go more to the molar site what about
obfuscated code you will never be able
to translate obfuscator code or code
that is for example available in an
image or an exploit or something like
that and static translation will never
be able to catch all of these cases so
we move over to dynamic translation
dynamic translation starts the original
program and gives the illusion that the
original program is run but in the end
in the back end we have instrumented
program on the fly like a JIT compiler
does and we have a just-in-time
translation of basic blocks into the
instrumented program we capture all
indirect control flow transfers and
translate them on the
I and gifted loose illusion as if the
original program would have been run and
we translate every instruction before it
is run so whenever we see an instruction
that is about to be executed we
translated first catch it and can add
our instrumentation and all that so a
binary translator looks a little bit
like this we've got the original program
on the left hand side there's there are
couple basic blocks that are connected
through control flow instructions
there's a small loop that we see and
then we start start a program under the
control of the binary translator the
binary translator uses translation
tables to our to translate individual
instructions and copy them into a code
cache the code cache then contains the
translated version of different basic
blocks which are then interconnected and
optimized the table generator is an
offline part that compiles down into
translation tables with are used Edward
I'm these translation tables specify how
an individual instruction is translated
and what action should be taken when we
encounter such an instruction so we now
have the translator that processes
individual basic blocks at runtime and
copies them into the code cache if we
have an outgoing edge or a couple of
outgoing edges of a basic block that are
not yet translated we add trampolines if
three prime would branch to trample are
two basic block number four we would get
back into the into the binary translator
and then at that basic block and
translated at runtime on the fly branch
back to a context switch into the
translator translated copy the
translated basic block to the code cache
to keep up the illusion as that the
program the program always says that it
is executed in an original location if
it reads its own
machine code it will reach the machine
code of the original program it never
sees code that is copied into the code
cache and the code that is executed to
keep up the dilution are to fix all
these are for example return instruction
pointers on the stack or to be able to
execute indirect control flow
instructions we have to keep the mapping
table between locations in the original
program and locations our code cache
these two locations translated using a
mapping table that can be accessed at
runtime tier so so much about the basic
overview I will now discuss the
prototype for a translation system like
that that's called fast BTW this is a
dynamic binary translation system the
system itself is actually machine
independent operating system independent
but the focus of this talk is ia32 and
linux operating system i asserted you to
be able to to access and translate code
of a platform that's most commonly used
and linux because it's open source we
can access the Lauder's and change the
whole program like that so our for for
an efficient binary translation we need
to be able to specify these translations
and instrumentations there are a couple
of approaches will which will use hard
code translation tables based on
individual instructions this is very
hard for a programmer but it's good for
the binary translation system the binary
translation system uses these
translation tables that describe
individual instructions to translate
them from the original program in into
the are into the code cache but as i
already said the manual construction of
these translation tables is really hard
a program has to encode individual
machine code instructions has to know
the two whole Intel instruction manuals
by hand and all and it's really air
prone and really hard so what we propose
is a automation in a high-level
description we have translation tables
in the back and a table table generator
we've got a high level interface in a
C++ library where you can specify high
level transformations on the machine
code itself these high-level
transformations are then compiled down
into translation tables which will then
be used at runtime so we offered a high
level API at compile time to select
individual changes of the machine code
and compile these Intel I to your code
tables down into optimized translator
tables which are used of the table based
binary translator later on what's
actually possible to offer for such a
compile-time high level interface we can
offer different transformations based on
these up code tables that kept catch
memory accesses does this instruction
access memory what are the source
destination or auxiliary parameters is
it a floating point instruction and SS
instruction what kind of up code is is
it a arithmetic opcode is it a lot store
what kind of immediate values are there
and so on and so on we are able to
select and specify these
instrumentations at compile time and
will then generate the opcodes and
translation tables that are used at one
time so that is all done at compile time
we have two high-level interface and
we're going to now move over to the to
the runtime interface the translator
uses an iterator based approach whenever
we execute code that we have not yet
translated we started iterator and
iterate through every single instruction
we decode the whole instruction also if
it's like multiple bites long and
execute a specified action for that
individual instruction that is specified
in the earlier translation table it has
been generated so this iterator based
approach iterates through basic blocks
copies them into the code cache
and then transfers the control back to
the to the translated code to achieve
low overhead we have to master several
than the mentals one of them is we have
to use a code cache for translated code
so that we can really have two regions
late every time that we execute it we
have to use in lining for four functions
to reduce the overhead of function calls
well the function call itself does not
incur too much overhead but the return
instruction is an indirect control flow
like any other and we have to master all
other indirect control flow transfers to
achieve low overhead because these are
the ones we cannot fix up we have to do
a online runtime look up whenever we
have an indirect control flow transfer
is it for example an indirect jump an
indirect call or a function return we
have to go through the mapping table do
a lookup and then transfer the control
to the translated location in the code
cache this is a lot of overhead but we
cannot like change the the jump
locations in the code cache because
otherwise we would like reveal the the
code cache and the translator code and
we don't want to do that so to master
these indirect control flow transfers we
have several optimizations usually we
have to run time lookup and the patching
that is required and these individual
indirect control flow transfers are then
replaced by a software trap which
branches into the binary translator
execute the whole look up routine and
transfers to control back to the
translator program this is expensive so
we have several optimizations that are
done in fat BTW to reduce that overhead
and to recover from these these
overheads for one there is local branch
prediction which predicts the location
where the indirect control transfer will
go to there's in lining of a fast lookup
into the code cache which will check for
the most common parts that we have a
direct hit in the mapping table
and we can build on the fly shadow chump
tables and I will now discuss these
optimizations in a little bit more
detail the local branch prediction that
we use caches the last one or two
targets in the code cache it emits a
specified code sequence which caches two
or one targets that have been used
before if we have a cache hit in that
target uh we can direct we can redirect
control flow to the correctly translated
entry in the code cache if you have half
a cache miss we look the target up in
the code cache we fix the location and
we we update the cache to local cache
this results in like three to five
instructions if we have a cache hit and
a couple of more instructions if you
have a cache miss this optimization
works pretty good if we have like a very
low change rate of geese of these
indirect control flow targets and a
limited set of targets the second
optimization is the fast lookup we emit
a fast in line look up into the code
cache which checks the first location
which uses the target our does some a
fast hash and looks looks it up in a
mapping table if it is a hit we can
directly transfer the control to the
translated instruction if it's a Miss we
transfer back into the binary translator
but this optimization is more complex
and cost us about 13 to 14 instructions
which is more expensive than for example
the prediction the next optimization
that we have is a shadow jump table a
lot of the overhead that we measured for
example in the energy CC benchmarks and
all that comes from shadow jumped or
from jump tables GCC tends to generate a
lot of jump tables for example for
switch statement and initializes these
jump tables the predictor will be pretty
bad do a pretty bad job because there
are a lot of targets and we have a lot
of cache misses if we now
detect a chump table that is used in
memory we build a shadow jump table with
translated targets check the translated
target our branch to the translated
target and activates the correct one
otherwise we branch into a backup backup
function this results in a very low
overhead of only five instructions if
the tar is already translated and we
have a shadow chump table in place
similar to the tree branch prediction
but with a whole chump table that is in
the back so the problem is all these
optimization is that each optimization
is only effective for some locations and
a very specific program behavior if you
have a low number of changes and only
few changes if you have a low number of
targets and only few changes use a cash
this will be pretty fast we'd do the
lookup the first time and can then reuse
the cash all the time if you have a high
number of targets and many changes we
have to fall back to a fast lookup
that's real that's the fastest possible
to handle these cases if the location is
many different targets and the targets
don't change and all are close to each
other you must presumably have a chump
table that's why use a shadow chump
table so but it's really hard to select
these optimizations at compile time if
you see an indirect contraflow transfer
for example in an indirect call or an
indirect jump we don't know if it's if
you should use a prediction if you
should use the fast lookup or if we
should use the shadow shadow table so
that's not known at at translation time
only at runtime an adaptive optimization
on the other hand can select the best
optimization for each individual control
control transfer that's why we have an
adaptive optimization that is used for
all these indirect control flow transfer
we start is a prediction for one or two
locations count the number of mrs. and
Miss predictions that we have for that
for that optimization if we have a
specific threshold or reach specific
threshold we recover to a fast lookup
riva rewrite the automatically generated
codes that is in the code cache are or
we construct the shadow charm table if
the control transfer uses a jump table
as well so these adaptive optimizations
are then able to bring competitive
performance to a table-based table based
translator ought to show you some
numbers I will now discuss the the setup
of the benchmarks we use a motrin little
translation to show the translation
overhead only we used to spec CPU 2006
benchmarks to evaluate the performance
of our binary translator and we use the
test data set to show translation
overhead for short running programs of
between 2 and 40 seconds and we should
use the reference data set to show
translation overhead for long running
programs this is up to a couple or up to
a thousand seconds on our on our machine
related work that we compare ourselves
Swiss as HD trends another table based
translator which where the user has to
supply translation tables by hand and
hard code all these translation actions
dynamo Rio and I are based binary
translator which translates machine code
into an intermediate representation does
some optimizations on that intermediate
representation and translates that back
into machine code and pin a tool that
also uses an ir based approach for to
generate a machine code and translate it
between the different formats so to show
you the numbers for the reference data
set I selected the 4 verse performing
benchmarks so that you can see the
individual overheads we compare fast BTW
HD trans pin and dynamo rio the we show
the overhead compared to an untranslated
run on the right hand side you see the
over
head of the average overhead overall
spec CPU benchmarks we can see that our
table base translators kind of have a
problem if we compare them to dino Morea
for some benchmarks the ir based
approach is able to outperform and
outperform the table based approach in
some cases but if we check the average
overhead this is low for both table base
translators and I are based translators
if you look specifically at the pearl
bench and deal then a deal benchmark we
see that the problem of the table based
binary translators is the huge number of
function calls we have for each of these
benchmarks we have more than 20 20
billion function calls that cannot be
inlined in our optimizations but
nevertheless an overhead of fifty
percent is still pretty low for the
Pearl benchmark this is an interpreter
that's running a other program which is
then interpreted in binary translator so
the ir based approach will of course
have some performance edge on that one
and we see in the analysis here that
there are more than 20 billion function
calls where we can still or where we
could still do a little bit better but
nevertheless on the average overheads
the if you compare these the table based
iterators show the same performance than
the than the ir based ones if we check
indirect jumps on the other hand we see
that we have the champ table
optimization and the predictor these two
are both together are able to cover
about ninety-nine percent or more than
ninety-nine percent of all the indirect
control flow transfers that are there
therefore indirect jumps so we won't be
able to do much better for indirect
jumps no matter what optimization views
if we go to the indirect calls we see
that for salon and deal we have almost
or close to a hundred percent prediction
rate where we used the prediction
optimization but for pearl bench and gob
mkay we still have the prediction
doesn't work so good and we have to fall
back to the fast fast lookup in the end
now if we go to the test data set for
short running programs on the other hand
we see that table based iterators have a
performance edge compared to I are based
iterators these are programs that
executes between four and forth between
below one and 40 seconds we see that
most I are based approaches are never
able to recover from the translation
overhead and cure so in the end and I
are based approach is able to to keep is
not able to keep the low overhead that
it had for four long running programs
and table based approach will be able to
outperform these I are based approaches
so if we compare the reference and the
test data set for our binary translator
we see that for some benchmarks for
example for / bench we see that the test
data set is even faster than the
reference data set this is due to
different instructions that are executed
and a smaller set of a smaller set of
code that is executed so we don't incur
as much runtime overhead you to the
translation as if we would run the
reference data set so to summarize we
have a higher overhead if we have a lot
of indirect control flow transfers
function calls and cure some overhead
even with our optimizations an electron
flow transfers without caches or chump
tables at
overhead to these translations or if you
have high collision rate and an
sub-optimal mapping table then we have
to recover to then we have to issue
expensive recoveries and try different
scheduling strategies on the other hand
if you have low overhead if you have few
indirect control transfers and the cost
of the indirect control flow transfers
can be reduced through the optimizations
so to conclude fast BTW the flat
available shows that it is possible to
combine ease of use with efficient
translation we offer a high level
interface at compile time compile down
too fast translate ails these
translation tails are then used at at
runtime and at runtime we show that we
have comparable overhead too I are based
approaches for long running benchmarks
comparable average overheads too long
running benchmarks and we have a
performance edge for short running
programs up to 40 seconds and other
other benchmarks the adaptive
optimizations that we've shown select
the best optimizations based on an
individual location compared to earlier
strategies where we had to select an
optimization based on a class of control
flow transfers we now can select the
best optimization based on each location
often indirect control flow transfer and
this gives us a performance edge
adaptive optimizations that I've shown
you here are necessary for low overhead
in the table based binary translators
and the the table based translator
poverty is available as open source you
can download it from the from the web
page you can use it in your projects you
can specify your transformations ah I'm
is that I would like to thank you for
your attention and I will be happy to
answer any questions that you have
a couple of questions first of all have
you considered doing static jump table
recovery you can get over ninety-nine
percent of indirect branch targets just
by static analysis of at least spec 2006
benchmarks yeah that's actually one of
the targets we would do if we were a
company the the best optimization that
you can have is a would be to use a
combination of static translation and
dynamic translation so you can keep up
the whole illusion of running the
program in the original location but you
already did a lot of the static
translation have it in the in the back I
mean you could you could create the
shadow jump table ahead of time because
you know what all the targets are and
it's really easy to have to build all
those second question whether yeah the
problem is the problem is that ah that's
actually if we if you would if this
would be released as a if there would be
more people working on it we would have
already done okay right that's one thing
the other thing is the construction of
the chump tables or the costs to
construct these jump cables that is
really really low even at runtime we
have a almost negley overhead for the
translation it's the the translation
itself is just a look up in the in the
different tables and a copy of the
machine code instruction to the code
cache so this is really cheap and if we
construct a chump table we allocate a
page for that at some specific cards
initialize that page of as a lookup or
like backup targets and then minute the
target is executed first time we patch
it back and this is all like very low
overhead and this won't change the big
picture okay second question it looks
like your performance numbers were
really close to HD trans and dynamo Rio
so sell me why am I going to pick fast
BTW over those guys it's fostered and
dynamo rio if you go to short
benchmarks aha that's for one thing okay
it's faster than HD trends in most cases
HD trends doesn't really offer the
complete set of instructions so it's
only for a limited subset of I is 32
instructions okay it doesn't perform it
isn't able to execute all the essence
instructions all the exemptions and all
that and for HT trends you have to
specify the transformations in machine
code so you specify what machine code is
translated into which machine code and
you're not able to use a high level API
so fast be offers a pin like okay I like
that's good where's speed of low-level
implementations so it combines the good
things of both worlds so we have like
very low overhead but you can use high
level a high level API to select your
transformations okay there are no more
questions I've got a third yeah so since
you're using this table based API it
sounds like it would be a lot easier to
add a target so x86 64 arm something
like that how much work do you think it
would be to implement another target off
as a translator for arm for example
right right well you would need to
specify the translation tables right oh
I know how much work do you think the
translation tables are actually the the
tables the opcode tables artistic copy
from the ia32 instructor or from the
Intel instruction manuals okay I have to
just write all these these tables by
hand and bring them into format that is
possible by the table generator okay and
of course you will have to adapt the
table chain generator a little bit to
the / calera tees off the arm CPU for
example all right thanks so if you look
at this in a multi-threaded environment
do you need to like stop the world every
time you update their code cache or like
do the translation that is otherwise
another thread could be running there
yeah actually that's an other difference
to some of the other approaches are we
have a threadlocal code caches and
straight local trains
nations so we differentiate between each
threats and we catch a piece red create
and all these fork system calls that
would change the layout or the number of
the threats and all that and we keep
translation local so each thread has its
own thread local code cache this enables
some other speed ups so the code cache
will only contain code that is local to
that's rat and if we have a locker
threat we will generate code for the
lockers thread only if we have a worker
thread that code cache will only only
contain the the code for the for the
worker methods and this kind of forms a
greedy trace extraction where we
generate traces of code that are
executed on the fly and our threadlocal
caches will then contain these whole
traces in there this enables even some
speed up in some cases and if you look
at the paper we show that we have for
some spec benchmarks we have up to three
point five percent performance
improvements even this out all the
binary translation running in the
background
just a quick question so what's the
difference between a reference and test
data sets I was computable aspects e
piÃ¹ offers many kind of runtime flags
and you can select which ones you want
to there's the rapa the test data set
just to test your implementation which
will only run for like between a below
one second and up to 40 seconds it's
like a limited the spec cpu benchmark
set consists of many different program
kernels and they have different sizes of
input data sets the test data set is a
lot smaller than the reference data set
so the reference data set just lungs
runs for a longer time I see on I mean
in the grass you show actually bigger
time for reference like much bigger ways
that actually or yeah actually the next
one the next slide yeah the next slide
yeah and here so I see like the know BTW
is like oh sorry so it doesn't know
that's the test data set the random of
the test data set is a lot smaller like
it shorter to run the test program I got
confused so you say that test contains
many more but each shorter in the same
time it's like I mean I the test data
set like you have the same set of
programs the test data set is just a
smaller input set so smaller into a
difference more right yeah it's bigger
reference runs longer i c'n in the
graphs you guys performing much better
in the shorter one actually right yes
because the ir based approaches if you
run for a thousand seconds you have to
imagine yourself there I are based
approaches which take the machine code
translate it into a high level
intermediate representation then do some
optimizations based on that intermediate
representation and then emit machine
code these I are based approaches will
be able to recompile the code all the
time if it's run if it runs for a very
long time but on the other hand if it
just runs for a small amount of time yes
I are based solutions one will not be
able to recover from the initial
transfer
translation overhead the day that they
had okay thank you welcome quick
question so have you uh heard of strata
and have you compared your your approach
strata it's a binary translator yeah
from Jack Davidson and Bruce Childers
from and they've done a lot of work with
and the the main contribution of their
work is that it's extremely low overhead
so I was wondering if I think they've
gotten like four percent overhead on a
complete run of spec so I was wondering
if you compared your approach with
strada and ah I think the program isn't
our like the the binary translator is
not available openly so I was not able
to compare myself with that one right
yeah it's not right you kind of have to
email Jack and nag him a bit and then
maybe in a few weeks you can get it but
yeah yeah just you know later is open
source it's GPL you can use it in any
project you want and it's kind of easier
to use and compared such projects with
these closed ones and the work he did it
was a couple of years ago right right
right that was a good while ago around
early 2000s but I'm school yeah yeah I I
checked his work I read the papers but I
was not able to compare myself to him or
reproduce his numbers or yeah that's so
what happens when the source instruction
set is not the same as the target
instruction set then of course you would
need to specify the translations from
the source to the target instruction set
you could add the very how it works is
like we've got a huge table for each
individual instruction we've got an
actual function that translates that
instruction and we have the whole of
ia32 specification in the background
that we can use you would then need to
specify a mapping between the
instructions and then you could
translate them so so the question was
have you done that and if you have how
do these numbers compare I haven't done
that but I've talked with a couple of
guys about it who are interested in it
ah and I don't know how the numbers
would look like currently it's an
eyesore to to ia32 translator which
specifies on load translation overhead
low runtime overhead to add a profiling
instrumentation at the debugging
information and all that you can for
example you could check all memory
allocations check to use the usage of
different pointers scart pages map pages
and other option would be to like
encapsulate the running program and
guarded against overflows against the
against exploit and all that check
system calls if they correspond to
specific specific parameter set and all
that but I have not done cross-platform
like transformation but it's yeah I'm
talking about who the Sun guys are there
other questions okay thanks again for
the attention and you're happy and free
to downloads the binary translator try
it out and write me if you have run into
problems and I can't might be able to
help you thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>