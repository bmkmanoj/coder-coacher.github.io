<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Spark: In-Memory Cluster... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Spark: In-Memory Cluster... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Spark: In-Memory Cluster...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/qLvLg-sqxKc" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay thanks great so yeah so I should
wine you all before I'm a systems grad
student so I'm not an expert on machine
learning I'll try to you know to make do
with what I can but this is a system
that's very applicable to machine
learning so just for some background the
amplitude at Berkeley about a year ago
that's focusing on Big Data and spark is
one of the computational frameworks
we've built in it to make it easier to
do a lot of sophisticated computation
and big data so just to be clear about
this the environment we're targeting
here is large-scale commodity clusters
they're either private data centers or
the public cloud and there's been a lot
of interest in these as a computing
platform of course as we all know to
handle the biggest data sets and tools
like MapReduce have been very successful
but if things like MapReduce and and
dryer even though they are very
successful they have some limitations
they're not great for all types of
problems and it turns out in a lot of
problems relevant to machine learning
these tools aren't very efficient so
what's what's the actual limitation or
problem so a lot of these tools be they
MapReduce are diet are built on
essentially the same execution model
which is a cyclic dataflow so a graph of
operators so basically you you set up a
graph of operators they could be a map
and reduce test and you passed the
records through them you load these
records from a stable storage system
like a distributed file system and you
pass them to and you write them back out
and this is not an accident by adopting
the dataflow models these systems can do
a lot of things automatically that are
very hard to program by hand so they can
decide where they scheduled the tasks
based on data locality they can
automatically cover from failures or
from slow nodes or things like that and
they make it much easier to program
these big clusters but a cyclic dataflow
is not always efficient and one case
when it's not efficient is if you have
an application that needs to repeatedly
reuse data
or generally share data across a set of
operations so one example of this is
iterative algorithms and in many of
these algorithms you are applying
essentially a similar function to the
same data set
and being able to use that efficiently
is hugely important for performance
another example that motivated spark is
actually interactive data mining so as
soon as you put a big data set together
and even say when you're training your
classifier you may want to ask questions
about the data interactively and here
you really like to be able to load a
subset of the data and then query that
interactive speeds but with current
frameworks both of these applications
and go a lot of overhead because they
have to go and read the data from a
stable storage system on each step and
then write it back outs to send it to
the next step and in a lot of these
applications you've measured more than
90% of your time might be spent in doing
this i/o and deserialization and all the
stuff associated with sticking the data
into a storage system and so you scale
out to this big cluster it looks good
you're getting linear scaling but your
algorithm is still 10 times slower than
it could be given your hardware let's
just some some graphical representations
of some of these applications so with
iterative applications you can have sort
of two cases one cases you have the same
data set and you just apply many
functions on it and and you know get
back some results the other thing you
might do is you might also transform the
data set and and share the intermediate
one so this is the kind of thing you
might get if you're doing a particle
filter or something like that and in
SPARC our goal to make these efficient
our goal is to let you keep these
working sets in RAM instead of going to
the stable storage system so the idea is
you know if you you can load your data
set into memory distributed across the
cluster even if it's bigger than any
single machine can hold and then you can
process it very quickly your ham is you
know at least a factor of 10 maybe a
hundred times faster than most this base
storage systems and you can do the same
in the one at the bottom so from a
systems point of view what what the
system's research was about here is how
do we design an actual programming
abstraction that makes this both fault
tolerant and efficient so we wanted to
let you work with distributed memory
without worrying about fault tolerance
yourself so that this can work at scale
in the largest clusters and we also
wanted it to be efficient
so how do you get both of them so the
traditional way that people have worked
with distributed memory either and
things like distributed shared memory or
if you say a database or memcache D or
key value store these are systems that
basically give you a fine-grained
interface to access each record of the
memory so say each cell in a table and
even though this is you know this is a
nice way generic interface it's very
hard to do fault tolerance efficiently
for them because the only thing you can
really do is either replicate the data
across machines or lock the updates
across machines you know every time
someone writes to one machine it sends a
copy of that update to someone else and
when you're working with big data these
are both expensive these require
shipping the data over the network which
is about two orders Mac of magnitude
slower than the memory on your machines
so how do we solve this the programming
model we came up with is based it's
called resilient distributed data sets
or rdd's and it's based instead on
coarse-grained operations so instead of
going and accessing each cell of the
memory you do an operation to a whole
dataset at once something like a map or
goodbye or filter or a join and what
this allows us to do for fault tolerance
is to recover by recomputing things so
we just recall this lineage so we
tracked the sequence of operations that
you used to build each data set and then
if we lose a slice of that data set we
need to apply that operation on only a
subset of the data to get that slice
back and if nothing fails then of course
there's essentially no cost all we're
doing here is logging one operation to
apply to many items so here's what it
looks like so first of all in the one at
the bottom imagine for example that we
lost this whole output data set there
then spark will not redo this iteration
down down here and get the data set back
if we lose both of these it will know to
redo both of these operations and then
the more common case is that only one
machine in your cluster fails and you
lose a small piece of your data set and
in that case spark will read
and rebuild only that piece so it
doesn't need to go back doesn't need to
hold back to a checkpoint and in fact
you can often do the recovery in
parallel on multiple machines so so this
is you know high-level overview of the
model what is it good for so even though
we've restricted the model to this
coarse-grained operations what we found
is that it's actually applicable to a
lot of parallel algorithms including a
lot of machine learning algorithms but
other things as well and this is because
a lot of data parallel algorithms apply
the same operation to multiple items
anyway so that's why the lineage based
fault tolerance strategy makes sense and
one of the cool things we've done is
we've showed that using rdd's we can
efficiently express a lot of programming
models that people have so far expressed
as build separate systems for so we can
do the kinds of things that MapReduce
and riot can do but we can also build
some of the things that people have
designed for certain kinds of iterative
applications so for example Google has a
graph processing system called Pagal and
we can express that using IDT's of a
bunch of folks have built iterative
MapReduce there's also this bulk
incremental work which is incremental
sort of stream processing and we can
also do some new applications that these
models don't so it's actually a pretty
general programming model so in the rest
of the talk I'm going to get a little
more concrete and show you what programs
Authority these actually look like and
give you some examples and also talk
about some of the applications that
people have built using spark then I'll
talk very little about the
implementation just to tell you know how
how easy this is to use and assuming
that the internet works and stuff I also
want to try to do a demo so let's go
into this so the programming interface
so one of the cool things we've done in
spark to make it really easy to write
programs is that we've we've built the
API inside the scholar language Scala is
a functional and object-oriented
language for the JVM it looks a lot like
Java and what this lets you do is run
operations like map and and reduce and
just write the functions right there
as if you were doing functional
locally so it's a very concise interface
and in in the API you basically get
these things so you get ID'd ease these
look a lot like collections in Scala
except that they're distributed across
the cluster and the other thing they
provide is you can tell a spike which of
the IDS to keep in memory we call that
caching you get operations on rdd's
there's transformations which build a
new RDD and there's actions which which
computer results and give it back and
then you get some special things that
look like shared variables they're not
fully general shared variables but they
useful for some tasks so I'll talk about
those more later
so first let's so just the RTD stuff so
here's an example of something you might
do interactively actually all this stuff
by the way is usable interactively from
the scala console too so this is just
going to show some of the operations so
here you know imagine you're you're
running a big web site and something is
going on and you have these terabytes of
logs being written we're going to load
just the logs into memory and then
search for different patterns in them so
we have a cluster here we have one
master node the driver and a number of
workers and we're going to start by
typing this so this here is defining an
RDD which is a file in the Hadoop file
system and this is a collection of
strings next thing we want to do is to
filter out the ones that that contain
Ares so that's the code for this so
filter is one of the transformations and
the code in red there is scholar syntax
for closure or lambda function so you
can really have any Scala or Java code
in there it's it's not just string
processing things okay and this gives us
back a new ID D representing just your
lines we can do more transformations too
for example maybe there tab-separated
fields and we pull out only field number
two so that that might be the actual
message in the in the record and then we
tell the system to cache these messages
so so that just the messages will be in
memory okay so after so up to this point
nothing's happened on the cluster yet so
all these operations are lazy until you
do an action and that's what we're going
to do next which is we're going to count
how many of them contain foo and so so
this is an action so now spark has to
actually run something to give you back
a number and it will basically come up
with a with a query plan based on the
transformations you did so far and on it
so first thing it does is it looks at
where the data is distributed in HDFS
and then it sends us to the machines to
process each of the blocks and each
block you know gives you back the count
of Aries for that one and it also saves
in in the cache the actual messages so
the partitions of the cached ID D so
that you have them for later so next
time if you do another way on this data
set let's say bar has is now the thing
we're searching for it spark will know
where these are cached and it will
schedule based on the cache placement
and you'll just hit those and return
back much faster and of course you're
not just limited to doing filters and
and maps you can do group bys joins all
kinds of other operations so to give you
a sense of the speed of this one of the
tests we did is full text search of
Wikipedia this is the 50 gigabyte data
set on 20 machines and it takes about 20
seconds with Hadoop or with spark with D
on disk data and it takes less than one
second with a cache data set so this is
really this is the difference you get
between memory and disk performance and
if you want to leverage the full
computational capability of your cluster
this is what you have to do each and
we've scaled this up just for fun as
well so we've scaled up to one terabyte
of data on a hundred machines and then
we can still do a full-text search in
about five seconds okay so the other
thing I wanted to show here is the way
this lineage stuff works so as I
mentioned before we recover from
failures by recomputing just the missing
data and the way we do that is by
building these lineage graphs so when
you do transformations you're like
filter and map spark has built this
graph of how to derive the DAO twitter
ID D and if it loses a partition like if
we lose say this one over here then it
knows that it just has to find this
Block C on a different machine
on your map and filter and rebuild it
and you can take of a mattress much
faster than restarting the whole job
okay so let's let's also show some some
machine learning applications now so
this is this is the simplest one and
it's it's probably too simplified for a
nips audience but it's still I think
gives you the sense of how you would do
other things so this one we're just
gonna do logistic regression and we'll
do it using a gradient descent so the
idea here is you have these labelled
points and you want to find a good
separating hyperplane and you can start
with a random hyperplane and then you
can compute the gradient to it so this
is a parallel sum something you can do
with map and reduce and then you can
move the line in the direction of the
gradient where a little bit and of
course you keep doing that
and eventually you get to a pretty good
solution so here's how to write that
inspark and this is again this is actual
Scala code that you could type in so
there's two pieces first at the top we
we load the file maybe the points are in
a text file we passed them to this
function to turn them into point objects
and we cached those and that's because
we're going to be using the points in
every iteration so we want to keep them
out and then we start with a random
parameter W and we do this map and and
reduce to compute the parallel sup and
that's that's essentially yet this is a
batch gradient descent you can also do
stochastic gradient descent there's a
version of map that lets you see
multiple elements at once for each
partition of the data set but it will
look very similar to this so how does it
actually perform this is a gonna test
we're and this is on about twenty ec2
machines and we compared logistic
regression in Hadoop against part so
what we see with Hadoop is that it has
the same cost for each iteration about
two minutes and that's because it's
doing the exact same thing it's loading
the data from the file system its
deserializing it and then it's doing the
math for logistic regression when we use
SPARC the first iteration took a little
longer but further iterations are just
six seconds and so it's about twenty
times faster
and of course the more iterations you
have the more this matters so the reason
for this there's actually a number of
reasons we did some profiling of this so
part of it is definitely the disk versus
memory but another big part is actually
just parsing the data just communicating
with the distributed storage system and
then deserializing the data and is it's
because in this logistic regression
algorithm the amount of computation per
byte is very small it's just a dot
product and so all these costs of just
talking to another system getting the
data back and converting it to your
format actually dominate is ninety I
guess about ninety-five percent of the
time here is in Hadoop is spent doing
that it's not spent doing your math that
you wrote down okay alright so as one
last example I wanted to show something
that's less data intensive and more
compute intensive and that's going to be
collaborative filtering this is again a
little simplified so this is you know
kind of Netflix challenge kind of thing
we have partial information about how
users rated some movies and we want to
predict the other ratings and the way we
can do this is we can model the ratings
matrix as a product of these smaller
ranked matrices a and B and we're going
to do alternating least squares so we're
going to start with random a and B and
then we're going to optimize a byte by
keeping be constant and doing least
squares and then we're going to optimize
B by switching it out and just just do
this until we converge so here's how we
write this just in normal Scala without
spark so what you can have you can read
your eating's matrix you can have a and
B starting out as just random columns of
the matrix and then the syntax at the
bottom we're just going to take the
indices 0 up to you and for each index I
we're going to update that user so that
will give us a new value of a and then
we'll do the same thing for B so this
update user will be our least squares or
whatever you want to do to update this
particular user here's the simplest way
to write it in spark there's a problem
with this but but this is
what you do so here we were using this
operation paralyzed which is taking this
Scala collection zero until you and
turning it into an ID D so that
different nodes will work on different
slices of the range and then we can do a
map and collect at the end is to take
back now we've computed all of the
columns of a but they distributed across
the cluster so collect brings them back
into an array that we can work with and
you can do the same thing for B so this
you know this looks fine and it will it
will on there's one problem with it and
it's one of the features of spark deals
with that so that problem is that you
have these closures and these functions
here involved are which is the ratings
matrix which is a very big object and by
default if you don't do anything special
Scala is going to package this this I
with each closure and we're going to end
up sending our th node multiple times
once on each iteration so so we're
basically going to spend a lot of time
shipping around this this sample data
are so what can we do to avoid that this
is where one of the shared variables I
mentioned comes in so we have a feature
called broadcast variables that lets you
if you have a large parameter vector
send it to each node only once and the
way you do that is you just wrap around
I with spark bad cast and then here you
use you have to call our dot value to
pull it out on that note but this will
make sure that it's sent only once and
it will use an efficient mechanism to
distribute as to all the nodes and in
this job this gave about a 3x
performance boost of within the naive
version okay we we actually you're being
systems people we did a lot of work to
optimize this so towards this was one of
the most interesting things we found as
people started using spark is they had
these problems like large parameter
vectors that we we didn't really
anticipate we were focusing more on just
making things very palpable so when we
first started with this we did the
broadcast by writing each file to HDFS
and Hadoop who can do this do it as a
feature and Hadoop called the
distributed cache that does this
we quickly saw that as we scaled this
communication became the bottleneck so
this is in in this example once we got
past 60 nodes we actually run slower
because we spend more time broadcasting
so we actually came up with a more
efficient algorithm for broadcast and in
commodity networks and data centers
calcio net that has essentially the same
cost that even at high scales and it's
based on BitTorrent it's it's so the
idea is all the nodes collaborate and
sending blocks to each other and it's
optimized for data center topologies and
we have if you're interested in this we
have a sitcom paper about this you can
also see so in this graph we're just
comparing it with other ways of doing
podcasts including vanilla BitTorrent
okay so so that's you know that was sort
of about spark itself and the
programming model let me also talk a bit
about applications we did using it so
spark is open source and a number of
people using it both in Berkeley and
outside mostly at some startup companies
in the Bay Area so some of the
applications we've done at Berkeley
these are applications that I should say
machine learning researchers have done i
I didn't do them but they're sort of
real applications that are there
research one of them is a traffic
estimation system for auto traffic
called mobile millenium and I'll talk a
bit more about that because it's a cool
application
another one is a spam classification
using stochastic gradient descent called
Monarch and another the company that
uses spark most heavily is convey ba
which does video distribution and they
do analytics and anomaly detection using
spark and we've done a bunch of other
things as well so let me just give you
an overview of this one because it's
kind of a cool application so mobile
millennium is is a project in the civil
engineering department at Berkeley and
the idea is that they want to estimate
our traffic conditions in the city very
accurately without having to install
sensors they're just by using GPS data
from some vehicle
that have GPS on them and initially you
know they wanted to use people's mobile
phones they also found out a good way to
do this is to just set up a deal with
say a taxicab company so you get these
data they get samples every minute and
what the data look like is a you know
you got a position for each taxi cab
every minute and you can immediately see
this is it's not super easy to do
inference based on this because the taxi
cab might have moved several blocks in
that time and so what they're trying to
do is just by looking at how far had
moved and how long it took to estimate
the traffic condition on each link of
Vodun in the city so this is just what
some some of the data looks like for
this this is from San Francisco and
there's there's a cab company in San
Francisco that they work with and so
each day they get about 500,000 data
points and you know the points I just
show you these cars moving so you can
see it's kind of cool you see more data
on the roads or people travel more but
then that's probably okay because those
are the ones that have traffic problems
and I don't know if you can see it but
in the downtown area there's also a lot
of noise because there are a lot of tall
buildings so this is this is what they
have to work with okay so so basically
the challenge is you have this data that
is noisy and it's access pretty sparse
one sample per minute and in addition to
knowing - predicting the travel times
you have to predict the paths taken so
in this example you know first you may
not be sure exactly where that GPS
rating came from because it's not in the
middle of the block there and then you
know the car might have taken this path
or it might have taken this path and
then in any case whatever it took the
output you want is something like this
where for each link in the network you
get a distribution actually of travel
times on that link so because because
the travel time isn't constant and they
actually have fairly complex
distribution that that they use based on
actually analyzing how hill traffic
works okay so how do they do this they
use the e/m algorithm to estimate both
of these unknown simultaneously so if
you knew the path of each car it would
be
possible to estimate that the travel
time or it would be easier at least if
he knew how long it spent on each link
and if you knew the travel time
distributions for each link you could
infer which links that the car took
based on how where it moved in that one
minute so they use essentially important
sampling for each pair of points they
generate some possible paths it might
have taken and times along the links of
the path and they weigh them based on
the current parameters that they have
for the links and then they update the
parameters and repeat so these are the
the E&amp;amp;M steps the two steps shown here
and both of these things can be done in
SPARC using some of the operations we
have so the first one is a map flatmap
is a map that can output multiple
elements for each input the second one
is a distributed group by or distributed
reduce and for the parameters it's
actually a pretty big vector so they
ended up using broadcast and the code
for it is the core loop of the code is
actually about 20 lines of code most of
the code outside of that is dealing with
the specific distribution they have for
their traffic models okay so so yeah so
here are some some results showing how
this scales this is running on a cluster
at nernst which is you know department
of energy computing center and they're
scaling from I think four machines up to
160 machines each with 4 cores and here
they have essentially almost linear
scalability and it's because the
algorithm is actually mainly CPU bound
so it's it's relatively easy to scale
the more nodes we do however have a
speed-up from using spark features so by
using caching they gained a factor of a
speed-up over the initial version and by
using podcast they could disseminate the
the initial data and the parameter about
four times faster so this is and there's
a paper on this at the symposium on
cloud computing this year that says the
details of what we did ok so so that's
does this application the other thing I
want to talk about in the application
is the this thing I talked about
initially about how I D these are a
pretty general computing model the model
so why does that matter so you know one
reason is that it's just interesting to
find a model that's sort of more
fundamental and and not specific to one
type of problem but the other reason is
that because we can support these these
other interfaces on top of our duties we
come that programmers efficiently
intermix these in the same program so
you can imagine you can have a program
boy you you do a map and reduce or
sequel query to load some kind of graph
into memory then you do some specialized
graph processing operation and then you
do some machine learning operation on
the result of that and with spike you
can actually keep the data and memory
across all of these and have fault
recovery across all of them and so on so
just some of the models we support
MapReduce and Diet I think you've seen
from the operations we have that you can
do map energies and goodbyes and so on
one interesting one here is prego this
is the graph processing model from
Google that initially looks very
different from MapReduce is based on
passing messages between the nodes but
it turns out that you can implement it
using rdd's and you can also go surely
implement all the placement
optimizations that Google did so things
like keeping the graph partitioned in a
specific way so that you minimize
communication you can use rdd's give you
control over partitioning so you can do
that and those are their models that we
have done as well some specific ones
that we've built we've implemented bagel
as a library in about 200 lines of code
it's called bagel for Berkeley bagel
we've built her loop all so this was
iterative MapReduce just as a proof of
concept and a more sort of extensive one
that we're working on now is hive hive
is a sequel interface on top of Hadoop
and - spark is called shark and so this
will give you the ability it'll be
compatible with Apache hive so you'll be
able to do sequel on big data and we're
going to add machine learning specific
functions into it so you can also do
different operations I say k-means from
the command line into
we talking and finally let me talk a
little bit about the implementation and
then see if I can actually show you guys
this thing in action so SPARC is
implemented on top of another system we
we built in the lab called mesos which
is a coaster manager that can
essentially efficiently share the
cluster among multiple users and
multiple different programming
frameworks so using mesas you can on
spark on a cluster have it coexist with
Hadoop or with MPI or other kinds of
applications and share resources
dynamically between them
SPARC is implemented from scratch it's
not a modified version of Hadoop or
something like that but it can reuse any
input source and any serialization
format that Hadoop does because it uses
the same API that Hadoop does to talk to
these systems so you get all of that
essentially for four feet to be able to
use your existing Hadoop data and even
though we we built in Scala we didn't
actually have to change Scala itself or
the compiler it turns out the way it
compiles the closures is sort of good
enough that we can use that to run in
parallel so this is great because we
don't want to be supporting some kind of
weird version of scarlet that's not
standard okay and that is that is the
one slide on implementation you can ask
you know for more details after so yeah
so last thing I'd like to do is try to
do a demo of this and we'll see if the
internet works for you but hopefully
it'll be okay so this guy
no don't huh
oh you did what's it yeah okay
yes right
okay let's just see whether this works
yeah I don't know let's see if this
lecture
seems to be
do you think it would be possible to fix
so I should i oh oh I see so it maybe
I'll wait a little bit maybe if there
are questions otherwise I have a video
of this too so I could do that but it's
it's cool I seeing it live I think yeah
yeah you think so okay so okay let me
just do that fun then so you have to
trust me that this is not a fake thing
and hopefully this will actually work so
this is so this is the the demo with
with Wikipedia I was mentioning and so
here I launched a cluster on ec2 with 20
nodes and this 50 gigabyte Wikipedia
dataset and so this is the SPARC
interpreter and you can type stuff into
it that standard scholar stuff and you
also get this variable called spark
context that is your your waited to do
stuff in parallel so the first thing we
did here no stop oh man okay this is
going faster than I can talk so first
thing we did that at the top file is we
reference this file in the Hadoop
distributed file system which is
Wikipedia it's a tab separated values
file and so this gives us back an RDD
containing strings okay and then next
thing we did filed that first this is
just a peek at the first line in the
file and we just get that right away it
just opens that line of the file and so
you can see this is a tab separated
format and there are sort of five fields
for each page on Wikipedia there's the
ID article ID there's a title I guess
this was alphabetically first there's a
date last modified there's an XML
version and you don't see it but there's
a fifth field that's a plain text
version of the article
okay so let's go on from here okay so so
here we're going to define a class to
presenting articles and we're only going
to work with two of the fields the title
and the text so this is how you two
define a class in Scala and next we are
going to actually compute these article
objects so we're going to use a map
we'll split each line by tabs we'll
filter will pick the ones that have
exactly five fields because some of them
don't have the last field the plain text
because they're things like images and
then we'll build a new article from each
one okay and now we're going to search
so we haven't asked the system to cache
this yet so it's still in on disk and
we're going to search how many contain
the rad lab that lab was the previous
lab before the amp laughs oh it was
reliable adaptive distributed systems
and you can take a guess how many
Wikipedia articles mention had lab so
while you're taking the guess this is
running or this has run stuff on the
cluster and now it's it's finishing up
all these tasks and okay the answer is
one so there's one Wikipedia article
that mentioned the lab okay next thing
we're going to do is let's try this with
the cache data set so let's first we
call articles that cache and we'll call
filter on it now since the data isn't in
memory at the first time we have to load
it again so it takes about the same
amount of time which is hopefully about
20 seconds that's that's how long it
took okay and it's running all this
stuff
and yeah took about 20 seconds and the
answer is still 1 and if we try it again
it took about one and a half seconds so
this is again this is the difference
between being in memory and the last
thing we're gonna do here is let's see
what what's the title of that article
that contains MATLAB and when we run
that but you can see ok it's the article
on Dave Patterson so yeah so I think
that's all there was for this illness oh
wait oh actually no that's let's see
this part too because so this is kind of
important actually so now we're going to
count how many articles contain Berkeley
because we're very narcissistic at
Berkeley we just search for ourselves in
in these things that's what the lab is
oh so about eleven twelve thousand using
Berkeley and next let's see Stanford
okay
oh and only ten thousand okay so there
we go so this this is one of the main
takeaways from the talk okay yeah so so
just to finish off thanks for listening
so we have we think ID these are pretty
powerful and efficient programming model
we also think they're cool tool to build
high level abstractions and so that you
don't have to worry about the low-level
fault dollar and stuff and this stuff is
open source it has a going open source
community and we invite you to check it
out
so that we focus under tax
and if you know all the blocks have been
one candidates - so we have
siding with today
instructions which you know which is not
okay so I said maybe that's 10 bucks or
100 bucks
you know the best again with with them
so he
master
if you want to implement something
yourself
celebration
yeah
yeah
yeah
and their customers because that would
be never be
the patient's inspire is that if you
need to spare this
because there is sequential stance so
and it should get a speed like this you
know so this is so you can still get
some speed
which visionary
examples for people so we have some
support
havens which addresses may be useful for
some things
libraries
a little bit</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>