<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Testing Engineering@Google &amp; The Release Process for Google's Chrome for iOS | Coder Coacher - Coaching Coders</title><meta content="Testing Engineering@Google &amp; The Release Process for Google's Chrome for iOS - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Testing Engineering@Google &amp; The Release Process for Google's Chrome for iOS</b></h2><h5 class="post__date">2014-11-03</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/p9bEc6oC6vw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">
CHELSEA: Thank you so
much for coming tonight.
Welcome to the Tech Talk series.
I'm Chelsea with Google.
We're so happy to
have all of you here.
Here's the brief agenda
that we have for tonight.
We're really excited to have
our speakers and welcome them.
Without further ado, I'd like to
introduce our technical manager
on the test team, Ivan.
He'll be starting our talk.
So thanks again.
IVAN HO: Hi, everyone.
My name is Ivan Ho, and thank
you for attending the event.
See a lot of new
faces, obviously.
There's about 250
attendees and a lot
of people who couldn't make
it either, so that's great.
Hopefully we have a couple great
presentations for you today.
I will talk to you
a little bit more
about what I do at Google,
and in general what the test
engineering culture
is as Google.
So this meetup actually
started back in 2010,
and it's been a
resounding success
with a number of
events per year.
Today's focus is around
how we do things on iOS,
and just a little bit about
the test engineering culture
at Google, which I'll
spend a few minutes on.
For those who are new here
to the Google New York City
office, we're quickly growing
and expanding since I joined.
And just to give
you an idea, this
is the second largest
engineering office outside
of Mountain View
for Google, so there
are a large number
of projects that we
work on here in this particular
office, and this building
specifically.
I don't know if
everyone knows this,
but Google does
own this building.
We do have a lot
of employees here
and also at Chelsea Market.
There is a lot of
interesting tidbits and facts
that are on a slide about
this particular facility.

So let's talk a little bit
about the culture at Google in
regards to test engineering.
As you can imagine,
there's a lot
of challenges that has to
do with test engineering,
and it's not an easy task.
And it certainly cannot
be a manual task either,
otherwise we would not be able
to deliver the type of software
we do at such a rapid
pace, and at such a scale.
So there's obviously
a lot of automation,
a lot innovation
that comes with it.
And just because I am in what's
called the test engineering
community, that
doesn't mean we are
fully responsible for quality.
I think there's a
stigma out there
that a lot of tech
organizations,
a lot of businesses, think
about quality assurance or test
engineering as, hey,
it's the QA guys.
It's their job to make
sure that everything works,
and nothing breaks, and there
are no bugs then, right?
Not true at Google.
It's a collective effort
from the folks who
write the software,
to the folks who
actually deploy the software.
It's a collective
effort to make sure
that we have a smooth process
for delivering software out
to production.
Everything we do, we have
to make sure we scale.
In order to do that, we can't
do that with just human beings
alone, but we also
have to do that
with a lot of innovation
and automation.
And so we certainly have to make
sure we spend our time wisely.
Do we spend our time making
sure every single issue,
every single use case
assigned has been tackled?
We have to balance
quality and velocity.
We want to make sure all users
get the features that they
need, and making sure that
all the critical issues have
been taken care of immediately.
So there's a good
prioritization process
on a per product basis
to figure that out.
As far as-- I'll just say it
from a generic standpoint,
the test engineering roles that
we have at Google, which I'll
talk about later,
we're very much
ingrained in every aspect of
the software development life
cycle, from the design
aspect to the implementation
and the deployment aspects.
So it's not the very tail end,
where, hey, here's a bill,
here's a binary, go
deploy, go try this out.
Tell me what breaks,
log a few tickets,
and then we fix the
bugs and we try again.
That's not how it works here.
It's a very iterative
process here,
but it's also a very
embedded process too.

And we always strive
for optimization.
So if we know that
there's a lot of things
that are very
repetitive, there should
be no need to repeat that.
Automate that as best we can
on every single platform,
every single language possibly
that the product supports.
And you can imagine
with a Google,
with a company of our size,
and with the number of products
that we have, that's
a lot of languages.
That's a lot of platforms.
That's a lot of browsers.
So you can imagine that
there's a lot of optimization
that we do here.

So I will talk about
two roles that we
have in our test
engineering discipline,
one is called software
engineer in test.
You may not have heard about
this term outside of Google,
and for those who have, that's
great, I'm glad you did.
But we are software engineers.
Software engineers
in test are folks
who are passionate about serving
our developer, development
community within the company.
So as a software
engineer in test,
my customers are developers.
I make sure they
have the right tools
to write code, build code,
release code, and test code.
Not so much as the
end user focus,
which is what a test
engineer is responsible for.
So I need to make sure that
I have the right automation
frameworks, the right debugging
tools, the right monitoring
tools, the right
level automation.
That developers can
feel very confident
that when they check code in,
things are not going to break.

I want to make sure that--
the product that I work with
is specifically AdWords.
So with a product like
that with millions
of users using AdWords,
we want to make sure
that our developers are
confident that they can ship
the software with a
very consistent process,
in a very regular fashion.
Now, the key thing for us is
that we listen to developers.
We understand their pain points.
We want to make sure that if
they are stumped by, let's
say, for example, if
they have to figure out
how to put this new
feature in, but scale it
at two million users,
how can we help do that?
If they don't know how
to figure that out,
we will help them
figure that out.

And I talked a lot about
automation already,
but for AdWords,
hopefully a lot of people
here are familiar with
it or have used it,
and the large majority
of AdWords customers
are using the web front-end.
That's what I focus on,
so there's obviously
a lot of cross-browser,
cross-locale related testing
associated with that.
And how do we automate
all that process too?
So we don't have human
beings that do all that work.
It's just-- we can't ship
software fast enough that way.
These are some of
the key buzzwords,
the key skill sets
that we look for when
we talk about software
engineers in test.
They have to have the
passion to write code,
whether it's in
Java, C++, Python,
those are some of the core
language that we have.
They're focused
primarily on making sure
that developers are efficient,
and that release engineers are
efficient, and test engineers
efficient at what they do.

We do a lot of
innovation generally
because if you think of yourself
as a software developer,
for those that are
engineers out there,
you're right in the feature.
You know that feature
very well, but you may not
know how it integrates with the
other parts of the application
or a third party system.
Well the folks in
a test community
have a much broader or wider
breadth of experience in that,
so our role is to
help them understand
how those things work, and
how to make those scale,
and how to automate all that.
And certainly, if we find a
need to help them automate
certain things, that's
where we step in.
We figure out what
needs to be done.
It's very rare that
developers say, hey,
go do this, or go do that.
We have to come up
with the ideas and say,
we think it's a great idea
if you do it this way.
Here's the best practice for
applying a set of unit tests,
or best practice for
an innovation test,
or having a continuous
build and test environment.
So these are all kind
of things that we
try to evangelize within
the development community.
We do-- from where I've
come from in prior jobs,
in prior organizations,
it's always
been that waterfall model.
You write code, have
a binary, next phase,
QA for two weeks or two months,
whatever the product is.
And it's not a very
iterative process,
and so we definitely
streamline all that here
with most of the products.
So let's talk about
the other role.
So now talk about the quality,
the user side, the domain.
So these are the people
that-- the test engineers are
the individuals,
and engineers too,
that also read code,
that also write code,
and they're responsible
for understanding
how a customer is going
to use this product.
And that's what majority of the
test engineers do at Google,
and they work on the
functional aspect.
They can show that all the
use cases that we can think of
are properly
validated, implement
into correct automation
to support that.
So they apply a
lot of the things
that the SETs
built, to make sure
that the test automation
is carried out.
And they're also
software engineers too,
so they need to
write code, they need
to understand code as well.
So while there are
some repetitive manual
processes involved in
some cases, if they
find that to be the
case, automated way.
Another key aspect that test
engineer's going to take on
is the planning and strategy.
So they understand
that this is how
customers going to
use this product.
Here's how it's
going to impact them
when a new feature comes out.
And so they figure out-- they
help the engineers figure out,
hey, have you
considered this scenario
when you write this new feature?
You added this new
widget on the screen.
What happens to all
these other widgets?
What happens if
you disable this?
Should these other
fields be grayed out?
These are the kind
of things that
are part of the process that
test engineers give out.

Some aspects of what a test
engineer-- core competency
is a lot of planning.
They are very data driven.
They focus a lot on
product quality, obviously.
They take the best in
breed of test tools,
and best practices that
apply to the product
that they're responsible for.
Just to give you an idea,
these are some core products
that we have at Google where
we have dedicated engineers,
SETs and TEs that are working on
all of these types of products,
and then some.
There's a lot more beyond this.
And obviously there's
a lot of presence
here within Google New York
where we have these two roles,
we're always looking
for good engineers
to help fill those roles
and work on these fantastic
products.
And then the
disciplines that they
focus on, they vary on the
right-hand side from release
automation, developer's
tools, to debugging
troubleshooting tools,
to security testing.
All those are very
important, and obviously
for certain products, some are
more important than others.
Some are doing a lot
more work on mobile.
Some are doing a lot
more work on back-end.
Some are doing more work
on front-end, like web
applications, so a variety of
technical skill sets involved.
And frankly, it's a
very different challenge
if you think about
what Google does.
We not only have to
worry about a product,
but the number of users that
we're potentially touching,
the number of languages that
we're shipping a software in,
and the scale that
we do things at.
So it's a very unique
challenge within Google
for any engineer, whether
you're a software developer,
whether you're an SET,
or whether you're a TE.
So that's what makes my
job fun here at Google.

So what I'd like to do is
introduce the next speaker.
She's been a long time Googler
for eight years starting
in the Google New
York City office.
She's worked on a number of
products, Google Health, Local,
Google Docs, Drive, and,
most recently, Chrome Mobile.
So I want to introduce
Lindsay to come up and speak.
[APPLAUSE]
LINDSAY PASRICHA: Hi everyone.
My name is Lindsay
Pasricha, and I'm here
to tell you a little
bit about Chrome
for iOS, our release
process, our development
process, and
everything in between.
I'll tell you just a little
bit more about myself.
I've been here since
2006, and Ivan's right,
I've worked on quite
a few products.
It's interesting here at Google,
you can be here for eight years
and feel like you've
not spent that eight
years on the same product.
You can switch products.
You're never forced to
switch products or anything,
it was by my choice.
But we also have 20%
time, so that's actually
how I ended up on Chrome.
I was doing 20% time when I was
on the Docs team for Chrome,
and it kind of turned into
something a little bit
like, well.
Maybe I should work on that
product for a little while,
so I ended up switching
teams that way.

Chrome for iOS.

Can I ask how many of
you are iPhone users?
Oh, wow.
Cool.
How many of you are
iPhone developers?
OK, still very cool.
I like it.
I like it.
Well, iPhone and iOS
development at Google
is definitely different
because most people
when they think mobile
and they think Google,
they think Android.
And so a lot of times
here at the company
they'll say mobile
this or mobile
that, they'll talk
about mobile development
and they'll completely leave
out that we have iOS products.
They'll not mention
Gmail, or Maps, or Chrome.
So for us, we're really
the underdogs here.
We're really
blazing our own path
and trying to do things right
by the user within an ecosystem
that we do not own, and we
are subject to the same rules
that an individual developer
is as a large corporation.
So in this talk I'll kind
of go over that mobile space
and some of the
challenges there.
I'll also tell you a
little bit about how
we develop our product, our dev
channels, and development flow,
and our testing solutions
that we have set up.
So, mobile development.
As I mentioned before,
Android is a big one
that people think
of when they think
of Google mobile development.

When we're working
on iOS products
at Google it's kind of
like, sometimes I'll
be using my iPhone
in the cafeteria
and somebody will walk
up and go, what is this?
Why do you have an iPhone?
And I'll be like, well, for
what it's worth colleague,
we have iOS products,
and we support them.
And we want people to have
a great experience on them.
And we don't want to just
leave that whole ecosystem
unrepresented by Google, so we
want to have a presence there.
And then they're
like, oh, OK, OK.
But you'll see it sometimes even
on mailing aliases internally,
they'll be like, who
has a iPhone here?
What?
And it's very interesting.
We want to have
a good experience
and presence on both ecosystems.
It's great that
Android's done so well,
and that we own that ecosystem
and we can be the primary,
from a Chrome perspective,
be the primary browser
for that whole mobile solution
is like a dream come true.
But still, many
users are iOS users.
Many users will stay
iOS users, and we
want to serve their needs
on that platform as well.
So those are our two
biggest ecosystems.
I will acknowledge I didn't
put Windows mobile phone here,
and for anybody who that
offends, I apologize.
I just don't have
a lot to say there.

So, development environments.
So for those of you that are
familiar with iOS development,
you're probably aware of Xcode
being a very major option
for your iOS development needs.
We use Xcode with a combination
of some internal solutions
as well.
Not everything's like super
proprietary and secret though.
We're using gyp, gyp files, to
manage our dependencies just
like a lot of
other projects are.
We use GIT, which is
our source management.

Have any of you used GIT before?
Any of you GIT developers?
How many of you are like SVN?
It's a hard transition to make,
I will tell you that much.
I was using traditional
SVN solutions here
at Google with our
internal products,
we weren't using GIT yet.
And then when I moved to
iOS, we were using GIT,
and it's a whole
different world.
My favorite thing
is that there's
no centralized
repository really,
and change lists, or CLs as
their traditionally defined,
don't really exist
in a GIT world.
It's just your hash.
Whatever hash your
change is assigned,
that's your identifier.
And it's been quite a
learning process for me,
but we develop on GIT.
We develop our milestones,
so you might notice sometimes
that Chrome needs to update.
It'll be our release
number 27 or 30 or 32,
these are called our milestones.
We'll use GIT to develop on
our 36 branch, for example.
And we use a lot of
automated build solutions
to make this go as
fast as possible.
So we use bots, which are
devices, physical devices,
that we'll have
set up in a room.
We have a lab and somebody will
manage those devices if they
like totally just become borked.
We have to have somebody
go and restart them.
It's a real joy.
It's a real joy.
But one of the great things
about iOS development
is up until now, it's been a
very limited number of devices
that we have to support,
limited screen resolutions.
Your iPhone 5 is your iPhone 5,
that's your screen resolution.
You don't have to worry too
much about variances there.
Same for iPad, you
have your iPad retina,
you have your iPad
without retina,
and you have your iPad mini
with and without retina.
So our matrix is not explosive.
Whereas my counterparts
on Chrome for Android
are in the pits of
hell because they
have to support so many
device configurations.
And they all are
launching somewhat
with Google's oversight, and
without at the same time,
so you can imagine how
sometimes they find out
about a device
that's coming out,
they're like, as
surprised as you are.
And they have to
quickly make sure
that they're
correctly supporting
the resolution and the firmware
specifications on that device.
And of course, some of
these hardware providers
are making their own tweaks to
Android, which can also end up
being catastrophic
for some of our apps
which assume some parts of our
bootstrapping won't be changed.

So, good for them.

But it will change
with this iPhone 6
that we're all reading
about in the news.
I don't know anything
you don't know,
but you know this
larger screen that we're
seeing leaked images
of and stuff like that.
Maybe my device support
matrix will explode.
We'll see.

So the interesting
thing about what
we're trying to do
on iOS specifically,
is we're trying to bring that
Chrome experience that mostly
everybody enjoys
on their desktop
to a device where there
are a lot of restrictions
in how you can operate.
And there are a
lot of restrictions
on what APIs you
can use, and what
services you can
provide to a user.
So having our
browser on Mac OS X
is a very completely
different experience
than having our browser on iOS.
On iOS, Apple is exposing
to us the UI WebView
and that's all you get.
And for those of you who
know about UI WebView,
it's a black box.
You don't get to ask
for more than what
is given to you in terms
of the public APIs,
and they're quite limited.
And they're buggy,
which is also cool.
And so we're working
with those limitations
against a browser that
is the native browser
for that ecosystem,
which is Safari.
Which has access to all those
private APIs that we'd totally
love to have, and makes very
good use of those private APIs
that we can't use.
So we're a bit at
a disadvantage.

Apple's not in a rush to enable
a user to change their default
browser, so we have
to kind of work around
that, and earn our users.
And our users have to
go out of their way
to open their content
in our browser,
and we appreciate that.
And we want to
make sure that they
have a good experience when
they do this rigmarole to open
their link in our browser
instead of in Safari,
because it can be
quite challenging.
So native browser versus
secondary browser, it's just,
we're really the underdog.
We're really trying to
earn the users trust
and desire to come back to us.
And there's some
really cool things
we're doing to try to make
that as appealing as possible.
So for you iPhone
users, have any of you
noticed that we introduced
this single sign on experience?
Yeah?
By a show of hands?
Oh, good.
Wow, well I'm glad
you guys noticed.
So one of things we introduced
in the past few milestones
was-- well I guess
it's not that recent,
but it was last fall--
was the ability to,
if you log into your Gmail
account, which many of you do,
because you can't really
do much with your Gmail app
without logging in.
So if you log into
your Gmail app,
then you have the
option without having
to type in your password all
over again, to log into Chrome.
And what this does is it
makes all of those tabs
that you had on your
desktop, available to you
on your iOS device,
and vice versa.
And this way you
have sync tabs, you
have all of your
autofill forms, lots
of good we know you
experiences, in that way.
This was a huge deal to Apple.
And of course they weren't
like the biggest fan
of us creating an
authentication layer,
but many other companies
are now doing this.
We're seeing this with
Facebook in a lot of places.
We're also seeing this for
smaller development companies
and game developers, where
they have multiple games
and they want you
to save your data.
And they want you to have your
data, the level you were at,
saved from your
iPad to your iPhone,
and other experiences like that.
So it's a valuable feature
that Apple initially
didn't want to enable because
they didn't want there
to be security risks,
which nobody does.
We don't want to
introduce that either.
But it just made managing
our authentication
Keychain so much easier,
and so much more secure
actually when you don't
have users thinking
they have to type in their
password all the time.
Things become a lot more
secure because they're not
just on paypaluk.csv,
or whatever URL,
not checking which
provider they're on,
typing their Paypal password.
It's better when we're
managing that Keychain.
We know that they
authenticated already,
and we're offering that
profile to that user
without having to
authenticate again.
So they weren't with it at
first, but we got approved.
And ever since then,
we've never looked back.
This has been a really
big improvement,
a nice experience for our users.
Another thing that
we have to deal
with as being just
another guy, just
another Joe Schmo
in the app store,
is we have to have our releases
signed and reviewed by Apple
the same way everyone else does.
It's not a simple process.
I'll go over a little
bit more later.
And we have to--
for many of you who
aren't doing iOS
development, it might
be surprising to find
out that you can't just
test your app on iOS.
You can't just install
it and willy-nilly
start playing with
it on the device.
You have to have a
developer account.
Then you have to assign
to that developer account
your device and its exact ID,
and then they'll approve that.
And then you can start
installing your dev sign
builds, but you only
get 100 devices.
100 devices to do that with.
And so for a Google, that
could be a little limiting.
Like 100 devices I could use
in our automated test set up
for Chrome, to be specific.
So Maps would get no testing.
Gmail would get no testing.
This is really limiting.
We've had to really
work with Apple
to say, listen, we
need more leeway.
And they've given us a lot
more than 100 which is helpful,
but it's still very
constricting and we
have to be very precise with how
we use our resources on the iOS
development side.
And then the other
thing to mention,
so you might have noticed
how on the app store
you can't get beta
versions of applications.
This is very
different for Chrome
as a product, where we live by
giving our users Canary builds,
giving our users beta
builds, and getting
that early feedback.
It's part of how we
manage our test solution.
So not having any of
this in Chrome for iOS,
really makes us the ugly
duckling in the Chrome group.
They always assume
that we have, they
as in Chrome Desktop,
always assume
that we have users
that are external,
using our beta and Canary.
They're constantly
shocked to find out
that that's strictly
not allowed on iOS.
So it's a bit of a struggle for
us, but we found a lot of ways
to work around it, which I'll
go over in the presentation.

All right, so our release and
dev process for Chrome for iOS.
We do run internal only dev,
Canary, and beta builds.
So this means that
other Googlers
can download and install our
pre-stable versions, which
doesn't end up being as many
users as you might think.
We still don't have
anything like what desktop
has in the millions of beta
users and millions of Canary
users.
We have a very piddly, like,
couple hundred beta users,
or something like that,
because many Googlers
don't have iOS devices.
So we can't take advantage
of our tens of thousands
of colleagues using our
products, because they
all usually have Androids.

So just to kind of give
you an idea of when we're
planning our app development,
we have a six to nine
week release process that begins
with getting feature approvals
for our milestone.
We do this across all of Chrome.
So this is for desktop.
This is for Android.
This is for iOS.
These all get approved
by our stakeholders.
So our VP of engineering goes
over each of our features
and will either OK it for that
milestone, or will say, no, you
guys.
You're looking a
little amateur here.
You need to go back
to the drawing board.
And we've had both happen to us.
When I first joined Chrome on
iOS, which was about almost
two years ago now, they
were getting sent home
from our VP of engineering
without their features approved
very often.
Because they were
going in unprepared.
Their feature had
not been enabled yet
on dev or Canary, which means
that even their teammates
weren't using that feature
yet, which you should probably
be doing six to nine weeks
before you're going to an app
store release with
millions of users.
So it wasn't surprising that
they were having a rough time.
So I had to kind of help
them with preparing for that.
And a lot of it came down to
being at the place of feature
development where you still
have polish and bugs to fix,
but you're not doing heavy
development on the branch.
Nobody likes to do heavy
development on the branch.
You have to get [INAUDIBLE]
every single CL.
It's really slow.
You'd rather just be able
to check in your changes
into the code repository
and see them instantly.
So everybody wants
to develop on trunk.
So that's what we kind of
moved towards is pushing back
that development cycle
and saying, if you're not
developing on trunk,
you're not going
to have a good time anyway.
So why don't we all just
do ourselves a favor
and get that development
done earlier.
And if your feature's not
ready, you should wait.
This was really hard for our
product managers to hear.
Our product managers were
not happy with this change,
but they're a lot
happier now that they
can count on a
release all the time.
We don't miss releases anymore.
One dirty little phase of
iOS, Chrome for iOS's history
is that we used to only release
odd numbered builds because we
were so often behind on
our development process,
that we'd never get to start
working on the features or any
of the changes for an
even numbered release,
and we'd skip it.
So when I joined, we had
released 19, 21, 23, and 25.
Those were our releases.
And I was like, you guys
have a good thing going
and I'm happy for
you, but I think
we'd like to just be
like the rest of Chrome
and actually release an
even numbered milestone too.
So we have since
matured in this way,
and we release all of
our even and odd numbers.
But there's plenty of
fire-drills and disasters,
so I can tell you about
those as we go farther.

So we do-- that's
about six weeks
of polish, bug
fixes on the branch.
This is where we're doing
a lot of our testing
for our next
milestone that we'll
get up for review by Apple.

During this process,
it's regression testing.
We're really going into the
details of new feature testing,
and looking at any of our
automated systems, which I'll
kind of go over and in
detail in next couple slides.
We're going over the results of
those automated tests as well
as manual solutions because,
for those of you who do,
or even if you work on Android,
mobile development in general
is not known for its
awesome automated testing
frameworks that are available.
Desktop is in a
much better space.

So yes, we'll usually
submit, after about six weeks
of development on
the branch, we'll
submit for Apple's approval.
And this can be an
interesting process,
something that we've had in
our app since we launched
can be emailed to us
as Apple, and Apple
won't email you the details
of why they won't approve you.
You get an email that
says, we should talk.
We should talk.
Call me when you get in, and
they leave a phone number.
So you never really
get a written overview
of what they reviewed, it's
a very black box process.
You're just kind of wondering
if they've started review,
and they said don't even
talk to us if your app hasn't
been under review for two weeks.
Don't ask us what the status is.
Don't ping us for, like,
are you almost done?
None of that.
They're like, don't even
speak, we'll contact you.
We'll let you know.
So we wait, like good little
app developers for two weeks,
usually.
And sometimes it will be
a question about something
that we've had in from
the beginning of time,
and it'll just be a matter
of confusion by the reviewer.
And we'll be like, listen, this
has been here the whole time.
Why would this be a
blocker for us releasing
a Chrome for iOS version now?
And they'll be like,
oh yeah, my bad.
Approved.
Or they'll be like,
let's talk to your VP.
So then we might
have to have our VP
talk to somebody else at Apple.
But since I've
joined, we've never
had a milestone
completely blocked,
it's usually just
a misunderstanding.
And we're able to clear
that up, and get it on
out to the public.
The fun part about getting
it out to the public
is because we go from that
piddly number of internal users
at Google to millions
of users in the public,
sometimes we find out that stuff
doesn't work like we expected.
There's so many websites that
do so many creative things
with JavaScript, with
HTML, we can't possibly
cover it all, even with the
best automated solutions.
So
Many times we will find
out that something is not
loading the way
that was expected
and we need to do a quick
fire-drill respin of this
build.
And those will be-- you'll
see them in the app store.
It'll be like, here's your
newest release of 36.0.2062.1.
And then two days later you'll
see, Chrome has an update.
That is when we've
had a fire-drill.
And we've been
freaking out and trying
to get something fixed for
the public very quickly.
Apple doesn't usually take two
weeks with those, thank God.
We can put a note
when we resubmit
that this is for a bug that
went out with our last release,
and usually they're
pretty kind and give us
less than a week of review time.

But these are fun.
Respins are something that
I'd like to say we don't do.
I'd like to say we're so secure
and mature in our development
process that we don't need
them, but we do, and it happens.

So a bit about our
automated testing,
so our bots are our salvation.
These automated solutions are
absolutely the biggest thing
that helps us get that early
feedback on our code quality.
So a lot of these builds, it
doesn't look really fancy.
You can see from the screenshot,
it's just green and red blocks,
and either you
passed or you didn't.
That's really it.
But we'll have our group of
the test cases, and one of them
might fail.
We have end-to-end tests,
we have unit tests.
And we have them running on
each one of our channels.
So on the dev channel we'll
have it as a pre-submit check.
On the Canary, it will be to
the merge set of all the changes
that are going into
trunk, and we'll
have our automated
test, unit test,
and end-to-end
test running there.
And then we'll also have
more end-to-end tests
that are running also on
the beta channel, which
is our closer to
app store release.
But these are just a godsend.
And we're lucky enough, as
the Chrome product area,
that we have a whole group
called the infrastructure team,
and they manage our devices.
So those devices that I
was telling you somebody
will walk up to physically and
restart it when it gets borked,
we have a whole team of people
that will take care of that
for us in Mountain View,
and it's such a luxury.
When I was on Docs,
they didn't have that.
So two years ago I helped
launch Drive on iOS
and it was such a stressful
thing because we had no bots.
They have no infrastructure
team on the Docs team,
and they're still working
on that by the way.
And when you only rely
on manual testing,
you're cycles can be limited
by how fast your manual testers
can return this.
And really your developers want
their feedback earlier than
what your manual testers
can give us feedback.
Our developers want to
know at pre-submit check
that an important test failed,
because that's during the time
that they're in context
for that change list.
They don't want
to have to go back
a week and a half later after
the manual testers finally
got that test case
and make that change.
So I can't recommend enough
finding a framework that
will work, and I'll talk to
you a bit about the frameworks
that we found.

So, we have the traditional
unit tests which are also--

Is there like a hand up?
I wasn't sure.
Sorry.
Don't mind the hand.
So we have our traditional
unit tests, which are great.
You want those lower
level tests to be checking
your expected variable values
at various states of your code
passing.
So we keep those, we add
to those, we live by those.
In addition to that, we
have end-to-end tests
with the KIF framework,
which is based off Iota.
This we found to be a little
bit more reliable, less flaky
than UIA.
For those of you who have
done any automated testing,
you are familiar with,
whether it's desktop
or whether it's mobile, your
end-to-end test frameworks are
sometimes at the mercy of your
load times, of your web pages,
and when text fields
and objects in the DOM
become visible to your operator,
which is the test framework.
So we have those problems
too in KIF, the same
as we would on desktop.
But we found that
with KIF, we can
rely on the accessibility label.
So even on desktop,
accessibility labels
are hugely important.
You should be doing them anyway.
This is something where you want
to be inclusive to every one,
visually impaired.
You want to make sure that
every element in your app
is accessible.
So the added bonus is,
if you make your test
framework dependent on
those, you'll automatically
be addressing the needs of
those who are visually impaired
and need those labels
on those UI elements.
So it's a great
thing to depend on
for your end-to-end test cases.
And these, for us, we'll
write for every new feature.
We'll write end-to-end
tests that are just
simply test cases,
test scenarios.
A user opens Chrome, logs
in, navigates to a website.
It loads, we verify some text
on it, and then we'll close it.
That's a very simple scenario,
but that's an example of one
that would be an
end-to end KIF test.
One of the funny
things I discovered
was that I wanted to navigate
to google.com on one of my test
scenarios, which is like,
pretty assumable here.
And so I'm navigating
happily to google.com,.
And I'm expecting
that, of course,
google.com will have
accessibility labels
on their search field, right?
Totes no.
They had no accessibility
labels on almost anything
on that page, and I was shocked.
Because usually we're not
even allowed to launch stuff
like that, but GWS
is special, GWS
is what we call
Google Web Search.
They're special.
And they are the reason our
company is, aside from ads,
so they get away with some
stuff that even they shouldn't.
So they have a pending bug.
We'll see when it gets fixed.
I'm sure my baby will
be born before then,
might be graduating
from college.
So yes, aside from
the end-to-end tests,
we have performance tests, which
these are just a set of URLs
that we're just running
through and comparing
the performance per run,
in the same milestone.
You don't want to compare
different milestones
because the changes
per milestone
might affect your run.
But you just want to watch for
incrementally over the release
cycle of your
current milestones,
that those numbers don't go up.
Those have been
very good for us.
They've also helped reduce
some of those fire-drills
that I mentioned where
there's some random URL that
doesn't load.
These have been very helpful.
And the screenshot test below,
same thing, list of URLs.
Taking a screen shot,
and we can have,
we can set our
screenshotter to say,
if there is this x
percent of change
in how this URL displays, we can
have a manual tester review it.
This is good and bad.
I love the idea, but
a lot of homepages
change by 80% between runs,
and it's all content change,
and ads change.
So we get a lot of false
positives with this,
but it's still
faster than trying
to go through all
the URLs manually.
So we kind of wait it
out we still take it.

So manual testing,
we do still have it.
We don't want to, but
we do still have it.
And any mobile developer
and mobile product developer
usually has manual testing,
even if it's just the developer
himself running through his
app and checking it manually.
You can't escape it.
We just don't have
the solutions,
especially on the iOS base.
So what are we doing
for manual testing?
Sometimes there's test cases
they can't be automated.
I'm trying to think of a
couple of good examples for iOS
specifically.
There are some user types
that are rate limited
that we can't run through
some of our scenarios as fast.
There's special account types
that have monitoring on them.
These are usually
enterprise accounts
that are owned by
admins, so if you've ever
heard about Google Apps
for schools, for example.
Some admins put
restrictions on actions
for some of their faculty
members, for example.
And so this would
be an example where
we couldn't do an
automated test,
because it would run
through too fast.
What else do we have?
The other thing that we like
to do with our manual testing
is, for those of you who have
worked with automated testing
whether it be desktop or mobile,
sometimes those test cases
get marked as flaky, because
again, sometimes your text
field shows up within
those five milliseconds,
and sometimes it shows
up in six milliseconds,
and your test will fail if it's
waiting for five milliseconds.
And a lot of times a developer
will mark a test as flaky,
without telling anyone.
So that's why we still
keep a manual check
in a lot of our
regression testing,
just in case the test
has been turned off,
nobody knew, but we want to
make sure it's covered anyway.

Test cases that haven't
been automated yet.
So for any of you who have
launched a product ever,
you'll know that when
you're in launch mode,
you collect technical debt,
you accrue technical debt,
to be more accurate.
We have this as well.
When we launched milestone 19,
which was our first release,
they did not automate anything.
They didn't automate typing
in the Omnibox, which
is like super basic.
So we've made up some of
that debt on the Chrome
for iOS team, but
certainly there's
a lot of things that
weren't covered.
One of the biggest things that
wasn't covered since our launch
until about four weeks ago
, was our tab switcher,
which is pretty important.
No automated test.
Why would you do that?
Apparently it wasn't important.
And the other thing,
is new features.
So a lot of times we're
getting that approval
from our executives, our
engineering VP, when we're
going up for a new feature,
you're automated tests
aren't written yet.
And sometimes you might get
through the whole milestone,
and not have them
written until the end.
It is part of our gating
process for development
of any new feature,
but sometimes it
doesn't happen
until the very end.
This is one of
the biggest things
that I could say we changed
on the Chrome for iOS team
since I joined is requiring
the ownership of quality
to be on the software
engineers plate.
That it is required for
them to be able to say
they launched a
feature such as search
by voice in Chrome,
which was a big one.
If he wants to have that
accolade on his Google resume,
he has to write the test for
it, or else it doesn't launch.
And this is something
very hard to get agreement
on from your stakeholders,
your executives, your product
managers.
They don't like to
not launch a feature
because it doesn't
have an automated test.
But they really like it when
they find out that it broke,
and we never released that
version because it never
made it past
pre-submit, or it never
made it into Canary because
of our end-to-end test.
So this was very difficult
to get buy-in on,
but I can't recommend it enough.
So just to give you
an idea for where
we exert most of our
effort for manual testing,
we don't do it early
on in the process.
We really rely on
those bots that I
was talking about earlier,
those continuous builds,
and having those be the
early signal on quality
for the developers.
So on the dev channel,
there's no manual testing.
On Canary, we'll do comparison
testing and CL Delta testing.
CL Delta testing is
somewhat shocking to people,
but for anybody who's
done manual test,
you'll know that if
you want to make a heat
map or a matrix of
where your risk is,
it's going to be in the change.
Which sounds really
obvious, but people
forget that all the time.
They just make test
sets and they go,
we're just going to do all
these regression test cases,
and that's where we're
going to live at,
and we're going to be great.
But you can lessen so much
of your load to manual tests,
and you can also
really bring up you're
confidence about your
release candidates,
if you're going through
the actual changes that
are in that release candidate.
One of the biggest things that
some teams fight tooth and nail
is giving release notes
to their test team,
or to anybody that's their
counterpart, about what they've
changed.
They fight release
notes to users.
But if you're tracking
what you've changed,
you can test around those
changes really tightly,
and feel very confident
about the quality of what
you're about to
send out to people.
Quite sad.
Sad story.

So on the beta, by the time our
product is reaching our beta
and we're in that six week
stage where we're just
doing those bug fixes and
polishes before we send
to Apple, this is where we
are doing that regression test
case set, where we're doing
some traditional test cases that
are outside of our CL Delta,
and we're also polishing up
our testing for
our new features.
And whatever changes are getting
merged in, those bug fixes,
we're checking the change
list for those as well.
And then for stable, so
once we send out that update
to you guys in the
app store, we're
just downloading the same
app that you guys are,
and running through.
OK, if I'm updating from
milestone 27 to milestone 37,
because milestone 27
we use client login
and then milestone 37 we
started to use that shared login
solution that I was
talking about earlier.
This is where we'll do an
upgrade test on production
to be able to test that
with production signing.
Because again, Apple
will sign your releases,
so that's not a test case
we can do at the dev stage,
because it's not
stable signed by Apple.
And we'll also just quickly
run through our new features
and make sure they're
working well in production.
Sometimes people forget to,
on their development servers
for a new feature,
they'll have it
checking that your user
agent is Chrome for iOS
and that you're a dev build.
And they forget on
the day of launch
to include the stable flag.
So that's pretty cool.

One of the big things coming
up with iOS 8 is TestFlight.
And I was super
excited about this
when they mentioned it WWEC.
They were like, we're going
to open up a beta user
population for iOS
developers and give them
that relief they've
been looking for.
And so I'm in a conference
room freaking out
during the keynote for
this because I'm like, yay!
And so what this is, is
you can use TestFlight,
so this will only be
available once iOS
goes gold master, which is what
they call when it goes public.
You'll be able to offer your
beta product to 100 users, so
again 100 limitation.
But it's not per company,
it is per product,
which was relieving to find out.
And the only other
downside to this
is that you can't have
stable and beta coexist
on the same device.
So if you do invite a
user to your beta program,
they can't run your stable
app from the app store.
So that was kind
of disappointing,
but other than that, I'm
very excited about this.
I'm looking forward
to the iOS 8 launch.

One other point is, since I
can't have coexisting builds ,
I'm not going to bother giving
our users a Canary build
because if it crashes-- I
don't know if any of you use--
can anybody raise your hand
if use the Canary distribution
of Chrome?
Some days, it's painful right?
Some days you have to IM
each other and be like,
don't update, don't update,
don't restart, don't restart,
because you can't type in any
text field randomly one day.
So this one, I'm
not going to bother
with planning Canary
builds for iOS.
It will probably just
be beta where that way
it's a little bit of a
higher bar of quality,
and we can make sure
that users won't
be IMing each other
not to update.

So just to give you an idea
of other types of testing they
we're doing, of
course we're going
through accessibility testing
and internationalization
testing.
These are very important to us.
We don't do full
sweeps per milestone.
I don't know how
many of you have
had the pleasure of
using VoiceOver in iOS,
but it can be quite slow
and also very janky.
So we do that.
We'll do full sweeps about
twice a year across the product.
And then outside of that,
we do the same thing
I was mentioning before,
focusing on your change.
New features, areas that
have had changes in code,
we'll do accessibility
runs on those.
And that helps us
kind of manage the gap
in between those
full test sweeps.

Same thing for
internationalization.
The upgrade testing
I mentioned earlier
is mainly about making
sure that any user
that updates from
release [INAUDIBLE]
builds that are earlier
in the milestone flow
to our current one.
Nothing is broken
for them, sync,
sign on, all of that good stuff.
Security testing.
We go through security review
with every single feature
at Google, and there's an entire
security team assigned to that.
So for our major features,
those will have the entire teams
review.
For smaller features,
they'll just
have a single security
engineer's review.
Geolocation testing,
also very important.
Some of our best bugs have been
versions of websites that are
only served to
certain geolocales,
so we can't repro it .
And even if the site doesn't
serve a different version
according to your geolocation,
sometimes, us as a company,
we have different versions
of Google Web Search,
for example, that are being
pushed to data centers
across a 24 hour period.
So one of our
fire-drills was that GWS
had pushed a change
to their mobile view
and how they served
our Omnibox searches,
and I couldn't repro
it, nobody in the US
could, but everybody
in India could.
And that's because their data
center had been refreshed.
None of our data centers in
the US had been refreshed yet,
and we didn't know that
GWS had pushed a new build.
So we were just like,
why do you see it?
Why don't I?
So we spent 24 hours
panicking, and then
finally the data center that
I was hitting got upgraded.
I was like, oh, that's
what's happening.
So those are always fun.
And then a user
type testing, which
I mentioned earlier, enterprise
users versus consumers.

So this is my overview.
I just yelled at you guys
about iOS development
for the past 30 minutes.

Do we have any questions?
And you can also ask questions
about Ivan's presentation
earlier, about the products
and test roles as well.

We'll start with the microphone.
So if you have questions, just
head up to the microphone.
AUDIENCE: Bob Gezelter.
You noted a problem with
testing on images because
of different web
server responses.
Any particular reason
why you haven't just
written the necessary code
to log what comes back
to each of the URLs and
stick it on your own server
so you can do run to run?
LINDSAY PASRICHA: We've done
those and we have those.
So what he's talking about--
AUDIENCE: That would
seem to cure the problem.
LINDSAY PASRICHA: Yes.
So there's two different
issues, though.
This is great.
So saving the HTML
of a certain page
and just making sure that
the rendering happens
exactly the same
between two builds,
is definitely something
we set up and we use.
But then what happens is those
sites implement a new view,
and that will break.
And so then those tests
don't help us in that space,
so we have both.
AUDIENCE: I was
just looking for not
being able to reproduce the test
you did on the previous rev.
LINDSAY PASRICHA:
It helps a lot.
And also it helps for
like sometimes they'll
have ads that won't serve.
These type of
things, we definitely
think that downloading a copy of
the HTML, the DOM of that site,
and then comparing it between
milestones is [INAUDIBLE].

IVAN HO: --responses stick a
rev number in them, please.
LINDSAY PASRICHA: Rev
numbers, fair enough.
AUDIENCE: Thanks
for your tech talk.
I was asking can you go into
more about the bots you use?
We've used continuous
integration
with [INAUDIBLE] bots,
but then in the past
I've dealt with Jenkins
and some other things.
But can you give us
more detail on what--
LINDSAY PASRICHA: Sure.
What we have set up is
there's a continuous
build system called Pulse
that we use within Google.
And you don't use that to
run your end-to-end tests,
you use that just to create your
build and to sign it properly,
so that you can run your iOS
instrumentation on top of that.
In order to run iOS
instrumentation,
you always have to have
dev signed correctly.
So Pulse will handle the
building and the signing,
and then we have what
we call the waterfall,
and you can actually find
this just on chromium.org.
They have the
waterfall for desktop,
we have the same thing for iOS.
And usually the same version
numbers, so nothing's
too surprising there.
But we have these different
bots that are running our tests.
So for desktop, they're
using different solutions,
they're using like
Selenium for example
to run their end-to-end test.
Just for us what we have
set up is, some of them
are Mac OS X boxes
with simulators open.
The problem with those is
that they always end up
with a lot more memory than
the actual real devices have,
and so we miss memory
issues on those, so we only
use those for certain things.
And then otherwise they're
running on actual devices.
We're using our development
instrumentation and the KIF
test to do those
end-to-end tests
and serving those results up.
And this is-- our
waterfall, we're
able to reuse what's
from Chromium,
it's simply just a web
solution for collecting results
of the test run, and
showing the passes.
So you name your
test run, unit test
for sign in, unit test for
Omnibox, and each of these
will show up in the waterfall as
green or red if they all pass.
And then the same
thing for our KIF test,
we'll label it KIF test
for first run experience,
and that will show up
if all the tests pass,
they'll show up as green.
Is there something else
you're looking for?
AUDIENCE: No.
LINDSAY PASRICHA: OK.
AUDIENCE: Thanks.
AUDIENCE: Hi.
Thanks so much for hosting
this awesome event.
I was actually wondering
about something
that you, sir, talked about
a little bit earlier on,
which is [INAUDIBLE]
people at Google can test.
So I was wondering how much you
and the Googlers that you work
with choose to specialize in
certain products, versus being
more generalist and testing
lots of different things.
IVAN HO: Yeah, that's
a good question.
So Sandy and I, we work in
the ads and commerce group.
And with the ads and commerce
there is a lot of products
that we cover.
Not just AdWords, but there's
a lot on the display side,
there's stuff on the ads
inside, there's Google Wallet,
there's payment, there's
all sorts of things, right.
But our roles are, in
terms of in general
as far as Google
engineers are concerned,
there's a lot of
flexibility and movement
from project to project,
from one technology area
to another technology area.
So if I wanted to do
some work on desktop
to mobile and that's
a whole other group,
that's an opportunity
that is available.
So we're not tied to a
particular product necessarily,
nor are we tied to a
particular specialty.
MALE SPEAKER: I think there's
two different aspects of this.
You as a test engineer or as
a software engineer in test,
are not tied to a
specific product,
you can go work on
whatever product.
On the other hand, we
find that as a whole,
the discipline
works better when we
are tightly coupled
to a specific product.
For example, you work
with AdWords, right?
So your team is very
familiar with AdWords
and can speak to their
needs better than I could.
I work on Publisher platform
and I can speak to their needs
better than Ivan can.
And I think that's one
of the key differences.
We don't have a Google-wide team
that does test infrastructure,
we have specific teams
for specific products.
IVAN HO: And one other thing
that makes our jobs even more
interesting, even though we're
in two different groups working
on two different products,
we share common experiences.
So if we're working on
desktop [INAUDIBLE] products,
web related tests in, or even
a similar ad serving platform,
we share the same--
we more than likely
have common problems
that we like
to collectively solve together.
There's no internal
competition whatsoever,
it's just a matter of, hey,
what's the best practice?
Has someone done
this with mobile?
Or has someone done this?
So for example, when ads are
rendered on the web browser,
how do we know
those are correct?
Well, how many ad
products we have that
does this thing, right?
So everyone has the same problem
across different products,
and we just trying
to find a common way
to solve this problem
collectively as a group.
AUDIENCE: How do you
communicate between
those different products?
IVAN HO: Yeah, so
that's the good thing
about having a
community within Google.
So in ads and commerce,
we have several hundred
of engineers around
the world that
have these regular discussions
around top challenges.
Or even just sharing ideas of
innovations and accomplishments
is a way to get the word out.
AUDIENCE: Thank you very much.
IVAN HO: Welcome.
LINDSAY PASRICHA: We also
have testing grouplets
where we exchange ideas on how
we've done testing solutions.
Grouplet's are groups
of people within Google
that talk about certain things.
So we have a
testing grouplet, we
have a Grayglers grouplet,
which is Googlers over 50.
We have Gayglers grouplet,
which is the LGBT community
within Google, so
lot of grouplets.
There is a testing
one where we'll
do demonstrations
of the solutions
we've put together for
each of our products.
Your specialized
knowledge is definitely
considered when you're
applying at Google
in that if you work
for an iOS product,
or you work on an iOS product
you have iOS experience,
they'll be targeting iOS,
the test managers who
have headcount, who are ready
to hire, that have iOS products
that they're owning.
But the reason why people
kind of need to be generalists
is because you don't know
which test manager's actually
going to bite for your
resume, so you kind of
have to be a well
rounded candidate.
So that's why I feel like
super specialized in only one
type of specific thing,
it's a little bit harder.
But when you have that
generalist knowledge,
a lot of people will
be interested in you.

AUDIENCE: So first, thank
you both for fantastic talks.
So you mentioned that
Chrome for iOS shares
is a lot of the instrumentation
that Chromium uses.
So how much of what you do
and work with is open source?
And how much of it
leverages Google's
proprietary internal
infrastructure?
LINDSAY PASRICHA: A lot would
be hellaciously generous.
We don't share as much as
Chrome on Android, for example.
The reason why we can't
is because of UI webview,
which I mentioned earlier.
So basically a lot of
our rendering engine
is completely irrelevant
in the iOS space,
because basically we can only
use the public APIs offered
within UI webview,
soon to be UK webview.
So when I say that we
reuse some of our parent,
or I call it desktop legacy
platforms, some of our legacy
platforms tools, I'm
usually talking about some
of our more common
infrastructure, which
is like those bots,
our waterfalls,
and how we're handling
our release process,
going up as a common
product with our VP
and having the same
evaluation of our features,
stuff like that.
But stuff that we
share from upstream
would be like sync,
bookmarks, net stack, some
of our net stack and
security, security checks
for fraudulent sites, autofill.
It's not that much, especially
when it comes to rendering.
It's disappointing, I wish
we could reuse more of it,
but it's just because we
have a UI webview limitation.
AUDIENCE: And how much of the
peripheral infrastructure,
things that aren't maybe shipped
with Chrome but are used in the
build process and
the testing process?
I'm sure a lot of iOS's stuff
is not translatable to Android.
LINDSAY PASRICHA: Not.
But really the best example
of those peripheral services,
is that infrastructure team
that will manage our bots.
AUDIENCE: Not open sourced.
You can't open source the team.
LINDSAY PASRICHA: That's
not open sourceable.
But I don't know
what the path to open
source looks like
for Chrome for iOS
especially because we can
only reuse some things.
I think that a lot
of people in Chromium
feel like they've open
sourced the upstream
and that that might
satisfy and be good enough.
I don't think that we
have any executive who's
burning for us to
make everything
else open source when we're
just using the UI web view.
But we have the same
problem actually
with Chrome on Android where
the reason why they haven't done
any-- they've started
their open sourcing,
but they're also running
into the same thing.
Well, this is open
source upstream.
What are the
compelling arguments
for doing the open source
work on a smaller platform?
For me, I think that open
sourcing legacy platforms
is not as important as
opening up mobile platforms.
But there's actually
a lot of controversy
around that
internally, some people
feel very differently
about that.
AUDIENCE: Thank you.
LINDSAY PASRICHA:
That was helpful.
AUDIENCE: Thank you very
much for doing these talks,
they're really, really helpful.
Just first a comment, whenever
I see an iOS developer,
I ask them if they have set
their triple tap on their home
button [INAUDIBLE]
accessibility for VoiceOver.
Just to see if they're
actually testing,
because that's the
easiest way to do it.
So the question I have--
LINDSAY PASRICHA:
Interesting point.
The only thing-- you can do
a lot more for accessibility
testing more than VoiceOver.
You can do contrast testing, you
can do magnified text testing,
and you'll be surprised at some
of the things that this breaks.
AUDIENCE: The
question I have though
happens to be around, not
just different devices
that you did talk about,
but different iOS versions.
So when iOS 8 comes out, we're
going to have extensions,
we're going to have
all these things that
aren't compatible,
backwards compatible,
and we have a lot
of legacy devices
that only support iOS 6.
Can you explain how you're using
testing to develop and also
release software that's going
to be compatible with all your--
LINDSAY PASRICHA:
Well one thing that I
think is important is
for everybody to draw
a baseline of what is your
minimum number of users
that you're willing to
respin your milestone for.
If that's 5% of users being
broken, if that's 3%, or 2%,
or 1%.
We have drawn our line
somewhere around 5%, which
for the jailbroken
example I said earlier,
actually jailbroken
users weren't
that high of a
percentage at that time.
But we still did it just because
those are a loud group of users
and they are the ones who will
jailbreak their phones to make
you their primary browser.
So we had to get it together.
But for iOS 5, for even iOS
6, if these platforms are not
going to be more than a
x percent of your user
distribution, then
you really should
think about how much you're
investing in supporting them.
Because having
device proliferation
and configuration, like all of
your different configuration
permutations, adding
for 3% of your users,
might not be as compelling.
For us, I draw the line.
I get to draw that line.
So for iOS 8, when
that comes out,
I get to look at the numbers
on the following-- usually
it's about two weeks, you'll
see your adoption in iOS happen.
And if iOS 6 is not above
that threshold for me,
I'm going to say, A, you won't
fix the bugs if I file them,
so B, we won't support it.
AUDIENCE: And Apple may be
supporting legacy versions
of Chrome in the future, I guess
if that's one solution to that.
But then you can't
also update that,
and if there's any
changes and you're
locked, especially if there's
any server components.
LINDSAY PASRICHA: Oh, yeah.
No, no no.
We have left users on
iOS 5 with a sync bug
that I feel immensely
guilty about, but it's done.
And to them, I am sorry.
AUDIENCE: That's
totally-- If you're
willing to accept that
technical debt, that's fine.
But what do you do in testing?
Is there any sort of automated
testing that you check for?
Deprecated method, or
anything like that?
LINDSAY PASRICHA: I mean,
we do keep our bots.
For iOS 5, for example, we
kept bots running for iOS 5
for I think it was six
months past my threshold
for any support that would
be like manual testing
support or our
device configuration.
So my compromise there was that
we will support bots running
on our Canary and
beta for iOS 5,
but if the tests don't
uncover those failures,
then we're not going to be doing
any additional test passes.
Because I'll tell you
what, the developers
won't fix the bug anyway.
They don't want to fix the bug,
it's not an interesting problem
to them.
And then it becomes
an argument of me
trying to say, oh, well
this effects 3% of users,
and that's not a
compelling argument.

Is that disappointing?
You look really disappointed.
AUDIENCE: [INAUDIBLE]
LINDSAY PASRICHA: It's true.
It's legitimately
a lot of users,
but when you're doing
that back support,
it is at the cost of
something in your future.
And so you have to decide
whether you'd rather
support those old 500,000
users who refused to upgrade
and are behind in a
lot of ways, so even
Apple's encouraging
them to like let it go.

So then that would
be at the cost
of us using those new features
like you mentioned, extensions
and sharing and stuff
like that, in iOS 8,
and we'd rather do that.

Last question.
AUDIENCE: About
estimation of testing
like as a percentage
of a new feature.
How much for a
percentage of that time
is allocated to testing,
and is it a standard metric
or is it something where
the amount changes depending
on the complexity
of the feature?
LINDSAY PASRICHA: The percentage
of time through that six week
period?
AUDIENCE: Generally if
you're estimating a feature
and say that the
software engineer--
LINDSAY PASRICHA:
Oh, like estimating
how long it would take to test.
AUDIENCE: Would you say you
have a standard number that you
just kind of slap on it
for testing, or is it
something where you
work with the engineers
to determine the task plan?
LINDSAY PASRICHA: It certainly
depends on the feature.
One huge feature that
really changed our lives
was the shared login,
and that's a feature
that was like comprehensive.
And I needed, I told them, the
first four weeks of our branch,
we're testing, and don't ask
me any questions about how it's
going, and I'll call
you, type thing.
So that was a huge one.
But smaller features--
there's an example in 36,
I think we had a new
first run experience.
And I was like, all I
need for that is-- I
look at our device
configurations
that we have to cover, the
iOS versions that we have
to cover, and just
look at the test cases
that we've written out for
that, and I kind of estimate out
that way.
And usually I can get pretty
close to the number of days
we need.
AUDIENCE: Testing
versus building,
is it kind of half and
half, 20% test, 80% test?
So if the feature takes
100 hours to build,
what would be the time that
you would kind of allocate
in your head for testing be?
LINDSAY PASRICHA: I don't know
that I have a good enough view
into how many hours
they spent developing
it to say that it would
be 50% of the time,
or something like that.
AUDIENCE: Don't have any
real conception of the time
spent building it--
LINDSAY PASRICHA: I
do and I don't, it
depends on the feature.
But I would say
it's not equal, it's
a lesser amount of
time to do the testing.
And it's certainly a even
lesser amount of time
if the developer
really understands
what their feature
impacts, and can give us
a bit of a footprint on what
other peripheral things we
should check in terms of
a [INAUDIBLE] map as well.
Then if they can
really reduce that,
then you're testing
time can be 15% or 25%
of what your
development time was.
But most developers don't
take care to do that.
And so we spend time,
like fucking crime scene
investigators,
trying to figure out
what their feature
does in this case, when
they could have just told us.
IVAN HO: I just want to
add a little bit more
to that because in
a different context,
in a web application space
like AdWords for example,
the product's
always running 24/7.
Millions of users are using it.
We're literally
changing the tires
on a car when it's driving
100 miles per hour.
And that still happens.
We don't slow down one
mile per hour at all.
So for our
perspective, that's why
we have automation in
place for a lot of that.
So there could be dozens of
developers putting new features
week after week, and maybe
half of those features
will go out the same week or
could be the following week.
So for us, we have a very
consistent constraint
in terms of how much
time we have to test.
So for us the onus
is on the developers,
and the onus is on
the SETs and TEs
to make sure that there's
proper automation in place,
because we can't let--
we have to release.
And some products release
on a very regular basis,
like we're talking
about within days.
AUDIENCE: It's about making sure
that people do it up front--
IVAN HO: Absolutely.
Absolutely.
So when you are involved
in building projects
like that at a global
scale, at a scale that we do
or even close enough, you have
to think along those lines
from an engineering practice.
LINDSAY PASRICHA: One of
the things I didn't mention
was it depends on how actively
the developer is still
changing that new feature
throughout our testing process.
If they're checking in
a lot of changes still
and we're in week
three of our branch,
then they've wasted some
of my testing iteration
because I'm going to have
to go back and retest some
of those configurations
and footprint.
So one of the biggest things
I could recommend actually
is for any product, especially a
client installable application,
the closer you get to
the time that you're
going to Apple
review, you need to be
way more strict about
which changes you will
merge into your
release candidate.
And I'm talking like,
take it seriously
because that is exactly
what will keep you
from launching on
time and will keep
you launching odd
milestones for a year.

Yeah.
Yeah, I mean, it's
collaborative.
I don't do it unless I'm pretty
confident that anybody that's
sane would agree with me, right?
People who argue
against it with me
usually are a little too
invested, or just not informed.

MALE SPEAKER: Yeah, one
more thing about that.
We find, at least
on my team, we find
that if they involve
us in the planning
process for that feature, we
have a much better idea of what
the affected areas
are, what the risk is,
and how long it's going
to take us to test.
When they drop a feature
on us like, oh, by the way,
we decided to do this.
We're going to
try and launch it.
Usually the answer is, we
don't have time to test,
it's not launching.
That's kind of
our blanket answer
because it's a much more
interesting process when
it's collaborative,
versus when it's
a one way flow of information.

IVAN HO: All right, I think
that was the last question.
Thank you for your
attendance, and thank you
for the questions.
FEMALE SPEAKER: We'll keep
you posted on future events,
so stay tuned in the
meetup community.
And thanks again to
our amazing speakers.
We really appreciate it.
We hope you had a great night.
</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>