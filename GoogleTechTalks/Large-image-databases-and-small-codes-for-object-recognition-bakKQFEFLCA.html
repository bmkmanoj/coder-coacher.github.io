<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Large image databases and small codes for object recognition | Coder Coacher - Coaching Coders</title><meta content="Large image databases and small codes for object recognition - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Large image databases and small codes for object recognition</b></h2><h5 class="post__date">2008-05-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/bakKQFEFLCA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm really happy to introduce Rob Fergus
he has a fantastic pedigree he's been at
Oxford Cambridge and Caltech at MIT and
now he's an assistant professor at NYU
he's worked with some of the best people
and computer vision including Andrews is
Sherman ketchup arona and Bill Freeman
and today he's going to talk about some
work that he's done with Bill and
Antonio taraba and a few other folks at
MIT on taking very very large data sets
image data sets and extracting
organizing in such a way so that it's
very easy to get similar images lots of
similar images and then seeing what you
could do with that and it's it's it's a
great exercise to sort of think with
what would you do if you had something
that had billions of images in it and
with a few microseconds you could
extract 10,000 very similar images so
without further ado Rob great thanks so
um okay this is going to be a paper at
CVP are you know many work in that Tony
oh but you know yo-yo and bill as well
this is sort of following from the work
at it as a postdoc wise at MIT so just
to clarify for everybody what I'm
talking about here I'm interesting the
problem for object recognition so you
take an image like this you know just
later pixels and you somehow want to
understand the objects in it you want to
understand there's an elephant here and
a person sitting on a sofa and things
like that okay so it sort of pixels in
high-level representation out that's
that's the overall goal what we're
shooting for and we're gonna talk about
today really is is the this notion of
the you know the internet contains just
billions of images okay and so here you
can just see a sort of random sampling
of images you might find and you can see
there's really everything okay really
spanning the visual world all kinds of
people scenes places objects and so on
and it just seems like you know you
could obviously get all this data for
free off the internet and it seems like
you're just an incredible resource maybe
we can use it for recognition somehow
okay and then you know at the moment
people haven't explored this too much in
the computer vision community
okay so we had a little sort of toy
experiment so we went out and gathered
about clothes on 80 million images using
the image search engines okay so we use
your one and the various other ones out
there to collect as many images as we
could we just took a list of nails from
wordnet which is an electronic
dictionary so we took all the sort of
non abstract nouns in English you can
just see some examples here and some of
them incredibly specific thing to
people's names and places and so on and
we just sucked as many emerges as the
engines will give us for each one of
these okay and one sort of detail was we
did stall them a very low resolution
format I'm not gonna go to that too much
today if you want to read more about
something implications of this you can
look at this tech report that we wrote
it's on our web pages but anyway with
this data set we were able to do some
fun things and and the catch one of the
caches of course with this data even
though it's free is that the engines as
you're all too well aware perhaps is
that the engines are given noisy outputs
you type an aeroplane and the images the
return don't always contain an aeroplane
and of course the only labelings we're
going to have for the images in our data
set again it's gonna be the noun which
we typed in to get it in the first place
so frequently the labor is gonna be
inaccurate and so any recognition
technique we're gonna propose it's gonna
use this data has to somehow get around
this issue okay doesn't and at the end
of the talk discuss a little bit how we
can address that but what I'm really
going to talk about is is that what what
I think is really exciting about with
one exciting were able to do with these
80 million was to show that very simple
methods like nearest neighbors actually
works very well when you have large
amounts of images so you know here's
your little queer image you want to know
what it what it is if you just have a
few thousand images in this case over
8000 then the neighbors yeah you know
there are some other people and so on
but there's quite a few weird objects in
there too okay you can see is you just
increase the amount of data you have
this is our sort of at 80 million you
can see by now you're getting really
people with the same sort of pose same
sort of background same color clothing
you know the arms are in front of them
and doing the same sort of thing so in
fact you know that the the content of
these images is really very very similar
indeed to your query okay
and so just by you know having a lot of
data and finding the closest neighbors
you know the hope is that these guys but
you know having the same objects in them
are going to do a fairly good job
perhaps you might be able to use them to
to tell you what this what's in this
image okay that's the basic idea and so
this is a very simple thing but you know
this is quite a different direction in
some sense to the way the most computer
vision algorithms work at the moment
when they really only consider you know
a few hundred training images and then
try and build some very complicated
parametric model so in this case we're
not in a really nonparametric we just
want to find neighbors that look similar
to our query and then figure out how to
transfer the the text labels we have
from these images to this guy okay so
that's the overall scheme I'm going to
talk about now one of the really
important things here of course is what
distance symmetric how do we somehow
decide that this image is close to these
guys and far away from all the other 8
images in our 80 million and and that's
it so in some sense are sort of vision
problem has now been morphed into sort
of defining this similarity metric
between in pairs of images okay and just
to show you that you know this is this
is just one example of people but you
know if you have you know landscape
scenes you can see that really but tell
me you're down here things have only
much much more consistent and they were
just a few 2000-now you guys and this is
80 million I'm guessing you pretty have
a couple orders of magnitude more than
this you can you know I mean simply
going to get more and more consistent
with a scene that you have up at the top
there and see me this is just another
another example okay so for the kind of
scheme I'm talking about oh sorry yep
okay so okay yeah so we filled out the
duplicates within each word but I'm
sorry the question was do you filter out
duplicates in any way the answer is that
we will track the duplicates within each
word but between words we haven't done
anything yet okay so there's some of the
experiment in the tech report where
we'll be very sensitive to that we have
manually removed them okay the
technology we talked about today
actually would let you remove the
duplicates very quickly but it's not
something we're done with a new to me
okay so just to come back so the scheme
I was talking about so this is this is
gonna be going to find your nearest
neighbors and try and use them to
classify your example it's clear the
primary things you need lots lots of
data
alright but then this raises the quite
important issue here how do you find
your neighbors quickly and if you you
know if you've got a you know 80 million
or billion images in your in for your
guys case that's actually a non-trivial
problem okay trying to find these names
quickly and of course implicit with that
is what distance metric you're going to
use and that's that's really somehow
loading in businesses metric has to
embody our understanding of the visual
world so it needs to be kind it's not
gonna be something just simple like
euclidean distance okay and then there's
this other problem at once you've found
those neighbors you need to understand
having to transfer them from your this
set of neighbors that you've found to
your query image and how you gonna do
this if the labels unreliable like
they're going to be in this big
collection of data from the web so the
the talk is going to be in too hard so
I'm just gonna talk mainly about how you
can do fast retrieval using some some
compact codes this is to get your set of
neighbors and then the second part i'll
show you how you can use those neighbors
to perhaps tell us in your query image
so start off with now what we're going
to use is the concept of binary code so
every image in our dataset is going to
represent it by a compact binary vector
alright and the idea is that images that
have similar content that is similar
objects in roughly similar
configurations want to map to similar
binary codes all right and the distance
we're going to sort of compute in this
binary code space is going to be a sort
of Hamming distance that's just the
difference the different the numbers of
bits are different between two codes as
you can see innocently these are
examples here and this is what our
borrowing an idea here that was
introduced recently by a rustling cough
and geoff hinton and they applied this a
text documents but we're gonna sort of
ply to the visual world and see how
works for images ok so just i understand
why these binary codes are so useful
well the idea is that your binary code
has described each image it's going to
be an address in your computer's memory
ok and that if nearby images you know
that have some sort of similar semantic
content amount to similar dresses then
you can find your neighbors with a
hashing framework ok and as you guys all
know you know hashing is a great thing
because you're now independent of the
total number of images in your data set
ok so just to understand that the
overall picture has a queer image you're
going to compute some semantic hash
function obviously this is the magic box
we shall talk about
death in a second but this is going to
map you to some address in memory and
then all you need to do is just flip
bits of that address to explore sort of
Hamming ball of a certain radius around
the query image okay and then you simply
just return the the images that you find
that have been mapped to these codes
okay and so knows your runtime is that
independent number of data points but it
only depends on somehow the volume of
this humming ball all right and that's
one motivation for keeping the codes
quite compact because you can clearly
see that the volume of this home ball
it's going to grow so exponentially with
a number of bits do you have in your
code and the radius of the ball okay so
as I said the key question is what's
this thing okay the semantic hash
function and but before I explain that I
just want to sort of motivate you know
again this idea of having a really
compact code so you know I'm guessing
here you guys probably have a few
billion images or index somewhere if you
you know if you're interested in doing
video there maybe you could have another
few orders of magnitude to this number
if you got a big pc maybe you've got to
be you know big pcs these days prob
about 10 to 11 bits if you have a big
cluster you know big some distributed
thing and getting you could have out of
fuel as a mountie to this number but the
point is the ratio is roughly similar
okay now there's sunshine making here is
that all your these binary codes need to
fit in memory okay if you're gonna do
some initial search if your sins you
have to go to disk at all your kind of
dead all right so I'm assuming that all
these codes are gonna have to fit in
memory now if that's the case then
you're left the budget you have / images
about 10 to the 2 bits alright and 100
bits it may seem a reasonable amount but
then just to give you a picture here so
your home had typical home photo.jpg
compresses about ten to the seven are
little tiny images are little thirsty by
32 thumbnails which are out as small as
you can make them before you really like
to start losing information are still
about 10 to the 4 bits okay so you can
see that you're still about tours mine
to you too big and so this semantic hash
function it has really has to be
essentially a dimensionality reduction
thing as well and so we got to squeeze
down each image to an incredibly compact
code you know as it were okay so just to
summarize them we want the this semantic
hash function has two wants to preserve
the neighborhood structure of the input
space
okay so you've got two images containing
cars need to map to similar codes in
code space we also want the codes to be
very compact you know less than 100 bits
or so and preferably fast to compute as
well so when you get a query image in
you can compute the code very quickly
and then do your hashing hashing scheme
yep and right now um no no I mean I so
I'm kind of assuming that you know if
you if you assuming each video has a
thousand frames you had three orders 92
that and assuming you have like a few
thousand computers available if you had
a big cluster then you would add you
know three or as a mountie to that
number so I'm assuming that you know I
mean I'm really interested in scaling
here to like you know all the video on
the web or things like that so I think
you know the ratios we're gonna remain
reasonably reasonably constant that's
the idea okay so we're gonna explore
three approaches here so the first is
locality sensitive hashing so this is a
quite a commonly used way to find
nearest neighbors quickly in high
dimensions I briefly explain a sort of
approach using boosting I'm gonna spend
some time talking about something called
restricted Boltzmann machines okay these
are quite a sort of new thing in machine
known each other people quite excited
about we're gonna play each of these
three things to each of these things
going to design for us somehow this
semantic hash function that is hopefully
going to as what do dimensionality
reduction as well for us and fulfills
our each of these criterion and we're
gonna see how they compare on some big
data sets now we're gonna cheat slightly
here we're not going to take the pixels
as input to the whole scheme okay we're
actually gonna in fact just somehow
choose a sort of hand designed
representation for all the images for
each of those three schemes okay so
we're gonna use something called a gist
descriptor and essentially what it does
is it takes the image it did to
completely doesn't use color information
it computes a sterile pyramid so it's a
like sort of several oriented bores are
different scales and then there's a
little bit of dimensionality reduction
just to take that down to 512 dimensions
for each image okay and you know
empirically people have sort of seen
that you know the l2 distance between 2
g's vectors seems to a reasonable job of
capturing some sort of notion of
semantic distance between images in
terms of
in terms of the content of the images so
this is going to be so for each image
we're going to compute this just
descriptor and then we're going to try
and reduce the dimensionality of this
and turn it into a binary code that's
how how things going to work now just
explain an accounting sensitive hashing
so this is an approach that it basically
relies on put upon taking random
projections of your data okay so you
hear some some data obviously is going
to be in high dimensions fiber than 12
dimensions but I'm just showing you two
here and what's gonna happen is you just
start throwing down directions and
projecting the data points down into
different bins in this case you've just
got a single bit so you're gonna have
two bins all right and then you can take
another projection and take quantized
that into dip one or two bins and and so
on you can continue going like that and
you can see it's starting to partition
up your space so when you get a new
query point like so you simply look at
find the points that that landing
somehow the same bin as it were the same
the same portion in the space all right
and it's an approximate framework but it
has a nice garan you know theoretical
guarantees and that's why a lot of
people you people use this now one
obvious problem of this is that it does
take random projections so in other
words if your data is completely a
uniform randomly distributed then it
might do okay but typically with image
data even though in even in the gist
representation the the vectors are not
going to be distributed evenly
throughout throughout the space so the
problem is that many of those
projections usual randomly will just be
off in the direction of noise okay so
they'll be kind of garbage and won't
really be carrying sort of
discriminative in from useful
discriminative information so what we in
fact do is we just give lsh a little bit
of a help by we compute the first sort
of npc a components and which gives you
a sort of low dimensional subspace our
low dimensional linear approximation to
the manifold within your within your
space of just descriptors and then we
take ground projections within that okay
this is just to give it a bit more of a
sort of a reasonable chance of picking a
good direction this this helps that
anything's out a bit
ok so we also tried a boosting approach
where we took this is a was an
interesting paper in icc be in 2003
where they had a variant of boosting
that could learn distance much that
learned similarity matrix and the idea
to try to train it you sort of give
pairs of images that you think is
similar and you give pairs of images
which you are different okay and that
forms your sort of negative and positive
instead of examples and then the little
you're going to boost is going to learn
us on ensemble of simple classifiers and
each in each each classifier is actually
gonna be just a simple sort of binary
regression stump that's just gonna
compare a single coordinate in in the in
the pair of images okay seeing the rib
of all below some threshold and that
thresholds going to be learnt and which
coordinate it uses is also going to be
learned to sew so this is a fairly sort
of simple adaptation of boosting to the
problem but I want to some more time
talking about these restricted Boltzmann
machines so what if these are actually
an instance of these sort of deep belief
Nets when I'm visit showing you here is
a single layer in what's going to be a
sort of multi-layer Network okay so you
have two sets of knows you have visible
units and hidden units the gist
descriptors are going to be put in here
at the bottom of the network okay and
then you they're gonna they're going to
activate these hidden units by a
symmetric fully connected make weight
matrix W okay and as we got in general
in general these nodes will be binary
and stochastic at the very input they're
gonna be actually real value because the
gist descriptors real valued but the
typically they're sort of binary in
stochastic and that the probability of
them being on depends on the some bias
on each of these units and then some of
the weighted sum of the activations of
these are the visible units okay and so
there's some sort of energy function
here that in learning you want to
minimize by adjusting the weights in the
in the connect the two layers of nodes
and also the biases on the on the hidden
and visible units as well so this is
this is the term with the weight it uses
the islands in the weight matrix and
these are the bias terms and so I'm you
can convert this to a probability by
just sort of you know exponentiating and
then normalizing out and this kind of
architecture has the nice advantage that
because
you don't have links between the nodes
here doesn't either you get conditional
independence on the for these
conditional distribution so that you
know as the probability this guy being
armed depends only on these guys not on
his neighbors all right and that lets
you learn like thats you learn and i
would estimate the weight matrix and the
biases using us or something based
scheme called contrasted divergence
alright so this is just a single rbm
layer and what we're going to do is to
stack them into a framework like this so
the input just descriptor fire and 12
dimensions is going to be the bottom
this is going to be art the first IBM
and then as week when i'm stack on top
of that the second one and then a third
okay and by we choose the number of
layers that's something we have to
decide on and as we vary the number of
hidden units we can sort of reduce the
dimensionality here okay and so we're
going to what we're going to do is in
fact this is in fact the architecture of
the system we use and we're going to
vary n at the top we're going to change
number of hidden units up there and and
so each unit here corresponds to a
single binary bit okay so you know just
factor in binary code out at the top and
the and so the training obviously what
we need to do is figure out you know
what weights can we put in here that
somehow preserve the the distances
between images between this the gist
vectors at the input as well as possible
despite having this for this compact
binary representation okay so training
proceeding to two separate phases okay
so the first one is pre-training okay
and this is essentially unsupervised and
the idea is here you're just going to
somehow estimate the weights in each of
these big blocks okay there's no label
information used and this is obviously
great because you know with the date
from the internet we don't have any
labels we can really trust and we can
essentially got as you know unlimited
amount of data that we can throw this
pre training phase and the intuition is
it's just going to move those parameters
to the right kind of ballpark such that
they sort of model the density the input
data fairly well and then there's going
to be a second phase of training where
we can make use of label information now
and we're gonna sort of forget about the
network being stochastic and just treat
it like a giant neural net and you're
going to essentially back propagate some
a function to tweak the weights in each
of the layers which is the ain't going
to move your your parameters to some
local minimum that optimizes whatever
whatever thing you care about so just to
say when we're not going to worry about
reconstructing the images at all okay so
these code up here are only going to be
interested in sort of preserving the
distances between images we could
imagine designing code up here they're
also allows to reconstruct the image
compactly but we're not we're not going
to do that at least in this talk anyway
okay so just to just talk about the
pre-training phase now so here you're
just input descriptors you do your
contrasted divergence sampling scheme to
estimate the weight in this between
these two layers okay and once and so
that means once you're finished doing
that every input vector is going to give
some activations on the hidden layer up
here okay and then you can take those
activations and use them as input to the
second layer okay so in this sort of
greedy fashion you're going to go up the
network training everything okay and so
you're gonna get and so on up this and
then at the top you're not gonna be left
with some binary codes so that's the
pre-training and then you're going to do
a discriminative fine-tuning phase and
so here the objective function i'm going
to use is something called neighborhood
components analysis okay that's what
this is going to do is to it's just is
to somehow tweak these weights so that
we really try and get images that we
think are close by at the input close by
in code space so just to so what we do
is we take for each training example we
define a set of neighbors in the input
space as in the nearby just descriptors
or perhaps using some human labels and
those then become essentially the same
class as a particularly particular query
point so in this little toy example
assume you got to got two classes of
data okay what I'm showing you here is a
visualization of their coordinate at the
output of the network now normally
that's going to be a binary bit when i'm
showing you here is a visualization of
somehow the probability of activation of
each of each units at the top okay and
what the this is the cost function we're
going to minimize just I don't worry too
much about the math essentially the
position of this thing is going to
depend on the
waits in your rbm network okay and if
you try to optimize this criterion what
it's going to do is to twiddle those
weights in such a way that you know
given a particular query point you want
to pull your images of the same class
closer to you and then you want to push
away ones that are that are close to you
but of a different class okay so this
way you'll somehow I'm trying to
preserve the the neighborhood structure
of the original input space and then you
pick a different query point pull the
guys the same class toward you and push
away with the other guys alright and so
if you do this it's good it's just going
to preserve the structure of the
original high dimensional space but this
is gonna be somehow in your cut this is
all happening your code space okay so we
tested on to two different collections
of image of data so the first one we had
to something called label me this is one
of these open source tools that lets you
upload photos and draw around objects
and attach a text tag to them we had
twenty two thousand images on that one
so we had 20,000 training examples and
two thousand tests all right and we had
the luxury of ground treat segmentations
essentially for all the objects to
pretty much every pixel in these images
is labeled okay such as obviously not
gonna be the case set for the web data
but you know we can lets us about things
a bit more closely now here because
we've got some ground truth labels on
the images we can actually define a sort
of ground truth metric between images
okay so in other words you can just look
at somehow the number of pixels that
share the same label in a pair of images
and that's gonna define few somehow how
close the images are and because we've
got these labels the NCAA back
propagation thing I just just talked
about is essentially a supervised
operation okay you're really going to
you know bill define your set of
neighbors in your input space based on
these ground truth labels now and then
for the web data we have 80 80 million
images collected from the internet now
we don't have any labels already trust
there so we give up trying to sort of
use the week labels inform it in the
form of the now attached to each image
and what we in fact try to do we ask the
the up in the back propagation we simply
try to preserve the l2 distance between
the gist vectors at the input so in some
sense is actually unsupervised because
we're not using any human human
annotations here at all
okay so but we're just assuming that the
gist vector is going to do a really good
job of describing the true distance
between images and we just try to
reconstruct preserve that that just
metric but just in a much more compact
fashion yeah and obviously we don't have
any we're all our labels that we didn't
have a noisy in some sense okay so first
I would look at some experiments doing
retrieval okay and then I'll talk about
how we can use it for recognition later
so just to show you some examples here
we're looking some plots in a second
here are some different query images you
have of you know street scenes and
sunsets and so on these are the
neighbors a few of the ground truth
neighbors as defined by the human labels
all right so you can see you know sure
enough look fairly similar the data set
20,000 images is you know a reasonable
size you'd hope to find face summer
images if you just try euclidean
distance between is that to a similarity
metric which is you know one that's
often used in retrieval you can see it
does a nice job for the sunsets where
the color information is very important
but when you actually have some
structure in the scenes it really breaks
down okay it seems to not understand
that this is a building at all and you
can see here as well Henry doesn't do a
very good job the gist descriptor which
is if you don't you can see this these
are the neighbors as defined by that so
this is this 512 dimensional space and
you can see that it does actually seem
to a reasonable job okay so you should
be looking at this this column is
somehow the gold standard all right and
you can see there does a reasonable job
of finding things now it doesn't it just
doesn't use color information so it
doesn't do quite so well on the Sun sets
and so on and the final three columns
here are the neighbors found by our rbm
network okay and so the number here is
the number of bits associated to each
image say this is eight bits so that's
one bite for each image you can see the
neighbors you get with that so they're
only 256 possible it states each image
can be assigned to this is with 16 bits
and this is with 32 okay so with those
two bits you can see that she does a
reasonably good job of approximating
they are matching the performance of
gist and it's still pretty close to the
ground truthing here seems so the color
information isn't in the gist of
Scripture so it's having a trouble
difficult time on the on the sunset
images though okay so just to quantify
that a little bit we can plot a graph
here
where we sort of pick us a retrieval
sweet draw we've taken for a query image
we find like the foot closest say 2000
neighbors okay and we look at what
fraction of the true 50 nearest
neighbors are contained within that set
of two thousand okay and so you can see
for phallus H and purple it's a fairly
small fraction just over twenty percent
for if we use the gist descriptor which
is this high dimensional thing quite
expensive to compute this is an exact
distance somehow we get about sixty
percent won't know a bit less than sorry
55 and then the rbm code yawning green
seems to get about sixty percent so you
can see here the rbm code is actually
exceeding the performance of the gist on
descriptor even those only got 32 bits
and somehow the gist the script was the
input to the whole network okay it's
because you've got these ground truth
labels that can in the back propagation
phase that you really sort of really fit
the true metric rather than the gist
distance just to show you these two
curves here so one thing you might say
is what about KD trees right there
pretty good at doing approximate nearest
neighbor schemes fast and indeed you can
get them to be pretty fast this the
fastest the best one we found was David
lowes implementation he uses it for sift
in 128 28 dimensions and I so grabbed
his code and hacked around a bit bit
with it and tried it out on on these
just descriptors now they might be fast
but they're also pretty approximate so
you can see they're actually you know
quite a lot worse than the the curves
we're finding I also try to just reduce
the dimensionality using PC a little bit
before I did the the KD tree just in
case that helped and it did help but
it's still not as good as good as our
codes we've got so you can take a
cross-section now you can now just you
know say ok and for a given number of
you know images retrieved say 500 what
fraction of the truth 51st what traction
the 50 true news neighbors do you
actually find as you vary the number of
bits in your code you can see without
ice age it improves but pretty slowly
we're both the boosting and the IBM's
you actually do pretty well you can see
it's a bit disappointing that the green
curve does it seems to saturate to some
extent once you get to 32 bits and
there's probably more of a limitation
due to the the actual input
representation than anything
else but you can see by 32 bits is
already actually exceeding the the
performance of the gist of the gist
metric okay now if you tried the same
thing on the web images 30 bits doesn't
seem quite to be enough so the data
looks a little bit more looks to be a
lot more there's a lot more straight you
know it's a lot more difficult there's a
lot more of randomness and some sense in
there and the 30 bit code doesn't seem
to do such a great job so you can see
they're here now as you increase beyond
30 bits there's quite a distinct
performance improvement with things this
is for the IBM's if you try the lsh
approach you can see that it's really
call it works with 256 bits lsh you're
about comparable to 30 bits with your
IBM code so just to show some examples
here are some query images bear in mind
for the web data we don't have any
ground truth okay so what we're trying
to do is match the gist metric that you
see in this column here all right so so
just things that this image is close to
these guys and that is you increase the
number of bits in the cut in the code
and the rbm you can see that the
neighbors do indeed become more
consistent with these I mean that 30
bits there a bit flaky but certainly by
128 or so they're doing a reasonable job
sure yep that's right so we're not using
them at all of this at the moment yeah
well we're going to use them for
recognition later on actually we're not
using in the retrieval phase but they're
not using any way for that I'm sorry yep
I'm not for not for not for this one the
boosting did actually do call it worse
on the web data we did that's why I'm
not showing it the IBM's seem to win out
fairly fairly convincingly when we tried
it on the on the web data so just to
show you the timings now so we actually
did this on a subset of 80 million but
because computing some of these timings
was hard but the key point is that ok so
slightly complicated table so this is
the label mate images and the web images
if you try let's just look at it so if
you do if you compute the binary codes
and just do brute force search it does
actually you can do it pretty fast on
the label me but it starts to become
expense
if with the web data now if you can keep
things under about 64 bits you can
viably hash use the hashing scheme I
described in which case you have this
nice thing that really is even with my
terrible coding it's still you know a
few microseconds in terms of look up
time all right that's a--that's
a--that's-- that that assumes you've got
the gist vector okay but once you've got
the gist vector sorry that's not show
you so that's once you you could be the
gist vector takes a few milliseconds
propagating it through the network so
once you've trained the thing is just a
feed-forward multi-layer neural net
that's very fast that's again a few
milliseconds and then the actual
retrieval of the neighbors at least with
the sort of with in this case for a 30
bit code with the radius I think I come
exactly how many bits radius it was
probably three or four bits look up
timers in the sort of micro second mark
okay so I talked at the end I'll talk
about how we you know obviously gonna
mention training or IBM's on a different
implantation perhaps jpg coefficients or
MPEG coefficients so you wouldn't have
this sort of annoying pre-processing
phases that might slow you down the key
point I said this this this time is
obviously somehow in a constant given
the size of the data set yeah sure
Wow
okay right so that's a good point so
actually we did tight so I went to this
is one of the other sort of KD trees oh
sorry the question was why is the spill
trees slower than the brute force for in
this label me datapoint here so we did
actually go out and try and find these
different implementations this is andrew
moore's group at CNU they had some an
implementation of a spill tree the real
problem is is that the gist vector here
is in this case 512 dimensions okay and
and these these KD trees typically break
down in high-dimensional high
dimensional spaces and so the thing was
just cripplingly slow we did find an
alternate version which was very fast
but very approximate which is what I
showed on the previous on a couple of
slides ago so that's why it's lower
basic that it gets Toki murder by the
dimensionality of things okay yeah so
does somehow the important numbers on
this slide okay so just as it this is if
you if you have labeled fully labeled
data like the label me which is
obviously a bit of an ask you can
actually some pretty funky things for
recognition okay so the idea is hedgerow
Wells's of this guy so heads your input
image just show you the different sets
of neighbors here you get with your
ground truth metric with the first two
bit IBM code and with your gist
descriptor ok so this that number
there's number of bits you know just the
script Turkish fiber and 12 dimensions
of real valued numbers and i'm showing
you down here the sort of the segment
that the human labels for each of these
are for each of these settle retrieve
neighbors and what you can do so for
this little image here each pixel here
has been decided its label has been
decided upon by sort of majority voting
framework on this set of neighbors i'm
finding not just 16 of them but perhaps
I things about 50 or 70 or so of them ok
so you can actually you know just by
transferring the pixel labels from your
neighbors found by rbn you can actually
do a pretty good job of labeling in fact
the the input image and you can see here
you know the car even though it's very
small and sent to the image you can
still sort of successfully labor and
label it and so on and this number here
tells you what what fraction of pixels
in this image are correctly labeled i'm
using the label we now of course this
relies upon the fact you actually have
on these these nice set of labels and
you might have that for some
some sort of peekaboo mem JS or things
like that but you're not you know you're
not gonna have it in general all the
images on the web this is just to show
you what you can do in that case all
right so just as a quantified things a
little bit here I'll just let's just
look at this plot here so you can see as
you increase the number of bits in your
code this is the fraction of pixels
correct you in your in the previous
slides images as it were and so as you
increase number of bits you can see it
gets close to the the number of pixels
correct you get from gist but then it's
somehow doesn't quite beat it the black
line is the ground truth metric ok so
you can see that you know just doing
this this just voting based on the set
of neighbors even if you have the ground
truth set of neighbors available you
still don't get a hundred percent okay
so you definitely drop something
something there so this is of course
averaged over many many test examples
this is not just a single test example
ok so are now I'm gonna spend a bit
talking about how you know you found
these name is very quickly using your
hashing codes but how you're going to
then actually due recognition how you
gonna say what's in your query image
alright so the problem you typically
have is this so there's your query image
and then you found very quickly using
your semantics hash codes this set you
know the these set of images all right
and they have these are the nouns
attached to them these need the nouns
you used to query the engines in the
first place to download them and you can
see that frequently they're completely
weird nouns all right and sometimes
maybe the objects you thought were close
are actually completely different to the
query and so you have to somehow
overcome thatís you and then you once
you have to understand that well you
know Grover Cleveland I guess is some
sort of person but in fact you know we
don't really care about the fact it's a
Grover Cleveland image we just want you
know we want to say this is a person
okay so we're somehow we have to
understand there's a difference in
semantic meaning between this word and
person okay so this is a sort of one
potential way of doing it so we use
wordnet all right and I were denounced
this less called its electronic
dictionary if you type in were like
aardvark it doesn't just tell you sort
of you know that it's a noun in fact
gives you the sort of parents of the
word if you like so tells you know it's
a mammal a vertebrate
an animal and organism will weigh up to
entity so everything is an entity and if
you sometimes the words might have
multiple senses and so on you know it
defines a grass structural relationship
between all words in the English
language okay some I'm not talking in
the visual world here now I'm talking
purely in the sort of textual world and
what we do is we simplify this by taking
the most common meaning okay so we take
the first sense and just the words on
the diagonal here and you end up with a
giant tree structure which relates all
the nouns which we'd gathered images
okay and so just to sort of give you a
quick visualizations what you're seeing
here is that is a heavily decimated tree
of all the some of the animals that
we've got okay so you can see you've got
mammal percentile carnivore in a k-9 dog
wild dog fox and so on so it gives us
somehow it tells us how the words relate
to one another all right now we see this
is decimated I show you the whole thing
it will be completing comprehensible
okay so what we do is so here's your
queer image you want to classify I said
I should make clear at this point for I
me although it is possible to localize
the object than the image for what I'm
talking present in just them in the next
few slides we're just going to attach a
single text label to the image okay so
we're gonna classify the global content
of the image we're not attempting to
somehow segment the image or localize in
this case the person within the image we
can do that but I'm not gonna talk about
it now so here's your query image you
compete or just factor you run it you
then Ram it through your IBM network get
a semantic a binary code you do your
semantics hash and get your neighbors
here all right now for each of these
each of these guys each these images is
gonna have to each of these guys is
gonna have a noun associated with it
okay and that noun is going to define a
branch in that giant word entry that was
just describing okay so for example this
isn't we won such branch all right so
you know entity object person ignores
I'm scientist chemist now each of these
guys is gonna have a branch just like
this and of course it'll have different
depending on the label attached is gonna
be a different branch and what we can do
is we can somehow merge all these
different branches just accumulating the
number of votes at each node in the in
that's in that giant wordnet tree okay
so as everything's an entity entity gets
pretty much all the votes you can think
of okay so maybe there was 75 or
something neighbors in the first place
so 73 of the images have a branch which
includes the entity node now as you go
down the tree you can see the votes
start getting dispersed amongst the the
different branches of a tree okay that's
what I'm showing a red here is that the
branch which has the most votes ok and
the moment we're doing something very
simple we're just taking you know one
image one vote nothing what
sophistication that you could think
about adjusting the vote based on the
distance or based on the rank of the
image when it was fun and so on and so
forth I'm not doing any of that it's
just very simple okay so the idea is
that it you can now sort of classify
different semantic levels by just
deciding where you want to operate if
you're interested in a very general
classification and is it a living thing
or some sort of you know land form or
you know somehow that you can simply
take a slice and just look at the number
of votes of each level in the tree okay
if you want to pass people more specific
you could slice down here and you can
see that in fact person seems to be
getting a lot of votes over over the
other classes all right and so you can
see this is another example this is a
woodworking vice something you might
clamp piece of wood in the neighbors
here you'll notice actually don't really
contain any I you know woodworking vices
but they seem to be mechanical objects
when you wrote in the word net scheme
you'll see indeed that the this is in
fact the ground truth this is the branch
belong to this image that we've labeled
just for evaluation purposes you'll see
that this branch agrees fairly
fundamentally with with it with the
branch you obtained by voting okay all
the way down to device something that
okay now that the whole purpose of this
is that you can you somehow boiled off
the labeling noise I mean any the
consistency within the labels will
manifest itself somehow in the most
common branch and he says that's the
hope and and it's a sort of failure
automatic scheme and it lets us somehow
relate the fact you've got two images
one labeled person one labeled sir
arthur conan doyle let you understand
that in fact they are basically contain
the same thing that is a person so for
example if you wanted if you wanted to
build a classifier to decide whether you
know it's a personal not the thing you
all you need to do is in fact look at
the number of votes that land or the
person note okay that's that's all you
need to do to
to classify with an image contained a
person or not alright so just to show
some results now now one class which is
particularly some are prevalent in our
data of 18 million are people so you
know blots that people take photos of
their friends and the family and so on a
stick them on the web on web pages and
so when we download you know all that
there are many nouns which have it as a
parent the person node and for each of
those nouns you're going to get lots of
images containing people so in fact we
found in our 80 million about twenty
three percent of all images contain
people we want to sort of measure them
by hand and the key thing is is that you
know when you have twenty three percent
of 80 million you really do have an
awful lot of images of people such that
when you have a crew much like this the
crew images are really the set you know
same pose same sort of you know skin
color and hair color you know if you've
got some weird image like this like two
people hugging you know you can find out
that images which are very close right
you know two people you know hugging
each other with the same black
background and maybe even sort of
similar cover colored clothes all right
so this is so in some sense you know
that's in space of all possible it
natural images this part of the space is
fairly densely populated just by
collecting 80 million images or so and
so so if in other words if any class is
going to work with this kind of nearest
neighbor thing and then voting it's
probably gonna be people so to test this
out we collect use a separate set at the
separate test set of images a thousand
images from out of Easter that you're
showing you some of them here we talk
about person in downloaded the images we
got we have high resolution versions
available of them so we can run other
algorithms on them to evaluate as by
means a comparison as well as sort of in
a little thumbnail versions if we can
free so want as well and just to make
clear there's no overlap between our our
big 80 million training set we're gonna
use for nearest neighbor look up and
this test set okay so the task is was
there a person in the image or not which
is you know something that perhaps you
normally use a face detector to the side
I'm sharing a precision recall curve
here if you just the raw altavista
ranking starts off pretty good for the
first leg maybe web page or maybe less
than that even and then it drops down to
some sort of baseline performance okay
so you can see that why not there are
actually got off people returned so even
just a random
algorithm would achieve a performance
like this if you try running bilder
Jones on the sort of thumbnail size
version 32 by 32 you end up with this
magenta line here the if you weren't on
the high-resolution images you get the
green curve and if you run it on our
tiny images that is we compute the ville
30 x 32 thumbnail compute the gist
descriptor from that and then compute
the binary hash codes find the neighbors
do the voting we get the curve and bread
okay so you can this is a 64-bit code
all right now this was now the key point
of this code was not trained but there's
no it was never trained recognized
people it was just trained to preserve
the distance between gist vectors at the
input of the rbm okay so there's nothing
class specific about this code and you
can see that it's a little bit lower
than the bureau Jones detector for low
recall but a high recall it seems to do
better okay so this is you know complete
sort of dumb nearest neighbor method
here actually doing pretty well compared
to a sort of one of the sort of sort of
leading face detectors that the right is
out there all right so this is slightly
doesn't quite film the rest at all but
just to show you that you're not
constrained to remove instance we in a
previous paper we use several different
hand design distance metrics so we
haven't really run these experiments yet
with the binary codes but we're sort of
doing that at the moment and on the
x-axis we're increasing the number of
images in the data set and we're doing
and we're going to try and classify
three different groupings of classes
these are very general things like food
or or geological formation these are
much more specific things like bird or
insect and so on and on the egg sacs on
the y-axis you can see here some sort of
average air under the ROC curves this
tells us how well we can classify each
of these classes individually the number
by each word here is the performance we
achieve when we using the this metric D
shift which is this some sort of hand
design metric with 80 million images and
you can see that the curse emits a bit
there a noisy but they seem to be sort
of going up fitting on offense of linear
fashion the red dot is you know I
probably was you know I'm not sure how
many guys you have exactly but if you
extrapolate this out you can see that
for some of these some of these semantic
levels you would actually be getting a
fairly good recognition performance
based on this sort of nearest neighbor
voting scheme okay you can imagine if
you were to train our BMS with class
specifics class specific information
you'd really hope that they would
actually do much better than for example
the previous slide you can imagine
training your IBM code to try and find
people rather than just to try and
preserve the similarity metric for all
images in which case you'd hope the
performance will improve so in some
sense this is the red curve would seem
to be sort of a lower bound on what
you've worked really possible to achieve
and similarly yeah you can imagine
training for sort of common classes on
our BM code and so that we haven't
plotted it here we just were running
experiments at the moment ok so just a
sort of start concluding here so one
important observation so we're kind of
predicating everything on here on the
assumption you can really get millions
of examples of a given class you want to
recognize now if you take the label me
dataset and you plot out if you order
the classes by how often they occur in
the data set and then plot plot that out
you can see got a sort of parallel
relationship so in other words there are
very few classes like ten percent of the
classes account for the vast majority of
labels in the in the database and you
might think this is some sort of quirk
of label me and other people have found
sort of similar things okay in other
words things like people things like
cars things like you know sky and road
and things like this occur incredibly
often in our visual world all right but
there's this massive tail all the way
out here there's all kinds of weird
objects that you see now and again like
like aardvarks and sort of hedgehogs and
things like this all right now there's
no conceivable way so our recognition
scheme is only ever going to work in
this kind of framework here where you
really will have millions of examples of
a given class available to you there's
no they're never going to work you're
never gonna be able to find like 10
million you know aardvark images or
something like that okay so there's
still a place for the sort of
traditional methods that people are
trying to think about and vision that is
building quite complicated parametric
models that can train off road to a
small amounts of data but I think that
definitely is an interesting Avenue here
like if you do have just absolutely tons
of data trying these sort of simple
nearest neighbor things with perhaps
some sort of relatively clever distance
metric to find your neighbors okay so
yeah so it's a sort of basically saying
that but I've talked about
sort of panacea to sell computer vision
but perhaps it does have a role for
search for certain classes okay so just
to wrap up so show you how you can build
some pretty compact codes here for
retrieval right i mean 32 bits is pretty
small and you can the big machine or
with a cluster it then becomes feasible
to store really you know huge numbers of
images the number of bits you need to
get a decent material performance does
seem to depend on the data set and so
this is something we definitely feel
there's a bit of a lot of room for
improvement I the codes were getting
with the IBM's don't seem as efficient
as they could be and that's one thing we
were sort of exploring the moment to
understand what's going on there I think
the gist descriptor input is a bit of a
fortunate kind of hack I mean we did it
because we had trouble training the rbm
straight off pixels they're kind of
tricky things to train these are BMS I
mean you can imagine these more turn
representations maybe you know so you
don't have to even decompress the image
like MPEG or JPEG coefficients and if
you could train things like that then a
might work better and be it roars to be
faster as well but i guess the overall
message really is um you know you can do
fun things lots of data and you guys
must have you know lots of the images
and you can you know your your expertise
is in retrieval so it would seem seem a
kind of natural method for you guys to
try out and obviously i mean i've talked
about still data here but video it seems
an obvious application give them at it
you know the thing the hope the methods
are designed to scale up to sort of
billions of billions of images and so on
ok so I'm that's it done
so did purchase sort of it second of
this layer cake structure where you know
first thing you do is you take an image
and you aggressively downsample it and
then you turn it into a Justice crafter
is this on yeah so it sort of had this
layered approach where you know first
you take an image and then you greatly
down sample it and then you take the
gist district scripter and then you run
it through an RBM and then you try to
change the coefficients in the rbm so
that it it preserved neighbors yeah and
so I'm sort of wondering that you know
if something goes wrong at any one of
those layers you've sort of you've lost
a lot of descriptive ability and I mean
maybe you're hinting at this with digit
the JPEG coefficients that is there
definitely is nothing actually right so
I mean what the beauty buttons rpms they
are sort of bottom-up approach okay and
if you can understand how to train them
correctly which is not a trivial issue
then you'd like to stick them directly
as close as possible to the raw pixels
yeah right and so then that's something
we definitely do and I took agree and I
think um you're quite right there just
you know having is intermediate
representations in there is kind of a
waste of time actually yeah so I mean
you can't compute the gist escrip to
direct on the high res thing you don't
need two necessary on for the flesh to
buy 32 we did it in these cases because
all our web data happen to be a 32 by 32
anyway but um yeah i mean the slight the
tricky issue is essentially that that
first layer of your IBM if you don't use
the gist thing has to be a sort of
sparse over complete basis and if you
have a you know it's they can get i mean
if the image is even first 2 x 32 color
that's 3,000 dimensions vector for each
image and then your spot of a complete
basis prod needs to be about 10,000 so
you have a 30 million element weight
matrix you need to learn and geoff
hinton can do it I can't haven't figured
out how to it yet so so that's very much
work in progress I think you'll see
people actually getting the album's to
work directly on pixels or perhaps some
sort of you know jpg grandpa coefficient
space yeah
I
um you mentioned this issue that many of
the classes may have say less than a
hundred training images but at the same
time you have the semantic hierarchy so
for example 100 images of a hundred art
of aardvarks and andy nurse maybe our
complimentary and could be used in some
yeah that's actually right so um right
so that's one of the beauties about the
coordinate thing it lets you aggregate
you're relatively small set of images
for each of those obscure nouns well
there's different species of aardvark
and then you got I don't want our bike
is what is it it's a time is it a mammal
and maybe maybe if you go up to the
level of mammal you can recognize okay
you some indica mechanics got like four
legs and all quadra pet or something and
that's exactly right so you may not be
able to recognize it so that's true so
that gives you this interesting thing
the word net thing if you don't have
examples you're forced to move up to a
little higher semantic level so your
fault to classify mately more general
sense then if you have like in lots of
images of cats cuz always you know
people like to put the pictures of their
cats on the web so you could probably
you know do quite a good job recognizing
even individual species of care
potentially if your IBM did a good you
know was trained well so one quick
question Jose is in the label me problem
it seemed like the boosting performed in
pretty close to our VMs right I'm the
labeling the label me problem that you
had yeah boosting was pretty close to
our BM right right so the question might
be is in the web problem are you really
not optimizing the right objective
function is not the right loss function
for well I should someone one of the
reasons n times that yeah so quite
possibly I mean so Antonia with me
Antonia would independently trying to
you know he was doing the boosting out
in the IBM's and we're trying to sell
fight it off to see who would win and we
had a fairly good you know I mean
he did antonis pretty Compton with
boosting so so he did a good job
implementing it in terms of whether
optimizing the right metric or not
that's certainly very possible the
difficulties if you don't have nice
labels telling you what's really close
then it's tricky now you know so what
you really want a big collection of data
from the web that has labels and maybe
you guys have it but we don't so for
that if you've gotten his big IBM's you
know you do need a reasonable number of
training examples to train on I'm just
wondering how much how much do you think
NCA kind of you know how much did I
contribute to quite a lot so if you did
try valuating the retrieval performance
after the pre-training face and the
performance did improve
when prince was quite a lot worse than
when you did the NCA backpropagation
having said that the NCAA is a bit ugly
and I'm not sure that's one thing that
doesn't necessary scale because
fundamentally that said that's an N
squared if you have n training samples
you've got to somehow compute you know
the error function depends on the Civil
N squared it was you need to compute
somehow the distances to every other
neighbor and that quickly becomes quite
tricky when you've got you know 20,000
training examples you know it starts to
kill you a little bit so you so until
you might want to talk about NCAA for
something a little bit a little bit
somehow better on foot I mean there are
lots of ways of learning similarity
matrix and we use just NCAA which is
just one possible approach yes question
was how many images did you guys try on
ok so the pre-training we slept and we
had about then I'll know it wasn't that
big as I say because we kind of we had
the advantage of this just input
descriptor now if you had the gist if
you didn't have juice you'd have to this
big over complete first letter I was
talking about because we didn't have
that we got away with this fairly small
network so we trained on I took we took
something from picaboo and we took some
from label me and I think I think the
pre-training phase used to use about 70
or hundred thousand images with what you
know the gist vectors thereof and then
the back propagation used we didn't use
either the N squared is here I just as
just mentioned it was difficult to
actually pump through all 20,000 labeled
you know label me images on this and
same with the web date oh I definitely
couldn't pump through all 80 million in
the back prop so I to mention it then
squared yeah so so so I think you know
we got to like a fairly lame number I'm
afraid probably like you know hundred
thousand some like that or possibly I
mean I a bit Sean I could have pretty
one foot we've had more time for the
super deadline that particular been a
bit more but yeah I mean that's one
that's one thing so that would be we
didn't unfortunately plot graphs of how
they have over 20 performers have
changed as we varied the pre-training i
think is fine i think that make a huge
difference I think the backpropagation
face having more label examples there
big difference yeah how critical is the
gist descriptor in your in your work I'm
trying to get a sense for you know what
would a even a dumber descriptor behave
as your side of your database increases
say you know color is to grab comparison
or something even you know McGee written
so so you can you can um so there were a
few little indication so it was it this
slide so that's this is the somehow the
this gives you some measure of some this
is got this is the the dis some
performance measure using like human
labels okay so you're never going to
beat this in some sense and you can see
that we drop well it is quite a
substantial fraction just based on on
the gist using the and of course in this
case this is the yeah this is in this
case that just is the input
representation to the rbm and so you can
see that the things having tough time
kind of beating it essentially how about
a different target matric so yeah
instead of you know just something else
yeah we interesting to try that um right
i mean sony was a one thing for example
definitely could include would be color
information i mean i'm sure that would
probably help a little bit but I see
what saying it just wop out of one we
didn't try it every find be fun to try
sort of it's interesting my asset is
where where where should I put my effort
in if I wanted to reduce this works I
put it in you know in the descriptors or
instead should I put it in and it looks
to me that the sort of the RB I know
you're the model that that is used
behind it seems to be the most critical
part but I sort of oh yeah what I mean
it i would so I shoulda nice what I'm
trying to do is actually it throw away
the just completely and just go straight
to the pixels or the JP coefficients I
think you know if the IBM's do their job
properly I mean this is essentially what
you know people like McRae Leo and yan
are doing is trying to Train really good
you know feature presentations for
learning images that first and if you're
sort of v1 v2 kind of layers of your
visual system you know you that's what
you want to be learning in these first
layers of the RPM and instead of taking
these handcrafted things that are kind
of had pretty you know basically guesses
at what's good you should you should
learn yeah so I mean I'm advocating
learning over hand design
yeah they skip soon yep just a second
ago you said that you know if you added
color information this might be a great
combination right well i think it was
anyhow i mean as a sort of physique
about weight I mean the consideration
right so now so you wanted to do this
with just in color yeah now how would
you integrate this in your hashing
function because really what you want is
you want everything that has this
texture and everything that has this
color right but these are two separate
metrics but they all be able to learn
those correlations right I mean that's
how you just you just stop it in the box
saying they're not correlated right so
say your class like why me automobiles
right you're not checked independent
either i mean the color and the and
content and other than the texture so
much so I'm saying that you're talking
about your text you're just features
you're talking about another feature but
I was saying this hash this whole system
makes sense very much sense described
and going from a single type of feature
descriptor the gist in this case and
you're saying you do with JPEG I'm gonna
know because so if you could imagine
just concatenate into things together at
the input and I mean the all the rvm
does is try to it tries to learn the
correlations between them so you know as
you as you move up the network it's
increasingly kind of the hidden units of
each each node become increasingly sort
of independent to one another so that
you know all the correlations between
the variables the input somehow should
be should be learnt in those in the low
low layers of the network so I mean
that's a very mind that for example the
first layer your weight matrix is dense
so you can have a single hidden unit
there which would couple elements in in
both the both your input descriptors and
so on so it should be it should be able
to learn those i think i get i get
immediate everybody Eric point shiny
making like let's say cars are
independent of color right so basically
once you've mixed color in and now all
the cars in your set will be further
away right cuz they're different color
well known but you ain't seeing your
NCAA metric woods have labels saying
that it would say that you know the NCA
when you're the back prob that would be
be in there no I mean within for certain
things color and important so maybe a
global global rbm work for label me
Lewis color is yeah I mean
asteria there's always this fixed
relation between classes and you know
you can create this one feature vector
and it can just sort of split it up but
I'm saying they're very frequently
you're looking for something as a
combination of both it seems I'm i guess
i'm not quite I mean and the IBM is just
this giant box that tries to learn I
mean just just as they're a structure
between coefficients of so I mean if we
if they're for certain types of input
think you know for example color is
useless and for certain types of input
the color is good I would hope that get
you know get when you do that when you
do the back prop you would have
different sets of neighbors like your
car neighbors would all be different
colors for example and so for that type
of input the thing would understand that
it shouldn't it should ignore the
certain types of texture car like
texture it shouldn't care about the
color information and throws it vice
versa tie so I'm not quite sure sort of
thing that's a yeah I didn't quite oh
it's a problem we can talk offline it
yeah me yeah sure</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>