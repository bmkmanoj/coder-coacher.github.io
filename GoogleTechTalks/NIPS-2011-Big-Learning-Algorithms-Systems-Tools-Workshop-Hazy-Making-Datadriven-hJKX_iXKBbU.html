<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Hazy - Making Data-driven... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Hazy - Making Data-driven... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Hazy - Making Data-driven...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/hJKX_iXKBbU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">bright and early awake at this early are
so there is no better way to start off
the morning than Chris right here he
works on large-scale data management
he's worked on streaming data
probabilistic database and recently a
lot of both in machine learning and so
I'm really looking forward to this talk
and I hope you guys fix American
excellent so first thanks to the
organizers for inviting me it's been a
great workshop yesterday was fantastic
and thank you all for getting up early
in the morning so Samir mention i'm
actually a database guy so i have
slightly a different take on the big
learning kind of thing and what i'm
going to tell you about in the meat of
the talk is about my umbrella project
which is this thing hazy whose logos up
at the top which is trying to build and
maintain large scale statistical
applications so not just construct them
for 11 kind of task but maintain them
over time before I get in that I want to
give you some thoughts though about from
my perspective what's interesting about
the Big Data craze or the big learning
craze and I think it's a little bit
different than maybe some of the things
we've heard so far so if you haven't
heard big data is the future everyone
knows this even the Economist has this
thing that everyone likes to throw up
which is actually the term data to lose
which i think is about 20 years old but
everyone's excited about Big Data you
know things like Hadoop have taken off
to an unbelievable extent people are you
know there's a lot of momentum around
and everything you want to do you just
do it on Hadoop and all the sudden it
scales there's also a bunch of interest
from the sort of corporate side IBM is
betting very heavily that Hadoop really
is going to be the answer to everything
that's what the big insights project is
really all about and even classical
database vendors like green plum or
Oracle who have had these great
relational databases for 30 years are
jumping on the Big Data train so big
data looks like the future but being
kind of a contrarian when I look at this
it kind of makes me a little bit
uncomfortable to be honest with you and
the way I think about it is it's clear
to me after thinking about this for a
while that big data is absolutely
awesome if you're a vendor or you need
consulting dollars if right now you want
consulting when you say hey I do big
data someone will give you money but
it's not really clear to me that the big
is really the heart of the problem okay
so to understand why I don't think it's
really the heart of the problem I want
to drill into really what does it mean
to be big what
big data so how big is big so is it a
gigabyte well doesn't really intuitively
seem all that big but there have been
some you know work that you've seen that
says oh we're doing large-scale stuff
it's it's a gigabyte and I'd actually
argue that there is interesting stuff in
this thing but it fits on your phone
right i mean a gigabyte is not a huge
amount of data if it's on everyone's
phone in this room probably but
yesterday we heard a talk about
embedding processors and i would argue
that the talk of putting you know
convolutions and neural networks inside
chips and putting them down there and
dealing with gigabytes scale data shares
a lot of things with what i find really
interesting about big data so at first
is to suggesting this doesn't really
think it's really not that much about
the sides so is it a terabyte well a
terabyte is big on to dupe if you talk
to the guys who are in yahoo they'll
tell you we have a really large scale
machine learning problem the matrix are
trying to factor it's about a terabyte
okay terabyte is main memory in a lot of
applications it's not really clear that
the terabyte is really where the pain is
coming from now i should say my
community has a terrible reputation when
dealing with people from the hadoop
world so I do love to do one of my best
friends wrote you know with the intern
who wrote Hadoop so I'm booster okay so
I like this stuff but it's not that
they're dealing with problems that are
sort of fundamentally large it's because
they have an infrastructure that is sort
of this big data that dealing with a
terabyte dealing with the spills and all
that stuff that comes along with it is
difficult so maybe it's a petabyte maybe
when we get to a petabyte size is the
only important plant now barring
basically two applications that I know
about I don't think you have that
problem one of those is oil the other
one of those we heard yesterday and even
if you saw what happened yesterday and
be Collider experiments they very
quickly reduce the statistical or the
learning problem down to something that
was much much smaller smaller than any
of the order of magnitudes I have up
here so I have to hear of this nice
quote from John Doyle which I like quite
a mutt bit which is let's end
paedophilia and the thing that he's
trying to get across there is just
loving size is not it's not a good thing
okay it's not about size so what I'd
suggest to you is that something
other than size is the common thread on
all these different applications that
have sort of big data spirit to them so
what is that common thread what do I
think is really interesting why do I
even you know not throw up in my mouth a
little bit when people say big data and
I do like big data I'm a booster so
there are big shifts actually that are
coming that are driving this big data
trend the first one is an observation
that you know Google gets a lot of
credit for which is that more signal
seems to be more complex model nine
times out of ten now clear there's not a
testable statement really in any sense
of the word but the intuition is if you
have a hard problem make sure that you
can't solve it by slinging together a
bunch of easy data sources that are out
there very difficult problems can be
solved in this way in fact this is one
of the main drivers why businesses want
to acquire big data business is like
using simple things like linear models
they understand acquiring a huge amount
more data say oh we need to have more
data about our customers got it and then
we'll run our linear model so this is
one of the reasons we're really driving
it's one of the driving forces I would
argue the other thing that's driving
this is more at a technical level and is
what ties together in my mind the things
that we would talk about is a gigabyte
scale being in the same spirit is big
data and this sounds at first like a
kind of trivial switch but I think it's
actually kind of deep people are moving
now computation to the data not the
other way around and if you think about
it this is kind of the common theme of
the Hadoop stuff of the stuff that's on
an iPhone it's we're taking a
sophisticated computation and putting it
to where the data actually acquire a
terabyte isn't big and Hadoop I mean
isn't big in the real world if you like
it if you could buy main memory for it
you wouldn't do it but if you living
with an entire processing infrastructure
that is Hadoop based then you may have
to do some of your machine learning
inside to do so looking at all this this
is how this is sort of a in the context
of this workshop this is one of the
motivations for the kind of work that
we're doing in my group and my bet
looking at these two trends is really
what we need is something that's going
to allow us to quickly sling together
signals to take advantage of one where
the data lid okay so the data live in a
database we're going to try and push it
into a database if the data lives on
your phone then we're going to try and
look on your phone and as researchers
what we're going to do is try and find
the common patterns or the common
abstractions that are across each one of
these different applications and that's
going to be driving my research
alright so that's my thoughts on big
data so now what I'm going to tell you
about really with that little bit of
context in mind is the actual project
that I've been pushing on which is this
thing called hazy okay and hazy really
started when I when I came to the
University of Wisconsin this was just a
couple years ago and I wanted to I did
at some work on probabilistic databases
and I wanted to build real applications
okay and probably sick databases was a
great research effort other a lot of
great theoretical results that came out
but there weren't a lot of real
applications that were built and I
wanted to build and understand real
applications and so hazy really was in
response to this I thought well you know
let me look now at the trends that are
driving you know forward all these
different data management issues and try
and understand for these kinds of
problems you know what's really driving
why do people care about this messy data
that's inconsistent data and the really
there are two things I think the first
is now data are available in an
unbelievable number of formats so about
the first 30 years of data management
people were really only concerned with
traditional record oriented data very
structured precise data that looked like
banking data over the last 10 to 15
years people have started to look at
text because of the event of the web but
by volume this is actually a small
fraction of the overall amount of data
that's available there's data in you
know video and speech and OCR and in
sensor data and it's not that this data
is new per se but it's that there's real
business value so one of my favorite
examples is of sensor data so sensor
date has been around for a while but now
people are taking sensors and sticking
them on your car and pricing your auto
insurance in the US based on it Flo from
progressive is going to take her little
sensor put it on your car understand how
well you drive and then charge you based
on that information a second trend is a
continuing trend in Swan I think people
in the room are very familiar with is
that there's this continuing arms race
to understand your data more deeply the
reason I use the phrase arms race is
that even small changes and how well you
understand your data pay off to huge
extents so if you're a scientist and you
understand your day is slightly better
than the other team then you're the
discover and there they also rent if
you're the business and you come out and
you understand the market that there's
an emerging market and no one else does
and you get first mover advantage you
get in there and you take over the
market you're a billion-dollar company
in there nothing so there's this huge
sort of
nonlinearity if you like to
understanding your data so for this
reason so looking at these two trends
what I decided to do is say well you
know what's the common theme between
them so people in this room definitely
know that for the second one statistical
data analysis is you know really at the
heart of a lot of the state-of-the-art
methods so it makes sense that we're
going to have some kind of statistical
thing in our background in our bag of
tricks but actually in each one of the
data acquisition problems above whether
it's OCR or whether it's speech or
whether it's the sensor that's in your
car its first passing through a
statistical model before you're actually
able to use in any kind of reasonable
applications so if we want to build
something that goes all the way from
data acquisition all the way to analysis
we're going to have to have some amount
of statistical tools so what hazy is is
basically taking those statistical tools
and integrating them with data
management techniques now the data
management techniques come in right away
because if you're pricing someone's
Insurance you also have an entire claims
infrastructure or reporting
infrastructure you have all those things
that live in traditional databases that
are being manipulated by an entire tool
chain that's 20 years ago so you have to
figure out how to put these things
together now hey Z's research goal in
this context it as I said to understand
those common patterns it's not to take
every single model in the world and cram
it inside whatever system we're looking
at and just you know take every pair
wise combination it's a try and look
across the applications that people are
actually building and say here's a
common design pattern the reason that
design pattern so important is that we
can abstract it away and the system's
builders say look here's a nice
decoupling between what you need to do
and some operation that's going to have
to happen again and again and again we
can optimize the heck out of it that's
really what systems are is all about
okay so said more succinctly the thesis
of hazy is pretty simple the thesis is
the next breakthrough and data analysis
may not be a new data analysis algorithm
not an individual particular algorithm
that you're adding more fancy tricks to
but I'll bet and I'm actually I am
betting that it's in the ability to
rapidly combine deploy and maintain the
algorithms that we already have so what
hazy is going to try and look at is how
to take and combine all the different
things that are out there now these
combinations are going to happen at two
levels there are going to be things that
we're going to talk about that are how
do we take a big
collection of these different tools and
sling them together in a nice
programming language that's something we
work on in my group we have a project
that I'll show you in a second there are
also other abstractions and some of the
stuff i'll talk about today which is
where we look for common algorithms by
which we can implement a whole slew of
models that's the second thing kind of
abstractions that we look for sort of a
common pipe that allows us to implement
many many different machine learning
models and that's what i'll talk about
in the technical portion of the talk
okay to understand this thesis in any
kind of detail we had to figure out what
people were actually doing right what
are people doing so what we did in my
group is we started building stuff okay
and what I'm going to tell you about our
three applications that we actually went
ahead and built with people who cared
about the data to try and understand
what they're doing now in each one of
these applications they all have the
same flavor there's something that
someone is doing right now in industry
but we're not going to do what they're
doing an industry we're going to do
something that's a couple years ahead or
a couple years sort of far out science
fiction since we don't have any
competitive advantage on the industrial
strength stuff ok so I'll describe both
of those parts of the application when I
walk through it so one application that
people are doing right now is trying to
understand how well products and people
are doing in the marketplace so every
company that I'm aware of right now
selling products they have
customer-facing entities they're running
ad campaigns of some sort and they want
to understand certain things about that
how they being perceived now what's
great for them is that never before in
history have you been able to get this
information more freely okay it's
generated on a bunch of different
sources right now it's on Twitter's on
facebook some blogs and what people
doing in a bunch of different startup
companies is doing about the simplest
thing you can imagine which is a company
comes to you and you figure out a report
that says here's how you're being
perceived in New York or here's how
you're being perceived on Twitter and to
a large extent these reports are
constructed manually now a lot of new
companies are coming along and they're
using tools that you know and love right
there using things like CRFs and support
vector machines to extract from all the
text that's out there what products are
mentioned what people are mentioned and
how they're being perceived they're
starting to sling these things together
to try and create reports to give to you
know actual companies that care about
how they're being perceived now this is
a kind of a classical application this
is what you know people are doing right
now the
is not something that we work on in my
group we have no competitive advantage
er well we work on our things like this
which are a little bit farther out
things like machine reading which is a
big DARPA project so here that setup is
were given a huge collection of text and
i want to show you this in a little bit
of detail and we're asked questions over
that tech corpus of text things like
lists the members of the brazilian
olympic team in the corpus with their
years of membership now we're not told
that we're looking for anything about
the brazilian team ahead of time we're
told things like you need to know about
membership relations okay and moreover
when we answer this question we have to
be able to prove it so we have to be
able to actually return text that says
this particular person was on the team
for this length of time here's a
document that it's substantiates that
and return reports now you can guess
from the fact that DARPA's in front of
it but this is something that would be
of interest to the intelligence
community right this is basically the
intelligence problem and read a bunch of
texts figure out who's being mentioned
and put them into some kind of
organizational hierarchy so I'm going to
walk you through a demo we built that's
a little less nefarious in a sec okay
but first i want to show you how this
whole thing works so the way it works is
we have a system which we call Felix and
Felix uses all your favorite characters
right it's got Hadoop in there he's got
green plum in there if you know Condor
it's as high throughput workflow system
and Felix is an infrastructure that's
talking and stitching together all of
these different things if you looks
actually if you're a machine learning of
a certain flavor of machine learning
it's basically a big mark of logic
processing infrastructure which may
sound strange that I say that and then
stitch together everything that's what
it's doing no really ok what Felix is
going to do to answer this thing it's
going to suck in a huge amount of
documents now the demo I'm going to show
you in a second sucks in about half a
billion documents which is roughly 15
terabytes of data once it's
pre-processed and linguistic things so
it's a it's a hefty amount of data it's
not really truly massive but it's a big
amount of data it's the web okay it also
sucks in a bunch of other structure
databases things like freebase it's
going to suck in all kinds of structure
databases that we may write a crawler
for for interested in knowing about
movies maybe we'll write a crawler for
IMDb and throw that thing in there as
well ok and felix is a very flexible
infrastructure to do now to answer that
question that we had on the last slide
we basically do the standard thing that
everyone does in text analysis and I'm
just going to show you this just
introduce some terminology what we do is
we first do a phase called entity link
we take every document that's out there
we run every linguistic tool we can
imagine on it ok so we run you know the
end up
tools from Stanford to linguistically
parse the entire web we're on all the co
reference tools that are out there this
gives us basically a very rich feature
space for every single mention that we
just named entity on the web you know
people product place thing capitalize
now and common now every now that we can
find okay we blow up a huge feature
vector about and then what we try and do
is resolve that this particular string
is referring to a particular person okay
so like this string is actually Barack
Obama right it's actually the president
not just another guy named Barack Obama
that's a phase called entity link once
we've done this we've late raised the
level of abstraction from text to actual
entities that we can reason about and
bring into play some of the structured
knowledge bases that are on the side and
then we can do something that in the
text analysis terminology is called slot
filling which is basically relation
extraction trying to figure out the
member of relationships now we figured
out all the mentions in the world of the
Brazilian Olympic team we've figured out
all the different individuals that are
on that team and now we're going to have
to try and pair them up and what this
boils down to again is extracting a huge
number of features and every signal that
we can imagine throwing into this thing
to try and figure out that these two are
participating in some kind of
relationship and those features are
linguistic cues you know like you know
bob was on the team okay that's a pretty
good linguistic you we'd want to learn
that automatically all right so before I
show you the diamond i'ma show you a
quick a very rough demo in a second I
want to explain what the two key
features of this system Felix are the
first thing is that Felix is going to
allow many of those best in breed
algorithms to be used for individual
points we're not going to write our own
linguistic par so we're going to use
Chris Mannix cool we're not going to
write some little way of doing you know
named entity extraction were going to
use them off the shelf CRF we're not
going to invent a new classification
model we're going to use logistic
regression I mean with you know that's
what we're going to use okay so the best
of breed algorithms for a particular
task we're just going to stitch them
together the second thing which I'm not
going to talk about too much is that the
way we stitch them together is with a
very simple rule language and actually
after I show you the demo I'll show you
that the rules to create the demo only a
couple hundred lines okay this language
looks like sequel or if you looks like
prologue or it looks like data log it's
a logic-based language it has some
notion of uncertainty in it has weights
and all kinds of rules and what this
allows us to do is very quickly bring in
signals from structured sources with
unstructured sources and put them
together it also allows us to give very
simple and course feedback
to the system like don't do that okay
which is very helpful here's a rule that
you should never violate for consistency
now the challenge and one of the things
that's interesting about Felix is how we
scale it up two terabytes on how we're
able to take this very expressive
language this Markov logic language and
run it on the web but I will instead of
telling you how we do it and all that
stuff I'm just going to show you that we
do it so just one little bit of sort of
pseudo scientific validation the tap kbp
challenge these text analytics challenge
state-of-the-art systems have an f1
score about 20 we weren't competing in
the challenge we just spent about two
weeks one student working on this we
were working on a very related challenge
though called mr kbp for DARPA but our
f1 score was 39 ok so we were able to
clean these things together use the best
in breed that was out there and get a
very high quality system basically out
of the box ok all right so now awesome
animation and then a quick demo here's
the awesome animation will have to wait
for a 10 ok so the demo is a venue lee
very rough I my student has only been
working on this for a couple weeks but I
absolutely love it so I just like
looking at it so you're gonna look at it
right ok ok here we go all right so this
is a this is a web page this is not
public so hopefully you can't see the
URL but either way it wouldn't be very
hard to like reverse engineer what's
going on here what we've done here is
we've created a high recall version of
Wikipedia ok so we've taken all
Wikipedia Wikipedia is very high
precision right it's entered by humans
every fact that's in there is manually
curated in check but if you want to know
as I said every mention a Barack Obama
on the web it doesn't it can't help ok
now what we've done here and the stats
thing as you can see from a particular
web crawl it's actually this is only
this is a small across as the clue krull
not the 50 terabyte chrome you can see
how many times we found Barack Obama
mentioned so he was mentioned here and
two hundred and ten thousand different
documents in a million sentences he was
mentioned in some videos metadata we've
only crawled this only has about 200,000
YouTube videos in it we should be up in
the millions very soon what's
interesting also and I'll show you in a
second is cute is that we also find
things inside content so actually in the
spoken content which hopefully will be
able to pull off but depends on an
internet connection ok so we're doing
this we're constructing this high recoil
thing and just to show you that it's
more sophisticated than simply text
matching let me redo the second
sentence that Felix says here's a
related entity to Barack Obama which is
Hillary Clinton okay so Hillary Rodham
Clinton so it says has he forgiven
Barack for trouncing good old Hill
otherwise a shoo-in for heading the demo
ticket in this election okay it figures
out that good old Hill is actually
helped hillary rodham clinton and if you
actually dig through the document
provenance you'll see thats establish
who he is right and the fact that he is
in context with good old Hillary is
actually a valuable signal that heat
bill clinton being mentioned means that
Hillary should also be mentioned all
right okay so we we have some high
recall stuff we have some stuff here and
there's some video and content stuff but
I want to show you something else that's
pretty cool okay so you've all seen the
info box in Wikipedia right so here's
Barack Obama's Wikipedia and what we've
done is we've tried to use that relation
extraction to substantiate facts that
are in that box so for example here says
Barack Obama's the president united
states that's a fact right that he was a
present United States and we'd want to
substantiate that he's a senator from
Illinois so on now unfortunately for
font reasons these won't show up but
what's going on here if you click on the
red icons it's showing you it's sort of
top five sentences that thinks
substantiate this fact that also which
is not listed substantiates a whole host
of other facts and gives you provenance
for those so you could add them and if
you wanted to you could make a wikipedia
editor on steroids for this which is one
demo project but what to me is very cool
hopefully the we don't have any UI bugs
are things like this so here we want to
know that Barack went to Harvard okay he
went to Harvard Law School we can
substantiate that in a video okay all
right so let's try and figure out how
we're going to do this so here's the
video wait about 10 seconds and you'll
hear the magic words that Barack went to
Harvard Law School but maybe not in a
linguistic order you would expect okay
give me one second to try and up the
volume outside of this this business so
we have a little sound issue we pause it
back out all right
I hope they have no I am sore anything
up so that was common to his mother is
from Kenya just like Barack Obama's
father and his parents like Barack and
Michelle Obama went to Harvard Law
School so so there it had to reason and
do actually a little bit of co ref on
the text of the actual video and it
finds the spot in the videos we're
actually that provenance and relation
extraction occurred ok so it's able to
do this automatically there's no hidden
tricks here it's not like my student
found this and put it up in there this
is this right now is all automatic with
no human feedback whatsoever all right
of course if you were really building a
system and let's say you were taking
money for it you would put in some
humans feedback that's actually where
the rules come into play quite a bit
because you can tell it things like
don't you know don't read different
sources there's all kinds of interesting
crazy sources in here I shouldn't
probably play this but I get a kick out
of it so I'll do it so what is happening
underneath is the language is Markov
logic so what it's doing every one of
these rules is just a fact with a weight
on it there are each one of those
weights are learned in some way and it
tries to resolve it and how best it can
it has some notion of sourcing that
allows you to write hard-coded rules to
say don't trust particular things
there's actually is a hard-coded rule
because I think it's funny to include
fox news videos all the time so it will
try and buy us up fox news videos
whenever it can so here's one of those
so we're trying to establish here that
Barack is a united states senator I'll
probably shouldn't play this but
whatever fortunately some thought I
personally had characterized it
inappropriately I regret that was not my
intention I certainly didn't mean to
associate the word terrorist in any way
with Senator Obama and his wife so so
that was that was its finding that
actually Barack is a senator so I was
very happy that that was highly rated my
student actually made a different
version of the demo where he was he was
automatically learning reliability of
sources and that demo went away that
clip went away and he fixed that almost
instantaneously because I was clicking
on that all the time okay so back to it
okay so all I want to say about that
okay we don't have to watch the
animation again all I want to say about
the demo is we were able to build it and
because of fun reasons i'm not going to
show you the program if you're
interested I can show you the actual
Markov logic that's underneath it that
has the weights it's that part is public
I can give it to you right away
the reason that the entire site isn't
public is I don't trust well I shouldn't
some poached eggs with there's a chance
that my students may have all right let
a copyright at some point so keeping it
private is something we'd like to do
until we can clear it with everyone that
we haven't crawled something
inappropriate they're okay but if you
want to see the program and how it
actually works i'm happy to walk you
through the infrastructure all right
okay so let's switch gears a little bit
so remember what our goal was we wanted
to understand different applications and
we wanted to understand the common
design patterns that occurred across
them so one I mean this is a pretty rich
application building this all the way
from text all the way to answering
questions and you know building the UI
on top so there are a lot of different
pieces there are a lot of different
design patterns that you have to extract
out just to be able to build it easily
but let's look at an entirely different
domain that has nothing to do with text
processing and try and see if we can
learn some of these same kind of similar
abstractions that are around so another
thing that we work on in my group and
ben and i collaborate on quite a bit
then record up there is working with a
bunch of physicists called ice cube okay
and I'm describe to you what ice cube is
in a second but basically what's going
on here is we have a bunch of physicists
and at a high level they're going to do
some statistical processing to
understand and interpolate sensor
readings right remember that this
relates to kind of to like you know flow
sticking that sensor in your car so
there is potentially some downstream
business value before I tell you
specifically what we're doing in this
project let me tell you what ice cubes
all about so ice cubes pretty awesome as
you saw yesterday physicists are awesome
they get to do crazy things that like we
need to learn how to lobby as a
community so what these businesses have
done over the course of about twenty
years is get about 300 million dollars
and they use their three hundred million
dollars to purchase a Giga ton of ice at
the South Pole it's a couple kill Emma
kilometers deep okay and what they've
done to this these kilometer cubed block
of ice is instrumented with a bunch of
sensors so those little strands there
are all hanging sensors about 10 or 20
meters apart so forget it so each one of
the little dots that's shown there is
actually these things called a digital
optical module the digital optical
modules about this big around and what
it's doing is it's looking for light
from basically from the bedrock now
first glance this seems a little bit
strange right why is it looking for
light
from the bedrock right ice maybe it
should be looking at light from the Sun
now care about light from the Sun they
care about very high energy particles
those that can pass through the Earth's
crust and they're basically using the
earth as a gigantic filter so when the
particle passes through and then
interacts inside the ice they can tell
that it has you know reasonably high
energy right off the bat okay filter out
a bunch of false false hits so the way
this whole thing works since it's based
at the South Pole is the following what
happens is a particle goes in light
shoots out they figure out that there's
some kind of detection the sensors read
that there's some light I'll show you it
not in a little bit more detail there at
the South Pole though so an algorithm
has to say that this is an interesting
event so to a first approximation an
interesting event is one of these that
has passed through the Earth's crust and
not one of these things that's raining
in through the top of the ice that is
just light scattering around the reason
I have to decide about interesting is
they have to ship from the South Pole to
Madison okay and they're going to dissin
this satellite link is actually
constrained so they don't want to ship
absolutely everything for every one of
these events because they're happening
you know hundreds of times a second and
very very small time windows once it
gets to Madison there's lots of data
analysis that happens okay on top of the
stem now together with Ben we r NS spend
some students we're working at both the
top level which is the in Madison you
know complicated physics detection stuff
but we're also because these guys are
awesome and we really love them we're
also allowed to work at stuff in the
pole okay and deciding whether or not an
event is interesting so what I'm going
to tell you about just for one slide is
what happens at the pole and what we've
been doing with them for the last couple
months so a key step there are actually
a couple but a key step is basically
detecting the track so what you see on
the screen here is basically the it's a
little bit dim but you can picture each
one of the sensors this is the light
intensity that's hit the sensors in a
particular period of time this is the
data that you get back the time that
each sensor went off and basically a
rough count of the number of photons
I've actually hit that particular doll
from that what we want to do is actually
figure out the track of the particle now
here this is a pretty good and easy case
right you can kind of guess that it's
through the middle now it turns out that
the better you can get your angular
resolution you know the more you know
happier the physicists are so even with
a problem like this there's some
non-trivial amount of
engineering what you have to get to get
it perfectly correct no just there's
another bit just as an aside if you look
at a lot of the data there is a lot of
noise and spurious stuff inside the data
going on all the time so to decide if
something's interesting as I said we
wanted to figure out if this track is up
going right if it hasn't because upward
trend now one thing that's very
interesting is the structure that's used
actually to do these neutrino
calculations we use very simple stuff
okay it shares a lot in common with some
of the text processing things that we've
done to cut to the chase but we actually
use a basically simple rule simple data
filtering and simple regression
concatenate it together it's not a very
very complicated model now you may look
at that and say okay these guys put some
rules together they did some regression
and if it said hey that's great thanks
thanks for playing but actually even
these simple models were able to beat
what they had already had on the pole
for quite a while and in fact more
violence who's a co advised by Ben and I
his code is going to be pushed to the
pole in the next science run so what do
I take away from this this paradigm of
being able to have simple data filtering
slinging signals together quickly and
using state-of-the-art existing tools
it's like nothing complicated is going
on here so cuber loans just simple
robust regression concatenated with a
bunch of common sense filtering that
encodes a little bit of physics
knowledge and we're able to actually
improve something that people have
worked on for a for substantial amount
of time so I promised you in each one of
these applications that there would be a
you know motivation that we care about
and then a cow's motivation no to make
money so what's the cows motivation to
make money here and yes I can do the
point is actually that many of the data
and houses that are out there are
actually physicists if you go to Wall
Street culturally they're really not
machine learning people they're not see
us people there physicist bar they don't
know how to program sequel they don't
care really about much about machine
learning you know Monte Carlo they know
a little bit of regression they have a
different bag of tricks than a lot of us
have now what was really a promising and
hopeful to me yesterday during the talk
was that we learned that you know
statisticians are there and they're
starting to you know convince some of
the younger generation and physicists
that yeah there's real value in these
tools you should learn about them so
it's a really hopeful time for machine
learning tools to kind of migrate over
there but historically the people who
are doing this on Wall Street they don't
care to wits about those kind of things
to a first approximation it's not
entirely
alright so the last application will be
very brief the last application is
something that my student at rune was
doing and the high level motivation the
motivation that I care about you know
the liberal BS motivation shouldn't say
that anyway is that social scientists
have been left out of the digital
revolution by and large okay so we've
been concentrating a lot on the
physicists and we help biologists but
social scientists really are still going
and looking up books and they I learned
that they care about things like what
are the other books on the shelf and
what books are nearing and they're not
being able to search through large
volumes of texts now Wisconsin there are
a couple of people who are actually
trying to change this and trying to look
at using you know computers as a tool to
understand their social science better
and one of the things that we looked at
because we are interested in you know
all these different forms of content
that are out there is helping out
English professor to try and do things
like crawling you know all of their
information and doing statistical
analysis just like we would do for the
web the problem and the difference from
the web is these guys all the stuff they
care about is in books and that means
that to get at the information they have
to scan them after on OCR now you've
probably used google books right you
type in something into google books you
get the word back and the word that
comes back is correct okay so yeah Lucy
our works pretty well that's actually
what we thought when we started the
project we weren't interested in doing
anything with OCR but that's because
you're sensitive to precision write the
answers that come back you appear to be
correct but what about the recall how
many times if you ask for a word in the
corpus you actually get it what fraction
of the much more difficult question to a
sentence so when we started doing this
for some of these mining tasks like
finding all the synonyms of a work we're
motivated to try and figure out what are
the recall of these tools and it turns
out that the recall is actually quite
low okay it's around thirty percent at
the point 3 if you like and so that
motivates to go oh my god if these
people are going to use it for mining
these tools simply aren't appropriate so
I'm not going to hammer too much on the
details here but I'll tell you that
basically the underlying model that is
used in OCR it's actually very similar
to the kinds of models that we use and
other kinds of labeling tasks the CRFs
they're called stochastic transducers
but really if you squint a little bit
they're basically the same kind of CRF
like model that we know and love for
machine
the group of slightly different
tradition but if you look at the
algorithms if you look at how they're
handled if you look at how to compress
them they're very very similar so you
may be surprised but I can even come up
with a casa motivation for doing this
and the cows motivation is that we want
to unify these things called content
management systems which are all the
rage if you know about recent
acquisitions with database systems okay
so if you know what this company
autonomy that was acquired by HP for ten
billion dollars they basically do
content management okay now content
management systems allow you to search
your voicemail your email stuff like
that what we want to do is implement
integrate them with the rest of the
reporting hierarchy inside a database so
that you can write reports over your
emails and things like that form
processing by the way turns out to be
like a multiple billion dollar industry
that I had absolutely no idea about
until I started this so there's a bunch
of interesting things going on so what
do I want you to take away from all of
these different applications the first
thing is that just taking statistical
processing and applying it to a bunch of
different domains opens up a huge number
of new application there's all kinds of
fun stuff to do in this base and there's
actually all kinds of low-hanging fruit
because there are a huge number of
applications that can benefit in some
way from being enhanced with noisy data
the goal of what we're doing is to try
and develop the ability to rapidly
combine deploy and maintain all these
existing algorithms so how are we going
to cut down on the effort requires to go
after each one of these models now
traditionally even when I was talking
about on the slides each one of those
models like a transducer there's an
entire tool chain for how people
manipulate transducers Oh someone's got
an SVM tire tool chain for that Oh
logistic regression that's a different
package that seems ridiculous to me as
assistance builder so what we're going
to talk about is a very simple algorithm
that allows us to time many of the
underlying problems together and that
algorithm is a simple one it's an old
one which are incremental gradient
methods and all those things if your
opinion attention that we're in bold
those simple models SVM crfs all those
things will fit into a framework that
we've implemented in one pipe so we get
all the data management stuff for free
where the data link now we've done this
in a couple different places but I'm
going to share too with you if I have
time
one is invited database which we'll talk
about first because it's probably the
most alien to you and then also one that
been talked about yesterday in the
optimization thing we had a poster on
which is doing things in shared memory
which is a paper called hog-wild which
as it has a great name I will try and
show you a picture of a pig and a hat no
matter what even if I don't get to it
all right so where are we in this so
what I showed you is that we should have
three application areas for Haven what I
showed you was the different application
areas and I showed you that there were
these common models that were there this
is one low-level way we're going to
stitch these things together as i
mentioned offline if you're interested I
can show you the higher-level way that
we stitch these things together the
programming language or Markov logic
models so what we're going to talk about
now is a project that's called Victor
and its sub project called Bismarck we
got a student named Victor so it's a
little bit awkward to have a project
called Victor too but now it's called
Bismarck by my students and what this is
going to do is it's going to apply those
regression kinds of methods inside a
parallel database infrastructure a lot
of interesting data lives inside there
and in fact that web data even though
you think web beta it must use Hadoop we
only used to do for feature extraction
all the real hard core processing is
done inside a big parallel Greenplum
instance ok 130 node cluster so let me
show you how that stuff gets done what
I'll do is give you a very simple very
very quick introduction I know this is
old hat for everyone in the room but
basically the kinds of things we would
want to do our add in things like this
is a little web site that was built at
the wisconsin degree websites DB life
built on my colleague all we want to do
is add simple features right now that
look like things like classifying
publications by subject area ok so we're
going to use that as a running example
we have a bunch of papers we want to
classify them by subject area this is
some graphical motivation for that so
what we're going to think about our
things which are basically linear models
as everyone here knows let's take a very
simple example where we want to label
each paper as a database paper or a non
database paper so the first thing we're
going to do is feature extraction we're
going to take each paper and map it to
some high dimensional space I got two
dimensions so I'm not into two
dimensions once we have those things we
want to classify them via plane that's
why it's a linear model ok so we want to
split them into database papers or not
and the way we're going to do that is
try and find that X there which is the
normal vector to the hyper good now
so as you know we're going to consider a
supervised problem for the moment so our
actual input is each some set of points
as labeled as database papers or not can
we get back to the same so one of the
great ideas that I love from
optimization is rather than thinking
about this algorithmically will think
about it my terms declaratively and
we'll just say that what we're going to
do is score each one of those planes so
there are many planes that we could pick
we're going to pick the one that
minimizes up cost function okay so X
here is the model and this is all just a
head fate to introduce a bunch of
notation I without being very dry about
it and we're going to look at things
that look like that okay the why I here
is a paper vector and it's label so the
feature vector and possibly the label is
both now the thing that's interesting
here is different models right give you
different apps so if I want you know
least squares that you take a square
distance a hinge loss at sbm log loss
logistic regression different at
different model throw on a regular is it
too if you like okay as a systems person
when you see this as a nice unifying
abstraction when you're told about all
these different models you say hey all
these things are just convex programs
all senor your heart sings you say oh my
god i can implement one thing i can
decouple everything and work on this
problem and not have to worry about what
people are actually doing and separate
the modeling task from the algorithmic
times that is a huge huge conceptual
thing that allows us to do a lot of
interesting systems go it's not ours
obviously so what we're going to look at
are these frameworks of these basically
regularized inverse problems we have a
regular Iser out in front or prior
pending on your terminology we have a
bunch of data points we're interested in
these summable things where we have
basically a model and a bunch of
different data items make sense in the
kinds of applications we care about that
it would have this kind of nice additive
structure examples of this that we've
seen so far to sell the paper
classification fits this model a
neutrino tracking actually that why I
could be the dom sensor eating doesn't
have to actually be a label for the
regression problem the Huber loss you
can crank it in there as well in CRFs
you have the whole document and the
labeling is basically an entire vector
you can cram it shoehorn it into this
model as well which we were using in the
text processing and in Netflix you can
imagine the wires or the users
movies and ratings you all know the
netflix change okay so why do I bother
introducing all this notation well as I
said conceptually the point is by
looking at this specific problem we can
get a whole slew of applications that we
care about not absolutely everything
that's not our goal but a whole slew of
interesting applications that we can
build as I'll show you one second in one
simple way now perhaps you want to quite
appreciate this because it seems a
little bit it may seem not very shocking
to you but this technique is no more
difficult to compute all these models
are no more difficult to solve then a
sequel average so averaging a bunch of
them okay that's it now why is that
significant because database engines are
unreal they are unbelievably fast that
aggregating huge sums of numbers right
summing up numbers averaging numbers and
dealing with structures so as long as we
can mimic that data access pattern we
get to leverage 30 years of systems nars
in there beating the crap out of thing
and making it scale so that's going to
be the high level win of tying these
things together so good so what's the
technique so the technique we need is
gradient methods so gradient methods
we're going to get to incremental in a
second are very simple right just to
recall you start at a point you compute
the gradient you walk in the opposite
direction that gives you a nice little
crazy looks like this ignoring step
sizes for the moment in incremental
gradients right and if as long as it's
bowl shape or convex we're gonna walked
out eventually and get to the bottom
right it's going to complete now
incremental gradient methods right the
idea is just to select a single data
item at once to approximate the gradient
so the same iterative scheme works start
an X approximate the gradient index but
instead we're going to still lacked some
J you can think about it at random but
that doesn't really matter it's easiest
to see if it's at random because clearly
is going to be an unbiased estimate of
the gradient we're going to form some
estimate the grading okay and then what
we're going to do is we're in it since
we have this estimate of the gradient
we're going to move in the opposite
direction now this is a you know very
old classical algorithm it's known that
it converges for convex problems right
it has known rates of convergence is a
nice stable solid algorithm okay that
has been has told me is rediscovered
every 10 years
so why do we care about these things
well the real you know I dams converged
you know for comment from any problems
and all those things we care about
really or convex but the real reason we
care about them is that they're very
very fast okay they convert very rapidly
for the kinds of problems we care about
for messy data kinds of problems you're
going to get something that's you know
very close to the optimal solution not
maybe not 10 to the minus 10 to the
optimal solution but 10 to the minus two
or three very very quickly and for the
kinds of problems we care about the
noise is so high that's all we want to
get with it the other reason we care
about them is a deep technical
connection which is the IgM processing
is basically isomorphic to the sequel
average okay so there's some terminology
here that's familiar database people the
ex you can think about is an accumulator
this is just a sum and g is an
expression on a single topic okay which
is actually the what an aggregation
needs the consequence of this is that we
can solve statistical models in a
database essentially for free we can use
the infrastructure that's already there
we don't have to do anything else so if
you're not familiar with the way
databases work or they aggregate though
these things that are inside called
user-defined aggregations and have three
steps in this extremely extremely simple
there's an initialization which takes a
state it there's a transition which
takes a state and maps it takes a data
item and produces a new state then
there's a termination state just a
little simple at tomica okay doesn't
care what the functions are as long as
this type one long as the Senate ometer
so for average that state looks like
this this is a naive way to compute
average but it'll suffice for my example
the state is some vector in r2 right you
could say our n cross our to what
everyone number of terms and the running
total the transition function is I take
the number of terms I have the current
running total give a new data item and
add them up when I'm done i compute the
average okay not great for numeric
precision but good enough for an example
now what's an IGN the state now gets a
fancy name it's no longer the state is
now called a model and that's that
expecto the transition is i give you a
data item a particular data item when i
read it i compute the update and that's
my new running count of them all exactly
the equation the XK plus 1
xk equation we had on the last slide
when I terminate it's actually even
easier hey I don't need to do anything I
just returned the model that I have all
right so this is all well and good and
syntactically it's a match and so we've
at least at this point achieved our goal
of being able to syntactically shoehorn
this stuff into a database but the
question is does it work so there
actually are some very subtle
differences between what and aggregate
assumes and what a gradient is there but
I'm going to argue that they don't
matter so you das are typically
community there are things were the Sun
the order of the summation doesn't
matter but gradient methods are
community right if I take the data if I
take the data items in a dip in one
order versus another order I mean at the
end end up with a different solution ok
they're also algebraic which doesn't
really matter too much here because just
a technical thing about how they can be
averaged ok so I'm not going to go into
now I'll argue that I teams is not
really very hard or formally neither
commutative more algebraic so you
shouldn't go sticking them into this
structure but they're morally both ok so
what I mean well there aurelie
commutative in the sense that different
orders do give different exact results
but who cares about exact results we're
doing this because we're messing with
noisy data anyway they're converging to
the same underlying solution so it
doesn't actually matter which order we
evaluate them in we're eventually going
to converge the same optimal solution
it's a performance thing about how
quickly we converse of that solution of
the order matters it's not about
correcting now this is the key because
these two properties which are required
for you das whole which did basically
the system is underlying is assuming us
is going on this means that I GM's will
work basically in parallel off the shelf
so we can take them and put them into
green plum and Oracle which is something
that we're doing and scale them for free
and in fact all that logistic regression
that we were doing on the web in minutes
running 16 terabytes through in a couple
of minutes I didn't program all the
different ways the data access was going
to work I wasn't intelligently
prefetching I wasn't shuffling the tasks
I wasn't making sure that they were
equally balanced in the note I had to
write none of that code I wrote that
stupid you know three line thing and
actually my student end up ripping out
my code and putting his ok so that's it
you write those little gradients and
everything else sings ok and there's a
little bit of control software because
databases are not ideally suited for
iteration you have to do a little bit of
control outside
but that's about it now one thing I
wanted to point out while I'm here is an
effort called mad lib which is an
open-source effort to be great if you
guys could contribute to its a backed by
greenplum but it's not meant to be tied
to greenplum that they want people to
implement algorithms now the reason that
this is a little bit unique is because
the underlying company greenplum
actually will introduce you to customers
they've introduced me to customers
they'll actually tell you about real
problems they're extraordinarily open so
as a plug for them it's called Madlib
net it's open I think Joe Haller Steen's
baby from Berkeley but you should all
definitely contribute so I'll plug them
well the speed obviously depends on the
configuration in a particular problem
right just a progression has 30 new
features as the absolutely needs
feature yes no sabria featuring record
why I've ever
English expression every way into bright
moon
and under the covers in the system
50 survival necessary
so maybe that lost their programs in
here is a technique called person
supervision so the rules that be
treating zips up
is something to happen
but using freebase basically
big cross validation that's peach the
tenants
silver standard training examples you're
assuming that need not because you're
happy with the PBS your brow shaper
correct get back a little features using
cross-validation sparse regressions
putting down a number of features with
small children never happens if always
contribute service back to the cannon
human website or make you think they can
select and keep those teacher
I'm not really
incarceration sreenivasan show honey
head
absolutely so it means better moving
targets like
have a billion web pages I think in the
end of the new inventions as our anybody
mutinies which each have one of those
features in it
what is taught to try and see if this
reported we have a weekly weather
understand sir I
crystals are one
features will have a huge number
featuring as we have all the linguistic
Arsenal commissioners around which
everything comes back the car sir walter
reference information
each one of those missions in this round
any these
way female sports directors were
celebrating
there
me too much more than four
assistant
in addition everything is every curve
piece of code here that not be
purposes you can't tables
I scandal we put it over my
virtual machines so easily
short of breath and push Michelle
Phillips
so if you want to try to keep that
testimony
ok
super software
sighs
the truth is data why you want to
maintain these systems not do our
competition construction we have enough
I see
probably
tax preparer
a visual
mesa solar masses toxic normal kid and
see what are exerting a hole in the
world but a large fraction we realize
because we wanted to see what Piper to
push our analysis as personal as
possible it means how about a while it
did not play again company of haters
everything kind of has to go on
bye
that's a great friends around two ways
you can go so you can take your you have
to option you can take scratching their
array systems i doing that's definitely
a very plausible way ever seen where the
second peripheral says look there's an
entire truly infrastructure that's there
maybe you're going to run ten percent
slower then you could absolutely
everything ventilated people care about
right now we want to have those tools in
our tuition so your monies make that
trail now down the road what what's that
seem to already be happening especially
with these guys that we were very vote
though is that you say hey you are a
page from at as humans you're causing a
scene on a factor of two be here when
you accommodate they're actually very
responsive to trying to fix and do those
things because there's nothing
the beam 2x faster a dissemination go to
a competitive okay we're too especially
warm you should ride across so as long
as its competitive pressure some of
those engineering these are getting rid
of murder has people now I actually
think is pretty ideal infrastructure
program specifically in terms of rating
method because artistic a sticker we're
just a cosmic comet that's other really
so I'm not sure that the fundamental
reason to be exciting
a virgin
yes
so all the suitors
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>