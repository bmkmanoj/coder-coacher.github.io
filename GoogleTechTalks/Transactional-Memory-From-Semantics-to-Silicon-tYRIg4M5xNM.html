<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Transactional Memory: From Semantics to Silicon | Coder Coacher - Coaching Coders</title><meta content="Transactional Memory: From Semantics to Silicon - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Transactional Memory: From Semantics to Silicon</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tYRIg4M5xNM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">welcome to the latest in our series of
languages talks today we have a guest
speaker from outside of Google because
not enough people from inside of google
are coming to me and telling me that
they want to give talk so everybody
should come to me and tell me that they
want to give a talk even if you don't
want to give a talk you should come to
me and tell me that you want to give a
talk today we have Adam well coming to
us he's a research scientist at intel's
programming systems lab he recently
graduated from purdue university with a
PhD and has been working on
transactional memory and he's going to
fill us in on all of the wonderful
things that they're doing at purdue with
transactional memory thank you I've
already been used to do so I will only
say say this this work is in
collaboration with all the people that
work in and my lab at Intel and
programming systems love their names are
listed on the slide and the title is I
think some wood bulb but that's that's
really what we are what we do there we
sort of cover the whole spectrum of
issues related transactional memory
starting with programming language
constructs semantics and going to
investigating possibilities for hardware
acceleration of these constructs so
that's what I what I am going to talk to
you about today I'm going to give some
some general motivational on why we even
bother to think about new constructs for
programming for concurrent programming I
will talk about the construct we propose
and other people propose and a little
bit about their semantics so how they
behave in programs and then I'll present
our own version of how these things
should be done or could be done and then
how proposed to accelerate them using
using hardware support future hardware
support so just to give you a bit of
motivation let's just just go through a
small exercise of designing a concurrent
data structure a concurrent map so we
would like to have three operations
would like to have different threads
execute these operations concurrently
safely so that there's no interference
there is no corruption
internal the of internal metadata the
operations are put get and remove so
Java used to do it that way obviously
using Java syntax we just have
synchronized methods so we essentially
say if one thread started tries to
execute one of these methods all the
other threads have to wait as this
mutual exclusion and that appeared to be
not so great in the case when I had
mildly hope you had a lot of readers
that wanted to access this the structure
concurrently so let's say was
pre-initialized and there was an
occasional right or remove but the
majority of access worries so there was
a refinement on that proposed a while
ago and this is actually abstracted away
version of what's now in the Java can
contrive a concurrent package and java
jdk 1.5 so the idea is that well let the
readers try to do these things
optimistically without synchronization
and only if we detect some sort of
interference then we will try to fall
back on having these things fully fully
serial so I just think the cog just got
a little restructure because put and
remove potentially can could have some
internal optimizations as well possibly
but the gist is that you would like to
have the reads more efficient so now we
get to how to probably use this this map
and it works for simple operations
obviously if you put multiple threads
executing puts gets remove dead then
it's all going to work nicely but what
if we will not do something more
complicated like like transfer value
from one map to the other then things
start becoming a little more tricky
because suddenly you have no good way to
express this paradigm and you have no
way to say that the transfer from
valleys between two maps is supposed to
happen as a unit so in this in this
particular example you probably would
like to say you would like to see some
consistent value read by threat t2
rather than you know have it read things
in the middle between when the front
window transfers happening so in java
using synchronized construct the way
would you do it
probably just enclosed these things in
and global logs that would just guard
these accesses and make them cereal
again but this sort of comes back to the
original problems that we have with the
first implementation of the map which is
we're active coarse-grain this this
concurrent thing with readers is not
working anymore because we have global
logs but also in this particular example
two programmers wrote things
independently and they they acquired
they wrote the code that requires locks
in a different order and does they
created a possibility for a deadlock so
this up this particular fragment of code
can actually get stuck just because he
acquired locks in a different order so
this is the motivation why one of the
things differently because would we not
really want to do is you want to just
specify the desired properties rather
than hard code the actual protocol for
synchronization into into the
application we just want to say we want
things to happen atomically we just want
them to be isolated from one another
when different threads execute different
pieces of code so in this case we'll
just enclose these different operations
that are balanced by themselves atomic
will enclose them and go another bigger
atomic blog and we'll hope that the
runtime system will take take care of
everything for us so the gum so that was
the motivation and and I'm going to go a
little into a little bit more detail of
how these things actually behave so the
goals that we have is that we would like
to have the ease of use of coarse grain
locking like you know in the first
example that I presented but at the same
time would like to have the efficiency
of fine grain locking and at the same
time we would like to be able to compose
things so that you don't lose the
benefit of fine fine granularity locking
efficiency but you retain safety so you
don't have these nasty deadlox or
anything like that so the the
inspiration for how to do this
differently came from the database based
world where you have this notion of
transactions they have certain
properties relatively high level
properties and they're typically the
ones that are mostly referred to are the
acid properties dragon listed here
cinema city consistency isolation in
your bill
but some of these properties did not
make much sense in the context of a
programming language like consistency
had something to do with bringing or
actually moving data database the
persistent state from one consistent
state to the other we don't have such a
notion in and in the programming
language same with durability we really
don't make anything is durable once the
power is off everything's off so we can
scratch these these two properties from
the ones that you really want to have
and that's what really call the
atomicity so we sort of fold them
isolation and the notion of data data
base isolation and Adam icity into the
concept of other messages we want to
have in programming languages and so
there's a very high level view of things
but they're they're different flavors
possible just because you can provide
different isolation and atomicity levels
and the reason for that is that we're
operating a slightly different context
here so in a database world essentially
all computation was enclosed in n
transactions so you couldn't have
non-transactional computation if you
want to use atomic and where knowledge
is synchronized in Java then suddenly
you still have will still have pieces of
computation that will be not in closing
Atomics you'll be non synchronize non
non controlled and so you still have
this possibility there will be data race
which means that you will have access is
to share data from inside and from the
outside of these atomic blogs and this
is this is it's very and I already said
that it's very true that programmers are
interested in a high level properties
like you know let's make this isolate
and let's not care how this is done but
for performance reasons you can may
sometimes have to compromise that and
these are the reasons why why you may
have my way why you may want to have
different flavors of these of these
atomicity in isolation properties the
major division is as that we have right
now is between weak and strong atomicity
and this is exactly the example that I
am showing here is exactly the kind of
situation that I talked about before
which is that one thread is performing
computation that's supposed to be atomic
the other one is just operating on the
same data but
a programmer forgot to enclose in an
atomic block so then what happens this
is a perfectly legal piece of code maybe
not the best written code but we still
need to provide some properties for the
programmer to understand the behavior
and this relaxation of the high level
abstraction comes in the in the second
bullet and essentially transactions have
to resolve conflicts and there has to be
a mechanism that allows them to do this
and typically this is done by just
redoing certain parts of computation by
rolling back the the computation that
went wrong and just you know we doing it
so that in hope that they will actually
go right the next time and unfortunately
effects of these rollbacks can can
become visible in this kind of situation
so I'll just walk you through through
through through an example of how this
particular fragment of code could be
executed under the week notion of
atomicity so let's start with thread t2
reading a just a feel of an object bar
and let's assume that initially the
value of this this field is zero then
we'll have a thread that's actually
executing atomically read from another
field of another object fuel ex of
object foo then 32 updates the same
field that that the atomic thread has
already read so the next read from the
atomic thread will actually return a
different value just because that a
memory allocation has been the memory
location has been updated and so really
really not nothing under this this
particular motor nothing nothing really
can be done but it's unclear if this
type of behavioral is the most desirable
one because from some point of view
threat t1 they want to be completely
isolated from everything that happens in
the system so it shouldn't see anything
else that that happens between it starts
its selmak block and finishes its atomic
block and when we proceed with the
execution we will will have threat
atomic threat to update a I field an
object bar and then we'll have threat
that is not not atomic to read the value
from that field unfortunately we have
these nasty all bags so on conflict 31
my actually not successfully complete
may have to be this computation may be
redone but thread t2 has in this case
already read the value that has been put
into memory by threat t want so now what
do we do it has seen I computation that
conceptually shouldn't happen because it
will be rolled back so these are the
sort of the the problems that come with
this particular isolation model but this
can be implemented relatively cheaply
which is not necessarily what we can say
it's the obviously the aboard which is
not necessarily what we can say about a
stronger isolation atomicity model too
strong animosity so in this particular
model we guarantee that all computations
isolated from one another so regardless
of whether a thread is executing within
an atomic blog or outside of an atomic
blog no values are being are being
transferred between between these two
threats and we'll use the same example
ooh okay that is not what I intended to
show on the slide the code fragment
should be there let me think what I
should what I can do about that yeah
okay let's just pretend that it's there
essentially what will happen is that the
thread will read the same values within
an atomic block rather than reading a
value that has been updated by threat t2
which we cannot see at this point and at
the same time threat t2 will see we will
see only the original values will not
see the value that has been put into
memory by threat t1 by his ma dlm
committed and I apologize for this quirk
so then even if the conflict happens the
isolation has not been breached but
again as you can imagine this particular
model is much much more expensive to
implement yes
so the question is what does it mean to
be atomic if if we don't know what's the
granularity of atomicity it's in that
case it's just a single operation so
essentially the whole atomic blog is as
seen by by a thread that's not using
this construct a tall as execute as
executing instantaneously so this is
this is the notion this is the notion
about a massive stop of strong animosity
here and there are multiple there are
several constructs that um can be used
in for this style of programming I'm not
going to go into detail on exactly how
they how they behave I believe that
actually has been talking this series
that discussed some of these construct
like like we try or else but their
introduction of this concert is
essentially required to make this thing
usable in the real world because if you
have full isolation between between
threads then there's no communication so
you have to provide facilities that will
allow you to to at least to some extent
pass values between transactions or to
support some some form of condition
synchronization and these are the
constructs that are around the slide
other than the generic atomic block
which is the retry construct and and the
or else construct which lets you which
lets you choose different alternatives
of alternative paths of execution and
this is we haven't invented these
constructs we only adopted them to Java
to the Java environment because they
have been previously introduced in a
constable in the context of functional
programming languages so I think that's
that sums up the sort of the language
integration semantics part of the talk
and now i will discuss our own
incarnation of the transactional memory
system so how these things are actually
implement under the hood so how we make
it actually work and perform as the
overview of the architecture of our
stack that we have that we have built
and the PSL we support
both Java and C although my talk today
is mostly as bias definitely biased
words that towards the java the java
portion of the stack but very similar
mechanism is actually available for see
that slightly different sets of
challenges that you have to that you're
facing when you're dealing with a non
managed language environment so
essentially have a transactions enabled
application on top of the stack and
below that you have in case of Java you
have a vm virtual machine and a legit
compiler that are aware of these
extensions and there will be passing
them the passing requests to for this
data tracking and coordination into the
transactional memory runtime which is
implementing the gist of of this of the
functionality and on this he said we
typically don't have run time
environment so essentially everything is
channeled through the compiler and this
transaction memory runtime in our case
it's we can run it both on top of
existing real architectures simpie
machines but we also cannot end up of
simulators so that we can experiment
with the accelerator acceleration of our
stock this is the most the most
interesting part of this I believe of
this the circuit picture is that is the
actual transactional memory to run time
and we just implement one flavor of a
possible well it's just it's just one
possible flavor in which you can
implement the whole the whole
transactional memory system that has
been backed up by a certain amount of
previous work and profiling and and
these type of things that let us choose
what we believe is the most efficient
approach at least for the for the
application that we've tested so we do
and place updates which means that
threads when executing either with
within atomic blocks or outside of
atomic blocks they'll write directly to
memory rather than an alternative
approach where they can buffer their
rights and apply them all to memory at
the point when the atomic block is
finished so I'm only I'm only going to
concentrate on the on the approach with
in place updates but that arm
that puts an additional requirement on
the run time system which which is to be
able to undo the effects of computation
because they are already in the memory
so if the transaction violated the
invariance the other most in isolation
variance it has it may have to be rolled
back so we need to maintain metadata
information that will will allow us to
do that and it's called undo log it's
very it's analogous to the database to
the way that they device transactions
are being done with respect to data
accesses this again was experimental
chosen as the best combination of for
the for the implementation with the
optimistic reads because we're trying to
make the reads as cheap as possible just
because they tend to be they don't be
more pervasive and typical java
applications than rides so we
essentially let the reads well Ed's lets
they let them we've led the Ritz proceed
and validate their correctness at the
end of the transaction so we hope that
everything will be alright and we'll be
able to commit the transaction if that's
not the case that will only and then
only then will will have to roll back on
the other hand we implement pessimistic
rights which means that we use exclusive
locks to to prevent a concurrent updates
to the same memory locations and on
rights we have the diversion numbers so
that the weeds them can then use them to
to to validate themselves and check
whether the to a halt transaction was
actually valid but because we use
exclusive locks there is a potential for
deadlock but this is not the dead log
that we've seen before which would be
sort of triggered by the programmer it's
dead log that may happen just because
the choice of the implementation so we
essentially we were obligated and we'll
do we do resolve it internally so we can
break and the transactional world it's
easy because we can break the cycle of
the deadlock cycle and we just redo one
of the transactions and the rest of the
transactions can proceed but we also bet
have that functionality in our system so
how do we really translate this atomic
thing into into the actual code this is
this is sort of a very abstract view of
things but we essentially started
transaction and commit the transaction
when the atomic block starts in and then
finishes and we track all these data
operations that happen in
side of the on the shared data that
happened inside of the inside of the
atomic blog so these abstract procedure
function method names DM right and tmw
hour which is team ride and TM read
their roles are explaining here as well
the right separation is supposed to
acquire lock and ugly the undo log the
read operation is supposed to report the
version number so that then they can be
compared to the version numbers they're
present in memory for it facilitated
validation because deadlocks can happen
we have to be able we have to be
prepared to handle them they can happen
at a point when we write so we need to
have a mechanism to actually return
control to the beginning of the
transaction we do the undo the effects
of this transaction and we execute it
again and that's language it dependent
that's why it's so abstract and secret
in this in a case of secret we can use
long jumps in case of Java we we
actually facilitate we actually utilize
modified sort of exception like
mechanism so that essentially you can
walk this stuff using similar similar
exception a similar mechanisms to D to
the one to the one that's used for Java
exceptions so how this tracking locking
of things is being done it's being done
by introducing certain data structures
in there in the runtime there are some
data structures that are required per
piece of data which for example in Java
to be per object or per field it depends
on what you choose and there also some
that are needed / threat or per
transaction super data we have we have a
transaction record which essentially
contains information about the state of
this piece of data with respect to to
this to the state up there of the
transactional execution and it can
either contain a version number it's a
pointer size fuel and can contain either
a version number or a lock in case this
state item has been written per thread
we maintain this sort of piece of meta
metadata that describes the state
directly describes the state of
transaction and it also contains
pointers to two additional additional
metadata which is the undo log and two
additional data structures that are that
are used for the purpose of validation
and there will be an example that
hopefully will make
clear what what they're used for yes yes
on the next slide I there's a little
presentation of what we can do and we
can do both essentially so the default
actually is the granularity of data
management is an object so we
essentially we embed we embed the
transaction record into the object
header so extent object headed by one
pointer size field and then we had
direct access to it when we couldn't
have a reference to an object but we
also we also allow work level level of
granularity and that's for Java it's
just this its fuel level ground larity
but it's also used for C because in c
you don't have a notion of an object
right so you have to you have to either
do this on the on the perp word basis /
memory world basis or / cache line basis
and so in this case you actually have an
external map that maps address is
essentially to to to the table that
contains the transaction records in Java
it would be rather happy to be hash
codes or equivalent of hash codes rather
than addresses because obviously GCS not
really you know using addresses in Java
and presence of copying GC is not really
good because like it can change and then
depend in the course of the program but
yes we do we do we do support both so
this is an example of how this how this
actually works how to put these all
these things together these this
metadata this transaction record and
again hopefully this will make make it
much more clear you know how this how
this whole mechanism works so we have to
get the threads both of them are
actually closing at all either
operations are enclosed in Atomics and
essentially what what they're trying to
do is there one the thread t1 is trying
to copy value from one object to the
other both fields using some local
variables so because this is an atomic
execution we would hope that a t2 will
see either the state before the whole
thing happens with thread t1 or after so
with either will see the values 0 0 in
the fields also see the values
summon in the fields so let's see how
this how this is going to proceed we'll
start with thread t1 reading a value
from field X up object foo and what
happens done is we do cord a version
number that's associated at this point
with this object which is in its
transaction record which is shaded blue
midst three then we have the same thing
or knowledge is something happening 42
it's reading a field X from object bar
and the version number is associated
with this object is 5 so it's recorded
it records is this value in the we'd set
thread t1 then performs right the field
of object of object bar and then
multiple things happen so one thing is
that it has to this threat has to record
the version number that's associated
with this object at the point of bright
and you'll see why and later on and then
it has to update the value it has to
update the undo information to be able
to potentially rollback this transaction
in the later point so it's essentially
recording what's in this object in this
field at this point so it is easy but
the value is zero then it can acquire
then it can modify the field because it
just copy the value from from object foo
and acquires a lot so it puts its own
transaction descriptor t1 just obviously
this is just a just an abstraction of
transaction descriptive but it puts its
transaction descriptor into the
transaction record to indicate that this
object is now exclusively logged by
transaction one at this point when
thread t2 is trying to read from the
same object it will encounter log that
is how has already been acquired by
transaction t1 and it will have to wait
thread T t1 and the meantime can proceed
it'll read from from from from objects
to again and it doesn't have to now
actually does record it does report the
the version number for the field of this
object which is which is still three and
then we will right to object bar which
again means that it has to update the
undo information it doesn't have to
update the
right said just because the subject is
already is already logged by the
transaction so so there's no need to do
that and at this point a threat to 20
transaction or 32 can proceed to its
commit at this point will perform a
validation so we'll have to compare the
version numbers for the object that this
this this transaction has read with the
actual version numbers that exist in the
main memory at this point and we do that
they're the same which means that you
one can successfully commit at this
point it will release the log updating
the version number to indicate that the
value of the values in this object have
actually changed as a result of a comet
which will make the threat T to resume
its execution it doesn't it no longer
has to wait on the lock and enter and
the data structures are no longer
relevant preferred to mod and 32 can
proceed to to eat from from another
field of object bar it recording the
version number this time it's different
version number obviously and it's in its
reset and then 32 gets to the point when
it's trying to commit its its
transaction and it also compares the
values from read set to the right set
but this time they're different so this
this means that another thread in the
meantime has updated this object while
teachers transaction was in flight that
indicates that there was a conflict and
transaction to you has to be aborted yes
they are essentially 31 bit there are 31
build value so it's I'm sorry yes thank
you the question was whether it's a
problem that the size of the version of
the site of the decides the size of the
field that contains the version number
is a problem because of the potential
for overflow and obviously this is a
possibility but it's large enough that
we actually we actually don't do
anything about it but yes but still
theoretically this you know some bizarre
workload could potentially make it to
break but I probably should have said
this before we were actually sort of a
research lab so this is a prototype it's
not you know it's not anything that that
will happen any time soon at least you
know in a until product or anything so
this is this is all researching stuff
yes actually we do support master
transactions we support closeness to
transactions will actually do support
both types but for clothes nesting what
we do is because we have sequin oh sorry
I should have repeated the question it
doesn't mean it doesn't seem like like
this supports nested closeness like it
like it like the system supports nest of
transactions and my answer is that mean
d do support close nesting and actually
a recently also open nesting and the way
it's done so I'm not sure what would be
the read the problem the undo procedure
or in general so the way we do it we
have we had the notion of the amendment
oh so at the point when we start a
nested transaction we remember certain
state of this transaction and add
additional metadata structure that's
called a memento and this for example
contains a pointer to the undo log at
the point when the message transaction
stop started so essentially the undo
procedure will be able to roll back just
the point of you know the snapshot of
this memento being taken rather than to
the beginning of the transaction and so
this allows us to you know just sort of
selectively just roll back the effects
of the
transaction rather than to roll back the
whole thing yes it's it's not going to
happen in Java but I was wondering what
happened since you suppose see if there
is a DMA transaction value have a
synchronous cream I'm sorry
oh so it's the question is about how to
handle I oh this is a very interesting
problem and in Java it's as you say it's
it's somewhat simpler just because the
interface between the virtual machine
and the operating system is rather well
defined so you can play all sorts of
games I have to say that we we actually
do not have a great answer for the C
code for the sea case because well you
just don't have exactly the you know the
everything is done through the library
so you yeah it's difficult to to
determine that an operation that you're
performing at this point is going to be
really dangerous in the sense that is
going to corrupt some data structure or
you know right to to a channel or
something like that so the best the best
we can do right now is we can prevent
these operations from being reacts acute
it so we can make a transaction that is
about to perform an operation like that
which we would have to know that this is
the operation that we want to handle
that way but we can prevent this
transaction from Lee executing itself
essentially it's becoming a meta you
know a an uber transaction and everybody
else that will try to conflict with that
transaction will aboard but that doesn't
necessarily mean that we're getting rid
of all the isolation related problems
because this transaction can still write
to a channel that a non-transactional
code can read and you know this this is
really all very dangerous if you will so
yes I short answer is I suppose we have
partial solution but not the full
solution to that so you keep taking how
about the member states
robach a mistake what about register
we're treating your competition your low
values your register if you see a
conflict you want to roll back to the
previous yes all purchases there so in
Java it's you know you just have to keep
track off so it's it's I think it's it's
done at a slightly higher level at least
in case of Java so essentially you
remember the values of the local
variables and so yes you you have to
restore them at the point to the same
state they were at the point when he's
when you started the transaction yes if
they were in two different bar objects
the teachers will see a wonderful
yes but so you say that the the update
of so the question was what if thread
what it feels X&amp;amp;Y we're in different bar
objects so essentially would have three
objects in here and all three objects
would be used within the left's and let
the tea once atomic and all three
objects will be used in t TUS atomic
well we would just have you know another
object another object that would be
locked at the point of update to that
field alright updated yes it's the only
one it would be sufficient for it to see
just one field field of an oven of one
of the objects it has been updated by t1
to abort so it cannot see anything from
that happen in t 1 i'm not sure if this
answer the question yes what about at
the end but suppose that the data was so
inconsistent that it blew up before
that is that now I understand so the
question is what if what if an execution
that is invalid in the sense of the of
the atomicity property leads to let's
say an object I feel of an object being
nullified when it's not expected that
this transaction will see an all in this
field ever and this is the result of
let's say a value that's visible because
it has been updated but not committed so
it will be rolled back by the in the
meantime the threat has seen them value
null and it throws a
nullpointerexception right that's sort
of the scenario I don't think we have
any special mechanism to handle that I
mean so okay let me let me think about
that no we do handle exceptions that
happen in transactions and in our case
you actually yes you actually you
actually commit a transaction so this is
actually somewhat somewhat sensitive
sensitive aspect of the exceptionality
assembled as a somewhat sensitive aspect
of this whole semantics so what we do
and that's from deviating from a
question but maybe we can take it off
offline I'd be you know more than happy
to to talk about it because now I'm not
sure which question I should answer
yes so let's let's take the question of
line I just say say really quickly that
we commit on the exceptions so
essentially I don't think we do anything
special we would just say you know an
exception happened committed transaction
and if this exception is not being
caught then it will it will it will
cause a problem right so there's
fancy yeah I agree
I really I have to say I I have to think
about it a little more and so hopefully
by the end of the talk I'll be able to
give you a better answer I think there
was another question but maybe it was
not anymore so now we I'll talk a little
bit about how to make this a little more
efficient because this simple
translation that you see over here it's
sort of intuitive and that's how most of
the systems have had done it but it's
not necessarily the most efficient one
so you have these operations that nicely
abstract you know this transactional
wrappers around around memory accesses
but it's again not necessarily a best
solution because there is some
redundancy within these operations that
can be eliminated this is just an
example of what you can do to make this
go a little faster and and this actually
need makes the whole thing go a little
faster our a lot faster so essentially
we split we split the operations on data
accesses into into more fine-grained if
you will so instead of having these
coarse grain I'll print for writing
operate of open for these cars going to
transaction writing transaction read we
have open for ride open for read and log
which are which are separating the sort
of functionality of that wasn't closed
in these in these transactions
previously and what it allows us to do
is it allows us to eliminate some of the
operations that that we had to perform
previously at every attribute data
access so we expose the redundancies and
for example we don't have to log the
object object pool every time we access
it it's enough when when we do it once
and that's exactly what the transaction
what the TM open for right it just locks
an object so we can do it once and the
compiler can sort of people optimize
away everything else we also don't have
to then open for read because we've
already written we already acquired a
lock for this object so so it's ours we
don't have to worry that this suite is
going to get invalidated because we're
writing to it nobody else is going to be
able to to get to it and we can also
mean
see some of the logging operations so
essentially this this big chunk of code
that we've seen before it can be it can
be abbreviated to something a lot more
short a lot shorter and not cheaper and
this is just an example of an
optimization there are others that we
adapted some of the standard
optimizations that are already existed
in the compiler infrastructure there
were there were some roughly minor
modifications mostly or mostly most
interesting ones will do to nesting
because you cannot let's say you know
eliminate some of the redundancies
across nesting levels just because of
these partial rollbacks that we would
like to support so you cannot just
eliminate logging operation let's say on
the nested under logging operation on
the nest in the nested scope if you ever
want to roll back to the to the to the
nest transaction start but we have also
some other transactions civic
optimizations we did we try to detect
immutable data immutable class musical
fields and we don't have to insert those
barriers those tracking operations for
them also if something is completely
transaction lachlan never escapes the
scope of a transaction we also don't
have to do it so that allows us to
eliminate cemetery redundancies the
barrier in this context it just means
those memory extra memory operations
that the transactional memory system is
inserting for every data access and we
also do in lining of the fast parts of
these of these operations just just a
standard technique obviously to to make
it would make it go faster but these are
the optimization effects is just that
I'm showing just this very selective for
clothes we have a lot more data in some
of our papers not going to present them
here because that was just supposed to
be a scope and i'll give you but it you
can you can tell you can see in here
that while we still have a lot well a
lot about thirty percent overhead over
over the case when there is no snow
comparison control and the most
optimized case we're actually is about
forty percent it brings us down quite a
bit from about eighty percent overhead
which which is still pretty good because
other transactional memory systems that
we were we're aware
they had in single-threaded case they
had much higher overheads on the order
of like twice as slow so we consider it
to be a pretty good single threaded
result but obviously since you have all
these tracking operations you you have
to expect that you'll be paying some
costs in this case and the question is
whether this you get it back in the
scalability and performance for
multi-threaded work plus on it and when
you actually have physical parallelism
so this again just an example of I'm
coming back sort off to the to the to
the roots the beginning of this of this
talk when I presented this example of
the hash table that has undergone a
certain number of refinements with locks
in mind and then we've implemented it
also with with Atomics so we have four
four types here we have unsafe which is
essentially no synchronization
whatsoever we have synchronized which is
just regular Java synchronize rapper
concurrent which is this fine grain
optimized implementation using still
logs and our atomic employment a shin
which is which is just again simple
wrapper so it's essentially the same as
bullet to but using Atomics rather than
synchronized and for one hundred percent
reads we are actually worse than than
the concurrent refined log based
implementation but we scale pretty well
unlike that the sort of the the standard
synchronized version which is which is
not scaling very well speed up this is
not is not it's not constant but when we
when we throw some rights into the
picture then they compare and hush mob
the log based version is not suddenly
not doing it's not doing so well anymore
and an hour atomic simple atomic version
is still is still scaling scaling quite
good I think I I think I don't have all
that much time right out ten minutes so
i think i think that's about this about
what i need so there's thought there is
still this single single threaded
overheads and we're just you know trying
to figure out what to do with them and
whether we can optimize them away using
some other mechanisms and one of them an
obvious sort of solution to think about
it would be a would be hard work so
eration so this lights presents so the
foundation for what we try to deal with
respect to hardware acceleration which
is what are the major overheads of
sopranos actual memory so that's the
transaction memory system that's in
dying done entirely in software and it
you can tell it's mostly the overhead is
and yes
no this is this is this is just the
overhead of the actual track of the
actual transactional execution so so the
question was whether the overhead comes
from instrumenting all code or just the
one that that's sort of you running the
actual transaction the actual Atomics so
this is actually very in this case we
only insert the overhead the armed the
overhead is only is confined only to the
transactional code so we don't insert
those barriers trucking operations on
for this particular example on the non
transaction code because we're doing the
week atomicity thing that I talked about
before
yes transaction will see that value the
transaction will see it about this is
this is the that's exactly why I were
introduced this weekend strong atomicity
notions because essentially most of the
system start are currently in from
implementing week out of my city and I I
agree this is probably not the best or
not not the most intuitive program model
but the overheads are related to
struggle MST are still relatively high
so it was it's it's it's it's an
interesting trade-off and we are
actually leaning towards trying to push
the strong animosity model which is you
know perhaps everything isolated from
everything else but but that's we were
able to get better results only recently
with respect the optimizations and bring
down the overheads so this stays data is
actually based on the week a domestic
model when there's no other hat on
non-transactional code but at the same
time we have weaker weekly guarantees
with respect to what transactions can it
can uncle or cannot see so um in most of
most our heads most of our most of the
overhead comes from from the real
barriers the operations that are used to
travel the read operations and from
actual validation of these these root
operations that happen that happens that
commit so I'll talk about how we propose
to deal with that but before I wanted to
just give you one slight overview of
what hardware transactional memory
proposals around and so the major one is
as the sort of traditional harder
transactional memory HTM which is a
somewhat restricted system in the sense
that it allows to a execution of
transactions very fast but as long as
their cash read cache resident so on the
on a physically parallel machine if your
transaction operates data that is fits
entirely in your cash and this case gets
never invalidated by and any other
thread then it has been isolated so you
don't have to do anything you can commit
the transaction and you know leaf
happily live happily ever after but this
obviously is very restrictive because
really it's difficult to encode any more
interesting transactional semantics like
open nesting closed nesting would they
really mean in this context and also
the size of transactions quite quite
seriously because well they're
restricted to the size of the cache the
machine has so what has been proposed
next is is hybrid which which
essentially says if your transaction is
simple and small enough I secure using
hardware but if you fail you can fail
you can always fall back on software in
which case you can handle arbitrary size
and arbitrary semantics but the that has
the particular yes yes thank you very
much yes yes absolutely so yes that
first execution was used HTM via the
second execution uses STM and the the
downside of this particular approach is
that you still can accelerate only
transactions that are very restricted so
they do not then on that the ones that
have arbitrary size and semantics there
they're still going to be executed
purely in software so that's that's
where we come in an hour and our idea
which is to rather than baking the
semantics of a transaction or putting
restrictions on this size of a
transaction is putting in all the hard
work we propose to eliminate these
overheads that we think are the most
serious ones in the transactional
execution using hardware using hardware
support and we call it has TM which is
hardware accelerated software
transactional memory and does the idea
is rather simple we would like to have
support for for marking of blocks and
the cache lines and so a thread can
actually claim the ownership of a cache
line and and it can query whether it
owns still owns this discussion but the
other threats can actually come in
invalidated cache line and invalidate
the mark bit so that the thread will be
aware that it's not it's not knowing it
anymore and that gives us the ability to
implement a powerful powerful pin to
filter and software essentially these
two questions can be very cheaply
answered so have I access this location
before and you know has anyone in the
meantime modified it from the time from
the time I ax is up till now yes I guess
maybe that's a little redundant but
that's that they essentially question
that we would like
two other would like to answer yes
no yes oh okay that was a question from
there so this is this a proposed set of
extensions to the to the instruction set
architecture and it's just the names of
the methods essentially that would be
used from the from the code to mark
these beds to query those beds and reset
all the bits which would be which would
be needed to implement the algorithm
which I'm going to very briefly
described on the next slide actually so
yeah we do support because we eliminate
only those sort of very generic
overheads we support arbitrary certain
transaction semantics because we're only
child we're only telling you that you'll
be able to reduce the overhead of the of
the validation and you will be able to
reduce the overhead of the or eliminate
the redundancy and the read barriers and
that support allows us to shrink the
size of the code for the we accessed
racking operation from 12 instructions
to instructions this is until assembly
instructions and this yes no okay so
this allows us also to execute
transactions in this hybrid transactions
have retired with transactional memory
mode as as described before because if
we can we can tell that none of the
cache lines none of the data that we've
access has been invalidated and in this
case we can completely skip the
validation procedure and we can also
completely skip the real barrier
operations essentially having the the
same solutions hybrid transactional
memory so this we call it aggressive
mode because that that can fail just
because you know we rely on not on
nobody actually accessing this this data
at the same time and there's a small
team is a shin that you can you use
those bits across the across the
transaction boundaries but that's I
think that's about it and this is this
slide this the set of numbers describes
how much this hired the ostium
acceleration which is the overheads in
the symbol for every case and it's on
the order of thirty forty percent at
least yes how you anticipate handling
cash evictions you just for set to the
software model so the question is how do
we anticipate how many caning cash
evictions so if I understand it
correctly so you're asking what happens
if you are running in the aggressive
mode yeah so since this is essentially
the same thing as as the hybrid TN that
I very briefly described before the
handling is essentially the same which
is if anything goes wrong you have to be
execute the whole transaction because
it's only at the end when you you know
when you realize that or actually it's
at the point when when the cash gets
evicted when you realize that that's
something went wrong but there it's too
late so so you actually you have to you
have to essentially we execute the whole
the whole transaction in the aggressive
mode and in the non aggressive mode the
eviction of the cache line results and
all those marked bills being reset so
when you get to the end of the
transaction you try to perform commit
you realize that some of the data that
you've accessed before has been accessed
by other threats because the cash man
has been evicted and so you you're
obligated to perform a full-scale
validation procedures to go all through
these version numbers and compared them
with you know what you have in your weed
side so compared what you having you
reach it with what you have newer in the
memory and then you can still commit but
obviously that takes time so ideal
situation is when you get to the end and
you none of your mark beds have been
have been invalidated by cache line
evictions and then you can say I'm done
you don't have an interpreter underneath
you so that you've actually generated a
code which is presumably using one model
or the other then then falling back from
that from the aggressive form to the
other form implies having that other
form of code somewhere right yes so that
the barrier has to have the actual data
the transactional wrapper around data
access operation has to have that the
ability to dispatch between the between
to the two different modes right but in
the case so it's only I think this is
only the problem when so when you when
you run the aggressive mode you actually
don't execute the barriers because you
assume that nothing is going to happen
so it's only when the when you have the
non-aggressive case when you basically
say okay I'm checking the mana and
checking the bit if I if I already own
this dish cache line I don't have to do
much more than that I just can just
return and read the value if i do not
own that that then I have to record the
version number and go ahead and wait
until from mid time to decide whether
I'm still valid or not so there is this
there is obviously the the conditional
there but that's that the other that's
how that's how it's handled so again the
overheads already reduced by at least
thirty forty percent with respect to the
software transaction memory and these
are the numbers for actually they're
coming from our simulator and what this
set of eyes on you say the new set of
iso instructions have been implemented
and you know it shows how these things
scale on you know on from one to two
cores and the hybrid TM is again it's
just this mode when you essentially once
you lose a cash in on your dead so
that's why it occasionally scales even
worse than uh than the software
transaction memory because you just have
to you just have to redo your not only
ways so you have to do the same thing
that this software transactional memory
has to do later on but he wastes time
while executing in this hybrid TM mode
so essentially the the bottom line of
this one this slide and the next slide
is that the hestian scale is better than
then all the other than all the other
schemes
and that's that's that's the end of my
talk conclusion I just I just wanted to
say that you know we believe that this
this may be about what's our alternative
for programming with logs and the SDM
still the pure SDM status scales pretty
well but there are still over heads that
are will be nice Oh be nice if we could
reduce and therefore we need some sort
of efficient and general purpose
hydroxylation and so we're we're again
we're working on all aspects of the
system and and hope to to get you know
better and better results as a time as a
time elapses so thank you very much and
I can obviously take more questions yes
whatever hey good so there are oh thank
you the question is how do we handle
long from long long term long-lasting
transactions transactions that would
essentially operate in the background or
essentially long transaction that could
be preempted by short-term transactions
and therefore be killed every time they
try to track for an extended period of
time that's so there there is no ideal
solution but we're working on a piece
too so one of them is it's more or less
sort of convention management related so
it's it's I mentioned that before it's
to try and make this transaction at some
point when you perhaps be you know
whenever you're able to decide let's say
based on dynamic profiling and
conventional manager or something like
that that this transaction really has to
finish now because you know it's been
reacts acutis ohm any times that it's
ridiculous so we make this transaction
revocable so essentially if there are
you know other transactions that are
independent with respect to their data
set from this transaction that will
still be able to execute in the system
but anything that would try to aboard
this longer along running transaction it
would essentially bounce off it and you
know we get aboard itself which would
allow us to you know once and for all
for all commit that long running
transaction and it obviously has its
downsides you you probably limit
concurrency in the system and in general
by doing that especially this
transaction is really really long but
this this is a viable option for example
for if you have a data structure that
needs to restructuring like rehashing of
a hash table or you know rii rebalancing
of a tree then you may want to you know
just say okay just make this rebalancing
happen and now be done with it and you
don't have to necessarily no limit
comparison there in the other parts of
the system just just because you want to
balance that tree another solution is to
try and use open transactions which are
a very dangerous construct in some sense
for an everyday programmer but can be
very powerful construct for an
experienced programmer and what that
allows you to do is it allows you to
relax isolation of transaction and a
controllable semi controllable fashion
and there's there's a whole model that
sort of comes comes behind it but the
bottom line is that you know in the
middle of the transaction you can have a
piece computation that you can say I
commit it and you know I I'm willing to
say that that this part of computation
will be visible outside of this this
transaction to others but I'll have a
mechanism at the higher abstraction
level that will be able to restore
invariants in case the big outer
transaction is getting aborted and so
what it lets you to do is let's say lets
you you know update a counter in the
long-running transaction on which
everybody else would contend and block
while you're still currently well you
see while you're still running and
obviously you again you have to take
care of the situation when his
long-running transaction gets a boarded
this country's you know it's exposed so
what do you do but there are handlers
that usually better user the programmer
can can program that will help you
restore this imbalance in the higher
level but again these are these are very
sort of research ideas at this point
there is snow one fixed set of solutions
for that problem yet but it's it's a
very interesting problem
well thank you very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>