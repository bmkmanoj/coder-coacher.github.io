<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Photo Tech EDU Day 1: Photo Technology Overview | Coder Coacher - Coaching Coders</title><meta content="Photo Tech EDU Day 1: Photo Technology Overview - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Photo Tech EDU Day 1: Photo Technology Overview</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7SuDOMhUUMg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome to photographic technology
day one we've got coffee and doughnuts
in the back for anyone that didn't hear
that already and in future weeks we may
not try to bribe you that way it's kind
of futile trying to bribe Googlers with
food or money but we try thanks for
coming today I'm gonna I'm gonna start
with kind of an overview of not not the
whole course curriculum but sort of what
I considered to be the front-end part
the stuff about light and imaging and
optics and sensors and then there's a
whole bunch of open-ended material that
that follows in terms of what you do
with the images that you get out of a
camera we're going to try to cover this
this kind of front-end material is going
to be subject of probably a half a dozen
more lectures when we get into depth in
the different areas and then there'll be
another at least that many probably more
on sort of the applications and back-end
and how do you how do you handle and
process and apply these digital images
we've got several volunteer lecturers
signed up to help and probably do half
the lectures myself or something like
that today I've got way more slides than
will fit in an hour so I'm thinking of
splitting it up over today and next week
and it's it's still just an overview
we're not going to be going into depth
in terms of how to how to calculate
things with equations and so on but well
we'll get to some of that in later
lectures because you do need to be able
to to do some of these calculations to
to make reasonable decisions about
photographic system design and operation
so let's let's jump into it here so
photography is basically the the art and
science of writing with light of
recording patterns with light and that's
what it means and light is a really
magical substance that you have to
understand and so we're gonna address
its properties in terms of rays and
waves and photons and so on and you have
to kind of approach light from all these
different sides in order to to do the
things you need to do with it
to understand and analyze photography as
to whether it works or not human vision
is the ultimate judge
when we make photographs we generally
are intending to make reproductions of
an image that a human can look at so the
human eye is the thing we're trying to
satisfy with all this and that makes the
problem extra complicated because the
human eye and the human brain is a very
complex system so the the technologies
that go in between between the sort of
light in the real world and the light
fields that we reproduce on a piece of
paper or a screen for someone to look at
there's a whole bunch of technologies in
between that's the subject of this
course and it's all under our control
there's no there's no magic there which
is the the point of this slide sometimes
it looks like there's magic and
sometimes the the magic really has very
simple explanations like my friend Steve
Chong here who likes to take pictures of
eyeballs in motion and so on and he
really does take glass eyeballs and put
them in motion he's not faking this
stuff up in Photoshop and sometimes he
turns the picture upside down which is
why the water is looking like it's going
up but the point is there's no magic
it's all understandable so we need to
understand the technology respect the
physics please our eyes play some tricks
and have some fun so I made up my own
three laws of photo dynamics these are
some of the concepts we're gonna try to
get a clock across in this class the
first one is that even an ideal camera
needs enough light to make a good photo
you can't make great photos in the dark
you can't make great photos with infant
Tesla infinitesimally short exposure
times and tiny apertures and insensitive
sensors and so forth you get you got to
have enough light that's that's
fundamental number one even for an ideal
camera the second thing is there's no
such thing as an ideal camera well
there's an ideal that you can kind of
define but you can't ever build it in
practice and we're going to define that
ideal and and understand what it means
so that we can analyze quad
quantitatively how much light you need
to make a good photo and the third thing
is that the closer you can come to that
ideal camera the better you are of
course so the ideal is out there you
can't quite get there it's like the pot
of gold at the end of the rainbow but
it's something you can strive for so
we're gonna learn how to do these
computations and different circumstances
what goes wrong what's what's not ideal
about camera
and what what can you do to bound the
problem part of the course is going to
be me going off on historical tangents
to explain why we look at things the way
we do and so the first of these is
Johann Kepler who a couple years after
he was finished getting rid of Tycho
Brahe he with mercury poisoning he if
you haven't read that book that's a good
one
he he figured out a bunch of stuff about
optics and there was a there were some
long-standing problems at that time this
is before the discovery or the invention
of the telescope
there were lenses around but people
didn't really understand imaging they
they made images with pinholes and
pinhole type camera Obscura's and they
there was this prospective esteem that
had grown up in the in the field of art
and painting so people kind of
understood the idea of rays going
through holes and making images but
there were two big problems with this
that people didn't understand that
Kepler figured out the first was how
come if you image something through a
square hole or a triangular hole you
don't get a square image or a triangular
image and people had a lot of trouble
with this and they they they had this
notion of a single ray from an object to
an image being an important thing but
they didn't really have this concept of
a of a bundle of rays and and they were
quite confused as to whether light
really propagated in straight lines as
rays or not then so Kepler took that
that notion of straight line propagation
very seriously and analyzed what would
happen if the hall wasn't a tiny little
round hole so he had like a little
triangular hole and he put rays through
it and found that each each point from
the object would make a little
triangular blur in the image space but
you'd still get a good image you'd still
get a round image of a round object it
just had this little triangular blur
pattern around the edge that you
wouldn't hardly notice so he figured
that out by taking the notion of light
propagating in straight lines very
seriously and literally which other
people hadn't quite done and by taking
advantage of the concept of a whole
bundle of light rays as opposed to
single rays forming an image the other
the other problem with pinhole imaging
that Kepler
at that time was the the observation
that several people had made that if you
if you made a pinhole image and you
looked at the moon with it you could
measure the diameter of the moon get the
angular diameter of the moon in the
night sky you could do that same
measurement when the when the when the
new moon was occluding the Sun in a in a
total solar eclipse you could measure
the the dark disk of the moon against a
bright background depending on which way
you measured it the bright moon against
the dark background or the dark moon
against a bright background you got a
different diameter and they were really
confused about why would the moon be
changing its diameter and Kepler was
able to show that the moon is not
changing its diameter what you're
measuring are the different edges of
these little blurs that you get from
your pinhole and though the pinholes
were big enough and the measurements
accurate enough that they were actually
getting different different enough
numbers that they thought they had a
problem and Kepler kind of resolved all
that so this was the beginnings of
geometric optics and understanding
imaging now Kepler went on to do a lot
of other good stuff like he improved on
the the Galilean telescope and showed
how to make a telescope with two convex
lenses that would do a better job of
imaging distant objects he was able to
draw pictures of rays to explain how
imaging worked and he was able to do all
this before the laws of refraction were
known he didn't have Snell's law yet so
he and I'll show you what that is in a
second but he he estimated that when a
ray of light went from air into glass
that the angle relative to the normal
would be reduced by about a third
so if you had like three degrees of
angle of incidence you'd get about two
degrees of angle of refraction that kind
of estimate that two to three ratio of
angles was plenty good for analyzing all
of optical imaging and telescope design
and so on until you got into subtle
effects like spherical aberration so he
was frustrated that he didn't know the
true law but he had stuff that was good
enough to make a lot of progress
around the same time Rene Descartes did
a bunch of stuff he wrote a paper
there's my sound gone did I did I kick
my switch you still hear it okay this is
okay Descartes published a book called
Nadia Petryk and he he he had worked out
Snell's law from Fermat's principle of
last time people say that he he worked
out Snell's law because Snell had
already worked it out and told it to him
and and but he hadn't published it yet
but so Descartes was the first one to
publish Snell's law but people think he
got it from snow which is why they named
it after but he he worked it out
differently Snell had measured it I'll
show you what it is in a second next
slide but but but the current was
actually able to to kind of rationalize
and say where it came from which was
forget exactly how Fermat got himself
involved but he had this principle of
least time which says that like going
from point A to point B takes the route
that's quickest and if you if you use
that principle you can derive Snell's
law it's a good homework problem right
there
a few years later actually a few decades
later Christian Huygens went further and
explained how the principle of least
time can actually be derived from the
notion that light propagates as waves
with different velocities in different
media and the thing that's important to
realize here is this the the notion of
light propagating as a wave is
fundamental and it has fundamental
important consequences when you're doing
photography and imaging so this is where
the the wave the the nature of light
going from Ray's understood in the early
part of the 17th century to waves in the
later part of the 17th century was a you
know fundamental progress in being able
to understand imaging systems
unfortunately it kind of got put on hold
by a couple of centuries by Newton who
didn't didn't buy into the wave idea at
all so I took a took a few hundred years
to come back
so Newton thought light was particles
and and that idea came back again with
with Einstein and that's also
fundamentally important to photography
as we practice it today so because when
you have an image sensor in a camera
even if it's a piece of film with a
bunch of little crystals of silver
halide or something it basically works
by discrete light absorption events
photons little packets of energy that
cause some change to happen in a crystal
in a in a silicon solid-state image
sensor we basically can almost count the
photons that come in by counting the
electrons that those photons generate
that's what the ideal sensor does
anytime light goes into the silicon
crystal a photon gets gets kicked up
into a state where you can collect it
measure it count it and if you can count
it with a fine find enough resolution to
say exactly where it is not just how
many but how many in what location then
you get a perfect ideal image as long as
that location is split up finely enough
compared to the wave diffraction effects
that there's essentially no loss of
information that's that's sort of how
we're going to define an ideal sensor
the ideal sensor tells you where every
photon came in with a resolution finer
than the diffraction of your imaging
system these systems have noise the
ideal systems have noise because these
photon absorption events are statistical
so this this noise is called shot noise
it's the it's the statistical
distribution of a bunch of independent
absorption events the the important
property about light as particles is
that every one of those particle
absorptions is independent in the sense
that it's it's not like there's some
intensity and if you've gotten ninety
percent of them and miss ten percent of
them you can you can count on the
statistics of the ten percent you miss
know that the statistics only only
depends on the number you absorbed and
we'll get into that in more detail later
so going back and looking at image
formation why not just use a pinhole
camera by the way a lot of the a lot of
the slides here I've just put together
illustrations to talk about in most
cases from public domain sources a lot
of the public domain books off google
book search have provided images like
this one and a lot of the slides are
illustrated from material off Wikipedia
which are all publicly public publicly
usable repurpose of all integers about
half of those images and Wikipedia that
I've used that are related to
photography are images that I put on
Wikipedia so some of them are mine and
some of them are other people's but
they're not mine anymore cuz I give them
away a few places I've got illustrations
that are not public domain and I've
tried to give credit where appropriate
for this so this is just a picture of a
camera obscura that means darkroom for
those of you who don't speak old French
or Latin or whatever it came from you
poke a hole in one wall and you get an
image on the other wall from Lees light
rays coming through so why not make
cameras this way and the the problem is
that this this is very non-ideal because
you don't get very much light and you
get a lot of diffraction blur the
diffraction blur depends on the size of
the hole that the light is coming
through that's making your image and the
smaller it is the more diffraction blur
you get and with a pinhole if you make
that hole bigger to get less diffraction
blur you get more blurred due to that
effect that Kepler found where you're
you're blurring with the shape of the
hole if you put a lens in the hole you
can get rid of that by converging the
Rays so the the ideal camera uses a lens
to converge the Rays through a large
hole to limit the diffraction blur and
get a lot of light you find lots of
illustrations of this concept of imaging
in the early photography literature for
example this one by Herman Vogel wrote a
really nice book on photography 1874
about the chemical action of light and
photography and it's got good good lens
descriptions and so on so the the upper
image here shows a light coming in a top
ray coming in parallel to the optical
axis and going to a point that defines
the focal length and then another bundle
of rays from a object point propagating
to the left to make an image point so
it's very simple all you have to do is
make lenses that converge all the Rays
from a point in the object to a point in
the image plane and of course that's
sounds simple but it's practically it's
it's impossible to do perfectly and to
do it ideally means that that's
something we can define and analyze so
let's see so this this is the notion of
focusing taking all the Rays from a
point bringing them back together at
another point the Hermit Vogel referred
to that point as a build Punk'd which
surprised me because after I had spent
about four years understanding
researching the history of the word
pixel and traced it back to the word the
German word build Punk'd which is the
German word for pixel and the original
patent on television I then found that
you know 15 years earlier Vogel had the
same word in his his paper on
photography for an image point built
Punk means a picture point so he wasn't
using it in quite the same way that we
use pixels today but he had basically
the notion of an image point already so
when we think about ideal imaging we
have to think about what are the one of
the things that limit the perfection of
your image what limits the the sharpness
or the detail of the resolution that you
can get in an image in an ideal camera
you don't have infinite depth of field
depth of field is the the range of
distances in the object space over which
your image is sharp enough there's my
signal cutting out again I sounded it
sounded funny here yeah let's see did I
do something funny on this
sometimes I hear myself funny and then I
know it's not working so you can think
of a limited depth of field as a defect
in a camera that is if things are
further away from where your focus
they'll be blurry if they're too close
compared to where your focus they'll be
blurry but I don't think of it as a
non-ideality I think of it as a property
of the ideal camera because you need
some collection area in your lens to
make an image and that defines a cone a
cone of rays in your object space and in
your image space that will give you a
sharp image that's an ideal situation
it's there's a trade-off to be made
between how much light you get and how
much depth of field you get but that's
all within a certain idealization of how
perfect lenses work non-ideal effects
are deviations from that so even the
ideal camera is limited by depth of
field and the ideal camera needs enough
photons to make an image and therefore
it needs some time to collect those so
there's a there's an exposure time so
the the sharpness of your image is again
limited by motion effects if your
subject is moving or your camera is
moving during the thousandth or
hundredth or tenth of a second or
whatever it is that you collect photons
you're gonna get blur again that's not
because your camera is not ideal even
ideal cameras have that limitation so we
need to understand it you get
diffraction blur again when you when you
change the diameter of the lens to
trade-off the amount of light and depth
of field you're gonna change the amount
of diffraction blur as well so it's the
the properties of light the the particle
properties and the and the ray
properties of focusing and the wave
properties all these things are part of
the idealization that causes the ideal
camera to have limits lose signal again
here we go
and finally the shot noise so the
diffraction blur comes from the wave
effects the depth of field from the Ray
effects the motion blur is a is a time
that it takes to get enough light and
the shot noise is again having to do
with enough light and the particle
nature of light so all of these these
ideal effects depend on all of the all
of the ways of looking at light as Ray's
waves and particles and we need to
understand all those properties of light
these cameras also have non-ideal
properties have the note in the upper
right basically says that more more
light always helps so if you can if you
can take a given camera and increase the
amount of light on your subject by
bringing in extra lights or cranking up
the intensity of the Sun or not working
on a cloudy day or putting a reflector
in the scene or something like that
always helps so it helps with this
trade-off that you have in the ideal
camera space non idealities also can be
helped by having more light in many
cases but things like the resolution of
the sensor or the size of the grains in
the film that you're using the
aberrations which are the the
imperfections in your lens the extent to
which it's different from an ideal lens
the flare or glare or ghosting which
means the the paths for light to scatter
around bounce off of your film bounce
off your lens back to your film things
like that to create excess light that
shouldn't be there other noise sources
like electrical noise in the readout
when you're trying to count photons and
your a to D converter is actually
counting five or fifty electrons worth
of noise that's a significant non
ideality so we have to understand all
those kinds of things did I skip on
so in the front end of the camera we
want to study lenses and sensors that's
where all these effects of light come in
that's all the things we need to
understand we need to understand image
formation by lenses the limited depth of
field the motion blur aberrations flare
distortion we understand the light
sensing sensing by silver halide and by
silicon we're probably not so interested
in film anymore so we'll focus on
silicon distinguishing the ideal effects
of shot noise and sensors from non-ideal
effects resolution limits aliasing
nonlinearities and so forth and then
we'll need to understand how to how to
do color I haven't mentioned color yet
but it's a you know it's a huge topic to
go from a really almost ideal refined
black and white or panchromatic image
sensor to then figure out a way to get
color out of that introduces huge non
idealities into your system and a whole
raft of new problems and then of course
all the practicalities of how do you
capture the data off of the silicon how
do you how do you store it how do you
process it what do you do with it so
these are the kind of topics we'll be
getting into so that was the that was
the sort of first pass overview now
start going into a little bit more
detail if you want to take any of this
stuff seriously you'll want to have some
books to refer to these are the probably
the best two that I've found that cover
a lot of this material on cameras again
not on applications as much but on
cameras light and photographic imaging
the manual of photography this is the
9th edition I've got all the other
editions as well if you want to look at
them but the ninth one is the first one
that includes digital as part of its
content so it says photographic and
digital imaging the one on the it still
doesn't have very much on digital it's
it's kind of new additions to these guys
but it's it's really good
compendium of the material accumulated
over 150 years or so with some really
top-notch authors Sydney Rey there is
people make fun of his name because he's
so big and ray-tracing he's written all
the sections on ray-tracing in the books
huh
good guy
ralph jacobson the current president of
the Royal Photographic Society in
England and Geoff average is a professor
emeritus at Westminster University and I
don't know about Axford this other book
pretty recent 2006 actually is the
copyright date on it has a whole bunch
of really good chapters on details of
what goes on in digital cameras and
image sensors it's got a little appendix
in the back that'll let you figure out
how to do all the calculations you need
to figure out exactly how many photons
you're gonna get if you take a picture
and I saw 100 with a normal exposure and
so for instance it's got all those it's
got the numbers and the examples and all
that stuff you're going to need so let's
start looking at the camera
what's a camera it's just a box with a
hole or a lens in it that images are
seen on to a focal plane so
geometrically it has some field of view
some width that's defined by the size of
the film or focal plane in the back of
the camera and by the the distance of
the pinhole from that plane or effect or
equivalently the focal length of the
lens so it's it images some part of a
scene out in the real world
typical cameras image about like I've
drawn here it's what's called
rectilinear imaging so there's a flat
plane and out in the object space that
images to a flat plane in the image
space and things on that plane retain
their relative sizes there's no
distortion of size that's kind of the
ideal in most camera lenses the
alternative is you get some distortion
the extreme alternative when you want to
make a really wide angle view is lenses
that give you extreme distortion like
fisheye lenses that will map a whole
hemisphere of space into a little circle
you can't map hemispheres to circles
without some distortion of the relative
sizes of things and that's what a
fisheye lens nests so there's there's a
big range between these rectilinear or
non distorting lenses and fisheye lenses
that to start a lot but for most
purposes we just think in terms of
rectal in
we mostly think of magnifications just
near the center and we just sort of
simplify away everything we can because
you can by just looking at the center of
the image the focal length tells you
about everything you need to know about
how the how the objects face maps to the
image space a camera has settings
typically there's the the traditional
film camera has a shutter speed and
aperture setting and a focus setting and
you can get you know plenty of old
simple mechanical cameras you know like
these these miracle cameras that still
work even when there's no battery in
them that's what we're talking about
here but even the modern electronic
cameras have essentially the same
controls this this is a picture of a
camera it's like the one I learned on
camera the my my granddad bought in 1922
to take pictures of my dad when he was
born kodak autograph ik jr camera and
it's it's a piece of cake and you can
learn everything you need to know about
photography from old cameras like this
this one has shutter speeds that you can
see going from 25th of a second to a
hundredth of a second it's got these
other settings marked b and t for ball
band time you guys that have taken
pictures probably know what that means
we might talk about it later it might
not now at the bottom it's got this
aperture setting scale this one goes
from us 4 to US 128 and when i was a kid
the u.s. scale was already about a
hundred years ops obsolete and so i
didn't quite know what that meant but
not quite a hundred years but fifty
anyway and it turns out that's uh that's
a non-standard scale but there's
corresponding F numbers that people
would would use instead today there's
still a lot of old Kodak cameras that
use this u.s. scale which stands for the
uniform system and as at one time was
the standard adopted by the Royal
Photographic Society of London and that
these numbers are actually equivalent to
f8 f11 f-16 twenty to thirty two and
forty five case anyone needs to know
when you have a camera like this you
have to figure out how to set the
exposure time in the aperture and
there's a lot of different ways and
tools to do that nowadays on digital
cameras is a typical ways you take a
picture and you look at the response and
see if it's too light or too dark and
change it so there's kind of a feedback
loop that prevents you from having to
think about it but if you're designing a
system you're probably going to want to
think about it you're probably going to
need to calculate how much light you're
getting and how much noise you're going
to get on so there's lots of tools you
can use to estimate the amount of light
that's available in the scene that you
can use to set your camera with and this
is one such such tool it's a two slide
box wood and brass slide rule called the
herder and dry field Zak to negraph it
has a cardboard insert you can get a set
of different inserts for different
latitudes and this will let you
calculate the amount of light available
as a function of the time of day time of
year and latitude which is pretty cool
and so if you're if you're designing a
system for outdoor photography you might
want to look at this it's got an awful
lot of information and it'll help you
figure out how much light there's going
to be more typically though what a
photographer would do is is say well
bright day near noon I'm not near the
North Pole there's kind of a standard
amount of light I know what that is if
I'm in a darker day or a higher latitude
or bad season or something I know how
much to adjust it by and you just kind
of work this stuff out in your head
based on your experience those rules
like the sunny 16 rule which says set
your camera at f-16 set your shutter
speed at that the reciprocal of the ISO
speed of your film and it'll be about
right for a sunny day and adjust from
there
the settings on a modern lens are
essentially no different this this
example of a Nikon lens that has a focus
setting and an aperture setting the
focus setting lets you read out the
distance that the lens is focused to on
a scale here and it's got a bunch of
other markings on it so the the lens is
marked with its focal length 35
millimeters here the maximum aperture
one to two which means that the lens
will open up to a diameter equal to one
half of its focal length it's got the
f-stop scale here so you can set the set
the aperture this is the diameter
measured as focal length divided by that
number and it's got these these markings
on the focus scale that that let you
tell for different F numbers what range
of distances you should expect to be
sharp in your image so this is the depth
of field scale so there's a lot of
complicated numbers on lenses but
they're all actually pretty easy to
understand once you once you know what
they're trying to tell you and you can
do these calculations for things like
depth of field and and amount of light
corresponding to different F numbers and
all that stuff quite readily we'll get
into a little bit of that here's what an
F number is we typically denote f number
by the variable n capital n the N of a
lens and it's just the focal length
divided by the diameter this is the that
the effective diameter the the diameter
that determines how much light from the
object space gets through the lens it's
so you'll you'll find that that
corresponds to the diameter of the
entrance pupil when the lens is more
complicated than the simple lens we've
shown here there there are standard
scales essentially labeling by different
conventional roundings of powers of
square root of 2 the square root is in
there because the the area is doubling
for each increment here so though since
the area goes as the square of the
diameter
how many people already know all this
stuff that's good about half that's
about where I expected to be so I'm
gonna you know trying to drag the people
that aren't totally familiar with it
along so we'll have some common
terminology there's a bunch of stuff you
need to know about how to manipulate
lenses lenses are generally specified by
the curvatures of the the radius of
curvature of their faces so that's what
the R 1 and R 2 are in the upper picture
picture here but those are kind of the
manufacturing details of the lens you
probably don't really need to know that
unless you're designing lenses but the
the behavior of the lenses is specified
usually by what's called the lens
equation which is this sum of reciprocal
distances is equal to 1 over the focal
length so the the distance is s 1 and s
2 in the lower picture are the distances
from the lens to these these two
conjugate points that are in focus with
each other so the points are said to be
conjugate if the rays emanating from one
point converge at the other one so the
the distances at which that happens s 1
and s 2 which are the distance to the
object and the distance to the image are
related by this formula the sum of their
reciprocals is equal to the reciprocal
of the focal length you can also look at
things like the ratio of those distances
and that's the magnification of this
system it's it's shown here with a minus
sign because the lens inverts the image
so that means the magnification here is
negative s 2 over s 1 if the object that
s 1 is imaged at s 2 that ratio of
distances can also be expressed in terms
of the focal length and one of the
distances as is shown here this this is
pretty handy thing to to use sometimes
if you're doing close-up photography
magnification is something that matters
a lot if you're if you're shooting
things that are off at just at a nearly
infinite distance your magnification is
essentially zero
you don't usually express it in these
terms you would express it instead maybe
in focal plane distance per angular
measure
like that that that doesn't doesn't have
that that infinite distance factor in it
there's one of my one of my favorite old
opticians as more it's fun rar from the
what was it called the the Zeiss the
Carl Zeiss works in Jena these guys
worked out a lot of really great modern
optics in the late 19th century that's
still very relevant this particular one
is how to how to think about calculating
the depth of field if you've got an
image imaging system on the left here
where I've shown kind of a box for the
camera with an image in the back of it
but I haven't I haven't given you the
parameters of the box I haven't told you
how how big that image is or what the
focal length is or what the f number is
turns out you don't need to know any of
that to compute the depth of field and
that's what phone drawers showed and his
method is that you can you just look out
here in the object space and if the if
the camera is collecting light from an
aperture of some diameter you can and
and it's perfectly imaging some plane
some object plane out here into the
image plane you can figure out how much
blur any point in the object space has
by projecting that point with these with
these cones into this plane and then see
how big the blur is in this plane and
then image that so if you can define how
big an acceptable blur is at the object
plane in terms of some angular tolerance
e times the distance to that plane then
you can calculate these points where the
anything closer than here would make a
circle bigger than that anything further
than here would make a circle bigger
than that so the the range of things
that are in focus are from this near
limit to that far limit which you can
trivially work out the algebra from this
geometric picture and this is this is in
contrast to the way people normally work
it out using the lens equation and
working it out in the inside the camera
in the focal plane in the image space
with this method you don't have to use
the the lens equation at all all you
have to do is figure out what your
tolerance your angular tolerance
is and this becomes a very simple method
so I'll say more about this and also
other other applications of the
outside-the-box concept so without
knowing the focal plane or the size of
the you know the size of the focal plane
of the focal length of the lens or the
f-number of the lens just taking the the
box as a box with a hole in it you can
work out the diffraction limited
resolution at the at the object you can
work out the depth of field at the
object you can work out the number of
photons collected from the scene and and
how much exposure time it takes to get
an ideal camera to make an acceptable
picture of that object you can do all
that outside the box just from knowing
the the diameter of the lens there's a
question yeah
yeah okay the question is might my
argument suggests that the the depth of
field that you calculate this weight
depends upon the resolution that you're
trying to get out of your optical system
and that's that's absolutely correct and
and the other part of the question was
that that's that's not what these scales
show you we go back to the scales on
this lens there's there's no way to put
your opinion of how much resolution you
want into the interpretation of these
depth of field scales on the lens and
that's correct these depth of field
scales on the lenses are based on
somebody else's opinion so if you have a
different opinion this these are
generally based on most conventionally
the idea that the diameter of the blur
circle and the focal plane should be no
bigger than 115 hundredths of the
diagonal of your image that that one
over 1500 number is has been fairly
consistently used for the last 25 to 30
years and earlier than that typically
slightly laxer numbers were used like
maybe one one thousandth so if that's
not the resolution you're aiming for you
have to adjust your interpretation of
these these distances and if you want
twice as much resolution on this scale
you just want to get twice as you want
to cut the distance in half twice as
close together
I don't want another question
also as amusements why the f-stop scale
is useless what a linear measure of the
net to emotion
we use a schedule yeah
excellent question the question is and
we I don't mind taking a lot of
questions so I'll go into this I got I
got plenty of time in coming weeks to to
go on so let's let's stop and talk about
this because it is very interesting
question the the f-stop scale has a is a
linear measure of the diameter of the
lens but the amount of light you get
through goes like the square of the
diameter so why don't they use a scale
that's more like the square and there's
several aspects this so so the one is
the the f-number scale is a very natural
thing to define it's it's
non-dimensional it's a ratio of
distances so you don't have to put any
units on it if you go with a measure of
the area of the aperture relative to the
focal length you have to decide how
you're going to measure that so now
you're going to have numbers that either
have some arbitrary scale factors in
them or you're going to have some some
distance units in them or whatever if
you go back to this Kodak camera the
u.s. scale the uniform system was
adopted specifically for the reason
you're asking about the numbers here are
proportional to the amount of exposure
time you need to give the image so when
you when you open up by one stop from 16
to 8 instead of exposing for 16 seconds
you can expose for 8 seconds or
milliseconds or whatever so that was the
purpose of those scales but these scales
were very unintuitive to compute things
other than exposure you couldn't easily
look at the diameter of a lens and the
focal length of a lens and know what
it's us number was you'd have to compute
its F number and then as you take the
ratio and then you go through some
mapping to figure out what it what it is
so both systems have some merit the F
number scale is also it you know it's
easy to get from these simple
measurements and it's non-dimensional
and there's no table mapping you have to
go through or arbitrary steps to define
it
and because it's because it represents
the diameter of the lens it represents
the the sizes of these Ray bundles and
these angles which lets you do a lot of
very direct calculation of things like
the depth of field and that angles and
so on let's see the other yeah here's
some did that answer your question so
depth of field is really important and
it's it's the thing that that limits how
you use an ideal camera variate very
often because you have to you have to
get enough light but you also have to
keep your subject in focus and there's
there's always this this this tension
between getting enough light with more
time or getting it by a bigger aperture
diameter that causes things to blur more
so historically there's been a lot of
different ways to calculate depth of
field and these are some of the the
analog computing devices people have
used Kodak put out a lot of these books
of data books that had they were they
were big on these little circular slide
rule computers the one on the top here
is the depth of field scale for
wide-angle lenses and the one on the
bottom is the depth of field scale for
telephoto lenses and they had a bunch of
different settings in here where you
could set a combination of a focal
length and a film format and then you
could you could look you get different F
numbers showing up in little windows
here that lets you take a range of
distances here and and fascinating
little devices I had this straight slide
drill that lets you do a similar thing
by you you picked your combination of
your lens focal length with your
aperture and put it against this mark
here and then on this scale you have
aligning with each other all the
corresponding near and far distances and
it's the same kind of thing you get on
lenses themselves or you've got you've
got these aperture scales now most of
this stuff is disappeared with digital
cameras most of the lenses nowadays
don't have the depth of field marks on
them anymore
and you're kind of shooting blind or you
take a picture and you look and see if
it's sharp enough and then change it if
it's not but but if you're designing a
system again you have to be able to
compute this stuff if you're trying to
image
hi-resolution book pages from a camera
and make every page sharp even as you
turn pages and they get closer and
further for example you have to work
that out and so that's why we're gonna
be talking about it the other thing
that's really important in photography
is this notion of tone or tone rendering
tone scales the idea that that different
amounts of light and dark get get turned
into reproductions of light and dark on
some scale that doesn't necessarily bear
a simple objective relationship to the
original scene and this is this is often
overlooked in digital photography
there's a huge history here and yet
people will often kind of but assume
that's irrelevant and kind of assume
what's called objective tone mapping
though so the the assumption that you
want in a reproduced photograph shades
of light and dark that are in the same
ratio of the shades of light intensity
in the original scene is a bad
assumption don't make that assumption so
we want to study this notion of tone
rendering and this this picture from
from Kenneth Mises book on photography
he was the he was the head of research
for Kodak for many years talks to what
he calls the the cycle of tone
reproduction that has to go that that
has to do with scenes and and negatives
and developing negatives to different
contrast ratios and printing them as
positives and viewing them with the eye
and illuminating them and viewing them
and so forth all of these steps of
illumination and eyes and and
development and printing all these
things are important in determining what
the what the tone reproduction scale
looks like and there's many different
ways to get from over and underexposed
negatives to decent looking prints but
they don't come out looking identical
they have different tone scales that
aren't gonna aren't gonna reproduce very
well here so let's all look at them too
closely but it's not just a question of
how do you reproduce a given tone scale
or what can you reproduce or what it
what are the non-idealities but the huge
question is what is preferred what looks
good so that's been studied
and vision is a non-trivial and
nonlinear sense so the you know
preference is a very subjective thing
and this shows three different examples
of the mapping from the subject
luminance the original scene to the
optical density of a reproduction
density is a term that's often confusing
originally came from herder and drew
field who the guys that did the ACTU
negraph they were the guys that sort of
founded the whole science of
photographic densitometry and
sensitometer and they density to them at
the amount of silver per unit area in a
developed photographic image so it's a
it's a literal density it's like you
know how many you know micrograms or you
know grams per square centimeter or
something like that but optical density
which is fortunately pretty much
linearly related to the literal physical
density is a measure of the transmission
or absorption of light and a higher
density one density unit means a factor
of ten loss of light so if you
illuminate a print and you get a
reflection of ten percent that's an
optical density of one or if you
illuminate a transparency and you get a
transmission of ten percent that's a
density of one if you get a transmission
of one percent that's the density of two
so that's this vertical scale D is a
logarithmic measure of objectively
measured light coming out of your
reproduction and the horizontal axis is
a logarithmic measure of the objective
amount of light in the original scene
the dashed line would be what we call an
objectively accurate tone reproduction
and the the solid curves represent the
the kinds of tone renderings that are
found to be preferred typically in
photography in in a typical reflection
print the the slope of the preferred
curve is a little bit a little bit
greater than unity in the middle which
means you want a little bit of extra
contrast in the mid-tones of your print
for it to look good and you want it to
be smoothly compressed toward dark and
light on the two ends you don't want to
RUP clip and you want it to be a little
darker on average than the original
scene and the reason for that is that
the I'm sorry that's not darker that's
lighter you want it to be lighter than
the original scene because in the
original scene you tend to have things
like you know light bulbs and and
sunlight coming through windows you get
a lot of kind of bright spots that you
don't really care about everything else
is relatively dark on a photograph you
want to take all that relatively dark
stuff that's where your subject is and
brighten it up so lower density and so
that's kind of what's generally
preferred but if you're viewing a slide
in the darkroom
a transparency this would this would be
in a darkroom and this would be in a
kind of viewing on a on a viewer in an
average lighted room you get again
different preferences with
transparencies there's an even greater
slope in the middle meaning you you
really want more contrast for the thing
to look good and you really want it to
be darker because otherwise it looks
washed out when you're viewing it with
projected light and so you you have to
look at how these things are being
reproduced to figure out what kind of
tone scale you're after yeah
good question how do these curves relate
to the curves and levels dialog in
Photoshop it's a complex relationship
because in Photoshop the the scales that
you're working on are not log luminance
they are gamma compressed intensities of
reproduction primaries so it's a it's a
different kind of non-linearity but the
same kind of curves are important but
they don't quite look the same they
still should be somewhat s-shaped but
you know the question is compared to
what so you know you don't generally
have in Photoshop any measure of the
original log luminance of the scene
unless you're working with raw data in
which case you might yeah another
question back here
and skiers here
yeah and which how do these curves
relate to the to the what the monitor
does and that's another good question
that's that's part of that same thing
and when I mentioned that Photoshop
you're working with gamma compressed
intensities that's because the the whole
notion of a gamma comes from the
original behavior of cathode ray tube
monitors where the the current coming
off of a hot cathode response
nonlinearly to the voltage you put on it
and that's that's sort of the origin of
the use of gamma in television
representations that led to computer
representations but there's actually
much better reasons to use a gamma
compression curve and I'll explain what
all that is the outline for the course
mentioned that there's like a whole
module on gamma because it is a it's a
huge important topic and hugely
misunderstood I know that's not a an
answer exactly it's just deferring the
question to a later time so film is
nonlinear that the term gamma in
photography is used a lot in film and I
don't know if it originated there or if
it originated in the in the television
technology I have to have to look into
that but the gamma is the slope all of
these curves that I just showed you you
can measure the slopes at different
points and the the slope in a log-log
space like this is called gamma because
of the way density is defined these
curves are headed downhill you may off
so you may typically want to look at it
the other way and or use the the
absolute magnitude of the slope but
basically the slope is called gamma and
when it's greater than 1 that means
you're increasing the contrast or your
reproduction has more contrast than your
original film has this to this is called
a herder and dry field curve H and D
curve or a d log e curve it's a plot of
density versus log exposure and that
these curves for typical films have a
what they call a straight line portion
in the middle and the slope of that
straight line portion is what's
typically called the gamma of the film
you can actually measure gamma at all of
the other non straight points as well
and plot
as a function of log exposure and so
forth you have a lot of control over
these these numbers these gamma numbers
and that's one of the things that Ansel
Adams taught in his whole system of
photography based on zones and so on is
it you you control the gamma how long
the time and temperature of development
of your film determines the gamma that
you're going to get out of it you have a
lot of you have a lot more control in
developing your your film negative than
you do in your print so you want to do a
lot of that work upfront when you're
composing the picture figure out what
kind of contrast you want in your scene
and develop it appropriately we don't
have to do that with digital cameras we
can do all this stuff after the fact
because the the sensors tend to be
pretty inherently linear so once you've
got a negative and you want to make a
print from it you stick it in here and
larger and you project it on foot on
sensitive photographic paper this is
this is a cool device I I bought one of
these Kodak home enlargers off eBay a
few years back because it has on the
back of the II saw a little circular
slide rule for computing exposures and
enlargements and everything and I
collect slide rules so they had to have
it but so I had to show it to you but
the point is that when you go through
this process of taking pictures and
developing negatives and making prints
each of those steps does something to
the tone scale and you can you can map
you can cascade these nonlinear
functions with what's called a Jones
diagram named after Lloyd Jones who use
this as a method for studying and
explaining tone reproduction back in the
30s and depending on what steps you want
to illustrate your Mayer you may have
one or two or three or four of these
quadrants and you can you can map around
a circle with this and end up back in
the back in the luminance space to
compare a an original objective scene
luminance with a with a reproduction
which is which is what this one does so
this quadrant 4 here is the reproduction
you can see how he's getting an s-shaped
curve that matches the preferred
reproduction by by controlling the the
gammas and the nonlinearities of the the
negative and the print
and you can put you can you can put
different functions into these quadrants
depending on what you're trying to do
you can put in perceptual model of what
the eye does to see if to see if the
subjective reproduction then comes out
looking like the original scene so you
can you can kind of close the loop on on
whether you've reproduced the perception
of the scene by modeling perception if
you want to that's that's a little
harder so we might look a little bit
more at these kind of things I didn't
get as far as I wanted to today but I'll
spend a few more minutes I want to get
into a little bit more about optics now
and talk about how rays of light are
refracted by lenses this is just a
picture of a little it's called the
cylindrical lens because it's surfaces
are only curved in one dimension instead
of two and these are these are good for
making science fair projects and so on
an illustrating light refraction but in
photography we use spherical lenses for
example this this represents a globe of
glass that would focus light this guy
even saw Arabic guy as you can tell by
his writing looks like we lost something
here he analyzed the refraction of light
back in 984 ad and wrote it up and he
was working not on imaging so much but
on making burning lenses so he could use
the power of the Sun to to burn things
and the little figure in the in the top
right with Lions I've reproduced over
here practically to in to illustrate
what it was that he was figuring out
there's an incident ray which he's drawn
the distance from a point a point where
the ray is incident on an interface
between air and glass this distance l1
to some some normal line that he's drawn
off to the side in some arbitrary place
and then there's a refracted ray which
is extended back to that same line of
the distance l2 is measured and what he
observed is if you do this little
construction than the ratio of l1 to l2
is a constant no matter what angle that
ray of light hits that
the ratio of those two distances is
constant this is in no way intuitive and
it was a important fact that got lost
for about six hundred years until snow
recovery discovered it by essentially
the same construction and this this rate
this constant ratio of distances can
easily be recast with a little bit of
trigonometry into the ratio of the signs
of the angle of incidence and angle of
refraction which is the way it's
normally represented today but in fact
Snell who got his name on the law drew
the same picture that didn't solve did
600 years earlier or 500 and so him
anyway
so this is this is a at the time it was
not easy to understand there was no good
reason to believe that the sine function
had some magic to do with light or that
this ratio of distances had some magic
to do with light it was just something
they observed by a lot of painstaking
experimental measurement they wanted to
understand what the relationships were
between these angles and this is the
best they came up with but it turns out
this is easy to understand once you go
through what Huygens did and you
understand that light propagates those
waves and there's this notion called
Huygens principle where they each each
point through which the wave is
propagating acts as another source and
you get these little as if there's
spherical waves coming out from every
point and they interfere with each other
when they interfere in phase the light
keeps gone and when they don't
that sort of cancels itself out and
nothing interesting happens and Huygens
drew a bunch of pictures to illustrate
this effect for example these are these
are wave fronts from different points
where these wave fronts hit a surface
each one of those reradiates at a
different velocity and where these wave
fronts are all in the same phase you get
a wavefront moving on this way and you
can compute the the angles of things
from that you can compute it from the
principle of least time which is what
that picture was about you can draw it
as the ratio of these distances which
are the signs of the angles so Huygens
was able to explain where this law came
from in terms of a physical model of
light so this was really
he was really the first guy that that
figured out how to model light
physically to explain what's going on he
he also figured out how to compute the
shapes of lenses that would converge
light from a point perfectly to another
point within within the within this law
what within what the law says will
happen he can compute the exact curve
and it's not spherical and he also did a
lot of work on spherical lenses where he
says we could make spherical lenses
because when you grind two things
together circle is the shape you get any
other shape is essentially impossible to
make and that's that's still true today
most lens surfaces are spherical because
that's what you can grind so he said
given that constraint that that's what
we can make how do they behave and so he
was able to work out and characterize
what's called spherical aberration he
was also able to show that if you have a
spherical surface you can correct its
aberration by an a spherical surface and
again converge light perfectly to a
point this is all in the 17th century
you don't have to necessarily under
understand all this stuff as deeply as
Huygens did but his little book is
actually a very fun read and I'll
recommend that one too I have a library
of books that well talk about from time
to time and let you guys borrow them and
read them if you like and let's cut it
off here and we'll continue this next
week thanks so much for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>