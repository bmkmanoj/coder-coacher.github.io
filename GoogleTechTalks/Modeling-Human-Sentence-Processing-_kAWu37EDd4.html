<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Modeling Human Sentence Processing | Coder Coacher - Coaching Coders</title><meta content="Modeling Human Sentence Processing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Modeling Human Sentence Processing</b></h2><h5 class="post__date">2008-04-19</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/_kAWu37EDd4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so today we have vera den bergh with us
and Vera is a intern here just finishing
up her internship normally she's at the
University of Edinburgh in Scotland
we're in the Department of informatics
where she's studying a combination of
computer science psycholinguistics
computational linguistics and she's
going to tell us about models of how
people understand sentences sentence
structure and I guess how computers can
take advantage of that so take it away
oh and I wanted to say that afterwards
we'll be going to lunch anyone who wants
to ask more questions can join us and
Vera's here for another week if anybody
has interest in discussing this further
thank you okay thanks for the
introduction so I'm going to talk about
my PhD about modeling a human sentence
processing right so and why would one
want to study that at all so on the one
hand side there's the theoretical
interest of how does the brain process
language and here cognitive modeling can
help us to understand better in terms
you know if we model what's going on in
brain our model exhibits the same
behavior that we find in how humans do
sentences then maybe that can inform us
about that and I'm looking I'm focusing
on syntax so that's the sentence
structure and looking at how different
sentence structures are more or less
difficult process but there's also you
could also use this for applications in
natural language processing so for
example in all kinds of different
natural language generation systems and
you could use this to optimize the user
interface because if you have a very
open text generation system there are
lots of different ways in which you can
formulate a certain number of facts and
how do you choose among them so one
possibility would be to use such a
function where you put in sentence at
the top and what you get out is
the sentence has this in this difficulty
in the word by word basis we can say
that here and there are certain spikes
in processing difficulty for example and
then we could tailor that for different
situations so for example for dialogue
system it's been found that if you
complete and present the user with
really complex sentences or like
difficult decisions they perform worse
driving they do driving errors not so
they like like the dialogue system less
so you know depending on the situation
when they've other stuff to do we my to
might want to change that function just
say okay now we're going to generate
really easy stuff and then the uppers
questions of course how do we know
what's difficult in the first place and
the way that I'm finding out about this
is using eye-tracking and the idea there
is that processing difficulty is
essentially proportional to the fixation
time so if somebody you know looks at
word for a very long time or goes and
retreats paths of sentence again that's
probably where they have some difficulty
and if across several subjects everybody
has you no longer reading times on the
particular part of the sentence then
that's where we say here's some
processing difficulty so here you can
see this person's wearing an eye tracker
here two cameras that scan the eyes and
it's calibrated against the screen here
so we know exactly for how many
milliseconds this person looked at what
letter in the text other methods for
finding out about processing difficult
you would of course be like brain
imaging self-paced reading where people
just you know press spacebar to go to
the next word for example are just
asking them comprehension questions but
I'm not going to talk about these and
I'm going to be focusing on I tracking
so in my talk I'm first going to talk
about networking corpus and here the
focus on core plus because that's not
what you people usually do in
psycholinguistics people usually do like
we're defined experiments
with a small set of sentences but I'm
going to use a course I'm going to talk
about that and secondly I valuated
different models of processing
difficulty on this corpus on this data
source and thirdly i will talk about my
own centralizing model which i'm
proposing so for the acrylic King corpus
traditionally people always use this
experimental approach here so in this
paradigm materials are carefully
designed to eliminate any kind of
potential confound so if you want to
find out the bars especially a
phenomenon you just keep everything else
table and just provide a minimal pair
have people read that and then see what
which one comes out to be more difficult
the advantages there is established
reliability and validity people have
done this for years but there are some
drawbacks I think so for example the
sentences are typically presented out of
context or very minimal context just
because you can't keep people in the
labs for you know ten hours just to come
across the things that you want them to
come across right also the materials a
constructive menu by the experimenter
and sometimes you know you these items
you want to like wow does that ever
occur in real text or is this even
grammatical so and the last thing is we
know that humans learn really rapidly
and if you want to test for some really
rare phenomenon then the questions of
course do people you know quickly find
the strategy even do during the
experiment to cope with this spec
special phenomenon so you may be biasing
your results by presenting them with
these things more than would be normal
in the corpus everything's in context of
course
there are lots of data points and we can
test the theories and lots of different
constructions and but of course well
let's control actually no control over
the materials I mean of course we can
just select the ones that we won so that
gives us some control over what we
analyze and then the last thing is yeah
people haven't done this before it is
not for a higher level processing so
it's not an established method to use
the corpus here but I'm going to talk
about why I think it's a valid method so
another point is metallurgy key an
advantage of the courses we can just use
this one resource and use several
theories all evaluate them on this one
corpus resource and then see how they
perform on this data also we can find
out how much of the variance in reading
times we can explain and this is
interesting because with the
experimental paradigm usually only
compare two conditions and you see like
the one the one you know here readings a
bit slower than the other condition but
you don't really know how much of the
variance you explain you see there is a
significant difference but how much of
the variance among all variants that's
in the data is that and that's hard to
assess in the experiment to paradigm and
we can then assess how well these
theories that have been and produced
based on the experimental paradigm scale
up to naturally occurring text ok so did
Andy corpus is the resource that I'm
using is quite and decorous because it
was produced in 10g in Scotland and it
contains 53 1000 words of British
newspaper Atticus that's about like 20
newspaper articles there from the
independent newspaper and they were read
by 10 different people and this is what
the
looks like that we get so this is this
is the text from the corpus the red dots
here at the fixation points so when we
really text the ice don't move smoothly
right they always jump in seconds from
one to the next and then the blue
numbers here how many milliseconds the
person fixated on this particular letter
our spot and you know when you look at
this you see that often function words
are skipped so for example these kind of
very frequently occurring words are
skipped where's content bearing words
are excited more often and once we have
the state of the different analysis that
we can make so for example if this is
you know how the person read the text in
time then we can analyze the first
fixation times which would just be
number five here or the gas duration
which would be like five and six
together or total ringing chime and this
give us different stages of the process
process so the early measures here would
give us more information about like
lexical access processes where these
ones usually reflect more of the you
know deeper understanding processes but
since we don't have clothes control over
the text as I said before the lots of
different aspects that can influence the
reading time at a specific word right so
that's not only the the syntax there's
going to be like the word length the
word frequency and all these have
influence on the fixation time in a
specific word and since we don't control
for those we have to somehow account for
them and see what of the variants
remains to be explained in the first
place so the way we do this is using
hierarchical mixed effect models and
they have a dependent variable this is
you know the target variable that we
want to
it that would be for example first
fixation duration or gas duration
totally reading times you know the ones
that I talked before about before then
we have the venom random variable sorry
and that's the subject because each
person may you know have with some
different reading characteristics so we
want to take you know those make a
separate regression therefore the
subjects and then we have the
independent variables so this is on one
hand our model that we want to test but
on the other hand it's a bunch of
confounding factors for example the word
length the word frequency for example
this part of speech tag off the word or
the fixation lending physicians all the
different things that we know have
influence on reading times and now so as
a proof of for that there's no
established and reliability for using a
corpus so what I did is first step is I
took this example relative clauses and
there's a lot of consensus in the
research community that object relative
clauses are more difficult to process
than subject relative clauses so an
object relative Clause is something like
the reporter who the senator attacked
admitted the error and this here is more
difficult to process than the subject
relative Clause who attacked the senator
okay so there's a lot of consensus there
and I said okay if we can find that in
the relative clauses of this corpus we
find the same pattern and you know our
regression methods are powerful enough
to find this pattern in the data then
that's some evidence of course that
proof right but there's some evidence
that maybe this corpus is a useful
additional resource that we can use to
find our results so this graph here is
not from my data this is from from the
literature and this is self-paced
reading times the cover bath years for
object relative clauses and simply this
test
the reading times are longer if it's an
object relative clause on this part of
the sentence here respectively so for
the relative Clause analysis the
relative class again I used the Dendy
corpus and passed it with a shiny a car
sir so that we know what the sentence
structure in the corpus are then I
automatically extracted the relative
clauses from the Dendy corpus and I
found that there were 434 relative
clauses fourteen percent of them object
relative clauses and most of them a
subject relative Clause but that's what
you would expect a subject reg of class
are more frequent so that's fine and
then I did three experiments on the
different regions so for example
comparing the reading times on the
relative pronoun in the two different
sentence types comparing the reading
champs on the verbs in the sentence
sentences and I'm not going to talk and
detail about those experiments I'm just
going to use one of the experiments as
an example so this is for reading times
on the main verb so that's comparing
reading times on verb like this and the
subject courage of choice to rien camps
at this verb here and as I said the
literature says that really can should
be longer here so this is the result
from running the model and I'm going to
talk you through this so and what we
have here the intercept would be the
base reading time so to say so
everything else being equal you would
expect that the word would have a
reading time of 263 milliseconds I mean
this is for verbs and in these relative
clauses then if it is a relative Clause
type in order to predict a good reading
time we would have to subtract 177
milliseconds from those for each
character in the world so in the world
and we would add another 20 milliseconds
then for the lock frequency value you
know for each unit of those who would
subtract another 11 milliseconds so this
essentially means longer words is longer
reading times and they're just more
frequent words shorter reading times and
this is the patterns that you would
expect and the important thing to take
away from the slightest that the subject
in subjects class condition the reading
temps were shorter so that's what their
regression model predicts and this means
the webs are read faster in the subject
relative Clause condition so that's some
evidence that the corpus does actually
give us the same patterns that we expect
so then I implemented to previous
theories of sentence processing
difficulty and evaluated them on this
corpus and there's a bunch of work
that's been done before in this area and
I picked two of the methods surprises
and dependency locality theory and the
reason I picked these two are that
they're kind of complementary I think so
they cover a different aspects so I'm
going to explain them in more detail in
the next slides surprise the enterprise
that's a for you can think of it as a
forward-looking measure so the
comprehension difficulty at your word
it's essentially proportionate to the
amount of surprises or surprise when
hearing that word and the idea there is
that you know if you hear the beginning
of a sentence and you're you know I have
a good expectation of what the word next
words going to be you're not surprised
you know that's really easy that's what
you are expecting any way but if there's
something that's really surprising and
surprising structure a surprising word
and that's going to be difficult and in
a more mathematical sense surprises the
words negative log probability in
context
the underlying assumptions of this
theory are that comprehension is
incremented so when you get new word
input you update your representation of
what's happening in the sentence so and
generate these expectations what was
going to come up in the sentence right
that's when it is sad but there's a
bunch of open questions here of course
so for example what would be a good you
know if you want to implement this and
you have to ask yourself what would be a
good grain size for those work
probabilities do we only want to look at
the structure probabilities or also at
the lexicalized and probability so
what's the grain size of the prediction
that's going on in our heads so to say
if we take this as a serious approach or
other features may be important for
example enema c and so on but have
influence on our probability models in
our heads so for EM here this is oh
sorry so in order to determine the
surprise at a specific word you have to
calculate the prefix probability at each
word so for example if you want to if
we're hearing the reporter then there's
a bunch of different representations
that the reporter could be like just the
subject of a sentence or like it could
be modified later there are all these
different possible structures and the
surprise the prefix probability of the
reporter the string the reporter would
be the sum of all those different
probabilities of the trees and then I'm
surprised it would be that you know the
next word for example you get something
that's compact compared to but only with
the structure then you have to get rid
of the probability of this structure and
the surprise is just the difference at
each word how much of the probability
structure do you have to
forget so to say so the idea is if you
know the word that here is only
compatible with a structure that's very
very low probability then that's really
difficult so here's when we let this
implementation run on a sentence you
know that we saw before in the reg of
clothes for example then here we say
okay and this is just for strike this is
non lexicalized right so once we've
heard that we're not surprised to see
that the sentence that's with a
determiner then once we've seen a
determiner and we're not surprised that
there's a noun following but we're
fairly surprised that there's a relative
pronoun coming after the noun because
that doesn't happen all of the time
right and then once we have seen the
relative pronoun we're not surprised to
see a ver but relatively more surprised
to see the charmila and so on I mean you
get the idea so when I ran this on all
this sentences of the dandy corpus I
found that there was a positive
coefficient here and that means that
this predictor so they surprised it at
each word in the sentence is a
significant positive predictor of
reading time in the corpus so that would
be support for this kind of theory I
also did this with a lexicalized
surpriser and just you know simple
engrams but actually lexicalized
surprised it was not significant over
just you know using a simpler measure
that such as a simple ingram
probabilities as an estimator but
structured surpriser was good over and
above the other predictors and the
second theory which I want to talk about
is dependency locality theory and this
one is quite different from surprises
and that it's a backward-looking measure
so this captures you know given what
you've heard so far how difficult is it
to integrate that but with what you're
hearing now so if you have a long
distance dependency or really deep
center and embedding that's difficult
according to this theory so integration
costs a course at the heads of phrases
when the pendants are integrated so for
example the verb is ahead phrase here
the verb has the dependents who and the
senator and when we get to the verb here
in the subject right of class condition
we only have to integrate this thing
here and it's really close so it's quite
easy to integrate but in this object
relative Clause case we have two things
that we need to integrate to dependence
and this one here is a bit further way
so this adds up to more difficulty so
theory would predict that this should be
more difficult in the object right of
class condition so I implemented this on
top of the mini power dependency parser
so that I would get the difficulty
predictions for each word in the corpus
according to this theory and what I
found was actually very well very
unfortunate I thought at first plans
because I found that the integration bus
was a negative predictor so that would
mean more integration costs make it
makes it easier or faster to read the
sentences really not what we want right
because you know this shouldn't happen
according to the theory at all and the
theory had been shown before to be like
valid and you know lots of different
experimental data found you know this is
actually what people find so I was
really surprised to see this and even
though you know significantly negative
so I looked at the data in a little bit
more detail and thought okay possible
explanation is you know actually the
sentences that we're looking at are
quite different from the sentences that
you will usually use in their
experiments they have all kinds of you
know other words in them not just like
subject verb object sentences
so maybe there's a lot of words that we
just don't see any integration cause far
because the theory doesn't cover
predicting any anything for those words
and actually so this plot here shows the
predicted reading time on the residual
so all the other factors taken out and
you know given the integration costs
what would you predict as a reading time
and here actually we see a positive
slope it is relatively noisy especially
for words with like very high
integration costs just because there are
very few data points but the important
point here is that this one at zero is
relatively high right especially since
these ones are quite sparse it's higher
than all of those ones so this way may
drive the effect that we see was a
negative coefficient there so I thought
okay what if we only look at the nouns
and verbs which are the words that the
theory actually does do predictions for
so for announced everything was fine we
get a positive coefficient so that means
when now and has a positive integration
costs like high integration costs then
it's longer reading times that's you
know a good thing you know supporting
the theory and but for verbs I didn't
find that I didn't find a significant
prediction for verbs and actually verbs
are you know the main thing that this
theory does predictions for right verbs
are most often they had soft sentences
so there should be it's really
disturbing if this doesn't come out
right and but I found and that auxiliary
seemed to facilitate processing at the
verb and yeah so one one factor that
contributes to what the predictions of
this theory are also the underlying
syntactic formalism so how what are the
dependencies between the words
you know this is only bids on top of
that so if you used another syntactic
theory so for example HP SG or so the
predictions may differ so it's really
important to also choose a syntactic
theory that captures you know that tries
to model the processing psycholinguistic
processing so but I want to conclude for
deity for the moment so the findings
were that the I'd he predicts 0
integration for most of the corpus so
this theory which you know has been
developed based on the experimental data
only really didn't provide a lot of
coverage for you know real problems for
using this on a corpus did make their
corporate finance but not for verbs and
there's maybe some stuff that we want to
look at in more detail so the summary
for the broad coverage analysis are that
this backward-looking theory had some
positive but mixed results and surprise
i did work for structural integration
and the question that comes up here's
since I seem to be capturing really
different parts of processing can they
be combined maybe or can they be
improved can we generate a syntactic
processing theory that takes advantage
of what we've learned so far so in the
last part of my talk I'm going to talk
about the syntactic processing model
that I'm proposing sorry
so the requirements for building
syntactic processing model of course
that should be cognitively plausible it
should have cross-linguistic validity we
don't want to assume that German brains
work differently from American brains or
so just because you know they were faced
with a different language and then we
want to be broad coverage so we want the
theory to cover at the different words
and actually make predictions and of
course you want to account for a large
body of empirical findings from the
literature which come from those
experimental paradigm so the assumptions
in my model are incremental T so
incremental tea processing means that on
a word-by-word basis when you perceive a
word you build your representation you
update your idea of what's going on in
the sentence and here I say in the
strict sense because I'm assuming for
connectivity at each point so that means
that you don't defer any processing to
later you try to be really eager in
building up your representation of
what's going on in sentence sorry let's
first talk about this then recently it's
been found that people do it and seemed
to do prediction when they hear a word
so you know when I talk to you if I just
stopped in the middle of the sentence
maybe you would be able to fill in the
word that I was maybe not being able to
remember at that moment and also i'm
going to assume parallel processing so
you could either say okay all the
processing the brains then serially you
know one by one we go through different
analyses and then when you find the
right one here like hey and stood what
you said and or that could be all
parallel
and yeah the other thing that I want to
do is you know combine this forward and
backward ideas that I talked about
before and factually all of these
assumptions are kind of a controversial
among psycho linguists so as an anecdote
so for example my first supervisor
actually like assumes all of those or
beliefs all of those my second
supervisor doesn't believe any of those
and that it's good to always have some
controversy I guess so the important
point I want to make here is that the
interesting thing you know the primary
motivation about why one wants to build
a model is that we can really follow
through and see what the implications
are of saying okay human processing is
incremental what does that imply if we
say that and then can we maybe from
those questions generate you know
contact more experiments or find more
evidence and whether we can support this
claim
so the basic concepts of the model that
I'm proposing is that at each stage we
have a set of expectations about what we
still need in order to achieve a
graphical sentence so once you've found
your head like the determinate ver you
may want to say okay you know still need
some now necessary need some verb you
have these expectations and all the
expectations are strong like starting
the brain or kept track of in the brain
and parallel and they're starting memory
so of course memory is not infinite
right so please not at any given point
in time so you can only keep track of so
many analysis at the same time and
there's some decay so sometimes you know
you've certainly experienced that you
say if you start your sentence you know
go on your sentence and by the end of
the sense you've forgotten actually how
you started your sentence right so decay
should capture that effect and then
there's also a verification component
which plays with the kind of prediction
or expectations that you build up and
that actually links this kind of
framework to processing difficulty so if
you know I didn't expectation of what
what was what was going to come up in
the sentence and I find it then that's
going to be really easy if it was
something I was strongly expecting and
that's what this says and if it wasn't
too old and if if you know I had a very
strong expectation for something to come
up but didn't come up I have to forget
this analysis that I was building up so
far and that was really high probability
analysis then that's going to be very
difficult as well so this is just the
form
that formalizes at this model so in
order to build such a model I have to
define the linking theory this is
essentially what I just was just talking
about on the last slide and then I have
to build a parcel that satisfies the
constraints that I gave in the beginning
so she has been incremented parser in
order to give us the right information
in order to generate the expectations
for example and has to be probabilistic
because I want to keep track of
everything in parallel and all the
probabilities of expectations and
analyses that I have to choose my
grandma formalism that you know I want
to build those parts on top of and I'm
not going to go into much detail here
and you can ask me about that later but
I true Street chose tree joining
grammars because I thought they would
let me best implement this strong notion
of incremental og so what are treated
running grammars I don't know if you're
familiar with them essentially how the
work is that a lexicon entry is
represented and buy in my territory so
for example Peter would be represented
by being an NP verb like sleeps what we
were presented by being the head of a
sentence and needing a noun phrase in
order to buy the sentence and there are
two elementary functions in tree joining
grammar one substitution that means when
something is requiring a noun phrase
like here that's noted with a little
arrow here and then something else can
satisfy that requirement then we can
join them together and they can build a
tree yeah like this tree here and the
other one is used for recursive
structures so for example here at deeply
and from the lexicon we know that deeply
would be an adverb that
needs a verb phrase to join into so in
its multi favor and this would just find
some an accessible node of the same type
and expanded so like this sorry these
two go into here and with a new branch
there so as I mentioned before it's
really crucial to my theory and that we
can generate these predictions or
expectations at every point in the
sentence they're really important to
calculate what what we're still
expecting you know to build a
grammatical sentence so for example and
here I want to talk about how we can
generate those expectations of what
these expectations are generated by this
theory so for example if we've seen the
horse seldom so far there's no way that
we can connect the horse and this seldom
tree together right they don't they
don't have any open slots they don't
have any matching categories so if we
don't add any additional functionality
here then we can join them together
right so here we would have to just keep
them around until we see set the head of
this phrase but then we wouldn't be
fully connected right won't be
incremented in the strongest sense
because we haven't figured out what the
relation between those two is so in
order to make this fully incrementally
we need to also have those connecting
structures and at this point we would be
predicting that there has to be a verb
phrase later once we've seen this even
though we haven't seen the verb yet
another way of generating predictions is
through the lexicon entries because this
verb your nose that it's going to need
to non phrases in order
by the sentence so if we always connect
our structures as far as possible we we
always can just read off the graph what
we need in order to build a grammatical
sentence service that's going to be our
expectations and once we have those
expectations so here for example we're
expecting the verb phrase and we haven't
actually seen anything that's the head
of a sentence yet so we predicted those
notes denoted by the blue errors here
and then once we see what we are what we
were predicting we have to unify those
two structures right we always have to
make sure that everything we predicted
we actually do see later on otherwise we
were just over generate and generate
incorrect representations so um here's
the relative clause again I hope you
know tired of relative clauses yet so
this is an example for an object
relative clause here so if you had like
my grandparents who and we would
interpret this as an object relative
clause so you know who my father said
whatever or my father talked to for
example right it was this kind of
structure and so this and so we would
have these two and parallel structures
to keep track of and they have different
probabilities up to a subject relative
clauses occur more frequently than
object relative classes for example so
say okay this one has a certain
probability this one's certain
probability next word we see is time
time also has different structures it
could you know it could be a verb could
be noun and depending on what it is it
would bring a different lexicon entry
with it so now in order to you know
predict how difficult it is to have time
at this point we have to consider okay
can we connect this structure with
anything in these
and in fact we can because this can go
into here so we could say my
grandparents who time has made whatever
i'll read it in the face or so we could
connect this or we put see like oh this
one how does this connect with any of
these structures and in fact we can join
it em with the structure down here match
it up you know everything fits nicely
and the predicted the predictions are
satisfied and so I mean these
probabilities are of course made up so
once we have the incremental passer you
can just determine them on the fly and
calculate what our predictions at each
point in the sentence how difficulties
of the structures are and then if you
would get after time who get a new verb
then we would know okay kind of beam
this structure because this one doesn't
have any space to put a verb in it
anymore and then when we would know that
okay this structure we have to get rid
of so that's causing processing
difficulty so then the question is how
how can i generate all those lexical
entries automatically from a part from a
tree bank and one way to do that is to
so this would be the annotation in the
pen corporate pantry ring for example
and the first thing is we have to figure
out what is the lexicon entry from the
corpus so we their method other people
have done that and have have transport
form the pantry bang into an Tek formats
so that's not a problem and the second
thing we need is we need to figure out
what the connection path is so that's
essentially how much of the structure do
we need to fully connect all the lexicon
lexical entries that we've seen so far
and so at every point where there's some
color
and a little loop here that we don't
haven't seen a lexical entry for yet so
for I'm here for green we have within
this you know at the point we've only
seen then Italian seen people so this is
something that we need to predict first
so we can automatically generate the
necessary prediction entries for our
lexicon from the structure okay so I'm
getting to the end of my talk so the
test for implementing this model speed
yeah firstly transform the penn treebank
into this format which I just showed
then extracting the lexicon and this is
like how far along I am so far in my PhD
the next step would be to train a
statistical incremented parser for this
theory their number of other incremental
parcels around there like they work on
other gramma formalisms other versions
of the grammar formulism but essentially
it's you know where no problem and then
I have to define a probability model for
my particular tree joining grandma
theory and lastly once i get all the
predictions on a word-by-word basis
according to this theory i be able to
evaluate it on the eye-tracking corpus
again and compared to the other theories
that i've evaluated before so in summary
and the high level my talks been about
how the syntax inference processing
difficulty now how can you model this
processing difficulty in summary first
I've talked about the sentence the
eye-tracking corpus that I'm using as a
resource shown that there's some
evidence that it's a valid resource that
gives us complementary evidence that you
know in addition to experimental studies
also i have evaluated two previous
theories of processing difficulty and
lassie i've proposed the my own theory
which tries to combine you know
different aspects from the previous
theories and model prediction and strong
and true mentality to in order to see
what the implications of those
assumptions are okay thanks for your
attention and yeah feel free to ask me
some questions okay um so you know you
talk a lot about measuring difficulty of
written sentences and reading but
probably mostly this would be used for
spoken speak you know sentences right i
mean when the person is driving or in
some other way engaged in the computer
is talking to them and i assume that
would relate but can you talk about
that's the difference between those two
right that's a very valid point they
might not you know the sentence
structures are of course going to be the
same but maybe we are measuring
something different when we look at
people's reading behavior that's a very
valid point one reason why we look at
reading is when people are just
listening it's really hard to know
what's going on in their brains so it's
a more difficult to have a correlator
for that and so one one thing that
people do is for example visual word
paradigms so they have people listen to
a sentence and look at the picture the
picture may have like different objects
in it and there's you know findings that
for example like one funding for
prediction for example would be that the
person sitting there looking at picture
was a cabbage and
and a fox and something else maybe a
tree and the sentence is it's a German
experiment but so sentence essentially
says like the Fox eats oh yeah the Fox
and then eat but in German can be that
the fox's the obvious well yeah the Fox
eats and then people anticipate it's
going to be the rabbit and not the
cabbage for example all right the more
interesting case of course when you say
the rabbit eat or is eaten and then
depending on which one it was each or is
eaten you know it'll be the Fox of the
cabbage and the idea is that if people
you know integrate the Fox and they eat
correctly given the syntactic structures
that either you know going to look at
the cabbage or at the Fox depending on
which one it is so you can do some
things with those little red paradigms
and another another thing is people are
looking at how big the pupil is so
apparently but that's kind of a new
thing I guess so if something is really
different like you're concentrating very
hard thinking very hard about something
apparently the pupils open a bit wider
so they try to measure how how far open
that is but it's you know I don't know
how noisy letters and I haven't done
anything with it myself
okay could you say again what is a
lexical surprise oh yeah the question
was what's next could surprise it as
opposed to structure it surprises so
let's see we're sorry I want to show it
with a picture so here the probabilities
would be from these kinds of rules for
structured surprises what's the
probability that an S has an on phrasal
verb phrase and so on and for
lexicalized surprises you would add some
more rules that says like how probable
is it that there is a determiner how
probably is it that the reporter like
given that we have now and how probably
is it that it's going to be the reporter
so you know for very frequent words you
get high probability numbers there but
if you have something that's a really
weird word you haven't heard before or
yeah once in your life you know that's
going to make the sentence also more
difficult according to the lexicalized
strict ok all right so it's one o'clock
so we should probably end the talk but
bearable to be here to answer any more
questions and thank you very much very
ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>