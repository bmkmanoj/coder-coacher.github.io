<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2016: ClusterRunner - Making Fast Test-feedback Easy Through Horizontal Scaling | Coder Coacher - Coaching Coders</title><meta content="GTAC 2016: ClusterRunner - Making Fast Test-feedback Easy Through Horizontal Scaling - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2016: ClusterRunner - Making Fast Test-feedback Easy Through Horizontal Scaling</b></h2><h5 class="post__date">2016-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/V7fhx1i4qPE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright so next lightning talk up we
have taejun and Joseph hi I'm TJ this is
Joey we are software engineers on the
productivity engineering team at box and
to those of you who are familiar with
what the productivity team does we own
all the tooling and services relating to
continuous integration development and
testing so I think of tool to just get
Jenkins test infrastructure and a
service called cluster Runner which is a
tool that we built in-house a box a few
years ago and open sourced and it's the
topic of our talk today so cluster
Runner is a distributed service that we
built to achieve a single goal and
that's to execute tests and the fastest
way possible and cheeses by horizontally
distributing workload across whatever
computing resources that you allocate
clutch runner plus runner officially
utilizes these resources by using
historical timing data from previous
bills in order to best distribute the
workload you interact with plus runner
through a restful api that we provide
and this lets you integrate close runner
with any automation or CI that you
married or you have now i could talk all
day about all the features of cluster
owner and why it's so great but showing
is rather than telling and so we've
dedicated the second half our talk into
showcasing cluster runner in action so
i'll be deferring the list of all the
features until then now this being the
test automation conference I don't need
to sell you on the importance of writing
and running automated tests well here
because we care deeply about shipping
quality software in a timely manner and
we want to explore the various aspects
of achieving that so the aspect we're
focusing on today with cluster Runner is
test volume specifically running the
most amount of tests in the shortest
amount of time possible now test volume
when you first start writing unit tests
is not really an issue because there's
few enough of them and they run very
quickly your developers have no issue
running them locally on there dead
machines and eventually adult you will
probably adopted CI system such as
Jenkins to run them remotely and
asynchronously on a more powerful
machine than your macbook probably but
as time goes on and your co-pays girls
your test code base should also go user
developer should be writing unit
hopefully and that leads to the
development and test iteration loop
getting longer and longer which in turn
turns into engineer's losing
productivity waiting for their automated
test Suites to complete and we saw this
up in first hand a box so we introduced
developer written integration and unit
tests in 2012 to our primary code base
which is written in PHP so we use be
treated so a couple weeks in our entire
test we took about 10 minutes to run on
a single machine serially and a couple
months later we're up to 45 minutes for
a single build our year later up to
three hours and today we're up to 30
hours to run our PHP a test suite once
on a single machine serially so this is
what success looks like you want a lot
of code coverage you want to instill a
culture of quality and testing in your
engineers and getting to this point is
very difficult and it merits a talk on
its own but today's talk is about it's
not about evangelizing good testing
practices it's about what do you do once
you reach this point because you
obviously can't expect your developers
to wait 30 hours for a bill to complete
that's absurd and this is not a built
that we want to run once per release
it's a bill that way when run every
single time a developer pushes a coder
view merges the trunk has some sort of
experimental changes that they have in
some road branch so a box that roughly
translates into several hundred builds
every day so the target here to run the
30 hour test suite several hundred times
every single day so what'd he do well it
seems like a problem that you can throw
money into serve in the form of test
hardware to paralyze the test execution
so test servers are relatively
inexpensive compared to the salaries of
engineers especially in Silicon Valley
so it's an obvious investment to make
it's one that most companies do you make
so let's say you got the purchase order
approved let's say you ordered and
delivered the servers your data center
guys have networked in RAC these servers
now all you need is the actual software
to paralyze the execution of the tests
to utilize this new hardware capacity so
before we determine what tool to adopt
to do that we first have to analyze what
our requirements were so a box we got
several teams got dozens a git repos
written in several different languages
and the other own corresponding unit
testing frameworks additionally we have
functional test performance linters and
all that good stuff and as I mentioned
it's on the previous slides some of
these test Suites take a very long time
to complete so based on these facts of
life of bucks we devised the cider
requirements for the tool that we want
to adopt to paralyzer test so first
requirement is that he has to be easy to
configure and use the engineering teams
that box own their own CI system and the
configurations of their tests so if the
tool requires like an entire dev ops
team to configure in use they will
simply not adopt it second requirement
is because we have so many different
code bases and test technologies and
frameworks used it has to be technology
agnostic because we don't want to use a
tool that works just for one peep one
team it's kind of work for everyone and
last but not least let's give us best
his feedback and the whole reason why
we're investigating this is to get
fastest feedback so that's an obvious
requirement so at this point we did what
any responsible engineering team would
do we looked online for existing
solutions ID like something open source
but we wanted to where we were willing
to pay for an enterprise license if we
found something that we liked and at the
time when we searched we didn't find
anything that about all of our
requirements which is very surprising to
us because it felt like it should have
an installed problem by now and so we
decided then to solve the problem once
and for all and we invested a couple
quarters into a building cluster runner
and we made it publicly available in
2014 by open sourcing it so we've been
using cluster owners since 2014 and it's
an absolutely irreplaceable service in
our CI stack now the 30-hour test suite
I was mentioning earlier runs today in
17 minutes 17 minutes so if you to get
you an idea the order of magnitude of
the difference 30 hours is war time that
I spend writing code on a very good week
17 minutes until i spend replying to an
email so the only reason why it takes so
need two minutes to complete is because
our longest test file in our Patriot
test suite takes about 16 or 17 minutes
to run so if someone were to break up
that house file to swallow our test
files plumber owner would give you
feedback and as fast as the next lowest
test in the test we all right that's
enough context i'm going to hand it over
to Joey to show you guys closer
interaction
thanks TJ all right go okay so I'll go
through the four main steps that you
would use you would follow to set up
plus runner up for your project so the
first one being bringing up the cluster
so getting the cluster under service
running on your infrastructure the next
part setting up your projects so
basically just adding a config file to
your repo and then after you have your
cluster setup and set up your project
obviously you want to execute your build
and look at the results so first
bringing up your cluster cluster runner
it's all contained within a single
executable so we have the master service
and the slave service all part of the
cluster owner executable so this is
pretty straightforward there's no extra
dependencies you just run cluster in or
master you give it a port and then you
run clusterin or slave on all the slave
hosts that you want to attach to the
master and basically just point it at
the master all right and this can be
done manually this can be done by your
by some config management tool this is
just how you would do it manually cool
so that's it for setting it up that's
once it's up and running you can hit
endpoint on cluster runners API so
cluster owner communicates via rest
api's this one is the slave in point so
just taking a look at what that would
look like so this has two connected
slaves to it and this just gives you a
whole bunch of info on each slave so
basically its current status so if it's
working on a build or not and then some
kind of state information like how many
executives it has no type of thing cool
so that's it for setting up the cluster
it's pretty straightforward so setting
up the project so as I mentioned you
just add a single cluster owner gamal
file at the root of your project repo
this is a small file will show you an
example here in a second it tells
clustering or which commands you want to
run to run your tests and also how to
distribute them across the
infrastructure so yeah here's a very
simple example of cluster runner gamal
so you can define multiple jobs in a
single cluster any ammo file this just
has one job named phpunit
and the first section commands tells
cluster how to run your tests so in this
example we're just running phpunit on
each test file the second section
describes how to distribute your tests
across the infrastructure so a single
unit of work and cluster Runner is
called an atom and this atomizer is
command tells clusterin or how to
generate those atoms in this case every
atom corresponds to a single test file
so we're just running a find command
giving it star test PHP this will just
return us a list of all the test files
and those will be plugged into that
command section up above we made this
very simple intentionally this was like
one of our main requirements when
designing cluster runner at cj mentioned
we're a central team at box so we own
the cluster and the infrastructure that
it runs on all the individual teams own
these files in their projects so the
individual teams on their tests they
decide what goes into the cluster or
animal file so this makes it really easy
to separate ownership cool so this it's
like an example of the individual
commands that would be generated after
we combine those two sections together
so we're just running PHP unit on each
test file in the repo all right and then
so each of these commands basically gets
distributed across the infrastructure so
now we're ready to execute a build so
this is another sub command of cluster
runner cluster run or build this so this
works like an API client of the cluster
run or master basically all it does is
send an API request to the master you
could easily just do that API request
manually so this is pretty
straightforward you just give it where
the master host is you say it's a get
type project give it the URL to your git
repo and say which job name within your
cluster runner gamal that you want to
run alright and that's it for running
the build so if we could play the first
video this is a visualization that we
built to kind of visualize how a build
runs so the big circle represents a
build the small dots represent the slave
hosts so you'll see that one popped off
right away so that means
all the work is in progress that slave
couldn't pull any more work for this
build and then obviously there's a few
tests that take a lot longer than those
first test so these second posts will
take a lot longer and it's done okay we
can switch back to the slides cool so
the build ran great now we actually need
to look at the results to get anything
out of it so here's another end point on
cluster runner the build end point just
the build ID so you look at this it'll
give you a ton of information about what
happened during the build so just kind
of starting from the top you can see the
status is that it's finished there were
no failures it gives you a human
readable details message final five sub
jobs are complete an error message if
there was any number of atoms etc the
term sub jobs basically cluster owner
batches just kind of an aside cluster
owner batches atoms into sub jobs to
send them to this slave house it just
does that to reduce network overhead and
sending tons of atoms to the slaves so
that's what sub jobs means another nice
thing about this is this this will list
all the failures right here if there
were any luckily in this case that we're
not cool you can drill down even further
with the API so you can look at each
individual sub job that ran looking at
this it's kind of small hopefully you
can read it basically it will list every
atom that ran which command that which
command it run and the amount of time it
took the amount of expected time it took
so as TJ mentioned cluster owner is
keeping track of how long each test
takes to run and using that information
in future runs cool and you can also get
the exit code from here so customer uses
the exit code to determine failure or
success the last way that you can get
build results is you can actually
download an archive file of all
containing all of the build results so
this is pretty similar to the last two
API responses i showed if you look at if
you download this file and expand it you
get kind of a breakdown of each sub job
or atom that ran
and then you get the command the console
output for that that test run so this
would be like the console output from
running phpunit exit code and amount of
time customer also lets you in your
cluster under gamble to put files into
an artifact directory so if you did that
to save artifacts from the build they
would be in this archive file as well
okay so here's kind of the say a
different picture of the same build that
I showed the same animation i showed you
can see that one slave host finished
very quickly there were two other ones
that went on for a little longer and
maybe one of the first things you notice
is that this is not the best way to run
these this is not the best order to run
these into taking advantage of your
whole infrastructure so cluster runner
will it'll look like this the first time
you run a build the next time you run it
it will optimize so it will use that
timing data from the previous run it'll
run the longest tests first and
generally you get way better timing for
future runs cool so here's a quick video
that we took a while ago this is our
production cluster instance that box so
it's sped up but you can kind of get an
idea of how dynamic this is and you can
see slight house switching from one bill
to another they don't have to wait for
you know all the sly pose for single
bill to finish before before they can
work on another build yeah so that's
that's we can switch back to the slides
thank you oh so that's pretty much it
for setting up cluster runner and
running your build this is kind of our
wish list what's next what we want to do
next AWS integration with auto scaling
so right now cluster owner has endpoints
for adding slaves and removing slaves
pretty easily it'd be cool if we could
do that dynamically and scale up the
the fleet as we needed it also docker
support so running tests in containers
just to avoid environment differences
improvements to the deployment mechanism
in place upgrades so that you don't have
to take the whole cluster down to update
it and then a web UI so the
visualizations I showed our kind of just
individual dashboards right now so it'd
be cool if we could integrate those and
actually have a full web UI so yeah we
love it when people get involved we love
feedback if you want to check out more
go to cluster intercom and that also has
a link to our github repo and if you
have any questions email us at
productivity a box and we will be happy
to help you and that's it oops very good
thank at aging and Joseph although I
think if you had built that cute
visualizer first people wouldn't have
minded waiting 30 just watching that's
probably true it's very saved a lot of
time people always just stare at the TV
in your area it would punch that for 30
hours we have time for one question
because of a lightning talk but how do
you distribute the binary to the testing
nodes turn signal so this is a part of
our deployment mechanism to close
memberships with a pretty simple
command-line tool and a part of it is
closed for our deploy and you specify a
list of slave O's so the master takes
its cup its own copy of this binary and
deploys it to those slaves so there's a
prerequisite requirement right now where
the masters and slaves had to ssh-keys
exchanged beforehand otherwise the
deployment will fail so the short answer
is the binary ships with both the master
and the slave in the same binary so you
can run in different modes that's part
of the deployment thing that we want to
improve yeah that's something we're not
happy with that we'd like to deploy it
as an RPM and put in Yemen stuff but
it's on our wish list all right very
good thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>