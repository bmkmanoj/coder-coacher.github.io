<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building a Safer Web: Web Tripwires and a New Browser Architecture | Coder Coacher - Coaching Coders</title><meta content="Building a Safer Web: Web Tripwires and a New Browser Architecture - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building a Safer Web: Web Tripwires and a New Browser Architecture</b></h2><h5 class="post__date">2008-03-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/H9_mG_yAoTQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">all right i'm gonna eat of the security
team and it's my pleasure to introduce
you to our speaker today Charlie rice
Charlie rice is a is a PhD student in
the Department for through science and
engineering at University of Washington
studying with Steve Gribble and Hank
Letty his current research focus is on
improving the security and reliability
of web content and web browsers and this
is the topic of the top in the past he
has also worked on models of Wireless
interference with David whether all
Charlie received a BA in at ms in
computer science from Rice University
not named after him I'm sure you get
this where he worked with Corky
Cartwright and Peter trishul at Rice he
was the second lead developer for dr.
Java a widely used educational
programming environment really Thank You
minute so hi I'm Charlie rice I'm going
to talk about our work at u-dub to make
it safer to browse the web and I'm going
to focus on two topics in today's talk
the first is how we've seen that some
webpages are being modified in flight
that is between the server and the
browser and some of the implications
that has and how we can detect that with
what we call web trip wires in second
I'll give an overview of our ongoing
research to design new web browser
architectures to make it safer to run
all the active code that's on today
sweat and my work is motivated by the
fact that it's really not safe to browse
the web today and you can see this in a
pretty typical browsing scenario where
you might have several windows open in
your browser including some windows
you've been to before like a webmail
client may be renting movies and then
some windows you haven't been to before
some pages like search results now
there's a lot of bad things that can
happen in these situations so first one
of these search results could try to
exploit a vulnerability in your browser
to install malware in your computer and
you've probably seen this in the news
there's lots of cases
where some of these exploits appear
either in banner ads or on pages to try
and install software on your computer
just by visiting a web page
unfortunately this is far from the only
problem when browsing the web so you've
probably heard of cross-site scripting
attacks this is where an attacker has
the ability to place their own script
code on someone else's web page to do
things like stealing cookies or
modifying the page contents we're
leaking other private information from
the page now yahoo mail for example was
vulnerable to a cross-site scripting
attack in 2006 that meant that an
incoming email could run script code as
part of the mail client and in doing so
it could then send mail to other people
in your address book or send spam to
other people there's also what are
called cross-site request forgery
attacks where pages can take advantage
of the way that browsers handle
credentials and try to abuse that so say
you're logged into netflix in one window
another window in your browser could
send a request to Netflix in the
background and your browser will send
your cookie your Netflix cookie with
that request because it's going to
Netflix the server now in 2006 Netflix
was actually vulnerable to an attack
where another site could change your
account settings just by sending one of
these requests in the background
insights have to go out of their way to
make sure that these attacks don't
happen in practice now today I'm going
to talk mainly about what might be an
emerging issue which is that when
clients were request a web page they
might not get the page that the
publisher intended but instead some
modification of that page where for
example an ad might have been inserted
by their ISP and so this could really
change the page that they're looking at
and might have some serious consequences
so in today's talk I'm going to focus
mainly on this last issue of how pages
are being modified in flight based on a
recent measurement study we've done at
the University of Washington and i'll
show how publishers can detect these
sorts of changes to their own pages
using what we call web tripwires then
I'll take a step back and give you a
broader overview of our work
on creating a safer web browser
architecture so that some of these other
problems that I presented can also be
addressed in the long term and this is
ongoing work that I'm doing it you done
okay so in the first half of the talk
I'll cover what in-flight page changes
we saw in practice and how we were able
to detect them this is joint work with
Steve Gribble and Yoshi kohno at
University of Washington and Nick Weaver
at the International Computer Science
Institute this is a paper that will
appear at NSD I this April so we were
inspired to do this measurement study
based on a news article we saw last
summer that suggested that some ISPs
might be inserting ads into the pages
their users requested now we were pretty
surprised to hear this that this would
actually be happening and we're over
this this implies that the ISP actually
has some box on their network that is
doing deep packet inspection and looking
at the contents of web pages and then
modifying them to insert ads or
JavaScript or other content so we were
surprised by this and we set out to
answer the question okay how often does
this occur in practice so in this part
of the talk i'll show how we were able
to detect these changes the results of
our measurement study in terms of what's
being done in practice some of the
dangerous consequences we saw that came
as a result of these changes and then
what publishers can do about it in terms
of web tripwires ok so at a basic level
we realized we could detect these
changes using some simple JavaScript
code on our page and we call this a web
tripwire it's a piece of JavaScript code
we put on our page which gets sent to
the clients browser and runs there and
it does an integrity check on the pages
HTML code so we can see has the HTML
code been changed since it left the
server so for example if an ISP had
injected an ad that would have modified
the HTML source code and we'd see that
change and we display a message to the
user saying this page has been modified
in flight we'd also report that change
back to our server so that we could
analyze it later now we've put this
online on a web page at Vancouver CS
Washington dot edu and i'll leave that
you're all up there for the remainder
this part of the talk you're welcome to
visit at any time you're on a network
that you think might be altering your
traffic if you just want to put them to
the test but to give you a little bit
more of a feel of how the web tripwire
works we've built it so that it can
support just normal everyday pages where
the client first fetches and renders the
original HTML page and there's no change
at this point but we've added a single
line of JavaScript to the page which in
the background will then fetch a
separate javascript file containing the
web trip our logic now this web trip
wire also includes an encoded version of
the original page you can think of this
I could check some or just a second copy
of the page as a string that we have
encoded in the tripwire code now the
idea is the web tripwire will then
compare the actual page it received the
HTML source code with what was in the
web trip wire and see if there was a
difference now it's a little trickier
than that because in the browser you
can't ask for the pages own HTML source
code as a string because the JavaScript
only has access to the browser's
internal representation of the page in
the DOM and that varies from browser to
browser in terms of what it what it sees
so we use a little trick where we just
send an XML HTTP request for the
original page and then we'll get that
back as a string and because our page is
cached in the clients browser doesn't
even involve a network request so we can
then compare that string with what we
expected it to be to see if there's any
different okay so to use this in a
measurement study we wanted to get lots
of people to come visit the page lots of
people on many different networks so we
could see our some networks changing the
page and some networks aren't and so on
so we posted a story to slashdot and to
dig into some other news websites to try
and get a large crowd of people to visit
the page and this works pretty well for
us we it ended up getting posted to
these sites and we got visits from
50,000 unique IP addresses over the
course of about two and a half weeks so
this gave us a nice data set to work
from in terms of is our page being
modified in flight or not
okay so what did we find we were
honestly would have been a little
surprised to see any changes at all but
as it turned out we saw six hundred and
fifty-seven of those IP addresses report
some change back to our server so it was
about 1.3 percent of all the traffic
that visited our page now that was a lot
more than we expected now many of these
changes are being made by client
software these are things like proxies
that are installed on the client which
when you think about it you that's you
know some clients have ad blockers and
popup blockers and we'll see this sort
of thing we won't see changes made by
browser extensions those are done within
the browser itself but if the client has
some software installed outside the
browser will see that change but we did
see some changes that were made by
agents in the network as well including
some changes made by Internet service
providers and enterprise firewalls and
what's more we saw that there's really
sort of a diverse set of incentives here
for why people between the server and
the browser might want to change the
page and that these incentives aren't
always in line with what the publisher
has in mind so the publisher might be
interested to know if their page is
being modified in flight so I'll give
you a sense of that by walking through
some of the categories of changes that
we saw and this includes changes made by
ISPs enterprise firewalls client proxies
and even some cases of malware okay so
we did see about 16 IP addresses that
were affected by injected advertisements
so this is about two point two point
four percent of the changes that were
reported to us and these are done by
companies like neb you add that contract
with smaller is ps2 then inject a piece
of JavaScript code into a web page into
every web page of the client visits and
then it does behavioral tracking so that
it knows where clients have visited and
then it optionally displays an ad based
on that clients browsing behavior
there's also some free wireless networks
like Metro Phi and lockbox Metro Phi's
here in the area where you can get free
wireless access but they subsidize it by
showing you ads on every page and those
ads are shown by actually modifying the
page content
is putting the ad right into the page
now from the ice piece perspective this
is nice because it's a way for them to
generate revenue off the people using
their networks on the same time it's
maybe annoying too many users and it may
affect publishers own revenue streams
when they display ads on their own pages
so they're met their ad may not be at
the top of the page anymore or we don't
know if there's been any cases of AD
replacement but that would certainly be
a serious case now another growing
concern here is that this may be a trend
that is increasing since the study we've
seen some more companies that are
advertising the service that will talk
to isps and say we'll inject these ads
into your traffic and these include
companies like perf tech front porch add
Zilla and form which you may have seen
in the news recently is having some
large contracts with major US UK ISPs so
these companies are looking at deep
packet inspection for web pages and then
modifying them to show ads now this
isn't the only type of change we saw
ISPs making we saw another 30 IP
addresses or so that were reported
changes that were based on compression
so on low bandwidth networks like
cellular networks the ISP might modify
the web page to make it take up less
bandwidth an example of this is image
distillation they'll first ship you a
low quality image of an image a low
quality version of an image and then
include some JavaScript so if you click
on the image it'll send you the high
quality version so these are being done
in some networks as well okay so at the
firewall level we saw some changes being
made by enterprise firewalls that were
actually designed to increase security
so this may be a type of change that you
might want to allow to your page in some
cases and a frequent case we saw was a
product called blue coat web filter that
would insert a piece of JavaScript code
into web pages that would look for
malicious behavior when it got to the
client so is the web page trying to
change things about the address bar or
is it trying to do anything that would
make the the page seem other than it
is to the client and so this is a case
where it might actually increase the
security of the page to allow some of
these checks and from the enterprises
perspective this is nice because it
might make the web a little safer for
their clients and from the publishers
perspective it may also be nice to know
if this is happening because they may be
able to assign a lower risk to some
clients than to others for example if
you see a client who has some of these
checks inserted they're probably in a
well-protected Enterprise versus a
client who's browsing on a free wireless
network who might have somebody snooping
over their shoulder looking at their
wireless traffic so maybe nice to know
if these checks are being done okay now
most of the changes we saw were being
done at the client and this like pop-up
blockers and ad blockers account for
about seventy percent of the changes
that were reported to us and these are
done by products like zonealarm which
are personal firewalls which might be
designed just to block port connections
in general but they're also modifying
web pages to prevent pop-ups and it's
done by dedicated products like ad
muncher and proximate Ron these are some
proxies you can install that will modify
the page to prevent ads and prevent
pop-ups now from the clients perspective
this might make some pages less annoying
especially if there's lots of flash ads
and it's very distracting from the
publishers perspective on the other hand
this might impact your revenue if you're
a dart being viewed now a final category
is that we were surprised to find we
could detect certain types of malware
using web tripwires and for example we
saw one case where a client had adware
installed and this modified the page
pretty heavily so normally when clients
visited our test page they'd see a frame
that had something like this which is
just a piece of text and a couple links
in it and what the ad where did to the
page was it added a few extra links and
i'll highlight those here in red that
had double underlines and if you put
your mouse over these links then a new
frame would appear that showed an ad
that was targeted to whatever that word
was and so the adware was actually going
through the source code of the page
some of these words into these little
pop-up links so that was a pretty
invasive change in terms of the source
code of the HTML we also saw two clients
that appeared to have been affected by
worms and our hypothesis here is that
this is done using art poisoning so we
saw that some clients would report our
page back to us as if it had had an
exploit injected into the top of the
page and what we think happened was
there was another client on their local
area network that was infected by a worm
and that client started sending out art
packets to other clients on the network
saying I'm the router send your web
requests through me so now when this
client requests a web page it'll go
through the infected client who has a
chance to modify it before it gets to
the the client who requested it and when
it does that it has a chance to inject
an exploit to try and spread spread the
worm we saw this in two cases we also
saw a case where this similar practice
may be happening on the web server end
where an infected client in the server
room was able to inject itself between
the server and the outside world to
inject exploits to all visitors to the
site now obviously these sorts of
changes are just there to help them al
where authors either to make money off
the ads or to spread the worms and they
pose a clear risk of the user so it's
it's clear why you might want to know
when these are happening ok so now I'll
take a closer look at some very
dangerous consequences that come as a
result of these changes beyond just why
a publisher might not want it happening
to their page so there some of the
changes had unanticipated consequences
for example they broke web pages that
they were injecting content into we saw
in several cases that some of the
in-flight page changes caused JavaScript
errors on our our page so in some cases
it meant our web tripwire code didn't
work we just didn't get a report from
that client and we had to get feedback
from the user saying that the javascript
error occurred and this is what was
happening to the page we also saw some
of these changes affecting
popular web sites like MySpace and
several web forms so you can actually
find some of these sites by searching
around where a user's post to a cut a
space like post to a site like myspace
actually has a little piece of pop-up
blocking code in it and it turns out
this was injected by see a personal
firewall where it was trying to block
pop-ups but it was injecting the pop-up
blocking code into the wrong part of the
page so that when the client posted a
comment the pop-up blocking code showed
up in their post and so there's some
posts that just don't have anything to
do with a comment and then some users
who were very confused saying why is my
computer doing this to my post so it's
sort of an amusing example of what can
happen when you inject JavaScript code
and that interferes with the pages
execution now unfortunately it gets
worse than this because we also saw that
there were some security vulnerabilities
introduced by proxies that were
modifying the page and here I'm talking
about cross-site scripting
vulnerabilities where an attacker then
gets a chance to place their own script
code on someone else's web page now in
the common case this is usually caused
by a bug on a web page itself in terms
of the input handling so if some input
gets sent to the website and the website
reflects that back to the user if an
attacker can hide script code in that
the running script code on your page and
web developers go to great lengths to
make sure that those bugs don't appear
in their pages they put a lot of effort
for this now what we saw was that some
proxies were modifying the web page and
then injecting code that was vulnerable
to a cross-site scripting attack so then
that original page would become
vulnerable even if it was all otherwise
safe and this was done by products like
ad muncher and proximate Ron and I'll
talk more about how the vulnerability
worked in a second but I want to point
out two things here one is that the web
developers didn't really have any
options there was nothing that the web
developers themselves could do about
this because the vulnerability didn't
happen until after the page left their
server in second for the clients who had
one of these vulnerable proxies
installed the proxy was actually
modifying most if not all of the web
traffic that the client was receiving
meant that most if not all of those
pages became vulnerable to cross-site
scripting attacks and in the common case
this was most HTTP unencrypted pages but
in the case of proximate Ron there was
even an option to have the proxy serve
as the endpoint of encryption for HTTPS
pages so that they could block ads and
pop-ups on HTTPS pages well that meant
that suddenly every page that the client
requested even encrypted pages became
vulnerable to cross-site scripting
attacks okay and if you view the the web
browser is like an operating system
where it's actually running lots of
programs from lots of different places
suddenly all programs are becoming
vulnerable to attack and that's sort of
analogous to a root exploit where every
program you're using becomes vulnerable
to attack so how did this work well in
the normal course of execution these
proxies would inject a small piece of
JavaScript code into every web page to
try to block the ads and block pop-ups
now as part of that these proxies would
also include the full URL of the page in
a JavaScript comment now because it was
common it wasn't even affecting the
functionality of the plugin it just
happened to be there but unfortunately
the URL is something that the attacker
can influence so the attacker could
place some script code at the end of the
URL either after a ? to make it a server
parameter or after a hash to make it an
anchor tag on the page and then when the
attacker convinces a user to visit that
page side by sending them a phishing
email or by redirecting them from
another web page then that script code
will get copied by the proxy into the
body of the page itself and then that
code would run if it's formatted
correctly allowing the attacker to run
code on that page okay so the first
thing you could use this for is a
phishing attack on some types of banks
some banks use an HTTP front page that's
unencrypted has a login form for your
username and password and they don't
switch over to https until after you've
clicked the submit button well in those
cases you can use one of these attacks
to make that password form send the
password somewhere else but we built
another exploit that shows how serious
these attacks are so we
Delta page that would detect if a user
was using one of these vulnerable
proxies effectively by using a web
tripwire we look to see if it had
injected code into our page and if so we
would redirect them to Google and then
in the URL we'd include an extra piece
of script code so that script code would
then run on Google's front page and
modify this search query form so that
when the user typed in a query the
attack code would propagate to the
results page okay so now we're running
JavaScript code on the results and we
can modify any of the results to point
to different sites or to show different
content but what we did is we modified
every outgoing link on the page to also
include our attack code so this means
we're now running script code on every
page the user visits in this browser
window until they type in a new URL by
hand or close that browser window so
this is a very serious exploit that we
can watch what the users doing and
modify anything on any one of those
pages ok so we've reported these
vulnerabilities and they've been fixed
so if you're using one of these products
be sure to upgrade to the newer versions
that don't have these vulnerabilities
but it's really a demonstration of how
much power these web page writing
programs have that is effectively giving
them route power over all of your web
browsing traffic and I also want to
point out that by using web tripwires we
were able to detect these exploits these
vulnerabilities more easily and in these
cases it was because we could search for
the URL of our own page within the
changes that were reported to us so
those were the cases we could flag for
further analysis because if the change
was including the URL that means it's
including something that the attacker
has influence over and we could look to
see if it could then be further
exploited okay so now let's talk about
what options publishers have so at a
basic level how should publishers react
to the fact that their pages are being
changed in flight the first thing that
comes to mind is probably just
encrypting the page if you switch to
https then your page won't be modified
in flight and some of these problems
will
go away but you're probably also
realizing that that's not a practical
solution in all cases you can't tell
every webpage on the internet to go
switch over to https overnight just to
deal with some of these changes and it's
all so rigid in the sense that you may
be disallowing some checks that you
might want you might want some of those
blue coat web filter checks you might
want to support some transparent caching
in the network which would go away with
encryption and you might just want to
avoid some of the costs involved in
terms of the actor latency and the extra
CPU overhead on the server so we offer
another option which is less secure but
can help you deal with the fact that the
pages are being changed in flight and
that's that you can use a web tripwire
like we used in our study to detect when
these changes are being made to your own
page and again this is just a piece of
JavaScript code you can add to your page
that runs in the background and it's
very easy to deploy because it doesn't
interfere with the normal execution of
the site it's just a piece of JavaScript
code that does this check and reports
back to server and find something
different now we've built this in the
form of a configurable toolkit that
makes it very easy to deploy on a normal
web page we've also built a service
version that allows other websites to
include a trip wire from our server and
not have to have any code on their site
so then we can report to them if anyone
has changed their site in flight and in
that sense it's sort of similar to what
Google Analytics does for normal website
usage where users you know visitors
might come to a website and then Google
Analytics records how they visit the
site it could also report how if that
Paige was be modified in flight now the
downside of this is that this is not a
cryptographically secure approach so if
an adversary we're in the network
modifying the page in flight and they
knew about the web tripwire then they
could disable it but we can try to make
that difficult for them and practice by
using code obfuscation techniques and
other things to make it hard to on the
fly tell if there's a trip wire
how it's working and disabled it's that
it's not reporting any changes so to
give you a better sense of that I'll
walk through some of the trade-offs
between using HTTPS and web trip wires
so HTTPS is really designed to ensure
integrity and confidentiality and it
will prevent all in flight changes to a
page so that will include some of these
detrimental changes but it may also
prevent some useful services you might
want for your page web trip wires won't
prevent changes but they will allow you
to detect and analyze and possibly react
by notifying user or going after the
people that are changing the page if an
in-flight page change occurs in terms of
strength https is cryptographically
robust whereas web tripwires may face an
arms race if ISPs want to go out of
their way to try and disable them but we
can make that difficult in practice by
using code obfuscation and finally in
terms of cost HTTPS can be quite
expensive in terms of getting a
certificate terms of the computation and
the extra round trip times for setting
up the SSL handshake whereas web
tripwires can run in the background so
they're very inexpensive to deploy ms
just a piece of JavaScript so we've done
some performance measurements just to
show that and that we've built a web
page and compared that to switching to
HTTPS and switching to web tripwires and
in terms of latency we don't affect the
start latency of the page at all the
page will start to render at the same
time and the end latency is affected by
a small amount as the trip wire runs but
again that's just in the background
versus HTTPS which adds a large overhead
to the start latency of the page and in
terms of throughput the extra overhead
for the server for web tripwires is just
serving these extra bytes for the trip
wire code and the known good
representation of the page versus HTTPS
which has to do a lot more CPU overhead
for SSL now in these experiments we were
not using any crypto hardware to
accelerate the HTTPS and that could
improve the situation for servers but
that would be another cost that you have
to pay
okay so an overview of this part of the
top we've seen that web pages are being
modified in flight for some subset of
users about 1% of the people who visited
our page and that these have had real
negative impact both from the publishers
perspective in terms of injected ads or
annoyances and in terms of security
vulnerabilities and so there's real
incentive to know when this is happening
to your page and we've seen that these
page rewriters can have very dangerous
power and that you should really put a
lot of security analysis before you use
them finally we've seen that web trip
wires can be used to help publishers
react to these changes if HTTPS is not a
viable option okay so now let's take a
step back and let's address one of the
threat tight I talked about at the
beginning of the talk and we have some
ongoing research at u-dub to look at how
we might be able to improve this
situation by modifying the web browser
itself to try and create a safer
environment for running active code on
the way and this is joint work with my
advisors Steve Gribble and Hank levy
okay so how do we get to the situation
where we have so many threats from
active code and part of it is that web
content has really evolved significantly
since the web's early days where the web
used to just be effectively static
documents with HTML and images and maybe
a little bit of script code and the web
browser could serve primarily as a
document renderer that would just show
you web pages and let you navigate
between them but things have changed
significantly since then now there are
many websites that have significant
amounts of JavaScript code and other
types of active content flash Java
Silverlight and so on that are
effectively forming programs running
within the web browser and these
programs need to be isolated from each
other and the browser has to act like a
runtime environment for these programs
in that sense the browser's really
become analogous to a small operating
system that has to run these programs in
a robust way and unfortunately today's
browsers are not architected like
operating systems they're still built
like document renders
and that's not providing us a good
environment for running these programs
so that has led us to a lot of these
threats and there's many more threats
than just these in flight page changes
we've talked about the fact that we have
browser exploits we have cross-site
scripting attacks we've crossed out
request forgery attacks and even other
types of interference between web pages
within the browser and we want to be
able to support a safer environment for
running these programs so we really need
to be able to support the fact we are
running web programs and to get there we
need to have better ways to define what
a web program is and how the web
browsers are architected to support
these programs so we've bought this down
to what we think are a set of four
architectural principles that need to be
improved in today's web browsers and we
presented this at the hot nets workshop
last fall and I'll walk through each of
these before concluding so first we need
to have better ways of defining the way
web programs are defined are specified
on websites and another supporter within
the browser we need better ways of
preventing unwanted code from running
within a web program we need to be able
to isolate these web programs from each
other within the browser itself and we
need to be able to apply uniform
policies on what a web program can or
can't do regardless of what content type
is within the program so the first step
is identifying the boundaries between
programs and on today's web this is a
little tricky because a web program is
not just a web page obviously it's a set
of pages that can communicate with each
other in the browser and today's
browsers use the same origin policy to
say when pages can or can't communicate
with each other so if two pages come
from different origins then they're
treated as isolated in the browser and
they can't talk to each other but we
feel that in the long run this is a
pretty fundamentally flawed policy that
it's not what you it's not what would
match your expectations of what belongs
in a program or not so for example in
some cases it's too narrow you may want
to be able to include code from
different origins in the same web
program by creating a mashup say between
content on myspace
and content on google maps now current
browsers make it awkward to do that
safely because those pages come from
different origins the same time it may
be too broad then you may have lots of
independent web programs all served from
the same origin like lots of user
profiles on myspace but because those
are all from the same origin they're all
getting lumped in the same box at the
same time it's not a very secure
boundary because it can be easily
compromised and you may have heard of
DNS for binding attacks coming back into
the spotlight recently where origin is
really just a name that's loosely bound
to an IP address and this means that you
can go to a web page and your site like
evil calm and the first time you asked
their dns server what their IP address
is it says oh I'm this internet address
which is their real server and then that
page asks for a frame later on and when
you go back to evil calm and ask their
dns server what's your IP address they
report us an address that's on your
local area network instead and so now
your browser thinks that these two pages
one from the internet and one from your
local area network are part of the same
origin and lets them to talk to each
other so it allows an external site to
spy on the contents of your internal
network so there's many reasons why we
might want to move to something beyond
the same origin policy within the
browser itself we think we need support
for this new abstraction this idea of a
web program and this is something again
current browsers just don't know what a
web program is we're trying to infer it
from the same origin policy and it's
just very awkward and beyond that we
also need an abstraction for an instance
of a web program running within the
browser because you could have multiple
instances of the same program and you
might not want to prevent that from from
supporting that case now the important
thing here is that we need ways to sign
exactly which resources belong to which
web program so this is which code which
documents which images in which data are
allowed to be part of one web program
and not another so how can you define
those boundaries now this is still an
open question but we've started look
at new ways that you could define these
boundaries and one way you could do it
is by using keys to define the program
boundaries so if you have a key pair the
author could have a private key that is
effectively a secret that identifies
your web program and then the public key
is served to the client so to the client
a web program becomes that public key in
a set of documents and resources that
are signed by the private key so the
browser now knows which resources belong
to which web program and when to
distinguish them now maybe you might not
need to do this in all cases but in
certain applications where you really
want strong boundaries between your web
program and other web programs it might
make sense to support this as an option
and some of the nice properties or that
a single web server can now host
multiple web programs that are
independent of each other just by
generating multiple keys multiple
servers could now host the same web
program by sharing that private key with
other servers that trust and there's no
pki required per se because we're not
trying to say who's providing these web
programs just that this web program is
separate from another web program now if
you wanted to go that extra step and
provide and say who was providing the
web program you could do that just as we
do with ssl certificates so this is one
approach that we're starting to look at
to see could we do something better than
same origin all right so the second step
is preventing unwanted code in the
browser and this is familiar again from
cross-site scripting attacks where an
attacker has the ability to place attack
place their own script code on someone
else's web page say by putting something
in the URL and having that reflected
back to the user now we've also seen
that even if you prevent all cross-site
scripting attacks that may not help in
practice because someone may change the
page in flight and inject script code in
fact ninety percent of the changes we
saw in our measurement study we're
injecting script code into our page most
of the inflight changes were script code
so this is saying that web publishers
really don't have control over what code
is running on their web page if the page
is being modified in flight or if
cross-site scripting attacks occur and
so that's really a big concern if you
don't have control over your program
then how can you say what it's going to
do or not do and so from that
perspective we really need ways to
authorize what code is allowed to run in
your web program and one approach that
we're looking at to do this is using
what are called script whitelists and
there was a project from University of
Maryland an AT&amp;amp;T called beep and another
project that we've done concurrently at
the University of Washington it looks at
defining a set of hashes on a web page
of every piece of JavaScript code that
you want allowed to run there and if
some piece of JavaScript code appears on
the page that is not in the whitelist
say by a cross-site scripting attack or
bias in flight page change then the
browser should know to ignore that so it
has a white list of exactly what causes
the law to run now if we take this a
step farther and say that a program is
defined by a key pair you could then
sign that white list and say make sure
that in flight page changes couldn't add
things to the white list on the fly and
a white list could also be more
difficult to to modify than some of the
other logic in terms of cross-site
scripting attacks we also want to
generalize this beyond just JavaScript
code to other types of active content as
well like flash and Silverlight because
you don't want somebody who inject a
flash thing which might be just as bad
as JavaScript now we have seen that
there's some challenges here for dynamic
pages if your page is generated on the
fly in particular if JavaScript snippet
on your page or generated on the fly you
might not know everything that belongs
in white list in advance and so we've
looked at some techniques that can help
with that too in some cases maybe by
signing the whitelist or signing the
pieces of JavaScript code to show that
they're approved so okay so now we need
to have ways of isolating these programs
from each other in the browser itself
and that is even if these web programs
even if we solve the first two programs
problems of defining what the program
boundaries are and what's allowed to run
in the program you need to prevent other
programs from interfering with your own
web program and so cross-site request
forgery attacks are one example of this
where if you're logged into Google and
the green page
some other page can forge your quests to
Google server as if it was coming from
the client we need to be able to prevent
that and we also need to prevent things
like resource contention between pages
because one page actively using the cpu
or one page crashing the browser
shouldn't make you lose your session
when you're using other pages and so we
can get there by providing this better
isolation between program boundaries we
can try to isolate the credentials
between websites by having a notion of
different instances of a web program so
if you're logged into Google in one
instance of a web program and another
site tries to send a request then you
can isolate the credentials between
those two different instances and you
could take this a step farther so you
want to support today's persistent
cookies and session cookies but maybe
you could have an option of a certain
type of cookie that doesn't persist
across instances of the page so these
requests will fail you can also look at
how the browser's architected and try to
put these different web program
instances in different processes because
os's already do this very well for
existing programs so that if one program
crashed is it doesn't affect other
programs we have a tech report online
that shows how you could do this in
current web browsers just by putting
content in different pages based on
where it's coming from okay so finally
you want ways to apply uniform policies
to what web programs can and can't do
regardless of what type of content they
have and here if you have just a stock
web browser and a web page with some
HTML and JavaScript code you probably
have a fairly good idea about what that
page can or can't do to your computer
just based on the browser security model
but as soon as you start installing
things like java or flash or Silverlight
all bets are effectively off because
those plugins all have their own
security model that's independent of the
browser and not only might those have
bugs in them but they also have subtle
differences between them so when you
start composing these different
environments with different security
policies web pages can do things that
they couldn't do before and an example
of this came up with DNS or binding
attacks
in a recent Stanford paper they showed
that a web page could make a DNS
rebounding attack more effective if they
combined JavaScript code in Java code
because each of those had separate DNS
pinning policies so that one a request
from JavaScript would make one request
and asked what the IP address was and
the Java code would make a separate
requests and then you became vulnerable
because you were composing these content
types and unfortunately the same thing
is true with browser extensions if you
install things into Firefox like grease
monkey or adblock you can also be
subvert in the browser security model
and in fact grease monkey had a security
vulnerability in 2005 that allowed web
pages to effectively subvert the same
origin policy so at that time there were
no boundaries between web pages and it
was because of a pretty subtle thing
grease monkey was doing and they had to
go out of their way to fix that
vulnerability so when you start
composing these environments that can
violate the browser security policy
policies you really can't reason about
what a web program can or can't do to
your computer or to other web programs
now we took a first cut at this with the
browser shield project that I did at
Microsoft Research two years ago we were
effectively trying to build an
interposition layer for web pages by
rewriting JavaScript code and our goal
here was to prevent exploits of known
vulnerabilities in the browser so if we
know the browser is vulnerable to x y&amp;amp;z
vulnerabilities we could look at a web
page and say is it trying to exploit
those but we might not know if it's
going to do that after the fact using
javascript code so we need to interpose
on everything that JavaScript code does
to make sure it doesn't exploit those
vulnerabilities now that gave us a nice
policy framework for saying OK
JavaScript code can do this but not do
this we could use it for any type of
policy now the way we built it was
actually as an in-flight page change so
this might be one of those security
checks that you might want to allow this
was nice from a deployment perspective
because it meant we didn't have to
modify browsers at all we could just
deploy at a proxy or deploy it on the
server and say that anything that
decline
visits or any client to visit a site get
this protection for free but on the
downside there were a lot of challenges
to this some obvious ones are that we
couldn't handle HTTPS pages from the
proxies perspective because I was
encrypted it was hard to protect other
types of active content that might not
have been as easy to rewrite as
javascript javascript wasn't easy to
begin with and importantly it was hard
to handle things like browser quirks
where one browser might handle malformed
input in one way and another rouser
might handle it another way and we'd
have to predict exactly what a browser
would do on a given piece of HTML to be
able to enforce a policy decision on
them so this has led me to my current
line of research which is looking at
building one of these interposition
layers within the browser itself so you
can say exactly what web content can and
can't do two critical browser resources
like the Dom regardless of what content
format it is and we want to be able to
enforce policies on these different
types of contents um now we might want
that not just for the Dom but for other
resources as well like file system in
the network so if we could get plugins
like flash and like Silverlight to
effectively treat the browser as the
system call API instead of the real
operating system at least for certain
key types of resources then the browser
could enforce policies on those to sit
on those calls and we could then have a
better step of reasoning about what a
web program can a cat do even if it has
plugin content okay so that wraps up the
browser architecture part of the talk
and overall we've seen that there are a
lot of threats on today's web web
in-flight page changes are one of those
and web tripwires can help defend again
or help detect when those are happening
if not prevent them from happening in
general but beyond this in the long term
we feel we do need new browser
architectures to make it safer to run
all this active code and handle these
threats and we think that's going to
involve having better ways to define
program boundaries authorizing what
codes allowed to run within a web
program isolating these web programs
from each other and then of enforcing
uniform policies on what these programs
cannot catch
so thanks and happy to take questions I
didn't even process was there anything
in that page that goes
the trigger on occasion
so the question was do we have anything
on our page that was intended to trigger
modification by I speed we didn't have
specific things that that tried to draw
their attention but we did try to make
the page look realistic from a
commercial perspective so we had text we
had links we had some meta tags in the
page we had some some certain keywords
that might draw their attention an
interesting thing is we also heard that
some of these ISPs we're targeting just
calm pages and so we actually served
many frames on our test page from
different top-level domains we had a
dot-com page net org dot edu and so on
to see if these were being modified and
in our results we saw that in most cases
it affected all of the frames from any
top oval domain there are some cases
where it was just calm and there were
some cases where we couldn't determine
what the policy was but it was some
top-level domains and not others
anything else yes the interest is our
microphone hey Brad oh sorry okay so for
the cypher browser architecture we have
a flagged a problem right because if you
want to put program boundaries these are
just Safety reps that someone can remove
in flight yeah so in flight changes will
matter here and there may be cases where
you want to have the equivalent
protection for HTTPS now if your if your
web program is encrypted with HTTPS you
have your servers you have with
certificate that the browser can
identify and it will be difficult for an
in-flight and in the middle agent to
modify that without changing the
server's own certificate now in the
general case yes in flight agents may be
able to modify these these programs in
flight what we view is it's not likely
to be in the best interest for isps and
other parties to make real malicious
changes to content and flight that it's
more likely they're going to be doing
things to try and make revenue off it or
do analysis and maybe that's bad from
a code analysis are ya functionality
perspective you might break something on
the page but it's not likely to
introduce malicious things it's it's
much harder for an adversary to actually
get on that path between the end user in
that server um the one thing that
completely puzzled me in your talk was
the authorized code section where you
talk about the code being authorized by
virtue of white lists where do you
imagine these whitelists come from yes
so the idea is this the web of web
publisher would provide a whitelist for
their own web page saying here are the
scripts that I expect to be there and
this is really because there's no one
else who's qualified to say what scripts
belong on a page and what don't this
means that the publisher does have to
know in advance what all those are and
the publisher would have to take steps
to make sure that no one else could
modify that white list but we do think
the publisher is the one responsible for
saying what code is authorized
if I said yeah with regard to what you
said about the cross-site request
forgery if I understood correctly you
were proposing that when page from
origin a makes a cross-site request to
origin be that the browser not
implicitly provide the user's
authorizing information regarding origin
B is that correct that is one approach
you could take that would likely break
some of today's sites the way that they
use cookies and things like remember me
or the fact that you're logged in you
could try to introduce this as either
another type of cookie or certain types
of cookies don't get sent on these
cross-site requests okay so there's the
reason i ask is there's currently
controversy with regard to future web
standards with regard to new forms of
cross origin request yes some of those
proposals for example the cross origin
xmlhttprequest carries authorizing
information and proposes that servers
have yet and yet another akal mechanism
for approving them and then there's for
example the cross origin jason request
that whose rationale is exactly that it
does not carry any implicit authorizing
information can you say something about
how your work on the current ex
cross-site request forgery issues bears
on evaluating these proposals for
extensions so i think that based on how
we've started to look at how sites are
using these cookies this is still
ongoing work that there are some types
of of cookies that you do want to send
on these cross-site request in general i
think in terms of these proposals where
you're actually having one website
request data from some other website
that i would tend to lean towards the
not providing authentication credentials
in those cases because then that just
gives websites another tool that they
could try to learn things that they're
not allowed to learn
and that you should really use another
mechanism if you want to allow those
credentials to pass I guess so I was
just curious um you you can't seem to
have started the assumption that the
in-flight modification of the pages is
bad and I mostly agree I was just
curious because the thought struck me
that you made the point that one of the
things doing in flight modifications is
free ad based internet so if you just
stopped it you'd also be stopping that
as a business model and they just
wouldn't be ad based internet and what
are your thoughts on that that's right
so I think that there may certainly be
other ways to support an ad-supported
internet free wireless networks that
don't depend on modifying the contents
of the page itself there there may be
things you could do in the longer run of
having at least frames that are not
modified and then showing ads in a
different part of the browser that
aren't modifying the contents of the
page itself so they're still isolated or
you could try to support it through
other forms of AD display that are
outside the browser window itself the
the content part of the window I don't
have exact solutions to propose here but
I think that the idea of injecting
content into someone else's web page has
some pretty serious consequences and
that's probably not the best way to do
okay so thank you again</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>