<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>IM2GPS: estimating geographic information from a single image | Coder Coacher - Coaching Coders</title><meta content="IM2GPS: estimating geographic information from a single image - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>IM2GPS: estimating geographic information from a single image</b></h2><h5 class="post__date">2008-08-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dgif39IKT9A" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">i'm james hayes I'm from Carnegie Mellon
University my advisor is Alexia froze
and he was actually here almost exactly
a year ago today talking about some of
our work that our group does and I'm
going to talk about more this is so this
is all work that I've done with with all
osha and also tomash Mellish ebbets
who's here so before I talk about I am
to GPS I'm going to take a quick
backtrack to some related work of our
own of which we presented last year at
siggraph on image completion so if you
have some photograph and you want to
let's say get rid of part of it because
you don't like it um or because that
data was missing from however you image
this you want to be able to complete
this and not have anybody realized that
there was data missing that you
manipulated the image there's a lot of
nice previous work on this it can be
summarized into two main areas diffusion
based methods which are going to kind of
propagate colors from the boundaries in
words and maybe try and follow straight
lines they work nice for very thin lines
but for any large hole there you don't
preserve texture so they don't really
stand a chance then there's texture
synthesis methods and okay these can
preserve texture they're explicitly
trying to preserve the textures that
they see in the rest of the image but
since they have no idea how to
reassemble that texture in general
scenes it's kind of hopeless so what's
so hard about filling this hole well
like like a lot of computer graphics
these days actually we're limited and
how we can edit an image by how well we
can understand the image and in the
limit you have to have like a great deal
of image understanding here you'd have
to really parse the image and recognize
that this part of the road and that it's
a plane tilted of this angle and
illuminated thusly and then rear end or
something appropriate and this would all
be very very hard so our idea was kind
of to try and find some other image that
had a similar semantic skeleton some
similar scene that was so close you know
with a building like here and sky and
ground so close that we could just steal
whatever content that image had in a
corresponding point and you know of
course it's a big question whether or
not you can find scenes similar enough
to permit this operation
and the result the of this work that's
pretty cool is that a lot of times you
can so here's a result for this image
and the algorithm is pretty simple for
an input image which would be an
incomplete image you're going to build
some relatively compact scene descriptor
this is the gist descriptor that Antonio
Torralba and odo libo developed it's
just the basically a measure of how much
edge in or the energy there is at
different frequencies and different
orientations bend into a very low
spatial resolution say four by four and
then we also have a tiny color image and
we're going to build those same scene
descriptors for a big database of
unlabeled images well not big by Google
standards I guess and then find some
nearest neighbors and ideally these will
be similar scenes and in this case they
look pretty similar and then for each of
them will try and do some local
alignment and then produce 20 image
completions and let the user choose
among them so it 20 because the scene
matching in this project wasn't that
reliable and because image completion is
kind of I mean it's an under constraint
problem there's a lot of reasonable
solutions it's kind of up to the user
decide what they want so in this case
I'm going to overlay it with one of the
scene matches at the best alignment that
it found and so here it is before any
blending operations so I mean obviously
you still see the seam there then after
a graph got cut and puts on blending it
becomes pretty seamless because the
scenes were already so close together so
I guess the takeaway message from this
work is that you would counter scenes
that are similar enough Oh similar
enough that they permit photo realistic
image editing operations I guess and
kind of a theme of that research and IM
to gps that i'm going to talk about is
that our belief at least is that
computer vision and graphics becomes
easier if you're in a space of similar
scenes so like this is the space of
general images let's say hugely diverse
and it's very hard to do computer vision
tasks you know to detect faces or well
okay maybe faces are pretty easy to
parse images when you have to deal
this type of variety but if you're
looking at a certain type of scene then
suddenly you will eliminate a lot of
variations in and viewpoint and scale
and illumination and orientation so it's
much easier to build discriminative
classifiers or to parse images so these
are just groups of similar scenes in
general may be extremely hard to define
a plate detector because they're kind of
texture less objects that appear very
different at different angles but if
you're in the space of similar scenes
then maybe it's not so hard and the
second point is that it's not that
difficult to get into a space of similar
scenes for most images it seems like you
are able to find semantically similar
neighbors and that's a lot about what
this next work is going to be about no I
am to GPS all right so if I show you
this image and I ask you where it was
taken which I will do where was this
taken yes so it's no Saddam in Paris and
people can say that because it's often
viewed and often photographed so we have
a lot of samples of this and it's
visually distinctive for the most part
it's a fairly unique looking Cathedral
and so this is a type of landmark
recognition and and actually there's
people in Google working on this as far
as I understand Hartman Devon talked
about landmark base geolocation system
at cvpr this year what about this type
of image if I ask you where this was
taken any guesses Israel okay anything
else hmm still Paris okay those are
reasonable guesses not San Francisco
maybe I think I don't I don't think
those are very good guesses I think most
people would say that this looks more
like Europe than the Americas or Asia
now for a computer to recognize this
unambiguously it would be difficult but
may be feasible maybe with the sparse
visual words based on sips some very
efficient matching and some geometric
verification and the right data like
Google Street View for the whole world
maybe this is unambiguous enough that
you could efficiently engineer that give
you the address of this it's a it's a
big problem maybe somebody's working on
it i don't know but humans will
recognize this in a more generic sense
they can't be expected to have seen that
before and they just kind of have to
associate it with similar scenes say
okay it looks kind of like you're up me
here's another image any guesses where
this is Virgin Islands okay good guess
anything else goes to eco good there's a
lot of other places this could be I
think it's even more generic than the
previous one that could be New Zealand
Philippines Thailand maybe even Hawaii
and for an image like this I think I
think even if you had google boat view
for the entire coastline of the world
and some efficient match sure that this
scene is too generic and to dynamic to
expect there to be any discriminative
features that would reliably you know
match you to the correct location I mean
you know not if the tide change is not a
4 hurricane hits so you kind of need to
reason about where this might be in a
generic sense just you know associate it
with similar types of scenes that you've
seen before and in general geo-locating
from single images is impossible right
there's uninformative images sometimes
adversary uninformative because they put
up an Eiffel Tower in Las Vegas say you
know things like this they're trying to
trick you so first we need a lot of data
so for this project we got six million
plus maybe six and a half million
geotagged images off of flickr
the scissors images annotated by the
users themselves they seem to be pretty
accurate I'd say that there are 99
percent accurate within a few kilometers
I'll scroll over to the next hemisphere
so the map is a it's a jet color map on
top of a world map so you can see that
there's peaks and population centers and
tourist destinations not surprisingly
and if we look at a random sample of
these photos this is the type of stuff
you see so it's very diverse some of it
is specifically recognizable as a
landmark like sagrada familia but most
of it you could kind of only reason
about in a generic sense and most of it
would be very challenging in fact to try
and say where these pictures were taken
so for any given query we're going to
look for similar images in this data set
and we need to decide what features
we're going to use to compare images so
we're going to start with several of
them so inspired by antonio to our
elbows work we're going to try tiny
images just the simplest thing you can
do right is make a low-resolution image
and compare images pixel wise so just 16
by 16 in this case another one is color
histograms and something used an image
retrieval literature a lot I'm a text on
histogram so this is a filter bank based
text on histogram with 512 entries so we
line features and what I mean by that is
we find straight lines and images stay
start with k any edge detection look for
straight line segments and then make
some histograms of the statistics of
those segments like which direction they
were pointing how long they were another
feature geometric context so this is the
work of Derek horn coin and in that
given a single image it's going to
assign a probability for any segment to
be ground sky or vertical and also some
other sub classes and then finally just
the same gist in color a small color
image that we used in the cigarette
paper that I just showed you and if we
find nearest neighbours using those
features in this data set this is the
type of result you get so that's the the
image I showed you before and here are a
lot of alleyways that are as you can see
very structurally similar similar colors
a semantically similar about the same
scale and actually geographically
similar to some degree if you look at
the text labels under all of those you
can see that they're pretty much all in
Europe if I plot those on a map this is
the first 500 nearest neighbors plotted
on a map actually and ok so there's
peaks and the population centers like
Rome and Paris in London but if you look
at the rest of the world there's almost
no confusion an image like this only
seems to come from Europe so it is
geographically informative to some
degree if we want to make explicit
guesses instead of just having this
probability map then we can look for
modes and in that distribution say with
just mean shift on the geographical
coordinates of all these matches and the
peaks come out in places like New
Barcelona and London this image was
taken in fact in Barcelona and that's
not where our biggest cluster of matches
is they're kind of in Germany it's kind
of so it wouldn't be our first guest in
this case so this might be a failure
case but it's still reasonable in the
things that it's getting confused by now
if we go back to the landmark image lots
of people photograph these so for a lot
of landmarks even though our feature set
is mostly global seen statistics and not
especially discriminative you can still
find landmarks pretty reliably and then
yeah the biggest cluster is going to be
in Paris and then if we look at this
image again there's more confusion there
scene matches all over the place
Philippines maldives palau mexico
thailand arkansas that one's maybe not
so good that most of them look pretty
reasonable the biggest cluster happens
to be in in Thailand which is the
correct one but it's it's not not bigger
by much of a margin so we got a little
luck either but again the confusions are
pretty reasonable if you look at where
the other clusters are warm tropical
coastlines here's another image so all
the scene matches are in the American
Southwest for the most part I mean
there's
other reasonable looking scenes
elsewhere but definitely definitely
hones in on the American Southwest which
is correct in this case even though it's
interesting I guess is that none of the
pictures none of the scene matches are
the same physical scene they're all just
you know geographically nearby scenes
that are correlated in some way color
and texture pretty discriminative in
this case doesn't need to be outdoor
images here's basketball court and tends
to peak at you know cities that have NBA
teams things like that picture of a
zebra in Africa so the scene matches
again not discriminate discriminative to
the level of zebra recognition but just
recognizing this type of grass texture
with foreground objects does tend to get
you a lot of matches in Africa and the
biggest cluster is in Kenya which is the
correct location in this case alright so
let's quantify the actual results so the
test set it was it's 230 images I showed
you a quarter of it before and this is
performance of our algorithm with
respect to database size so over here is
the full database 6.3 million images the
likelihood that the first nearest
neighbor seen match is within 200
kilometers of the query is about sixteen
percent so in absolute terms it's not
terribly impressive but but you saw the
data set it's pretty challenging as you
decimate the data set down to nothing
then performance goes to chance chance
works out to be one percent in our case
if you take two random geotag flickr
images there's a one-percent chance that
they'll be within 200 kilometers of each
other so yeah my feeling was that this
task was pretty hard and I wanted to
actually see how well humans do it just
out of my own curiosity so we've started
to run some little pilot user studies
asking humans to do this ask you give
them an imagery say where was it taken
they answer that by rotating this globe
around and putting the cross air on on
their guests and to help the humans out
I gave them a political map that has
mountain ranges on it and how
all the population centers on it the
humans might need more help still they
can say oh that's the forbidden palace
in Beijing and then kind of where is
Beijing I don't know but then again if
they don't know that then maybe you know
they really are failing the task they've
only associated this image with the text
label and not a location I don't really
know what would be fair but it seems to
me that humans are probably doing around
ten percent on this test set there's
going to be a huge variance here though
I mean if you give it to a child than
they're just not going to have the
experience with the world and know where
things are if you give it to an expert
and give them enough time then they're
going to do really well I don't know
what the limit is but maybe even fifty
percent or something on this test set if
you're you know forensic Lee accurate
you can leverage knowledge about geology
and vegetation and culture really well
most humans can't do that so they don't
do that well if we look at the
performance of individual features we
can see you know which ones do well so
the green lines are the percentage of
times that the first nearest neighbor
matched only with that feature is within
200 kilometers so with all features it's
sixteen percent just like on the
previous plot see individually the gist
descriptor does pretty well but some
other features do well the texture and
the and the color especially after
clustering it and even even these line
segments statistics and these ones
benefit from the clustering I think
because they're just weak or features so
they kind of need more matches to
reinforce each other so the tiny images
and the geometric context weren't very
useful so for all the results have shown
so far actually it's just these four
features that we've used it didn't
really help or hurt to include the other
two and here's real quick another
quantitative result where we can say
well how does performance change if we
allow the algorithm to make multiple
guesses and and here we can see that the
clustering really helps this is the
purple line that you know after say six
guesses than the median error of Hmong
the best of those guesses is within
within 500 kilometers so if you give it
a lot of guesses then you know one of
them is bound to be right
and again we're doing a lot better than
chance so all right here's another
result and this is pretty common the
scene matches look pretty reasonable to
us as an semantically they look kind of
similar that's arid highlands but
geographically they're not well
clustered in fact none of them
correspond to the correct location which
was Morocco in this case we have you
know clusters all over the world in
reasonable places but they're just
incorrect but because they are well
correlated with some Geographic
properties of the world that the
location of those we can still use this
geographic estimate to to get some more
knowledge about our query image so let's
say we have some secondary source of
geographically arranged data which is
just a map right that's all a map is
right some geographic data arranged on
the world so this is a map from NASA
which I've taken the derivative of so I
get global gradient elevation it's just
how hilly it is everywhere in the world
and you can see indonesia and the
himalayas and chili things like that so
if we sample that map at every location
that we thought this image might be
located at we just sample all those
locations on average let's say something
very simple then we arrive at an
explicit elevation estimate or he'll
enos estimate for this image right and
let's see I'll do that for the whole
test set and what I'm going to show here
is the top 10 to the top and bottom
results of the test set they're being
sorted by their estimated Hill eNOS and
the hill eNOS was estimated through
their geolocation or through their
estimated geolocation so it's very
reasonable the ones up top are extremely
mountainous including this one that we
couldn't accurately geo locate but we
could at least tell that it was a
mountain and then on the bottom or a lot
of cities actually because cities like
London and Paris are very flat regions
we can do the same thing for any
geographical data this is population
density and then we sort our test set by
popular
patient density the estimated population
density and you know the first one is
Hong Kong and then India and in London
and then at the bottom we have these
barren regions and again we didn't use
the actual known geo location of these
images for each image we estimated where
it might be in the world and use that to
index into some map here's another map
this is land cover classification from
NASA so for each of these classes we can
say you know which part which members of
the tests that are most likely to belong
to this class snow and ice you see these
glacier images on the top we kind of run
out of snow and ice images there's we're
going to test that's about 240 images
this is sorted by probability of being
forest and the first two here are
mountainous but there they are forested
there's no there was no mountain class
in that land cover classification map so
so that that's the correct class to put
those in Savannah you know probably only
a few images and the tests that really
correspond to Savannah strictly speaking
but but these are some of them that are
kind of more evocative of Savannah's
water we can I've just showed you
sorting so far we can have a
classification task I think I'll just go
through this quickly we can get a ground
truth a light pollution measure say so
this was a map of light pollution again
from NASA so for each member of our test
set we can assign it its ground truth
light pollution and then sort that set
and this is the first half and this is
the second half the yellow ones we've
split down the medium so these yellow
ones were saying our urban and the other
ones were saying our rural so this is
ground truth and then we can use our
geolocation estimates instead of the
ground truth geolocation to estimate the
amount of light pollution and this is
the resulting ROC curve it's a binary
classification task urban and verses
were also chances fifty percent but in
the best case
this best one has area under the curve
of point eight for a belief so it's not
bad here's the actual this is our
estimated light pollution now it's it's
sorted by estimated light pollution and
the red ones are mistakes so I think
these results actually look very
reasonable so if you look at the first
couple of mistakes those definitely look
like rural locations but according to
their actual GPS coordinate it was in a
city I mean you can't tell that first
park is in London so oh well I think a
human would probably rank them very much
like this as well and here are the
things that it thought or cities and
again you know I mean that definitely
looks like a city to me but for whatever
reason it didn't have enough light
pollution at its actual location all
right so I'm going to talk about some
failure cases now and at the same time
I'm going to use that to motivate
geolocation driven from local distance
function learning and so this what I'm
about to talk about is inspired by the
work of Andrea Frome at Google who is
sitting here and in collaboration with
tomash of Google but also of CMU who's
sitting here so let's this is one of the
results this was a member of our test
set and we estimated this to be its most
similar scenes okay so three of them are
pretty reasonable but a lot of them are
garbage and you've got to ask well how
many you know we've got to have more
pictures of the Sydney Opera House in
this in this data set it's hard to it's
hard to know but we can check no here's
here's a map showing the distribution so
it it peaks in between London and Paris
which basically means it was chance if
we look only at the scenes only at the
images that were actually from this
geographic area and then try and find
the most visually similar images this is
the set we get so we did have a lot more
sydney opera houses in our data set the
scene matching was just not doing a very
good job and this is showing the
distribution of these
so you could say that these are now
let's consider these goal images this is
what we wanted our scene matching to
produce when we searched the whole
database and we'd like to coerce the
scene matching to produce these results
somehow and this is so we're going to
try and learn a new distance function
and the distance function we're learning
is a pretty simple form it's just going
to be a waiting on the elementary
distances a strictly positive waiting so
before I mentioned six elementary
distance functions now I'm going to have
20 because I've added some more sub
features and we're going to have an
optimization that weights each of these
different functions each of these
different distances so like the
distances between the five by five color
images or the distance between the color
histograms so the absolute numbers don't
matter they're only the relative sizes
do this is showing the relative waiting
of various features that we have and
they're still mostly kind of global
scene property features and this is what
the optimization learned for this
Australia example and then this is the
new scene matches using those distance
weightings applied to the whole data set
so so this is not this is not a
geolocation result because I've cheated
and I used here you can see it peaks in
Australia now obviously I've used the
I've used the actual location of this
image to help train the distance
functions so so really what this is is
an example of what the training result
looks like from this local distance
function learning so what we could do is
apply this little algorithm to every
single image in our data set for every
image in the data set we can say ok what
are your nearby visually similar images
all right I want you to you know return
those scenes and I don't want to look
like any of the other images from you
know hundreds of miles away and we can
run this rather large optimization on
all six million images but that's a bit
too it's a bit computationally
prohibitive so we need to find some more
clever way to do that but I'm going to
show you some more of these local
distance function learning results kind
of
the hope that they that when you've
learned one of these for every exemplar
in your data set that you're going to
get similar results for a novel query
which i think is fair you could say oh
you're just overfitting here because
this is training data but the distance
function fixed is that distance
functions we're learning don't really
lend themselves to overfitting too much
means just a positive combination of
other feature weights and those features
are pretty global scene properties so
here's this is the the Barcelona query I
showed you earlier and this is ok this
is the results that we saw earlier and
now if we find the limit ourselves to
our kind of geographic neighborhood and
then find the most visually similar
scenes well I mean that looks pretty
similar to me actually like a similar
set of alleyways and if we try to learn
a distance function which keeps these
closer to us than the rest of the images
we don't really it doesn't really work
we get almost the same results that we
got before so it wasn't able to it
wasn't able to improve this one you know
if if the nearby geographically nearby
images just aren't more similar along
any axes then then there's nothing you
can do and that's good I mean it
shouldn't it shouldn't over fit the data
and try and force it ok so again here's
here's another image here's its nearest
neighbors according to just the default
waiting's of the elementary distances
and ok bye partly by luck it speaking in
italy which is the correct location
although it's not actually matching to
any of the same physical location is
just finding other similar monuments in
italy which is reasonable i guess but in
this case if we look at the
geographically nearby matches we can see
oh that there were actually a lot of
other pictures in the day set of this
exact physical artifact not just these
towers but all of these images actually
are el Duomo in Florence they're all
just you know connected group of
buildings and they all have similar
patterns it's very strong linear
textures on them so if we learn some
since function for this example and then
apply it to the database as a whole then
you know now we are able to find you
know much better nearest neighbors and
here's another example default distances
it's kind of peeking in London which is
not very reasonable for that query
here's the geographically nearby and
visually similar images ok those look a
lot better to a human those seem like
they should have been the matches so
when you try and force them to be
similar it finds some waiting color
happens to be very very discriminative
in this case I guess the golden and
brown tones and now applying this again
to the whole data set here's our nearest
neighbor so now it's finding a lot of it
you know exact matches and just a
similar thing this is sacre coeur in
paris the initial matches are terrible
but if we try and force it to be like
the geographically similar images this
is the result of the learned distance
function so everything i've shown so far
is its kind of landmark base i'm just
showing you ok not as better on
landmarks we're kind of training it to
go from category level recognition to
instance level recognition maybe which
is nice but what why i'm really
interested in this work is to try and
just is the hope that by forcing things
by using this local distance function
learning that will improve seen matching
in general so i'm going to show more
results that maybe support that so
here's a query and here's its default
set of nearest neighbors and they're not
bad I mean it's nighttime and they're
mostly urban but not a not all of them
are skyline pictures there's a fair
amount of these concert pictures and you
know just not appropriate scenes so
here's the geographically nearby and
visually similar images so this is Hong
Kong and once we've limit it to Hong
Kong only now now the scene matches look
a lot better
and after the local distance function
learning all right so what we would have
liked to see is we would like to see
Hong Kong we have three matches now will
be the dominant match but you know it
couldn't quite manage that although now
there's more Hong Kong matches the nice
thing is though the confusions look more
reasonable now it's got all of these
other night time skyline pictures that
weren't there before so this is kind of
encouraged and here's the picture of
Thailand again and these are the matches
i already showed you in the order that i
already showed you if we limit ourselves
only to thailand photos here's the scene
matches and then this is the the weights
that the local distance function
learning chose and here's the new
neighbors for this query so we got more
pictures of Thailand and which is great
and we've kind of filtered out some of
the unreasonable ones I'm going to skip
ahead a little alright so another query
and these are the default nearest
neighbors and they look good at first
glance but if you look closer you
realize that a lot of them are not snow
this is sand sand sand Mexico it's sand
and Athens that's my it's just stone so
a lot of those are actually not snow
they're actually pretty bad scene
matches if you examine them more closely
so if we coerced it to instead match to
more geographically similar images and
these are similar images from Canada
then you know not only do we now do
better geolocation we have some images
of Canada but we've gotten rid of all of
these images of snow so it's now it's
actually matching you know I mean we've
gotten rid of the images of sand sorry
so now it's actually doing a better job
matching in a generic sense not just in
a specific geographic sense another
result these are the default nearest
neighbors for this query on the left
here's the geographically near similar
images
and here's the result after local
distance function learning so you'll
notice that initially a lot of the
confusions a lot of the not very good
matches we're kind of I don't know
tropical islands seen from the air
things like that but if we limit it you
know only two pictures of England and it
kind of learns that oh this shouldn't be
you know so much of a tropical island
this is probably more like things in the
air so you get a lot more sky pictures
this is my favorite result so this the
nearest neighbors for this query using
the default elementary distances the
default waiting which is just all them
equal are really terrible this first
scene matches may be reasonable because
it's at least an outdoor in a similar
climate but well most of the others are
just really bad their texture images
that are semantically unrelated so if we
limit it to similar images from
Afghanistan and India and Pakistan then
these are the the nearest neighbors just
markedly better I mean now they are
actually outdoor images in the same
climate of the right scale but so in
general this local distance function
learning has helped quite a bit if
you're nearby image is if you don't have
nearby images that actually look like
your query so in this case this is this
is the query in all cases actually and
it was from Libya here and it couldn't
find enough similar images in Libya so
it kind of expanded the radius to
include Greece and then even among those
images it just can't find anything
similar it's a very odd image it's very
overexposed and it's pretty uniquely
composed so can't find anything similar
so of course you can't learn a
meaningful distance function you can
improve your geo location or your scene
matching and here's another failure case
so this is for Maryland apparently
there's some kind of tank museum in
Maryland and it decided that this house
would be similar to a lot of Tanks
alright so in summary I think that using
this geographic labels that we have and
forcing images to be similar to their
Geographic neighbors is actually
improving the scene matching in general
which could improve a lot of other
computer vision and computer graphic
tasks that leverage seen matching but
what I've kind of showed you isn't real
results it's kind of initial training
results and I hope that I hope that
that's indicative that if we were to do
this on our entire six million image
database or bigger that that we would
get qualitatively similar results all
right if you guys have any questions
please let me know yep
well I mean not really I mean for each
query image it's picking out certain
features that happen to be discriminated
for that scene oh yeah of course so he's
asking if there was any consistency and
the distance functions learned for
different scenes and I was saying that
you know it not really different scenes
pick out different features or different
combinations of features which happened
to be discriminative in the case of that
scene or that class of scene it's never
one feature by itself that would never
really be discriminative it's always
some combination but sometimes one is
weighted much more than another do you
have did I answer that well okay
oh yeah I see what you're saying it's
just repeating them what if they're
visually similar and geographically
similar you'd like the waist to be
somewhat similar boat do you know if
they were right I guess I haven't
explored it well enough to answer that
my intuition is that yeah they would be
I mean that needs to be the case for
this whole thing to work because it in
this local distance function learning
what you're giving is you're giving
every example point in your data set a
decision process that it gets to execute
any time you have a query so exemplar
gets to decide whether in new query is
its is this neighbor or not and it has
to be the case that those that to
similar scenes would produce the same
type of distance function or else the
framework just won't work because you
know the exemplars the thing that the
exemplar aren't actually be appropriate
for the query so difficult to explain
yeah
well I mean you mean I don't quite
understand your question you mean
grouping them visually or geographically
okay no I haven't tried that no we
didn't the only clustering we did there
was kind of an after-the-fact after we
had our scene matches we tried to
cluster them geographically and the hope
that it would kind of throw out outliers
that you know the the largest mode of
the mean shift would be the correct
geographic location and you know it does
help performance a little bit but not
that much actually yeah so one of the
things that I thought hurts is just a
comment one of the things I thought was
really interesting was your sort of
implicitly leveraging like a prior on
what are popular things to take pictures
of right so you know there are a lot of
glaciers in the world but maybe there's
only a few that people actually like to
take pictures of so that exactly pretty
cool and so as far as your the question
is so as far as the distance the local
distance function like i don't know if i
missed but how would you use that like
in some application well i mean it so
ideally if we've learned better distance
functions for every exemplar in a
database all six million of them then
when we try to do geolocation estimation
then you know it's going to then the
results are going to qualitatively look
like you know the much better results
that I kept showing after the that the
local distance function learning so I
mean I don't know I think it could
almost double the performance of this
geolocation task but beyond the
geolocation task it appears that the
scene matches that come out after you've
learned a distance function
are more reasonable you know not just
geographically but according to all
other relevant semantic properties like
you know I don't know climate which
objects that likely it can contain what
the scale is so you know I'm hoping that
the scene matches will be better for
whatever other computer vision or
computer graphics tasks leverage seen
matching which right now might not be
that mini but I think there's going to
be more and more being published in the
near future yeah right well yeah so each
exemplars an exemplar in our data set
which it is after the local distance
function learning is done it now the
exemplars a decision process to decide
which things are its neighbors are not
based on the weights that it just
learned so when you get a new query you
know every single query every single
exemplar is going to you know execute
its decision process so this one's going
to decide was this new query a neighbor
to me or not it might say no I might say
yes so I mean that's how it works it's
it's kind of weird cuz you're twisting
things in Reverse you're letting the
individual exemplars decide the
neighbors with a different distance
function for each of them so when you
went to local dishes learning you're
implicitly trying to force out features
from the given image right the most
prominent ones have you tried the other
way where you let those features come
out of your feature vectors so instead
of assuming you have a fixed way of
computing distances if you come up with
some non linear squish-squash way of
coming computing distances which was
learned to be locally smooth mm tomash
maybe you'll answer this better or
andrea i haven't played with any of
these other ways of learning nearby you
know learning your scene matches better
but there there are a lot of really
clever ways that I think you could do it
and this is just one of them and this
yeah it's pretty it's pretty limited
we're only learning right scalar waits
for the existing distance functions
so what you suggest might be nice is
time of day or date of the year of
information available on these photos
and could use something like if it's an
outdoor scene it's likely to be during
the day or an or in the evening it's
likely to be inside to do some sort of
correlation well to answer the first
part technically yes every one of these
images has a time of day and a date
attached to it but they're generally
very inaccurate in the time stamps come
from a camera and the camera is being
used by a tourist from New York in Paris
so the time zone is incorrect or
ambiguous or they never set the date on
the camera to begin with people usually
don't correct this after they've
uploaded it that's just the exif data
from from the photo so you can't trust
the absolute time and date very well
maybe you could just what month it is
better maybe not because if the camera
doesn't have EXIF information than
instead flickr just fakes it just gives
it instead the time when it was uploaded
which is kind of unfortunate but yeah I
wish that data were more trustworthy so
that we could explore you know these
other things like you mentioned I mean
instead of just estimating geolocation
you should be estimating time as well
it's another axis and I'm sure there are
a lot of correlations there yeah so on a
similar line I guess one of the
interesting things about the problem
you're solving is that the input is just
a single image but one can think of
other domains where we might have more
meta information with that image for
instance if the images on a web page
then we'd have context around it of
course EXIF information is not always
reliable so it's wondering if you
explore other input parameters that
could help to find similar images ID on
the geographic axis of or whatever the
computer vision or graphics
abnegation might be no I can't say I
have but the only the only label source
the only data source I've actually
leveraged is that the GPS coordinates
although yeah there's a lot of a lot of
other cool ways to try and do it
anything else
all right oh oh yeah you know I know
your stuff is basically done with a
single image have you ever tried taking
a short series of images and saying okay
you know this one clusters in beijing
and hong kong in new york and in this
one clusters in hong kong in England
well Hong Kong is common and it was
series of thing yeah that's something
we're looking at right now so I said
that you can't trust the absolute time
stamps at all but maybe you can trust
the derivative of time stamps I mean it
too if the pictures were taken five
hours apart then maybe you can trust
that difference actually and if you have
a set of images and you know that they
were taken within certain amount of time
and you know what each of them cluster
individually then you know it's
reasonable to say okay could they have
actually traveled this far in this time
if not you know okay they must have only
been in Hong Kong so yeah it makes sense
that you could definitely do a lot
better if you're leveraging sets of
photos and I'd imagine with us
somebody's random pictures from Paris
for a day out you know any individual
one of them might be quite
undiscriminating to be a few in there
that are really discriminative and I
think with the set of images you would
you would you do really well on this
task
anything else
yeah then that case let's thank James
his governess one more demonstration of
what large data I can do so thank you
for coming over and going to the nice
talk</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>