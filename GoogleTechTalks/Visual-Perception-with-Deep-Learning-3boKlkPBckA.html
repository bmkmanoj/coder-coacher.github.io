<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Visual Perception with Deep Learning | Coder Coacher - Coaching Coders</title><meta content="Visual Perception with Deep Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Visual Perception with Deep Learning</b></h2><h5 class="post__date">2008-04-10</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/3boKlkPBckA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a great pleasure to introduce young
McCune yawns a professor in the current
Institute at NYU which is the joint math
and computer science department there
yawns done a lot of great work in
learning and vision he did his PhD in
France and then was in Jeff Hinton's
group in Toronto and then went to AT&amp;amp;T
where he was for a number of years he
ran a department there which did a lot
of very practical computer vision
including some OCR and check reading and
the DejaVu document compression system
which a lot of you have probably used
yawns ah a great proponent of kind of
not forgetting what all got us
interested in AI in the beginning and
thinking about kind of hard vision
problems and how we can really build
representations that scale up to really
recognizing objects and stuff like that
so I think that's a great motivation to
keep us all on track yan is someone who
was almost involved with Google at
various points in the past but it never
quite happened but it's great to have
him have him visit so he told me a great
story yesterday which is that he you
know accidentally forgot to answer an
email from Larry Page in 2001 which I
think it's a distinction that falls to
very few people these days so anyway
today's going to talk to us about visual
perception with deep learning thank you
Sam all right so it's really pressure to
be here last time I was here was I guess
in the early 2000 to place was very
different back then but some of the same
people are here I can't recognize me um
Peter was here
okay so I'm going to talk about computer
vision machine learning and sort of bit
on a large scale and also on about
robotics to some extent so one of the
one of the problems I've been sort of
obsessed with for for quite a long time
is it's sort of how do we learn
representations so I'm not necessarily
particularly interested in vision per se
I'm interested in so complicated
learning and the reason why I'm sort of
doing vision and robotics is because I
think it's one of the most challenging
problems for learning so if we can solve
computer vision with machine learning I
think we can pretty much solve
everything
you know maybe not language maybe you
know Fernando will disagree with that
but but you know I I think it's sort of
very challenging because it's very
high-dimensional varies of complicated
so you know what tells you that you know
this digital airplane you know it's the
same whatever viewpoint you're looking
at at it from what background it's it's
in front of what lighting condition the
appearance of it will change a lot and
what what the good learning system
should do is is sort of develop internal
representations for those objects that
are invariant to those transformations
possible transformation of these objects
and somehow learn it by itself and you
know the brain does it pretty well and
there's considerable evidence that it's
actually learned it's not sort of built
by genetics or anything and you know we
ought to be able to reproduce this in
computers and so far we really haven't
been able to do it to do a good job at
it certainly not with we're learning so
you know one question is how do we learn
from just a few examples in just seeing
a few examples of objects so we can
probably readily identify those flying
contraptions as funny contraptions and
but it's kind of likely that many of you
have never seen you know those
particular airplanes before or I don't
know if you could call them airplanes
actually but you know so somehow you
sort of developed some some
representation of flying contraptions
that sort of match this particular
objects so there is evidence that human
recognition is a very very fast process
people have sort of measured you know
how long it takes for the signal to go
from so the retina to
the usual temporal cortex which is where
objects are encoded and it's about it's
less than 100 milliseconds so there's
basically no time for a sort of
complicated process it takes play to
take place
it's essentially a feed-forward process
at least for so recognizing everyday
objects not for so complex certain
complex visual tasks but but for sort of
recognizing you know a chair car things
that have a short word it goes extremely
quickly and what anatomy tells us is
that the visual cortex
you know until sort of this ite area
it's composed of roughly 10 layers of
stuff neurons so think about a neural
network with about ten layers now we
don't have any learning algorithms that
can train a neural network with ten
layers
we're so traditional learning learning
algorithms and so one of the things I'm
going to try to tell you about this sort
of new new sort of categories of running
algorithms that to some extent can
address that problem ok so here is a
sort of traditional way of doing
computer vision you start with an image
you build by hand with yours or
experience some sort of way of pre
processing this image in a
representation that takes care of
eliminating the irrelevant variabilities
of this object you to say illuminations
and things like this and then you plug
your favorite trainable classifier on
top of it and hope for the best
and this is a you know it's a method
that works pretty well in a lot of
instances but but but it's not going to
explain how you know we solve every
learning problem that we're facing with
so one problem in particular is that for
any every new test that you want to
solve you have to rebuild this
preprocessor essentially by hand and
that's not nice thing to do so I kind of
call this sort of the shadow
architecture shadow in the sense that
it's got you know sort of a fixed more
or less fixed reprocessing and then some
sort of you know adaptable layer on top
which is very simple but so it takes
care of classification so an example of
this is the popular kernel method
methods which essentially work by
you know you build a kernel function
which essentially sort of it's a
matching function that sort of measures
the similarity or dissimilarity between
your your current inputs and all the
samples you have in your training set
and then you take the those matching
scores compute a weighted sum of those
scores and that's the decision okay so
the only learning takes place in those
coefficients that's where that's where
those coefficients are subject to
learning they can adapt but that's the
only place where where learning takes
place and so essentially is glorified
template matching um you know if the
kernel function is something that
measures similarity let's just grow a
template matching it's kind of a little
notch above nearest neighbor
classification if the kernel is very
simple then it's like linear classifier
which is you know nothing we didn't know
about before so there's no way we're
going to solve sort of complex AI
problems like like natural language
computer vision speech with this kind of
model it's just not going to work it's
like you know climbing a ladder to sort
of try to reach the moon it's just not
going to work at least not with learning
you know might work by building it by
hand that there's big success in this
but you know I mean I'm not saying that
there is no success in sort of
engineering approaches to this but but
it's not how we're going to build sort
of a learning machine that solves a lot
of problems so so the alternative to
this is something that some of us for
the last few years have have come to
called deep learning and it's the idea
that you can sort of stack multiple
modules each of which is a trainable
module and by sort of having multiple
layers of those each of which has kind
of can implement complex nonlinear
functions you can sort of go from sort
of low-level representations to sort of
higher level internal representations
that maybe you can capture those
variations I was talking about so
perhaps as you go up the layers you
might be able to build representations
in which you know a view of this object
under any kind of viewpoint is
essentially the same the representation
is invariant to those transformations
and but of course you know how are we
going to do this so the first the first
thing you can ask you know from a
theoretical point of view is why would
we need deep architectures the theory
tells us that with the kernel methods I
was telling us about I will tell you
about before you can approximate a new
function you want so why would you need
anything else and of course the answer
to this is a problem with efficiency you
cannot represent there are a lot of
functions you cannot represent
efficiently with just a kernel represent
internal tag representation you need
essentially a very very large number of
template's with which to match your
input vectors to do a good job so for
example if you want to recognize this
with template matching you need you know
a view of this object like this like
this like this like this like that and
then if you want to recognize all the
type of planes you need templates for
those other types of airplanes if you
want different backgrounds you need
templates for different backgrounds
different eliminations
and pretty soon it explodes in a sort of
exponential way there is no way you can
solve the problem and that's the problem
you know every computer vision a person
knows about you that you have to get
around so there is some theoretical
evidence which are not going to get into
that if you have multiple layers you can
sort of more efficiently represent
certain types of complex functions and
for you know those of you in the
audience who sort of have engineering
backgrounds if you want to design a
circuit say boolean circuit for boolean
function you can always represent it as
a conjunctive normal form of this joint
a normal form that's basically a
two-layer implementation of that
function right any function can be
implemented with two layers but it might
require an exponential number of in
terms or as if you allow yourself to
have multiple you know logic layers then
you might end up being a lot smaller you
know so you want to implement a 32 bit
by 32 bit multiplier you're going to do
it with a with a two-layer DNF it's
going to be very large okay so you're
going to do it with you know addition
and carry propagation and things like
that which takes more layers so there
are functions that are more efficiently
represented with more layers and what is
true for logic circuits is also also
true for you know recognition functions
so basically a deep architecture trade
space for time that's essentially what
it does so this is kind of a joke that I
kind of borrowed with from from geoff
hinton
I think you have a talk here if you want
which is saw several attitude you can
you can you can have to approach the
problem of sort of building artificially
intelligent machines the first one is
defeatism you know since there is no
good sort of parameterize ation of the
set of functions i would sort of
implement artificial intelligent tasks
you know let's just parameterize a much
smaller set for each specific task to
careful engineering that's the approach
that most people have adopted for the
last 50 years this denial kernel
machines can approximate anything we
want and you know the learning theory it
tells us that you know we can learn
anything with this well we need anything
else but I kind of set a few things
against this argument before a third one
is optimism which I'm sort of placing
myself too squarely into just because
you can but not efficiently you can
approximate but just not efficiently
okay so this you know exponential number
of template's problem you can't do it
but it's just exponential okay so let's
see how far we can go with deep learning
in a kind of supervised way and this
isn't some relatively old work that I've
done but that some of you have seen but
um you know I want to show it here so
there are a few situations where you can
use sort of traditional supervised
learning algorithm like back propagation
which is sort of traditional you know
learning algorithm for neural nets to
Train deep architectures up to something
like six six or seven layers and is it's
if you build a particular structure into
the into the network okay so let's say
you want to do a face detection object
recognition
you know robotic vision something like
that you know that there is very strong
correlation between a brain pixels and
images and so you can sort of get
inspiration from the visual cortex and
say you know I would combine you know I
want to detect local features because
local features are kind of a good way of
representing objects so people have had
this idea for a long time
neuroscientists in the 60s
some people insert computation
computational neuroscience since the 70s
and I've been working on this since the
80s and the idea is basically to build a
neural network in such a way that each
unit in a particular layer only looks at
the local neighborhood in the previous
layer and you can build some sort of
shifting variance in that thing so
rather than going to the chest of this
let me show you an example so here's an
example of such a network here is a
input retina if you will where each
pixel is kind of you know represents
sort of a grayscale or something and
this is the first layer the first layer
is composed of multiple planes and which
we call feature Maps and each feature
map each pixel here is a result of a
linear combination of a local patch in
the previous in the input which is
repeated over the entire field so
essentially you can view the results you
can view one of those plane
as a result of applying a convolutional
filter to the entire input and then
passing the result through a sigmoid
non-linearity so it's like a it's like a
filter you apply to the entire image and
you pass it to a non-linearity but the
coefficients in that filters in the
endure those filters are going to be
learned right so each of those planes is
the output of a different filter the
coefficients which are learned
supervised and as we go up the layer we
repeat essentially the same operation
but we reduce the resolution every time
reason for reducing the resolution is to
kind of build a little bit of shift
invariance in it okay what distortion
invariant so if I shift the image on the
input you know as you go up the layers
the the shift is not as big because the
resolution is lower and so the
representation doesn't change as much
and so it's easier to take care of those
variations respect to shift rotation
scale etc so people sort of build
systems like this with you know some
simulated them since the 70s Fukushima
with a new acog Neutron so I've been
working on this since the 80s Tony
Puccio has a similar model equal to H H
max which it's been working on the last
few years as kind of a model of the
visual cortex Shimon Ullman is kind of
similar models but now sort of very
different learning algorithms but you
also use completely different learning
algorithms divi low so it's been working
on similar models it's part by Paju and
with this we can do all kinds of cool
things like object recognition phase
detection so let me show you a demo of
this actually just for phase detection
so let's see not this one
okay so this is a compilation on that
phase detector so I'm holding this EULA
webcam here I shouldn't put any myself
so I've not used myself to train the
system so unfortunately I don't know if
I can put it at fernado because it'll
too far you might be a little too too
small here in the picture yeah it's not
gonna see you because it so far but um
you know we can detect sort of any
number of phases essentially in real
time this is not a particularly
efficient implementation either so I'm
gonna be Pete Peter happy this is
actually implemented in this
oops and you went back to the first
slide okay so if we compare sort of
those kind of deep architectures with
sort of six seven layers or so with sort
of shadow one like support vector
machines on visual tasks it works much
much better it's faster it learns faster
it is got better accuracy so on things
like mes which is a handwriting
recognition task we can get down to
about 0.4%
with this whereas SVM's
have a hard time going below 0.7 roughly
and are much much much lower for object
recognition the difference is even
larger I'll show you a demo of this in a
minute so here is a task for object
recognition so we have five categories
animals humans airplanes trucks and cars
and for each category we have ten
objects we're going to use five objects
for training and five objects for test
and for each object we have lots of
different views of those objects under
different illuminations different
viewpoints and so if you use images like
this with sort of different eliminations
of viewpoints you train on those tests
on those you get on the order of six
percent error with those convolutional
nets and on the order of you know eleven
percent error or so with things like
super vector machines in the sort of the
best instance this is from raw pixels of
course you can always extract features
and do a better job but this is sort of
to test learning algorithms it's not
kind of to test vision if you go to
slightly more complex images so the same
kind of objects but placed on sort of
really complex backgrounds the
difference is even more striking so
support vector machine really doesn't do
a good job on this is so you know the
algorithms basically break down you get
something like 43 percent error which is
horrible with a commercial net you can
get down to about 8 percent and with a
few tricks you can get down to about 6
and and this runs you real trying to and
the learning also is much faster with
those commercial nets than what you can
get with SVM so let me show you a demo
of this again
okay so I haven't tested this here so
I'm sort of going on other on the limb
here okay so I'm holding this webcam
again here instance your airplane put it
on the ground and this is an airplane
let me try out something else perhaps
this at this little GI Joe guy that's a
human and that's an airplane ah let me
see if I have something else
maybe that that's a truck it's a truck
without the wheels no it says an animal
okay so that's a dimple syndrome okay so
you see so we can do a decent job at so
real-time object recognition it's
independent of viewpoints a fully
independent of lighting condition
relatively independent of background
although it doesn't do such a good job
and background it's really not a object
detector it's more of an object
recognizer so he wants to classify any
object it sees as one of those five
categories so if I point it at the crowd
here is going to find all kinds of weird
objects in it okay because it's really
not been trained to kind of reject
things as anything else and what it's
been trained on okay so this is not the
ultimate answer to computer vision right
it's not but it does sort of you know
viewpoint invariant lighting invariants
to some extent background invariant
object recognition it can deal with
small inclusions as well okay so what
we've established is that we can do a
relatively good job at sort of
classifying objects if we have enough
training sample oh let me show another
another one so this is another
application of supervised convolutional
nets that we apply to driving robots
so we basically build this little robot
guy out of you know radio-controlled
track parts and have it driven by
someone using remote control and the
person was instructed to basically drive
straight until an obstacle would be
roughly a meter and a half of two meters
away and then veer off okay and so we
recorded the video together with the
steering angles and then trained one of
those commercial Nets to basically map
the sort of the videos from the single
frame from frame pair from the two
cameras to the steering angle
provided by the human and after the
system is trained we just let it loose
and let it run by itself and I've got a
couple videos of that so
so this is weather or what looks like
when he drives itself around in this
sort of busy backyard here question
yeah it works slightly worse but not
hugely worse and so this is sort of from
the viewpoint of the of the robot so
here it's kind of seeing this car going
off to the right things its backhoe or
whatever bringing up to the left a
little bit and so negotiating its way
through this little narrow passage here
also it's completely reactive there is
no planning there's no you know there's
no tricks
basically we feed the raw video raw
video frames to the to the commercial
net and just let it loose it works
pretty well ok so this is what we can do
is going to supervise running with deep
architectures with essentially zero
pre-processing just raw pixels okay
and I know some people in the room has
been playing with this for other
applications okay the main problem with
this is that it needs a huge amount of
training samples so the you know we
build handwriting recognition systems
with this that require you know hundreds
or thousands of samples face detection
it needs something like fifty thousand
samples the robot is twenty minutes of
driving so it's several tens of
thousands of frames the the object
recognition is something like three
hundred thousand images individual
images so it's you know it's painful so
we can't use this to you know you know
let a robot loose and expect it to kind
of learn the tasks by itself you know it
would require too many labeled samples
so we needed something like more akin to
a biological learning which would be so
unsupervised so if we had a machine that
could serve run the features by itself
and then require only sort of a few
samples to sort of map those features or
impress you know in variant
representations to categories with a
very simple classifier though the ideal
and that's basically the idea behind
this sort of deep learning sort of
serving you a love interest behind those
sort of deep learning methods okay so
this is sort of more technical part of
the talk right not right now
so the basic idea of
deep architecture's is is the following
other way I view it is the following
basically we're going to build a sort of
hierarchy of modules okay so each of
those module is kind of a layer or stage
in the in the architecture and each of
those modules takes two inputs so it's
got really no no real add put to speak
of it takes two inputs one of which is
going to be the actual input things from
the camera let's say and the other one
is going to be feature representations
of those things and the output is a
single number which you can interpret as
a log-likelihood or energy which is
going to measure how well those two
things match okay so if you put the
image of a track and the sort of feature
vector for track those things will will
tell you happy and if you you know it
also things don't match he'll tell you
and happy okay and we're going to stack
multiple layers of those and the input
of one stage is going to be the output
of the previous stage so basically you
know if we find a feature here that sort
of matches that we're going to use that
as input to the next layer and he's
going to find a feature that matches its
own input okay
so the way we operate the machine is
that we put an input and then we search
over
feature vectors here for the one that
minimizes this this output negative a
likelihood or energy okay once we're
happy with this one we now search for
the value of this guy that minimizes
this output that we keep going okay and
we're going to train this is a with some
criterion that I'm going to talk about
in a minute so each of those boxes is
going to have trainable parameters which
I denote by W okay so let's focus on one
of those modules so you can view this in
sort of probabilistic framework as a
module like this giving you sort of a
probability over Y or density function
the probability density function of a Y
given the parameters inside of that box
as e to the minus some constant the
energy of that box divided by the sum
over all possible where the energy okay
so it's the traditional way of turning
energy functions and energy function
into a probability distribution using a
guess distribution and this energy we're
going to define as the minimum over Z of
an energy function that takes all of
those three variables into account okay
all those two variables into accounts
y&amp;amp;z okay so we have e of Y Z and W and
you have Y and W is just a minimum over
Z of E and E of Y Z and W all right so
again the process is we plug in Y fine
for the Z that minimizes the output and
that's a of y vyw and if you want a
probability density that's that's what
we do this is like a maximum posterior e
inference estimation of Z using a you
know a density model okay so how are we
going to train this the way we're going
to train this in sort of this
probabilistic framework we're going to
give high probabilities to think we
observe and low probability two things
we don't observe or the energy in energy
terms we're going to give low
probabilities to think we observe like
this and that and high probability to
everything else okay shape the energy
surface in the right place in the right
in the right way okay so the traditional
way of doing this is using sort of a
probabilistic approach you you say I
want to maximize the likelihood of the
data under the model okay so P of Y W so
basically when you observe a sample you
say I want to make the probability of
that sample is high under the model and
I want to make the probability of
everything else low and the fact that
the probably everything else is going to
be made low is because of this
denominator term which forces the needle
oops some of the probabilities to be one
essentially all right so take minus log
of this you have a nice function to
minimize let's negative low Lackey who
loss so this is the energy of a training
sample and this is the sum of I forgot
the log here there's a log here since
the sum this is a log of the sum of the
exponential of the energies of
everything else okay imagine do is a log
so you can do this with gradient descent
okay so and gradient descent will say
change the parameter vector in such a
way that the increment with which you're
going to change the parameter vector is
going to be the derivative of the energy
with respect to the parameter vector at
the training sample okay and then you
have this horrible contrastive term
which is the integral over all possible
input vectors of the derivative of the
energy with respect to the parameter
vector weighted by the probability under
the model of that particular input
vector okay listen you can't
cute because it's a sum over all
possible inputs if you have a high
dimensional input that's horrible you
can't compute this okay so basically
what what this will do is that it will
push down on the energy of the correct
of the current training sample and push
up on the energy of everything else and
the problem is that making the energy of
everything else large this will be
complicated because there's just too
many everything else's so that's what we
call an organization problem that the
machine learning community has been
struggling with for four years but if we
had a model like this would be really
cool we could do all kinds of different
things like say denoising you know you
know if you have a good model of sort
probably sick model of the world we
could predict everything that happens in
the world so you know if we had say a
model over images which would of course
be high dimensional of this we could say
you know here is you know clean images
we have low energy and noisy images
would have high energy so if we have a
noisy image we look for the lowest you
know the low energy image that's nearby
and we basically cleaned up the image
okay so there is a trick that Jeff
Fenton came up with a while back called
contrasted divergence and the idea is
not to pull up on the energy of
everything else but only pull up on the
energies of nearby points to the
training samples that happen to have low
energy okay so what you really want is
if the energy has that shape as I show
here and your training sample is is here
under this this green arrow you want to
make the energy of this flow and you
want the energy of that guy high because
that happens to be not a training sample
that has low energy it should have a
higher energy right so push up on this
guy and sort of reshape your energy
surface that way if you do this enough
times you know the energy surface is
going to take the right shape and
eventually have minima local minima
around training samples so this is nice
and and cool and I'm sure Jeff talked
about this when you came here and
there's other things where it works
really well but it's got a problem which
is that in high dimensions it doesn't
work so well the problem is that there
is just too many places to have to push
up yeah whose energy you have to pull up
dimensions is too high so if your energy
surface is very flexible
you know pushing up on all the areas
that need to be pushed up it's just just
hopeless you sort of never can get it to
adopt the right shape so you need some
sort of more wholesale way of pushing up
the energy of things you don't you know
you don't observe and there's a way to
do this for a particular type of
architecture that I call the
encoder/decoder architecture okay so
remember the the modules that we're
going to stack we're going to build this
we're going to build them in the
following way we'll build them in the
way where we start with an input run the
input through an encoder some sort of
parameterize function we're going to
learn we're going to get the feature
vector here we're going to run that for
a decoder and reconstruct the input from
that decoder and what we're going to
train this thing to do is basically
minimize the reconstruction error under
some constraints on the code and with
the right setting we don't have to pull
up on the energy of anything okay and so
if we are happy with this we can kind of
stack multiple layers of those to kind
of build those sort of deep learning
architectures okay so in fact the
architecture we're going to use is
something a little more general than
this it's something like that where here
is the input we run the inputs with the
encoder and the code here is again an
input it's not an output to anything
it's an input to every module this is
you see the arrows go into all the
modules okay so what we're going to do
is we're going to search for the value
of this that minimizes the overall
energy and the overall energy of that
module is the sum of three terms it's
the sum of the reconstruction energy so
it's the difference between what the
model wrinkles tracks from the code and
the original input plus another term
which is kind of a prediction energy so
this energy is the difference between
the code that we get here and what the
encoder predicted the code should be and
the third term is a something we call
this porosity
term it's a constraint on the code to
make sure that it doesn't have enough it
doesn't have too much information in it
and I'm going to say in a minute why we
need that so basically the way we
operate this is plug an input here look
for the value of this that minimizes the
sum of those three terms okay once you
have that turn tune the parameters of
the encoder and the decoder so that is
overall ug is
mised okay and we're going to put
several types of modules within those
encoders and decoders and obtains
different and supervised learning
algorithms so this is one particular
type so principal component analysis for
example it's kind of a special case of
this where you don't really have a
sparsity code you sparsity constraint to
make the code low dimensional and you
make the encoder and the decoder linear
okay so start with an input run to an
encoder decode it minimize
reconstruction error and train the
encoder and decoder so that the
reconstruction error is minimized
that's principal component analysis
restricted Boltzmann machine which is
Jeff interns favorite model is something
like this also I'm going to go into the
details of this but just efficient
suffices to say that you can sort of
view it in those terms as well and
there's a number of other algorithms
like k-means Passover complete basis of
thousand and field and business pursuit
that are basically versions of this
where you don't have an encoder okay so
these are situations where you plug an
input and you run a minimization
algorithm over Z to figure out which
value of Z minimizes the reconstruction
error but no one tells you what they
should be there is no predictor that
tells you what you should be okay so
those of you who know what business
pursuit is that's that's what it does so
business pursuit for example the cost is
a Euclidean distance between the
reconstruction and input vector the
decoder is a linear matrix so you
multiply the code vector by a linear
matrix and the sparsity constraint is an
l1 sparsity so it's the sum of the
absolute value of all the components of
the code okay so basically plug an input
vector find this this guy that minimizes
a reconstruction error under the
constraint that it has to have lots of
components that are essentially zero
okay and the training is you train the
the basis functions here so that the
reconstruction is minimized the main
problem with this model is that it's not
efficient if you only use it as a
feature extractor because you have to
run this optimization algorithm every
time you get a new input okay so what
we're proposing is something that
basically has a predictor so not the
architecture of the type that I talked
about before that is going to predict
what the
value of Z is that minimizes the
reconstruction error okay and the good
news about this is that once you've
trained the system you can basically get
rid of the decoder you just have to run
the encoder and that's the feed-forward
process they can go very fast okay
now here is the here is the main idea of
this of this talk them a new idea and
this talk basically which is that we're
going to put some constraints on the
code and the constraints on the code are
going to make sure that the energy of
everything else that are not training
sampled is high and the reason why that
happens is a little mysterious but I'm
going to tell you about this in a minute
okay so the main insight is is is the
following
actually let me show you an example of
this imagine that the imagine that this
code vector here is binary it can only
take two values okay it's just one bit
so if you can only take two values there
are only two possible values that the
output of the decoder is the output of
the decoder when the input is 1 and the
output decoder where the input is is a
zero I mean the Z when Z is 1 or Z 0
right what does that mean if the cost is
say Euclidean distance that means that
only two values of Y can have zero
reconstruction error ok the only values
of whether I can have the reconstruction
error are those two values for which the
decoder the two outputs of the decoder
for Z equals 0 for Z equal 1 okay for
every other value the reconstruction
error would be large ok so here's an
example where when you make the
information content of the code small
one bit that the energy surface only has
two values for where it waits low every
everywhere else is high you don't have
to push up on the energy of anything
it's automatically high because the
information content of the code is small
ok so it's kind of represented on this
real drawing here ok these are the two
values of of Y of the reconstruction for
you know is equals 0 and Z equal 1 and
if the reconstruction error is a
quadratic function then you get an
energy surface with that shape in the
space of Y imagining that Y is when
dimensional but if Y is
multi-dimensional it's just multiple
balls so there is a sort of general idea
that restricting the information content
of the code alleviate the need for
pushing up on the energy of everything
which is the thing that's really hard to
do in to solve the normalization problem
ok
so if we have information constraints on
the code then we can happily just
minimize the energy of the training
samples and just rely on the fact that
the code has low information content to
make sure that the energy of everything
else is high and that's basically the
idea we're going to use
okay so the question families asking is
do you have the situation where by
accident you would also lower the energy
of lots of other things without paying
attention so basically that's controlled
by the sparsity by the the sparsity
constraint so if you have very highly
sparse code where codes where the
information content is very small then
there's only a few configurations that
can have low energy but if you are a
little weak on that constraint then you
know a lot of areas can have low energy
and you run the risk of having lots of
spurious low energy area points as well
so it's how you control the sparsity
constraint and I'm going to say a few
words about this minute okay so here is
the algorithm okay so we have this
architecture so we have so we tried
different architectures I'm gonna go
into the details of the differences
between them these different ways of
making the code sparse so this way that
we used here and server earlier papers
of two or three years ago was by
basically having a bunch of sigmoid
functions with very very high threshold
and so once you run the code through
this it becomes parse just because you
know a lot of them get clobbered to zero
and so the algorithm is as follows you
plug an egg an X you find a Z that
minimizes the sum of in this case here
are two energy terms but in the general
case three energy terms including the
sparsity and once you have those you
adapt the parameters of the encoder to
minimize this error iPod adapter
parameters the decoder to minimize that
error and you're done okay do this over
train examples so let's say you train a
system like this with linear encoders
and decoders on just one slab of
sigmoids on
handwritten digits you get this so these
are the representation of the filters
that are learned by the system linear
filters are learned by the system to
reconstruct from the code okay and there
are parts there are digit parts like a
dictionary of atoms if you want so
essentially what this says is that
because the code is sparse what it says
is that you can reconstruct any digit
through a weighted sum of those basis
functions of where the coefficients are
almost all zero except for a small
number of them okay let's define our
parts
so for example this seven here can be
reconstructed as sum of nine of those
parts with sort of significant
coefficients see all the other ones are
essentially essentially zero if you
train it on natural images you get
filters like this that looked like you
know edge detectors and if you use this
to train the filters of a convolutional
net so the type of network that I showed
you earlier with which I did phase
detection and object recognition or
handwriting recognition we actually beat
the record on the handwriting
recognition benchmark goal in this with
you know a 60,000 training samples so
with if we include distortion we can get
down to 0.39% which is sort of not
really significantly better than what
the Microsoft guy got like Patricia
Martin I guess you participate in that
as well right so these guys got 0.4
percent and we get like point three nine
it's not really significant but it's
sort of slightly better it's very
similar very similar techniques and the
filters you get with the sort of
unsupervised filter training I can look
really much nicer than if you are purely
trained with supervised purely
supervised method using backprop the
filters are not nearly as clean let me
skip this okay all right so the one
method that we've developed very
recently is something we call the
predictable business pursuit so it's
basically business pursuit where the
decoder is linear the sparsity term is
l1 but the encoder is a number of
different nonlinear neural nets
basically okay so it's basically a
version of business pursuit where
instead of having to minimize the energy
function respect to the code to figure
out the features you can predict them
directly from the from the input using
the encoder okay and where the system
runs to produce our codes that you know
are not only can we construct to those
who are easily predicted by the encoder
so we use sort of various types of
encoders which I'm not going to go into
the details of we get very similar
results as the previous methods also
parts we train on a list we can use
those features as the input to a
classifier and measure as a function so
this is to answer your question as a
measure sparsity you know what
performance do we get so let's look at
this example so here we trained and
supervised this this feature extractor
and supervised with 200 features which
is very small and then we take those
features feed them to a linear
classifier that we trained supervised
with 100 sample 1000 samples from a list
ok and then we measure the test
performance on the NS data set and these
are the two curves so this is the error
you get on the training set and the air
you get on the test set and the x-axis
is basically how sparse you're making
the system so sparsity increases this
way ok so if the features are not sparse
enough you don't get good accuracy if
they're too sparse you don't get good
accuracy because there is not enough
information in features there is one
sparsity level I wish you get the the
best accuracy so it's kind of a
trade-off and we don't know how to set
it unfortunately other than by
cross-validation please
so this is comparing different way so
these are the kind of filters we get
using this algorithm on natural images
and it looked really beautiful and sort
of you know neuroscientist tell us they
look very much like filters you see in
the early area of the visual cortex in
the brain of mammals and we can train
lots and lots of filters to reconstruct
things so this is training 1,024 filters
12 or 12 filters and we get every filter
does something
there's no none of them dyes which is
kind of cool we skip this
second you need massive data you need
lots of data is only about data and you
have as much unable data as you as you
want
certainly here you have as much enable
data as you want right so but even we do
so I will just tell you study from you
guys so so that's really not an issue I
mean you can have as much on label data
as you I mean for images is really
knowledge so there's another variation
of this that we use to learn invariant
features which are not going to go into
the details of it basically comes down
to sort of separating the outputs of the
encoder into two parts one that tells
you which features are present and the
other one that tells you where they're
present and I don't have to go into the
details of this but I just want tell you
that we use this to Train so basically
what what you get from this are kind of
movable parts now okay
so the filters that are run by this
thing are kind of a set of parts that
you can move around and by moving or
moving them around and combining them
you can build you can reconstruct any
any say handwritten digit so you train
the system with only a few filters and
this is how you can reconstruct this
nine as that nine that's kind of a
weighted sum of those filters and the
red frame indicates the position where
you have to move you know place those
parts the cool thing with this is that
you can train sort of a shift in variant
handwriting recognized object recognizer
with this vision system that includes
the kind of the feature extraction layer
and the subsampling layers of a
conditional net as when when swooped so
we get those really beautiful filters
you can plug them into a commercial Nets
and do object recognition with it and on
sort of standard data set like I take
one when we get decent results and not
great results in - in terms of they
don't beat the records the but they they
work okay something like on the order of
fifty five percent correct recognition
when we did this the state of the art
was like around 60 now actually someone
came up with a method that sort of gets
to the upper 70s but we haven't worked
on this since then
okay the last thing I want to mention is
a project that we within which we use
those ideas to is kind of the the next
project - the little robot I showed you
about we showed the little robot project
to Peapod DARPA and
they decided to start a big project
around the idea of using machine
learning to drive robots around called
lagger which stands for learning applied
to ground robots so this project took
place in the last three years it ended
just last month and and each team there
were nine teams involved from different
universities and companies around the
u.s. each team was given a couple of
those robots which consists of kind of a
body with you know a bunch of computers
inside two stereo camera pairs that
gives it a fairly wide viewing angle a
GPS receiver a neutral measurement unit
you know accelerate accelerate
accelerometers and gyroscopes inside and
odometry and a bumper that's all so
there's no active sensors there's no
laser rangefinders or anything it's just
cameras okay so the goal is basically to
use learning to sort of row envision for
robots and the technique we use for for
this was again using convolutional nets
the same systems that we use for the
demos I showed you to identify to
basically discriminate between obstacles
and traversable areas in front of the
robot but you know we don't want to have
to kind of label every object as being
traversable on that so we designed a way
to make the system they essentially long
that by itself and it's a technique that
wasn't you know sort of the general
concept was not invented by us it's
called so near to far learning and the
basic idea is that for things that are
nearby you can get the information as to
whether something is an obstacle or not
from stereovision so by triangulation
you can figure out if something sticks
out of the ground was on the ground okay
and that works up to about 12 meters
1250 meters or so past about 10 or 12
meters you don't get enough separation
between the eyes to really do a good job
at figuring out to something you know
what distance is something so in that
situation you have to you know far away
from the far range stereo it does
nothing to you but at least in the near
range you can use stereo so what we do
is that we
build the system in such a weather we
get labels for things that are within 10
meters and then we train online while
the robot is running train a little
classifier to discriminate between
obstacles and traversable areas and then
we apply this classifier on the fly to
the same frame to the entire frame and
having classify everything in the frame
including things that are very far away
up to about 100 meters okay now for this
to work things that are nearby I have to
look the same I think that are far away
so the way we do this is basically using
a trick which is that you know pretty
much where the ground is so you can
trace sort of virtual lines on the
ground that correspond to you know a
line at 10 meters and 50 meters 20
meters etc and you can take objects that
are still around those lines and since
you know how far objects which foot is
on that on that line how far this object
is you can sort of normalize it in such
a way that its size will be roughly the
same number of pixels regardless of its
distance so this is what's depicted here
so you see this big rectangle here
corresponds to an object that's nearby
whose foot would be on the first line
okay and we crunch that down to I don't
know 20 pixels high something and then
the next line over things would be a
little smaller things of the same size
would be a little smaller so we can shut
down to the same number of pixels so we
get those bands which are labeled on the
horizon where if an object appeared that
band it pretty much has the size the
same size in terms of pixels regardless
of how far it is okay now for the bands
are on your by we get labels from it up
to about 10 meters from stereo so every
window here or stereo system tells us I
know this is reversible or not so we
trained a little classifier with this
and then we apply it to all the objects
in the rest of the bands and then we so
the features are extracted by one of
those convolutional nets which is
trained with this sort of deep learning
arkad algorithm actually we compared
deep and just supervised and it works
pretty much the same and and this is
these are the results so here is kind of
a raw image this is what stereo tells us
so stereo tells us this is traversable
this is the foot of an obstacle and this
is an obstacle sticking out of the
ground then online the classifier is
trained for that frame and then we apply
it to the entire image and it tells us
you know all the way down to
like our readers from there what
everything is same about okay same here
so this is not based on color or
anything it's based on you know each
decision for whether it picks is
reversible or not is based on a fairly
large window around that so it uses
context and everything you know it's the
speaker job so we can detect you know we
can detect that there is a little
passageway between those bushes all the
way up at the top that there is a corner
to turn down there at the bottom and
from those we can build maps and have
the robot to drive itself so let me show
you a video of this and I'll conclude
with that
okay so this is sort of a slightly
accelerated version this is a
short-range vision system which is are
very fast and reactive on its own thread
so you can sort of jump in front of it
and it will it will avoid you very
quickly the long-range vision system is
actually pretty slow it takes about one
second to kind of process a whole frame
but it's only for things that are you
know from five to a hundred meters away
so it doesn't matter if it's slow so so
this is the system without long-range
perception so here it runs directly
towards the people and then you know it
needs to get about ten meters away
before it gears off and the next
sequence is with the long-range vision
turned on and here it is from the start
that there is a wall here that has to
avoid so it sort of directly veers off
to the side so the system looks like
from the sort of console control console
and so here it's in this sort of busy
footpath in a forest and these are the
for the left and right eye pair the
labels that are produced by the sort of
wrong range vision system or by stereo
stereo starts at about twelve meters and
up there is kind of the fast reacting
short-range vision system that does sort
of fast obstacle avoidance and here is
the map that's being built as the robot
runs around it's kind of a hyperbole
polar map that's centered on the robot
that we build as the robot drives and so
this is accelerated twice here so the
robot runs at about fast walking speed
and here wants to go to a goal that's
kind of slightly to the left of this and
it really wants to turn but it realizes
they can't go through those tall grass
so it sort of waits for the road here
same here the goal is kind of behind
this dish bush about hundred meters away
and it wants to go there but it needs to
avoid all those brushes and sort of does
pretty good job and it's basically using
on which perception for that and that's
the sequence is this is a forest path so
now the goal is about our hard meters to
the left it really really wants to turn
left but it see since past really far
away's and so it knows that you know it
has to go straight to fit one if it
wants to reach the goal okay with that
well that's interesting
so I'll uh start here and uh take
questions oh this is really fascinating
work
do you hear misgivings about what's
evidently primarily a military
technology well I mean you uh work with
whoever gives you money right um so
actually no I don't good point
no yeah me I have some is giving about
this but this is only so here is where I
put the limit okay I only put the limit
on things where I can publish so as long
as I can publish the things that I work
on it's fine with me if they see that a
secret you know it's it's it's sort of
past the point that I actually wanted
more work on in your encoder decoder
framework is the encoder always
feed-forward
or during training do you have to do a
search for the code to enforce the
sparsity so during 20 you have to do is
search for the code as the training kind
of gets better and better I mean you
know sort of as you go in training
eventually the the optimal code is very
very close to the predicted code and so
once the system is trained you don't
actually have to do any minimization
just run directly
so the old you know the volt encoders
have been around for quite some time the
old ones were you know where you will
this work instead of the fancy Boltzmann
machines and stuff like that with the or
they have a particular problem with
woodworking here well with the kind of
purely supervisor right yeah well I mean
so the only problem is that it requires
lots of data if you have lots of data
lots of label data you know you should
go for that but in the absence of lots
of label data you have to use those
those are supervised training I mean I
also believe that so this is another
problem which is that with those
unsupervised peer training techniques we
can actually train very very deep
network regardless of their structure so
even if they are completely unstructured
we can train so 10 million nets so some
people at NEC recently we heard about
this at a snowbird workshop last week
have trained the you know deep ml net
with 15 layers you know something
ridiculous Jeff in turn also has done
this rod Fergus also has a paper and the
the next which conferences is CPR I
believe where where he has kind of you
know very deep architectural so to
encode situations through image image
search image matching so you can sort of
do you know extract very very compact
codes for four images in this case so
fairly low resolution images with full
images trying a very very deep network
to get to sort of something like hundred
twenty bit 128-bit code and those you
can use as essentially hash codes to do
you know locally sensitive semantically
meaningful hash code so you can use a
constant time search within a huge
collection of images using those compact
codes so I mean there's huge potential
for you guys using those deepening
techniques you know not just for
recognition or for driving robots around
but for you know search for sort of you
know finding objects and images for you
know index YouTube whatever that's all
next you know projects you think you get
get money from someone else on top
okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>