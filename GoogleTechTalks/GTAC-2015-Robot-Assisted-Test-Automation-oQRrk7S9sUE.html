<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2015: Robot Assisted Test Automation | Coder Coacher - Coaching Coders</title><meta content="GTAC 2015: Robot Assisted Test Automation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2015: Robot Assisted Test Automation</b></h2><h5 class="post__date">2015-11-26</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oQRrk7S9sUE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Without further ado, I bet you're wondering
what this robot does. I am.
So Hans and Natalia from OptoFidelity are
going to talk about robot-assisted test automation.
&amp;gt;&amp;gt;Hans Kuosmanen: All right. Hello, everybody.
My name is Hans Kuosmanen, VP of Test Solutions
at OptoFidelity. I'm here with Natalia Leinonen,
our senior software engineer. And we're really
excited to have this opportunity to tell you
guys about robot-assisted test automation.
So the things that we're going to go through
today, first of all, I'm just going to tell
a little bit about who we are and where we're
coming from.
And we're going to tell you things about measuring
user experience of mobile devices.
Also, the hardware testing landscape, where
the requirements are coming from, and so forth.
And also, when you should use robotics to
test applications and devices.
We have a short case story about Google Chrome
TouchBot that we've delivered during the summer.
And then we will have the demo, which everybody
is probably waiting for.
If we have time, we're going to have some
Q&amp;amp;A later on, too.
So first I'd like to tell a little bit about
OptoFidelity, partly because the marketing
people just told me to.
[ Laughter ]
But also because I like to tell you where
-- what's the background of the company.
So we're a Finnish company, established in
2005. And we've been working with test automation
for R&amp;amp;D for mobile devices. So we're kind
of hardware-centric people.
We have a heavy competence in the optical
test and measurement, so imaging, machine
vision. That's pretty much the projects that
we were working on in the beginning.
And then at about 2008, we started getting
more and more involved in robotics. The reason
for that was the emerging of touch panels
and touch sensing on those devices, on mobile,
multimedia devices.
So far, we've been delivering solutions to
pretty much all the major ecosystems and also
to most of the chip vendors that provide technology
to smartphones and tablets.
We -- In the space of mobile device testing,
we are pretty much the leaders in robot-assisted
test and automation, which means utilizing
robotics in activating the products, simulating
humans. We're kind of building a human simulator
for testing devices.
The human-like UI performance testing is something
that we are using high-speed cameras and external
sensors to measure the actual user experience
of the end user of the product. It can be
either application or just a device itself.
On touch performance testing, we have delivered
systems for characterizing, calibrating, testing
touch sensors, very accurate robotics involved
there.
The first project, actually, that we started
off with was about video quality testing,
so perceived video quality of mobile device.
We have technology, we can measure and analyze
video directly from the screen of the device,
because many times, you don't have access
to the actual video playback, or you don't
have reliable access to it.
And one thing that's pretty close to the video
playback is the UI animation and how smoothly
the UI is actually working. It also gives
the end user the perception if the UI is good
or if it's bad, if you have, like, a -- frame-skipping
or things like that when you're actually operating,
opening apps or switching between apps and
so forth.
There's actually also some images of the type
of robotics that we have delivered. So we
-- This is just one example. So we provide
a lot of different -- a lot of different platforms,
a lot of different form factors for different
applications.
Then to the subject of measuring the user
experience. I mean, user experience is kind
of a large concept. So there's things like
look and feel and, you know, what kind of
colors you need to have on the UI. That's
not within our scope. So in our scope, we
are looking for things that we can actually
measure and get measurement results, raw data.
There's a couple of things on the slide there
that are contributing to a good user experience.
And these things need to be in order for the
end user to perceive the quality as being
good.
I just want to highlight a couple of things
where robotics is going to add some benefit
and some value to the testing.
One obvious thing is the touch sensor. So
there's really no other way to characterize,
to validate the performance of touch other
than having robotics in there and artificial
fingers, styluses, and what have you. And
the touch is really something that's emerging
also to wearables and to cars. It's pretty
commonplace already in the smartphones and
tablets, obviously. But there's a lot of different
applications coming up, even just household
appliances are having touch on it. So touch
is really big, big thing. I'm sure there's
a toaster with touch interface on it.
So we are providing solutions for the touch
accuracy, touch responsiveness. There's also
proximity, like hover of the touch. That's
another user input to the device. Also force-sensing.
And now, like, surface pro and the iPad pro
-- now the stylus seems to be coming back
as an input solution or an option. And it
seems that there's much more technology behind
it. So it's not just the replacement of the
finger. There's, like, angle and force-sensing
on those devices also.
So we can provide the equipment, the methods
to test and characterize all of this. Imaging
is another very important area. We heard on
the keynote that there's 300 hours of video
being uploaded to YouTube alone per hour.
And, you know, everybody's -- 20% of Internet
users are -- they have an Instagram account.
Everybody's posting -- even me -- posting
their lunch pictures and breakfast to Facebook.
So user -- end users are expecting the image
quality to be perfect and the video -- recorded
video to be really good.
So what that means is that the device manufacturers
have to find ways of how to get a better image
quality, how to get better video quality.
And it's not just adding more megapixels,
it's, like, integrating the sensor, like acceleration
gyros, all this into the imaging chain. And
this makes it very complicated to use software-based
instrumentation in the product, because there's
so much interaction between these different
hardware components inside the device.
Then there's much more sensors being integrated
into the -- the products, like near field,
wireless sensing, and gyros, acceleration,
you name it. There's more and more stuff coming
up. So the more complex the device gets, the
more interactions you get, the more difficult
it is to use software-based agents to measure
and to test and validate.
So we are focusing on doing all the testing
nonintrusively, so we don't have anything
running on the product. We're using external
sensors, cameras, we're using robotics to
activate the products. And this gives us a
much more robust way that works across different
types of equipment and devices.
So the UX, being a very broad term, where
are the requirements coming from?
So, well, the requirements are coming from
the end user. That's one of the reasons why
we are kind of doing the human-like testing.
We try to test the way that the end user's
perceiving the operation and the feedback
from the product.
The app and solution developers are kind of
the in-between messengers to the operating
systems, ecosystems, of what kind of features
the end users are looking at. And also finding
new ways -- new attractive features for the
end user to want. So that information kind
of gets into the ecosystem level. And then
there's a whole complete value chain behind
it for building those devices and validating
the components that go into them.
There's certification in some cases, like
Google has the CTL, I think. And Windows has
the HCK. So there are some requirements that
come from the user -- the end user level into
the actual hardware and the components. But
oftentimes it's just the equipment manufacturer
that pretty much make up the requirements,
make up the limits.
And then all these limits -- or the methods
and the limits go into the value chain to
R&amp;amp;D teams, to production of the equipment,
to after sales, like, refurbished business.
All these guys are using the same requirements.
And our solutions, as they are, like, end-to-end,
not intrusive ways, you can use them for all
of these different players on the ecosystem.
The other way to look at that is, like, the
product life cycle. So first you try to validate
the components and the sens- -- the actual
bits and pieces and the chipsets and what
have you. So the chipset and platform suppliers
are doing most of the testing. The device
manufacturers get into it once they start
to integrate these into products. Then there's
going to be functionality, stability, and
interaction of those hardware components.
The device OEMs start to do most of the testing.
And then once the products are launched and
once they are, like, finalized and new versions
come out, there's a whole area of system-level
testing that I'm pretty sure most of you guys
are involved in that involves the software,
the applications, the server back-ends, and
what have you. So you get operator and service
providers, application developers are more
doing the actual testing. But this might -- this
is kind of a living thing that some of that
testing is kind of going back to the device
level also. So there's -- not all the platforms
are very stable in a way that you have to
take into account a little bit of the hardware.
And that's why there's a lot of talks also
in this conference about testing in the hardware.
It's very important. You can't just go and
test everything on an emulator.
So instead of just using a device cloud, when
would you be testing with a robot?
Well, you could test everything with a robot.
But a lot of times it doesn't make sense to
have all your test cases run on that. But
there are specific areas where the robotics
is going to add a lot of benefit and a lot
of test coverage to your test portfolio.
Most importantly, it's the complex interactions.
So more and more hardware components, more
and more applications are running at the same
time when your app is running. And oftentimes
it's kind of hard to make sure that your test
conduit or your instrumentation is not actually
one part of that complex interaction and affecting
your test results.
Also, when you're using production software,
you might not have all the instrumentation
available. You might not have all the debug
features and the hooks. Or if you're testing
how your devices are booting, like, this is
more like operating system stuff, so if your
device doesn't boot, how are you going to
get your instrumentation software up and running?
So a lot of times, you need some sort of robotics,
like, just pressing the power button or, you
know, disconnecting a power cable or something
like that.
In medical industry, there's a lot of regulations
that you cannot have any instrumentation in
your -- in your system. You can't even have
extra hardware components in it to, you know,
pull out a display to a frame driver or something.
So you don't really have the means to get
the data that you could use on your test cases.
Whenever you need controllable, repeatable
UI actuation, that's also -- a robot will
do it. You can do it per millimeter, even
100 micron, 10 micron, even. If you need to
move the product around, for example, to activate
the sensor layer of it, then robotics is the
way to go.
Also, if you need real-live sensor data, like
image stabilization of a camera, for example,
it's -- when you're optimizing your algorithms,
it gives a lot of benefit to actually move
the camera around, move the targets around
in a predetermined fashion so that you can
iterate your software using the same test
conditions every time.
Benchmarking against your, you know, neighboring
operating system is also one thing that if
you're doing it with software instrumentation,
you're probably not going to be comparing
apples to apples. You might have a little
bit different performance of your software
instrumentation on different operating system
platforms.
And then, as I earlier mentioned about the
touch sensor, if you need to do things very
accurately, to the, you know, 10-micron level,
robotics is really the only way to go. And
a lot of this is hardware-centric, but some
of this also trickles down a little bit to
the software layers.
The -- some of you might have heard about
the Google Chrome TouchBot. So it's a system
we delivered to the Chromebook team over in
Mountain View. And what it's used for, it's
used for measuring the end-to-end latency
of the user interface on Android and Chrome
OS devices. So it's part of, like, a continuous
integration system where the new firmware
uploads are getting automatically into the
devices. And as you can see here, there's,
like, a -- five units on the same robot. And
you can fit another five there pretty easily.
So all of these get the new software built
into it. And then the robot will go and activate
the product, actually, a little bit similar
way what we have here on the demo. And we're
looking for things like how quickly the display
is responding to the actual physical touch.
So there's, like, two systems. One is -- the
one in the picture has, like, a two-finger
actuator, so you can do also pinch in and
zoom motions. And then there's another one,
which is actually the biggest robot we've
ever delivered, which has one-finger actuation,
but you can fit several -- much more units
under it to get -- to get tested during the
integration cycle.
But then it's demo time. So this is probably
what you've all been waiting for.
So I'll just go -- let Natalia do her thing.
Can we get the previous slide on there, please.
&amp;gt;&amp;gt;Natalia Leinonen: Yeah.
&amp;gt;&amp;gt;Hans Kuosmanen: There we go.
&amp;gt;&amp;gt;Natalia Leinonen: Okay. So here is the demo
setup. So we have an actual small, three-axis
robot here. And it's equipped with a brass
fingertip for interactivity device and a high-speed
camera to measure the latencies of the graphical
user interface, and also a small camera for
navigating the UI and for configuring the
location of the device.
And can we have the livestream now? Okay.
Great. So our software which runs on this
laptop here provides easy-to-use programming
interface. So the user who will use this robot
test device doesn't need to know about anything
about how to control the robot or about robot's
coordinate system.
Once we have configured the location of the
device, then we can just say that tap on a
certain location or swipe across the screen
or take a screen shot and verify that there's
certain icon or text visible on the screen.
And in this demonstration, we'll be measuring
so-called pen-to-ink latency which is basically
the distance between the fingertip and the
line that is being drawn while we swipe with
the finger. And there are also several other
types of UI latency measurements like, for
example, how long it takes for a device to
react when a user taps or how long it takes,
for example, for camera application to launch,
things like that. But now let's take a look
at this demo here.
It's very simple Python script which will
execute three identical swipes on this device.
And it will measure the pen-to-ink latency
for each swipe and then plot the results on
the screen. So let's start.
First, the robot opens our test application
which, basically, just draws a line on the
screen immediately after receiving the touch,
after detecting the touch.
And we could use any painting software. The
reason why we're using this here now is that
we can be sure that there is no extra latency
coming from the app itself.
And here we are using a simple Python script.
We have tools for creating simple measurements
like this without knowing any programming
languages but using -- creating Python scripts
provides better flexibility and better integration
to continuous integration tools or test frameworks.
So here we are just providing an API, and
it is easy to use and can be integrated into
anything.
And now we have the results and probably can't
see it but it's -- the pen-to-ink latency
for this device, which is Nexus 4, is around
80 milliseconds. And usually anything below
100 milliseconds is considered to be immediate
so the results are pretty good.
[ Laughter ]
So good.
And, yeah, that's it.
&amp;gt;&amp;gt;Hans Kuosmanen: All right. Are there any
questions?
[ Applause ]
&amp;gt;&amp;gt;Yvette Nameth: Thanks, Hans, Natalia, and
robot.
[ Laughter ]
Let me sneak across. So it seems like the
first one is: Is it fast enough, including
movement, image recognition, et cetera, to
be applicable to game testing?
&amp;gt;&amp;gt;Hans Kuosmanen: Well, this robot here is
really, really a low-end thing. So you can
get more speed with Scala robots, delta robots.
The more problematic probably is going to
be the image recognition part. We do have
solutions for measuring frame rates, for example,
directly from the screen. But detecting icons
and then -- it depends on how quickly you
need to react to them. And at the end of the
day, you can just build a server rack, you
know, behind it. It's just math pretty much.
&amp;gt;&amp;gt;Natalia Leinonen: Depends on the game.
&amp;gt;&amp;gt;Hans Kuosmanen: Yeah. With a laptop, it's
probably not going to work right now. We'll
wait for a couple of years.
&amp;gt;&amp;gt;Natalia Leinonen: Yeah. But if you are playing
Angry Birds and Clash of Clans with this robot,
you can see videos online how this robot --
&amp;gt;&amp;gt;Hans Kuosmanen: Yeah, please go to YouTube
and search for OptoFidelity. There is a whole
bunch of fun stuff there.
&amp;gt;&amp;gt;Yvette Nameth: So how do you test your own
test machines are working?
&amp;gt;&amp;gt;Hans Kuosmanen: That's a really good question.
Well, we do characterize the robotics for
accuracy and the timing with external equipment,
like optical encoders and things like that.
I'm not sure if that answers the question
directly. We're using ready-made libraries
for detecting icons, for example, if the question
is more of are we get getting false failures
or getting false passes. We come -- the background
of the company is from image processing and
machine vision. So we've worked with a lot
of the image libraries for icon detection.
But -- we haven't implemented all of the algorithms
behind it. We tend to more use the existing
libraries.
&amp;gt;&amp;gt;Yvette Nameth: Okay. It seems like this
only makes sense financially to run in very
high-value scenarios? How feasible is it to
roll this out for functional testing? How
feasible is this for functional testing for
middle or lower-finance or lower-value scenarios?
&amp;gt;&amp;gt;Hans Kuosmanen: Well, right now what we're
seeing is this is used for a subset of testing,
a subset -- the things that cannot be done
with software or the software instrumentation
is not reliable enough.
Robotics is developing all the time. So the
prices are coming down. The speeds are going
up. And in the near future, it might be financially
more -- well, I mean, we're happy to provide
all this stuff. That's not a problem.
[ Laughter ]
But, you know, right now it's probably just
a subset. But we're going to wait for a couple
of years, and I'm pretty sure there's going
to be a lot of -- a lot of low-cost robotics.
I mean, there's, like, 3D printers coming
out every day. And all this development goes
into the robotics. We are kind of harvesting
on that.
&amp;gt;&amp;gt;Yvette Nameth: Great. I do have time for
one more.
How is this robot controlled? Can it talk
using Java? And how is the validation done
across different devices? Like, do I need
different scripts for different devices?
&amp;gt;&amp;gt;Hans Kuosmanen: Yeah. So the API that we're
using, it's actually HTTP. It's like a REST
interface. So that will work pretty much with
anything. So if you want to use Java, that's
perfectly fine. We provide a Python library
or wrapper.
&amp;gt;&amp;gt;Natalia Leinonen: CSharp, C++.
&amp;gt;&amp;gt;Hans Kuosmanen: The interface is really
simple. Then the tools for doing the configurations
and positioning the device and all that, those
come with the system anyways. So the interface
when you are building your scripts is really
simple. And you can use pretty much whatever
language you want.
&amp;gt;&amp;gt;Yvette Nameth: What about reusing the same
script on multiple devices? So you have a
Nexus 4 and a Nexus 10 that you want to run,
say, that speed test on.
&amp;gt;&amp;gt;Hans Kuosmanen: That's possible because
-- actually the icon detection -- we are not
using bitmap comparison on the icon or the
OCR. So if the display sizes are different,
even if the color scheme is a little bit different,
it will adapt very easily. Obviously you can
write a script that doesn't work but, you
know... Always. A possibility.
[ Laughter ]
&amp;gt;&amp;gt;Natalia Leinonen: As long as the navigation
logics doesn't change, then it should work.
&amp;gt;&amp;gt;Yvette Nameth: And one last question. Are
these robots meant primarily for development,
or are they something that might be used for
sampling of the entire production run?
&amp;gt;&amp;gt;Hans Kuosmanen: We have delivered robots
for production of devices of equipment. So
we're using the same technology or parts of
the technologies. It's not identical to this.
So typically the type of things that we're
demoing here, these would be on -- R&amp;amp;D team
would be interested in these kind of results.
But we're using the same technology. We have
some robot platforms that we use on production
line. And the technology that we've developed,
some of these -- like the actuations like
the fingertips and locationing of the device
-- we have automatic locationing -- that is
also used in production.
&amp;gt;&amp;gt;Yvette Nameth: Thank you again to the two
of you and your robot.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>