<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2013: Finding Culprits Automatically in Failing Builds - i.e. Who Broke the Build? | Coder Coacher - Coaching Coders</title><meta content="GTAC 2013: Finding Culprits Automatically in Failing Builds - i.e. Who Broke the Build? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2013: Finding Culprits Automatically in Failing Builds - i.e. Who Broke the Build?</b></h2><h5 class="post__date">2013-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SZLuBYlq3OM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so we have one last lightning talk
before we take a break here and so we
will make it go like lightning lightning
being exactly 15 minutes so it's like
lightning from the Sun D here or
something like that and so with that I
want to introduce J laws if J and we
wake Rama Rama Rama Vajra
who are from the University of
California San Diego and are gonna be
talking about finding culprits
automatically in failing builds ie
who broke the build and I had
opportunity to have dinner last night
with Jalal and it was a very fascinating
discussion so with that I shall hand it
over
thank you thank you auntie hi everybody
as Tony mentioned I'm coming from the
University of California San Diego I'm a
PhD student there and I'm hopefully
finishing in a month to join Google and
my colleague here feedback is already
working at Google and incidentally he's
also a graduate of UC San Diego so our
talk today is about finding culprits
automatically in finding builds in other
words who broke the build so let me
first give you some background on how
this project came to life so I was an
intern in the Google New York office
last summer and this started as
basically a research project on how to
do this kind of job automatically and we
wanted to see if this is feasible if
there were any strategies that we could
use to make this a reality and lucky for
me we actually found a good way to put
this into production and right before I
left we started using it in two projects
and we got some good promising results
which I will give you some examples at
the very end all right so let me first
describe what a culprit is so a culprit
change list is we define it as SEL that
breaks the build so as you sow the
during the talks yesterday and today
most of the companies today including
Google use continuous integration and in
a typical set up you have a green build
in which the
compiles it all the tests pass and
everything's good life is good and then
developers commit changeless and those
great boxes represent change list and at
some point in time the continuous
integration machines download the code
they build it and some tests fail so we
have a red build so now what we need to
do quickly is to figure out which of
those grey boxes is the culprit and it
caused the bill to break the reason we
want to do this is because we want to
have more green builds which means we
will have a better quality software
because if you have a green build we can
have a best faster development and
release cycle meaning we will be
confident with our build as long as it's
green so we can release new versions of
it and also importantly we are gonna
have fewer engineer hours wasted so let
me expand on that a little bit so when
you have a setup like this typically
developers take shifts on watching the
build if the build fails whoever is
watching it on that week or that month
stops what he is doing and then he
starts investigating what might have
caused the breakage there are different
ways of doing this and I'm gonna
describe different types of automation
on this problem and there we wear our
solution lies but pretty much if you do
this manually then it's not a very good
thing to do because you're gonna erased
a lot of manual time of your developers
and basically should have an automated
way to solve this problem so let me talk
about how this can be automated or
already automated in different types of
tests so the first type of test is unit
tests as you can imagine these are
pretty short tests I'm talking about
seconds here and if you have a problem
like this if the build fails in your
unit test you can pretty much do the
following you can just build every
single CL separately and possibly in
parallel and you can easily find the
culprit in seconds or at the most
minutes right if you have medium-sized
tests which take a bit longer than the
unit tests
then these are also more computation
intensive then instead of building
everything in parallel because you don't
want to use lots of computation
resources you do a trade-off between
time and computation obviously which is
coming in computer science as you know
so we can do a binary search instead so
what we can do is we can target the
middle change list we can build it if it
fails then the copied is probably to the
left so we Vicar stood left if it passes
we go to the right sorry vice versa if
it passes then we go to the left and we
basically do a binary search recursively
until we find the culprit CL now these
were already handled problems in Google
when I came in and the biggest problem
lied on integration test so these are
tests that take a very long time to run
and I put the eight-minute cutoff there
but the ones that I looked at could take
hours let's say two hours right and baby
basically the strategies that I describe
for unit tests and the medium-sized
tests obviously don't work for those
because every single build takes two
hours so then we have to have a
different strategy to automate this and
as I said when I came in there was no
easy way to find to find culprits like
this automatically so the solution was
to manually investigate the changeless
all right so basically our proposal was
to have a completely automated tool that
gives us suggestions about the culprit
and do this for every single time that
the build fails by itself and then
provide the user some information about
what it found and what could be the
suspects and the important things are
here it should be fast
potentially it should give you a result
within minutes it should be cheap it
shouldn't use too many computing
requirements or tsa's as I mentioned
these can take two hours to run and it
should be good obviously good is a
subjective word and I'm gonna show you
the results at the end then you can
judge for yourself all right
so this is the very high-level overview
of how this works so the tool monitors
the build if the build fails it
immediately starts looking at all the
change lists that went into the build
and it ranks them so it calculates this
metric that I call suspiciousness and
it it then puts the most suspicious ones
on the top and then it just basically
sorts them rivers and tells the users
that here are the suspects that I think
are the culprits please take a look at
them alright so obviously in this flow
as you can imagine the interesting part
is how do we do the ranking right so
obviously for every single change list
we look at all the files in all of them
and on a very high level I cannot give
too many details but I'm gonna explain
how we do this on a very high level so
we basically use two heuristics right
now the first one is which is rather
trivial if a change list has more files
than another change list obviously it's
more suspicious because it touched more
stuff in the build tree the second one
is a bit more interesting its distance
so if you think about if you think about
the build tree so it's basically a
directed acyclic graph and your library
your project has lots of dependencies to
other libraries so if you think about
this as a tree there's a root of the
tree let's say in Python it's the core
Python libraries right and the heuristic
is say is the following so if I depend
on a library that directly is my first
adjacent library then it's more likely
to break my build than a library that's
up in the chain that is closer to the
root this might seem a bit weird but let
me explain why we thought about this so
think about a library that is closer to
the root there are two important things
that we observed about such a situation
first of all the libraries that are
closer to the root would be if somebody
makes a change in that library they
would be more cautious for example if
you're changing the core Python library
or something that immediately depends on
it obviously you know that lots of
projects in Google depend on that so you
you are more cautious on making changes
and there's a more rigorous review
process and so on and secondly let's say
you still made a mistake and you
introduced the bug and you broke the
build on that project
immediately since there are lots of
projects that depend on that because
it's closer to the root what will happen
is some continues build some continuous
integration project other than yours
we'll hit it immediately because lots of
projects are running continuous
integration and it's very likely that
the first one that's already running
with the new changed project will hit
that bug and they will immediately
realize this and figure it out and fix
it so this is basically the heuristic
why we use the distance as a metric to
calculate suspiciousness the important
thing here is the heuristics are
plugable in this case we used to but we
had hopes on using other ones for
example if something fails you can look
at the logs hopefully there are some
keywords in the logs and in the diffs of
the files that were changed and maybe
you can correlate them and so on
so we didn't implement that but what I
want to say is these are pluggable so
you can combine these and make your own
heuristic and so on it's a matter of
experimenting and finding what works for
your project and with that I give it to
be back to to show you how this looks so
basically after Cheryl left for his
internship I joined about the same week
so I only saw it working for the project
that he was interning on and I thought
it's a pretty cool thing because I had
seen my teammates tear their hair out
when they were finding out who booked
the build so I spent a couple of weeks
hacking things together a couple of
months hacking things together and
getting a prototype ready so you just
have to go to go to the UI and say look
this is the build tool I have and what's
this thing for anything that breaks
anything that is indicating like a
failed test in feed or anything like
that and moment something breaks go and
figure out which is the latest green
build which is the latest red built
figure out which Sears between that are
you know things like automated CL slate
updates which are not really human
errors you remove all of those do some
pre-processing and then actually get the
culprits and then send them to the team
inside of the project and I also ended
up implementing some of these smarter
heuristics a bit so things like if you
have logs the logs will contain messages
that say you know this is the place
where an exception was raised or this
file didn't compile so you can do a lot
smarter things like just figure out
where this file was modified and that
gives you a lot more confidence in your
scores so since January
middle of January we have about six or
seven projects using this prototype it
has already been in this it already has
investigated something like two to two
hundred fifty hundred two and fifty
breakages that's about thirty breakages
per project in this two to three month
so that's pretty significant and
essentially it lets you find the
culprits in something like two to three
minutes instead of half an hour to an
hour so you can just you know get back
to your work instead of having to dig
around other people's code and basically
that's it so this was a prototype that
are developed and it's now being
integrated into a proper production
continuous build system and that's a
little slower but it's getting there so
hopefully if the message is actually in
place it should be possible for people
not to worry about you know going
through people's code mailing them
asking them why I did this and it you
test it properly and so on so forth and
there are some results basically we
often end up analyzing because they're
large integration tests the number of
changes we are talking about are in
hundreds of thousands so one example
there is like that last one is the most
significant seventeen thousand Ciel's
between the green and the red and it's
able to tell you that in your ranking
number seven is the answer it's pretty
significant you don't have to worry
about filtering out hundreds of seals
and hopefully is going to save a lot of
time for even the feature to and I wanna
finish by reminding the three things
that we wanted to have with this project
one good results and I think we can have
some promising results here and second
one is it should be fast and the first
prototype that I implemented last summer
took like six hours which was suboptimal
we can say and then we did some runs on
it and we did some optimizations and
then we took it down to s image in like
two minutes
obviously use lots of caching and so on
and that's the third thing that I want
to mention a resource efficiency we
basically don't go to the file system
and you know query things we we use
caching as much as we can and that
that's why this is so fast competition
so we don't need to compute the bill to
dynamically because once the CL is
submitted we pretty much know what the
bill tree was at that CL so a lot of
these things are real committed to save
time and the last thing I want to
mention is I would like to
make a point on what Ari mentioned doing
is a keynote speech yesterday so he
mentioned that Google is having starting
to have some scalability problems in
running all of the unit tests in all of
Google and after we implemented this and
saw that there are some promising
results we had some discussions about
integrating this tool to the core
technology infrastructure in Google and
the idea is basically if this tool can
actually suggest the culprits or suspect
suspect CLS before after they are
submitted before they are built then we
can actually build all the CLS even for
unit tests instead of building all of
them in parallel we can actually build
them in a certain order for example we
can take the first hundred and then
build them in parallel basically the
most suspicious ones and then we can
take the second batch and so on so
basically the idea is if you see a CL
that fails then there's pretty much no
point running the sales after that right
because like after until that is fixed
they are gonna fail as well so basically
the idea is to integrate to to the core
testing infrastructure so that we can
have more efficiency in testing at
Google and that concludes our talk great
thank you Joe thanks Rick we can take a
on either aisle I see some people lining
up so how about the left first
although shadi I don't think you get
this that's okay hi I'm Shelly I work
with the chrome team first very nice
tool really nice work just wondering how
do you deal with false positives and
what's the right way to update the
heuristics that you have based on the
results I should say I mean we
experimented with six projects so far I
mean when I'm see we experimented with
two projects and now it's six
unfortunately I didn't have enough time
to do a formal evaluation which I would
like to do once I joined Google in
summer by formal evaluation I mean
literally going to a ground truth like
baseline from all the unit tests that
Google run already and run this tool on
them and see if I can actually identify
that the ones that actually fail so
after we do that evaluation I'm pretty
sure we will have a good
understanding of which experts like
which heuristics work better but we
basically like experiment to do a couple
of them and these two look like the best
ones speaking of false forces there also
there also are other tools that I
identify flaky failures so if the same
desk is filling in the same point again
and again and it's not really a failure
the user can notify the tool saying that
look this is not a true failure it's a
flake
so that basically can tell our tool that
don't trigger on this failure wait for
two three more to happen don't think
it's actually feeling okay and with that
we've actually run out of time so thank
you guys and we are</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>