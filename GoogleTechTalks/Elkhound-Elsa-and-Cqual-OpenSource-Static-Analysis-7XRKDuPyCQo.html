<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Elkhound, Elsa and Cqual++: Open-Source Static Analysis... | Coder Coacher - Coaching Coders</title><meta content="Elkhound, Elsa and Cqual++: Open-Source Static Analysis... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Elkhound, Elsa and Cqual++: Open-Source Static Analysis...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7XRKDuPyCQo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is Scott mcpeek I'm
talk about some work that I've done as a
grad student at UC Berkeley I have since
graduated about seven months ago and
they've started work at Coverity the
work i'll be presenting his joint work
with my advisor George Nicola John
codamol Rob Johnson and jeff foster who
were grad students the same time I was
Jeff is now at Maryland Rob Johnson is
sony Brook John codamol is going to
start a Coverity in a few months as well
Daniel and George are still at Berkeley
okay so the the theme sort of this talk
is about extensible program analysis
finding ways to write analysis that then
let the end-users customize these to
provide rules in language constructs
specific to the things that they're
trying to check and so we want to have a
parser that allows language extensions a
an internal language infrastructure
which additional like type checking
rules and so forth can be added and
ultimately a program analysis framework
that allows you know program specific
idiom recognition and checking of
specific properties and it will mean so
the start of this the sort of three
parts of this the parser the parser
generator the parser and the analysis so
that the first part is a parser
generator this is called elkhound it's a
tool that generates parsers that use the
Geor which is a generalized LR parsing
algorithm this is an extension of lalr 1
in which you can give it a grammar at
essentially any context free grammar and
it will generate a parser for that
language it does not have to be lalr 1
so this has two main advantages first
allows unbounded look ahead so it
doesn't matter how far ahead in your
input stream is required to look to
disambiguate something the GL our
algorithm will be able to determine the
correct parse no matter how far a way
that disambiguate er is and in fact you
can use ambiguous grammars and the GL
our algorithm will give you rather than
a parse tree a parse forest
essentially all the possible parses but
represented in a compact form so this
doesn't have an exponential explosion
now glr has been around for a while the
first main paper was in 1985 but the l
command implementation is novel because
it's actually ten times faster than the
other implementations that previously
existed and the the main reason for this
is because it actually combines glr and
lal are taking advantage of the fact
that for most programming language
grammars the majority of the input can
be parsed using lal our algorithm and
since this is a much faster algorithm we
can use this as long as we can and then
when the ambiguities arise actually
switch to the more complicated algorithm
and so it seamlessly switches back and
forth between these two and is able to
therefore be competitive with bison in
terms of like a conventional la Isla
implementation in terms of performance
furthermore unlike addition of previous
dlr implementations you can use
arbitrary reduction actions most glr
implementations build a parse tree first
and then give you that and you can work
on it but a parse tree is very large so
instead elkhound uses just user-defined
reduction actions that are executed on
the fly during parsing rather than
actually materializing a parse tree so
just in case you're not that familiar
with these technologies what's the issue
with lalr well the problem basically is
that it has to decide what the proper
interpretation of a token is with just
one symbol of look ahead so I've given a
little grammar here the language only
has three input strings there's a b c eb
d or a BD but the trouble is that you
can't tell whether a leading a should be
regarded as the beginning of a b c or
the beginning of a BD because you can't
see past that first be symbol you only
have one symbol of look ahead and so
there are two possible ways in which the
LR parser can handle this it could
regard it as the beginning of ABC and
shift the be in which case then it could
shift the sea and finally reduced to an
S or it could first reduce the a to an E
and then having decided that that's the
proper interpretation go on to shift the
B and then what would then hopefully be
a D and finally reduce that to an S so
the trouble is that we're just we're
forced to make decisions too early
in lal are so what glr will do is it
will extend the traditional lal are
parsed ack with the structure called the
graph structured stack that lets you
actually encode multiple potential par
stacks simultaneously so here along the
bottom i've shown what a lalr parser
essentially might be doing with some
sort of arithmetic input but what glr
will do is it'll add additional possible
stacks so this this graph structure here
is simultaneous representing three
different possible par stacks and so
basically all the possible
configurations of an lalr parser are
represented simultaneously these
configurations are kept in sync each
time you shift a token and any time one
of them fails to be able to make
progress you just get rid of it and keep
going with the ones you have so this
lets us essentially make the the stack
non-deterministic we just keep track of
all the possibilities going forward now
the novel park for elkhound is being
able to switch back and forth between LR
and GL are basically lr would be so any
place where you have sort of a linear
structure in your stack because that's
what an LR parser would look like but if
you have some more complicated area in
your your par stacked and that's where
you're going to have to use the GL our
algorithm and so the thing you need to
be able to do is looking at the top of
the stack decide whether the next action
to take if it's if you know what there's
only one possible action whether that
can be executed with the lr algorithm or
the GL our algorithm and this is fairly
simple to do we basically just annotate
each node in the par stack with
information about how deep you can go
before hitting either a split or the end
of the stack and so then the way this is
used is if you're ready to do a
reduction then you look to see is the
right-hand side of that reduction less
than the height of the non determine of
the determinism that you have on your
stack so if you're going to reduce like
a 1 symbol right hand side then you can
use lr but if you're going to reduce a 3
symbol right hand side then you're going
to get into the bubble here and so you
have to use the Geo our algorithm now it
would be easy to make the hybrid be of
the form any time that stack is entirely
near use lr otherwise use glr so why do
we go to the extra effort of being able
to parse despite sort of this bubble
here the reason is you have languages
like C++ for example where you have a
rule for a constructor and a rule for a
function call and it's not obvious to
the parser which one you're looking at
because you could have like a kool &amp;amp; B
colon see and there are different ways
of parsing which you know is the return
type or what's the qualifier and so
forth so with C++ you can get situations
where the beginning of your function is
still ambiguous you still don't know
what the beginning of the function is
supposed to be and you may never know
but the body of the function you may
find is completely easy to parse and so
you want to be able to parse the body of
the function quickly despite sort of a
latent ambiguity and so that's what L
count is able to do it's able to use lr4
whenever possible and then switch to GL
r when necessary the parse actions that
you would write are conceptually anyway
like they would be in bison you have a
right hand side this is a production
here exper is expert plus X / the right
hand side has its non terminals labeled
and then you can just write some C++
code that will be executed whenever that
reduction is fired it will yield a new
semantic value so as we fire reductions
we build the parse tree where the
abstract syntax tree is what this would
be bottom up so as you do this though if
you encounter an ambiguity if the
grammar is actually ambiguous is
multiple ways of parsing some fragment
then you'll have two possible semantic
values for the same sequence of tokens
and so these will be passed to a merge
function that's something the user
rights and is just passed in both of the
possible interpretations and asked to
return a new interpretation for that
fragment it could simply choose one and
discard the other or it could actually
construct an explicit representation of
ambiguity which is what's done in the
ELSA parser in some places the point is
it's up to the user to decide what to do
when you actually encounter the
ambiguity if your grammar is unambiguous
merge will never be called so that says
only if you actually have an ambiguity
somewhere in the grammar also because we
have
parser sort of potential parsers that
may go for a while and then die but
other arms of the parse continue we may
need to deal with memory management
depending on exactly what these actions
are doing so there is additional
functions do Pindell associated with non
terminals they're called when a semantic
values used a second time and when it is
ultimately not going to be used again so
if for example you have an abstract
syntax tree with reference counting you
can increment and decrement the
reference count using do Pindell of
course you can also just leave them as
no option use a garbage collector or
something like that ok so now sort of
transitioning into talking about parsing
C++ have a couple examples of things in
the C++ language that are difficult to
parse especially in an El Al our
framework and how we can actually use
Geor to make them easier so here I've
shown the same sequence of tokens that
actually has two naive interpretations
in C++ because of the overloaded use of
this angle bracket I could either end
the type argument to a template or it
could simply be a greater than and then
since plus conveniently can be either
unary or binary we can actually come up
with two completely coherent parses of
this thing so if you crack open the C++
standard you'll find buried somewhere in
chapter 14 that little sentence that
says if if you see a nun parenthesized
that is not not inside a parenthesized
expression greater than symbol as the
argument to a type argument to a
template then that's not okay all right
so the intent of the standard is that
you essentially terminate the the
argument as soon as you can so you don't
think of unpaired the size greater thans
as being the greater than operator but
this is not easy to do within an el al
our framework in fact in in GCC which
used in lalr parser up until version 3.3
they just implemented this wrong they
used sort of a hack with precedence that
worked most of the time but there's
plenty of inputs you could give it that
it would miss parse because if they
weren't doing what they needed to
because it was too difficult
but in a gilr framework it's actually
really easy because we'll just go ahead
and parse them both we'll come up with a
tree that actually just has an explicit
representation of this ambiguity and
then it's easy at once we get to the
merge function to just go ahead and
search through the tree for this exact
rule just any place you see an argument
to a template and then you see a greater
than you just discard that tree so we
can implement this rule within the merge
function of the the parser as once we
get both trees rather than having to
just see make the decision as soon as
you see the token has it even more
difficult example you're probably
familiar with the fact that in C and C++
you have this cast notation which you
just parentheses a type and that means
to cast but as usual we've got conflicts
between binary and unary operators so
this simple this fragment here could
either be a cast of the address of
something or it could be a bitwise
operator bitwise and so this exists in C
and the the way the decision is supposed
to be made is that if a currently looks
up in this scope to the name of a type
and this is regarded as a cast otherwise
it's a bitwise and the usual solution in
C is to do what's called the lexer hack
meaning as the lexer is Lexington's
tokens it actually consults the symbol
table to find out what the current scope
maps a name to however in C++ this is
especially difficult because of the
scoping rules so within the body of a
method inside of a class all of the
declarations in that class are visible
meaning that in this example here this
is actually a cast because what's
visible is that the typedef that
actually Kirsten tactically after it so
to implement the lexer hack for C++ you
have to essentially save as unperson
streams all the method bodies in the
class process all the declarations then
go back and restart the parser on each
one of those just so that you can figure
out what to type in what's a name but
again in the GL our framework this is
actually easy to handle we'll simply so
what this next slide is
sort of we can simply parse it both ways
and then this method body will contain
within it somewhere in ambiguity but
that's fine we've now parsed it later
when the type checker runs it's easy
because you're now looking at abstract
syntax to just visit all the
declarations ignoring the body's build
up a simple table now go back and
reparse the bodies and you've got all
the information you need so the idea
here is that because glr can have an
explicit representation of ambiguity
partial the ambiguous grammars we can
delay disambiguation until it's most
convenient we're not forced to kind of
bind everything together just because
the parser needs has certain needs we
just wait until we have all the
information then make a decision so this
next slide is sort of illustrating
exactly how that actually happens having
already parsed the input with two
productions one type name can be
identify or a variable name can be
identifier when we get to the type
checker if we have an expression with
two possible interpretations this is the
the usual ambiguity resolution rule you
can you can do other things but the
usual way of handling this is that you
just type check them both naively and
hopefully exactly one of them will fail
and you just retain the one that didn't
fail this is actually what else it does
in most places to parse C++ there are a
few cases where it actually does
something different but this is mainly
for performance okay slide is a little
bit out of order but okay so the feet
the features of ELQ on the parser
generator were that it would its
performance is within a few percent of
bison if you give it lalr inputs it does
not build a parse tree because the user
can do whatever they want with their
actions and it actually allows you to
generate parsers in several languages
the main 1i uses of course is generating
parses in C++ but Wes weimer another
grad student has insisted you know he's
got a right Stefano camel so went ahead
and hacked in a no camel back end and
then somebody's working in a c-sharp
back end so it can generate parshas in a
number of languages okay so now we're
talking about okay so the the ELSA
parser is I illustrated before its main
way
handles parsing is using using glr but
actually has a number of other features
that make it extensible it handles a
number of different dialects NC C++ new
extensions but then even including
things like K NRC and it does this by
actually using its own extension
mechanisms so there's a base description
that is an c c++ and then extensions for
each of the dialects that it knows how
to parse and it does this through
extensible components throughout the the
tool chain from the lexer all the way
through the type checker ok so the ELSA
parser is sort of a classic flow we
start with the pre process source parse
that it likes that into a token stream
the parser generates possibly ambiguous
ast type checker then resolves the
ambiguities and also annotates things
like the type computed types of
expressions and then some post
processing can be done crucially there's
no lecture feedback hack all the
information flow is strictly from left
to right so the lexer is just a
straightforward flex lexer this is just
written probably very familiar with this
you just have a notation for writing a
regular expression and then classify the
tokens accordingly the parser is based
on elkhound its merge resolve
ambiguities in some cases directly
during parsing like with a parenthesis
dangle bracket thing and otherwise it
waits until type checking as an example
of what the the parser specification
might look like this is for handling the
angle bracket we have here a
non-terminal in blue are our keywords in
the L count input language so I'm
defining a non-terminal called template
argument its semantic value type which
they have a little things that help out
here semantic value type is template
argument star so we're just declaring
the type of the semantic value yielded
by these these actions it has
productions so a template argument can
either be the name of a type or it can
be an expression because that is
ambiguous sometimes this merge function
will be called and what this is doing is
it's building an explicit representation
of the ambiguity
and right what does it do that yeah okay
so there's something confusing about my
slide but the the way it's supposed to
be handling the ambiguity here in fact
with angle bracket is that when it
parses something is an assignment
expression it then goes and checks the
produced tree to find out whether it
actually has a nun front besides greater
than if so it'll cancel that action and
then that that's the way this so every
time one of these fires we then pass it
through this additional keep function
which lets us filter out things that we
actually don't want to keep so now I
don't know why the ambiguity is being
kept me but that was just an example
this is a way you could retain the
ambiguity later as you you connect them
together and your abstract syntax i'll
show how that works on another slide so
but the point is this looks something
like bison input but with a few extra
things like this this merge resolution
the abstract syntax is defined using a
custom language that looks vaguely
similar this in this specification is
processed by a separate tool to produce
C++ classes that implement the abstract
syntax we have names for essentially
super classes like all the kinds of
statement nodes then there are specific
kinds of statements that are listed
these are implemented as subclasses see
possible subclasses associated with the
each node of the abstract syntax you can
have various kinds of data so here every
statement has a location and so this
superclass parameter means that every
time you construct a statement you'll
have to provide a locations it's a
constructor parameter we also have a
constructor primers are specific to
specific subclasses so for example if I
want to construct a while statement I
say new s while I first pass a location
and then pass these other things and so
those are then all collected together
into a single abstract syntax node we
can also have lists and you can have
fields that are not initialized this is
a one extension beyond o camel this is
so far a lot like the union types in
oakham
but here we have a a field a data field
that's mutable and doesn't have to be
set when you first construct this thing
you can also it since these are also
generating C++ classes you can actually
have methods within them as well so then
after parsing all this this ambiguous
abstract syntax is handled to the type
checker which is just a big piece of C++
code it has five main jobs it has to
disambiguate the AST has to compute type
annotations look up symbols resolve use
of overloaded names it actually does a
bit of normalization inserting some
implicit conversions that those are
visible to later analyses and it also
will instantiate templates this is a
necessary part for parsing C++ code
because anytime you refer to a class
value template ID you have to then look
inside of it to find out what the names
of various symbols are the types of the
symbols inside of it yeah so what I says
we don't build a parse tree right so we
don't have there's not an explicit
one-to-one correspondence between the
grammar and the syntax that's generated
but there is between the abstract syntax
and the resulting and that is a fact
there's a factor of 10 difference in
size that's why it's important if you
actually build a parse tree you have
major problems with locality so abstract
syntax is okay I claim okay so how does
the disambiguation actually work this is
the actual runtime representation of
ambiguity that we use so here I have a
program fragment return / NX / NY the
skin again either be a cast or in this
case a function call and so the return
has a pointer to the expression that's
being returned it's pointing to the e
cast interpretation but that has been a
pointer to the phone call interpretation
in the nit so this is actually forming a
linked list no other interpretations
here so then within the cast of course
we've got the various internal structure
there but then they both share the same
representation of x and y there's the
sharing that lets us actually represent
the the whole set that the
entire parse forest compactly because
we're actually sharing commonly
interpreted sub trees so then the type
checker simply has to take this figure
out which one to keep if it wants to
keep the cast then it'll just break the
ambiguity link it wants to keep the
function call it will switch the s
return pointer over to the function call
and then when it's done of course you'll
have just a simple tree structure the
main things else it does after the type
checker is complete it can compute a
control flow graph this is just an intro
procedural fairly straightforward it can
check over the abstract syntax to verify
invariance it will do some additional we
call lowering these are translating C++
constructs like their implicit like
constructor and destructor calls into
explicit function calls to make it
easier for program analyses and you can
print it back out as C++ if you like now
the extension mechanisms which is sort
of the point come in I'll show a sort of
at each stage this lets you add new
forms to your language I used the ELSA
parser in my PhD research to build a
program verifier and what I was doing
there was adding annotations to the
language and so I wanted to add new
syntactic constructs that would then be
parsed and type checked as the same as
the other things so I use this mechanism
for that so you can add new forms add
new abstract syntax add methods to type
check them and the goal is to do all
this without just directly hacking on
the artifact you started with so that
you can continue to share that with
other people or with other extensions so
the lexical extension is actually very
straightforward since this was just a
flex lecture we just syntactically Jam
the extension into the new lecture
description and so the new the new token
definitions will take precedence over
anything that comes after but now you've
got a new flex description that
recognizes the combined language the
syntactic extensions are merged by the L
count tool itself if I have these two
files this is the base description for
the ansi c c++ and then here's one for
new which has the question colon
operator the binary form of that these
are then just Union together so the
effective conditional
erm simply has all three of these
productions together and it will just
parse using all three of them the
abstract syntax is extended in basically
the same way the tool that reason this
knows how to combine these so here I've
got the new statement function of
function nested inside of another
function just as another possible piece
of abstract syntax another alternative
for a statement and then finally when
you want to actually type check these
you just use the just write T check
functions for them t check itself was
declared as a virtual function on the
abstract syntax node and so then you
provide a definition for atty check for
the new nodes and then the usual C++
virtual dispatch will make sure that
everything gets called the way they
supposed to ok so the the main features
of Elsa the parser it has this
extensible input language as i described
handles most of the C++ language the two
things that are just not implemented at
all our template template parameters
this is passing one the name of a
template uninst an shaded as an argument
to another template and the export
keyword which no one understands has
about a thousand test files I claim it's
reasonably well documented and it's
actually fairly fast though the original
goal wasn't actually to be all that fast
it's about thirty percent slower than
GCC too and much faster than GCC three
and i have not compared to GCC for ok so
now switching to the third and final
part is the the program analysis built
on top of this and here are mostly
presenting other people's work these are
tools that have been mainly a tool that
has been built on top of Elsa seek wall
was a tool to analyze C programs using a
qualifier flow paradigm that I'll
describe in a moment seemed cual + + is
the new version of that built on top of
the ELSA front-end the previous version
is built on top of the GCC front end
there's a constraint solving back end
that sequel uses called banshee that are
not going to talk much about and what
you can do with sequel specifications is
mainly sort of data flow kinds of
problems you can define flow sources and
sinks and then see if you know data from
a particular source can reach a
particular sink this can be done to do
sort of a tainting analysis that's the
main application but you can all it was
also used to do some stuff for y2k and a
few other more esoteric examples so the
basic idea with this this data flow
style analysis is that and this fits in
with a notion of extensibility because
you're coming in and and you're defining
what are the flow sources and sinks of
interest is that you have something
that's a source like here the get end
and since this is under control of the
user we say it's returning a pointer to
something that's tainted and then we
have some other piece of the system that
we are going to regard is requiring
untainted input because printf first
argument is as a printer that acts like
an interpreter and so if you allow it to
take on tainted input then it's going to
just run that program effectively and so
when sea cual will do is it will just
follow through the program making sure
that everything that's marked hated then
flows to all the places it can go the
return of get end is tainted this then
taints X this flows into the printf
arguments which is now flowing to
untainted and so we detect a bug so this
is the basic framework in which steep
wall operates you define your sources
and sinks and your blockers and this
produces a program analysis as I just
said the specifications are primarily
the sources and sinks we also need these
flow blockers things that are
effectively sanitize the input so if you
have something that like looks for
metacharacters in the language that
you're passing it to then that would
constitute a blocker this is mostly work
by jeff foster whose at maryland so for
tainting typical sources are things that
come from the user or the network sinks
are interpreters like printf system or
SQL execute they just said the blockers
were things that check for
metacharacters another example of a
similar analysis is in linux there's
this notion of user pointers versus
colonel pointers user pointers come in
through a system call but they the user
can potentially make them point2 to
various places they shouldn't and these
must first be checked to make sure that
they are in fact pointing into space
that that that program owns and at that
point they become sort of fully
validated colonel pointers and so we
need to make sure that these aren't
being
mix together we also need to make sure
that user uncheck user pointers aren't
being dereferenced so the main sources
are the system call arguments the main
sync is the DRF and these there's
various checking functions within Linux
that will validate a pointer and so from
Rob Johnson we have some results of
using this to analyze Linux this found
13 distinct bugs of this kind some
additional i guess it was different
properties as because that says 17
different security vulnerabilities this
technique found to found bugs that were
missed by manual inspection and other
automatic tools most of them were
exploitable one sort of interesting side
note is that there seemed to be this
notion of bug churn that in fact bugs
were being introduced at almost the same
rate they were being fixed which is sort
of a funny cultural thing I guess that's
another example yet this was actually
used to to find a y2k bug in the RCS I
think basically just finding any place
where a year variable could flow into
something that couldn't represent the
full precision the main features of C
sequel + + are that it does
interprocedural data flow it is control
flow insensitive and there's been some
work to try to add flow sensitivity like
being able to do protocol checkers like
open followed by close so far this has
been a problem this has not worked out
as well as it could jeff has an
interesting paper I forget exactly where
it was maybe pldi where I used the c99
restrict keyword to actually be able to
pick out pieces of the program say that
this part doesn't have aliasing problems
and that could be automatically checked
and as a result sort of you know so you
have an equivalence class of objects in
one state using restrict you could pull
it out let it transition through states
and then once they return back to its
nominal state it could be put back into
the collection using the restrict
keyword but this is somebody else's
research and not that familiar with it
okay so finally sort of continuing the
theme of X
I want to sort of mention a product that
i've been working on the Coverity which
is a new version of extend the Coverity
prevent system looks for bugs in very
source code extend is a tool that allows
users to actually write their own
checkers this one though it is
interrupter cedral is flow sensitive and
so it is good for doing protocol
checking and we're actually developing a
next generation version of this that may
be best based on the ELSA front end that
will allow interpret intra
interprocedural checking simplify the
language somewhat with the goal actually
being that we could actually write
specifications for bugs that would be
independent of the tool used to analyze
them this would be kind of the goal
really is to be able to say okay here's
what it means to you know have a
painting bug and then any tool could
come along conceivably and use this
specification and there's just a
question of sort of how fast and how
thoroughly the tool can execute that
spec so sister to to wrap up since again
the theme here is extensibility and what
just want to emphasize it involves both
analysis and language design together
that the language needs to support
allowing the user to put in their
specific the properties of interest and
then the tool needs to really work with
that and just to point out one element
of that with elkhound context-free it
accepts context-free grammars the whole
language of context-free grammars
there's no subset of context-free
grammars that's closed under composition
besides context-free grammars themselves
so if you want to be able to compose
arbitrary extension mechanisms as
grammars it has to accept the full set
of context-free grammars lalr for
example is not and so the sort of the I
don't know whatever the warm fuzzy
closing thing is that I have some hope
sort of believing in static analysis
that at some day it will be as a routine
for programmers to actually write
checker static analysis checkers as it
is today that they write test cases so
they're really trying to push bug
finding much closer to when the bugs are
actually read that's it some URLs yeah
yeah all this is open source available
well I guess the Coverity is not but all
of these are open source elson elkhound
are available under a bsd license sequel
I think right now is still GPL but all
of them are available for download at
the respective sites on this slide yeah
template template parameters are fairly
easy you know a week or two of work
probably they're not very often used
because there's this rebind idiom that
actually most people use instead and
works without them export keyword well I
don't really understand it very well so
i'm not sure i can even speculate on
exactly what's required they're also
only tries to be within one translation
unit so it's conceivable that depending
on the scope of the project export
actually doesn't matter but the truth is
there are actually some other just kind
of bugs it Elsa right now accepts all of
Debian but there are still plenty of
examples we have that don't go through
Elsa and so just kind of say what well
there's a there's a fair amount there's
key EE there's Mozilla there's cutie
stuff like that and so there are
certainly still bugs and Elsa and it's
really hard to speculate on how much
effort it takes to get rid of all of
them but typically if you start with a
project that's you know maybe a million
lines of code you can get it to go
through Elsa within about a week
something like that yeah
the parsing error messages are pretty
bad the GL count when it encounters a
basically when all the parsers die i'll
count just aborts it tells you
essentially what it was expecting but
otherwise it bail so that's not very
friendly I thought for a long time even
for years now to go ahead and had a
narrow recovery algorithm that Burke and
Fisher proposed many years ago but I
never got around to that so the parsing
error messages are actually pretty bad
but if you get through the parser the
type checking error message is actually
quite good because it has all the
possible interpretations so one place
where this comes up is that a lot of C++
programmers are not very familiar with
how to use the type name keyword and so
if it's missing for example GCC will
give you just an undecipherable message
but elkhound will just tell you right
here you know you should have or should
not have used type name because it's
able to see the whole thing and on
almost all cases it figures it knows
which part you meant and you can just
you know and then type names to L count
it's just a check it does after the fact
it doesn't actually use it for
disambiguation so for some things it's
actually very good but up until the
parser it there are messages report yeah
mussels book on SPF has some examples
and I found really hard to come by the
banana co-pilot
I would not be surprised that there's a
lot of crazy stuff you can do with
templates in principle all the basic
functionality is there like you can
write a program that will compute all
the primes that works in Elsa but the
there are lots of the name lookup rules
for templates especially with the two
things look up and what some compilers
implement versus other compilers
implement are awfully difficult to get
right I still don't thoroughly
understand them so I would not be
surprised at all to find that there are
textbook examples that don't work but
like I say it typically given a
particular example it doesn't take that
much work to actually fix whatever
problem may be there but I certainly
couldn't make any promises about what
was the new book David mustn't mall
sorry I see i have not no okay i'll try
yes okay okay yeah so again what's the
advantage of using enough Coverity well
it's yes so why use Coverity when all
this other stuff is free i well the the
main advantages of the Coverity engine
actually are the scalability of the
interprocedural analysis what sequel
sequel will do flow properties in a
reasonably scalable way but it can't do
flow sensitive protocol checking in an
effective way at this point and it's
unclear when that will happen so the
thing that Cove area does very well is
finding cases where you have protocol
violations that cross procedure
boundaries so use after free memory leak
various kinds of locking bugs these are
the things that are sort of the bread
and butter for Coverity and so it's
there it's there an analysis that does
the the you know procedural protocol
checking that's the main strength of
Coverity stuff yeah
we don't we use different types of
procedural stuff here we use a lot of
voters and so the traditional analysis
right
so are there so the question has to do
with the use of closures and
interprocedural analysis closures are
not on the top of the list of things to
deal with in or procedurally at Coverity
at least i think it's it's probably not
infeasible but it's just not a priority
right now in there I mean the the usual
approach with Coverity essentially is to
define what what's common and try to
adapt the analysis to do that it's a
it's always sort of a race to to figure
out what what are the idioms that the
programmers are actually using and how
best can we understand them and since I
mean it sort of the luck of the draw of
having having chosen a technique that's
not not common it's it's difficult to
write program analysis to deal with
closures and as long as no one else is
doing it very much then it's unlikely
Coverity will either no I agree it's not
possible at all it's just you know it's
it's cost benefit you know do you spend
time improving the analysis foreclosures
or do you spend time you know adding
another kind of protocol or working on a
new language to define these tigers yes
but I would assume that that
preclude the ability to accept to the
closures for is if you had direct access
to a covariant machine using
so the question has to do with the new
version of extend handling closures it's
it's I would say the idea is not
sufficiently baked yet that I could
actually say it precludes closures but
it is not on the radar of things to
actually go for what you say is quite
plausible that in fact it will not
provide sufficient functionality to deal
with closures and that if you had more
direct access to the Coverity engineer
could having brought it up I'll think
about what it would take to make that
possible but like I say it's not likely
to be the high priority item
okay thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>