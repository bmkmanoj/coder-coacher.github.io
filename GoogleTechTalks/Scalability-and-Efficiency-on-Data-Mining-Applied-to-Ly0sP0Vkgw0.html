<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Scalability and Efficiency on Data Mining Applied to... | Coder Coacher - Coaching Coders</title><meta content="Scalability and Efficiency on Data Mining Applied to... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Scalability and Efficiency on Data Mining Applied to...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Ly0sP0Vkgw0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hi everybody thanks for coming sorry for
the delay and anyhow it's my pleasure to
introduce you Wagner mayor Wagner is a
professor at the federal university of
inertia rise it's in the same serie have
a Google office in Brazil the
engineering office in Belo Horizonte he
has a PhD in Rochester and has been
working in parallel systems and data
mining using parallel infrastructure and
it's always nice to have him around I
always learn new and cool stuff for once
I've just learned that I should maybe
take more time thinking of my idea of
installing linux in my notebook so
thanks Ragnar because i have i'm having
hello hello yep ok so uh thanks for
coming and it's my pleasure to be here
and i'll try to show you some of the
work we have been doing there not just
on the data mining side without mostly
owned in the parallel computing side ok
so i believe that this is this is quite
well known here it won't take much time
here in terms of ok so we do have
several open problems in terms of
internet applications they are they
still open and in some sense they are
getting worse and worse by my time ok so
and then these are some of the problems
data miner believe everybody also knows
this is a this is a general presentation
and one thing that's important to note
is like there are still several open
problems in terms of data mining off or
what you have to search our patterns to
search and so on among those problems we
we have some application scenarios that
are very similar very related to
Internet in general okay and these these
are the ones i will use as a as a
background scenario for for our
discussion here today so the first one
is the spam detection i guess it's a
widely known
problem and the point in a spam
detection that makes it a bit harder is
how it evolves across time so recently
we have a new search of a PDF basically
spam dissemination and then since these
patterns they change from time to time
developing techniques that are able to
handle such evolution is not an easy
task ok we have document classification
where a classic up like demand a
classical application for information
retrieval in data mining and again we
can also be served at across time we do
have quite an evolution in those
patterns customer behavior
characterization is the same thing in
practice we we look at more at ebay data
and how to detect and formalize what are
the interests of the customers and even
information search in social networks or
what has been lately quite a challenge
for for search engines in general so
most of the blogosphere is not be really
being method well by the search engines
and they have quite a hard time finding
things there okay one thing that's a
several factors are common are among
those applications and these are the
background scenario we are going to use
the first one is that as those
applications evolve and and we the
information remains also increase we we
can see from the data mining perspective
more and more complex patterns to be
minded several of those patterns
furnaces there is a quite a large for
nowadays towards graph basic patterns to
be minded and in in fact there is not
really a consensus on what are the
patterns and how they are they should be
minded or even how to handle the
complexity that comes when you try to
mind such patterns in parallel we do
have quite
a quite significant evolution of those
patterns so even if we are able today of
finding what are the interesting
patterns for instance to do mine
information from orkut probably in one
month there will be several different
patterns or behaviors that may arise
there in the end moving global need new
techniques or at least to enhance the
existing techniques to grab it we are
it's it's it's I should emphasize that
we are still one one step behind in the
sense that we for several of the demands
we still don't know what to do in terms
of extract information from from places
like are cut and we do have large
volumes of it and this is this is also
quite challenging I believe that you
understand this quite well and in this
sense having techniques that are
scalable and efficient is quite is quite
a challenge as well so what it's
important to see that we we have in
creator of techniques and applications
we have techniques that are focused on
each of these three dimensions but it's
usually hard to find techniques that are
like that deal with the three dimensions
altogether okay so either the complexity
of the patterns or how they evolve
across time or being really scalable and
then this is basically our background
scenario here so how how is your
standard modus operandi in this case so
we start by detecting information
demands like these i just presented in
discuss it and then we grab real data so
most of our work is based on real data
we collected in several places we
evaluate techniques research novel
techniques and then we work on the
scalability and efficiency and validate
through case studies real case studies
we do a support a platform we developed
in brazil that's called and theater and
it's being used by several government
and
other institutions for mining data there
and this is a separate afford but it's
one of our motivations for pushing this
so this is basically an overview of my
presentation and what I focus more is on
the scalability and efficiency part and
not on the novel techniques who have we
have been working on in terms of data
mining okay and I also won't talk much
about the case studies or the hub the
other parts that we perform there are
quite standard in this scenario so
focusing on the scalability and
efficiency we we just saw that we still
have a continuous demand for novel
techniques and we like we we have a
scenario that's very clear that the
computational domain tends to increase
regardless the just a large volume of
data but because of the complexity of
those techniques most not most but many
of the problems we do have in this
scenario they are well known as NEP hard
and NP complete so we don't really have
like efficient solutions to them and we
usually adopt a heuristics towards
solving them and this is this is quite a
challenge when we look at the
dissemination of multi-core platforms
and how they are going to increase then
we are there is also open like a an
opportunity or a challenge for an new
level of abstraction in terms of
parallel programming end up in this
scenario how to ensure scalability and
officials okay so that's that's or
strategy well as expected we we are
looking towards pilot license these
algorithms okay and there's two
characteristics that are key to most of
the algorithms in this data mining
scenario also applied to those internet
applications the first one is that
usually those algorithms they are
fighting things in terms of both
cpu and I oh and this is not really
usual for the parallel computing
community for years they have been
working on on motto on scientific
kernels and parallelizing these
scientific journals that are very
intensive from the computational
perspective but not very intensive from
their perspective there has been in the
last 15 years several efforts towards
day I yo like I scalable IO initiative
and so on but at that point they were
very concerned about the volume of data
and nothing process in ena in a very
intense processing of such volume of
data and this is something that's quite
new to the to the parallel computing
community in general the second thing
and these are also bad news considering
the deforming work we have most of the
algorithms are irregular completely
irregular completely data dependent or
inputs dependent so most of the the
parallel the parallel compilers
parallelization compiler techniques are
not applicable in this scenario okay so
we do have a lack of techniques and
centers that may be applied here and
when we put those these two things
together achieving scalability and the
officiants in such scenario is quite a
challenge so it's not really a usual
that you you are able to to paralyze a
technique and make it scale to the tens
or hundreds of processors easily okay so
that that that's basically our whole
Grail what we want is how to paralyze
such techniques in the hundreds of
processors and keep efficiencies above
eighty ninety percent and so on okay
then we we have been working this
paradigm for the last four years and
recall with the anthill partner in fact
this somehow comes from the the initial
proposal of the anteater environment and
it
it's where we it's the kind of a runtime
environment where we implement the
techniques there is not nothing really a
in the sense a completely red called new
in the in the paradigm but I believe
that they approach that show to be quite
efficient and provided good scalable
applications and has been applied to
several applications i will show later
it may be the best contribution of the
the paradigm in the environment itself
so the idea is like to decompose the
application into filters that
communicate through flows so this is
this not really new it's something that
that goes back to the data flows in the
60 and 70 s and obviously we are
leveraging recent advances in networking
technology and this stuff end up by
doing these the interesting part is that
we are able to exploit three types of
parallelism in our applications and the
end and they are the data parallelism in
the sense that our filters we are able
to instantiate this transparent copies
of the filters that automatically break
the data or the partition the data among
the processes that are participating
this communication the test parallelism
comes into the filters in the sense that
a filter it has a very concise and
objective goal in terms of processing
and end by by emphasizing these we do
have kind of flexibility in terms where
we place the filters and what kind of
computation they perform and we do also
exploit a believe there's a and s in
excess that we don't have a synchrony
there in the sense that we allow all the
parallelism that may be exploited to be
exploited in the sense that are
restricted just to the data dependence
graph of the applications okay one thing
that's very interesting is by by
exploiting these synchrony as I show you
later we are able to do to vary the
granularity of both task and data
release because these applications data
mining plication is most of them they
not only are very very very very regular
in the sense that each depending on the
input data they behave completely
different but also during a single
execution their profile change the sense
that they start with a one type or one
data like larger data and then they
finish with more tasks and each desk
handling last data so to couple with
this this variability is having their
synchrony we are able to accommodate all
the the needs of those applications in
order to achieve this this scalability
okay so let me give you an example okay
so that business leaders are a very
classical application that's the
frequent itemset mining okay so what we
want is giving before we look at this
this dependency graph let me explain the
application I in fact I had are crashing
my machine this morning another creation
they lost some of these lights so I I
prefer to explain this so what we have
is giving a transaction database we want
to determine which item sets or which
groups of items occur above a certain
frequency threshold that's called
support so in this case I'm representing
here the search space of the problem of
a database that contains four types of
items ABCD and and what we want to find
is looking at our transactions to find
exactly a which of those possible item
sets are above a given threshold so then
you see that this this search space it
increases exponentially and the end up
this is these are very popular probably
in the data mining community okay in
terms of the data dependency graph what
the problem the problem works as follows
so basically
we start from from from single items
exploiting the fact that if a single
item is not frequent none of its
descendants you'll be frequent as well
okay so it's it's a very smart approach
it was developer about 15 years ago and
it's well exploited in their community
so if a is not frequent none of a a BAC
ad ABC or ABCD will be frequent and in
this way we are able to perform a quite
efficient cut in the search space of the
problem and and perform a smaller amount
of computations in terms of the
parallelization we do have some
challenge here so the first one is like
the in terms of the large volumes of
data we do have a reduction problem that
simplicity to the implementation in the
sense that to know whether a is frequent
you should like check its frequency in
now partitions of the data set and the
second challenge is kind of a
synchronization challenging the sense
that if you have those items here and
and when you it's it's it's it's kind of
a communication problem to determine
whether you may start computing a B
because you should start computing be
just when a and B have been computing
have been computed and you know they are
frequent okay so previous approach on
parallelizing this problem they usually
they have kind of a reduction outlaw
reduction at this point and they become
quite synchronous but the point is as we
go up in this lattice the number of item
sets increase and there's the size of
the data that should be inspected
towards determining whether they are
frequent or not reduce so that's that's
exactly one example of where I just
mentioned about that the the granularity
of the application chains across the
execution
so what are these steps if you look here
okay okay page up so in some sense we
consider as tasks in our approach that
that determine whether each of those are
items sets are frequent or not so these
are our tasks or major tasks and the
what we want is is to perform this as
efficiently as possible in as this with
the largest scalability possible as well
so what should we do in each case this
is this is a that there is one of the
approach that's called the equivalence
class approach and it's based on
candidacy so what's the idea if a and B
are frequent then a B is a potential
frequent itemset it's a maybe ac and BC
are frequent then ABC may be frequent
and then the first thing is finding the
candidates then we count the support for
each candidate and then we do the
reduction calculate whether it is
frequent or not there are other approach
that that don't use candidates but I
won't talk about them now so if this is
simpler to understand then the other
approach so we do have three major tasks
that are listed here just to emphasize
ok so this candidate enumeration that's
not expensive because basically what we
need to know to do candidates
enumeration is which item sets were
found to be frequent so far ok the
second one is knowing that we have to
check a given candidate then we then we
go to all data partitions and actually
check in how what's the frequency of
data item set in each partition in
finally we have to check out this to
some all these partial counts so that we
are able to all the partial counts so
that we are able to
to calculate the overall count and see
the item set is frequent or not okay I
just make a illustration here of how it
works so we calculate the candidates
send the candidates the counter and then
the counter what verify the support and
one interesting thing that's not here
because it got miss it in my slide
scratch it's like when you know that an
item set is is support is frequent you
sent back to this cell to this filter
somehow so that it may like enable other
candidates to be counted so it's
basically a cyclic approach why am I fo
sizing this here as as you might have
noticed at least the people who are if
there isn't one involved in this now
several of the things I'm talking here
they are very similar to MapReduce okay
and that's maybe the main difference
between at least when we check at
MapReduce some time ago is that we in a
produced you don't have this ability of
iterate and make cycles in the india in
the in the photographs that's one major
difference here in the sense that we do
is store data in each filter and we keep
state so that this application is not
stateless the application becomes a
stateful and do we allow all kinds of
states to be supported in this case
obviously degenerates are other problems
that we must handle and i'll talk about
this later okay go go okay so in terms
of the parallelization we exploit being
more specific to the nation's okay so
first we partition the data set as
expected so we have like several filters
several counting filters where each
filter has a partition of the database
and we have the item set partitioning in
the sense that we have filters that are
responsible for particular item sets
that's that's also a difference in terms
of the complexity of the applications in
this case we have
just two dimensions but you have we have
seen cases where you have four or five
dimensions differently and and and one
thing that we always exploit is trying
to keep the scope of a filter to a
single dimension in this case we are
able to do a much easier partitioning
than in other case okay and we we do
exploit parallelization both dimensions
in fact to the time we publish this work
this was also novel because all they
strategies so far they they focus on on
like partitioning either the candidates
are either the transactions or the item
sets and we did both and both are
partitioning in this case okay so that's
that's the overall view in the sense
that you you do we we merged the
candidate filter with the with the
counting filter and we have like the
item set filter over there and we have
like a continuous flow between these two
filters it's interesting that when we
know that an item set is frequenting the
in the upper set of filters then we tell
this filter that ok this is frequent
since their item set candidacy is a
cheap operation then this guy
immediately broadcast to the other
people in the sense ok so we start
counting these these filters these item
sets and as soon as we learn that these
item sets are frequent we already
notified the other filter that that's
basically the notion of a synchronous
consultancy for the earlier slide shows
that no that's basically the notion of a
synchrony in the sense that we as soon
as we learn that an item set is frequent
we let the other filter know about that
and then we may start other computations
why are you are still finishing that
counting so we do have an additional
level of parallelism there
so we do have like the communication
pattern between these shoe filters in
the sense that the front from the filter
has to see you you tell where like as
soon as you learn that an item set is
frequent you send information to all
other filters and the end up from this
counting filter as soon as you learn
what's the count of the item set you
send information to the reduction filter
ok and we use this this this labeled
stream that it's just a mapping
mechanism very close very similar to
map-read dukes temporally I guess both
were posed at the time we publish this
work MapReduce has just been published
as well so it's kind of a parallel
development we did for some time ok so
the labeled scream it's a it's a it's
it's basically a strategy of our way
sending data related to some piece of
information to the same place so it may
be seen as a hash function very similar
to MapReduce in the sense that we for
each label we do associate a hashing
function that we send the data always
the same instance of the filter and in
this case we do I is this a very
necessary in your case because we keep
state information in each filter and in
order to have this exploit this state
information and keep the whole
application consistent we need such
behavior but the intrinsic notion here
is like we do have a consistent path for
all the data thats related to item set a
B we will always go to the same instance
of the filter ok so what I'll do next
just any dots here did you understand
what's the catch so being a I need like
a short summer will be like we
means somehow extended a the MapReduce
approach I'll allowing you to have state
information we think the various
processes that that are included there
and do we exploit the same kind of hash
based addressing between the various
processes so what I present next are
like this this application was one of
the initial ones and after that we
perform at paralyzation of several other
applications another one thing that we
trying to understand is how the
characteristics of the data dependence
graph affect the parallelization or how
difficult or easier it becomes okay so
we we divided the several applications
according to the data bit dependence
graph using three criteria the first one
is regularity so we do have some
applications that are regularly they
have been paralyzed a classic example is
kaylee's ok so the clustering algorithm
that we basically what you have to do is
you do a reduction to determine the
sense right and then you broadcast the
centroid to everybody so that they
calculate the next iteration so it
basically it does not depend on the
nature of the data so basically just if
you do always the same thing and it's
very regular regardless the input ok the
second one is what we call dynamic city
I don't know if this is the proper word
but anyway and the point that there are
several applications that you don't know
the dependences prior to execution the
execution okay and for instance in the
case of the a priori the item set mining
we just saw you do know the lattice is
determinate depending on which are the
items that a core inside the data set
but if you pick for instance a decision
trees you don't know in advance who is
going to depend on who in terms of the
partitions that are being defined
because the partitions are define it
during the execution so the penya on
this dynamic city this made me like this
makes very hard for instance to have a
hashing functions in advance in finding
optimal hashing functions that you
balance the load in advance because you
never know exactly to where the data
will be sent next and you learn just
during the execution okay and then we
have this this last property that's
called monotonicity and it's basically
the fall we found some applications that
not only the the dependence graph
changed across this you learn the
dependence graph across the execution
but also it changed during the execution
so one classical case of this is a
categorical clustering cobweb for
instance that they build a hierarchy and
they learn about the what are the
discriminant of this hierarchy but
during the execution you have splits and
joints so I mean for some time you have
a given node that to each you have to
send data and then suddenly this node
vanishes so that's what we call mon on
it in city in the case the difference
between the issue is that in decision
trees you don't know in advance what are
the discriminant send what will be the
partitions but once you have the
partition it's stable in the last case
if you don't have a monotonic
application if you have a partition it
may change and for later in the
execution doesn't exist anymore so this
gives you a quite a problem in terms of
implementation so these are some of our
results so we do have a scenario of very
was like applications usually very
assist other applications where we see
the three criteria just nation instead
of dynamically type i put kind of a
static behavior so it's the reverse here
there s oh i forgot to mention this ok
this is this one bob s is yes ok sorry
for that and then though is no I like
this one of the slides I have to fix
before coming here and I forgot to to
change the ass by why so we do have some
applications that are very regular such
as k-means and this is a image analysis
application placenta ok and we exhibit
quite good efficiency in both
applications we do have the application
we just mentioned that's the April you
re the frequent itemset mining again we
got pretty good efficiency and and then
we have some applications this is the
the decision tree ok where we yeah well
we are not regular nor dynamic and
ending this case is it's like it's a bit
less but they still are quite good
efficiency they don't have the one of
the most challenged applications we work
it so far and this this application in
fact this this application here it maps
to to to something that should be quite
challenging that's going out doing a
depth-first search in parallel in these
quite complex graphs that arrives from
the three cluster 03 cluster maybe you
are aware of the bike lesser problem
where we went to closing two dimensions
in the three cluster case we want to
close in three dimensions we re at the
time dimension to the clustering and so
it basically becomes a it's a for each
time is locked you have a depth-first
search in a graph to find the x clusters
and then you have to find common by
clusters across time that's the trick
cluster
so maybe this is the worst result we
have here there is the cobweb the result
i just mentioned to where we have splits
and joints and we got a pretty good
result as well there is one last
application that i had put before and
it's a it's a different identity
resolution we did are quite a large
entity resolution and using our
probabilistic record linkage and we also
achieved something line 95 percent of
efficiency okay i'm almost finishing so
maybe that's the something that may be
interesting to discuss here okay so what
are you doing now so what are your
current efforts in this case we are we
are really discussing how these like
filters and flows and so on how do they
we are live with boutique oh okay so we
this this is this is a discussion we are
having we are not sure yet what's the
response the answer to that okay we do
have someone going efforts in scheduling
and in this case one general problem we
are looking at is the variable latency
this variable latest it may arise from
simple things such as like just having
machines in in different networks to
grids and now back to the multi-core so
one one important thing is how to couple
with this variable latency in terms of
these communications although a
synchrony gives at least a new shop
scenario to do such thing and one thing
that that we also become very important
in two haven't look much to now is how
to exploit reference locality in
particular in these irregular
applications so we it's not necessary
that you are completely strict in
exploiting this reference locality but
it's interesting that we we do as best
as possible in order to make the
application more efficient okay then we
have the dynamic configuration
or rare configuration of applications
because they change their profile they
change that the computational demands
and one one thing that we are at
measures that we have to come up with
scheduling strategies that evolve across
time in the sense that we can just
choose our scheduling strategy in
advance and and we have to take into
account for instance the reference
locality and one thing that we are also
working is what we call the automatic
partitioning in this case is not data
partition is gold partitioning so we
want to give in an application to break
it it into the filters and schedule the
filters to the processors in actually
looking for maximum efficiency okay and
in this case there is another dimension
this automatic partition that we want to
exploit is how to make applications to
to leverage common kernels of processing
among them okay so we did find several
of those data mining applications they
share common operations for it's a very
common operation this set of
applications we have here is joints
between lists and end counts so these
are two common things since one one of
our main goal here is to not move data
but just move metadata and summaries
about the data so this becomes a very
important thing okay and also within
this automatically partitioning there is
one thing that we we also did not work
much but now we have some demands is how
to deal with this trade-off between
replication in partition okay for
instance the a priori application we did
partition the whole data space so no
transactions are replicated
I mean more than one like they are
replicated in two or more instances of
the filters obviously there is a
trade-off there in terms of
communication versus storage versus the
overhead of maintaining this replication
information and this is another thing we
are wandering about discussing okay that
that's something we have been working on
somehow in practice when we did this
last application of the entity
resolution we did really face the
problem of your application in that case
okay and we had an initial
implementation but it's not generalized
yet that's it thank you very much for
your attention I hope I added something
to your daily activities
so I have a question how do you guys
deal with fault tolerance because the
whole problem of keeping estate is you
know if a node fails how do you recover
from that okay there is a reason mastic
this is where we did the first cut on
this okay so basically we employed
traditional replication techniques and
and then we also did like the main
problem is like once you realize that
the application failed you stop the
application you find where to put the
problem to execute which nodes are
really active and then you restart the
application from the last stable point
one thing that's that's interesting to
to mention it's like when we look at for
instance so I guess my curiosity is more
like how expensive what's the overhead
of keeping those stable point it varies
a lot it varies a lot for instance in
this case well what we had is like we we
keep just the alkane set count for each
item set the global one and from that we
have like a border from which we start
in case of a crash okay there is one
thing that's very complicated in this
case for sake of fault tolerance is that
these applications they are they have
very fine granularity okay so this is
this is even a war for sake of fault
tolerance because if we do have a fault
and you have are quite fine graining
applications the amount of debt you have
to keep is may be quite large but the
point is the the size of the state it
varies a lot okay from very very small
like state sites such as this this this
item set count which is pretty small
to having to restore larger like
portions of the transaction part that's
expensive part so your scheme you are
assuming that the application developer
helps you somehow to to determine what
they state that needs to be saved that's
not automatic yeah yeah that's true
that's true in fact what this interface
who have developed for 44 fault
tolerance we created this notion of it
was not really state I just missed the
worry the user has to create kind of a
topic space where the programming puts
whatever it thinks it's necessary in
this topic space and we also created
kind of a function recover function that
you get information from the top of
space and restart the application okay
and then we we believe that this type of
space is transparent to everybody okay
transparent in the sense that it's
disseminated to all processors so it's a
different part of the system we just
deposit think they're things there and
then we retrieve whenever necessary okay
and this is this is a different problem
that we are not tackling on in the sense
that okay you do have this tape of a
stable stable storage that's the name
you use so we have this stable storage
and we don't care how you are
implementing stable storage when my
algorithm Azul display it is a stable
storage for sake of recover more
questions okay so let me think Wagner
once more ok thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>