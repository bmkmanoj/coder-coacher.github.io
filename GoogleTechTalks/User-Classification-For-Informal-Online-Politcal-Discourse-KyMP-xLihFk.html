<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>User Classification For Informal Online Politcal Discourse | Coder Coacher - Coaching Coders</title><meta content="User Classification For Informal Online Politcal Discourse - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>User Classification For Informal Online Politcal Discourse</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KyMP-xLihFk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay so by way of a little background
just point out something that's that's
so obvious even Time magazine is on to
it but there's been a dramatic change in
a lot of the stuff that's happening on
the web these days so whereas five years
ago or so most of the stuff that you
found on the web ever a lot of it was
professionally edited text newspapers
magazines corporate stuff academic prose
even amateur produced sites often had
sort of in rough outlines the pattern of
a professionally produced site people
were writing texts for an anonymous
audience the author was taking a the
stance of an authority in producing
things for a mass audience that shared
relatively little context perhaps except
a very general public context with the
author now much more of what happens on
the web is interactive we have blogs and
wiki's and discussion forums and chat
rooms and social networking sites and
things where the point of the behavior
on the web isn't to produce a text the
text is a byproduct the main point of
being on the web is the interaction and
the text is only a secondary thing now
these different their course big
differences between chat rooms and
wiki's but in a lot of cases the the the
real point of being on the web is is
it's serving more of a social function
in the text is just a secondary part of
it so this is obvious what's may be less
obvious though is the consequences this
has for natural language processing
working with web text so for instance
this is maybe this sort of thing that
sentiment analysis or question answering
systems are have been designed to deal
with something like this this is an
editorial from a small-town Texas
newspaper we know it's an editorial
because it says editorial at the top in
boldface right and what it's what's it
about it's about minimum wage we know
that because it says minimum wage at the
top we know who's responsible for it the
editors of the lufkin Daily News we can
track down what the lufkin daily news is
its small-town newspaper we know
something about that we know what
editors are we know what editorials are
we got a date here and then
the the article the editorial sets off
to construct an argument about the
minimum wage several paragraphs the
first paragraph presents a bunch of
facts right things like the minimum wage
has been set at fifteen dollars an hour
since 1997 right and this is this chain
of facts which are which are put forth
to construct an argument about the
minimum wage now it's not that false
things never get printed in newspapers
but for the most part we can assume that
an editorial in a newspaper won't print
a boldface easily verified lie right if
they say that the minimum wage has been
set at 5 15 an hour since 1997 that's
probably true and the the article fall
was kind of a stereotype structure of
constructing this argument that the
minimum wage should be raised now we can
compare that to discussions about the
minimum wage on online sites devoted to
political debates all right so here are
four posts that I pulled out from a
discussion of the minimum wage each one
of them is constructing an argument that
at least in context can be interpreted
as an argument about whether or not the
minimum wage should be raised but the
kinds of techniques that we could use
for analyzing the arguments in that
editorial are not going to work very
well for these kinds of texts for one
thing we have no idea who the authors
are we know that the authors of the
editorial are the editors of a newspaper
we don't know who these people are right
they could be political science
professors they could be lunatics using
a public terminal in a library they may
be eight years old they may all be the
same person we have no idea of that kind
of background of what they are so in the
first one we've got somebody saying if
inflation won't happen with minimum wage
increases and why not put minimum wage
to one hundred dollars an hour well that
has the form of an argument for raising
the minimum wage but of course we'd
interpret that as an argument against
raising the minimum wage raise making
this sarcastic claim that raising the
minimum wage won't have any negative
consequences and there's all kinds of
idiosyncratic stuff in there you've got
these these web isms like LOL you got
the dollar sign after the hundred there
affect prices right through these little
kinds of things in there that you
wouldn't find
in professionally edited text right
there because irregularities the next
one what's the matter Rockhead you don't
want to help the poor needy people you
rich bastard kool-aid drinker so
Rockhead here is the screen name of one
of the participants right so this is
being directed to one of the other
participants in context I think what's
going on here is that this person by
parodying the kind of argument you might
make against Rockhead in rockheads
defense of keeping the minimum wage low
he is in fact defending rock head in the
community but that's not at all obvious
from what's being said right is you have
to kind of interpret that based on the
context the next one that was pretty
stupid man honestly grow up well this is
a pointed argument against one of the
other participants but unless you know
what the context is you have really no
idea what this person is saying they're
either making an argument in favor of
raising the minimum wage or lower if do
you have to know what there is is a
response to and you can't tell just from
looking at the post what is a response
to you have to really know something
about the community in the structure of
the debate the next one I don't have a
kid brother and that's the entirety of
the post I originally pulled out as an
example of a complete non sequitur
although it turns out it's not a non
sequitur much earlier in the thread
about minimum wage someone had
constructed this argument that suppose
you're working in a convenience store
and you've been working there for five
years and you've worked your way up to
nine bucks an hour and then your kid
brother starts working there at minimum
wage they raise the minimum wage so his
wages now are much closer to yours
you're going to be pretty irritated that
all that seniority has been lost right
the wage compression at the bottom is
going to make you unhappy and this guy
by saying I don't have a kid brother is
refusing to even participate in the
hypothetical situation that this person
is debating about so there is a
rhetorical point what he's saying here
but you really have to have a idea of
what the structure of the debate isn't
what the community structure is in order
to make sense of this simply looking at
I don't have a kid brother isn't going
to tell you very much of anything unlike
looking at the the editorial from the
lufkin Daily News you can see what the
structure of an argument for raising the
minimum
would look like now it's not to say that
there's no information in these
transcripts right there is in fact a lot
of information and for one thing you get
a record of the kinds of informal
interactions that are going on right
it's as if you had bugged a bar or
something you could listen to informal
public conversations yet the information
is all there available for us um
individual posts have very little
information in them you can't look at a
single post like I don't have a kid
brother and make any sense of it but if
you have thousands or millions of those
posts and their context then you can
start to get somewhere with it there are
all sorts of applications for this one
obvious one is in marketing right using
data mined from these sorts of
discussions as an alternative to
something addition to more conventional
market research work like polls and
surveys and focus groups right so you
can go out and look at these sorts of
texts and see what's going on what are
the issues that people are talking about
what are the rumors that are going
around what do people care about and you
can get kinds of information that you
couldn't get any other way but you can't
really do it manually it's you can't
really have as you can but it's probably
not cost effective to have marketers
reading through millions and millions of
posts to try to figure out what's going
on you really need some sort of a
statistical lever to get into this data
to draw summaries of it and most of the
work that I've done in this area has
been in to two domains looking at
informal political discussions and also
looking at discussions of
pharmaceuticals for pharmaceutical
marketing and it occurs to me that
there's a kind of an interesting
parallel between these these are two
areas where you might think marketing
would have no place right you should be
able to decide who you're going to vote
for or which medication you're going to
take on purely rational grounds and an
advertising shouldn't make any
difference right a politician is either
going to defend your interest stories or
they're not but yet there's a huge
amount of money spent on marketing in
both of these areas and it seems that
marketing is very important and yet it's
hard to measure exactly what the impact
of the market is in these areas
alright so one challenge though to using
this sort of data for market research is
that the population who participates in
these sort of things is by no means a
random sample of the population right if
you look at informal political
discussions these are people who have
chosen to spend their time arguing about
politics with strangers online that's
not I hope the typical person right it's
it's a tends to be a young male either
politically extreme or kind of playing
politically extreme person for
pharmaceutical research looking at at
people who participate in online sites
for people who suffer for various
various medical conditions they tend to
be people who are newly diagnosed or
have suffered some sort of a relapse or
worsening of their condition or
something like that people rarely post
to a support group saying I've had this
disease for 30 years but it's completely
controlled by my medication and I don't
even think about it why would they post
that right there they're off living
their lives is only if they have a
problem that they come to participate in
one of these sites so you can't just say
well forty percent of the people on some
site think George Bush is an idiot
because you don't know anything about
what those people are but if you could
segment those people if you could say
well these participants these people who
are posting they seem to be extreme
conservatives right yet they don't agree
with some of George Bush's policies well
that could be an interesting fact
because now you know something about
what kinds of people those are and you
can associate that with a opinion that's
being expressed or you could say that
these are people who are newly diagnosed
with the condition and these are the
concerns they have or these are people
whose children have just been diagnosed
with the condition and these are the
things they're asking about that's an
interesting fact other but you can't
really say something that well
thirty-five percent of the people on
this side are worried about generic
medications well you don't really know
who those people are so an important
first step for doing this any sort of
work with this is to try to figure out
who these people are to do some sort of
broad demographic classification people
guessing their age or their gender or
their disease status right whether they
are newly diagnosed with
disease or whether say their caregiver
or someone who's newly diagnosed or
perhaps their political orientation in a
political site so the experiments I'll
talk about today are aimed at trying to
guess the political orientation of
participants in informal online
political discussions and this is from a
dataset collected with Tony Mullen from
the politics.com site so we have about
five and a half million words of posts
collected from the site over about a six
month period I think there are four
hundred and eight distinct users that
showed up in this data set most of those
users in signing up for this site gave
themselves a self political description
so they gave themselves a label right
which we can then use to see if we can
predict what that label is from the
content of their posts right so we have
an idea of what they at least describe
themselves as politically of those 408
users nineteen percent of them posted
exactly once during our sample period
right and the most prolific poster was
responsible for 6,800 posts are almost
ten percent of the entire sample was one
guy the second most prolific poster had
about half that number in fact this is
the same pattern I've seen in every
single online discussion if you get one
of these right for posting distribution
you get this sort of the same sorts of
problems that zips law give us for any
sort of lexical analysis we get the same
thing here we get a huge number of
posters we have very little data about
and a small number of posters who seem
to really overwhelm the discussion I
don't really know why write these these
sites that have looked at have really
different social character and yet they
all have this in common and there must
there's perhaps of different reasons why
this happens in different sites but
anyone who's ever participated in any
sort of online discussion knows that
there are always a few people who feel
compelled to respond to everything even
if the response is I don't know I've
never thought about that before and
those seem to be the people who really
kind of dominate the number of posts and
also are very important in determining
what the character of the discussion is
okay so here's a breakdown of the the
political orientations that we've got
people could describe themselves
politically any way they want it wasn't
a multiple choice question so what we
did was we took the their self
description then classified them in
various ways to say republican or
conservative or right fringe centrists
and independence Democrats liberals left
fringe libertarians greens and then
people who either chose not to give an
affiliation or gave an affiliation like
Martian that we couldn't really make any
sense of right and then four things
these are fairly fine-grained
classifications we also sort of bucketed
these into a right center left and then
people who don't really quite fit into
the right center left pattern and in the
experiments I'll talk about today we
focus just on right and left right can
by looking at somebody's posts can we
tell whether their self political
description would be right or left so
one way to proceed with this is to
consider it like a kind of sentiment
analysis right and sentiment analysis is
a very simple one-dimensional summary of
a text or a collection of texts usually
trying to say say looking at product
descriptions or product reviews and
trying to decide whether the general
discussion about this product is
positive or whether it's negative do we
get a thumbs-up or a thumbs-down on it
this is a very simple semantic
representation right it's basically just
positive or negative in a single
dimension and the various ways of doing
this but probably the most influential
is based on tourneys work on what he
called a PMI I our method so in this
method what you do is you take the text
you're trying to classify tag it get
part of speech tags for it and extract
all the noun phrases and then identify
the semantic orientation of each of
those noun phrases by searching for that
noun phrase in a large collection of
texts like the web for instance and then
looking for whether that target phrase
occurs in the immediate vicinity of the
word excellent or the word poor so you
look for the association between the
words you're interested in and excellent
and for and you can use point wise
mutual information for measure
during that association alright so the
point-wise mutual information is an
information-theoretic measure of the
association between two things right
whether the joint probability of the
target phrase and the word excellent how
close that is to the predicted
probability you would get if they're
actually independent right if there's no
association between the two of them so
if the point-wise mutual information
between a target phrase and excellence
is high positive that means excellent
occurs with that phrase more often than
you'd expect just given the raw
frequency of excellent and the phrase
the same with poor and then the semantic
orientation is the point wise mutual
information with excellent minus the
point-wise mutual information with poor
so if a word occurs or target phrase
occurs with excellent much more than
you'd expect and poor much less than
you'd expect you're going to have a
strongly positive semantic orientation
if it's about the same right strongly
positive with both excellent and poor or
kind of neutral with either you'll get a
neutral or orientation or close to zero
and if it's much more frequent with poor
than you'd expect but not with excellent
you'll get a negative semantic
orientation so from some of tourneys
experiments here to some examples
looking at bank reviews direct deposit
has a weakly positive semantic
orientation unethical practices has a
strongly negative semantic orientation
you can take all of the target phrases
in a review average the semantic
orientations and you get a picture of
the semantic orientation of the entire
text and in tourneys experiments he got
an accuracy of 75 percent more or less
in guessing the semantic orientation of
a text although it varied a lot across
domains some domains like looking at
cars it was much more accurate looking
at movies it was much less accurate one
possible reason is just tastes differ a
lot more and so what counts as you know
everyone would agree on ethical
practices is a bad thing in a bank
massive explosions it's a bad thing in a
car maybe it's a good thing in a movie
maybe it's not it depends on your taste
and so the sixty percent accuracies were
more in the domains where taste mattered
more okay so we could try to apply the
same sorts of things of the political
text
the semantic orientation this method is
giving us a positive or negative
orientation because of the choice of the
anchors excellent and poor really though
we could use this method for doing any
sort of one-dimensional classification
of texts right so by using different
anchor terms we could get a different
Klein of sentiment so we could look say
at liberal and conservative as our
anchor terms right to get the dimensions
we're looking on use the exact same
method to get a semantic orientation
along that scale right so I tried that I
didn't use the entire web for it but I
used the 200 million word Reuters news
corpus right so there's a lot of
political discussion in that the words
liberal and conservative occur fairly
often in there from the the online
discussion forum text i extracted about
a hundred and seventy thousand noun
phrases so these are target noun phrases
that that may have a semantic
orientation and of those about 5,000 of
them actually did occur in construction
with conservative or liberal in the
reference corpus I was able to get a
semantic orientation for them and if you
look at the results that come out some
of them make sense now they don't all
make sense right I selected some here
that makes sense there's a lot of noise
in it but there's also something
happening here and so on this on this
scale liberal is positive and
conservative is negative so you get
things things like Jerry Falwell or Bill
Kristol the American Enterprise is too
far right us these are things that are
turning up with a strong conservative
semantic orientation things like condom
use or the economic policy institute
these are showing up with a strong
liberal semantic orientation so
something is happening oh there is some
noise and it is not is we're not getting
completely random results out of it okay
so going back to the political text
there were a hundred and eighty for
users who fell either strongly into the
left camp or strongly into the right
camp and they're roughly equally split
between left and right so if we just
call them all left we get about 50 to
percent accuracy like is fifty-two
percent of them are actually left some
some informal human sealing kinds of
experiments basically I just performed
this task of trying to classify users
looking only at the content of their
posts I was right eighty seven and a
half percent at the time I'm going to
try to get that improve that number
summer make it more reliable get a
couple other people to annotate the data
but at least as a first pass it gives
you an idea of roughly how difficult it
is right there are some people who post
once and that post contains no political
content at all and man or machine
there's no way of knowing what their
orientation isn't so this gives you an
idea of what the upper bound is this PM
iir method to apply just the way I
described it gives us an accuracy of
40.7 six percent which is substantially
worse than the baseline it's not
actually significantly different from
the baseline not quite significantly
different of course if you've got a
binary classification and you're wrong
worse than guessing you can always just
reverse the score if the system comes up
left will say right and then we'd be
right sixty percent of the time we still
wouldn't be significantly different from
the baseline and we'll come back to this
there's actually a reason why that's not
such a crazy thing to do that there's an
intuition behind that you'll see in a
minute but some other possible problems
with this well I used the Reuters corpus
as the reference corpus and not anything
else tourneys experiments were based on
on counts that he got from the web and
so he was looking at a corpus there was
many orders of magnitude larger than my
dinky little 200 million word corpus so
that's a problem right it it could be
that the reference corpus was just too
small it's edited text it's not informal
text and so it's going to have
differences from the text i'm looking at
it was collected in the late 90s so a
lot of the political issues of change
some of the figures are the same but
some aren't in the the Reuters corpus
there's a lot of stuff about Democratic
majorities right well it wasn't true in
the say 2005 when we collected this data
things have changed right so there are
some problems there but also I think a
key problem here is just that
political orientation isn't a sentiment
right and this method which is designed
to extract sentiments isn't getting it
what we want to get at here political
orientation maybe as a bundle of
sentiments it's maybe a collection of
sentiments although looking at the text
most the sentiment that's most expressed
most strongly is you're wrong more than
anything about things in the world right
the structure of the discussion is a
debate there's a lot of invective
there's a lot of strong sentiments being
expressed but they're not the kinds of
assent amounts that you get in a
newspaper editorial where someone is
constructing an argument about some
issue in the world which then can be fit
into the political spectrum okay so it
looks like sentiment analysis at least
this simple PMI I our method isn't going
to quite do it for us well another way
we could tackle this problem is as a
kind of text classification task all
right so we can take these texts and
apply any of the standard machine
learning methods we have for text
classification like for instance naive
Bayes to it and see what we get right if
you remember the the baseline was 52.1
7% applying naive bayes do it trying to
predict the summit the political
orientation of the author based on just
lexical frequencies in the text gets a
63.5 nine percent accuracy which is
better than the baseline it is
statistically significantly different
from the baseline it is an improvement
but it's really not very good right
might expect naive Bayes to give a
better result than that I also tried
using support vector machines and didn't
get anything significantly different
from this I imagine with enough
tinkering I could improve over this but
not by a whole lot right looks like
treating this as a text classification
problem sort of works but doesn't quite
work doesn't quite get us what we want
nine base what labeling disabused this
is the manual labeling that you
mella uses the the self description of
the users yep so some reasons why naive
base might not work one problem is there
are a lot of spelling errors in this
text so any sort of method that is
lexically based and like naive bayes
which treats words as either same or
different the the idiosyncrasies in the
spelling are going to greatly increase
the sparseness of any lexical counts so
there's a possible issue another
possible issue is a big source of data
sparseness is just the skewed posting
frequency distribution right we've got
posters where we have hundreds of
thousands of words that they've produced
we can probably guess their political
orientation someone who only posted 8
words we're probably no method is going
to work for them and so there could be
that it could be that we're getting that
tail all wrong the sparse posters were
getting wrong but we're actually doing a
good job on the more frequent posters
where we have substantial amounts of
text with them yeah number of users
right not to posts good users yet so
then the tail should affect that well
twenty percent of the users have one
post and so they have a small amount of
text so if we got all of them wrong the
test to split view it's five fold
cross-validation so the twenty percent
eighty percent split then across
do you know what the equity will be with
you the wake up on the under test yeah
I'll get to that in just a second yeah
another possible issue though here is
that the vocabulary that's being used by
posters to talk about issues is actually
the same regardless of what their stance
towards that issue is right and then any
sort of lexically based method which is
just looking for words to indicate
orientation aren't going to work very
well they're not going to be able to
connect opinions to issues which we
could use to get a political affiliation
so the spelling issue we could test that
so here's an example of a post the
person is producing a perfectly
reasonable argument it's sort of maybe
facile but they're producing a political
statement here but their spelling is all
over the place right and so any sort of
method which is looking for keywords in
here isn't necessarily going to
recognize Gotama know as the name of the
base in cuba and so won't be able to
correctly associate this argument with
what it's about so it's a simple test
for that which I tried taking a spell of
open source spelling checker augmented
its dictionary with some online terms
some online jargon and some political
terms taken from a collection of op-ed
pieces so this is professionally edited
text about politics so we can add some
names and jargon and stuff to it and
applied that to the the database of
posts and then redid naive Bayes and the
accuracy actually drops substantially we
drop from sixty-three percent or so to
fifty-eight percent which is a
significant reduction so not only
doesn't fixing the spelling help it
actually makes things quite a bit worse
a couple reasons why that might be one
is some of the spelling errors are not
actually errors and this is something
that comes up quite a bit in these sorts
of discussion so here we've got sounds
like typical Democrat tripe to me so try
peer is just plain misspelled the
Democrat is not misspelled II meant to
say Democrat and in fact that is a very
clear indicator of what
this authors political orientation is
that's a very useful piece of
information there if somebody says
Democrat they're probably on the right
and we don't really want a spelling
checker to come through and replace all
those Peas with t's there's also a lot
of this sort of stuff going on with
names right people's names are being
deliberately misspelled as a way to mock
them which then indicates very clearly
what the political orientation of the
author is so sometimes these
misspellings are actually extremely
informative and we don't want to gloss
them over another problem is that the
these texts are just really hard to it's
hard to correct the spelling and so the
post that I show before this is what a
spell does to it it does identify lots
of misspelled words but it doesn't
actually correctly identify the
correctly spelled word in for any of
them right and in fact it goes from a
text which although it has lots of
misspellings makes sense to something
that's almost got kind of a Dada quality
about it right parishioners real eased
they're all indecent right it doesn't
make any sense at all so it may be that
there's something to be gained from
doing some kinds of spelling correction
or some sort of normalization of that so
that lexically based methods would do
better but it doesn't look like it's the
irregularity in the spelling that's
really what's causing naive Bayes not to
work in this case the other problems
Veda sparseness looking at the
distribution of posters we could try
repeating the experiments but only
evaluating the results on posters for we
whom we have a fair amount of text for
so we could limit ourselves just to
those posters with more than 500 words
so we people who just posted a URL and
that's it right we won't try to classify
them using naive Bayes there are a
hundred of them the baseline for this
subset is fifty-three percent and I got
ninety-one percent of them right when i
tried to classify them naive bayes now
gets a sixty-seven percent which is
higher than what we got on the whole
dataset but that didn't solve the
problem I we're still getting sort of
lackluster results here adding spelling
correction to this gives a sort of a
similar reduction in quality sixty-four
percent so looking at just people who
post a more frequently does help some
right the sparseness of the data
for their infrequent posters is part of
the problem there's sort of good news
and bad news about that of course
increasing the size of the sample isn't
going to reduce the number of posters
who we have very little data for in fact
it'll increase the number of posters
that we have little data for the
percentage of posters who post once will
remain about the same but we're not
going to just be able to double our
sample period and solve the sparse data
problem now sometimes though depending a
lower up to we may only care about the
people who post fairly often we may be
interested in what are the core members
of the group talking about what are the
sort of long-term how do debates evolve
over time there may be questions that we
want to answer for which getting
accurate results about that tail just
aren't very important so it may be there
may be realistic applications we're
doing this sort of a cut-off or even
raising the cutoff would be a sensible
thing to do but I think really the key
problem here the underlying difference
there the underlying reason why naive
Bayes doesn't really work is really that
the vocabulary around issues is kind of
neutral with respect to your orientation
towards it most of the time right there
are things like Democrat right where it
really indicates what the person's
orientation is but for the most part
people call minimum wage minimum wage
whether they think it should be raised
or lowered or abolished or whatever
right it they call it minimum wage so
just to kind of look at what some of the
terms are that are used to buy people on
the left of the right I tried using part
lies management from info mutual
information again to extract terms which
are most strongly associated with
authors who have a particular
association right so the top terms for
people on the left the number one term
is Bush number two term is Iraq right
and going down to get administration
American president security war report
Dixie is a one of the participants he's
actually the heaviest poster on the on
the site he's on the right you might
imagine George Americans intelligence
staff told these are the terms that
people on the left are talking about if
you look on the right
well there's some noise there some HTML
entities in there that didn't get
filtered out when we were cleaning up
the data that particular users on the
right had in their signature files so
they showed up a lot that's spurious SR
is a username another one of the
participants where we get weapons saddam
liberals main man is another discussion
participant Liberal Democrat Bill
Clinton Democrats understands as in what
you don't understand is these are the
sorts of terms of people on the right
are using right so it gives you kind of
an idea of what the debates are like
people on the left are talking about
George Bush people on the right are
talking about Bill Clinton remember the
sentiment analysis results we got before
users who use terms which were more
closely associated with conservative in
the Reuters corpus tended not quite
significantly but slightly to be on the
left and users who use terms which were
associated with conservative in the
Reuters corpus tended to be on the I
guess backwards users who use
conservative terms were on the Left
users who use liberal terms were on the
right all right so for the most part
people were saying why the other guy was
wrong and they were bringing up things
that could be used to structure these
debates so that's maybe one reason why
reversing the answer you get out of the
PMI I our myth it isn't a crazy thing to
do that there may actually be that it's
getting at something there so that sort
of points to a direction we could go
with this right the debates have a
particular structure people are arguing
with each other we can use that fact to
try to get an idea of where a
participant fits into the structure of
the debate try to use that to guess what
their political orientation is this is
only the people have looked at miles
Efron used a version of tourneys PMI I
our method for guessing the political
orientation of bloggers and he used the
links between bloggers as a way of
augmenting the lexical data that he got
from and so he found that overwhelmingly
he and others have found that
overwhelmingly bloggers send a link to
like-minded loggers you get
these tightly knit communities of people
who agree and you can use those links as
something in addition to the text in the
blogs to try to guess which camp
somebody falls into other researchers at
IBM looked at using it posts which have
a character that's more like the data
that I'm looking at here to try to fit
people into camps in these discussion
groups the structure of the debates is a
little bit harder to pull out you don't
normally get explicit URLs linking to
another post or another participant in
the site sometimes you do get URLs
linking to newspaper articles of things
where people are bringing in you also
don't necessarily get as you would get
in using it the these kind of tree
structured threads where you can see
exactly what somebody is responding to
did you get a little bit of that you do
get people quoting other posts the bits
of post that they're responding to right
and this is a device that users have
developed for themselves for maintaining
some kind of discourse coherence in
these debates where you've got long
threads going on with lots and lots of
parallel discussions happening at once
with people moving from one to the other
the only way that the users can make
sense of it is by explicitly citing
pieces of other posts right so you get
lots of this kind of stuff right we're
here somebody is is quoting what MBL 727
said and then responding to it so we can
see now that this person is directly
responding to an issue that MBL 727
raised and we know now something about
the context of what this person is
saying that's a response to this
previous claim right now if you look at
these citation links so for each user
you look at the other users that they
have quoted in their posts you find that
out of 41,000 posts which were either by
left or right users about 4,600 of them
site another post by someone who is
either on the left of the right right
and overwhelmingly 71 percent of time
for people on the left and eighty-two
percent of the time for people on the
right
that cited post is by someone with the
opposite political orientation right so
what we can do is try to use this the
citation graph of whoo-hoo people have
cited and who people have been cited by
to try to fit somebody into the
community to see the kind of role
they're taking in the debates and here's
the hypothesis is that users that kind
of fill a similar role in the debate
that are responding to similar kinds of
people probably have the same political
orientation and try to use that as a way
of getting into the data to try to
figure out where people fit into it so
algorithm I used here is very simple
constructing adjacency matrix for this
graph of cosines right so a row in the
matrix would be for one of the users and
each value in the the vector would be
the number of times that person has
either quoted or been quoted by one of
the other users right so we get this the
structure there's a lot of noise and
that so I reduce the dimensionality of
it using some singular value
decomposition to take just those
singular vectors that have a singular
value greater than one so you sort of
compress the data a little bit it works
if you don't do this but you get
slightly better results with them a
little bit of analysis like that and
then that creates a citation space and
we can measure the distance between any
two users by the cosine of the angle
between their two user vectors right
very simple we get a distance metric
there we can then cluster users based on
their representations in this citation
space users who have quoted or been
quoted by roughly the same cast of
opposite characters probably are similar
in their orientation as well so I used a
hierarchical complete linkage clustering
so we get a picture like this this is a
little piece of the data and then cut
that off at a level that was determined
by a cross-validation and take all the
clusters that have a diameter smaller
than in this case point2 right so we get
these guys here these guys up here
of citation behavior that's similar
enough that we want to guess that they
all have the same orientation right in
this case they do all have the same
orientation they don't always this
method sometimes makes mistakes but for
the most part we can pick out groups of
users who are on the same team now it
would be nice if when we constructed
this citation graph we got this nice
bipartite graph where there was left and
then there was right and we could divide
it right down the middle doesn't quite
work that way in part because there are
a lot of different issues being
discussed and even someone who is
generally on the right say may
occasionally express a leftist idea or
sometimes you get someone who is on the
right who is making a very extreme claim
and other people on the right will say
well that's that's nonsense right you
which we were saying is foolish even
though they basically agree with them on
a lot of issues so you don't get a nice
division into two teams that debates are
much too complicated for that you can
get sort of groups of people who seem to
be playing the same role in the debates
all right so then the next thing they
did was to take all of the users who
fell into a single team group them
together and apply naive bayes to them
as a batch all right so now we've got a
lot more text about them the chances
that they're going to use one of those
terms that clearly identifies what their
orientation is that some member in the
team is going to say something like
Democrat which clearly indicates what
the orientation of the entire team is
goes up right and the cell remember the
baseline classifier gave us fifty two
percent accuracy naive Bayes raises that
to sixty-three percent accuracy applying
naive Bayes to the output of clustering
now gives us 68.4 eight percent accuracy
so it is a improvement over raw naive
Bayes on individual users right
something is going on here and if we
look at just the heavy posters people
who post more than 500 words the chances
that they're actually going to cite
somebody else and so this is that we're
going to be able to cluster them goes
way up and now our accuracy increases to
seventy-three percent which is still not
maybe as high as we'd like but we're
actually getting somewhere now towards
the eighty-seven percent or whatever the
human ceiling was
get on that the people who cite into the
might be of opposite orientation right
Oh overwhelming you so how do you take
that account the citation I don't write
no and so because well right so people
at you I also tried methods that
directly took that into account so we
could say that say if we know what
everybody else's orientation is and
we're trying to guess what this person's
orientation is we could say well they
have the opposite orientation of
whatever the majority orientation of
their sites are and that actually works
pretty well it doesn't it gets us if
there ever a right it was in the mid-70s
the accuracy of that so it's not that
much better than this because the
complexity of the debates is so high
right that it's not completely polarized
between left and right so this method
what are the clustering method I'm using
here if it turned out that citations
were always across the political divide
then this should be able to discover
that but it's also flexible enough to
kind of pick up that there may be
several different issues and one person
may be on the left for eight of those
issues but kind of patterning with the
right for one of them and it will it
will fall out from the clustering it's
the whole that that showed the picture
that you showed was it not represented
analysis because it looked like the
co-citations belong to the same
orientation is that oh so these are
people whose co-citation patterns are
similar but they may actually be all
citing people who are in some other
cluster rights in you okay so this seems
to be getting somewhere right by taking
the lexical content of what people are
posting and then adding to that
information about where they fit into
the community we can do better than we
could just by looking at the text of
their posts that we have a little error
analysis this graph shows so the points
across the bottom or people that this
method gets right that points across the
top or people that it gets wrong
cells or people on the right blue cells
or people on the left and the horizontal
access here it's a log scale of a number
of words that we have in all of their
posts and I don't know if you can see it
clearly but we do the people were
getting wrong to den to be sort of down
at the the lower posting density level
so the people we have less text for we
tend to get wrong more often but there
is this kind of interesting thing
happening here in the upper right hand
corner there are a bunch of users for
which we have tens of thousands of words
of posts that were getting wrong and
they're all on the right and I don't
know why this is just a sort of a puzzle
that's less but you can see that those
dots there in the upper right hand
corner they're all red in fact Dixie the
person who posted the most and is in
fact incredibly polarizing and it's far
to the right on every issue we get wrong
one possible reason for that is that
this guy is so far on the right that
even people on the right argue against
him because he's so far out there that
everybody is saying Dixie you're an
idiot that's the structure of a lot of
these things so it could be that such an
outlier they doesn't really fit into the
scale at all I don't think that's the
full explanation of what's going on here
I don't really know what's going on it's
a sort of a an area that for future
research other directions we could go to
try to resolve the stuff in the upper
left-hand corner users in both the left
and the right that we have moderate
amounts of text for a thousand or ten
thousand words of text we ought to be
able to classify them and we're not
getting them right directions I'm
looking at there are lots of technical
tweaks I could use right now you've bays
is not the best text classification
algorithm out there there are clustering
could certainly be improved there are
all sorts of ways that the algorithm
could just be sort of tightened up there
would probably give small improvements
but I wouldn't expect a substantial
improvement from any of those could also
try improving link detection so right
now I'm only putting a link between two
users if they actually quoted text from
the other users posts but there are
other devices that authors have for
establishing relationships between post
and for maintaining coherence in these
debates so there are lots of vaca tips
right main man is one
the participants and you get things like
main man it was strahm's birthday party
this is clearly a response to something
main man said although it's not actually
quoting what main man said so it could
be that that we could establish a link
between this posters and main man if we
just do that right if every time
somebody mentions another user's name we
create a link between those two users
the accuracy goes way down so there has
to be some classification of vocativ zin
to ones that are useful in ones that are
not useful there's also perhaps
something we can get from the thread
structure sometime if to sometimes if
two posts are close enough to each other
that the context of the previous post is
still active then a user won't have to
quote what was being said because it's
clear from from what's seen on the
screen what the relationship is again
simply making a link between each post
and the previous post in the thread
doesn't work that's much too too general
but there may be some way of
establishing which posts are actually
responding to earlier post without
actually quoting them so that wouldn't
increase the density of this co-citation
graph and not to increase the quality of
the the clustering then finally of
something that came out to me when I was
doing the classification manually most
of the time the way I was able to
establish to guess what somebody's
orientation was was not by the stance
they took on any particular issue but by
some sort of explicit linguistic
expression of their group membership
something like it's very sad you
conservatives can't win arguments on
your own merit you have to go Petty and
clone us liberals now you don't have to
know what this guy is talking about to
know that he has just outed himself as a
liberal he's just marked himself as a
liberal and looking at what arguments
he's actually God or who he's arguing
with is not nearly as helpful as the
fact that you conservatives is something
that only liberals say and us liberals
is something only liberals say you get
some of that kind of stuff in these
things there are subtler things
sometimes people say come out right and
say the reason I am a Democrat is right
well that person's on the left right if
you can find those those sorts of things
or I used to be a conservative until
right and then you can see that well
they've they've transitioned from being
on the right to being on the left there
are these sorts of things more subtle
things sometimes people will say every
time I hear some public figure say
something I cringe right someone maybe
someone on the right is embarrassed by
people on the right who publicly deny
the reality of evolution for instance
right so that's an indicator that the
speaker is identifying themselves as on
the right but it's much more subtle than
that then just coming out and saying
they're on the right that's sort of the
direction that I think this ought to go
into a deeper linguistic analysis of
what's in the text but not looking at
the issues that are being discussed or
the opinions that are being expressed
but for these sorts of markers of group
membership that's going to be the best
way to do this kind of classification
alright so it's just a summary of the
numbers of each of the experiments you
can see so clustering with naive Bayes
definitely does the best although it
hasn't completely closed the gap right
the human ceilings still I I did quite a
bit better than then the classifier did
and just to wrap up so the developments
in the web as we've moved from mainly
professionally produced text a much more
interactive behavior on the web has
created challenges for computational
linguists but also opportunities where
we can look into ongoing conversations
between people in the general public I'm
trying to predict political orientation
just from the language the posters use
is not as successful as we might like so
using things like PMI ir or naive bayes
which have been very successful in other
domains don't work nearly as well at
predicting semantics or political
orientation right and I've talked about
some reasons why that's the case but
bringing in for information from the
structure of the community out of the
stance the the role that people play in
the ecosystem of the discussion group
can augment the lexical contents of
their posts and give us better results
right and I mentioned some directions
that this is going on i'm also looking
at using statistical methods to
extract issues we can find out what are
the issues of people are talking about
and then perhaps an inventory of issues
could be used to classify people right
rather than looking at political
orientation as a sentiment we could look
at at it as a bundle of sentiments with
respect to a bunch of issues that are
being frequently talked about alright so
any questions well every time I see TMI
you use I wonder if you know I mean you
certainly know the the comments sure
many legitimate talk about TMI being
overwhelmed by a frequency of 12 yeah to
come to counterbalance that is that a
problem for the defecation um did you
try other men's of these peoples only
try other measures and find that TMI
little space no my own experience has
been the PMI works best for this kind of
stuff chi square you have to be careful
with it so I throw out any term that
occurs fewer than five times and for not
for tourneys method because I was trying
to reproduce what he did closely as
possible before that little lexical
discovery task where I was looking for
terms that were associated with
orientations I waited the mutual
information by the frequency of the term
and just in my experience it seems to
give very good results I haven't really
had much trouble with with outliers
overwhelming things so you but you only
had a colorful below football
but i remember right actually i filtered
out stop words for the lexical
identification task for attorneys method
since i'm looking at noun phrases not
individual nouns the frequencies tend to
be um you know looking at the output it
it sort of makes sense right there isn't
really crazy stuff showing up as being
strongly associated with liberal right
the top terms are all more things that
contain the term liberal most of the
time frame is how did you try to use
like a vital text classification i would
first try the standard spm light
approach with no link features and text
features just as as it's done yeah yes
I'd enjoy that and I got slightly worse
results in naive Bayes and I bet if I
spend a couple of weeks working on it I
could make it better than naive Bayes
but for the purposes of these
experiments it wasn't worth the trouble
but yeah I but easily isn't it the same
kinds of each one so you have the links
and the worlds and India you use the
links for rustling if the roads for life
face and just look slowing them into one
morning from the title I didn't try that
perhaps there'll be something I'll have
to try
don't you have any evidence that might
using more than one set of features can
help for example citation and something
else including the citations in the
classic edition model I don't have any
evidence that would help I don't have
any strong evidence that it wouldn't
help yeah
can you tell us about the pharmaceutical
applications did you get the same
results what was the classification so
they're not doing so much classification
is actually doing support for market
researchers and so in that case we don't
have we don't have the the real answers
right we don't know whether users
actually what the disease status
actually is so for that that kind of
work more of what I'm doing is is the
kind of word association things looking
for for associations between pairs of
words trying to build high precision
classifiers that can pull out a bunch of
posts from people who are newly
diagnosed right then a marketer can read
over it's not necessarily an exhaustive
list that's much more pragmatic stuff
which is very very hard to evaluate I
mean they're the evaluation is are the
marketing guys happy with the output of
it can they can they look at these
graphs and draw some conclusions that
helped them and so for that kind of an
application whether the I shouldn't say
this but whether the results are
actually true or not is not nearly as
important as whether they're useful for
spurring creative ideas so the
evaluation is much more informal and
much more is this useful right so that's
one reason why I haven't published any
of that stuff right it doesn't fit into
kind of what computational linguists do
although it's what marketers need it
would be nice to be able to evaluate it
but given that we don't often know very
much about who these people are who are
posting it's it's difficult to do so
it's not a classification
right no no it's more like well things
like what are the concerns that newly
diagnosed patients have right so for
instance 411 medication found that that
what the risks word pregnancy was
something that people came to sites to
ask about a lot right so that was an
issue that was an important thing that
people were talking about and that was
something that the the company could
address by making a pamphlet about that
delivered to doctors
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>