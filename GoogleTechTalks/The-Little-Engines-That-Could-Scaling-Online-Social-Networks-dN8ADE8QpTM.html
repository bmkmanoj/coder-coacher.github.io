<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Little Engine(s) That Could: Scaling Online Social Networks | Coder Coacher - Coaching Coders</title><meta content="The Little Engine(s) That Could: Scaling Online Social Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>The Little Engine(s) That Could: Scaling Online Social Networks</b></h2><h5 class="post__date">2011-03-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dN8ADE8QpTM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello and welcome to this tech talk on
scalene online social networks I'm it is
my pleasure to introduce just a poodle
from telefonica research in Barcelona
Wow thank you Mary God thank you very
much for attending the talk so I'm going
to talk like of a project and a paper
that is going to be presented at cecum
2010 this year and basically there what
we have done is like we have like a new
system of the importation replication to
scale social networks and well actually
right now is like probably already ate
but I usually give this talk when while
eating that's why I have the menu so
we'll first go like to the potassium
which is like the problem then the main
course which is the our contribution and
dessert which is the evaluation well
scalability as you know probably much
better than myself it's a very difficult
thing especially for social networks but
in general it's like it's a complex it's
a complex field like to scale up like to
serve like hundreds of millions of users
well it's extremely difficult however
like what we try to put our emphasis is
what happens when you scale like systems
that are like early stage in a way what
we want to do is like 12 is what we call
what people call the designers conundrum
which is okay when you are like a small
company so you have two choices you've
got you can go there like and you have a
product kind of like let's say that
Twitter or like something like this
small social network basically what you
can do is like okay you can just devote
all the research that you have like
making the system a scalable up to like
millions of users like this to the
programming exactly like lettuce or
spend a lot of resources there which you
might actually not have and you actually
lose the opportunity of the time to
market or what usually what happens is
that okay you start a small and you just
like to hope that you will be able to
scale up as you grow that can result in
like
success Oh can consult on friendster
demise and regarding a scalability well
how will scalability is pretty much at
least for like a dollar startups it's
pretty much solve by the by the cloud so
you have like will utilize
infrastructure and network but what you
don't have solve is the application
scalability from all the application
typical application layers I'm going
extremely fast because i'm pretty sure
that all of you are like more proficient
on myself on this area but basically you
have all the typical application layers
presentation application logic and so on
scaling this on the concept of the cloud
is extremely easy because those layers
are stateless since they are stainless
you just like can like add more machines
like runnin need another instance and
you like keep keep increasing the number
of machines and the computing power as
it users keep the user base keep glowing
however like is killing the data source
is not so straightforward because the
data source is independent is
independent it's dependent so there is
like you cannot like AB more machines
and hope that everything will be fine
because you have to like do like
correlated joins and all kind of like
operations that might actually like a
fight more than one server so what is
the typical solution like to scale scale
up well the first one is like is a
scaling up which is like do full
replication for replication basically
what you say is that you have like a
perfect copy of all your all the data in
like n servers and then you only have
like to coordinate like the consistency
across those servers this is the first
project Bishop people usually do because
basically allows you'd like to do a lot
bouncing across servers however
eventually you hit up like the limit of
the physical limits of the machine so
you can like go indefinitely the other
approach once you are done like with
full replication is like to go like okay
let's do horizonte partitioning or
shouting which is basically that we
split the data in sharks which are like
horizontal partitions of the data so
that the particular data is
dependent and can be put in different
servers okay that's kind of like also if
you do like this off-the-shelf solutions
like MySQL cluster for doing this
however there is like an important
assumption here is which is like the
data can be split the data can be done
and that's where actually the problem
for social networks arise because if
you're like I don't know like actually
I'm not Google well-known you have the
users of Amazon you can say like okay
all the users in Hamilton who start with
a or that the hash function model x goes
there like goes to the Machine and
follow up and then if you like need more
machines you just like do not the
partitions a little bit smaller and you
add more machines and there is no
problem because the users do not talk to
each other the users are not correlated
you can do like disjoint partitions but
in social networks decision partitions
do not exist because the main operations
the social network is basically a joint
on your friends and as you probably you
probably you all know like is there a
cherry picking problem so if you child
like to get like one cherry if they are
like linked basically we will take more
than one if you break them because they
can be broken then it will get one but
that's not the case for social networks
because you always like try to try to
fetch data from your neighbors so from
graph theory what we know like this is a
I don't know ayuh do you like to pick a
zacchara car get a car the club now
special network that is like very small
but you get the gist all out of it so
even that you can like do some
partitions on the data by colors like
for like many of the users when they try
like to go to their neighbors they have
to go to a different partition so that
means a different server and actually
like if the graph from graph theory is
that trivial to demonstrate that if you
have only like one connected component
it cannot happen that all the users and
other neighbors are in the same
partition like my definition that only
happens when you have one partition so
well where is the problem the problem is
that you cannot like partition and have
a clean partition because you
have these like neighbors you have this
thing that will always go outside how
the special Edwards deal with this
problem then well there is like
different different approaches like
different companies use a different like
custom solutions one custom solution
that they are from the poor Microsoft
Live uses is that they rely on they
still rely on relational database to do
like the operations and basically what
did we use like the Select and joints
across multiple images after the
database so what do they basically
basically what they do they have like a
caching system they have a caching
server at the level of the rack so
basically you like the they do like
random partition and whenever they have
to fight it from a remote server what
they go is like they do go to a cache
that it's on the same on the same rack
to like minimize like Network ir I or
Latin see this is kind of like eternal
case solution because it works but
obviously like only companies like
Microsoft or Google can implement this
kind of like custom solutions this is
not something that can work out of the
box for like people who are like
starting to build these systems then
because like the relation of the typical
relational database management systems
cannot like a scale very for social
networks it's not very appropriate there
is a tendency to use a key value stores
that probably you know key value stores
are like all these like no SQL movement
that is like dozens of there's like as
MongoDB Cassandra Kyle Stevie readies
HBase there is like there is a point in
which they were and they will eventually
run out of like cool names to to name
these kind of systems and kulick stores
are like very nice they are like very
nice for session labels because
basically what they do is like the
atomizer data they do some like implicit
denormalization so that they obstruct
from the application whether they taze
and that's efficient for the search for
the social network because because it
isn't the normalized however there is
like a trade-off here first the
trade-off is that you lose the acquire
language they have databases which is a
SQL that means not only the
expressiveness of
the quiet language but also you lose
like additional things like programmatic
queries or like 15 years of like quality
planning optimization so you are losing
all these things and then another
problem that key value stores calf is
that well basically that you lose a
abstraction from there from the late
operations basically the application has
to be aware of like what of like how you
do joins how you do like selects how you
do like filter how you do range queries
and all these kind of things and also
why will you see like the run is that
Cuban astrology can suffer from from
high traffic like a application that
have like a lot of like traffic like
Facebook or or Twitter like the
employment dissolution they eventually
can like even hit like a network the
network bandwidth limits I'm probably
like the same goes for Google and let me
okay so another the main course which is
our contribution so the confucian is
extremely simple and it's like I would
like to keep it at the concept to a
level so basically what we want to do is
like we want to maintain the data
locality semantics so everything was
perfect in the world where data is local
to a server because then basically image
that all operations can be resolved by
the current systems either mysql for
example or either key value stores
doesn't mean has to be one or the other
so we have this little sketch of social
network that we have here of like 10
notes and then let's say that we want to
split it into servers so what happens is
that ok you first I'll start with full
replication what happened is that you
eventually end up like with 10 notes in
its server so wittily the costume memory
and when it's I say memory here is like
not proper like ram memory but it's
kinda like a storage RAM skull like a
concept or definition which is basically
like how how many users you have to have
in its server so it would be like 10
users in each with traffic is 0 because
all the data will be local so you don't
have to face data from outside when you
want to resolve aquarium
and right traffic is 10 because
basically you have to maintain the
consistency across the the replicas
eventually the dog does not the scale
because you cannot feed like all the
users in a single server otherwise you
could go with only one already then the
typical key value store what it does is
like okay let's do like a random
partition based on highs on a hash
function and what you end up is a deal
the memory is very good because you only
like put like as many notes as you need
in the server so you have like a perfect
/ M load you don't have right track you
have right traffic assuming that you
don't have copy for redundancy but you
have a lot of free traffic why is
because eventually like for any know
that you query probably it will have to
fetch data from multiple servers then
you seem like okay that's a problem
because for Q value stores that
transparent but it has the power we
already mentioned and for databases you
cannot do that out of a box you would
have to do it like also like on the
client side and it is kind of like that
black defeats the purpose of like easy
scaling and transparent scaling that we
want to achieve so one solution that you
might think of and actually like in a
way that there is like some people that
already like proposed it a long time ago
it's kind of like okay let's not do
replication at the table level or at the
database level like in the case of full
replication but let's let's do
replication at the row level which in
this case the row level will be the user
so let's do like let's do a random
partition and then let's replicate those
users who are outside your server into
your servers so that all data is local
but obviously this has a dependent you
can already see here is that you create
a lot of replicas and eventually that
can lead to like a full replication
scenario and then we'll we propose which
is kind of like probably you already
like are gassing is to do like a social
base partition and then the replication
and then by the example that i'm giving
you here what you see is that the
specialist partition what it would do is
like partition the social network by
here and then basically what it happen
then you would have to copy for I don't
know what happened with a pointer okay
you would have to replicate for to the
first server and you will have to
replicate three to the next server and
then you guarantee there is my
definition well there is like the
guarantee that all queries can be
resolved locally and you have a very few
number of replicas so like so far so
good you might say but the problem is
that now you might think that I'm
tricking you because that's like the
very good example it's like a on paper
everything works but what happened in a
real social network I mean can actually
this work so what we went ahead and we
try it so basically we just like got
real social network data from Twitter
are cute on facebook I'll focus on the
Twitter data collected oz 4 million
users 48 million edges and 12 million
tweets for the beauty of 15 days which
this we call it ourself and this is like
a very good representation of what
Twitter was at the end of 2008 which was
like already like popular but not as
popular as it today and when I'm saying
this because it's like probably we have
like fifty percent of the tweets
actually no we have fifty percent of the
tweets that happened during those
fifteen days so that means I probably
would have like half of the network and
so on so it's like a very good
representation of what Twitter was
already at the time and the Twitter had
like to undergo like several like real
cost leary architectures to develop deal
with the traffic then when we go is like
okay which I like partitioning
algorithms partitioning algorithms is
like in not a new field it's like a lot
of work on the area and basically what
we do is like we compare random
partition the key value store then we
use like one example of like a spectral
clustering we use mattes which is very
well known if you work in graph
partitioning it's like it's ABC
algorithm it's got very well and then
what we do is like also we use like some
moderate optimization algorithm and why
mother without motor idea optimization
is because by the way is because I'm
come from physics and those like we
don't use a spectral partitioning we use
a different strategy which is like
optimizing for a metric called maserati
was proposed by newman
and it has like several advantages over
the spectral clustering although that
one can be reduced from the other but
anyway that's not the that's not the
point so basically what we do is like we
did like pick one of the majority of
domitian algorithms the state of the art
and we need like implemented some hacks
so that we have equals x partitioning
because community detection does not
have to yield equal size partitions
right and we finally have our algorithm
we let's call it spot on line and now
let me go to the results already so the
question is like okay if you want to
guarantee local data semantics so that
all queries can be resolved luckily how
many replicas do I have to generate
extra in real social networks so these
like the three broad here like money for
Twitter or code and Facebook for
different level of partitions from 4 to
5 12 and the replication overhead which
is like again how many replicas X do you
have to generate let's focus on Twitter
and the case that I want to have like 16
partitions because I have 16 servers so
what's a replication overhead well for
random it would be like three dot nine
format is 22 and ours will be like one
two three that means that if we
replicate on average one two three times
every user we can guarantee by for this
data set that all the queries can be
resolved locally and that's kind of like
long that's actually like a surprisingly
low number another nice property is that
you see this is like a log scale of the
servers the replication overhead for all
cases glows linearly here so its sub
linear growth it means that scales well
the replication overhead keeps clogging
as you add more servers but its sub
linear and then you will also you might
actually have noticed that this is this
K 0 here why because in here we have
like replicas that we have to generate
to maintain the data locality but in any
case there is like in real systems there
is like other replicas you have to have
that other one had the one for
redundancy because you don't want to
have like only single copy of data you
want to have a list
two copies of theta or K copies of data
and since we are actually like
replicating here can we actually combine
those two things like the replicas for
data locality and the replicas for
redundancy so well here we have the
results and the replication overhead for
like k to what means that you at least
you want to have at least two replicas
for each user for our our image to dot
44 which is like much better than before
because basically means that you already
were paying to anyway so as you add
still out 44 more replicas you achieve
data locality then now you might you may
notice that like the numbers like a spar
is like a little bit better than m o+
and mo para saludar Metis and all of
them are better than random as you would
expect then some people actually they're
like especially depending on the
community and talking they have like
some kind of like oh there is like a
problem here like some people say that
our algorithm is not much better than
Matty then meta saurimo plus because
they're in damn the improvement is like
over like thirty percent or like 20
depends on their on the case and there
are other people at on the contrary that
see that that they cannot understand why
our algorithm performs so much better
than Madison demo + so there is an
answer for all the questions first why
our algorithm does not perform much much
better than theirs is because our
algorithm is online and why and its
online is incremental why because we
need to like social networks are
extremely dynamic this is constant like
new users coming in I'm coming out and
there is always like edges being created
and not and every all these events and
obviously all the events of the system
dynamics which is like machine failing
adding machines and so on so all this
depends they are willing has to be able
to react to those events so what we
don't want is like to run like a
partitioning hour in which is offline
because basically we will have like a
synchronization problem there and also
we'd have like a stability problem
because what happened with the métis
and mo plus is that they are very
to the initial conditions so for example
if you change like one percent of the
link structure of one network and then
you run the same algorithm there is no
guarantee that the the placement that
the the resulting placement that you
will have will be like equal one almost
the same as before actually it can be
very different you can be like good
health so like a small change on the
number of links can produce that you
would end up like having to move a lot
of notes to keep the data locality
constraint so basically our algorithm is
like online for all these reasons which
alex amenable to online social networks
otherwise it's very problematic and then
if i were already made online how come
it perform so well well it's very simple
is because we are optimizing for the
right problem well the the other
diagrams they are not and that's kind of
like a provincial pricing so let's see
the example here so you have the social
network over there like down on the
first one so what happened that if like
you are like trying to use like typical
graph partitioning mechanisms they try
to minimize it cut the cut edges which
is like the inter partition edges and
the petitioners will come out which the
the particular generate will be this one
because this partition generates only
like leaves only like three edges but
these would require like five notes as
you can see it would require like notes
from E to I too big yeah to I to be like
replicated so it would be like five
notes that have to be like replicated
those two there is a pointer those two
should be replicated here and these
three has to be replicated there that's
because we want but actually that's not
what we want but we want it's like to
minimize the number of replicas that we
have to do and for this case this
partition over here it's better because
even though you have like four inter
partition edges you only end up like
replicating four notes well what
happened well it's kind of like
extremely is very simple i graph
partitioning algorithm say like in a way
what happens is that if you let's say
that you have 100 friends
so easily is like trying to maximize the
number of friends you have on your same
community or in into you so let's say
that ninety percent of my friends are in
this server but then there would be like
five friends which are there two are
there and 111 and actually that creates
like five replicas 200 server what we
want is actually a system with like that
okay I don't have so many replicas I
don't have so many friends in my machine
but i have like for example eighty
percent here ten percenter and ten
percenter that's much better than the
original partition of the decision about
partitioning well because we are kind of
like worst case so and let me just like
this guy for like extremely like quickly
the spell online so basically its power
line what tries to do is like minimize
mean replica problem and mean replica
problem is as you might expect is
np-hard so what we basically do is like
we do all I can no DC solution well use
like greedy optimization with local
information only and we have a
load-balanced constraint which is
basically basically based on back
pressure so the organ in the algorithm
um will react to the six events that we
mentioned like adding and removing of
either like notes edges or machines let
me just like Google it'll be like on
there on what happened when an edge is
created so let's imagine that you have
the situation here and actually it's
like there is no no don't worry too much
because that example is also on the
paper and it's extremely easy so
basically what we try to do is like we
have a situation which is like we have
data locality semantics maintained and
there is anyways created and the new
edge goes across to servers so what do
you have to do well first of all we have
to fetch only the local information so
the neighbors of node 1 and node 6 that
are the ones affected by there by the
edge creation event and then basically
what we do is like count what if so
there is like three possible scenarios
as you might anticipate let's move node
1 to node 6 add to the server 106
that generates three additional replicas
and we can calculate that only with the
logon information then what happened the
converse the other case is with like
okay let's move no chicks to the terror
we're not one is and that actually like
remove one of the replicas and finally
what we have is a strategy cool case
which is like okay let's keep it like
this just like generate the replicas
that are necessary and that general is
like two additional replicants so which
one do we go for we go for in this case
would go for the status cool why is
because this solution would be much
better because it minimizes but it
doesn't maintain the load balance in
constraint right so you are you only
like allow like replicas to move to
several that are like that contain less
replicas than yourself otherwise these
like to avoid like the degeneration that
would happen because it eventually like
all the better figuration is to have
everything in one server so to avoid
this we have the solution and which is
this simple this simple approach which
is extremely simple local information so
only the information of like the
neighborhood of the two notes affected
and it's a local decision so there is
only like the only noted can move are
one of those two plus their neighbors
obviously if they need to be replicated
so only with this algorithm will chief
by those results as before which
basically means that our solution the
contribution of the data locality can
actually probably be improved the those
numbers that we already gave if you like
go like a little bit more in deep with a
with the acronym however we because we
just like value the simplicity in a way
with just like estate at this level so
important thing is like okay now let's
go are the politicians law bans yeah the
a load bar like actually the coefficient
of variation of masters is extremely low
the coefficient of variation is like how
many fire how many times are you away
from the standard deviation right so for
application will reach and rights which
happen on the matter replica
especially violent 44 right is not so
well balanced why it's because there is
like we are not like we are like
treating the users as individual as
individual unit so we do not take it
into account the traffic that the user
generate however we could actually like
incorporate that but we chose not to for
simplicity but even in that case the
coefficient variation is extremely it's
extremely low and actually what happens
is that the data but you can see like in
this in this project here and this is
important because like in real social
networks Theresa like there is some
correlation or like the heavy users and
how many replicas to regenerate but the
correlation is not as high as you might
expect and actually that means that the
low band thing by this simple scenario
i'd buy this simple approach read it
works pretty well and another thing that
you have to take into account is that ok
how is the distribution of the replicas
so I'm telling you that the average for
Twitter was like two dot 44 right but
that means like a how is like the
distribution of the of the on how many
machines you have to be replicated so
basically what happens that is very
skewed towards like the low balance so
like 75 percent of the users for example
they end up like having like three
replicas which basically means like two
slaved for like the redundancy that we
need them anyway plaster matters so
that's like the 425 particularly people
we don't add anything and then for the
rest week a like add more norm add more
like replicas to maintain data locality
and eventually you end up like that 90 /
users to have like less than 7 replicas
and only like 139 out of like 2004
million end up replicated everywhere
that means basically like well we'll
have like some people will have some
like kind of like problems getting like
a consistent state but those I mean it's
very few people and those can be liked
by its very few people and you can do a
like eventually constant eventual
consistency we only implement eventual
consistency
you get like the end result really
quickly so now and then some people
might actually like okay but and here
like you're saying like the dual moving
notes and we don't like actually like
moving notes because moving notes have a
cost right so it's actually the cost of
our room what it is in a way because
obviously we'll have to move them but
it's not really that important actually
what we see here is like on the x-axis
what you see is like edge creation
events which is basically a timeline
like you keep adding edges as a network
grows and then you have the on the
y-axis it's like the rate of actions
upon an edge is created so really what
you see is the majority of the times
like about like sixty percent won the
system leaves a transition estate you
don't have to do anything at all the
other one doesn't have to move at all
why is because the the edges it created
means the asset is created is our house
already the two nodes in the same server
or if they are different servers they
already have a replica of each other in
each other server that happens a lot of
times why is because since we are like
building a social partitioning as we
grow when you keep adding edges most
likely like those edges will fall into
the right place because people do not do
not add edges at random they add edges
with semantics so this is like sixty
percent of the time you end up doing
nothing which is very good and then like
a like thirty percent of the time you
actually have to do some movements right
and okay what is the magnitude of these
movements well this is like the plot
that you have here and that you can see
that this is the DDF which is like a
thing is better and see like how many
times how many nodes are affected on
those movement events so like ninety
percent of the times you only have to
move to move two nodes which basically
like moving to notice me me meaning like
moving the all the data relevant to the
to those users right and then
opportunities like some nonzero
probability that you have like a big
event but this is big events are like
pretty much constrained
in all this case like the biggest event
that we saw he wasn't moving in a single
shot of like 130 notes which is kind
like pretty like a low so modern or
movements actually as a system obviously
we have to deal with like a system
dynamics also like how you add machines
how you remove machines that kind of
like does extremely high level i'm going
to say that we can go over like later if
you want adding servers is an no problem
you can add service like either like in
order preserving fashion or you can
actually like move like a fraction of
every user to that new server to
maintain your bouncing from the scratch
removing sturbridge however is not so
clean that's like the big that's a
little problem of it of the system why
is not so clean it's because we have a
structure and because we have a
structure because we are maintaining the
structure to maintain data locality when
what happened hit one you like remove
one you kill one server like to recreate
that server okay it means more about to
relate the server only means like moving
as many notes that you had in the server
but let's say that you know one that's
ever to actually like be recreated you
just want to get rid of it so that when
you limited to that all the notes that
existed in that server have to move
elsewhere and all the neighbors to and
that thing can okay happen in a single
point of time where you cannot do the
optimization so basically what happens
is for example in the case that we did
some some experiment so like 32 servers
and basically which are like you one
that would only affect like in theory
like see that one personal users we
ended up like that we had to move like
twenty percent of the notes instead of
like the three percent so that means
that removing a fail server it is not a
good idea in our system but then you
might you might think that well that's
not necessary because you never have to
remove service you never ask a loud
which probably not true but the problem
is that you can think that a permanent
failure is also like removing a failure
are removing a server when you have a
failure a machine fails
there is two things that you can do in
one if its permanent and you don't want
to but you are losing that server you
have to like be able to like reproduce a
state while you were with one terrible
mess then you are like having these like
budget scenario of like removing server
and then when we have to actually like
we are thinking of like doing it's like
a different strategy based on backups
which we can do because our system is
stable the partition do not change much
so we have AG actually some numbers that
kind of like seem to point that it's
like a very physical to do it like this
however what happens when you have like
transient failures and by champion
failures I mean those failures that
lasts less than X minutes that you might
actually not want to recreate the server
or like to put it down but you just like
want to ok let's wait until the thing
comes back online the whole system is
affected by this actually it is in a way
but it is very resilient why because
basically because we have this connect
community structure what happened is
that if once never fails what you can do
is like you can promote one of the
slaves of that masters had failed to be
a master then there is no guarantee that
that new promoted master will have all
the data local obviously because we are
we are not enforcing it but because of
the partition and we will extract the
social structure most of the neighbors
that had to be there to maintain data
locality will already be there because
they had to be there anyway to begin
with because we had like this kind of
like little balls and basically you're
comparing balls and intersection of the
balls I'll probably on the same servers
and the knowledge makes myself clear but
like with a little example it's like
easy to see so we can like we see that
for example let's suppose that once they
were like we have Twitter data 16
servers if one machine fails if we do
the promotion to a master using a water
filling a strategy so like the
one user picks which is the best
candidate and so on to one thing like
the robe and sink restrain we end up
like 65 / tell of the users have more
than ninety percent of the neighbors on
their best slave candidate that means
that for like the period of that play
for the time expand of that failure like
most of the user would have like a a
degraded performance of the 180 for them
but they will miss some to it so they
will move like but have some email that
is going to be that is being sent to to
them for that particular time but most
of the like for most of the users they
would actually not experience the a lot
of the relation see this come like a
graceful degradation and opportunities
not like a perfect solution but it's not
like a nice byproduct of a thing inga
indicated franti and failures that
happened a lot in a way people might
actually not be even be aware of those
like very high level of the architecture
between architecture what we have is
like well we have the server X that has
the front end and application logic that
we don't have anything to do there and
we have the middleware which basically
the middleware interfaces with the data
store that we want to use right and the
little tour can be like either my
squirrel or Cassandra or whatever you
only have to implement that middle word
for the particle detector we only have
done it for mysql and cassandra
basically like deprecation logic talks
to the middleware as if it was talking
to the datastore so basically in the
case of Cassandra we were implemented
the API in the case of my sequel with it
like a driver like thing for SQL so the
application talks to the latest or
through our middleware and between our
middleware what it does is like okay it
goes to the directory service which take
the directory service is the place where
because we don't have a hash function
here we have to have like a table lookup
to know if you are like user X you have
to you have your data in machine a so we
have to know that so it goes to the
directory service that answers this and
then it directs you to the right data
store and then just particular operation
by definition will have all the data
locks
the needham store that here you have a
cluster of three the registers are
independent of each other they don't
know they are not aware of the existing
off of each other and they can resolve
awkward is welcome and then what
happened is that what happens if those
those particular queries our rights or
updates then the replication manager
here is all alike also like a spying
those things if he's like an unselect
doesn't do anything but if it's a writer
an update basically it catches that it
goes to the directory service again and
he finds out where the replication of
that user is and just like without the
replay on the data store while the
replica was asleep replicas are this is
like a bay like cheap way to improve
reimplement like the replication typical
replication that mysql offers and now
like the reserve which is like actually
the part that I like it most is like
basically great we have the system the
numbers look kind of like good let's see
if we actually like the liberal what we
what we try to do so we take a Twitter
clone I don't know if you know la konica
or status on net is an open source
Twitter that is done by like kinds of
like the students because in a way like
Tracy like if you actually I said the
same piace that Facebook and then
complain so you can answer I think I can
say it here too is that if you only do
the system for like 10 people doing
Twitter and Facebook you take like 15
days like to implement this
functionality if you only like want to
have 10 users if you want to have like
million users that's a different story
right so basically why we take is like
okay let's take this Twitter open shirt
which relies the last price architecture
that has like a PHP front n the logic is
on PSP to it interacts with either like
my squalor populace and we feed it with
the data we put the data of that we have
of Twitter that is like a good
representation of what Twitter was at
the end of two thousand eight and then
we have our little engines which is
mentally 60 commodity desktops that we
are like a low budget as I would a
startup
with two weeks of RAM collected with a
gigabyte Ethernet switch and we test our
system which is our system which is a
Speier but also like we test our system
between the Twitter data set and Twitter
clone and mysql and also cassandra and
here we have the results so basically
what we do is like okay when we have the
data loaded and we have like the kind of
like what twitter would be like at the
end of two thousand eight we have like
we apply like different application
application level Ridge requests the
image request is not like get from the
key value store it's basically like a
call to the local directory service to
find out where this what one machine is
costing that Cassandra node then won an
external call to get the list of tweets
and like one multi get call to get the
content of the last twenty twenty so
that's kinda like a second a heavy
operation to avoid like problems of
caching with like worried the user only
procession and what we have here is that
the typical response time versus once
the PDF of the response time for spar
which is our system and the vanilla
Cassandra which uses random partition
and basically what we see is that well
typically typical resolve 99 percentile
we want to guarantee under 100
milliseconds we can serve like 100
requests per second while Cassandra
out-of-the-box can only serve 200 and
let me anything oh these like because
network bandwidth no it's a problem
because network bandwidth is not a
problem in this case because you see
like we have a GD network and Cassandra
keeps blowing but it stays like until
like 70 megabits which is very low in
that means that Cassandra eventually
will hit a problem when you increase the
number of servers but it's not a problem
and we see that our solution obviously
doesn't have virtually no network
traffic sorts
so we see like nice result for Cassandra
let's go from my square mysql the
experiment is a little bit different
while we do with my sequel is what we
can we compare mysql replication so like
every against my square with the spar
this comparison is really little bit
unfair because in one case we are
partitioning data and in the other
doesn't like like every every one of 16
clusters on the case of for replication
has a full data set however is not
answer on this case but it's very fair
on the from the application perspective
because from the application perspective
both full replication and our system
behaves like a centralized system so the
application doesn't have to be aware of
where they date a live for the
application the data is local so I can
do like SQL queries without worrying
about anything because I know that I
have the impression that the application
has illusion that all that is local so
in that respect this is like the fair
comparison to them well then bed ill
what we had is like 95 percentile and th
percentile under 100 is 150 milliseconds
because 100 didn't give any significant
results from SQL which is like can jump
from like 60 requests per second on my
square to 2500 so which is like a bit
improvement and I'm pleased the
disclaimer do not try to compare these
results were I'm not comparing out here
like my square with Cassandra at all and
it's like is a different scenario so why
do we have some imagine so big
improvement let's go first for their for
the max quell is because we do we know
what it likes of a billion table a
billion rows table with a billion rows
what happened is that because well the
systems use like the normalization so
basically you have as many like entries
in the database and tweets x the average
degree because you have you don't want
to do a join to fetch your left the lot
plenitude of your friends what you want
to have is materialized so you want to
have like a list of the late
you know you want to have an inbox right
with your list with the late last 20
tweets that you have to fetch so when
you lead each table keep going very fast
completely like when you have like
1,000,000 rose you start like hitting
you have used to like sit in this chaos
just trashing all sort of problems but
then what happened why but you might
think that ok 1 billion you / 16 d x 2
dot 4 which is our replication overhead
that until you give you like a big table
of like a 100 million aprox so why there
is no plan there well because in a way
we are doing data locality but we also
like doing data correlation implicitly
because that table is like 1,000,000,000
roast but like all the same users always
go to the same places and because of
that you like increase like a lot the
memory they cash the cache hit radio and
that's why it's like you improve out
that and then it can make performance
and like those like 15 years of like
optimization on SQL kicks in and you
have like a tremendous boost of
empowerment of performance on the case
of Cassandra this planation is like it's
different one it's also affected by the
memory hit ratio but there is other like
factors which is regulated that we
reduce a network latency because the
problem on this key key value stores I
live from these small slices is not
their bandwidth that you have but they
latency that that you get and even
though Latin cesium there on Cassandra
when you test them in isolation or like
a Memphis are like under one millisecond
when you keep adding them or like
multiple thousands of requests per
second they keep they keep adding a
little bit because what happens is that
okay you keep like pushing this
everybody hard that's server it's like
keeping a state because it's a
synchronous estate every time that you
keep more estate you go a little slower
and then those like little latin sis you
end up like with a guy like a smaller
like a
and ever increasing latency that
actually kind of like affected performed
as a particular server and then what
happened is also like is because like
you are like branching the request to
multiple servers you cannot actually
like stand the results until one of the
servers 11 until all the service that
contained information has have answered
but what one or like multiple if it is
like redundancy but in any case what
happens that if there is like little
spikes of traffic due to the CPU
bottlenecks that you have that I
mentioned before because of the state
what happened is that that also like
affects the overall latency of the query
because you are like paying the
performance you are like delayed by the
performance of the worst performing
server in the cluster and therefore less
but it can like adds up doesn't it
doesn't it start a lot but you have like
four times more improvement as we saw
like from the measurement with it the
only IO bottlenecks that we saw it was
like networking and cpu I'm with a
coffee they go the questions so that's
pretty much it so I know no questions no
comments yeah
let me go yeah also yeah the question is
that there is like a difference between
like the replication overhead that you
see on Twitter are on Facebook and
actually I also add that in orbit yeah
it's true like as you see like on the
slide the replication overhead for
twitter is like lower than for facebook
and an orchid there is like one reason
for this one there is like multiple
reasons once one is at twitter is the
directed net is a directed network while
our code on facebook is undirected so
when it's under will need undirected
opposite replication overhead it has to
be doubled by the fault because you have
like you don't have like this like how
then jizz and then on top of that what
also happens is that facebook an orchid
are like more dense than facebook so
like the morgans you are the more
replication overhead you will end up
having as you see like a orchid is it's
much more like dense than than facebook
and so on so you have like like a high
replication overhead however then like
actually that's kind of like a UH
premiere you might think that oh but
that means that you have your network of
twitter was not big enough and therefore
that's why you have like these like
extremely low numbers and then we have
like a slide here sorry we have a slide
here which basically tries to say this
so actually what we did is like we don't
have it on the paper because we just
like finished experiment so we basically
fetch the the people at geist they were
very kind and they gave us they just
head of twitter they have like 41 Billy
million users 1 dot 4 billion edges the
replication overhead that we had for
Twitter and 16 servers which is the same
before worth two dots 44 now it's two da
12 35 so it's kind of like a scale but
obviously it's true if you have like
undirected undirected networks that are
very dense
we'll have like a like high replication
overhead and actually if you have like
no community of structure you will have
replication overhead so for example I
might think that for example this for my
space where people cannot lie connected
random in a way that is like this kinda
like notion of friendship is not very
clear because you don't have this
community structure then the replication
they will have to do will be more kind
of kind of like closer to the random
yeah yeah yeah okay okay okay so the
question is that on the online on the
Iron algorithm we stay like that well we
have the case that when any which is
created you decide which of the three
actions you go to but you have to have
an initial partition do we use Maddie's
for this now we don't use matt is
actually the that's one of the cases
that I didn't explain degrading had time
but like when you clear like a new note
is assigned to the to the machine that
is left loaded that is lift loaded so
basically like we did a strategy and
then we use the exactly same myristic
from the very first edge so we do not
rely on any initial partition of any
kind
okay i see i see how do you know how do
you know yeah yeah how do how do you
know the order of the edges were created
okay we don't know for we don't know we
do not know for twitter or for our code
and basically what we do is like we take
the edges and we do a random
permutations and we suppose that the
edges are created like like with a
random permutation however for facebook
we have time there we have the
timestamps of the edge creation events
and actually these two plots actually
show you that facebook with permitted
order and facebook with the actual order
they perform like the replication
overhead is very much the same the only
thing that changes and actually there is
like a on the middle order you see that
there is like an error bar which is very
small it means that different
permutations of the edges do not give
like different replication server heads
so it's like it's very kind it's very
consistent and it's very like the index
really affects the order however is like
the number of movements but actually we
are like doing in our case if you have
the actual timestamps you would do less
movements actually by not by doing
random random resort like random
permutation we end up like doing more
movement because we don't take advantage
of like this kind of temporal locality
of like edge creations what happened
usually unless creation is your people
undergo like apple stuff like it's
called like densification well like you
start like doing a lot of like edges you
crazy like you add like 10 friends when
they five the next treat 12 and
eventually like you end up like adding
one every like 15 days so if you have
those events like altogether like less
movement button replication overhead is
like marginally less but it's like
negligible yes sir
yeah how long does the I want even takes
to converge they are in convert it
immediately because it's only like doing
a it's it's it's a local decision and
every time that you add a new edge you
do like you say like okay this X number
of notes and have to move to these
particular servers once the movement is
done it's a stable no no the only knows
who are affected the only knows whom oh
who can move are the two nodes that have
the edge what it can happen is that
other nodes will have to create like an
additional replica indicate they were
not there but their master will not move
their master will still be on the same
server so like actually in a way that we
our algorithm is can like extremely
simple stimulus tick because we wanted
to avoid like large optimizations that
could result in cascades is we don't
want that I mean how busy we don't want
it kind of like things
yeah well what happened if you have a
look ready like a close that community
in one server and then there are like
new users coming in to that server well
if there is new users the new user will
come to the list loaded server so there
is no like guarantee that they will have
to have to go there eventually you might
think that ok but because they will add
links to that close that community they
will have a tendency to go yeah but
there is like a is like the load bands
in condition that basically you can only
go as long as the server that you are
going to have less masters than the
terror that you are living to avoid
profanity in fact so basically in this
case is kind of like okay you are left
on your own but eventually what happened
is that the select particular ball of
users and then there are the new ball at
forming I mean they will not be able to
go here like to jump but what will
happen is that the new ones who are
coming that are so formal ball they will
they will be put together so yeah yeah
well what do you mean a spare what do
you mean a space like us on a storage
yeah yeah apps absolutely absolutely
texaco yeah so basically the question is
okay you want in like the load balancing
for the Masters but there is like some
kind of like globe asking for the
replicas because the replicas need to be
generated to contain local consistency
there is two answers to this question is
like we have an algorithm that that's
the two things at the same time but then
the that does like load balancing for
both conditions but then the numbers are
words Wow and we
like didn't presented so because here
you are like really spot-on unlike a
particular particularity of the thing
which is like Daleks have it will have
like many more replicants than others so
there is no like there is no way to
guarantee unless you enforce it that all
servers have like equal utilization on
other storage that's that's right but
for us it's not a problem in this in the
sense that because we assume that
strategy is very cheap right so we
prefer to actually have like like what
not check on that and achieve like bilo
replication overhead because we are more
concerned of like the delay that can
happen from the eventual consistency
then from the size of the storage and
then like let me go all of your father
then I still enjoyed this chip which is
probably true but then that might
actually you might think that that could
have an effect on on the performance of
the operations for key value stores they
do not because people in stores
basically work on like all memory way or
in memories of the replicas need to be
there but like only a fraction of
information needs to be on memory only
the fraction of information of one user
with the replica who is relevant to
another user the bulk of information
will never be loaded so that it's like
on the database that's a different
problem because that increases table
size so that's a good question and what
is the thing it can be like up to like
fifty percent while we saw is like up to
fifty percent a bigger so there is like
the smaller server and the biggest I
work and they can have like up to fifty
percent more storage space assuming that
all users have the same profile
information I mean that all you have the
same we do not we do not check either
like if there is like users that have
like I don't know like 100 like they
take like 10 megabytes and then each
other user that only take like 100
collides we undo that with which it all
the users equally which is not true but
that could be added into their into the
algorithm we have much
I think it instead of like for like a
particular example of like 244 of
Twitter i was i constantly referring to
i think it was like three dots five but
I mean that's kinda like a recall
because in a way but in this case again
dissolution the other one is already
seen post it is by like but the
adventure was extremely like it's a very
attentive approach because basically
what i was doing is like i was doing but
pressure to different levels but that's
kind of like Barry it's a very like a
stupid way to attack the problem because
basically you are like on optimizing for
like contradictory things so probably is
like a better approach to solve this
problem that could keep it lower but
that out of the box was three dot see
that like field five thousand nine I
don't cheat on something like big
something yeah okay if you have like
extra information metadata on the edge
how could you use that like to what will
be the extra information and what would
be like the like the benefit he
mentioned like time that it like
minimizes the number of movements as a
witch had before well another
information that will be very relevant
is like rich read patterns like how many
times because like now I'm also liked
doing the implicit assumption that the
everything that you publish and
everything that you do is like
propagated and read but actually that
not true like many things that people do
they are not actually ever read so
actually if you could like have like
read patterns
of like particular people you could
actually incorporate that because they
will be like the links that have like
suffer much more traffic and actually
they could be linked that if you want it
like to go like one a step further that
you could actually say that I don't mind
if I have to go to fetch the data from a
different place because that happens
like very rarely well I prefer to keep
this particular link local because it
happens are a lot of times so that will
be like the that will be like our like
actually dream dream data to have like
proper grid patterns but we we do not
have them and there is no way to to get
them from the public API sand and so on
and all the things I'm have to right now
I'm so focused on the rib pattern that i
cannot think of anything else so any
further question what well done well
thank you very much for for your time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>