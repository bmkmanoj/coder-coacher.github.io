<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Automated Testing Patterns and Smells | Coder Coacher - Coaching Coders</title><meta content="Automated Testing Patterns and Smells - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Automated Testing Patterns and Smells</b></h2><h5 class="post__date">2008-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Pq6LHFM4JvE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thanks everyone for coming who else was
a two o'clock at Tech Talk 42 and crash
the Girl Scout party there that was
there one too here sorry for this Ted
there was just a misunderstanding with
the rooms so yeah today we have Jada
here talking about unit testing or
testing in general test patterns and
test smells I mean you see the book it's
fairly heavy
thank you finally like I think it's
three years ago that should I mentioned
to me the first time that he's working
on the book and we usually meet each
other
once a year at an agile conference and
every year after that we met it was like
oh yeah I'm still working on the book
but it's actually twice as heavy as it
was before
and so I'm happy that he that he
finished now because we had 20 boxes or
something carry downstairs here with the
books a year later and we couldn't have
done it any longer
so let I want to hand it over to Jenna
and to talk about unit testing all right
thank you
oh and stuff one thing I always forget
this this talk will be made available
externally on youtube so any questions
any discussion that you have please
remember don't talk about confidential
stuff alright well thanks for the
introduction work yeah mark said in on
my unit testing tutorial at the IGEL
conference I think was two years ago
wasn't it and so the material is already
fairly fairly well developed at that
point and you know the last year was
just spent getting it actually you know
copy edited and all that kind of fun
stuff I apologize in advance for the
size of the book the classic example of
what happens when you don't get frequent
feedback and the publication industry is
still a little bit behind in terms of
you know rapid deployment and getting
feedback etc so it wasn't until I
actually received the the PDFs of the
page proofs which is like two months
before it goes to the printers that I
realized that it was 900 pages till then
I thought it was 600 pages and just then
when they pour it into the style it it
grows a little bit so that's just a good
good example of the value of feedback
which is really what unit testing is all
about so what I wanted to talk about
very quickly today is
just some stuff about unit testing and
feedback on your both the behavior of
your system that you're testing and also
some feedback that you can get on your
tests themselves by looking for various
kinds of smells in your tests not so I'm
going to sort of talk very quickly a
little bit about some of the motivation
behind the why we're doing the testing
and why we're automating it and then the
bulk of the talk will be about code
smells and behavior smells and this is a
sort of a third-order condensation
because it's it's this is like a
one-hour condensation of a three-hour
tutorial I give it conferences which is
itself a condensation of a two-day
course which is itself a condensation of
900 pages so we'll go fairly quickly
through some of these things just so we
get a sampling of some of the breadth of
things so why do we need to test well
software being error is a rather complex
activity writing software and there's a
lot of details that we need to get right
computers aren't very forgiving not
nearly as forgiving as people and a
large part of software is about getting
these details right and a large part of
it is sort of communication between the
people writing the software and whoever
it is is telling us what that they
actually want built so there's a lot of
things that could go wrong I mean
there's lots of things that can go wrong
technically we're all humans humans make
mistakes we make little mistakes in in
how we code software lots of
possibilities there and some of these
things will be caught by our tools and
some of them won't be just like you know
spell checker won't tell you that your
grammar is wrong that you use the wrong
version of the word there or something
like that and also there's a lot of
things that can go around communication
wise on our projects in terms of you
know what does the customer really want
so lots of opportunities to go wrong so
the question is how do we consistently
deliver high quality software that
people find useful and over the years
we've developed all sorts of tools to
try and do this things like tools for
capturing requirements better designing
tools modeling tools higher level
programming languages syntax checkers
semantic checkers all sorts of stuff
like that but none of these things is
really a substitute
for actually trying out the software and
making sure that it works when we're
trying software out there's different
ways we can do this unit testing is
something that we do some people would
call this debugging some people call it
unit testing depends on what you're why
you're doing it um if you know there's
lots of bugs in there and you know you
have to find them then it's debugging
and we can do this kind of testing
several different ways one of the ways
we can do it is doing things at the unit
level so making sure our little piece of
software works well before we integrate
it with the rest of the software and
then there's of course the testing of
making sure that the software as a whole
functions and people would call this
functional testing customer testing
acceptance testing there's a whole bunch
of different names that goes by but it's
basically testing the whole system
altogether rather than the individual
components and of course we can do all
this manually that's sort of the more
traditional way of doing things and more
recently with people being pushing the
envelope about how to do a lot of this
stuff so that we can execute it
automatically the problem with manual
testing of course there's not very
repeatable most of us aren't very good
at remembering exactly how we tested a
piece of software that we tested today
yesterday there's been a long weekend
involved of course our memory diminishes
somewhat further and six months from now
we're going to be pretty hard pressed to
repeat the testing even if we had the
time and we're prepared to put the
effort into retesting everything
completely after we'd made some changes
to the software so that's one problem
and the other problem is the whole
communication side of things what should
a soft piece of software do if it no
longer does what it should do how can
you tell what it was supposed to do you
know the common answer is well you read
the code and find out what it's supposed
to do but if it's not doing what it's
supposed to do how can the code possibly
tell you what it was supposed to do and
that's one of the areas where tests come
in as they help describe what the code
should have done now that it doesn't and
so it's communicating through time if
you will into the future to someone who
has to maintain this piece of code that
that you wrote what the software was
intended to do when it was written as
opposed to what it actually does now
that something has changed in the world
so one solution to all this of course is
automated testing you just unplug the
wireless card there so that we don't get
any more of those
so why automate tests well
my example of the book publishing
problem lack of feedback automated
testing gives us very rapid feedback if
we can have our test run every time we
change the code then we get rapid
feedback on our software and how we're
doing and then the question becomes how
do we find the bugs how do we know where
they are well if we get the feedback
very rapidly we know exactly where the
bugs are because we just put them in ten
minutes ago and that's a much easier
problem to solve than to find the bugs
that we're put in weeks ago you know
through a series the long series of
changes so it actually makes finding the
the problems debugging a lot quicker if
we have the automated tests running very
frequently this is one of the
fundamental concepts in lean is that we
want to avoid waste and the whole idea
of putting defects in and then taking
them back out again spending effort to
take them out is a classic example of
the kind of waste that lean production
systems strive to remove and so this is
one of the quotes from Shay go sing go
is one of the sort of the big lean guys
who came up with a Toyota Production
system is inspection to find defects is
waste infections to prevent defect is
essential automated unit tests when
written before the software is actually
even built are an excellent example of
in process inspections to avoid
inserting defects so you can think of
unit tests as being defect prevention or
bug prevention bug repellent if you want
to think of it that way the approach
that we use to automated unit testing as
a protocol scripted test as opposed to
the recorded test that a lot of the UI
testing tools use and we do it using
things like various members of the X
unit family you can also do it in some
other things like for example the Water
Framework for Ruby you can do sort of
more full system testing that way or you
can focus at the unit test level for
testing individual
classes and methods so what does it take
to be successful at automating unit
tests and having these unit tests help
you keep your software soft to keep the
software malleable because that's really
why we're writing all these unit tests
well writing tests is a form of
programming so you need some programming
experience next unit framework is pretty
simple but you need to understand how to
use it and of course none of this is
useful if you don't know what are the
right tests to come up with so you need
a little bit of testing experience these
are all things that are fairly easy to
come by given you know a little bit of
time does this lead us to robust
automated tests and the answer is
unfortunately no so the next question is
what do we need to do and specifically
why do we need unit tests and component
tests as opposed to just system tests
and the problem is that the system tests
will tell us that there is a problem but
then we have to start digging to find
out where the problem came from which
piece of code is incorrect providing the
wrong results component tests will help
us zoom in and discover exactly which
component the problem is occurring in
because we should have a component test
failing in that component and unit test
will tell us exactly which class or
method isn't providing the same results
as it used to so one of the things that
we find is that when you have a good set
of unit tests you always know exactly
where the problem is that you insert it
because it'll pretty much narrow things
down to an individual method of a class
so whenever you have a system test of
some sort failing should also have a
unit test failing if you don't then you
really have a missing unit test and
that's one of the ones sort of the more
project smells that that we look for so
you'll hear me refer to various parts of
the sort of the testing infrastructure
in terms of the test then there's a
system under test is whatever software
that we are testing sometimes called a
set some people call it the cut or the
class under test or the object under
test the out these are all basic
referring to the same thing and
sometimes we would care very much about
what
that piece of software depends on
because very few pieces of software
completely standalone and you probably
heard of black box testing versus white
box testing in the style of unit testing
that most of us do it's all black box
but it's the question is how big is the
black box when you're testing an
individual class or method you should be
writing black box tests for that one
class or method and the reason you want
to write black block tests is so that if
you need to change the implementation as
part of a refactoring exercise you don't
have to change the test because the
software should still do the same thing
it how it does it might be a little bit
different but it would be providing the
same functionality so it's a good
example for the idea of design by
contract what is the contract that this
component is trying to satisfy so as
we're writing tests what are we trying
to achieve with these tests well before
we write the code we want the tests to
act as a specification for what done
looks like when I'm finished writing
this piece of code here's what the
behavior should look like so test a
specification after the code is written
the tests act is documentation of how
the software was supposed to behave when
the test fails it tells us this is what
it should have done here's what it's
doing now it's different tests a safety
net is another objective which is
basically rapidly tell us that
something's gone wrong so when we're
doing some you know interesting
refactoring to try and improve the
structure the software very quickly get
told that hey you just change the
behavior of something because the
refactoring shouldn't change the
behavior and of course as I mentioned
earlier defect localization in other
words finding telling you where the
problem is so you don't spend a lot of
time in a debugger and then sort of the
whole this isn't one of the objectives
per se of writing the automated tests
but something we need to achieve given
that we have a bunch of automated tests
as we want to minimize the cost of
having all these tests there so the
tests need to be fully automated meaning
you don't have to do anything manual to
run them no manual setup or no manual
inspection of the results the tests need
to be repeatable that is you can run
them more than once and they need to be
robust so that
time the tests continue to provide the
same results so a sobering thought in my
experience
well done automated unit tests you have
at least as much test code as production
code that sounds like a lot and so the
question is how do you make sure that
all this test code doesn't double the
cost of writing your software because
now you got twice as much software so if
you think in terms of your economic
payback if you're doing development and
this is what it cost you to build
something over time without test
automation adding test automation is
going to introduce some extra cost and
there's a big hump here where we are
learning stuff know if that's going to
show up well in the video but that pump
there is the learning curve and after
you've learned some stuff you know the
cost comes back down and what we're
looking for is to make sure that we save
at least as much effort on the you know
on the ongoing state so that's that sort
of you know the economic payback is that
we've saved effort through debugging
avoided etc to compensate for the cost
of writing these tests if it turns out
we don't save as much effort because
it's expensive to run the tests or that
we need to spend more time and effort
maintaining the tests over time we may
find that the ongoing effort is more
than it would have cost us than if we
didn't have the tests at all and that's
something we want to avoid because
you're going to be saying well gee why
should we be putting all this effort
into maintaining these tests if they're
actually costing us the test should be
helping us go faster and maintain
quality not slowing us down so a lot of
what I'm talking about here and in the
book is about what are the techniques
that we can use to ensure that this
happens that the ongoing cost stays
lower the net cost is lower when you've
got the automated tests so let's talk
about some of these things what are the
symptoms that we will see a lot of
you've probably heard about the concept
of code smells Brian foot likes to talk
about the analogy you know with when
you've got wine snobs you know in there
swirling their glass and sniffing aha
there's a hint of strawberry and a
little bit of grass you know well when
we're talking about about code smells
hmm there's a bit of duplicated code
here and some hard-coded values same
kind of idea and so what the smells are
is they're symptoms that we observe in
the code or in the behavior of our tests
that tell us there's something we should
pay attention to here and the analogy
comes from originally the martin
fowler's refactoring book where he tells
a little story which is attributed to
camp back which is when he had his first
kid asking grandma back how do I know
it's time to change the diaper and her
response was if it stinks change it and
that's where we're talking about here
with code if the code stinks the odds
are you need to do something about it so
a smell is the thing that grabs you by
the nose it's the it's whatever you
actually observe in the code or see in
the behavior of the test and then we do
a root cause analysis ask why is this
happening and you may have several
different root causes for the same smell
and some smells have many different root
causes some root causes may indicate
several smells so I classify smells and
test in three categories one is the
obvious one of code smells so this is
smells that you see when you're looking
at test code behavior smells are test
Behaving Badly you're running tests and
something is happening that's not being
very helpful
in terms of understanding what's going
wrong and I'll show some examples of
these of what it looks like when you've
got some of these behavior smells and at
the third level at the project level
project managers and customers may also
detect some smells in terms of things
then I don't have time to go into that
today I just mention them in passing the
interesting thing here is that these
smells are related and that the code
smells are often one set of symptoms
that may also be are corroborated by
behavior smells and you may also see
project smells is all related to each
other all being traced back to the same
root cause so you actually have several
opportunities to detect the same problem
I'm sure everyone here is familiar with
patterns the idea of a recurring problem
- a recurring solution usually
independently and invented by a number
of people and someone like me comes
along and said well I've heard about it
from these people and I've heard about
it here and I did the same thing oh
three examples makes a pattern and you
know writes it down and so that's
largely what my book is being about is
taking these you know well-known
practices that people were doing on
various projects with automated testing
and writing them down and giving them
some name trying to find consensus in
the community about what's the right way
to describe this particular way of doing
automated testing so test patterns is
specifically the idea of applying
patterns to test code and a good example
of a test pattern would be a mock object
in my book I sort of classify the test
patterns at a number of different levels
we've got high level automation strategy
patterns like recorded tests versus
scripted tests we've got more detailed
design patterns like implicit setup of
fixtures versus delegated setup there's
patterns around how do you code certain
things like assertion methods and
creation methods and then you get all
the way down to language specific idioms
about how you do certain kinds of tests
like expected exception tests are done
quite differently languages with blocks
their languages that don't have blocks
so let's look at some examples for code
smells so you'd notice that you have a
code smell when you're looking at tests
in the tests are hard to understand the
tests may contain coding errors that may
result in things like missed bugs or
erratic tests which is one of the
behavior smells that I'll give an
example of a bit further down or you may
find that tests are very difficult to
write or maybe even impossible to write
because the right design for testability
isn't present in the code that you're
trying to test
maybe you can't invoke the functionality
directly maybe you can't just an she ate
the object you're trying to test maybe
you can't get it into the right initial
State or you can't observe the the
resulting state after your test is
executed so these are all examples of
symptoms that might tell you that you
have a code smell here
so some common code smells are things
like conditional test logic and tests
hard to test code obscure tests test
code duplication test logic and
production and so on
so let's look a little example this is
sort of the classic example of invoices
with addresses and customers and that
kind of stuff on them let's look at what
a test might look like and this kind of
test is pretty typical when I get
involved in a project and people of
being you know learning how to do
automated unit tests and they're trying
to get some existing code under test
they might end up with a test that looks
something like this and I mean you may
have trouble reading this especially on
the recording because there's just an
awful lot of lines of code on this test
it's about 35 lines of code and that's
way too much code in a single unit test
so let's break this down a little bit
and focus on different parts of the test
and let's go looking for some test
smells so here's the middle part of that
test which is actually verifying the
outcome there's a bunch of code here
that is looking at various aspects of
the invoice that we have created here
and looking at things so let's look at
some things what's wrong with this test
well here down here at the bottom we've
got this assert true/false now anyone
who's used j-unit a fair bit probably
realizes that that's kind of a strange
statement I mean you're supposed to have
a conditional expression in there on the
assertion and here we got a constant so
this test this assertion will always
fail so a very simple refactoring of
this obtuse assertion is to replace it
with an assertion that's a little bit
more intent revealing let's just say
fail that's a much better way of saying
exactly the same thing now this actually
belies a larger problem which we'll get
to in a minute first we've seen a bunch
of hard-coded constants here in our
tests and there's probably some
relationship between these numbers and
some numbers earlier in the test but
what is that relationship someone's done
the math in their head type the result
into the system so the person reading
this test six months from now doesn't
understand the business rules they're
not evident
in this test the other problem was
hardwired tested is it can lead to
fragile tests when the when these values
are used as unique keys and databases
and so on so what could we do well one
thing we could do is compare all these
expected values with expected values
that were the actual values with
expected values that were calculated and
here we don't see them being calculated
cuz we're only looking at one part of
the test but one of things at this
points out is we have a large number of
assertions and we're comparing field by
field to different objects an expected
object and an actual object and why we
need to compare them field by field this
test is too verbose and if I've got half
a dozen tests and they're all doing
these same six assertions in each ones
I've got a lot of duplicated test logic
so why don't I do that out using a
simple extract method and have a simple
assertion here now I could just boil
this down to an assert equals but there
may have been a reason I was comparing
those six fields because there were a
bunch of fields I didn't want to compare
and that's an example of test specific
equality there may be fields that are
don't cares and I want to make sure that
I don't compare them or it could be that
object equalities not what I'm looking
for I'm only looking for equivalence not
equality so in this case I've introduced
a custom assertion which is an assertion
method that I wrote nothing says that
only the creators of j-unit can can
write assertions everyone can write
assertions whatever makes your code easy
to understand whatever expresses your
intent well so here assert line items
equal takes two line items it behaves
just like a certain equal except that it
has logic in it that compares two line
items with what we care about in this
test makes our test a lot simpler
that lets us boil the tests down a bit
more here and now we can see some more
interesting things in this bit of
assertion logic so the next thing we'll
notice here is we've got an if statement
in our test now remember I said the
trick is to try and keep from having the
doubling of our code base because we've
got as much test code as production code
from increasing the cost of our tests
and one of the ways that we avoid that
is by not having any conditional logic
in our
methods problem with conditional logic
is you're not quite sure what you're
testing and it's a slippery slope once
you start introducing any conditional
logic and let into the body of the test
method whether you're actually testing
in potentially different circumstances
because you don't have enough control
over your test fixture or whether you're
just dealing with different cases of
things that the system might have done
instead it's a lot better to be very
declarative in your tests and in fact in
this case it's very easy to replace this
if statement with a guard assertion so
instead of saying if this do this
asserting otherwise do this other
asserting or just fail in our case we
can say assert equals make sure the
number of items is 1 if the item the
number of items is not 1 then we don't
proceed on to the assertion of the the
line items so here we reduce the amount
of code it takes to verify that the
outcome has occurred what the outcome
should be you know from 8 or 10 lines to
just a few lines so significant
compaction so let's go look at another
part of the test here so that was the
part where we verified the outcome let's
look at this stuff at the bottom here
sorry before I go ahead the stuff at the
bottom of this picture this finally
Clause what's that doing there so we do
this try do some stuff including
verifying the outcome and then we do it
finally and then the finally we're doing
some delete objects we delete the
expected line item we delete the invoice
the product etc so what's wrong with
this code why did we need that finally
and is this actually doing what we think
it should be doing so the reason the
finally is there is because when a test
fails assert throws an exception and if
we just did the assert and then we did
this clean up logic then the clean up
logic would never run which gives us a
hint as to what's wrong with this clean
up logic other than the fact that we
actually have clean up logic which I'll
get to in a minute is that assuming we
did get to this finally something went
wrong now is there any guarantee that
nothing else will go wrong here what
happens if delete object expected line
item fails will we delete our invoice in
our product etc probably not and what
that would lead to is when we're running
other tests or the same test at a future
time we may have leftover stuff lying
around think of it as test data object
leakage so like memory leakage is just
as insidious you don't know when it
happened when you detect it it's way too
late to figure out how it happened and
so you really want to try and avoid this
so if you're trying to do cleanup in a
less naive way you would have to make
sure that you do a dry finally around
each of these and that starts getting
really ugly and you thought that the the
code was very verbose before and hard to
get right well this just made it even
harder so it's better to try and avoid
this problem entirely so one option is
to move all of this out of the test
method into the set up into the teardown
method on the test case class which at
least puts it into one place so you have
to write it once but then it has to be a
bit more generic because it needs to
handle the situation for any of the
tests on that class but you still need
the nested try final ease there you only
get rid of one layer because the
teardown method is essentially a finally
on your test case class because it gets
run after every test method whether they
pass or fail better solution is to not
deal with this on a test by test basis
but rather build a generic mechanism to
keep track of any test objects that you
have to delete at the end of tests and
something simple like having a call to
add test object which adds it to a
collection which you can then later
iterate over and have unit tested that
delete all test objects method that we
see at the bottom to make sure it really
works in all the circumstances this is a
better solution if you really need to
have teardown logic in your tests but
having the teardown logic itself is a
smell because that implies that you have
some kind of persistence going on in
your tests and except for tests that are
testing your persistence mechanism you
really should try and test all your
business logic with no persistence layer
present in other words testing the
objects purely in memory
and that forces you to think in terms of
design for testability of having clean
separation between your business logic
and your persistence logic which is a
very good thing for a bunch of other
reasons as well so assuming you've done
one of these all of our test logic disap
all of our teardown logic disappears
from our test so that makes our test a
little bit more compact and now we are
down at the point where we can almost
read the whole test sitting and looking
at it on the screen here but there's
still lots that we can do here to make
this test better so what else let's look
at this front part of the test here
which is the fixture setup so here we
have a bunch of calls to create some
objects so we're creating a couple of
addresses we're creating a customer
we're creating a product and an invoice
in here we're seeing all sorts of
hard-coded test data and that may lead
to unrepeatable tests if that data is in
any way required to be unique especially
if I start cloning the same fixture
setup from one test to another and if
there's any persistence involved but it
just makes this test heart to understand
what is all this stuff doing here what
are we actually testing well let's look
at the name of that method test add item
quantity so seems to be something to do
with this method at the bottom here of
add item quantity to this invoice for a
particular product and quantity so
what's all this address stuff got to do
with adding items now one of the
principles here is if it's not important
for it to be in the test it's important
for it not to be in the test so one of
the ways of saying that the stuffs not
important is instead of using literal
values just generate some values and you
know so here we're calling a method get
unique string so we're just getting some
string and including it as a value that
at least lets the reader know that it
wasn't important to the outcome but we
can go one step farther and say to
ourselves well if this stuff is going to
be just generated and unique do I even
need to have it if it's irrelevant
information let's see if we can get it
out of the test
early so let's just remove all the stuff
from our test and see what we end up
with so we can use the concept of a
creation method so instead of using the
native object constructors because
that's what we happen to have which take
all these arguments we can hide that
behind a test object factory which may
be our own test case class and have
these methods create anonymous address
which says oh just give me an address I
really don't care about it but what it
contains I just need an address because
later on here I'm going to use that
address as an argument for my customers
so our test is a little bit more compact
now so now we can start working on our
test while looking at all of it at the
same time so looking again here at our
two calls to create an anonymous address
why do we need to create these addresses
well it's because we need them as
arguments to create customer why do we
need to create a customer well we need
it as an argument for the invoice okay
do we really need to create these nonce
addresses in every test where we need to
create a customer probably not so why
don't we factor out that irrelevant
information and hide that behind a
simpler version of create anonymous
customer
I need a customer I don't care anything
about this customer nothing's important
about it I just need one so we hide that
behind this creation method we can even
go one step farther and say why did I
need the customer it's because I needed
for the invoice constructor
okay well let's create a factory object
for the customer for the invoice and
have it create the customer for us it
wasn't important to be in the test it's
important for it not to be in the test
so now we've reduced our fixture setup
from a dozen lines of code to just a
couple and it's just the essential
objects that are needed for that one
method that we're testing so now let's
go back to the bottom of our test here
and take a look at some of this code now
that we've simplified the rest of the
test you know we can sort of see where
there's still some stuff that looks a
little bit unnecessary so what's all
this code doing down here
we're asserting we've gotta work getting
the line items from the invoice we're
asserting that there's one of them and
then we're getting the first one the one
at position zero and then we're
asserting the line items are equal what
are we doing there in plain English what
we're really doing here saying there
should be exactly one line item and it
should look like this so why don't we
say that directly so by creating yet
another custom assertion we can see a
certain one line item and here's the
expected line item so we've gone from
something like thirty five lines of code
in this test method to five lines five
executable lines of code so that's a
pretty severe compaction on this test so
when we look at our test method our test
class and look at all the methods on
that test class we can see what we're
testing here so we've got you know test
item quantity one item several items
duplicate product 0 quantity etc so
we've got nine knives now here a nice
list of test conditions so let's quickly
zoom into one of these other test
methods to see now that we've built this
little infrastructure of factory methods
and custom assertions etc what does this
do to how quickly we can write a test
like ask yourself how long would it have
taken to write that 35 line test method
and if we say look at several items here
how long will it take to write that well
we have to add another product because
we're going to test here creating an
invoice with two different products on
it so we added sorry we add another
product we call add item quantity twice
we create another expected line item and
now we assert exactly two line items on
this invoice and so this this test I
actually wrote on the phone while
talking with with a co-worker we were
doing this presentation a long long time
ago for an agile users group meeting we
say oh we should show what it what the
new tests would look like as well let me
just code that up and while we're on the
phone I wrote this test in 30 seconds
now that's an incredible productivity
in terms of writing tests and it's
enabled by having all these utility
methods that we've created in the course
of refactoring this one big test now you
can get here either by refactoring from
big ugly tests or you can start out by
saying to yourself as you are writing
what am I really trying to test here and
instead of focusing on what you have
available in terms of methods that you
can call work outside in and create the
methods that don't even exist yet type
in your test the way you want it to look
and then use your IDE to help you fill
in the bodies of those of those methods
and so you're actually driving the test
infrastructure from your test you could
write half a dozen of these tests before
even bothered to fill in an
implementation for assert exactly two
line items the other thing you want to
try and avoid when doing this kind of
stuff is avoid over generalizing do you
need to be able to compare three line
items five line items etc which is
actually a much harder problem than
comparing two and so what are the cases
you need to cover there are no line
items there's exactly one line item and
there's two once you've got two it
really doesn't add any value to do three
items four items etc so it's actually
much simpler to write exactly two line
items than to do even three so that
brings us to the end of the code smell
section that's just a sampler there's
lots of more code smells but I wanted to
get to behavior smells and behavior
smells are problems that we see when
we're running tests and the problem
could be that the tests are passing when
they should fail or fail when they
should be passing and the problem could
be that the tests are coded wrong the
root cause could be that the tests are
coded wrong or there's something wrong
with the system under test itself so
let's look at a very short list of them
here things like slow tests or a problem
tested or erratic and there's a lot of
different reasons for them being erratic
tests that are fragile
let's look at an example here of what
we're talking about when we talk about
slow tests one of the things that I
mentioned early on is it's good to have
rapid feedback
and when we're building functionality we
want to get feedback on the
functionality that we've built we're
editing the code on a regular basis and
every time we edit the code we could
introduce defects and we should be doing
some kind of regular continuous
integration build or daily build or
something so each of these times it's
important for us to be able to build and
run our tests very quickly if it takes
us too long to run our tests we will run
them less often if we run them less
often we get feedback last read less
regularly which means we've introduced
more defects between when we introduce
them and when we finally got around to
running the tests and finding them which
means we will now do a lot more
debugging so it's really critical to run
our tests frequently which means they
need to run fast so if you've got tests
that are running slowly we're going to
find that there's a lot of pressure to
make them run faster and there's a bunch
of different ways to avoid slow tests
things like you know get yourself some
faster hardware or avoid calling slow
code like avoid the database fake out
the database when you're running your
tests and so on or you can run fewer
tests these are all legitimate ways of
doing things one of the ways that you
want to try and avoid is to use a shared
fixture setting up a fixture once and
using it across many tests is a false
economy because that leads to a bunch of
problems other smells and a good example
of these smells and I'm just going to
skip over the shared fixture stuff here
because it's just you know fairly
standard ways of setting things up so
I'd really want to talk here about the
symptoms that you're going to see
erratic tests is when you're running
your tests and they're giving you
different results at different times and
so if you're running your tests here and
you've got a test that accesses an
object and this is a shared object the
next test comes along and updates a
value on that object say the value of a
is he changes it from 5 to 99 another
test comes along and looks it and it's
expecting 99 because it usually runs
after the second test if it turns out
that the second test failed and didn't
update it from 5 to 99 the last test
here which expected it to be 99 will
also fail
because the fixture isn't set up
correctly so there's nothing wrong with
the code what's wrong is that this Vic
test depends on another test to have run
successfully before it if you try and
run this test by itself
it'll probably fail I know that gets in
the way of trying to debug you know if
you want to run just one test and you
can't run it by itself another example
of an erratic test is an unrepeatable
test same basic scenario the first test
reads five the second test updates it to
99 third test reads 99 now we run the
same test suite again if the fixture
persists from one test run to the next
for example a database test number one
reads it expecting a value of five what
if find us that is 99 so now it fails
and you can get this situation where the
first time you run your tests some of
the tests fail and then subsequent runs
they pass or you could get the opposite
they pass the first time and they fail
in subsequent times if you've got both
kinds of tests in the same test suite
you got the pass fail fail kind and the
failed pass pass kind what you actually
see is the first time you run the test
test number three fails and the second
time you run your test test number five
fails and it looks like the failures are
moving around which is really
disconcerting which is the same symptoms
that you see when you have a test run
war a test run war is when you actually
share a persistent resource like a
database among several test runners and
if you're running these tests
simultaneously you can actually have
interactions going on between the test
Suites being run the same test suite
being run simultaneously on different
machines for example and you get these
random and these really are random
failures because it all depends on when
you run your tests the really insidious
thing about this is this problem happens
more when you're coming to a deadline
and this isn't just Murphy's Law this is
a real fact that what happens is when
you're writing new functionality you
only check in you know once every few
hours or a few days when you're fixing
bugs you might be checking in every 30
minutes after you find a bug fix it
check it in so more people are running
the test they're running a little more
frequently and therefore the odds of
actually colliding in these test runs
goes up and so the only way to really
solve this
to get everyone to stop you don't have a
token say I've got the token no one else
run any tests until I finish running
them okay mine ran clean okay now you
can you can run your tests so this is a
really nasty situation that you want to
try and avoid there's half a dozen
different root causes of erratic tests
there it's complicated enough that in
the book I've got an actual flowchart to
go through and ask a bunch of question
to figure out which variation it is the
ultimate solution for avoiding erratic
tests is to use a fresh fixture and a
fresh fixture is a fixture that you
build in the test and you throw away
when the test is done so each fixture
gets used exactly once
a synonym for the shared fixture is a
stale fixture it's a fixture that's
being used it's a previously owned if
you will right someone else has had this
fixture they've mocked with it you don't
know what the state of that fixture is
so as a rule you really want to avoid
raising fixtures across tests and
especially across test runs because it
creates all these erratic behaviors that
will just get you into trouble
if you really have to use a shared
fixture one option is to use an
immutable shared fixture in other words
build a bunch of reference object and
make sure none of your tests touch them
and then any objects that you do plan to
either modify or delete create them
fresh in the tests but maybe these other
objects that you have are needed to be
there to be referenced from the object
that you're testing and if you do need
to do a shared fixture make sure you
build it in a new version of it in each
test run otherwise you'll end up with
all these repeatable test problems
another prop common behavior smell is
fragile tests so tests that worked
yesterday but they stopped working today
you change something and these are
there's four variations of it interface
sensitivity which is the API or the user
interface using has changed behavior
sensitive is when the logic underneath
changed and you of course you expect
tests to fail when you change the
behavior but the question is how many
tests are failing
if tests that don't test that behavior
are failing what that implies that
you're using that behavior to set the
state of the system for
some other starting point of some other
tests and you really should have a way
of getting the system into that state
without having to exercise all the
behavior to get there so it's a good
idea to try and encapsulate a lot of
that from your test so that you don't
have to a lot of that logic duplicated
from test to test to other causes of
fragile tests or data sensitivity if the
tests depend on what kind of data is in
a database and that data changes and
this most commonly happens when a whole
bunch of tests use a common sort of
standard definition of the database and
then someone changes that standard
definition to add more tests but
accidentally break some of the existing
tests and these are all avoided by by
using fresh fixtures where you build a
custom fixture for every test every time
you run it in context sensitive is when
something outside the control of the
system under test changes for example if
your test is if the logic of testing
depends on time or date and you're using
the real system clock in your system and
you have no way of stubbing it out and
controlling it from your test you will
have context sensitivity you're
guaranteed that at some point your tests
will fail because you're in a month with
a different number days in it or a leap
year or who knows what
so to avoid interface sensitivity we can
make sure we use stable interfaces or we
can encapsulate the API from our tests
there's techniques for avoiding data and
contact sensitivity around using fresh
fixtures and test stubs and the last
smell I'm going to mention is hard to
test code because we're just about out
of time and here the issue is that
you're trying to write a test and the
test is either very difficult to write
because the right functionality isn't
available for testing or you're reading
tests and the problem is you can't
figure out what the test is doing
because it's sort of going through
convoluted paths testing the software
indirectly or you know other things that
is that you just can't get to the stuff
that you're trying to test there's a one
of the chapters in the book is dedicated
to test doubles one of the problems I
ran into on project after project is pee
we're using a different terminology to
describe the same behavior of test
doubles or the same terminology to
describe a totally different set of
behaviors so one of the things that I
try to do is to be very clear around
what are the different kinds of test
doubles so we distinction between things
like mock objects versus test stubs and
test spies and I won't go into any more
detail on that right now just because
we're at a time but that's it's very
important from a perspective of
communicating amongst members of the
development team of when you're talking
about how you're going to test something
is which variation that you're using so
that everyone is clear is how to go
about building those particular test
devils and just to wrap things up
hopefully this has given you a small
sense of what are the different kinds of
test smells that you might want to look
for and what are the different root
causes some of those smells might have
just remember the smells are the
symptoms that you see and so the root
causes are you know you asked why are we
getting the smell why are we getting the
symptom and there's been I've introduced
a few of the patterns about how you can
go about addressing these things
obviously it's a 900 pages you're not
going to get more than 10% of it covered
off in a short presentation like this
but give you something to start looking
for when you're reading test code or
running your tests and a very simple
recipe for being successful is to start
off by writing some tests start with
some easy ones don't start with legacy
code that's the hardest kind of test
code or production code to test why do
you think I have all this gray hair from
trying to test legacy systems as you
write tests look at your tests
note what test smells you discover run
your tests note what behavior smells you
might see ask yourself how can i react
to this test code to make it simpler
easier to understand or avoid some of
these behavior smells apply the
appropriate patterns that that address
that was particular smells and then
write some more tests and continue and
just constantly strive to make your
tests better
and that should get you a good set of
automated tests that are repeatable
robust etc the other thing to be note is
you have to be pragmatic you're not
going to get rid of all the smells
sometimes you're going to have some
smells so what is it costing me to have
the smell present can I live with it is
this the most important smell to to work
at removing how much it will cost to to
remove and sometimes you're just going
to leave them there and say not the most
important thing to deal with right now
otherwise you can get sort of jammed
trying to deal with this one test with
this one smell and you could be doing
ten other tests instead of working on
this one so what does it take to be
successful you need some experience
programming you need some experience
with xunit some testing you also need to
think about design for testability and
the nice thing about doing test-driven
development is that you are building
testability in as opposed to
retrofitting it on after the fact you
want to remove your test smells apply
the appropriate test automation patterns
and just continue to have a fanatical
attention to test maintainability
because that's what it's really all
about and that should lead you to robust
maintainable automated tests that's it
now open up so open up the floor to
questions
but that sort of voice the problem that
you still want to be able to test the
parts the example
you have any thoughts okay I'm gonna try
and repeat the question
highly paraphrase because I was a long
question first of all I'd like to thank
you for having actually read parts of my
book because I didn't talk about that in
this talk so the comment was that one of
the ways to avoid heart to test code is
to and particularly synchronous code in
other words where code where we're
launching another process and making
sure that it did some stuff is to test
the launching logic separate from the
logic that runs in the other process or
thread and that's that's just good unit
testing practice to make sure that you
break things down and test the concerns
individually so the question was do I
have any thoughts on how you can go
about actually testing the integration
of these little pieces assuming that the
little pieces have already been tested
independently and there are actually
some techniques for testing across
threads the biggest issue of course is
that if you try and do an assertion in
another thread you won't actually catch
it on the current thread when it fails
so that's the nasty surprise is that
especially if using mock objects which
throw assertions and you run those mock
objects in that other thread they can
throw the assertions all they want and
nothing will happen there are some
techniques in Java for how to catch
assertions across or exceptions across
different threads there's actually a
section I don't cover in my book because
my book is already too big but I do
remember some examples on how to deal
with that in the in another book which
is called unit testing with Java by
yo.hannes link I think is the the lead
author and he's got some good examples
of how to catch exceptions across
threads and the other issue with testing
that way is that as soon as you launch a
thread from within a piece of code
that's running from a test you're going
to have to wait because now you've got a
synchronicity in your test and that
introduces a form of slowness and tests
and so all of a sudden you're going from
tests that will run in milliseconds to
tests where you may have to wait one or
two
real full seconds and so you don't want
to run those tests in your unit test
suite you want to separate them out into
a test suite that you run less
frequently because you're not going to
want to wait for a hundred tests at each
delay for two seconds because now all of
a sudden you have a test suite that
takes minutes to run instead of seconds
to run so that's another consideration
and I hope that's satisfies your youth
problem would be not well yeah the
non-determinism that that is introduced
by having the asynchronous code is
normally addressed by putting a long
enough delay and to handle the worst
case and that's the problem is that to
make the delay long enough to make it
deterministic makes your tests really
slow and the reason why we want to try
and avoid those types of tests in a unit
test
perhaps
just lightly happy happy
yeah that's a good question so how much
functional testing do you need to do
relative to unit testing and the two
approaches are do lots of functional
testing or to do the bare minimum
automated functional testing supported
by a large amount of unit testing
hopefully that that summarizes your
question just the other day actually
heard my cone talking about this and his
comment was you want to think in terms
of a test automation pyramid and the
pyramid the base of the pyramid is your
unit test and so you have a very broad
extensive set of small unit tests that
test individual little bits of logic and
then the very point of the pyramid is a
small number of functional tests that
verify that the integration between all
the software has been done correctly and
you don't really want to try and test
all the logic in the system through the
functional tests that really should be
tested by the appropriate unit tests in
between you might have a smaller layer
of component tests that maybe are
testing some of the business logic
components and in my experience that's
an area where we often use fit to test a
component that contains all the business
logic and test it in business terms so
that we don't have to you know trust you
know the translation from business terms
into technical terms now having to be
done but rather we have the business
people involved who are who can say yes
this is the right way to describe the
problem they're actually writing those
fit tests themselves and so you have
this large number of thousands of unit
tests you know tens or hundreds of fit
tests and just a few you know half a
dozen dozen whatever is the minimum
number you can get away with functional
tests that focus on integration of
things as opposed to testing the logic
thoroughly that's my strategy anyway
any other questions well those are two
very good questions
well ultimately the the challenge with
testing asynchronous callbacks is that
these two things really one is does the
callback code behave correctly and the
other is is the callback being done at
all and you can actually if you think in
terms of breaking that apart and doing
doing the testing those independently
you can use different mixes of what part
of this sort of relationship between the
original code that setup the callback
the thread that is doing the callback
and the code that is being called back
you want to again this is the same sort
of asynchronous problem that we talked
about earlier and the idea is you want
to try and test each of those
independently and depending on which
piece you're testing the which piece of
software is actually being replaced by
the test and which part of that triad is
being replaced by some kind of test
double that verifies that the right
calls are being made will be slightly
different so if you're testing the code
that is should be doing the callback
then you would have your test driving
that code directly not asynchronously
but in from the same process and you'd
be mocking out the actual called back
logic with a mock object just to verify
that in fact it is being called so
whatever method should have been called
in the callback is what you'd be trying
to mock out so that's just an example of
sort of trying to sort of find what's
the smallest piece of code I can test
because that's that's what makes it a
unit test Michel feathers has a really
good little description and in his book
on legacies I think he's got in his
legacy software book but I reproduced it
in in my book as a sidebar which he
calls a unit test rules and if you if
you google that you'll see the five
simple rules that he has and and one of
the rules is if it if it's anything to
do with a database it's not a unit test
with anything asynchronous is not a unit
test if it involves a user interface
it's not a unit test and so on so it's
like a set of sort of you know
violations of if it's any of these
things you know you want to think about
how you can test it without doing these
things and it's very very practical way
of forcing you to think about how to
less less code at the same time you
still want to test all the code but just
don't try and test it all at once
all right well thanks everyone for
coming though some great questions and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>