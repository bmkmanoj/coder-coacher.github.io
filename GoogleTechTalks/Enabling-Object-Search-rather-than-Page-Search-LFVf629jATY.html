<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Enabling Object Search rather than Page Search | Coder Coacher - Coaching Coders</title><meta content="Enabling Object Search rather than Page Search - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Enabling Object Search rather than Page Search</b></h2><h5 class="post__date">2008-03-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/LFVf629jATY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">thank you all for coming on such short
notice great pleasure to have Andrew
McCollum here flawed you know Andrew
this works but Andrew was the director
of research at miss Payne labs and then
went to UMass Amherst where he's an
associate professor so the one summer
that I spent at whiz-bang I was lucky
enough to be there with Andrew and
Fernando at the same time and actually
Dave's flies Dave lie and drew back now
and lots of other people so and before
that andrew is a postdoc at CMU in
Rochester he's done a lot of great work
on information extraction and mining
knowledge from unstructured text and
some of you may have seen the site rexha
info that's a pretty extensive library
of academic papers and it's a very
similar to Google Scholar its mind
completely without feeds for partners
some of my google scholar so it's a kind
of pure experiment in information
extraction and tries to treat a lot of
things as high level objects I think
that's a good emblematic of the kind of
work that Andrew does this also has a
lot of great theoretical work on CRFs
and associated learning algorithms so
the whole spectrum anyway the talk will
be videotaped so tell people who weren't
able to make it to just grab that off
the fish page and Andrew will be around
also for dinner so if you're interested
in joining for dinner than dr. burning
all right Sam thank you so much in the
invitation and for that kind
introduction that very complete
introduction so it's great to be back at
Google I think this is maybe my third or
fourth visit and I was actually before
coming thinking back to the first talk
that I gave at Google which was back in
back in two thousand to see a little
frozen here
okay even escape it's not doing anything
for me are y'all filling with some soft
extraction library called mallet wait
Andrew his group maintain and every
suggestion said I fear with the world
and seen this before it's yeah I get off
there yeah and it's got contributions
from all kinds of people that you love
and trust like Andrew and Charles and
lots of incredibly competent people so
if you're looking for an open source
about sort of almost industrial strength
library you should check out Mallon all
over X is basically built on top of it
so that give you a sense of the scale as
we can run how am I doing here in jerz
you're doing great Sam bra so much it's
microsoft off fascinating briefly when
he first got to umass him and is what we
family decided to build an entire new
house which is absolutely beautiful
because it you should get a chance to
check it out but um they did the whole
planning and everything all from the
ground up Andy Andrews kind of in this
weird to body state for a while where he
has like one job as his own contractor
and then a second job as an academic
you're interested in design principles
and what it should look like when you
look from your kitchen through your
living room to the front door that you
can a ski and I've had it in particular
about a great book recommendation
Alexander's a pattern language and do a
lot of software engineering people in
need also you have fantastic book I'm
running it okay try okay Microsoft is
just catching everything ok ok here we
go hopefully this doesn't ring here
alright so i was thinking back to the
first time that that i gave a talk at
Google ok and when I presented I mean as
a representative whiz-bang labs actually
a talk on turning the web into a
knowledge base and I still have the
slides of it all the way back then and
yea which was essentially about
extracting objects from the world I was
actually here somewhat part of a secret
mission to try to get google to buy
whiz-bang and the answer that we got was
well no we really don't think that
Google's really ready to buy any
companies and so you know we won't they
look you look very attractive but you
know and you know of course aha things
have changed a lot at Google since that
since 2002 so I'm I argued at the time
that this is also a slide from that
previous presentation that it's often
the case that that people are searching
for pages but people are also very often
what they really want to find is an
object I want to find a company or a
person or a course and they want to be
able to do this search in specialized
ways and one piece of evidence that this
is in demand are that there exists sites
out there that support very object
specific search this is a website that
supports search / companies this is
actually an old slide you can tell the
company's name has changed elegon com
now zoom info does search over people
this is a search over courses offered at
universities all across the country and
you know why would we prefer to prefer a
thing or object search / paid search it
offers you a targeted restricted
universe of hits oh you even don't show
me resumes when I'm really looking for
jobs the vocabulary might be the same
but I'm looking for jobs you can do
specialized queries that are
topic-specific or based on
multidimensional features or information
that's spread across multiple pages you
can get the correct granularity and you
can provide some specialized display in
a way that's super targeted for the
kinds of slot values that these object
types present an additional attribute
that I've always found especially
fascinating is that once you've
extracted information about structured
structured objects you can do
traditional kinds of data mining on this
structured data to find patterns found
outliers provide very targeted kind of
decision support and let me just give a
few examples of this combination of
extracting structured knowledge and
doing data mining and then and then
we'll move on to some some technical
items so the first thing that we did at
this company whiz-bang labs was to build
a system that extracted information
about job openings directly off of
company websites the system would begin
at the homepage of a company
automatically navigate through it to
find
section where they list their job
openings and use some statistical
machine learning methods to segment the
job openings from each other
automatically extract the job title the
description area of the country where
it's offered contact information etc and
we launched the website which scared the
pants off of monster because it had more
than twice as many openings is bears did
and provided a very structured targeted
kind of search in which could specify
exactly what you were looking for and
get back a very targeted nicely
presented set of results exactly in
terms of job title and company and date
a twist at which the posting Lisp was
first placed on the web etc but part of
what I found most fascinating is that
then now once we had this this large
databases that we could mine this
database and in fact we did that once a
month whiz-bang ran various data mining
algorithms and discovered job trends
across the country you know high tech
jobs rising in the south and falling
elsewhere and we actually heard
anecdotally that several state and local
governments were using this data to help
set governmental policy because nowhere
else could they get data that was this
comprehensive or up to date just a
couple of more examples back in the late
90s at carnegie mellon i built a system
called kora that did information
extractions from research papers we
would take the PDF convert it to text
use the machine learning methods to
extract titles authors institutions
abstract citations etc and here's a
screenshot of what we built of course
now there's sightseer and google scholar
and and others and this also not only
allows you to do search for for research
papers but also to do some kinds of
mining here's a very limited kind that
comes from sites here it's the list of
the most site highly cited authors in
computer science modulo some you know
some person coreference errors and
here's something a little bit fancier
you can run some topic models over the
text of the papers to discover different
research topics and also the people who
are associated with those so
here this topic handwritten character
recognition you can see here the system
is saying the Patrice simard is a good
expert in this topic which is actually
true and lastly a project that we've
considered at UMass is working with data
it looks like this the Chinese Academy
of Sciences has a few hundred thousand
documents that are literally several
millennia old they are Qing Dynasty
archives memos newspaper articles and
these contain a wealth of information
about all kinds of a topics and one of
them is the weather people in these
articles talk about you know my cherry
tree bloomed early this year or my crops
have flooded in the no I feels a flooded
and my crops are bad and and these are
scientists who are trying to understand
long-term weather trends and this data
so valuable to them that they're having
humans go through these documents to
extract information about weather events
but they're having a little bit of
trouble and so we've considered other
some ways that we can use automated
methods to help them out so as a
precursor to this kind of mining or
object search we need to take raw
unstructured text and turn it into
objects which is what we call
information extraction right we start
with some text like this and a database
with the fields defined and we want to
extract from this text the information
to fill those fields and as a task this
involves doing segmentation to find the
boundaries of the text reasons we're
interested in classification to say that
no this is a company name this is a job
title this is a person's name
Association to say this person had this
title at this company and then
clustering or deduplication to collapse
duplicate information so we don't get a
messy database with with a lot of
duplicates all right so people have been
using machine learning methods to
address all of these all of these steps
and you know in some cases there are
even relatively
you know accurate examples of how we can
do these well and you might think that
if we had ninety percent accuracy at all
of them that would be quite happy but
but this is wholly insufficient you get
a complete mess at the end because
through this processing pipeline errors
compound and and if you take each one of
these ninety percent accuracies in each
one of these steps you really are only
getting sixty percent accuracy at the
end once you've done the multiplication
so you know for this reason for years
now I've been advocating the desire to
do more joint inference through this
pipeline as well as other natural
language processing pipelines and reason
about uncertainty past multiple
hypotheses and uncertainty through this
pipeline so that later stages of the
pipeline can help us correct earlier
stages etc and even if we think even
about just these first two steps
segmentation and classification part of
the magic of a finite state machine is
exactly that it does these two steps
jointly it's simultaneously reasoning
about where to put segment boundaries
and how to label things and that's I
think no one of the major reasons that
they're so popular now our larger
research goal is to pull them all
together and even furthermore to combine
reasoning about the database building
part with reasoning about the data
itself and no and I've I've talked about
that before I fact I think the last
visit I was at Google I talked about
this kind of joint reasoning what I'd
like to do today is first briefly
revisit the kind of models that we've
been using for the last several years
for these two first two steps and then
really the technical heart of this talk
will be to focus on the very last step
the clustering or the entity resolution
step okay so one with Fernando and John
Lafferty in 2001 we were looking at
various finite state models and that in
combination with the desire to train
them discriminative Lee rather than
generative Lee led us to these models
called conditional random fields which
you can
which here in its linear chain form is
represented by a finite state machine
here's the graphical model I'm not going
to give a tutorial on conditional random
fields suffice it to say for now that
you can think of them in this linear
chain form as like a hidden Markov model
that's trained discriminative Lee it's
like with just a progression where the
output space is not a single label but
an entire chain of labels with markup
dependencies among them and they've
proven very successful experimentally
not just in language processing but in
bioinformatics where Fernando has the
world leader in gene finding based on
linear chain CRFs as well as in computer
vision and many other areas okay so so
this is a certain kind of CRF and you
know here's an example of the kind of
experimental difference that it can make
when you look at this problem of
extracting these various fields from the
PDF of research paper even Markov models
we're getting accuracy like this another
discriminative training method based on
support vector machines but one that
doesn't really fully capture the Markov
property very well it's getting a
accuracy like this and a linear chain
CRF is doing us doing much better all
right ok so now I want to get to the
technical heart of the talk and talk
about this this clustering method so
here's the plan we've just talked
briefly about information extraction at
a high level and about conditional
random fields I'd now like to introduce
the entity resolution problem and talk
about 33 different basic ways of setting
up the problem as well as some
particular technical details for how to
get it to perform well and then we'll
also talk about schema mapping and then
we'll see how much time there is to
briefly talk about rexha at the end ok
so we'll start with entity resolution so
this linear chain conditional random
field this is of course not the only
shape of conditional random feel that
you could have that can come in all
kinds of different shapes and we're
going to talk about one that's quite a
bit more complicated than just a linear
chain to address to address Co reference
or
d resolution so first let's just dive
into the details in and make sure that
we're just all on the same page and
sharing the same vocabulary about what
the problem is you know coreference
resolution has many names it's you know
itself in different fields it goes by
record linkage or database record
deduplication or citation matching or
object correspondence or identity
uncertainty it's sort of an in-joke that
the research field that itself is
concerned with resolving many different
names for the same the same object
itself has so many different names but I
think this is really because it's such a
ubiquitous problem that it arises in
many disparate fields separately in
baith of each named it separately
alright so we're going to for now talk
about this problem in the context of
doing somewhat classic natural language
processing coreference resolution among
the entities that appear in a newswire
article but these same methods supply to
you know record linkage in other kinds
of structured databases and i'll show
experimental results from that kind of
data later so the input is a news
article with the named entity mentions
these are meant already tagged so here I
see mentions of people Secretary of
State Colin Powell he kondal has tons of
Lisa rice etc and it's my job to produce
as output how many different people are
being discussed and which mentions fall
into which clusters or which mentions
are referring to which separate person
so it's clustering problem and it's a
kind of clustering problem in which I
don't know how many clusters I should
have the traditional solution for this
method is to build a pairwise affinity
metric you you take two mentions and
you're asking yes no are these referring
to the same person or not and you do
that by considering many features of the
pair of the the contents and the context
in which these mentions occur and you
learn weights associated with those
features you take the features that
occur in this one particular pair you
sum up their weights and if that sum is
above some threshold you say yes they're
coreference and you take connected
components in the graph of these
decisions and declare the partitioning
that
found to be your favorite so let's just
I want to draw this problem pictorially
I'm going to build on this
representation later as I talked about
several different kinds of models here
we have five mentions strings that are
referring to people and the question I'm
trying to address is well how so I'm
going to call these mentions of people
and the trash question I'm trying to
address is how should I cluster these
mentions or is it the case that these
three mentions are referring to one
person and these are another another
entity or should I cluster them this way
or this way I don't know I need to
figure that out okay so here is a
picture of of the traditional model
anyway these pairwise affinity functions
are represented by these black box big
black boxes you can think of this as a
factor graph where well might be will
put the factor graph thing off for a bit
sorry this is this is the function
that's asking our this pair of mentions
are they referring to the same object or
not I'll get a score I'll summit if it's
above some threshold then I'll color
this edge green and I'll take this label
I'll put this label inside this circle
here meaning coreference yes I think
their Co referent here's a sum that was
below threshold I'll color it red and I
put the not Co referent label here
alright so let's think about how the
traditional solution works on this
problem I'll see I'm saying mr. Hill and
Dana hill at the same person I'm saying
Dana hill and Dana are the same person
I'm saying Amy hall and she are the same
person and there's also something about
the local context that makes me think
that she and Dana are also the same
person so I take connected components in
this graph and what have I just done
I've said that they're all Co Reverend
which seems a little unlikely to say
that she and mr. Hill are referring to
the same person so something seems
terribly wrong here what's the problem
well I'm making these pairwise merging
decisions independently from each other
when really I should be making them
jointly in independent relation with
each other so I'm gonna
take this problem and treat it instead
as a as a conditional random field in
which i'm going to make pairwise merging
decisions jointly across the entire
model i'm going to i'm not going to
score and locally normalize individual
pair wise decisions i'm going to jointly
normalize over the entire graph all at
once with an equation that looks
something like this this is a decision
over not just a single edge label but
the labels over all edges in in the
graph and my score function for this is
determined by some weights that look at
pairwise features and then also some
other part of the model that i'm going
to add that wasn't there before that
looks at triples of these labels to make
sure that they obey the transitivity
property I want to express this notion
that I'm not allowed to say in some
triangle of mentions that well these two
are Co referent these two are co
referent but the other edge of the
triangle is not co-references just not
allowed and I'm going to represent that
with some function that examines these
triples and some non learned wait here
that's just negative infinity to say
that I that I don't like they're so as I
consider scouring the entire model I
gather together the sums of the scores
on all of these edges summing over all
of IJ so let's make this concrete I'll
put put some weights on those edges I've
just drawn them I'll sum over all of
those I get here here's the score for
how much I like each one of those
individual edge decisions but I violated
transitivity and so this part of the
score is negative infinity and this one
doesn't even really matter so this is
one set of labels on edges that that I
don't like that it gets a low score
really you know the problem is here or
in other words if I change the label on
this one edge that I no longer violate
transitivity so let's go ahead and do
that even though locally this edge is
saying it would prefer to be connected
to be a yes let me change it to a no
which which simply involves changing the
sign on this weighting function and now
i'm no longer disobeying transitivity
this negative infinity goes away and
just as replaced by a zero and I'm
actually just for the time being not
going to draw these three way factors
anymore I mean I was only ever really
drawing one of them although there would
be many of them in the graph it was just
for the sake of clutter but to
completely get rid of clutter I'm just
not going to draw them for now and this
conference decision is represented by
this partitioning and and I can also now
ask about the score that I get by
summing together all of my edge mates
and I get some other number that's a
little bit lower than before but still
looking pretty good so this is the score
i get for this configuration that obeys
transitivity yes why does the no
decision between Amy hall and Dana not
violate transitivity looking at the yes
triangle between Amy hall and she and
she and dave okay so don't look at the
color but look at the look at the letter
label inside the circle so that's the
one you just changed yes that's right
that's right yes so so it was just it
was violating before and then and now it
isn't yeah I'm don't actually
instantiate these potentials right okay
so that's not do it in the beam search
or something yes so your absolute yes
you're absolutely right um you know
there would be a lot of triples I mean
I'm gonna talk extensively actually
about how to make this really scalable
and we actually run these algorithms on
our data from Rex are where we maybe
start with if the slide will say but its
many millions of mentions and do Co
reference on this and and these factors
are part of the definition of the model
but in in the engineering they're never
actually really instantiated okay so you
know part of doing entrance is to
consider multiple partitioning zand to
get the scores for all of them to see
which ones we prefer so for example I
mean this is just one partitioning I
could change some of the Seas and ends
here we go right now and this is another
coherent partitioning
that is this one and as I change those
season ends i also changed the sign in
front of the corresponding edges and so
i can get a new score for that that
scores negative 12 you know I definitely
don't like this as much so I prefer the
other partitioning to this one which
makes sense according to this course
alright so as these drawings and
examples are designed to make clear this
is actually a special kind of CRF for
markov random field in which inference
corresponds to graph partitioning it's
it's a kind of graph partitioning with
both positive and negative edge weights
and you know spin up significant
interest for a while it actually it
corresponds to this kind of clustering
called correlational clustering that
Avram Blum and others have been talking
about for a while and Erik Demaine at
MIT has become quite interested in and
so there's a whole theory community that
has been interested in algorithms for
partitioning these graphs it's an
np-complete problem I mean unlike the
case where you have only positive edge
weights where you can solve that exactly
when you have negative edge weights as
well as positive it's empty complete but
there's some nice approximate algorithms
that we can use that come out of the
theory community which we can leverage
all right so one of the main points that
I want to make today is that now we're
going to go beyond what I've just been
talking about yes Fernando so the
correlation clustering
yes the stuff based on what's but the
jam Puccini etcetera yes right all of
those are just put pairwise they do not
that's right there is no as far as I
know there is no extension of those
saying the approximation results for
instance for metric partition which is
only for parts for positive ways yes
don't generalize for anything beyond
pairwise I note that so that end in in
the correlation question I don't think
that you eat there all right well can we
make sure I'm following you so yeah the
work above is actually worked as making
the correspondence between markov random
fields and these kinds of partitioning
and yes the above the space are only
positive edge weights that's correct and
right right right yeah so this lower one
is using negative edge weights as well
and what's your questionable edge is
there are no hyper edges that's correct
and so in nothing that we've talked
about so far are there any high bridges
I'm about to begin to talk about I
pragya you have a glow in those oh I see
the connection to Brendan fields on the
on the upper on the upper right that yes
so that connection may be more loose
you're right you're correct about that
yeah yeah i mean the stronger technical
connection is to correlation last as
long as you go to hypergraphs that were
impulsive another I've seen all of that
stuff collapse I see look pilot yeah
yeah yeah yeah yeah sure right my sharks
baby yeah good okay all right so so now
I want to talk about where why pairwise
affinity is really is not enough and not
just in terms of a bank transitivity but
on a deeper sense and to make this point
I'm going to change the example slightly
first let me clean up i'll get rid of
the numbers and i'm going to change
these mentions to all vichy okay so here
I have some partitioning let me see what
we have here yep it corresponds to this
one here and at first it seems to be a
pretty good one right you know
Amy hall and she that's reasonable and
I'm not violating any any gender
transitivity here it seems quite
reasonable that she and she and she
would all be in the same partition but
as we think about it further we think
something is a little strange here here
I have a newspaper article in which
there is a person who is only ever
mentioned by pronoun nowhere in this
newspaper article do I name this person
which seems incredibly unlikely yes
that's right but even still you would
think yes that's right so um so this is
a kind of partitioning that I would like
to give a low score too but there's no
way in the current model to recognize
that something is fishy here because I'm
only I only get to look at pairs i only
see that she and she here together that
seems reasonable these two she's are
together and these two sheaves are
reasonable but no where do i get to see
oh here's a partition in which
everything is a she everything is a
pronoun and yeah so this model is just
not going to let me see that problem and
this is just one example of cases where
pairwise comparisons are not enough so
what would I like I mean I'd like to
have some feature that asked the
question is the case that all mentions
are pronouns in other cases and where
I'm doing a database record
deduplication I mean I may have
mentioned that have multiple attributes
but not all those attributes appear on
every one of the mentions and I want to
measure compatibility among all of those
attributes but they only really all come
together when I consider them all at
once or there are other cases where as
you're looking at names of people there
can be some ambiguity about what is the
first name and what's a last name and
and I could look at every pair of these
people names and decide that well
there's some reasonable chance that
they're referring to the same person
except that if they if they were all
referring to the same person there's no
common agreement about what's the first
name and what's a last name
good and another way of putting this is
that you know having two given names is
pretty common but having forgiven names
is not common at all and there's just no
way to see that that's the implication
of doing this co reference decision if I
only get to look at pairs there are
other cases where I might have
expectations about the sizes of the
partitions and there's no way to see
that when I see only pairs or you know I
may also want to ask now there may be a
lot of edges that are nicely weighted
but I want to ask the question do they
does there exist in this partition some
pair of last name strings that differ by
some threshold all right so I want
something more than pairwise I want to
be able to ask there exists and for all
type questions about the entire set of
mentions that fall in some partitioning
what I want is I want to be able to ask
questions in first order logic and to
give weights to the answers and this is
this is exactly what we're going to do
okay so now we're going to talk about
pairwise affinity and the combination of
a probability and first-order logic
that's here yeah so you know probability
and logic have been two branches of
artificial intelligence that have long
been separated and there have been
recent efforts over the years to try to
bring them together and this is this is
one of those examples you can think
about this as a conditional random field
in which the parameter sharing is
defined by clauses in first order logic
and we assign weights to these clauses
so what I want is a factor graph that
looks like this instead I'm going to
have some affinity function and rather
than looking at a pair it's going to be
connected to all mentions in a partition
and then i can ask arbitrary questions
inside this box I can ask arbitrary
questions about the whole set okay so
this factor graph represents the
functions i would have to measure for
one particular entity resolution
decision or partitioning and of course
doing entrance involves considering many
possible answers and scoring them all
and seeing which one I like best I could
consider that this might be the right
co-references decision and want to
evaluate that function
or this one or this one there are a lot
of them that I might want to ask right
how many do I need to evaluate well how
many different ways are there to to
divide five items into subsets there are
a lot and the issue is that I
furthermore I'm you know I'm going to be
working with news articles in which
there aren't just five mentions but
there are on average about 200 mentions
and so this is gonna get very big very
very fast so this kind of space
complexity is is common in probabilistic
first-order logic models other people
who've been trying to bring together
probability in first order logic so one
of them is you know various people who
have also been learning weights on
logical clauses where you describe the
world by making some statements in first
order logic assigning weights to those
statements and part of doing inference
in this model is to store those
so-called unroll the network or to
ground the network to fully instantiate
all the all the possible outcomes and
the variables that should get
assignments and doing so requires a lot
of space its space exponential in the
era d of the clauses so we're faced with
this question how can we perform
inference and learning in in models that
can't be grounded i right it's not just
a question of not having enough
computational power or time to visit all
the different configurations I mean I
can't even I don't have a prayer of
fitting them in memory so how can I
perform entrance in this case so let's
look at what some other people have been
doing some of the early work here
involved saying well let's only work on
small data sets where we actually can
completely unroll the network and fit it
all in memory and people who use
weighted sat solvers to do inference
here there are some more recent
alternatives in which you can save
memory by only storing clauses that may
become unsatisfied but there are still
some time problems here the
initialization for this algorithm still
is exponential time to visit and
evaluate all of those ground clauses so
another broad family of approaches are
markov chain monte carlo methods in
which i don't try to unroll the entire
network all at once i essentially just
visit a single configuration at a time
so one simple example is Gibbs sampling
I pick one possible output and then I
visit a single variable or maybe a small
neighborhood of variables and sample
some new values for those but the
problem in so many applications of gibbs
sampling is that it can be quite
difficult to move between high
probability configurations by changing
single or small set of variables so you
can get quite thoroughly stuck in in
local minima so we have been advocating
for some time a different inference
method inference an inference method
based on metropolis hastings which is
another mcmc method and what's special
about metropolis hastings well it comes
in two parts there's a model probability
distribution that you have traditionally
and then also a proposal distribution
you can think about this as some as some
jump function and metropolis Hastings
can be extended to partial
configurations in which the model is
only partially instantiated it's been
successfully used in generative models
that combine logic and probability the
blog work by Brian Milchan others and
like the key advantage that I want to
plug here for metropolis Hastings is
that it lets us design arbitrarily smart
jump functions so let me just make the
picture of what's happening clear and
then explain further what I mean about
this advantage of arbitrarily small jump
functions so rather than representing
all possible configurations and scoring
them all I'm only going to represent a
single configuration at a time I can fit
this in memory and I'm going to decide
which configuration I like best by doing
some random sampling I'm going to use my
proposal distribution or some stochastic
jump function to say well I'm starting
here let me make some change to that
suggested configuration to make it look
like this instead
I'll score this jump to decide how much
I like it and if I like it I'll stay
here and further explore from here and
if I don't like it I'll go back to where
I started or about the back one step and
and propose a different jump instead
okay so what does this jump acceptance
probability look like it consists of two
ratios there's the ratio of how much do
I like this new configuration more than
my old configuration as measured by the
full parameters of my model as as we saw
earlier with all the brown number scores
and there's also a ratio of transition
probabilities to the new state from the
old one and vice versa so what are some
nice things about this so first of all
it's just note in passing that since I'm
only considering ratios of scores the
partition function which is so difficult
to calculate just cancels and there's no
longer a problem this also means another
very nice property is you know even when
they only instantiate one configuration
they're often many many factors that we
would need to evaluate in order to fully
score the model but I can even skip
evaluating those I only need to visit
the factors that have changed between
these two configurations when I consider
that ratio okay so the other part of
this is the proposal distribution this
is the probability that i would propose
some jump from one configuration to the
next and what's the purpose of this why
is this in Metropolis Hastings this this
is a ratio that makes up for any biases
in the proposal distribution and maybe I
can motivate it by drawing the following
picture in the air imagine that you're
trying to do a simple kind of
optimization in which you're just
changing a one-dimensional variable and
there's some optimization surface on top
on top of this 1d space that you're that
you're trying to do inference or
optimization in and you've got some sort
of jump function that somewhat biased
and it tends to jump left a little bit
more than it jumps right if you didn't
account for this bias you would spend a
lot
time exploring the left hand region of
the space and not enough exploring the
right and and you would not do a good
job so let's see here so what's nice
about this this means that I can design
some arbitrarily smart proposal
distributions which will inherently be
biased knowing that the mathematics is
going to take care of that bias for me
so then if you think about the many
variables that are involved in doing
coreference resolution or various other
kinds of fancy inference if I if I try
to just change a single variable at a
time i'm going to invariably you know
mess up transitivity there we can get
you know quite creative and very
targeted in in designing very
specialized jump functions and really
it'll leverage that significantly to do
a better job let me maybe I don't have a
slide for this but make another example
of where smart jump functions that are
inherently biased can be useful we've
been looking at dependency parsing that
is based on factors more than just a you
know parent-child weights in the parse
tree that consider more global kinds of
score functions and and that's again a
case where we can't do exact inference
and we're doing inference by this by an
mcmc method like this and you know one
of the properties that we want to
preserve and the dependency tree is that
is projective atty that the the parent
child child lines in the tree don't
overlap with each other and so we we
essentially initialized with a
projective parse tree and then designs
you know highly designed some jump
function that preserves project Shiva T
which is you know there's a little
tricky and takes a little bit of thought
but it's not that bad and but it does
produce jumps that are biased in certain
ways and so you know we can do this
knowing that metropolis Hastings will
take care of these players
confused about whether the goal here is
search and optimization to find a single
configuration which has good score under
the posterior or whether the goal is to
sample correctly from the posterior yes
okay good I I knew you would ask this
and I'm glad you did yeah so metropolis
Hastings is really designed as an as an
inference algorithm and not as an
optimization algorithm and the answer is
that well we're interested in both and
there are cases where we would really
care about the marginal and we want to
do inference there are a lot of cases
also where we just want to find the
highest scoring function but i would
like to argue that the the interest in
being unbiased is appears in both of
those cases or maybe another way of
putting it and be happy to argue about
this is that we are doing a kind of
optimization where you take samples from
the posterior and then do a local greedy
hill climbing from each one of those
posteriors and getting good samples from
that posterior that are unbiased is a
useful precondition to have okay weekend
okay all right all right okay do you
have any evidence that this actually
mixes uh yes and we can take the rest of
the s good yes yeah yeah okay all right
so metropolis Hastings lets us freely
bake in domain knowledge about fruitful
jumps and and it takes care of these
biases it avoids memory and time
consumption with massive deterministic
constraint factors so July as in the
projective parse tree case or as in
preserving transitivity we simply build
jump functions that avoid legal states I
mean I don't have to add factors to my
to my CRF that disallow violations of
productivity or transitivity I I simply
build jump functions that make sure that
I avoid those and also since I'm always
using ratios I'll need to need to
calculate the factors that differ
alright Oh excellent I don't have a
slide for this but that's a fantastic
question and if you Brian Milch has told
me one-on-one that you know designing
good proposal distributions is one of
one of the trickier aspects of using
blog and one of the things that we've
been doing here is let's see here
essentially we're going to learn some of
the parameters of the proposal
distribution as well as of the model
scoring function and we're going to do
that by having the proposal distribution
share some subset of the parameters of
the model scoring distribution so you
know the proposal which we should needs
to be something simple enough that we
can calculate both the forward and
backward jump probabilities so it's it's
not fully parameterised the way that the
model scoring function is but but the
fact that it shares some parameters
means that as I learned a good scoring
function I'm also learning a better
proposal distribution all right ok so
we've talked a little bit about
inference and now I want to talk about
how to do learning how to learn these
weights on these clauses so let's just
set up the problem right the input for
this learning problem is some set of
first order formula and some labeled
data and I want my output to be weights
on each one of those for me so I want to
learn likelihood ratios that will lead
to good correctness decisions as defined
by my label data and so you know the the
traditional way to do this is
essentially to borrow the techniques
that have been used in all kinds of
other CRFs linear chain and others which
is to calculate a gradient of the log
likelihood of the the true labels in
some labeled data which in turn requires
expectations on marginals and we can get
marginal distributions by by sampling
according to some mcmc method but it's
really inefficient do too
this incredibly large sample space and
so what we're going to propose instead
is use it using ranking learning by
ranking rather than classification
simply to say well I prefer one
configuration over another so
pictorially or as an example this looks
like this instead of training the system
to look at some partition of mentions
that i'm currently hypothesizing as
being all co referent and to pretend to
issue the correct labeled say yes this
is a good partition or know this is a
bad partition and instead simply going
to learn well this partitioning is
better than this one and other reasons
that i like this is that it's often the
case that we're looking at partitioning
'he's in which neither one of them is
correct but one is a bit worse than the
other so here George shouldn't be in
either one of these but having he and
mr. together looks a lot better than
having she and mr. together so so I like
this one better so how are we going to
do this we're going to use a large
Martian update based on Myra and the
algorithm looks like this we have some
observed data we know what the true
labeling is we have some prediction
algorithm this is essentially our
inference algorithm bytes based on
metropolis Hastings and some initial set
of weights I essentially run my
inference algorithm to get some new some
you know some new state that I'm saying
I'm going to accept and if according to
my current parameters and if this
configuration is actually according to
the the true labels worse than where I
came from even though my score function
thinks it's it should be better than
will clearly hear my score function my
weights are telling me something wrong
and I should do an update and so I'm
going to update my weights based on you
know the improper acceptance of this
jump and so and this update is
essentially considers where we're coming
from considers the jump the bad jump
that was proposed looks at the truth to
generate some jump that indeed would
have been better in
dead and updates my weights to try to
produce the correct ranking among those
two alternatives yes the highest
question you know in proposal
distribution could buy a introduce a
problem here because you might not be
sampling the things you want to choose
compare in a special advised way all
right so so because you're going always
do say well I have two alternative and B
yep and that I'm going to compare yeah
but ones that you've been that you are
given to as training instances
effectively are the ones that are
generated by a proposal distribution so
how sensitive is the learning to follow
I said maybe not quite seeing the
problem because the proposed
distribution line is sampling over
possible configurations yes and those
configurations that was that you then
going to use to adjust your scoring or
yo p yep essentially all the ratio yeah
and when one War II would be that the
proposal distribution might be kind of
my affecting some hey of it and just
focus on the very small saucer set of
the possible instances that you might
see well so I mean it's certainly true
that that you know our proposal
distribution may not be ideal and and
maybe that's improved because it's
sharing some parameters with a scoring
function or maybe it has some inherent
limitations but that's a separate
problem I mean what what I see here that
I like is that the inference algorithm
that actually that I'm actually going to
use in practice at test time is the one
that I'm using during training so the
right the kinds of rank comparisons on
which I'm going to be training are the
kinds that I'm going to be faced with
when I'm actually running my inference
algorithm at test time and I'm learning
to make those
right I like that but I am wondering
whether there's something that if you
are a little bit if you're not really
careful about what q is could you sort
of get I think I mean if Q is due course
if Q is fundamentally myopic as you are
putting it or broken then yeah this is
not a learning procedure that's going to
escape from that side thing evaluating
you say your choice of you yeah I mean
other than by the sort of the final
accuracy her or user officially above
well um let's see here well okay so I
mean another thing that I like is that
here right we consider the Q prime that
was proposed that was given by my
proposal distribution the other one that
we're going to compare it with is
actually a cue that I generate by
looking at the truth right so in as much
as the parameter of the setup of my
proposal distribution and its
parameterization would allow me to
consider better jumps that I could
arrive at by considering the truth then
I think this is going to steer me in
that direction so I am alright I'm not
just comparing where I came from and
where I jumped I'm comparing my
incorrect proposed jump with something
that's better as defined by the truth so
I that makes me feel even better
actually about this in relation to your
questions okay all right we're just
about done with this so this is a
summary of where we are we are inventing
these advantages metropolis hastings and
here we're doing learning by ranking
right so don't train unlikelihood train
to properly rank configurations which
has a number of nice properties and here
are some real life the results that
we're extremely excited about on one of
these standard
data sets of newswire articles from the
DARPA ace program from 2004 this is one
of these data sets that the NLP
community has been beating its head
against for quite some time as you can
see actually in the world the best in
the world performance over time in 97 we
were getting about sixty-five percent f1
peers later we were getting 267 the few
years later 68 you know these may seem
like small changes about you know the
natural language processing community is
incredibly empirically driven and these
are the major results that people are
excited about and in one year we jump
from sixty-eight percent to seventy-nine
percent and you are pumped and where
does that benefit come from well here
you can actually see that based on our
own features we were doing even worse
than the state of the art so maybe we
could do better with more feature
engineering which is often the case we
get some jump from using instead of a
pairwise function a partition wise
function and we get also some jump by
using ranked based training instead of
likelihood based training and these
kinds of results carry over yes a really
interesting that even the bottom row yes
that just a rank brace train without the
new representation is gives you a better
improvement than anything else that
people did since 1970 yes yeah yeah I
think we're still trying to understand
that actually which is it which is
really puzzling because it just purely
an algorithm yes in search issue yeah
thing to do with yeah well it is I mean
I think I mean Claire cardi and others
have done experiments in the old style
reference current resolution in which
you just sort of you know subsample the
negative edges and your training and
things I which can make a big difference
and so I think it's it's some flavor of
that that's that's going on here and
that and that these people were not not
you know maybe training on to bias to
sample or you know too many no edges or
things like that is closer to what you
evaluate yourself and test time with
right there perhaps there's that too I
mean who cares about likelihood yeah
yeah yeah so we also obtained
improvements in data that this doesn't
come from NLP that comes from databases
so here is a database of mentions of
research papers there are citations that
appear at the end of of research papers
and they've already been pre extracted
into 14 different fields of titles and
authors and venues and years and volume
numbers etc and me and we're getting a
thirty percent reduction in there and we
also do coreference over research paper
offers which is an incredibly hard
problem and we find some improvement
they are also know so we're working on
it and we have it's better than it used
to be a Fernando Pereira and Portugal
that works on you know video compression
is still a little bit confused with you
but please email coming my way yes that
would be great yes right so the examples
didn't include those but the ACE data
set includes common nouns like the
president and things like that and so
those are included here and the features
are fairly standard at this point there
some string overlaps and various flavors
of string at a distance and number and
gender agreement yeah let's see I mean
some people I know you feat use features
from wordnet and other places and I
believe that we did also i don't think
that innovation is in the kind of
features that are you yeah okay yeah
yeah and I and I have a slide that I can
show you offline in which I actually
layout features should press forward yes
good that's right and in fact the rest
of these are really a lot shorter than
than they used to be so another
configuration of the model that you can
make for doing entity resolution is a
different one it's one in which the
factor
don't compare mentions against each
other we infer some canonical
representation of the entity in sort of
a database record form and measure
compatibility between the mentions and
the canonical form and this is a type of
entity resolution that we've been very
excited about recently not only because
I think it has some interesting
properties in this function but also
because a byproduct is that it gives you
canonical values for what you would put
in the database and in various tests it
actually works better even better than
the partition wise affinity all right so
the thing that we've been doing most
recently is trying to step back to an
even larger part of the information
extraction problem that includes schema
mapping as well in which as data you're
given multiple databases each of which
come with a different schema or layout
of what the different the fields are and
in these fields overlap with each other
but in all sorts of strange ways and you
have to resolve them against each other
and in particular we have been doing
joint inference through these three
highly intertwined problems of entity
resolution or coreference deciding on
those deciding on canonicalization here
all these mentions including some
misspellings how do what what are really
the canonical values in which i would
put in a single record and schema
mapping here i have four different
databases that are referring to
overlapping information but in you know
not well aligned ways here name
presumably first middle and last name
we're all going to appear as a single
string in this one field but here
they're split across and into three
separate fields address is collapsed
here but not here you know gender
doesn't appear here email only appears
in one place etc and I want to resolve
all these together and and you can
imagine how decisions in each one of
each of these three different problems
are highly intertwined with each other
and so doing this problem jointly
preserving the uncertainty from one task
to the next would do well I'm not going
to describe this in great detail but
it's another factor graph may very much
in the flavor of the previous one in
which I
have the factory graph as i did before
down here for entity resolution where
I'll do partition wise wide scoring and
here i have another its others oh it's a
little bit like entity resolution it's
just the entities are fields in
individual databases and i'm trying to
decide which fields are referring to the
same underlying information and then i
have factors that describe how these two
problems interact with each other and i
solve it all jointly using metropolis
hastings and we've been trying this on
data of contact information records of
contact information for people as they
appear as they as they are on the web so
we gather these from department faculty
listings alumni listings and extracting
address blocks etc off of off of home
pages there are about 14 fields in some
of these databases three fields and
others and there's just a surprising
variability and we get some very nice
results that show that when you do
indeed do this problem jointly you get
some real benefit alright that was
coreference resolution accuracy we also
get better schema alignment this this is
the result of schema alignment when you
do a cascaded approach where you do
coreference first and then do schema
alignment later or actually set the
other way around them I'm momentarily
forgetting the green lines are the ones
that are correct the red lines are are
incorrect and here we do much better
when we do this task jointly
no so actually let's see here there so
there are some other ones we could talk
more about the details of this offline I
don't have a lot of detail on the slide
but to answer your question briefly the
model is not restricted to just 121
alignments yes one-to-one correspondence
is right right right now when you have
schema elements you can have one female
and it corresponds to a set of elements
for the same yes that's right so there
are aspects that don't map over but our
current model is allowing that lack of
one-to-one correspondence you know okay
um all right and so now maybe just very
briefly I'll talk mostly about the
scalability aspect here so no we have
been applying these ideas in practice to
data that we at least consider a large
scale although I'm sure you wouldn't and
this has been in the context of building
enhanced alternatives to some of these
older systems that are research paper
search engines like Korra and sites here
and Google Scholar where our main aim is
to not only recognized research papers
as entities with citation relations
among them but much richer richer
universe of object types including
people and universities and publication
venues and grants and groups and
expertise etc and building the system
involves a pipeline that spiders PDFs
from the web converts PDF to text uses
conditional random feels to do
information extraction of many different
fields does reference resolution of
papers and people and authors and and
now grandstand venues and institutions
and then we do various kinds of data
mining on this data and currently we're
working with about 60 million mentions
of papers and about a hundred million
mentions of people
and our processing then yields 13
million research paper entities and
about 1.5 people entities and you know
we've implemented something that's
parallel not at your scale but the scale
that works for us and and it's been
quite tractable and what are some of the
ingredients of making this scalable you
know we avoid calculating 100 million
squared edge distances using a blocking
method that we call canapes that is of a
soft method of pre partitioning the data
into mentions that have some similarity
to each other and the method leverages
inverted indices to do so efficiently we
use you know some nice graph
partitioning methods that are fairly
efficient and you know a very nice and
important aspect of how the method works
is that previous resolution decisions
can be revisited and changed given later
decisions and evidence so it's not it's
not a greedy inference method and so
that you know the resulting system is
called rexha available through a web
page that looks like this you can enter
a query in that box and get back a list
of hits looking like this where you see
automatically extracted titles and
authors venues years abstracts etc you
can click on a title of one of those and
arrive at Adam attic Allegiant erated
home page for this paper which shows you
the full abstract the outgoing
references at the end of the paper
incoming citations from other paper that
site this one bibtex entry as well as
topics labeled topic labels that are
automatically assigned to discovered and
automatically assigned to these research
papers and you know unlike Google
Scholar and others people are first
class objects so I can click on Bruce
Crofts name here and arrive at an
automatically generated home page for
this person including a contact
information extracted from his web page
all of his publication is currently
sorted by date I could sort them instead
by publication citation count to find
his most highly cited papers here I see
his co-authors her
he's sorted by the date of their most
recent co-authorship event but I could
also sort them by number to find out
well who does he co-author with the most
i can also find nut Co authors but cited
authors this is who does Bruce Croft
site when he writes research papers and
it's in credits true of just about
everybody that you set yourself the most
it's not at all uncommon we can also
look at who cites Bruce Croft and find
fine today again again it's actually
it's true that that you know people are
cited by themselves most often but it's
interesting here we can see that um
let's see here where is this example
that I wanted to find so one of the
people that Bruce sites is hector
garcia-molina a professor who does work
in databases and one of the people that
site spruce croft is doug or the
professor who does work in NLP so we get
some sense of where bruce sits in the
research community sort of taking ideas
from the database community and
delivering them to the NLP community
which is which is uh you know a
reasonable summary of a certain aspect
of his work other papers have extracted
grant pointers so we we do information
extraction from the acknowledgments
paragraph at the end of the paper find
references to grant and we've imported
all of NSF's grant database so here we
see this paper is acknowledging this
grant we can click on that grant and
arrive at the home page for a grant it
gives its abstract its various award
number the list of pis and post of other
papers that are that also acknowledge
this grant which you know so NSF is very
interested in tracking the impact of
their various funding activities and
this gives them a way to do that we can
also search for people here I search for
machine learning and reinforcement
learning and got a list of people who
would be experts in this field and I can
also look at topics these are
automatically generated home pages for
topics where we run these uh no patient
topic models on the entirety of the text
in our collection automatically discover
that there is such a thing as this topic
information retrieval as described by
these lists of keywords and individual
words and phrases here's where the
topics it's in relation to others you
know it other topics that are citing it
and are cited by it as well as the top
papers that fall in this topics but
topic by various criteria as well if
it's profile over time so I mean it's a
little a little bit hard maybe to see
the trend but it's possible that this
research least the citation patterns or
the number of papers peaked during the
Internet boom time and there might be a
little bit of a dip okay yeah but they
have you know neural networks is really
nobody cares about their I'll never say
ok so we've been looking at trends of
ideas flow idea flows through networks
and with that I'll wrap it up and take
questions if there's any more time thank
you when you first do it on the articles
and then somehow I think there'd be some
benefit to doing it jointly where you
didn't commit to within document entity
resolution and lock that in before you
went off to do cross document but you
know my guess is that that that would
yield some efficiency which would not be
a bad first approximation go and then
take dresser questions offline their
various commitments but let's thank
Andrew again okay all right
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>