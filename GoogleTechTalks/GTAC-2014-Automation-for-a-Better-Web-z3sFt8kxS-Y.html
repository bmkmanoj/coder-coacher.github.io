<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2014: Automation for a Better Web | Coder Coacher - Coaching Coders</title><meta content="GTAC 2014: Automation for a Better Web - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2014: Automation for a Better Web</b></h2><h5 class="post__date">2014-11-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/z3sFt8kxS-Y" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">All right. So in the interest of time, I'm
going to keep going.
So our next speaker is James Graham. Welcome,
James.
James is joining us from London today. He
works at Mozilla. And he's been involved in
defining a lot of Web standards over the last
decade.
So I'll hand it over to James.
&amp;gt;&amp;gt;James Graham: Okay. I have a microphone.
I'm good.
Okay.
Great. So, hi, I'm James, and I'm going to
be talking about how we've been using test
automation at Mozilla to improve the Web for
everyone.
So it's kind of hard to introduce the Web;
right? Because it's become so completely ubiquitous
that there's sort of nothing new to say but
I think it's worth reemphasizing the point
that really it's the most important application
delivery platform in the world. It's the only
thing that you can get on, you know, any consumer
electronic device that you buy, from, you
know, your tiny smartphone or whatever, right
up to your TV. They'll all be able to access
Web content.
And in the 25 years that the Web's been around,
we've come a long way. So this is pretty much
the first Web page, and, you know, it's some
text and a few links and it has, like, the
most important features of a Web page. But
now we're doing cool stuff like this. So this
is, for those of you who don't know, this
is a screenshot of Firefox OS which is a whole
operating system where the user land is purely
written using Web standards. So everything
you see there is just HTML, CSS, JavaScript,
but you can also substitute in your own favorite,
complicated Web app here. You know, you might
want Google Maps or whatever you think is
a neat thing that's delivered via the Web.
And this is pretty cool, but there's a problem.
And if you speak to any sort of Web author,
they're going to tell you about how awesome
it is when they get their site running for
the first time in their favorite browser or
maybe they've been coding this feature for
a little while and then they load it up in
their second favorite browser and there's
this crashing sense of disappointment; right?
Because suddenly the whole thing that was
working so well is completely broken and they
have to spend time figuring out, well, what
went wrong.
And you seen see this on production Web sites.
So what I have here, and I guess you probably
can't read it, is a screenshot of the error
console from two different browsers, from
Firefox and Chrome, actually, from one production
Web site and what you can actually see is
they're both -- in both browsers, it's showing
errors. So you can maybe just make out that
there is some red text. What you maybe can't
see is the errors are completely different
in the two browsers.
So, you know, we don't know if this is because
it's sending different code to the different
browsers to work around some problem that
they thought there was in one of the browsers
or if the browsers themselves are behaving
differently given the same input. But it's
difficult.
And this is a ubiquitous problem. Once you've
worked on browsers for a while, you get pretty
tired of, like, find being sites that work
in one browser. You know, they work in IE6
or whatever, but they don't work in your supposedly
better browser.
And it's really difficult to diagnose. So
you have to spend ages and ages digging through
the source code of this site, you know, hundreds
of kilobytes, megabytes sometimes of JavaScript
to work out what's gone wrong.
And if you want to make a better browser,
then that sucks. That's not a scalable solution.
So this sort of lack of interoperability,
it sucks for authors who have to spend twice
as long as they would like trying to make
cool things, and it sucks for browser vendors
who are trying to make the Web better.
And it sucks -- you know, it sucks a bit if
you have a big browser with lots of market
share, but if you're actually trying to do
something new, then it sucks even more.
So this is a screenshot of Servo, which is
a Mozilla research project to try and make
a next-generation browser engine that uses
parallelization to speed up rendering.
And the idea is that sort of current browser
engines in the future won't be able to take
advantage of the kind of hardware we expect.
But what you can see here is the rendering
of a rather simple page like the Google home
page, it's not doing a great job, to be honest.
And if to develop this browser, they have
to -- every time it looks like this, they
have to go, okay, I'm going to dig into the
source code of that page to figure out what's
wrong, then it's going to take them an awfully
long time to get anywhere.
So this lack of interoperability, it's really
terrible for the current Web, and it's terrible
for the future of the Web. Something has to
be done.
And maybe it's no surprise in this venue that
I'm going to suggest that the answer is testing.
So in order to improve testing on the Web,
there's this project called Web Platform Tests,
which is being led by the W3C under the sort
of banner of Test the Web Forward. So some
of you may have heard of Test the Web Forward.
It was a sort of series of events that was
being run to get people into testing for the
Web. But now it's actually the general name
used for all Web platform testing that's being
done at the W3C.
And this site, TesttheWebForward.org, has
all the documentation on how to write tests
and is generally a useful resource if you
want to later find out some detailed information
about the things that I'm going to introduce
now.
And so the idea of Web Platform Tests is that
it's a collection of test cases written for
-- to work in all browsers to try to address
this interoperability problem by giving them
tests to work against rather than having to,
like, reduce pages every time that they find
a bug.
And the idea is that browser vendors will
contribute their tests, so authors, if they
run into some bug when they're trying to,
you know, get something working in a new browser,
can contribute a test and say, &quot;Look, browser
vendor, here's a test that I've put in a test
suite that you're already running. Please
fix.&quot;
And if you've ever contributed to an open
source project, contributing to Web Platform
Tests should be fairly familiar. We have a
GitHub repository, we take pool requests,
we do code review, you know, blah, blah, blah.
It's designed to be completely straightforward.
And the key point here is that it's testing
for interoperability, that it's really testing
to make sure that browsers all do the same
thing.
And that might sound obvious, given the introduction
I've given. But it's actually a sort of new
idea for browser vendors.
So historically, the W3C has done something
called testing for implementability. And the
idea behind that was that as part of their
process, you have to have two -- you have
a test suite, and two implementations have
to pass every test in order to be allowed
to advance that specification along what they
call the recommendation track. So for it to
become a proper specification.
And the problem there is, all the incentives
are terrible, because the people writing the
tests are probably in the working group. They
want their spec to reach recommendation. They
want to write the simplest test they possibly
can that nobody will fail.
And browser vendors typically have done something
slightly different. They've typically written
regression tests for their own browser. So,
you know, it's -- every test must pass. And
if our behavior doesn't match the spec, well,
then we write a test for our behavior rather
than a test for the spec.
In this case, we're trying to do better, write
tests for the spec, but we want every evil
test case you can think of. Because we want
to fix problems. We don't want to just, you
know, advance specs along some bureaucratic
path.
And how are we doing? Well, based on the number
of top-level directories in the repository,
we have tests for about 60 specifications.
There's about three and a half thousand test
files, which produced a little over 200,000
test results. And we have no idea what this
means in terms of coverage. And that's a pretty
interesting question, the -- you know, maybe
it's worth talking about afterwards.
So here I just have a simple example of the
kind of test we have. So most of these tests,
the vast majority of them, are written in
JavaScript. Although we also have some that
aren't.
JavaScript tests are written using this framework
called testharness.js which is pretty much
unique to Web platform tests. But one of the
early decisions we made is, we're not going
to take tests from every different browser
vendor for every different contributor in
whichever format they like. We're going to
standardize on one format, and instead of
saying we'll take Qunit tests and mocky tests
and, you know, whatever your favorite is,
we'll just take one test. So it's easy for
everybody to read.
We have a simple page here. One interesting
thing is we have this test harness report
dot JS. That's upstream basically a blank
file. The point of that is that it's extensibility
point. If you're a browser vendor and you
want to run these tests, any code that you
put in testharnessreport.js will never conflict
with upstream. You can always extend it there
however you need to report to your, like,
continuous integration setup.
If you've ever used a sort of xUnit-style
framework, it should look pretty familiar
to you. We have just a script here, with a
function called test, which it self-takes
a function. And inside that function, we have
this assert equals function, which just tests
who things for quality. And if that assert
doesn't throw, then the test passes.
We also have this concept of asynchronous
tests. A lot of the platform is asynchronous
and depends on events or promises or whatever.
The example I have here is just a test that
the load event fires, you know, when the document
is done loading, and so we call this async
test function with a test name, and it will
give us back a test object. And on that, we
have this sort of step function thing. And
in this case, what it's saying is, when unload
fires, give me a function back that will be
called, and it will basically run this T dot
dot, which basically marks the test is done.
That's maybe not very obvious in my 30-second
overview. But like I said, there's a lot of
documentation if you want to get into this.
So that's the client side of testing. But,
of course, for testing Web sites, you need
a server side. And this is a bit more difficult.
So looking at the prior lot here, every browser
vendor did something completely different.
So the W3C, for example, had a server running
using Apache and PHP, which they were also
using at opera. Mozilla had something called
httpd.JS, which is actually a Web server written
in JavaScript using internal Firefox APIs,
and the less you know about that, the better,
really.
[ Laughter ]
And they also used Python for some things.
I think that Blink uses lighthttpd and PHP.
People use Node. I'm rather sure Microsoft
uses something .NET-based, so on and so forth.
So we needed to pick something to do here.
And when you have this kind of decision to
make, it's always a good idea to start by
looking at, you know, what are my requirements
here?
So for writing a test server, one of the first
things you need to be able to do is you need
really precise control over what actually
gets sent in your test cases. So a normal
HTTP server will really want you to send something
that looks like HTTP. If I want -- if I'm
writing a test and I really want to claim
that the content length header is, say, 100
but I only actually send 50 bytes of content
on the wire and I want to check that the browser
does whatever it's supposed to do in that
case, then I should be able to do that.
So that immediately eliminates, it turns out,
most existing servers.
Similarly, you need control over timing. So
a typical thing might be if I'm testing, say,
script loading, then I might have two scripts
that the browser should start loading at exactly
the same time, but I want to make sure that
they're returned in a certain order, maybe
to check that they execute in that order or
something. And to do that, I might want to
delay the response from the server by one
second for a particular script.
So that's a requirement.
And this is a big open source project. We
want it to be an open source project, and
we want it to be easy to run these tests locally,
just on your local machine, no external dependencies.
We want it to be easy to run them in automation,
so no external servers.
So we don't really want it to be necessary
to, like, install Apache or something that's,
like, stupidly hard like that.
And doing simple things should be simple;
right? That's a good principle to have in
life. And in this case, what I mean by that
is, if you just want to serve an HTML file,
you should be just able to stick it in a directory
and have it be served. You shouldn't have
to, like, configure roots inside the server
or, you know, something complicated just to
write a simple test.
So the solution to this was to roll our own.
And what we came up with was wptserve. You'll
notice that I'm really poor at naming things.
We've had Web Platform Tests, which was a
test suite for the Web platform. We now have
wptserve, which is a server for the Web Platform
Tests.
And so the central idea here is that it's
a Python-based server that's specifically
designed for testing. And one of the things
we noticed is that PHP is actually quite good
for lots of testing use cases, because you
can just write a .PHP file, shove it in a
directory, and it will work. And we wanted
to replicate something like that, but for
Python.
So we have a very similar idea that you can
write a test.py file, put it in some directory,
it will be interpreted as Python code. A bit
unlike PHP, you have to have this main function,
which takes a request object and a response
object. So in this example here, we use the
request object to inspect the query parameters
of the request. So this request.get thing,
which, again, sort of familiar if you've used
PHP before. So in this case, if there's a
content-type parameter that's been sent, then
we build up a list of headers from the get
parameters. And similarly, at the bottom here,
the body is set based on either the get parameters
or some default value.
In this case, we don't actually use the response
object. We just return a list of headers and
the body we want to send, and that's all you
need to do.
In a more complicated case, we could actually
use the response object to get control right
down to the actual socket level. So if we
want, that will allow us to send any arbitrary
byte over the wire with any arbitrary timing.
So that basically meets our use cases.
There's a few other call features that we
decided we needed for writing portable tests.
So the one at the top here, we have this var
server ssl equals (indiscernible) thing, is
a substitution syntax which allows us to run
the server on different host names without
having to kind of rewrite them. So, typically,
when you run this locally, we suggest that
people set up webplatform.test in their host
file. And in that case, this calibrate host
thing will be substituted with webplatform.test.
But the W3C also runs a public Web server,
W3Ctest.org, that hosts these same tests.
And in that case, the host will be W3Ctest.org,
and the ports will be set up. So in this case,
we have ports HTPS0, which is the first SSL
port available.
And similarly, the second explicitly here,
the var server subdomain thing is setting
up subdomains. We have a list of subdomains
that have to be available, and this gets you
a domain name.
We also realized that having to write Python
code, even with the simple API that we tried
to design, is sometimes hard, and there's
common things where you might want to avoid
that. So we have this -- this pipes query
parameter, which is actually handled directly
by the server. And the idea is that that's
basically a set of precanned functions that
you can use to control various aspects of
the response.
So in this case, it's this trickle function.
And what it does here is it delays the response
for two seconds. So that's the D2. Sends 100
bytes of the response, delays it for one more
second, and then it sends the whole rest of
the response.
And there's a few of these functions for making
simple things simple.
And this is just an example of what it looks
like when it's working. This is actually running
from the public Web server. It's testing cross-origin
requests here, and you can sort of see, actually,
the person who wrote these tests put the server
name in the test title. Although they did
it using substitution syntax. You can actually
see in this case that it's running on W3Ctest.org.
But if I ran this on my local machine, that
would all be running on Webplatform.test.
There would be no access to this external
server, even though I'm testing cross-origin.
Okay. So that's my sort of very brief introduction
to writing tests for webplatform.test.
But it's also important to be able to run
tests. And so the very simplest thing you
can do is, well, I can load a test in the
browser. I can script the browser. So let's
script the browser to load tests. And, you
know, that's the first runner that we have,
is just a simple in-browser runner that will
load a test using JavaScript, get the results
from the test framework and load the next
page and so forth. And that's okay as far
as it goes. And, in fact, the W3C has used
it to do some implementation reports for standards
they're working on. I think there's an HTML5
implementation report that's been generated
using this tool. It's not something you'd
want to use in your continuous integration
system.
So I've talked about testing. Given the subject
of the conference, it seems only fair to go
on to automation.
How can we go from something simple like this
to something that's actually production-quality?
Well, when trying to do this, there's some
unique problems, or at least compared to the
other test suites we have at Mozilla, that
come from trying to use a test suite from
upstream. So these tests are being constantly
updated. You know, we might get three or four
every day new submissions that get merged.
And we want to track those updates. We don't
want to end up six months behind because we
don't have, like, some automated way of pulling
in new updates. And that's what tends to happen
if you import test suites, I've found, that,
you know, you import it once, and then people
forget about it, and then six months later,
you're, like, oh, okay, we're not testing
the right thing anymore.
We can't actually edit the tests or -- because
if we go around editing the test, then every
time we want to import stuff, we suddenly
have merge conflicts all over the place. And
that requires a human. We don't really want
to require a human to be involved every time
we update the tests. So we can't do the typical
thing of marking a test as, like, scheduled
to do in the test file.
And because we're importing these tests, we
can't guarantee anything about the tests themselves.
So the typical policy at Mozilla is, all tests
must pass. You know, they're regress tests,
they're expected to pass.
But these tests, they're not regress tests.
They can fail. Well, okay. Fair enough. That's
sort of not that hard to deal with. But they
can crash the browser. They can hang the browser.
This is a bit harder to deal with. We can't
just have something sitting in browser if
we can expect the browser to crash every so
often.
So we need to do better. And the solution
to this was wptrunner, which is a Python-based
harness for running these tests.
And the key design decision here was that
we want to run these tests by remote control.
Rather than having anything sitting inside
the browser, we want to control the browser
externally from Python, so that if a test
does crash the browser, we can detect that
that's happened and we can keep going.
And similarly, if it's hung, we can time out
the test, kill the browser, and keep going.
To deal with this problem that we don't know
what the results are supposed to be, we actually
store them all externally. So this turns out
to be a surprisingly complicated problem.
This was actually one of the hardest problems
of all to solve, because your sort of natural
reaction might be that sounds like it should
go in a database; right? We have a list of,
you know, test results. Well, the problem
is, we want this to be stored with the code
really, because every commit of Gecko might
have a different set of expected results as
people fix bugs, create bugs -- no, hopefully
fix bugs. And we want it to not live in the
version of the (indiscernible) and we want
it to be human-editable.
Obviously, supporting Gecko is necessary.
That's sort of the point of me working on
this, to get them running in Mozilla. But
I said we were testing for interoperability,
so wouldn't it be better if we could have
it running in all browsers? So Servo I already
mentioned. FirefoxOS. And then why not Chrome?
Why not Internet Explorer? Why not Safari?
You know, why not any other browser you mention?
And, in fact, all these browsers that are
listed here, it does already work in.
So for remote control protocol, it turns out
that WebDriver, which I guess a lot of people
have already used is a rather obvious choice,
because it's already supported by a lot of
these browsers.
So this is a kind of schema of the design
that we ended up with.
So I can't really point here, which is a bit
unfortunate.
But the input we have is basically a test
manifest, which comes from Web platform test,
and an expectation file which we store in
the gecko repository to see whether a test
should pass or fail. The output we have is
a set of log files, and then we have certain
internal components, which I'll talk about
a bit more in a minute.
So the test manifest is very simple. It looks
like this. It's just a big JSON file that
lists all the test and a bit of metadata like
does this test require a long timeout. The
only interesting thing here is this is all
generated automatically from the test files
themselves, so we don't ever require anyone
to write any metadata about the test because
if you need people to write metadata, they
tend to get it wrong or it gets outdated or
whatever. So this is all passed out of the
test files.
The expectation file is a bit more interesting.
So to solve this problem of what's a line
oriented format that meets all the requirements,
I ended up sort of inventing this sort of
novel format, which may have been a stupid
idea. Maybe somebody has a better suggestion
for me. I don't know.
But what we have here is basically an any
like file, but it's a bit more complicated
than that.
So each test that has at least one nonparsing
result gets its own file, and then the sort
of top level, we list all the sort of top-level
tests so here we have UTF16 be HTML. In theory
if we had a test with the same file name but
different query string, for example, then
we could have two of these in the same file.
We have the type of the test, and then skipping
down a little bit, we have this getting div
item ID. And that's a sub test from this main
parent test. And you can see the expected
result from that subtest is to fail.
What's more interesting is the second subtest
at the bottom of the screen which has got
this expected result, well, by default it's
expected to fail but on the line before, we
have something a bit more interesting. If
we're running in a debug build and the OS
is Linux, then it instead times out.
So we have these in-line conditional statements
that we can use to set these on a per-platform
basis.
So there are different approaches that we
could have taken to this, but this was fairly
flexible and sort of matched something that
Mozilla has had success with in other areas.
Sort of conditional statements, rather than
having, like, a different expectation file
for every combination of platform and, you
know, whatever we're running in our continuous
integration system.
Similarly, we can use these files to disable
the test. So right at the top level you can
see we have this condition where if we're
running it on Mac, it's disabled and the right-hand
side, it's just a bug number here. It's not
actually used by the runner for any purpose.
And the point of this is we can check into
version control and we can rather easily edit
it.
Okay. Then to get to run in multiple browsers,
we have this simple browser abstraction. And
this just controls starting and stopping the
browser and checking if it's alive or in the.
But it turns out that this is actually all
you need to get a vast amount of the functionality
needed to run across multiple browsers.
Just implement this API for every browser.
There is a little more you need because it
turns out I said just use WebDriver. Well,
it's not quite as simple as just use WebDriver
because every implementation of WebDriver
is a bit different. Mozilla has Marionette
which is not quite compatible with WebDriver
but is a bit better for us.
So we also have this concept of an executor
class, and there's one executor per remote
protocol, basically, and per test type.
And it's the thing -- It's basically where
the magic happens. It actually tells the browser
now run this test and convert the results
back to the internal format.
But this architecture with these two points
of abstraction has been very successful. So
for example, when we added Firefox OS support,
where the test runner and the browser are
running on completely different machines,
one is a mobile device and one is a host computer,
we found it only takes -- we already had some
libraries we could reuse, but it only takes
about 250 lines of code to support Firefox
OS excluding those libraries.
This is the output. It's not very interesting.
It's just JSON delimited -- line delimited
JSON telling us what happened in every single
test. Very useful to have a machine parsable
format. Mozilla used to use a lot of reg X
readable formats. That was terrible. Don't
do that.
Okay. Now I hope we have some videos to show
quickly.
Okay. So this is very hard to see, but this
is hopefully one running it in Firefox. Just
as a developer word, there's a little pause,
and then some tests start running. And you
can maybe just about make out on the right-hand
side there that it's telling you the result
of every test. It looks like this test here
probably times out or something. And then
at the end, we get a summary. And in that
case because we have expectation data, it
says all the tests got the expected result
even though one of them timed out.
We can also run it in Chrome. So again, I
just need to wait for the video to happen.
And one of the interesting things that you'll
see here is that it's actually designed so
that we can have more than one process running
the test at the same time. So here we're controlling
two instances of Chrome simultaneously and
getting them to run different tests.
And that's pretty useful if there's something
like a time out, because here we have two
tests timing out for some reason, they can
time out in parallel much faster.
[ Laughter ]
And that's pretty cool that we can do that.
It turns out that using multiprocessing in
Python or whatever adds a surprising amount
of complexity to what seemed like a very simple
idea.
Okay. So I've talked about whether it works
or not. It works, but does it work well enough
to use in a continuous integration system?
The good news is finally it does. It's now
running on every commit to Mozilla inbound,
so I don't know how many commits that is a
day but I think it's certainly hundreds of
commits a week or something.
You can maybe see this. So on the very left
of this picture you can see there's a W and
then some green one, two, three, four. That
means all the tests are getting the expected
results in all these check-ins. And this is
just running on Linux but now we also have
it running on Windows, too.
It took a while to get here. For a long time
I was explaining to people that the basic
purpose of my job was to turn orange W's into
green W's because it turns out that when you
run tests you find out a lot of them were
flaky. These are tests that had never really
been run in continuous integration before,
and a lot of the effort was just getting them
up to that quality standard.
And you can see we still have problems. Right
at the bottom of this screen there's an orange
4 with a little star by it, and that means
that somebody has come along and said we know
that's a flaky test. So even though that test
run has failed, it's okay -- you know, it's
okay.
We've also got it running on Firefox OS devices.
And this is forming part of the certification
suite that we send out to people making Firefox
OS phones to make sure they didn't break anything
that we didn't want them to break.
So there's a video of this, too, and I will
talk over the video because it turns out that
a lot of getting this to run is watching the
phone reboot a lot of times.
[ Laughter ]
So it's kind of my hobby.
Yeah, so we -- we can use this to check that
when people are shipping Firefox OS phones,
they aren't accidentally regressing support
for the Web platform because of changes they've
made in their local branch of gecko.
So this does a lot of setup initially to check
-- here it is checking can I connect to the
host computer? Okay, I can connect to the
host computer. Let's restart again and actually
run the test.
But, yeah, I think people were quite surprised
with this architecture, how easy it was to
get these running on a separate device.
I think with some of the architectures we'd
used before where we had tests controlled
by JavaScript in the browser, they were having
quite a lot of difficulty porting that to
mobile devices.
Okay. So now it's actually started and I think
it's going to run some tests. It installs
a very simple little app to run the tests
in, and here it is running some tests.
And, yeah, that was a few tests. Not very
exciting to watch, really.
So that's what we have today. But, actually,
there's a lot more that we want to do with
this.
So one thing we want to do, I said that we're
using WebDriver. Well, WebDriver itself is
being standardized in the W3C, so wouldn't
it be great if we were sort of using WebDriver
to test WebDriver in some sort of meta circular
way?
Well, it's not going to quite be like that,
but we want to add WebDriver spec tests to
the Web platform test repository. And so if
you're working with this stuff and you're
finding incompatibilities between different
WebDriver implementations, this is something
where we can take test cases.
What's more interesting than that, I think,
is that we're going to try to use WebDriver
to replace manual tests that we currently
have in the repository. So a number of things,
even in -- like the content window of a Web
browser, you can't test using automatic means.
For example, things that like require a trusted
user input are quite difficult. So like drag
and drop, or input type people's file or whatever,
but using WebDriver we can often automate
those things.
So something that Andreas Tolfsen (phonetic)
has been working on, is having an in browser
that calls back into wptserve to cause something
to happen in the browser you first thought
of, if you see what I mean.
So you write some code like give me a trusted
click event in JavaScript. It asks via HTML
XP request the server to generate a trusted
click event and it uses WebDriver to actually
perform that click. So you can write the test
completely in JavaScript, but they are accessing
this external server.
And of course we want more of everything.
We want more platforms, more browsers, more
test types. We want to test more different
specifications.
And this is something that everybody can contribute
to.
So to circle back to my original point, you
know, the Web is tremendously important. And
we're doing a lot of good work in adding extra
APIs to the Web to adding better layout primitives
to the Web. Flex box and so on. And we have
to if we want to keep pushing it forward,
if we want to write our browser mobile operating
-- sorry, mobile operating system in HTML.
If we want to write, you know, any kind of
native-like UI in HTML, we have to keep making
it better. But keeping it -- adding new features
is no use if we don't make sure that they're
implemented well so that people can actually
use them in a reliable way cross browser.
And the idea of Web platform tests is that
anybody can come along and they can say, okay,
I found this thing that doesn't work for me.
I can contribute back to that, and I can have
it running through wptrunner in the continuous
integration system of hopefully in the future
any Web browser, did you certainly today already,
the Firefox continuous integration system
within a few days or a few hours of submitting
that test. That's really where we want to
go for this.
And I think that will give us a really -- a
really big lever for improving the Web platform
in the future and allow us to really maintain
the velocity of improvement we need to keep
up with other toolkits.
Excellent. Thank you.
[ Applause ]
&amp;gt;&amp;gt;Sonal Shah: Do we have some time for questions?
Okay. Any live questions?
&amp;gt;&amp;gt;&amp;gt; So for the Web platform tests, it often
happens that you are writing tests for a standard
that is till developing. Case in point, for
instance, Web RTC or other standards and stuff.
So how do you deal with that? I mean, when
the standard changes, then presumably the
test should change as well. How do you deal
with that in the continuous integration?
&amp;gt;&amp;gt;James Graham: Yeah, so we don't have, like,
a great protocol for that but I think the
idea is that once people are running the tests,
you notice those things quickly.
Like if you're running the test every day
and then the standard changes, presumably
at some point you're updating your implementation
and you notice of these tests, you know, they're
out of date, we need to change them, we need
to, like, remove them, whatever.
So you actually have that incentive. Before,
the situation was that, you know, we had these
tests but people weren't really running them.
And so there was no way -- you know, nobody
was incentivized to actually do the work to
update the tests.
I think running the tests is key to having
good tests.
&amp;gt;&amp;gt;&amp;gt; Thank you.
&amp;gt;&amp;gt;James Graham: Oh, sorry.
&amp;gt;&amp;gt;&amp;gt; So you showed us that there is this way
of writing test site, but I was not clear
how the test oracles iterate. How do you specify
what is expected, what is not, because with
Web platform, many of the times it's due to
the rendering or how it was rendered or how
the layout of the page was; right? So how
do you specify those complicated oracles in
test?
&amp;gt;&amp;gt;James Graham: So for testing rendering,
we have a different test type. So I told you
about JavaScript tests. We also have something
called a reference test. And what you do there
is you create a page that should -- well,
you create two pages that should render the
same way but using different techniques.
So typically one would be the thing that you're
trying to test, like say you're trying to
test Flex Box, then you might generate some
complicated things using Flex Box, and then
one might be a simple page that you generate
using just normal CSS positioning.
And as long as they look the same, then the
tests pass. And the advantage of that technique
is that it's very robust to changes in the
browser that shouldn't affect the test. So
one typical problem you have with testing
rendering is that if you have, say, some text
on the page and then you update your layout
engine to anti-alias the text differently,
then all your tests start failing. But if
you're always running the two tests together
in the same version of the layout engine,
then changes that don't affect the test in
a sort of normative way aren't going to mean
that you'll have to update all your master
screen shots.
&amp;gt;&amp;gt;&amp;gt; Thanks.
[ Applause ]</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>