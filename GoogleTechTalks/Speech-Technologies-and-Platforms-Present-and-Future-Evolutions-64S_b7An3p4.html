<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Speech Technologies and Platforms - Present and Future Evolutions | Coder Coacher - Coaching Coders</title><meta content="Speech Technologies and Platforms - Present and Future Evolutions - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">⤵</li></ol></div></div><h2 class="post__title"><b>Speech Technologies and Platforms - Present and Future Evolutions</b></h2><h5 class="post__date">2008-03-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/64S_b7An3p4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay and what I would like to talk this
morning is to give you a few idea what
why the speech technology are as they
are today and try to describe a little
bit text to speech ASR and also speaker
verification and identification only
little bit otherwise he will take ages
hours and hours then I will talk a bit
of the platform and the standards that
is what I'm doing since 2001 what raman
said before I will conclude with giving
you my opinion about what's going on and
what can be the next step of the
technology and of the platforms
obviously you're free to interrupt me
and to ask question in any moment if we
can do something interactive is better
my presentation is a bit long so we will
jump from one place to the other also
depending on the time we have one hour
time ramen okay few word about la cuando
if you don't know la cuenta lo cuando is
an Italian company was born in 2001 when
Google was born in which here 99 so we
are a little younger than Google but
behind us there were 30 years of
experience because actually we are a
spin-off of them Telecom Italia research
lab so there are people that is life
that they are working on a sr ntps we
are 90 people and we are still owned by
telecom italia even if we are a company
we are one of the leader companies in
speech technologies certainly the
European leader and our at quarter is in
curing arrahman have been there so if
you want to come in tooting I would be
very happy to invite you to visit our
company and also to eat chocolate ring
wise and so on all the things that you
can do in theory I'm sorry Dave I'm
afraid I can't
that this conversation can serve no
purpose anymore goodbye for the people
like me and also Roberto phe nee if you
know that this now is was in IBM before
night ENT and now is in a small company
called speed cycle the people that
studied in the 80s the goal was to do
machine like Hal 9000 but after many
years the color reduced night as Roberto
is saying his blog would be nice to have
the speech technologies like the ATM
machine that they do an easy task they
do it cleanly simple to use and that's
it not to have very very high goal even
if he's interesting to continue to have
high goals but what is important today I
start to use those technology in all the
place where they can perform like the
ATM a beast a bit of history the history
of speech technology is very long is
start at least in the 18th century at
the end of the 18th century if we look
at the TTS only this is the arrow
designed by clot in the mid 80s before
the 18th century the idea was to have
Tolkien has underwear in the Middle Ages
in other period but in the enlightening
period they tried to studyin to create
southie for instant cruds in stein
construct some resonators that were able
to pronounce the vowels but the firm's
that tried to create a machine was von
kempelen that did the box that is over
over there and that using two hands in
those small picture is easier to see on
one hand you pant the hair on the other
hand so you are stretching this ribbon
thing here in order to distort and
create
eight different shapes and with the
finger you can press those in order to
make the sound this was a really talking
machine they reconstructed it and you
can listen out it was speaking as by
this is the Machine speaking I go oh oh
so this was the best in the 18th century
then in the nineteen more or less the
mother remained the same we are right to
the early 20th century and the
electrical analogue was discovered they
started to be used maybe the most famous
water that was present in 1939 there are
ten tutor circuits in the votive and
combined with the two energy sources
they give a total of twenty separate
components to be used in building up
speech sounds but now let's have mr.
Garrett and miss Harper actually show us
what the voter can do with these 20
separate sounds well we've heard the
voter make a word then by combining
words of course we get a sentence for
example Helen but you have the voters
say she saw me that sounded off e-flat
how about a little expression say the
sentence in answer to these questions
whose are you she whom did she say on me
brother TT so as you have listened there
was a certain degree of flexibility even
if the voice was not very pleasant and
there was a person of the typewriter
that was moving some keys in order to
control the machine then the 50 and we
arrived more clothes this is the IBM
singing with you a Merry Christmas and a
Happy New Year 65 I put in the
presentation many many other links if
you want you can try them well the
that was obtained in the 80s was
something I DS 35 several of the deck
talk voices I am perfect all the
standard male voice I am beautiful Betty
the standard female voice some people
think I sound a bit like a man I am huge
very a very large person with the deep
voice I can serve as an authority figure
these were the deck talk voice is the
best obtaining in 80s 1986-87 and they
are still more or less like these those
are for mondays we will briefly see how
they are done one interesting feature of
those kind of TTS what they can speak
very very fast and I think Rahman is
using a four-month ETS are you yeah 36
dead talk speaking at about 300 words
per minute the following is a list of
topics in today move in the birthworld
szeretlek love to Detroit first-round
matches were played in the wimbledon
tennis tournament oh no okay and if you
want to listen to all the example you
need to click to this link I just seen
that now they are restored this this was
a kind of historical project and so I
put the question how is possible to save
this was very interesting because they
made a little bit the history of all the
reserves lab the people that work at
before everything is wept because the
people are died and you lose everything
I have 20 free colleague in la puente
that are close to retire would be very
interesting to interview them and put in
a place their story because it remains
and in that project there are also the
audio of all those different variation
of TTS if we look more in general the
field of speech and language we see that
in the same period different pairing
were fighting at the beginning were
symbolic against stochastic so symbolic
they were derived from artificial
intelligent
Nick the two stochastic from statistical
and in the 70 and 80 s there were at
least four different fields that were
not talking each other so all of them
were doing research project but without
any correlation at the in the eighth in
the 90 those field try to mix together
and now they are borrowing techniques
one from each other in order to improve
the results certainly in the speech
technology the statistical corpus-based
for mollys were the one that gave the
better result and are the one that are
used today this is a bit from Italy but
we can skip that ok I will talk a little
bit of the speech technologies two days
and I mean what you can strut from an
acoustic signal that are obviously the
word and also the meaning if you can
grasp the meaning from the recognized
word but you can derive many other
feature forest and the language the
gender and also is very interesting
today emotional state or the person that
is speaking because they might be useful
to adopt the interaction to the speaker
or forest and the language to select the
right language to speak to that person
and on the other side for the TTS from a
meaning you need to obtain a text and
from the text the text to speech should
be able to read any kind of text is a
reading machine that should be able to
do like a human read everything even if
it's not easy I will start with the TTS
just few things if we there is a
sagittal of person the vocal tract
include the vocal chords that the water
are emitting the sounds and then two
main resonant cavity one is the mouth
and the other one is the nose and the
volume is closing the nasal cavity when
is not needed
so the glottis is doing something ID is
a kind of impulse signal and all he's
done on the resonation that is done in
those chamber and the result of that is
at the end to obtain these for the voice
its sound for the invoice it sound is a
bit different but this is more or less
what the person is doing the synthesizer
there are at least three big families
one is the articulatory synthesizer here
the idea is to study how the human is
working so to understand how all the
muscles and what are the parameters that
are in place enter to use them in order
to reproduce the voice the form a
synthesizer had a similar idea but they
don't care of what the human is doing
they care of the result there at their
time to reproduce only be the result
when the person is speaking those two
family are called parametric because the
idea is that there are set of parameter
and rules let's change them in order to
realize the text to speech but the third
family that at the time of clot in the
mid of the 85 ft 7 was a little bit
stupid technique that is a copy
technique that is concatenate if that
means I have the sounds I know that I
need to put some in close to the other
so I need to copy them one close to the
other and then to do some smoothing and
I obtain the tts this was the idea
behind the dye fondest and in the 90
there were only thine phone-based
because there were computational you
cannot have a very big corpus for the
TTS but at the end of the night is the
unit selection was discovered lo cuento
was one of the first to do that the idea
was very simple was if I'm connecting
small units the dye phone is the
transition between one phoneme and the
other so you're connecting the iphone
transit
from one phone him to the other and then
you add other parameters in the unit
selection the idea why not to search in
a database search the best match of
segments and then connect those larger
segment the result was a very very
natural sounding voice i will show you
this is what we did oh you know even
owning it on Olga a lot o Romeo in
bloody room this is Italian that was the
best done in the 80s now in the nineties
me t amo Mario ESO no la voce robotic
adil Oquendo escribir una frazee yo la
Leggero convo team naturally he says
write a sentence I will speak in
unnatural way well this was very
comprehensible all the tests done on
intelligibility were fine so if you have
a voice speaking like that you can
understand everything but was unpleasant
so the unique selection is doing this
Bongiorno tutti señoras y señores
buenos días hello everybody muzu ed
Avenue come tuck you say that this list
I'm by spear phones entities ietters
paha each bandidos to me for look when
do TTS you see that the naturalness is
completely different and actually the
same technique of those free and a
different it years so the third
generation can degrade to the second one
if you need to say something that is
very word and you cannot find longer
segments so is exactly the same
technique but if you find a better match
you gain in a very natural natural voice
yeah
okay the question is out the unique
selection is working I explained you
going ahead I will explain you now
because if you look inside the TPS there
are two modules the first one is natural
language processing you take a text you
need to identify the words from the
words you need to know how to pronounce
the word but you had more because you
need to give an international give
something more so this is done exactly
in die fun and unique selection the end
of the natural language is a sequence of
phonemes and other parameters that say
how long it need to be spoken at each
and so on this is the result the unit
selection is in the second part using
those phonemes you are assessing a big
database of recorded voice that is
labeled with the phoneme and or the
other parameter so if you want to
pronounce something you find the best
match in the database of the segment
that can be good match of pieces of the
sentence so you can have an entire word
but also beat of the words connected
together but as soon as those segments
are a little bit larger than a single
transition between two phonemes the
quality became ire and higher and so you
realize that so yeah
yeah in any case in the TTS today you
have a human speaker that is recorded
the point is what do you record for the
dye phone you record all the phoneme of
the language at all the transition
between phonemes of the language so the
corpus is smaller usually is a set of
sentences the speaker read the sentences
and the company like la cuando do the
lab labeling and so on on those
sentences is done automatically yes
urine and haze are on the sentences that
already knows what is spoken and you
need to spoil to to mark where the
phonemes are usually this is not perfect
then you discover that some mark is not
well placed and manually you move that
because sometimes instead of Roma you
leave some Rona because something of me
is missing and so the sound is different
and so you have a tweaking at the end
the unit selection you do another thing
you say well the Italian I need to cover
all the phonemes but also the most
frequent sentences and ward and so on so
you do a larger work so the speaker is
said or speaking half a day is picking
two or three days we are speaking of
number of our 89 number of hours are the
hours needed for doing a unit selection
tts
not not not not because you have the
connection otherwise it would be came
not eight hours but time for $100
million if you do that you will have a
very very very big boys there is a
tricky behind that because in the former
TTS you can tweak parameters so was easy
for IBM to make the voice singing in the
unit selection you are more related what
you recorded so either you recorded in
depth for instant for expressing emotion
young principle should record the
speaker saying sentences in different
emotional states that is very difficult
or what we are trying to do you need to
do signal processing techniques in order
to be able to change but as soon you
start to change the quality drops of
very high degree the voice returned
robotic and so is this is the challenge
today to have natural and flexible
before was flexible but was robotic so I
will skip most of the rest because we
already said so you do something when
you prepare the database for in
selection and then when you have a
sentence you do a matching inside the
database and then you do the call the
concatenation of the pieces and some
signal processing things there because
you need to tweak the junction point
otherwise you listen something weird I
will have you listened something from
our TTS hello and welcome excuse me can
I help you may I have your attention
please oh come on that's no way to talk
to a girl oh I'm so sorry I didn't
realize so in this game ah good grief I
cannot speak okay I do this in this case
what we did was forgive more expression
to the voices we started to do something
very simple that is to record
some set of sentences emotionally market
and so you have them in the database so
you can select those very market
question greetings and so on this is
obviously not the to have an expressive
voice but is to give more aspiration on
some of the sentence that you can
realize something that we are doing is
to try to customize the voice if you
have a single voice you can generate
other one hello I'm Susan lock windows
american voice and here's a be my
daughter hello i'm at me and my voice is
generated based on Susan's voice using
the new timber functionality and I can
also use deepa timber a deeper deeper we
have a team where you can change that
something sometimes the voice you
generate are nice and you can use them
in many case they are stupid or silly
but there are application also for silly
voice for instance gaming and so on so
is a good feature to have in order to
the to change another thing we did was
try to synchronize the voice that is
speaking with other audio for instance
music life here are you searching for
speech and music let's listen to a MIDI
as Mozart
today lock window TTS offers a new
feature using simple command tags
embedded in the text users can only
produce audio files and music areas of
arbys the musical toys and synchronize
change audio file so you can fade the
music and right people knowledge is my
command such mix play as well as your
class envision allow users to have
complete control on the audio sources
are you doing anything tomorrow would be
great to see you another work come
before eight ring later bye for now hugs
and kisses Susan if you have text
messages like those that are written in
a strange language the point is how to
render that you in that case need a
lexicon that is able to grasp what
throughout the world that need to be
spoken so for the expressive TTS what we
did was try to find category like
announcement apologizes compliments
disapproval and for all of them for all
the language find sentences that the you
the people usually says and record them
with different degree of market ness to
have them stronger and stronger so when
you are creating attacks you can select
those another interesting area is mixed
language we are in a moment where there
are many many caves where you need to
mix different languages the first
approach was you change the voice you
have a voice for Italian when you switch
to an English you change the voice and
find an English speaker that is very
unpleasant in the same sentence to
listen to voice alternating listen here
an example many movies have been
produced in filmed in Mexico even though
in many instances you would never know
it we can quote for example y tu mama
tambien or la ultima noche that are
examples of film shot in Mexico you see
that now is changing so many application
is not good because you will
many voices is but another way is to
find a bilingual speaker but is easy to
find perhaps bilingual even if not so
easy if need to be a professional for
doing recordings but if you have three
four five you need to find a five
lingual speaker is almost impossible so
we are trying to do an attempt to find a
way to solve in that the result today is
these many movies have been produced in
filmed in Mexico even though in many
instances you would never know it we can
quote for example e tu mama tambien
ahora la ultima noche that are examples
of films shot in Mexico so what we are
doing is there is a gasser that looks
like the text and try to identify if
there are pieces in another language if
we find the piece in another language
selects the language pack of the other
language and transcribe that using the
knowledge of the other language then
there is a mapping between the phonemes
of the target language and the one of
the voice that is speaking so in that
there are interpolation and the result
is not perfect is a bit funny but there
are many application that is useful
because if you are an English native
speaker is better to speak you in a
little bit english italian a little bit
english french little bit english german
that perfect because my name it may be
that you cannot understand it very well
and is difficult for you to recognize
this technique is heavily used in the
personal navigation devices where you
are in a foreign country but you are i'm
an italian speaker i go in German I need
to have the german thing right in a way
that i can understand for instance their
streets the cities this is an example
hello madam francis dude how may I help
you this is English English is saying
Francis so this is the English
translation I'm sorry
this is the translation of English with
the French in the middle that is
interpolating with the english sounds
under andy and the result is these like
this hello not on france was Dubois may
I help you so he's trying to say
Francois Dupin how the voice is able to
do that here are other funny example if
this is an Italian speaking German
adamant I que vous donc pas de interiano
a true tea light reading so if you go in
close to Germany all the Italian people
there are speaking this way because they
are bilingual in they speak most French
this is me yellow I in Lucca in the mle
cuando synthetic voice I can read in
English as well don't you believe me
well still with my Italian x my name is
Juliet this is my first time in
California hello I'm Korra this is my
first time in Sanford hula a todos hablo
espanol para in vaca kunis voy a maraca
una islam muy bonita ok so that is the
best that we have done today so there
are many interesting area of evolution
of the tts one is the prosodic contour
so to change the sprosila t of the voice
another interesting area is the voice
morphing would be very nice to have a
tts then you tweak a little bit and you
obtain your voice or the voice of
another person of an actor or whatever
you need so to have something then two
more feet in order to obtain the
characteristic of another voice so there
are attempts there are also some result
but still far away from then there are
many other interesting areas I will
reach to ASR because the time is the
time is running very fast in the ASR
though the the thing is on the other
side we have the voice we need to
extract some parameters from the boys
this is done by a front-end then there
is a kind of a search that try to match
inside some knowledge
what is the best realization of that
parameters that you received in the
input and the best one is the result of
the ESR that usually is a sequence of
Ward we're on top you can apply other
things for transform the word in a
meaning for instance a date from the way
you say in the date result this is from
a statistical point of view what do you
do you have a acoustic signal with
samples and you want to recognize word
so if you apply you want the con the
sequence that maximize the sequence of
words given the acoustic input using the
bias rule you can revert that and try to
find the the best sequence of words that
gives that realization times the
probability of the sequence of words
those are the two pieces of probability
that you try to model inside azar the
first thing the likelihood is done by
the acoustic model and the second part
is done by the language model is a
probability of the death sequence is
probably in the language that you are
you are considering the result they are
very high in control and also natural
environment speaker independent
especially if the vocabulary is not huge
if the vocabulary is huge or the people
is speaking in a very very natural and
spontaneous way there are still problems
to be solved by then the performances
drops this is always done our ASR that
is a bit not very typical because it's
an i breed neural network and hmm ASR we
are using neural networks for doing the
front end so we have the acoustic signal
is transform it in cepstral parameters
those are given to a neural network with
feedback
side and this is able to generate all
the emission probability for all the
unit that you have defined for your
language so in a single step you obtain
all the probability that can be needed
and this is if you do that in efficient
when can save a lot of computation
afterward during the search for modeling
the world we are using what all the
people use today that are hmmm but we
found that instead of today many of the
user system are using continuous density
hmm with try phones so they try to use
larger and larger units if the training
corpus is large enough to train in a
proper way those we are doing a little
bit of different things we are using
stationary Transition units all the
units are done like the one that you see
the first is a transition the second one
is sound and the other one is transition
from this sound to another one for
instance Foley the Italian we have 400
different units only other system have
more and more so our way of doing the
things is to save computational time
obtain very reasonable very high with
think result so we are able to easily
create new languages and recognize them
the performance are on small vocabulary
99% like digits on medium size you drop
295 if you arrive to 10,000 or 100,000
words you obtain eighty-five ninety
percent those are state-of-the-art
result that you can obtain so we have a
little strange way of doing ASR that is
giving us very good result in all the
language that we tested it a thing that
can be easily seen is that
the technology can learn on the task
this is a chart of the NIST and is the
benchmark did in the past is a public
charge you see that if the time moves on
this database you obtain better and
better and better result because
actually you are learning from the task
that you are doing the most difficult
one are like the red one switchboard
over there these are those where you
have very spontaneous speech and that is
difficult still to obtain less than ten
percent error rate but for Easton from
broadcast news that is very well
pronounced speech even on vocabulary of
65 k words you can obtain close to
ninety percent of error right I need to
speed up a lot I will just give you an
idea of well actually when you use the
SR you need to give some knowledge of
what you want to recognize today there
isn'ta magic is our that without
knowledge is able to recognize that
would be a phonetic recognizer that
recognizes only the phonemes but today
the result of the phonetic recognizes
are very low so is very error fool
because phonemes are very similar you
need to give a knowledge and usually the
knowledge is given either in grammars
that means I model exactly the language
that I want to recognize all the way the
user the P speaker we say is in the
grammar and that are the most effective
one because of a lot of constraint if
the user speak in the grammar you think
good result the problem if the users say
something different you cannot recognize
or you miss recognize you recognize
something for the other way the certain
is using search in
many large vocabulary things is to use
statistical language models that means
to model probability of sequence of
words the problem of the statistic
language model e that you need to train
those probabilities in the context that
you will use so the big limitation is
that if I'm doing banking I need to have
a system and start to acquire material
on banking on the interaction in order
to do so it takes months to tune a
language model unless you have magically
all those all that corpus already done
somewhere we did something on something
the states in the middle that is to have
a single note that is called garbage
that you can put in a grammar that is
able to recognize everything is not in
the grammar everything else so you if
you have constrained grammar you should
model only the content part that you are
interested in and then use the garbage
to take everything else obviously what
is taken by the garbage is disappears
because you will not know what is said
but the important is that what you are
interested in in the example is from
Rome to Venice or that kind of sentences
is recognized you see that the user can
say million a million of different way
of saying the same things so the
garbages should be able to capture that
and this is an example of the acer GS
grammar where this is the content part
from city to city and sit in the example
is ron dennis and those are the garbage
rules that are optional repeat from 0 to
1 garbage so this grammar is able to
recognize million of different sentences
with from Venice to Rome in the middle
so is a way of saving time in this
grandmother is also the generation of
the result and this is the example of
the result obtained
in one of those sentences we did that
test in different model one was to find
a general model for the language and
another one is to use fillers you can
say okay those words are not pertinent
to the content so I use them as fillers
in a loop for catching everything else
actually the better result was obtained
in la cuando using a phonetic models
using our unique our in order to catch
and that give us very good result we
lose very little so the probability of
losing the accuracy that we can obtain
is more or less the same and so is is
what we have inside the product even in
a yes are there are many other issues
one of the issue is we still need to
treat properly the signal that arrived
from voice over IP from mobile or other
different networks to subtract the noise
if is noisy we lose a lot in the
performances so perhaps you have very
good recognizing quiet environment then
you are in a car or you are in a pub or
you are in a street or you're in railway
station and your tempura result and the
other problem is continue to work to
model the out of vocabulary words and so
on so today we don't see that there is a
new good idea even in the research to
change the frame or over yes our need
someone that discovers something because
the hmm are doing very well they can be
extended so the work is on the edges to
treat those difficult pieces in a little
better way in order to obtain a little
better performance that makes this
re-usable in all the context I think I
need to skip the well I'd give just a
few on speaker verification then we move
head on the next step in speaker
verification the point is that either to
identify to verify that the person is
the one that claimed to be so to use it
as a password I same I'm Paulo then I
speak and from the voice is say okay you
are really Paulo you are not another
person or identification that means i
have many idea of different people i
want to know if the one that arrives is
one of those does that is used for
instance if you have many people for
instance in google to know if are you or
him or another person even if you do not
claim that everything is based on the
creation of the voiceprint that is a
biometric identification of your voice
in this case this is different from a
user because what is important the
unique characteristic of your voice each
vocal track is different a way you
pronounce is different from people to
people so the voice brain should capture
the unique things that the single person
has and then compare them do for the
verification okay I will jump ahead at
this point are there any question
otherwise I will jump in the more
standard and architectural part of the
presentation I put in the presentation
also some reading people are interesting
to go a little bit in deep also there
are some
I need to move in this part and ramen
and we were working in the w3c I think
in the year round the born of google and
how there was a big transformation of
the way of using the speech technology
because the standard body did a good
work in order to create standard for
simplifying the creation of application
the worst started near 2000 and the
first result were obtained and stable in
2004 and other in 2007 and few other
will come these did these in two
thousand Jim Larson that was the
chairman draw this picture is not a real
system is modules that can be a yes our
language understanding dialogue
generation-tts and what he did was to
draw all those possible standard that
can be created to help to do that work
this is the same picture today the
standing read our recommendation so they
are finished there might be a new
release but that really see is done the
work is that and other two or three are
the one in blue like these the call
control em are close to be finished so
we realize what was fought in two
thousand you can have all those module
completely standard that don't care if
you use my technology your technology
and other technology and also for the
developer of the service you can create
an application and then you will apply
it to different platform that was very
very different before that where you had
to be tied to a single vendor if you are
doing a sr with one you had to develop
the thing for him use his platform and
so on so was very expensive and then if
you move to another one you throw
everything away the standard made the
word possible in the field of speech
application
so you do the speech application on the
web because all those are XML languages
you can produce and have in web
application wherever you want then the
platform is a browser specialized to
those languages but a browser that does
HTTP GET and then return result and is
able to communicate and the goal of the
platform is to control the SRD TTS this
is a voice exam El platform so the key
point where it took seriously the web
parting in the field of speech it did a
powerful abstraction to write a voice
6ml application is easy also to write
grammar is easy maybe strayed use to
write grammars but is not difficult and
the voicexml delegates to many other
standard to do specific word for the
Assad or a surge esns is RSS ml for the
TTS and so on for the ASR what is doing
is that you have to format one is text
one that is called a BNF and the other
one is xml-based and you can create
grandma for both voice recognition and
DTMF if you follow the link you find the
spec on top of the grammar you can add
sis r that means to add the semantic
part of the grammar in order to build
the result this is an example of a
simple grammar that is recognizing only
Turing in all four possible combination
the textual ones the XML in on the row
ASR literal only for doing simple
transliteration if you have a noun and
you want to return another one or the
script one you have akma script
JavaScript so you can do whatever you
want on the result of the ASR in order
to building a rich result there are
still unsolved Odysseus one is to finish
the word on the lexicon and I'm
the author of that specifications of his
work for me also and another one is on
the language modeling today there is not
a standard that for the future must be
an interesting area to work to have a
standard for language modeling that
neatly interoperates with the Sergius
grammar and perhaps has all the s is our
capability of producing also in the
language modeling domain of dictation
and so on or reach results for the TTS
if the TPS does all those activities
usually the TDS try to do the best thing
that he can so you don't need to do
nothing you give the text and the TTS is
reading but in the case is not reading
properly you need to tweak something you
want to change the volume or the voice
or something else the SS ml is able to
give you few elements that you can add
in order to say in this pace p in this
place change the voice rise the pitch
lower the volume and so on this is an
example of a very simple document where
you have a few element for giving
structure like paragraph and sentence
and so on I will skip the rest give you
a small idea what is the work on TTS
today the main is international w3c
found that the Chinese people found that
the SS ml was not usable very well
because it's a tonal language and there
was not a control of the tones and
secondly they do not have the white
space among the codes so something need
to be reworking in another way w3c took
this seriously and did a set of
workshops trying to capture not only the
Chinese but also the Indian and other
European and also Semitic languages if
they
something different and what was
captured by those work shop is the work
that is done in SML 1.1 it will be very
similar to 1.0 but better cover the
international languages like Chinese one
of the end-battle Japanese Japanese and
many other this will not be a new
generation of a sesame that will be
weight tip for the next version I am
working on the pronunciation lexicon I
would very like that the work that we
did to create this format for lexical
would be taken seriously and in future
you can find easily resources in pls
because to pronounce something is
specific so we can find many many texts
while resources but resources on
pronunciation we are not very easy to
find and this is a simple format I skip
this where few elements I give you an
example like these you have a lexicon
the leg Simone is done by lexemes X
lexemes can have one or more grapheme
that is how you bright the things and
then you can either for him oralius or
both that alias is a transliteration for
instance for the acronyms w3c world wide
web to have the way of pronounce that
acronym and while the phoneme is if you
need to go to the phonetic
representation then in here i have many
example ok there is a question
yeah you are right there is a problem
there what pls one dot zero offers maybe
is not the complete solution but is an
attempt is and you can add in the
phoneme I do not have the example I'm
sorry here attribute let's say roll
something a URL so you give a URL to a
phoneme so you can have the same acronym
many many times but all of them is
characterized in some way the URL should
be a nice pace that helps you to know
where you can use or not that and to
make unique that Henry so that is the
way that we propose to solve the problem
of homograph that is the one that you
were saying towards spend exactly the
same way but pronounce it in a different
way then there are other cases
interesting for us and for ASR if you
are in the United States many words are
pronounced it in a different way from
different people of different region of
the United States or people native of
different languages so if you want to
properly recognized you need to give
those variation that the same words can
be pronounced it in this way but those
in that way salsa de tous if you don't
give that the SR will do some hair errs
will not be good so would be very very
nice to find in the time resources that
helps the people that want to do a
system in the easiest way because you
can find the tweaks already done by
other people and put on the web about
the pronunciation that is a point
because the fauna Titian are working on
IPA that is all in that chart since the
beginning of the LA the past century so
is a very old but very effective way of
deal with all the possible languages all
the language from Africa
china and so on are using both set of
phonemes but the problem is that there
are glyphs that are a little bit strange
they are unicode so you can write it
would be nice to find tools open source
that helps the people to use that
language because unless you are a
trained finition is very difficult to
create to customize the pronunciation of
the sentence of the world some people
proposal to have languages more based on
standard ASCII called Santa but in any
case are weird translation so I don't
know perhaps this is the more standard
one so it would be nice to have lexical
with these phonetic language and tools
that adds the people to do that I think
I can skip most of the voice exam L the
voicexml is the dialogue so is done of
prawns and grand Mars those blocks are
called dialogues in our forms and user
invoice XML you can use either yeah okay
sorry yeah five minutes or ten okay
sorry okay
okay i will skip the rest to jump to the
conclusion i have minutes five minutes
you can find the rest in the
presentation so in case you can see
something more future well on the
technology i don't see many many things
that are happening at the moment if not
to work on the edges on the difficult
task that are difficult things but
useful for using them on the user
interface there are a couple of trends
that are taking place today roberto phe
nee said that we are at the third
generation of speech application of
dialogues the first one was the one of
four or five years ago where the
interaction is very guided by the system
usually forgive information and the
grammar are very constrained and the
dialogue is simple then the second
generation is today where you have more
complex transactional services where you
can buy and sell things and so on where
you have 100 dialogue modules is
advocating another generation that has
10,000 or 100,000 dialogue modules and
in his example is given the kind of
troubleshooting where you have a problem
with a hardware you tie explain it and
an automatic system is helping you to
solve to do the test and so on but
another variation of vocal user
interfaces conversational interfaces the
one I said with the garbage before where
you recognize only partially the things
but the point is to have a very very
good vocal user interface that keep the
guy the dialogue going so the
interaction are more more more
simplified than the one of the first
generation of the dialogue another point
is that the speech can be and will be
pervasive everywhere from the car from
the station already today if you in
Italy go on a train in a railway station
there is the TTS on the train there is a
tts if you call a number you have a dial
system if you call a taxi so you can
have dialogue application and speech
everywhere and the other point is to
move toward more distributed application
in these the w3c is doing few things one
is developing state chart XML for
modeling and interaction is an
event-based language with a very very
solid semantics is also part of UML but
is XML and that can be the interaction
manager the one that controls the things
this is an example where you have a
parallel state charts so you can do this
and these at the same time for instance
controlling a GUI we end voice at the
same time so you can have the SC XML
that is doing the interaction and then
modalities different modality one is the
visual one the one that you see another
one is the vocal one another one can be
jest or attic and many other modality
you can have them and the state chart is
controlling all of them so the role of
voicexml becomes less monolithic of a
full application but small pieces of
dialogue that are called when you need
them from the state chart on top of that
also from an architectural point of view
there is a problem where you put those
as 6ml and you can think to have rich
devices with the interaction on that so
the device the browser will download the
s6 MLR run it there or if you are using
mobile phones or very small devices you
can use the modality from here that
using IP interact with a browser that is
distributed in the network and you
do is controlling the interaction in
order to realize the application in this
device 6 ml 3 dot 3.0 will be the
language to be using that context
because less modular you will profile it
you will customize it in a better way so
this is something new that is being done
in w3c and this is the hand of my talk
if there are further questions I will
appreciate or I will be to you I have
all also only audio historical ones so
you can put them to their so yeah yeah I
will give you the presentation I have
also a couple of other presentation more
didactic home SGS SML I will give to you
put there and the people if want to have
something there is a question okay
lower level of technology i okay he's
asking me if the voices bend at the
voicexml is an higher level API if there
are lower level api's well today the
best you have is what has done in the
IETF that created the MRCP protocol that
is media control Reza's protocol version
2 that is a protocol is not an API so
you need to create a client but there
are open-source client for MRCP so those
are the low level but scalable for big
application so you have the API but
usually the APR meant if you are
controlling your single instance of the
ASR and DTS so are for PC application on
your own pc you have some standard API
there are the java speech API and
microsoft's a pap eyes
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>