<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MONGOOSE: Ingest, Monitor, Rinse, Repeat | Coder Coacher - Coaching Coders</title><meta content="MONGOOSE: Ingest, Monitor, Rinse, Repeat - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MONGOOSE: Ingest, Monitor, Rinse, Repeat</b></h2><h5 class="post__date">2009-10-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/pvy5mjPrcPo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright welcome everyone my name is Fred
sour I'm a developer advocate for google
app engine Google web toolkit and I'm a
host today for Daniel grill from IBM
would like to talk to us about Mongoose
a project I think he's had been making
for about 10 years is that right yeah
and just monitor rinse and repeat and I
just wanted to give a brief introduction
known on Daniel he's a research staff
member at the computer science
department of IBM al maiden Research
Center in San Jose dan is currently in
the health informatics research group he
specializes in very large scale text
analytics for a variety of applications
from healthcare to pop music dan karkar
tected IBM's unstructured information
management architecture which is now the
de facto standard for text analytics
products here in his PhD in electrical
engineering from the Massachusetts
Institute of Technology in 2000 with
thesis work on distributed text
analytics systems he was named MIT
Technology Review top 100 in 2004 dan
take away so I'd like to start with a
disclaimer which is that Tyrone and rune
who here as well with me we're all in
research and there's one thing we hate
more than anything else and that is
getting the data to do an interesting
project you got you got this great idea
I can just you know analyze this pull
these features out whatever all I need
is the data sitting there to start with
that shouldn't be that hard um and it's
turned out over the past 10 years that
we've spent an enormous amount of time
just trying to get the darn data for
what we're trying to do and the talk
we're going to give today is taking a
step back from that we've realized hey
if you're going to spend a surprisingly
large amount of your life getting data
there's actually some interesting
research problems around it and some
approaches and some engineering hacks
that make it work a little bit better so
I'll start with the good news right when
we started this this is what we thought
for web resources you just have to do an
HTTP GET and the data is just going to
come back to you for databases there's
this jdbc thing it's fantastic it
connect to any database you send your
query you
get your results for other resources
there's there's got to be a good API
mean anyone who's got data is going to
put a good API over it make it easy to
get we also believe there was a tooth
fairy so we dove into this project
starting with a project called web
fountain where even just getting data
from websites turned out to be a
somewhat larger challenge than we
thought there's a surprising number of
ways things can go wrong so we'll give
it a little bit of a quiz on in one of
the projects we did there was a problem
with the misconfigured mail servers with
recursive and cyclic in links to
replicas of itself so you'd read a
message in one thing and it would point
over to another replica and you'd read
it there and I'd point back to the first
one that now there were three big
projects he you know for this there was
YouTube a healthcare provider and the
intranet anyone want to guess which one
we ran into that on actually intranet
not too surprising with a mail server
but how about videos not available due
to licensing issues that's clearly a
healthcare provider problem um so they
had a big echo video machine they'd only
bought 8 licenses for the the archive so
we connect to it to pull echo videos off
and eight physicians were sitting on the
licenses and it just died right there on
the table um website unavailable due to
accidental DNS poisoning yes we all
remember that fun with YouTube we
remember that fun at 2am with YouTube um
because we were doing a project that
required it so it's it they're
interesting ways that data acquisition
can fail and they're usually not what
you expect them to be so for the past
five plus years we've been delivering
products with IBM's product and
consulting business there are two big
businesses inside IBM for about the past
15 years at all madam we've been kicking
around analytics and web research in
general looking at ways once you've got
the data to do fun stuff with it and
we've done a number of service
deliveries that have turned out to be a
lot more late nights than we might hope
so we set out to say how do we address
the biggest problem in web and analytics
today and that is just getting the darn
data
and in we came to this realization when
we realized there was no tooth fairy and
we were just gonna have to bite the
bullet and figure out how to do it right
so the first project I'm going to talk
about is one that we did with the BBC
about two or three years ago it started
I hadn't realized it but the BBC's
actually was was commissioned to bring
music to the British people that was
their 1930s or 40s mandate so they take
it really seriously and for some reason
they decided that the top 100 chart of
music they were getting from the RIAA
might not be entirely accurate um I'm
not sure where they got this idea i mean
if you look the last time a major label
got five millions of dollars for
illegally trying to influence chart
position I mean it's been what easily
two or three weeks since the last the
last time those penalties came out so
they said wait a sec what if we just
look at everything people are talking
about on myspace YouTube last.fm bebo
you know look at sales on on iTunes
bring all this data together and we'll
create our own top whatever list of
music and of course they said since
you're going to do this we'd like you to
do it four times a day so we want every
six hours a new top thousand list we
want it to reflect what's hot at the
moment what's popular so we said okay
this can't be that hard we'll go do it
um and so the one of the big things we
ran into is there's really surprising
variant of English used by kids when
they're talking about music for example
if I say you are so bad that means
you're good if I say your album is a
bomb that's bad if I say your album is
the bomb that's good if I say your album
is sick that's good and in fact this
slang evolves and changes so rapidly we
we had to do a whole kind of NLP on
broken English who actually drew from
online resources like urban dictionary
to try to keep up to date with what the
slang of the moment was um in doing that
you know we had a basic flow where the
the data came in from a number of
sources
went through analytics to try to pull
out sentiment what artists were being
talked about there's some interesting
research there and then went through and
then posted up to their website every a
four hour four to six hours and then
they published this is actually going to
be shown in December in Chicago at the
SES um so this was a actually a real fun
project in The Guardian had a great
review of it when they sit because one
of the other things we did is we knew
the demographics of everyone who posted
so you could say what do 40 year old
women in the US like in the genre of
electronica and generate a charge just
on that so it's a lot of fun they
actually gave bi tools to two teenagers
and and they went wild with it ok so
some things that went wrong dns timeouts
a load-balancing side effects so you've
got 15 servers you're going to be handed
to one typical myspace will say this
server needs to go down for maintenance
so we're just going to stop accepting
inbound connections and start shutting
it down well if you're talking to it
while that's happening and then it goes
down and then you try to reconnect the
IP address you get a down machine again
and again and again thank you have to
back off flush your local caching and re
resolve the DNS to get one out of a pool
that's still up a failure of source
servers servers when they die tend to
get slower and slower and slower as
they're usually experiencing disk errors
after a while you have to throw up their
hands and try to find another server
that has that content some simply broken
api's um we won't mention there was a
video provider who I'm sure you've never
heard of who does these large
collections of online videos and they
had an API for getting data about what
was going on on their site and it may
not have worked exactly as spect so you
know we had a formal arrangement going
on so we called up and we said hey what
gives and they said oh it's easy we were
acquired by Google and the person who
knows how to do that move to another
exciting project and none of us know how
it works and that's true actually with a
surprising number of data sources that
the person who wrote the access data API
thought it was a good idea at the time
it as a 20-percent project and then
moved on with life and everyone kind of
assumed it was good enough until you
start hitting it for millions of
transactions a day malformed HTML clever
users who either embed tags in their
stuff to try to get something to pop up
or who answer questions like where do
you live in my head this is very hard to
resolve on an atlas and lastly spam over
fifty percent of the comments on some
artists pages are spam comments you know
come see my site whatever so you need to
be able to deal with all of that Gorp if
you want to do an accurate ranking
switchgears an entirely different
project that that we did this year was
drawing data from about 70 different
sources inside a very large health
provider grabbing information from
different reports and trying to compile
an at-a-glance view of someone's state
of health and this is cardiac health so
this timeline that you can see here with
all kinds of diseases is when they
showed up and when they were flagged as
as severe and whatnot on represents
something on the order of a hundred to
two hundred pages of text that's been
plus lab reports and EKGs and other Gorp
that doctors have taken in this case
over 20 years they generally take two
hours to put this mental model together
in their head when they first get a
patient by flipping through the charts
so they were pretty excited when we
could pull it all together we were
pretty excited right up to the point
that we tried to scale this out to a
large number of patients going through
this this workflow and we discovered
things like many of their archives had
zero length files a missing files
invalid JPEGs mid-session timeouts their
EKG machine was running a very
up-to-date variant of NT 40 from circa
nineteen ninety three and an SQL Server
variant from nineteen ninety one that
had a conflicting odbc stack so it would
occasionally wedge and drop the machine
um and you know we of course called the
maker and said what the heck and they
said oh the guy who did that in 1996
retired in 1999 and we don't know what
to do by our new machine and again
if this sounds a lot like the YouTube
example we started to run into this more
and more so if I could characterize the
when we came in we were in denial at
this point we were in anger if you're
familiar with this process um and so we
said okay is there anything we can do
about this is there any way we can make
it easier or less painful so let me grab
one example which is custom connector
code all of us have had to write this
there's a data source over here we want
to get data from it um it was written by
someone who it may not be dead solid how
I have to write a lot of code and then I
have to maintain a lot of code so we
said okay let's admit you're going to
have to do this there's no way around it
there's no magic what we can do is say
there's an awful lot of stuff that's
pretty repetitive about getting data
from a source maybe we can put some
libraries and templates around it and
some kind of a controller visualization
framework over it so that it's easier to
write these things so for example if I'm
saying string data get URL maybe I can
just wrap or get so every time I get
something I update a chart that says how
fast it's coming in maybe I've got some
rules about when I can access and how
many threads can be accessing maybe if
the source is done I want to block maybe
I want to attempt multiple strategies to
get the data if there's more than one
source and maybe I want a page a key op
if all else fails all of this is can be
embodied in is pretty much repeated on
any data source connector that you're
going to get so you might as well just
box it up as a library and in fact hide
it under that get so that you use a get
that has all of this built in but work
function identical to the built-in HTTP
gets so what is Mongoose basically it's
a research agenda and automated in semi
automated data collection technologies
can we pull things together from a bunch
of different places put them in a box
and make this awful job of writing
connectors and getting data or easier
and in doing so we've created a platform
for what we're calling worst case
scenario data flow or web data
management the model being rather than
saying there might be a failure we have
to deal with we say there's going to be
a failure in fact most of your life is
going to be failures you might as well
build your
just around the fact that it's going to
fail in some cases more often than it
succeeds and basically that okay so how
does it work it's two parts data comes
in through an intake platform which is
just a series of steps that get applied
we use the phrase ETL extract transform
and load the data from one source to
another source and then a control
platform in the middle as you might
guess the intake platform is going to be
tasked specific however the control
platform the thing that turns things on
and off monitors when they crash and
gives you some idea what the heck's
going on is generalizable intake
consists of data acquisition maybe
you'll get lucky maybe you'll do two
projects with my space so you only have
to build that acquisition piece once of
course my space changes and reef actors
their code in visual basic so anyone
ever seen this this is fantastic so they
have their whole site about once a year
I'm convinced someone just decides that
it'd be fun to change the name spaces
around so they change all the name
spaces and they hit refactor and every
page on the site changes and you know we
now have lines of code where you know
comment out okay now we're looking for
this class now we're looking for this
namespace now we're like no other change
identical code they just refactor
everything for the heck of it um
pre-processing you can often get data
that's going to come from your source in
multiple chunks sometimes you'll get a
chunk that's actually got multiple
pieces in it so maybe it's a page with
posts or maybe it's a post that runs
over in multiple pages you'd like to fix
that problem before you hand it on to
the next level language identification
there's things like legal music health
all have their own languages we've got
some NLP software just load them in and
do some base level general we know it's
not going to be great but at least spot
the artists at least spot the medical
conditions at least spot the legalese
and give you some idea so upstream
people already have that tag gap
domain-specific is looking at you know
what are the dimensions you're going to
care about and lastly adjudication when
you've got multiple sources that
disagree how do you bring them together
so this is basically content
dtl extract transform and load but done
on unstructured content the control
architecture which I'll talk a little
bit about later basically goes through
and it gets information from the
pipeline on how things are going and it
can poke the pipeline to change things
if things aren't going right okay so any
questions before I jump into specific
projects okay okay so the first thing we
found that works pretty well is to have
some kind of way of start if you've got
a cluster of machines and one thing with
ETL is they don't tend to be identical
machines and they don't tend to be
identical code running on them so the
standard scattered gather type technique
generally isn't what you want what you
want to say is find me three relatively
free machines and start this set of
processes on it so we did that and that
work pretty well and then we found that
the little thing that you had starting
it would occasionally crash right you
know it's a web service that you're
talking to and telling it to turn things
on and off and getting standard out and
standard error and all that stuff and
then we discovered that in UNIX what
happens when the parent of a process
crashes anyone you're left with an
orphan and there's no way to ever get it
back it just sits there until it dies
you cannot repair entender you next so
then we said okay we'll have one process
that owns it and is dead simple and
isn't going to break we're shared memory
over to the web service that because
it's a web service is going to go down
sometimes so that's the nananana sort of
breakdown that we had where you can
connect to the nanny and give it control
and monitoring information it can talks
by shared memory over to the this one
thing that just looks at your one
process since this is it using too much
cpu zuv running out of memory and log
standard error and standard in so that
you can look at them queuing and
batching this was another one that you
know it was worth biting the bullet and
doing right a cue for us is a task that
needs to be done that task will come out
you'll it will be in a state of being
processed it will either succeed or fail
if it succeeds it probably wants to go
into another queue to be done if it
fails it probably wants to go into an
q to be taken care of if you can make
this look an awful lot like LPR lpq LPD
etc then you don't have to train people
on how to use it they just type the
commands in a way they get but what it
lets you do is add new elements to your
flow just by changing the pipeline of
what batch things go into again and
again additionally you can play these
games where you can take your error
queue and periodically flush it to your
bug tracking system saying dear
developer here's something that failed
and here's exactly the input data and
exactly the code that it failed on when
they fix it they can hit a button and
send it right back into the process
queue and pick up with where it was
before so the data basically sits
waiting until the bug is fixed then you
can say okay I think I fixed all these
bugs please refresh them let me see what
falls back out in the era q this allows
us to do something we're calling agile
data driven development basically your
client wants you to deliver something
now yes they signed the contract
yesterday no they don't want you to have
a development cycle that you want to see
something immediately so they can begin
to iterate in that environment you need
some way to let the good data flow
through so that they can see stuff and
the bad day to get logged out so that
you can track and fix the problems
sanity checking this is kind of a fun
area we've found there an awful lot of
tests you can do on a stream of data
without having any idea what that stream
of data is what it represents or what's
in it for example is the quantity of
data today similar to the quantity of
data yesterday are the latency zhan
fetching similar to what the Layton sees
were yesterday in cleaning type checking
is this number you know did this stream
give me 5 7 13 12 and tomorrow it's
yellow okay something changed um do I
know that there should be monotonically
increasing my favorite example here is
and again I'm just going to pick on it
but there's views of number of times
people have you to video ever write me
you check it today and you check it
tomorrow and tomorrow the numbers
smaller than the number was today that
generally indicates either a negative
number of people watched the video or
something's wrong somewhere we found
this and frankly last.fm was a
lot bigger offender of this then then
YouTube was on you just have to cope in
their case they had actually a
reasonable explanation they had multiple
machines every week they synced them and
then they estimated based on that
machines how often that machine was hit
what they were seeing and then at the
end of the week they sink them and some
of them had drifted high which is okay
for a for an application unless you're
actually interested in how many people
listen to a song and lastly I'm
processing results correspondence and
reasonableness if you've got data that's
coming from two sources are they
actually correlating when you get an aid
is there always a matching V is there
one missing these sorts of tests don't
require you to know what you're dealing
with and therefore you can write generic
checkers for them on one that I'll I
threw in just because it was an awful
lot of fun to do was we rented the
question if you've got ranked lists like
sales data looks like this views look
like this listens look like this post
look like this they're all in different
orders about what's the most popular how
do you combine those in a fair way and
we actually found that voting theory is
an incredibly useful way to combine
ranked list of things people have spent
a lot of time figuring out a fair way to
combine ranked lists of different
opinions from different sources and we
implemented a whole set of voting theory
toolbox that let you take this data
merge it together into a sane result and
pass it on so I guess the ultimately the
value proposition is if you're going to
fetch and transform data figure out a
way to develop and deploy in parallel
parallel that's that agile deployment
without requiring agile end users
because in general end users will
complain when it doesn't work and they
don't care that you're in development
don't require the developer to consider
all the failure modes from the start
people are really good at coming up with
failure modes but they always forget
some of them counting on people to guess
how things are going to go wrong is just
to lose again do it in a reusable way so
we don't have to re-implement this every
time and detect and respond to failures
quickly people are much more
understanding if you fix their within 30
minutes than if you fix the error within
18 hours so detect the earth soon if you
can get it fixed before they notice it
that's the best of all possible world
but if they can call you up and you can
say yes we we've known about it for an
hour we're working on here's the status
so far you generally will have happier
customers okay so how well does it work
this is just examples from our medical
project so first tries which can be up
to 10 retry fetches for one set of data
sets 600 of them actually worked as a
272 needed two steps to try to fix the
problem or restart the machine three
steps four steps five steps etc so you
can see in many cases your first
alternate path is going to get you most
of the way there but it's a diminishing
returns type scenario so we said okay if
this hadn't had the system automatically
trying all these different ways to get
data instead of the one half full time
equivalent it took to run this half a
person it would probably have taken
closer to two to six people so we're
probably only saving something on the
order of a factor of 4 to 2 12 ish on
the number of people you need to monitor
a system like this and that's largely
because in order to monitor a system
like this that's running 24-7 you need
three chefs that's the reason the
numbers as large as they are number of
unhandled failures for this project was
in the hundreds over the course of a
couple of weeks troubleshooting manually
took about 24 hours for every problem
and response time when there was a
failure took two or more days from when
a failure happened to when a person
could get down one challenge we had with
medical data is all has to be in a
medical facility so when something broke
that means me getting in the car and
driving to a hospital in order to look
at the data so again you're not going to
make your problem go away entirely but
you might save three quarters of the
pain and heartache associated with with
getting the data going so what's the
research behind this this is our
acceptance of the problem and now we're
going to move on first off is that error
handling and is an awful lot like
planning and scheduling research in AI
if you have different ways to accomplish
a task that have different costs you can
use forward and backward chaining to
figure out interesting ways to try this
see if this were
see how often this works assign
probabilities for how good a solution
this is and the more we're call them
verbs that a particular fetcher
implements the more choices the
scheduler has on how to tweak things and
try to get the data it needs so that's
but that's a fun area to poke on another
one as i said is content agnostic sanity
checking with structured data we call
this type checking and it's easy with
unstructured data like videos you might
want to ask the question are all these
videos dance videos is there any way to
make that determination without actually
doing scene understanding on the video
can you look at bulk characteristics of
it can you look at training can you do
machine learning type things but on your
data feeds entropy Bay statistics seem
to work well in a surprising number of
cases if you've got a series of pictures
that are all of human hearts and
suddenly you get one that JPEG
compresses much smaller it probably
isn't a picture of a human heart even
without knowing anything else and lastly
machine learning so that the human
annotation required is minimal if I tell
you the past three days have been pretty
good keep it like that is that enough
training can you figure out what you
need to do from there add a distance
system.diagnostics um one thing we did
as part of working with myspace was
actually to map how their servers worked
which they were quite fascinated with um
looking from the outside what do you
actually get you what's your chance of
getting to a particular machine what are
the features of that machine can we
figure out what the maintenance schedule
for this machine is so we can stop using
it when they're going to take it out of
service figure out what the response
time for problems are if i call this
data center are they better than this
data center because I'd rather call the
one that will fix it in two minutes than
the one that will fix it in two hours
and lastly and this is kind of insidious
there's nothing more fun than calling
someone up and saying from a port scan
it seems you're running this version of
the software on this version of the
hardware and there's a known problem
here's a link to the patch we recommend
you install that the more you can help
the ops person the faster they will get
your machine back up on its feet and
lastly is data acquisition optimization
you're always in a case where you have
fixed resources
in the medical example we had two kinds
of cases we had cases that we knew were
coming into a surgery up to a month
ahead of time and we had cases that we
knew were coming into surgery an hour or
two ahead of pot the hour to ahead of
time ones were kind of interested they
generally were cases that another
hospital had said we don't know what to
do with this we are bundling the patient
up and we are sending them to you
because you are the premier center have
fun it's been nice so these guys will
hear that a patient is on the way they
actually have a couple of dot of
physician assistants who go through the
records and create a more or less power
point presentation on you which they
then present the doctor who looks at it
scratches his head and goes into surgery
so these are cases where there's a
higher value on getting the data now and
the patient who is in a more routine
situation maybe you want to defer so if
you know that this car EKG machine is
going to crash if you hit it for more
than 100 requests ask the important ones
first and that kind of overall data
acquisition optimization is a highly in
a highly nonlinear environment is a very
interesting research problem that we're
starting to kick around as well big
picture user interfaces I'm a very lazy
person um so I worked with a very
talented designer ondrea's to develop
this interface for a large cluster the
idea here is that each arm on the on the
cluster is some statistic about that
machines performance there's this fun
thing where I can ask you to point at
the weird-looking star and with zero
cognitive load you can see the
asymmetric start point at it and when
you touch the screen actually pop up an
SSH SSH session to that machine we ran
this on a cluster of about a thousand
machines it was fantastic we left the
display up by the door on the way to the
bathroom from the ops room so when they
got up from their desk they'd see
something was odd they would touch the
button and they would figure out what it
was and then they would go and heckle
the person who should have seen it
happen you know should have seen the
problem ten minutes ago so taking
advantage of environmental displays
interfaces that don't require you to
read long list things that just jumped
your attention is another
challenge we have especially on these
large data gathering projects so we've
brought written a bunch about it um we'd
love to get any feedback if there's
anyone interested in this kind of a
problem we'd love to figure out how to
collaborate thank you much any questions
absolutely so we're continuing to say so
Varun and Tyrone and myself working in
IBM's research lab down in in the south
bay and we get us a lot of customer
engagements where we're doing our called
first of a kinds whether someone wants
something that just doesn't exist so
they'll commissioned us to go down
figure out how to make it happen and go
for it and because we're all in the
information management sort of area most
of these involve getting data from alert
from a bunch of different places inside
you know whatever the client has so we
tend to use this on every engagement
every time we turn the crank we learn
something recycle reduce reuse recycle
but we'd love to work with people who
are also facing these kinds of problems
because we're definitely hate doing this
we'd really prefer someone else did it
for us but we've kind of accepted that
may not happen so we'd like to
collaborate with people who are facing
similar problems and share solutions
absolutely yeah
that's a fantastic question and it gets
even worse because in one of the
projects that that we were doing data
will flow into legacy systems that will
archive it so you might see the data
coming from the archive you might see
the data coming from the original system
you might see the data coming from the
display system and frankly ten percent
of the data mrs. each of them so any
piece of data has about a twenty percent
chance to be not presented in all three
even though it should so you end up
basically looking for maximum likelihood
join across all of them if you have a
dangling piece of data you say how many
pieces of linking data do I have and if
I have enough of them then I say they're
the same and if I don't then I leave
them as danglers but it's a hard core
it's a very hard problem see this is
someone who doesn't work at a database
company oh you would be surprised what
ends up in databases through no fault of
of the people involved if you have a
user interface where you have to type a
patient's eight digit medical record
number and there's no display back as
you type this on a keypad guess how much
data ends up in that database in kind of
a funky way and it's really a bad user
interface design decision no one's fault
but it does mean that the data that's in
the database sometimes is the worst data
not the best in a particular case yes
excellent great question it was an awful
lot of fun so what we did is we tried to
figure out and we spent a lot of time on
this of a purchasing an album is equal
to how much many times the interest of
posting on myspace right is it 13 and a
half times as much of an ex
version of interest how about you know
listening to it on youtube well you're
you know so we have this grid if active
versus passive creative versus
consumptive behavior and you know talk
to you know experts in the field who are
like how much cognitive effort does it
take to do is a mess because every
couple of weeks they ended up going well
those results look goofy let's try
tweaking them again so we finally
through our hands up and said okay this
is silly what if we said within a
modality we're pretty sure that the list
it makes is going to be reasonable right
so number one is going to be number one
number two is going to be number two
remember I want to say how much better
number one is than two I just want the
ordered list right so sales gives you
one last views gives you another list
etc now how can i combine these lists in
a fair way so what we did is we actually
said okay each of these lists represents
a voter that voter has a certain weight
you know like an electoral college they
go and they vote this is my number one
and I will give them this many votes
this is my number two I will give them
this many votes so you add them all up
and you can create a list that well
reflects the entire voting population
without one particularly swamp another's
so I don't know if you're aware of the
artist jeffree star who by the way
released an album finally yeah so this
is the most famous artist on myspace who
had released apparently no albums and
the reason is because this artist's
myspace page comes as close as possible
to getting pulled for being a little bit
offensive it may define the very top and
it may have something to do with these
friends with a myspace couple of the
myspace founders but he's got an
extremely in your face website uh he or
she I'm not sure yeah your choice um and
so there are always many many many many
comments in this one modality about this
artist because lots of people go to this
page but none anywhere else so how do
you fairly reflect that there is a lot
of support but it's not broad support so
what we ended up doing was looking
at a number of different voting schemes
I didn't realize this but in nineteen
forty something this guy Kenneth arrow
wrote a PhD thesis which later got him a
Nobel Prize for those writing PhD thesis
this is now the bar to beat write your
PhD thesis earns you a Nobel Prize in
Economics he proved that it is
impossible to have a fair election
mathematically and everyone said hey
wait a second that can't be and he said
no no there are five things you might
want an election have like if more
people vote for person a then person B
person a should win like five criteria
like that but those five create a
logical impossibility of a fair election
so thousands of voting schemes were
proposed for how to combine votes and
the one we ended up picking was a
variant on the one that's used in the
island country of naru called neuro
voting where your first choice gets one
point your second choice gets a half
point your third choice gets a third of
a point and so forth and so on and in
fact we found that it works better if
you say your first choice gets one to
the p your second choice gets a half to
the p you know where you can tune p and
that says how important broad support is
vs vs high support on your one but it
gives you a nice way to take a large
number of like 10 sources vote the list
together create a list that no one is
happy with that everyone is equally
unhappy with which sort of defines a
fair combination of those sources but
it's a really hard problem and one that
we were very fortunate we actually have
a theory group that has people who do
nothing but voting theory or did nothing
but voting theory for a while for a real
fun time look up olympic figure skating
voting which may be the single most
complicated voting system on the face of
the planet no one really understands how
or why it works but they run the gamut
from very simple to that we tried pretty
much all of them excellent any other
questions
right so he's got a question about when
something becomes popular so actually I
will go back to that picture so what we
worked out with the the subject matter
experts who happen to be BBC DJs and let
me tell you they were the funnest group
of subject matter experts to work with
ever they're definitely a stitch and
they certainly know what they're talking
about um but basically it was a decay
turned out to be the right thing to do
we're in any time period of six hours
the first in that time period votes got
full rank the next time period those
votes got a fraction of full rank the
next time period and it just decayed
away right so part of that is if you're
crawling every five minutes and the vote
wasn't there last time you crawled then
you've got a pretty good idea myspace is
actually pretty good about telling you
when vote showed up bebo for whatever
reason seems to have a bunch of apple
fanatics so when someone puts a post
it'll say it was a little while ago
within the last day a few hours ago and
so we actually had a heuristic that said
okay it wasn't here when we look now it
is here now we know they said it's a few
hours old we're going to arbitrarily
assign it this date um but we did hard
time stamp all the votes with our best
guess unix time to do this sort of decay
model and part of it was being able to
look often another part of that and this
is a another piece on the the
optimization we developed a pretty good
model of how often someone would get a
post so for example we knew you two
would get between x and y posts every
six hours and we knew what that error
was and we knew that by looking we could
reduce that error to zero so now given
an infinitely large set of artists which
there are on myspace and I know that you
never
get posts or you get post once a week
then I don't have to check you every six
hours I only have to check you when the
expected error grows larger than any
other expected error and then you do a
greedy algorithm to maximally reduce
your overall artists set expected count
error in any given time period and that
seemed to work pretty well that logic is
in the scheduler for the bat outfit for
the actually all of the requests flowed
into a scheduler the scheduler put them
into the batch with priorities the
fetcher is just dumb it just grabs work
does work grabs work does work and so it
nicely decoupled and we could have a
dumb algorithm which was just throw it
back in right away or we could have an
algorithm that was a little more smart
that said okay looking at these which
ones should I be spending my time double
checking on and that could be slopped
swapped in and out by just changing a
single line in the in the what to put
into next file which worked really well
but yeah it's a hard it's a hard problem
and it was a lot of fun to work on any
other questions excellent well thank you
very very much what do you guys work on
if I may ask what do you guys work on
web search spam excellent excellent Oh
excellent so nice a nice selection cool
well thank you very much if you have any
questions feel free Thanks
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>