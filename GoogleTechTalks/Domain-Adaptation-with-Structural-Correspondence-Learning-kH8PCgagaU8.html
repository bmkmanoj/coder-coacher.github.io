<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Domain Adaptation with Structural Correspondence Learning | Coder Coacher - Coaching Coders</title><meta content="Domain Adaptation with Structural Correspondence Learning - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Domain Adaptation with Structural Correspondence Learning</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/kH8PCgagaU8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">so so before I start I want to sort of
credit people who have helped me along
so shy ben de vida is a professor at
Waterloo Kobe Cramer as a postdoc in our
group mark dreads II actually I think
intern with you guys last summer in New
York City Ryan McDonald this now was a
fellow grad student is now at Google in
New York City as well and Fernando is my
advisor so so the problem i'm going to
talk about is hopefully one that anyone
who's ever worked with statistical
models has encountered in some form or
another and that's that you train up a
model in the lab with the data you have
you test it out it looks pretty good and
then when you go to apply it in the real
world it's just terrible and so this can
happen in envision where you have cases
where you know you may have good face
data at train time but when you go to
apply a face recognizer you might have
occlusion or lighting differences that
you need to deal with for gene finding
now this is becoming exceptionally
important because you may have good well
curated annotated DNA sequences from one
organism but you want to train a model
that works well for a different organism
and probably sorry probably the most
familiar setting two people here is in
speech in speaker adaptation where you
have transcriptions from one person but
you actually want to use a speech
recognizer in a case where a person has
a very different accent or vocal tract
length and so in text on this problem is
particularly acute because there's
really a huge variation in vocabulary
and style so you can have your domains
like financial news and blogs and
scientific texts so I mean one way to
approach this is just to say okay well
we've got multiple domains let's just
train a model in each domain and what we
can that way handle all the data we ever
encounter well that works well but if
you think about it
training a model usually involves some
sort of annotation and in the case of
something like translation this means
you would have to go and get a
translator to go and translate blogs for
you it's not something that's
particularly cheap and furthermore it's
unclear what exactly meant by domain so
obviously within blogs there's a wide
variety of different kinds of language
that people use I mean even within
certain types you know you may have
blogs about cell phones versus about
software again you know where do you
draw the line here so let me let me sort
of dive into two specific cases that I'm
going to address today in this talk the
first is sentiment classification for
product review so in this setting you're
going to we're going to receive a review
it's just some text describing how a
person felt about a product he or she
bought and the idea is we're going to
want to pipe this through a statistical
classifier SVM naive Bayes or something
and get out a rating that's either this
this is positive or negative okay and so
I just want to pause here and say I
realized that there are lots of this has
actually become a pretty hot topic in
the literature and there are lots of
people now working on this I guess
they're even people at Google in New
York who are doing a project on
sentiment now so where's the where's the
domain adaptation happening so you we're
going to look at a setting where you
have a annotated product reviews from
one particular type of product let's say
books we're together with each review I
have someone went through and told me
this review is expressing positive
sentiment or this review is expressing
negative sentiment but what we're going
to want to do is go to a different
product let's say kitchen appliances and
apply a classifier that we learned on
books okay and for kitchen appliances
we're not going to have any label data
at all okay so let me let me give you
guys two examples from these two domains
just to illustrate the kinds of problems
we're going to try to overcome here so
the first is from books we pull both of
these off amazon so this is
running with scissors a memoir this book
was horrible I read half of it suffering
from a headache the entire time and
eventually I lit it on fire one less
copy in the world don't waste your money
I wish I had the time spent reading this
book back so I could use it for better
purposes this book wasted my life ok now
let's let's look at a kitchen appliance
review from the same site also from
amazon i love the way the tefl deep
fryer cooks however I'm returning my
second one due to a defective lid
closure the lid may close initially but
after a few uses it no longer stays
closed I will not be purchasing this one
again ok so if you look at the the
actual words that people are using to
express sentiment in these two cases
actually it's quite different right you
read half of a book that means that you
didn't really like it but you're not
going to read half of a tefl deep fryer
again you don't say things like oh this
book was defective it just didn't work
and I'm returning it right it's not
something that's not the way you are you
express negativity about books so in in
practice actually if you train up an SVM
on books and you actually test it out on
kitchen appliances there the error
doubles so this is a pretty serious
problem the other task that I'm going to
address is more traditional sort of
canonical NLP task this is part of
speech tagging so we're going to get
some training data from The Wall Street
Journal and let me read to you guys this
this sentence so this is a large corpus
of annotated financial news The Clash is
a sign of a new toughness and
divisiveness in Japan's once cozy
financial circles and the task again
here for part of speech tagging is to
take each word and annotate it with its
grammatical function so to say something
like once cozy is an adjective toughness
is a noun and so on and actually there
are people at Penn now who are
interested in building sort of NLP
pipelines for biomedical abstracts right
so they're there in the biology
department and they want to build NLP
tools for for biology texts but they
don't have any labeled text right so
they get sentences like the oncogenic
mutated forms of the RAS proteins are
constitutively active and
interfere with normal signal
transduction so again the vocabulary
really changes a lot here what I've
highlighted here are words that occur
five times or more frequently in one
domain than in another so in particular
you have words like oncogenic which
almost never occurs in the Wall Street
Journal no matter how much text from
that particular domain i'm going to show
you and the same kinds of problem occurs
right so if you train up a
state-of-the-art part-of-speech tagging
in the Wall Street Journal and test it
out on medline the error quadruples okay
so before I i go into structural
correspondence learning which is the
topic of the talk i want to sort of get
us all on the same page in terms of the
supervised models that i'm going to be
using so these are just linear models
for text and I I guess everyone here
especially at Google will be completely
familiar with them but just to sort of
get on the same page as far as notation
so the idea here is that we're going to
take a document and we're going to
represent this as a vector in a high
dimensional vector space where each
dimension of the vector is a particular
feature like a word or a bigram in the
case of sentiment and for a particular
instance the dimensions which have
positive nonzero values are those words
which actually occur in the document so
here you say oh well the word horrible
occurred three times in this book review
I give it a point three red half
occurred once I give it a point 1 and so
on okay we also have a weight vector and
in this weight vector each weight sort
of corresponds to the propensity of a
particular word to indicate positive or
negative sentiment so you have things
like horrible gets a negative one
because it's an indicator that a
document might be negative waste gets
negative 1 point 2 and so on and the way
we're going to classify is just take the
dot product of these two things right so
this will give us a score if we add up
all the features weighted by their
weights and it's and the score is
negative we end up saying okay this
document expresses negative sentiment if
we add them up and it's positive this
document express is positive Center yeah
so
the problem I want to focus on again is
so remember we had a feature like
defective right and we had a bunch of
training data from books on which we can
estimate the the entries in this weight
vector we've never seen the word
defective this particular feature so
what do we do well the best we could
possibly do is give it a zero weight
right we don't know we've never seen it
before we don't know what it means so we
just give it a zero weight but in
practice of course this isn't going to
help us when we get to kitchen
appliances okay okay so the other thing
I want to mention is that so I gave you
guys a simple binary classification
setting obviously this this is kind of
the the state of the art for even more
complex NLP structured tasks you know
and we'll use the same sorts of ideas
this vectorized representation in part
of speech tagging as well okay so
structural correspondence learning will
cut this error that I was showing you
guys by forty percent and the basic idea
is just to use unlabeled data from the
target domain and and the the reason we
call correspondence learning is that
we're going to induce correspondences
among features from the different
domains so if we think about our
sentiment example this are these are
things like well the by Graham read half
in books is sort of like the word
defective in kitchen appliances right
there roughly mean the same thing okay
and if we can find good correspondences
then the basic intuition is that the
labeled data for the source domain will
automatically give us a good classifier
for the target domain right so if we if
we do a good job at learning these
correspondences we we can learn a
representation that's already going to
help us do adaptation well ya soon
so we'll see it's it's basically
many-to-many and when when the algorithm
gets fleshed out we'll see exactly how
that works okay so actually as far as I
know were the first people to use this
kind of idea for text but for those of
you again who know speech this is a
fairly common problem set up that they
have their where they wanted you speak
or adaptation and there's a technique
called maximum likelihood linear
regression works works pretty well for
them the setup is almost identical but
the techniques are going to be quite
different okay so scl as i alluded is a
two-step learning process in the first
step we get a bunch of unlabeled data
from both the source and target domains
and the idea is to learn a common shared
representation that map's source
instances and target instances into the
same low dimensional vector space okay
and then we're going to simply take this
low dimensional representation and learn
features for that to do good
classification right so fine now
provides us with a bunch of new features
and we're just going to learn weights on
those features to do our classification
so we kind of alluded to this before but
just to think about what are the
properties of Phi that we're looking for
well one we need to make the domains
look as similar as possible but also we
need to allow ourselves to do
classification right we designed our
feature space to have good
discriminative power and we don't want
to lose that power in doing this mapping
right so in particular you can think of
fulfilling the first criteria and by
mapping all the points on 21 you know
low dimensional point and that's
obviously not going to help us okay so
what's the intuition for how we're going
to do this so if we go back to our
example on kitchen appliances right
there was this word defective and we
said that if we only knew that this was
a negative word we could do well here so
how could we figure out that it's a
negative word let's take our unlabeled
kitchen contexts again look up the word
defective in a
show of kitchen appliances reviews from
amazon and see where it occurs right so
you get things like do not buy the shark
portable steamer the trigger mechanism
is defective the very nice lady assured
me that I must have a defective set what
a disappointment maybe mine was
defective the directions were unclear so
the words I picked here in blue are
things that you know basically could be
methods for expressing negativity about
either books or kitchen appliances right
so let's look up now these words in the
book context right so for not by the
book is so repetitive that I found
myself yelling I will definitely not buy
another a disappointment ender was
talked about for some small number of
pages altogether so ender is a character
in this book that this guy really likes
and you didn't get enough face time I
guess it's unclear its repetitive and
boring right so again we want to somehow
use the co-occurrence with these blue
features to realize that defective is
like boring number of pages or
repetitive right when you go to books
okay so what are these blue features
well we're going to call them pivot
features and they have several
properties that i want to make explicit
here first they have to occur frequently
in both domains they need to be good at
characterizing the task we actually want
to do the discriminative task I mean
practice they're going to number in the
hundreds or thousands so I showed you
three but we're going to choose many
more and we need to choose them using
the data we have so what can we exploit
where we have some labeled source data
and we have some unlabeled source and
target data for picking these pivots so
let me give you guys two examples of how
to pick pivots and what kinds of
features come out so the first is what
I'm going to call scl and that's just to
choose words and by grams that occur
frequently in both domains in a second
i'm going to call scl mi which is like
SEL but it's based also on the mutual
information with the labels so okay so
in the first case you get words like one
about when probably not such great in
terms of pivots but in the second case
when you
also include the mutual information from
the labels you have in the source domain
you get ones that look much better right
highly recommended awful loved it these
pivots are things that you know if you
could model the co-occurrence as well
you would assume that might be able to
do good classification so how are we
going to actually do this well the
pivots the idea behind the pivots is
just to use them to align other features
so if we go back to our first example
with not by the the idea here is that
we're just going to cover up not by mask
it and use the pivot features to predict
the presence or absence of not by in
this particular example right so if you
we're going to construct a single binary
problem and instantiate that across all
the data and say you know does the
phrase not by occur here yes or no okay
and we're going to train n linear
predictors here one for each binary
problem and the the thing the thing to
notice here is that each linear
predictor we train is characterized by a
weight vector right and the one issue I
want to point out is that these what I'm
going to call pivot predictors are
implicitly aligning features from
different domains right how do they do
that well if we notice that defective
and repetitive both have positive wait
for not by this pivot predictor then we
know that in that case we can kind of
say we'll hypothesize that these might
be aligned right okay so
primary pretty sure does ah what's a
negative example um any instance which
doesn't have the phrase not buy-in it so
many yeah okay so um we have all these
weight vectors if we construct a matrix
where the columns themselves are the
weight vectors from these binary
prediction problems know that actually
doing the matrix vector multiplication
gives n new binary features right where
the value of the I feature is basically
just the propensity to see not by in the
same document right so it gives you a
document back and I get a bunch of new
features which say could not buy occur
in this context yes or no right here is
each column is the weight vector for a
predictor of a pivot right exactly then
so you're saying my feature is C naught
by I plus 1 will be some other pivot
some other predictors right not by but
is what was the other one I listed awful
right thing like this right okay so
we're almost done but we've we created
these thousand features let's say n is a
thousand right if we had a thousand
pivots that's still reasonably large
right well and the reason I say this is
that there's a lot of duplicate
information here right you have
predictors that are like horrible
terrible and awful all are good pivots
but they tend to mean the same thing
right what we'd like is to have a simple
basis that kind of characterizes this
space and that we can use basically is
just plug in to a standard linear model
okay and we're going to do we're going
to construct this by computing the SVD
and using the top-left singular vectors
which I'll call Phi here okay so for
those of you who sort of know kind of
history of dimensionality reduction
language there are these two very
probably most famous papers which are
latent semantic indexing in a bayesian
probabilistic variant of that latent
daresay allocation I want to stop here
and kind of try and characterize just at
a high level what the difference is so
first these there these dimensionality
reductions are done on the feature
document matrix and by in particular by
picking pivots we can actually
characterize the kinds of
representations that we learn right and
this actually is important right because
here if we get a good representation
that's great but if we don't then
there's no real recourse to
understanding how we want to design a
representation to do a particular
discriminative problem right so by
actually choosing the pivots
appropriately we can direct this prop
this dimensionality reduction to give us
good features which are useful
discriminative Lee okay so now back to
the second step which is how do we use
this in a linear predictor so we have
these two vectors right I showed you
guys the high dimensional vector now we
have the projection of this say we took
the top 50 singular vectors we have the
projection onto a 50 dimensional real
valued space and we so we want to use
this in our class of in a classifier
standard linear model the way we're
going to do this is very simple we just
have we had a weight vector before for X
now we have another weight vector for
Phi transpose X I mean we just add the
two together right at train time we're
going to learn W and V together and at
test time the idea is to first apply Phi
and then apply W and V and the hope is
that again the representation we learn
here Phi is good for domain adaptation
in that case we'll be able to classify
instances in the new domain using v okay
so before I go into the results I want
to stop in and mention to sort of direct
inspirations for
for my design of scl so the first is
alternating structural optimization
which is a semi-supervised technique by
endo and zhang and the the idea there is
again to use to use auxiliary predictors
on unlabeled data to train
discriminative models on unlabeled data
and use that to kind of characterize a
reasonable hypothesis space for doing
good discriminative learning and the
second is this if for those of you who
know dimensionality reduction this area
of Correspondence dimensionality
reduction so the idea there is that you
have some high dimensional
representations of a single low
dimensional manifold and you want to
learn a manifold that that respects
these high dimensional representations
these high dimensional correspondences
ok ok so let's go on to the experimental
results so first for sentiment
classification we again all our data is
from amazon so we did we just crawled
the Amazon site and we pulled down a
bunch of reviews these are books DVDs
kitchen appliances and electronics these
are our four domains we had 2,000
labeled reviews from ehm between three
and six thousand unlabeled and so we
treat this as a binary classification
problem each review is it has together
with it a set of stars we take things
that are four or more stars pour in five
stars and call them positive one in two
stars and call them negative you
downloaded the stars for those three to
six thousand but you don't show right
now rhythm right we some reason why in
the data set you don't believe those
labels no no there's no reason actually
we could use we could in fact use them
as label this is purely for experimental
purposes so I mean well that's not quite
true we cure we sort of curated and
tried to throw out duplicates and do a
good job at finding reasonable reviews
for our labeled data but but basically
you know they come from the same
ultimate source okay so their features
we use yoona grams and by
which is pretty standard for this task
the mm-hmm so the technique you're
exclaimed does it assume that the that
the domains are given that you're you're
going to tell it this text is in this
domain that text is in that domain or
are you able to to generalize across
text where the domain itself is only uh
so all the experiments i'm going to show
you are from the first case but we can
actually talk afterward about firstly
potentially discovering multiple domains
and also using the same sorts of ideas
here when you have no idea where the
domain site are segmented it's both of
those are things we've looked at but
they're not part of this talk okay so
for the pivots we're going to use scl
and SEL mi which i showed you guys
several slides back and at train time
we're just going to minimize a goober
eyes diversion of the hinge loss you can
use whatever your favorite loss is okay
so so okay so before I show you the
numerical results i want to show you
kind of a visualization of the kinds of
projections that could potentially come
out of this sort of learning procedure
so the first the first is so what I'm
showing you in the top left here are
words that only occur in the books
domain and are negative under this
projection right so this is a single
column of phi so plot if you talk about
the plot you don't like the book if you
say something is predictable that's not
a good thing for kitchen appliances you
know the plastic if the little plastic
handle breaks books typically aren't
poorly designed although they could be
the kitchen appliances if you don't like
it you say it's poorly designed leaking
books don't leak positive you have
fascinating engaging must-read grisham
people like John Grisham on amazon and
it for kitchen appliances espresso is
sort of like the John Grisham of kitchen
appliances people just like espresso and
you have other words like our perfect
using this was a breeze I've been using
it for years now all these are ways of
expressing positivity that are specific
to appliances right so the nice thing
about this other than just being cute is
that actually remember that we're going
to want to train a discriminative model
here and even if we've never seen all
these words on the bottom we can tell
that poorly designed for instance
expresses negative sentiment by virtue
of the way its projected relative to
plot predictable and number of pages and
we do have labeled data for these right
so we can actually tell this immediately
from our labeled data okay so here are
the first set of numerical results
they're kind of complicated so let me
parse them a bit first what I've labeled
up top each of these sections is one
domain that we're going to adapt to so
this is testing on books here it this
80.4 is the result we would get if we
took all 2000 labeled data points and
train the classifier on that in books so
this is sort of like the the upper bound
how well could you do if you had a good
books classifier right each of these
sets of three bars is training in one
particular other domain so again like
electronics and kitchen are not very
similar to books DVDs are more similar
so in general the bars are higher the
baseline is what happens if I just train
an SVM and test it the blue is what
happens if i use the SEL features and
the green or what happens if i use the
scl mi features so the thing this sort
of takeaway message here is that in
particular how to interpret these if you
look at this set the baseline loss due
to adaptation is seven point six percent
the scl mi loss is around point seven
percent so you can do almost as well as
having books using only unlabeled data
from the books domain and lots of
labelled DVD data
mm-hmm a great results I'm not trying to
be negative about it I just want to
understand the red line labeled yeah
point for doesn't get to see the pool of
unlabeled data that the green and blue
bars got to see right that's right so
potentially spoiling someone thought
that they were semi-supervised learning
genius they might claim that they could
push the red line up a little bit yeah
actually you can push it up right a bit
and I'll show this you can't typically
you can't push it up okay you can push
the red line up yeah I mean it depends
how much labeled versus unlabeled data
you have you know you're you're right
will so sort of adaptation versus
semi-supervised learning in the next set
of results but I don't have numbers for
this particular task so okay so one way
to sort of point this out is actually
you know on kitchen and electronics
right so these are really similar
kitchen appliances are almost all kind
of electronics right so like both of
them can be defective a lot of the words
are the same so here you do get this
kind of semi-supervised result where if
you add a lot of unlabeled data you can
actually do better than the sort of gold
standard here the red line okay but the
the thing I want to focus on briefly
here is is the screw up right so how
some so somehow we actually did worse
using scl then you doing you're not
using the unlabeled data at all right so
what happened is basically we somehow
managed to misalign features from the
two domains right so you get things like
well there are all so you're doing
you're learning a representation from
unlabeled data so there's a lot of
variants in the in book reviews some of
it is whether or not someone's positive
or negative but a lot of it is like well
this is a Christian literature book and
this is a fiction book and this is a
nonfiction book and this is a self-help
book right so a lot of things you see a
lot of these mistakes that you see are
basically projections that look
reasonable for kitchen appliances by on
books
are actually kind of taught doing
topical discrimination okay so we
thought about how you might one might go
about possibly fixing this and we said
well what could you do with a minimal
amount of label data if you were just a
guy who wanted to quickly prototype
something what could you do with 50
instances right so this is 50 vs 2000
again we're assuming that we're using
the same training procedure before so
here's what we're going to do we're
going to train on the source data save
the weight vector here V sub s for the
scl features the low dimensional
features now I'm target data we're going
to simply regularize a weight vector to
be close to the weight vector we had
from the source domain okay so if you
look at this as an optimization problem
the first term is just the hinge loss
that we have before the second term we
want to encourage not using the high
dimensional features as much as possible
right with only 50 instances it's
unlikely that we're going to get a feel
for the kind of vocabulary you see in
the new domain but on the other hand we
might be able to learn something
reasonable about the low dimensional
features okay so the idea here is that
we want to keep the scl weights as close
as possible to the source we believe
that it's mostly right but we want to
correct the few things that we did wrong
right and we're going to do that by
trading off basically this first term
from this last term so this technique
actually is based on an idea from
Shelburne acero Chavez now here at
Google I guess he's in kirkland I don't
really know but the they actually
proposed regularizing on the high
dimensional weight vector right so the
hour the place we differ from them is
that we're actually we advocate using
the low dimensional feature
we think that by having this low
dimensional representation you can get a
lot more power out of this small number
of labeled instances you have okay so
here are the results basically the
baseline is exactly the technique of
ciao buona sera the idea there again is
to regularize based on the high
dimensional features that you had and
our technique is this variant where you
only try to match the low dimensional
features to what you learn in the source
domain and in this case SEL mi always
improves over the baseline for every
pair of domains that we have ok so I
showed you guys a bunch of results here
and I want to kind of help distill them
so first even without any unlabeled data
we reduce error due to transfer by
thirty-six percent and the thing I want
to point out is that if you have just 50
instances and use this other technique
basically it doesn't work at all and
that's because 50 instances just isn't
enough to help you with such a high
dimensional weight vector hundreds of
thousands or millions of features right
but if you have a good low dimensional
representation you can further improve
to a forty six percent relative
reduction in error okay so the other
task that I want to talk about is part
of speech tagging and so for this task
the data that we're going to look at is
quite a bit larger so we have a million
labeled words of Wall Street Journal
text and we're going to add two or three
million words of unlabeled text from The
Wall Street Journal and from medline
okay and again the task is to train a
tiger in the Wall Street Journal and
test it on medline we're going to use as
our supervised learner what I'll call
Myra CRF basically the the idea here is
that you want to
separate the highest scoring the best
label from the top highest scoring
incorrect labels by a margin and there's
a good jml our paper that describes this
that I highly encourage everyone to read
and so the other thing that I want to
specify here is what we choose for
pivots right so I'm going to focus on
sort of the word by word representation
so if you look at a three-word window
what we're going to use for pivots are
common left middle and right words
across domains okay actually so here
there's not we have results for that but
i'm not going to show i'm not going to
show them here they do do it does
slightly better when you use mutual
information okay so all right so now the
same the same visualization of the
projection onto a single dimension from
phi so only in MEDLINE you get words
like receptors mutation assays and
lesions negative under this projection
only in wall street journal company
transaction officials only in MEDLINE
that are positive metastatic neuronal
transient and functional and verses
political short term pretty so what is
this projection doing for us while it's
separating nouns on the negative side
from adjectives and determiners on the
positive side so again the takeaway
message is that even if we haven't seen
any of these words here on top we can do
a good job at discriminative learning by
using all the by by using their
projection onto this line and the
similarity with these other words that
we do have lots of labeled data for The
Wall Street Journal words right okay so
so here are the set of results comparing
sort of semi-supervised with scl so the
black line here is just training Myra
tagger on the wall street journal the
blue line here is trained the
semi-supervised method of an
Zhang this alternating structural
optimization and the red curve here is
scl so the first thing I want to point
out is that if you don't have very much
labeled data at all in the wall street
journal so these are learning curve sore
a number of wall street journal
sentences so you don't have very much
data at all then you can get a nearly
twenty percent improvement reduction in
error for part or speech tagging but
even when you have a lot so MX post is
another baseline that I just wanted to
throw up there this is add weight Ratan
parkies part-of-speech tagging and it's
sort of the standard out of the box
tagger that one would use if you wanted
to work on this problem so I'm here
actually there's a you can still
significantly improve overall these
methods and one interesting thing is for
unknown words right where you actually
don't know the you've never seen this
word before and this is kind of what
we've designed for you can actually get
a more than twenty percent improvement
even with a million words of labelled
Wall Street Journal text again so there
there are other methods for
incorporating labeled data in this
setting and in particular for these kind
of what are called structured problems
where you have a label that's that's
more complex you can potentially use the
output of a tagger trained in the source
domain as a feature for the target
domain so this was advocated by florian
at all nakal a couple I guess three or
four years ago the idea here is that we
want to just so how are we going to
compare SEL with a normal supervised
tagger we just want to Train one of
these taggers in the source domain and
uses as a feature in the target domain
and see how much improvement we can get
okay so looking at that this thick black
line here is not using any target data
at all so we just ignore this data
that's why the curve doesn't go up we
just train a source tagger this dotted
line is what happens when you use a
small number of in domain training
instances
and here what I'm showing the blue line
is the supervised tagger and the red
line is sel right using this combining
it with the target data using the same
idea of florian at all so the again like
for so for this side of the graph you
can get a nearly forty percent relative
reduction in aerosols from something
like 86 291 by using scl and together
with the this trick of using features of
the source tagger but even for a large
amount notice here actually using the
source data doesn't help you at all
versus not using it but once you train
the source tiger with SEL even with a
fairly large number of medline training
sentences you can still get a
significant improvement okay so i want
to end on a somewhat speculative note
for various people in this audience that
maybe we can talk about afterward so
first is machine translation i know
there are a lot of people working on
this here the the basic scenario i'm
envisioning is where you have some
domain-specific parallel texts like news
or legal texts or UN transcripts what
have you and but what you want is to
actually do translation in a very
different domain like let's say blogs
and there you may have actually lots of
similar corpora right people write blogs
in Chinese people write blogs in English
but they don't often translate them and
you want to exploit all this unlabeled
data so I could envision exploiting it
in two ways and I'll talk about the
specific an talk about this business
specifics of that offline but basically
you can obviously adapt a language model
and people have worked on this but you
could also conceived of learning trends
new translation rules based on similar
context to these the source data that
you have okay the other obvious problem
for this audience is search ranking so I
can kind of envision a scenario where
you have a query and a list of top rank
documents already what you want to do
re rank them based on some features of
the documents and the query right so and
and you may have some labeled data
either in the form of editorial data or
in the form of click-through data and
the adaptation here is well you might
have very different markets so in
particularly may have lots of good
editorial data for English but in
Indonesia you have barely any right and
yet you still want to be able to exploit
this in a reasonable way and the the
pivots I'm envisioning here are just
common relevant features across the
different models it's you may have
features that are relevant in both
domains and a bunch of other features
that you don't really know about you
want to somehow align these features so
and finally for those of you who are
more into sort of learning theory we
have some work on that as well that I
would be happy to talk about so the idea
here is that you have a model that
you've trained on one particular
distribution and you're going to test it
on another can we prove learning bounds
bounds on the error of that model in a
new domain so we have a couple papers on
that okay thanks yep I was just curious
like the idea of this structural
correspond burning ISM basically trying
to pick up the period features and in
Lancaster the features that you don't
see in the training data into those
feature space and into a low dimensional
reduction and then train a model of that
right um
do we still like user in the learning
times you have sort of two set of
parameters one for regional things one
for those you know Cuba two related
features do you still need the first
part yeah so that's that's a good
question I'm actually you do need it in
the sense that it improves results so
the reason for that is that there are
some things which you know doing
unsupervised learning you just aren't
going to model well and you need to kind
of pick up the slack there so we have
done experiments using just the just the
low dimensional representation and
basically that some kind of does in
between the two so wherever the results
i showed you are if you use just the low
dimensional representation it's usually
a little better than this then using the
high dimensional but not as good as
using both mm-hmm thinking about the
domain of book reviews on wondering
about significance of two sources of
noise and sentiment that i can imagine
one is ironing where somebody uses
ironing review the sense of their
sentiments might be flipped from from
what it appears from the word analysis
right the other one is that books
themselves are often criticisms or
praising various topics so discussing a
book independent of your sentiment of
the book you might be using criticism
and praise words and discussing content
mm-hmm yes so um so first these are
problems for sort of sentiment analysis
in general independent of the particular
method that you use but I absolutely
agree with you these do come up in our
experience it's quite rare but you have
to understand that someone going to
Amazon and and you know writing I lit
the book on fire is not the New York
Times Review of Books right there
they're not going to express sentiment
in quite the same way but but you're
right that it is a potential problem and
I think this is one reason why you don't
see num
as high as you would expect for binary
text classification right so after 2,000
instances if you you know look at I
don't know some other text
classification problem of Reuters task
right you're almost always well above 90
right here you're not going to see that
because there is quite a bit of noise
and that you're right i mean that's
something you have to deal with hmm sim
so in the feature selection that seems
like the selecting the pivots is very
important yes so when you do the mutual
information is it that you have to have
in your own domain hi mutual information
with the class or do you also have to
have high mutual information definite my
question yes I understand you're so you
all sit so I didn't say that but you're
absolutely right the assumption is that
mutual information with the label is the
same across domains so you can violate
this a little bit but in cases where you
violated a lot you're basically screwed
so you can envision instances where
there's some adversary that basically
takes the best linear predictor in your
source domain and flips all the weights
right then there's just nothing you can
do from that and unlabeled data right in
practice I think that it well it seems
like in all the problems I've looked at
both these and and a few others you
don't see this kind of flipping a lie so
the the mutual information assumption
seems reasonable common with
co-occurring both domains and then you
filter by mutual information again in
both domains well no no we don't have
mutual information in the target right
so we just assume that by filtering in
the source and using the commonality you
basic you also got features that are hot
that are highly informative in the
target yes the couch you use the hungry
pace and I would prevents you from
so um nothing basically there is such a
lot I mean so one is we're limited by
the amount of data we can actually get
access to obviously you guys have much
more than we do on our computers but but
there is a larger version of The Wall
Street Journal which has 30 million
words in which we are using for a
separate entity recognition task and it
works well for that there's no in terms
of scaling there's no reason why this
method can't scale up in fact all
training the predictors are of course
it's completely paralyzed about so um
you could do that fairly easily yes also
a hollow this would apply to text that
do not work all of our speech Tigers
which I feel like a blog comments you
know very short maybe not grammatically
correct ones yeah so again the basic
assumption the underlying assumption
here is that there is a sort of single
good model for all these different
domains right so it doesn't have to be a
model that you can train in one but the
idea is that if I had a lot of uh no no
blog comments to train on then am I had
a lot of Wall Street Journal text to
train on then I could train a single
good tiger that was good at both right
if I can't if for some reason you know
when I say the word dog in a blog it's
an adjective or a verb right i mean
there's slang usages that probably would
be particularly difficult to get that
that would be problematic but i think
you know i would love to try this out on
more widely varying domains and actually
the the mid line is pretty different I
mean it's there really some things where
I look at it and I look at it myself and
say I have no idea how to tag this I
have no idea what these people are
talking about it so
yes I guess both here okay did you try
of training in one domain then you'll be
using the classifier or the other domain
taking the ones from which it expresses
strong reference and then rich raining
on the SS so yes we did this is so this
approach is often called self training
and it can work very well sometimes it
can break horribly sometimes so I think
this is one problem with the these sort
of bootstrapping approaches to
semi-supervised learning order using
unlabeled data it's that you can
potentially introduce a lot of noise and
ruin the model you had before so in our
experience there are some tasks for
which it works really well and some for
which it actually works quite a bit
worse than a model that you know used
only labeled data there is a paper at
this year's ACL by tons young guy and
one of his students that basically study
a bunch of these other approaches and
they look at self training as well so
can sort of go for go look at that for
other references to self training yeah
and this adaptation we thought of as
learning a translation like trying cable
from the Wayne 12 doing two for the
important words I I I mean at a high
level it's possible i don't i don't want
to commit to that in part because i'm
not an empty guy and I don't really know
but in part because I think like one of
the I mean part of the power of this
method is that a lot of these features
are bad right a lot of the projections
by are actually noisy right but by
having a good discriminative model to
train on you can overcome that right
so it's sort of like you know if you had
a hard alignment it's almost like hat
you know doing hard clustering right
there's you sort of you throw out a lot
of information you had and you could
potentially have used that information
to recover a good discriminative model
right so I mean I do think that having
actually this sort of soft projection
onto a low-dimensional real-valued space
is very important pen ok sorry I was
wondering so for the selection of future
you have to factor basically you have
one is it a good predictor for the final
task right and you have one of them
which is kind of figure Iser does it
appear frequently which would mean that
it might be also good predictor for the
second task oh that's right yes so I was
wondering if to measure the like is it
is a German good predictor you each
other linear test fire you have actually
the doodle innovative than to say the
effect of thing wing the word in the way
I said so could you use that yes you
have so so there are lots of ways of
doing feature selection and mutual
information is by no means the best one
I don't I don't actually want to commit
to saying you know I love mutual
information for future selection
actually i would i would be interested
in looking at at either that criterion
or I mean obviously l1 regularization is
a good good criterion for doing feature
selection any of these I think have the
potential to be even better by
accounting for common if you know
dependencies among the features right no
I haven't tried that I haven't tried
that for example do you have any vessels
or progress to control the size from
okay so the actual identity of the pivot
features is very important on the Val
the number of pivot features is actually
not so important so they're basically
two criteria that you need to kind of
trade-off one is that you need enough
pivot features to basically characterize
the kinds of space you're in in terms of
training these predictors um the other
is that each individual predictor you
need to get good statistics for it on
the unlabeled data right so if a
particular / if you say to go to include
all the features and you have maybe some
features which only occur once or twice
even in a huge amount of unlabeled data
but you can envision I don't know ten
grams or something right in that case
it's hard to actually train a good
predictor um that generalizes well and
so this is why we choose sort of the
only the frequent ones like you change
the duration of using because you don't
like one that the error is getting the
same on each ways the errors weighted
the same but actually this is this is
exceptionally important because you can
get a much higher margin if you have you
know fewer instances to deal with right
so if you think about the kinds of loss
you know I mean higher margin basically
corresponds to a smaller weight vector
right so if you can train something and
the magnitude of the weight vector
actually does appear in the SVD
you typically don't do worse and the
reason for this is that for a couple
reasons one is you have all the features
from the original domain sometimes you
can do better if the domains are similar
enough you you have all the features
from the original domain to sort of if
you happen to learn a representation
that's not so good you can recover that
because you have the original feature
space right the other the other reason
though is that you know if these these
feet these domains are so high
dimensional right the feature space is
so high dimensional that actually for
text there usually is one good model
right there typically is one you know
reasonable model for sentiment of course
there are these words that you know
irony or words that switch polarity and
domains you know something like
predictable right it's a very good is
something bad for books but actually
your kitchen appliances you want them to
be predictable right so but there aren't
that many of those kinds of words so you
tip typically you can do as well or as
well as or better in the original domain
no I mean they're sort of their
heuristics you can use but none of them
are really good you know you can sort of
say oh well predictable in kitchen
appliances co-occurs with a lot of words
that seem to have positive sentiment but
in books negative but there's nothing I
haven't found one good heuristic that
seems to work well all the time like do
you need to apply the SVD before you try
to train that a component model but what
if you didn't do it with it because you
can you can basically put them on yeah
recognization on top of it yes so you
can it doesn't so empirically it doesn't
work as well um and I don't have any
really good theoretical justification of
that yet I actually hypothesized that
the reason the SVD is important is that
you actually don't really care about
predicting the pivots themselves right
and in fact that can cause you to kind
of over fit to the pivots what you want
really is to be able to sort of predict
kind of somewhat nebulous general
positive or negative sentiment right so
there's an there really is a
low-dimensional underlying
representation that you want that's why
I think the SVD is important how to
characterize that formally is like
beautiful sort of performance difference
where we can term 746 reduction Oh
instead of 46 I actually don't know sort
of the across-the-board but it tends to
do about again about half as well so you
still get an improvement but it's you
know it's not as big you know like it's
it's right in between the two I didn't
do sort of the extensive averaging or
cross validation that I did for the
these actual numbers that i'm showing
here so
ok</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>