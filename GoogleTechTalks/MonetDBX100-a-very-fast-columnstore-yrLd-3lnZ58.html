<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>MonetDB/X100: a (very) fast column-store | Coder Coacher - Coaching Coders</title><meta content="MonetDB/X100: a (very) fast column-store - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>MonetDB/X100: a (very) fast column-store</b></h2><h5 class="post__date">2008-01-23</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/yrLd-3lnZ58" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">my name is yonas Carson and I'm very
happy to host
Peter bonds here who is a old colleague
and friend from Amsterdam at CWI and we
spend a lot of time discussing databases
how to store data and so on and one of
interesting things in in monday is that
stores data column wise instead of
horizontally partitioned and I'm gonna
give Peter here a proper introduction
with his own brag sheet the balance is
specialized in engineering database
systems as designed and implemented
multiple database kernels both
relational and XML lately and inclusive
query processing transaction management
his unique technical expertise is in
sight in the interaction between high
performance data management and computer
architecture so it's really low level
stuff and he gets it working and his
goal according to our discussion at
lunch was to to make a cpu full and get
the most out of the i/o he obtained his
PhD at a university of amsterdam on
columnstore database architecture called
Monette TB and he spent some years using
this system for a real-time marketing
analyzes applications in a data mining
startup which which he was one of the
founders of this company has since then
been acquired by his BSS and peter da is
a research scientist at CWI a lot
further ado here's Peter okay thank you
your loss it works okay so
this afternoon I will be describing 1
l-nb x-hundred a very fast columnstore
this is a overview of a project that has
been running for more than three years
now and one of the other principal
persons involved in that project is in
the room it's a marching is my PhD
student in Amsterdam so I'm actually
doing his job now giving an overview of
of this of this project which was how I
will cover basically everything we have
done and I hope it will fit in 15
minutes so this is my plan I will first
talk a bit about the background of the
project so like I said I did my PhD
before and on one leb which is this calm
store and address this new thing called
x100 and I will talk about motivation
how we went from the one to the other
the principles idea about about in the X
hundred project is to do query
processing in a vectorized way and I
will explain what that means and i will
give some examples we start out by
focusing like you're not set on how to
do computations and relational query
really efficiently making the CPU fully
fully occupied and getting most out of
it on problems like data warehousing t
PCH is for some important benchmark but
soon found out that we were we had been
so successful in creating cpu efficient
database engine that it very hard hard
as we could afford it was totally i/o
bound so so then we shifted focus a bit
to improving io throughput to the system
and two techniques that will present
lightweight compression and cooperative
scans that are aimed at this
that's mainly it for the talk I want
them wrap up so well database I like
database systems I'm a database
researcher database bring you very much
sometimes nicer features like extraction
of the problem you're articulating and
how it's physically stored and how your
queries are eventually the strategy
that's being used eventually to solve
your queries and then you can create
optimizer that helps or do justice
automatically in hopefully nice way so
there are number of goodies from
database systems however we also know
that in certain application areas people
tend to hard-code their applications
anyway they choose not to use the
database systems and one of the reasons
they sometimes do that is because they
they feel that database systems are slow
one one particular field where this
happens is our information retrieval so
if you look at IR systems they are
typically hard-coded management of
infertile East data structures so and
people have tried to use database
systems but then they found out that
they were slow so actually I at the core
of database let's say well what database
shall use data warehousing ghost
appreciates benchmark which moves a
large commercial data warehouse I just
ran one query the first query which is a
simple query but it does a lot of the
data manipulation it selects
ninety-eight percent of the largest
table which can be hundreds of gigabytes
or even terabytes large depending how
large-scale data warehouse and it
irrigates compute all kinds of arrogance
I Renison 22 database systems my sequel
and that the database that I cannot name
because of the restrictions in the
license and on the one gigabyte I'd said
it takes about twenty six twenty eight
cycles to run that query and I just for
the fun of it I hard-code at this
particular query in a program in the sea
program and I would say I was a bit
shocked when I discovered that the
difference was
I mean I'm I was expecting a difference
but that it was two orders of magnitude
of performance yeah that's kind of a
significant so so yeah this research is
not maybe about finding new algorithms
that change the complexity of our
problems but it's about wondering what
is this big constant of a factor 100
there and can we do something about it
and if you can get one one order of
magnitude out of there that would
already be exercised okay actually the
sort of moon ellaby work in in general
has been ever been targeted at
efficiency and but people usually
perceive it like also yonas announced me
as well as a column storm so well why do
people do call my store as well in
certain situations where you have tables
with many columns and you only really
are your application is using a few of
them then you you can save a lot of IL
volume by a by Ewing curriculum by
storage that's obviously true but it was
not the motivation for using columns for
him already be actually it was different
so this is a of course it's it remains
true but the main reason was well we had
this idea of having a so we had the goal
of removing the interpreter from a
database architecture that was the
experiment in one enemy so one of the
reasons why maybe databases are a factor
of hundred slower than a hard-coded
program is that they use an interpreter
to interpret a query
and so one way to get rid of it is to
what I call let's say the risk approach
to database our algebra has to have an
algebra that is so simple that you can
actually do without any interpreter
meaning it's so simple there is no
degree of freedom every operation is
hard-coded and to make operations
hard-coded you actually have to remove
all those degrees of freedom one of them
being a record layout so so if you have
a model value say my operation only
accept arrays simple arrays they don't
need to know they don't need to see
those eraser tool passing into it and
they don't it interprets which which
which column from that table to fetch
etc so I'll give an example how are
things work simonetti be so you see in
detail columnstore saturated table with
an ID and name and age this this these
three columns are related by position so
that we call this a virtual audio voice
also indicating at this scholar my
country distorted grey one that realizin
so they're basically three errors there
physically okay the queries i wanna i
want to do some computational for people
with h large than 30 so in minetta be it
it works like this so you have this very
simple algebra that there's one column
at a time so here is the select
operation that selects all values in a
column bigger than 30 and then there is
the minus between square brackets which
just processes one entire column and it
produce a new one with a subtraction and
this and the yeah so there is no we
don't actually need to tell which column
to operate on and what is the expression
because the expressions are very simple
i do want one thing so that's what i
meant with a hard code that's very
simple operating you can see that there
is no there is no interpreter involved
in fact if you just look at the code of
the Select it's something like this okay
so it is a loop over a table
up array and it's just yeah does the job
in the simplest way possible and the
symbols by possible is nice because CPUs
like that they can really there are no
obstructions there are no api's that are
to be transferred traversed there are
the dependencies are clearly visible
when there are a few of them between
data and control and compilers they can
do very well with these loops over array
so that's what I actually optimized for
so in efficiency this this this algebra
can achieve things that interpreter
would not normally achieve however there
is a disadvantage of this approach which
is metallization so the red thing is our
result but in between we had to
materialize intermediates and but that
is the thing that one let it be does so
this is the final result so you get a
three column table out again vertically
fragment that of everything is always
practically fragment anything okay so
that's what's how it works and it can be
quite efficient but it's kind of a pact
with the devil so
I want high efficiency and I got it but
the devil took scalability for me
because the price I paid first
materialization so and this is still
true formula d whenever he said it has
this limitation however it did work I
mean in this particular query I'm going
to be scores 3.7 seconds and it well
ahead of the other relational
implementation is definitely more
efficient so we're going to be is still
out there it was my PhD topic it's now
open source you can download it under
the video CV I don't know its peak
sequel and xquery okay but this was kind
of the prelude so the question the
question that I had is well can we can
we actually do without this devilish
packed can we get high performance
without materialization okay let's first
look at how how our relational engine is
typically organized so you busy again a
query that looks a lot like the query we
had before now XP and 25 so we see here
three relational operators are those
operators are direct implementations of
the relational algebra we have scan that
scans a table from from disk supposedly
a selection operator and a projection
operator each operator consumes tuples
and it outputs to lose the it uses is
usually an iterator interface some open
next clothes each time you call next
stop down driven it tries to produce the
next to go by calling next on it's on
the children of the health or brighter
tree okay inside those operators
typically you have expressions and
expressions like big a bigger than 25 or
x 0.9 teen they they map down in the end
to things we call primitives primitive
methods that manipulate individual data
values so they provide the computational
functionality of the interpreter
for database engine so this dish you
find in any implementation so I'm going
to be X hundred so in the X hundred
system that we started you see here like
vertical gray areas which are not single
values with vectors so actually the main
idea here is what we call a blocked I
traitor approach so you wouldn't so this
is an idolater approach where you do not
pass to Porcia rounds with your pants
vectors around a very simple idea but
it's highly effective as we will see
later so it means that all the lower
level API IP is of the database engine
are expressed not in terms of single
values with in terms of vectors or
values okay so a vector contains the
notable on data for for multiple for
multiple tuples and all primitives that
you execute like multiplication or
comparisons they are so-called
factorized so they operate on an array
rather than a single value ok so again
here we use single arrays and you can
say well it's there like vertical slices
a vertical fragmentation of your tuples
it's good to understand that this
particular representation is not chosen
because we for data layout it may happen
for some situations to be a good data
layout but the main reason why we why we
do this is to make the primitives
oblivious of a record layout but just
telling them about the errors that are
actually going to process okay as
society I mean this vertical out
sometimes is nice for instance it is
very nice for simdi if you have a
primitive that multiplies in array
nowadays a good compiler will you seen
the instructions and to actually write
your Hill primitives x factor for
probably
so okay so again we have the same
pattern of of course that is a nice
photo for a compiler there are also
special factors which we call selection
factors are the representation may also
be a selection of vectors so we're not
all tuples in the area were not all
entries in the area actually participate
anymore the selection factors are used
to avoid copying so basically the model
is arrays with data in there and a
selection factor that in identifies
which data is still relevant and this
way the data flows through the pipeline
so in the extended system we we kind of
break this pact with the devil we we
have still are factorize primitives that
look very much like the algebra that was
in one enemy but now it's embedded in a
pipelined query engine that does not
necessarily have to materialize
intermediate results they can be
streamed through the pipeline just like
any other database system does okay and
this this simple ID is highly effective
it's actually strength and nobody or not
this is not done more I would say if it
gets gets guesses without any specific
efforts very near to the the efficiency
achieved by my hand code implementation
of this query okay so this was done
after a year of work and but I will but
that's not the end of the story okay so
something we worry about this making
good use of computer architecture so
currently we have you have to take into
account that erase memory hierarchy
there is not only disc and rum there is
also CPU cache that you may worry about
in your excess patterns and if you map
that on the X hundreds our stature as it
should be then you can think of the
factors as being the red stuff which is
in the CPU cache okay there is a buffer
manager what's traditionally the buffer
manager is actually a real layer lower
than the layer that you can actually
process data so it's it's actually in a
position where we would in the past have
considered this to be it is a real feat
to get data from your memory to your CPU
cache that is the blue stuff and far far
down we have a green disk storage and
melilla be examined is is a system that
does assume that it's operates on
problems that are significantly larger
than memories or displaced problems so
this is also important ok so the vectors
this vertical stuff is that is the ink
cash in the CPU cache representation but
we may choose to have our data in
totally different layouts in memory and
on disk another thing is that all
factors together should be tuned to fit
the CPU cache okay sorry if you make
those factors too large they've almost
fit anymore you will start to have cache
misses because of that the query
optimizer can look at the clearance that
are in execution and actually figure out
how many queries how many factors
queries are using and toonder the size
of the factors can here is what's on the
x-axis it's again the same query of the
data warehousing crazy where we do this
experiment on the horizontal axis we
vary the size of the factors in this
particular data size which is not large
it's the one gigabyte t PCH this table
has six million tuples so on the right
hand side we use vectors that are the
entire database column basically like
Mandela be would do column at a time
and on the left-hand side we are doing
what relational systems are usually
doing tuple at a time one value at the
time okay and I also plotted this other
results the the smash equal and the
handcart program you see them in there
so what's happening here is that as the
vector site is increased from 12 let's
say a couple of hundred sorry from one
to a couple of hundred the one effect is
that if you are if you are doing
processing by the hundred suppose the
number of function calls your
interpreter is doing is introduced
reduced by a factor hundred okay so way
less i try to next course and way less
primitive function calls so weightless
interpretation overhead and it's really
amazing how far this goes down it goes
down to some some some optimum however
if the effectors are increased further
performance were since again and this is
caused by the fact that those factors
are too big to hold in the kitchen so
you'll get additional cash basis because
of that so there is a clear optimum of
how how much data you should process at
the same time actually here you see a
very detailed experiment where we have
three primitives to multiplication and
addition where we assume that some
parameters are run resident the memory
some are already resident in the CPU
cache and you can actually see in the
shape of the of the curve but you see
the so the lowest the minimum is where
data is in a cache ok and there were in
the green line is where it's the
operation to add that has both its
operands in in cash there you can
actually see the l1 performance at 1k to
post in this case if it becomes too big
it's out of one
and then you are still in l2 and
otherwise should go go up okay and
finally I talked about the selection
factors even our query result at let's
say column at a time is equal what you
would get with malena be so it's
confirmed but there is a there is a
difference that is because of the
selection factors so the selection
factors allow you to prevent copying
data after you select from it so you see
actually the difference totally
explained here so it's it's amazing that
the performance of the external
prototype is almost the same as my
sequel with with one to play that time
and it's exactly the same as minetta
being with column at the time okay and
translate it into my previous graph one
leb wanted to be used as column at a
time so that means that it's its columns
are always rumors and they're never
catch trash again so you never get this
optimal performance
okay so this this works like for simple
calculations it's a really simple ID and
then I'm talking about but it's it is
not that easy to build an entire
database system on the principle of
vectorization you have to do some work
for it to our number of benefits so one
of them I already talked about its it
reduced interpretation of read less
function calls mainly but sometimes
having an intermediate unit of
processing data also gives you some
opportunities for instance if you think
about so some of your of the activities
that you normally would do for every
value you can maybe do only for every
hundred values and this this facility
allows you to put some of your work it's
the kind of strength reduction you're
pushing things out of a loop to do an
inner loop so for instance checking
whether you have enough buffer space if
there's outflow if you check if there is
enough four hundred tuples you can do
the check only ones in every vector and
there are tricks that you can play with
that if you do a merge join you can
check you get a vector of tuples you
check immediately the last one okay if
that one is bigger than your current to
put you can skip the entire vector
without touching any of it and in many
algorithms that are tricks you might
play if you have multiple multiple
values to process okay so basically what
we had to do is we have to come to all
the relational operators I figure out
ways to encode them in in in a
vectorized way so you can really do
really weird things so for each of
selection
yeah a selection so here you see this
selection code again very simple but
what it does it does anything else and
on modern CPUs you have to think about
or there are these effects of branch
prediction so so the CPU tries to
execute ahead it is so in this pipeline
models you operations are decomposed the
multiple stages and they are executed in
in pipeline fashion and even if the
nails is found then opera the CPU tries
to guess which is the next instruction
it puts it already in the pipeline but
if it guesses wrong you get a Miss
predict and if Miss product happens the
pipeline has to be flushed results have
to be thrown away in your culture and
slower and you can actually see this
happening so if you have this very
simple selection just the comparison and
you vary the number of tuples qualified
the selection at the worst case is 50%
because then it's very hard to for a CPU
tube to guess what's what's going to
happen so you see this curve this round
curve of how fast is loop runs and at
fifty percent it runs worse if it's if
it's very low or either fight very
highly selective than the neutrons best
and so there are tricks to you can do
you can for instance sorry if then else
is the obvious way you do it but there's
you can also do predication where you
transform the if-then-else in in a data
dependency so you that would piss
predication you you ink you always let's
say happens to port of the results but
you aren't you increment your pointer
with boolean with a 0 or a 1 okay and
then then you get constant a constant
line for that which is better in the
fifty percent case because there is no
misprediction
and you can even do weird tricks like I
mean for the very last activities you
can if you know that there is very few
to post qualifying you can test multiple
at the same time and if only one if /
for and bum and this and this and this
infrastructure allows you actually to
toy with that and you could dynamically
choose a primitive the primitive that
works best or that works best given the
current selectivity that the operator is
observing okay so you can squeeze some
performance with that another thing that
we have to do often is hashing so so so
what you have to do now is factorize
hashing means that you get a hundred or
a thousand values and you have to look
them up in an edge table but you're not
going to do it one by one but you're
going to push the thousand candidates
through the whole process of of hashing
so going through the book at hook a
chain list or step by step or at the
same time all together so this is
something my totally incomprehensible
drawing it more or less tries to explain
how you do that so all these factors
they move around you compute for all
vectors as value you do you compute a
slot number then you do a vectorized
comparison to check whether there is a
hit and you select out those that are
hitting and those that are not hitting
you're doing next on all of them and
then you're pumping them back into the
loop so that's factorize dashing and
that's what what this system does okay
so all these operators have some
factorized equivalent okay there are
details to be found in papers here there
is some work in IBM that in aggregation
and production also does some something
they called block oriented processing
which is related ok so this was the core
engine was actually completed or we had
a pretty good idea how it would work
after not too long time maybe a year or
two years but then we figured out we had
a problem so we had a really powerful
prototype
data a query engine that could process
huge amounts of data per second so it
definitely worked in in-memory
situations if you have such a powerful
technology you would like to use Don
large data which is not memory resident
and then then if you look at how how
fast this thing moves through the
through data in the case of my example
query it moves it around two gigabyte
per second nine hundred megabytes per
second and that was at that time at
least for us definitely out of scope for
the i/o systems that we have so the
system was very fast but totally I
outbound if you would do it on a on a
large dead set on one gigabyte so
feeling the Beast this thing goes very
fast there are 2 r.d is a type of
presents one is this lightweight
compression and the other is trying to
share these scans if you have concurrent
queries so first compression so the idea
of compression is that if you store data
compression on disk and you have a very
fast d compression algorithm that is
faster than bandwidth that your dish
provides and I'm talking about the rate
is here even then then you can win by
loading the data in compressed form and
decompress it in memory so in our
particular example of TP xquery one we
are looking at nine hundred two
megabytes per second but so we would
have to have a decompression algorithm
that runs at night in the mega bytes per
second but not even that because we
wouldn't want to use all our CPU only
for decompression so let's say that we
would second one sacrifice one third of
our CPU effort in decompression to have
two-thirds over to do the real work that
would mean a decompression algorithm
that runs at 2.7 gigabytes per second on
that Hardware current hardware that's
not possible with even the up the
performance optimized versions of normal
compression libraries they run with
maximum in in the 300 megabytes if you
use l0 so we invented some new
lightweight compression schemes to cater
for that so the key ingredients for this
new ultra lightweight compression
algorithms are column-wise compression
if you think about column stores and
compression there or calmer
representation and in compression are
natural partners because in it if you
store your data column by column you get
data from the same distribution near
each other and that helps
compressibility if you have a record
that contains vital varying
distributions it's hard to compress
please more effort secondly we would
like to use the factorized approach to
get high performance decompression
routines so not one at the time
compression surely and also look very
good at the properties of the algorithms
so if then else branches they are to be
avoided for instance because of this
branch misprediction last effects that
you get so first
let's look at what kind of compression
algorithms they are so currently we have
something called p for best frame of
reference actually frame of reference
was proposed by a goldstein at all some
time ago already very simple ID you just
store for this block in their case a
minimum value of base and your annual
and you just provides the deltas from
that base as your value so this works
for full numerical columns so be bit
value very simple and coding scheme
normal frame of reference doesn't allow
any things that are outside the region
bass and bass plus 2 to the power of B
which pets for a frame of reference and
does so so the idea is that you exploit
the frequency the frequent value should
should be inside this encoded region and
that can also be not encoded with values
in this compression scheme they're
called exceptions okay people Delta is
kind of the same but on the Deltans
between successive values so this is in
if you Inc in infrared at least you call
this gap encoding the gaps between the
various postings you find in many
databases you find ordered sequences or
class he ordered sequences where this
works pretty well so
so the dictionary encoding is it's
purely frequency based so you have a
dictionary of two to be most frequent
saw values and you've taught that
dictionary somewhere and you omit the
code that represents a value in the
dictionary in pest frame of reference
you can have exceptions again ok so now
have three schemes for the p4 before the
other MP dicked they all use the same
kind of storage so there is a forward
growing in your disk block there is a
forward growing area of of code words
these are the B bit code words the fixed
bed length code words and at the end of
the block growing backwards is the
section of exception so those values
that could not be encoded ok so now the
decompression so normally I mean there
are other column stores or database
systems that use compression what they
usually do is they decompress as soon as
the blockage being read from from disk
so this makes it easy to lie to process
the data because you don't have to yeah
the data is already ready for use let's
say in your peanut-butter manager
problem is that if you do that you're
effectively crossing the boundaries or
when you read when the when you're this
block is red you're going to decompress
it so actually to decompress it the cpu
is needs that data so after all the
whole block is moved to the cpu to the
CPU cache it processes it and then it
stores it in turn it remains there in
the furniture in your main memory until
they are some query is going to use it
and then effectively the cash had Minter
first three times first you get all the
data up then to store it in the
permanent Cheng to use it finally so so
in this system we we don't do that like
that way we keep our blocks compressed
in the buffer manager and only when we
use them factor at a time so
oops time we decompress so this is an
effectively so we are compressing
between the boundaries of CPU cache and
memory not between the boundary of
memory and disk and so you have to cross
the boundary less often and you can
store more data in your in your memory
which is also good so we did some
experiments later were in information
retrieval benchmark the terabytes track
where this was exploited so I mean this
system could score very well because the
entire index would be memory resident so
there's a terabyte of raw data with the
compression in the end there's nine
gigabytes in this case of reverted lists
and that was a storable 144 workstations
thanks to the data compression and you
see actually that this database system
is able to out place was able to outpace
a number of oh actually all because the
it did 3.2 queries per millisecond all
of those hard coded infrared list
systems that the information retrieval
people participate with anyway so this
is a value of false of having a
compression just for getting more stuff
in Rob so these algorithms they they are
they are coded to to squeeze two squeeze
the last cycle and so they yeah we are
be iming for this for this high
bandwidth that the to achieve at least
two gigabytes per second Lee to to be
able to decompress data without really
suffering the consequences or
performance penalties for that from even
a high-end io infrastructure so if you
think about it if you want to decompress
two gigabytes of data it means you have
only a few CPU cycles / / value to
decompress it so it's really performance
critical
a naive way so i talked about
compression schemes and they all had in
common that they that they have either
village that are encoded in a small bit
representation or there are exceptions
and a very simple way to decompress that
is to just have some kind of exception
code okay so if the if there is an
exception code you take an exception and
if there is no exception code you decode
the value so this is this very this loop
but it's an if-then-else loop so what
happens if you look at the performance
of that loop it's deteriorates quickly
again because of this branch
misprediction so as fifty percent
exceptions so you there is a girl figure
that shows what happens if there are
more exceptions and with more exceptions
it quickly quickly deteriorates because
of these melons predictions even is
Ethan else I actually use cpu counters
and you see that the number of
mispredictions can go through the roof
and fifty percent so there is an
alternative way that you can that you
can actually do this we call this
patching so that's why all these schemes
are called patched compression schemes
so instead of storing exception codes we
store a value there but we know that
this value here because it's an entry
point it is actually an exception ok so
the fourth value in this block fifth 33
one for is P it should be nine but it
says six so it's an exception so we take
the first exception okay so you would
emit 9 here and the 6s that in six six
steps from there there is the next
exception so it's actually just a linked
list so we use the beeb its values as a
linked list okay so this how it goes so
one further okay so you follow the
linked list by following the link list
you can patch up there
the value sequence so the algorithm is
real simple so first you just go through
the data and you decode it as if all the
values were good values and no exception
values and you omit the wrong values
there for the exceptions and then you
patch up your result and this is
actually faster so Caroline you've
worked at least you don't work the list
only for the exception so this loop
doesn't take as many iterations really
simple code and then this happens
performance vice so we really beat the
simple if then else and it gets in the
best gauge to two gigabytes per second
of decompression speed you see the three
different families their schemes so
petting makes two passes to the data but
it's faster because there are no brain
stem its predictions ok so we learn more
queries this is more data from the TP th
data warehousing benchmark so if you
have a slow if you have a slow io system
like we usually have not too many disks
for this perk or you are heavily I
outbound in general and then compression
translates more or less directly to
performance okay and then you do don't
surfer much see here in if you have more
days this Desai think this system at for
the 50 megabytes per second of
throughput in sometimes win but you
don't lose or you don't lose much taste
the due to the decompression and this
the other system is really faster that
if you compare it with a commercial
numbers that are these are official TBH
numbers from the TP g8's website it's a
it's not really a fair comparison I must
say because it's not
this is just a prototype but it's
indicates that the raw performances it's
very high you see that it beats often
those members of a system that this as a
hundred forty-two dish so it has ten
times more days eight times more course
okay so it's really fasting so we're
going to be X hundred and the
compression it is published in ic de
there is some more work in column stores
and and because like I said como stores
and compression really go well together
and the group of MIT of Stonebraker has
done a bit of work on that as well so
I'm off to my last topic what's my time
action five minutes ok now we'll be
quick so about disc sharing I might
actually skip something if some of it
then so tell you you see some trends in
in this so before I'm yeah I may
actually shorten it but this is what I I
do want to say so that a situation with
latest affairs bandwidth in this is
really kind of hopeless and currently
the
yeah it has grown so far apart and if
you look at what database database
installations do I mean if you if you
look at tpc c or t PCH installations
where they put at least 20 dispar core
or fot pcc more to get a balanced system
and if you think about North Accords and
what that's going to do with it I mean
it's really not sustainable anymore I
that means that s database research we
have to think of different ways of using
of using disks we have to go more
sequential and so here you see t PCH it
is very common to that they just buy
this that are or in those data
warehousing configurations they buy the
very smallest is that is available 18
gigabytes and then they have root and 40
of them for a total of two terabyte of
storage and we're talking about when the
gigabyte that's compressible down to 30
so this is make sense it doesn't really
make sense for me actually so it's kind
of an unsustainable situation right now
so it will not go like this if you if
you look at what you have to do the
situation forces led by the sea and
bandwidth so there is a small experiment
where you do on the one hand large
sequential reads so these are the thin
lines and the round lines are where you
do random random access and you vary the
block size so obviously there is a huge
difference if you have small block size
in doing sequential or random it's an
enormous difference really only when you
have a random access pattern with you
use larger block sizes your performance
gets more reasonable so what is a good
block size that permits you to do random
random access well currently it's about
128 megabytes so that is the energy
rising easier actually this
good block sizes is growing so that's
something that needs to be taken into
into account really and 4 kilobyte block
sizes they they are really a thing of
the past and then you can't really use
them in there and then you don't have
any freedom to schedule reach you should
only use them sequentially but so so
there are a number of things that you
can do as a database engine to try to
get more more actually my thesis that
databases should really focus on this
base problems on only doing sequential
access and if the end if you do
sequential if you do don't sequential
access it should be bulky like megabytes
at a time access so the things that you
could do is having death tables in
multiple orders available clustering
tables on joins and join order so you
have two tables in the same order
available already so you can do moose
joints between them mom using column
stores like molarity or money bx hundred
lightweight compression and coordinated
concurrent discs because all this gets
worse if multiple programs at the same
time are trying to use the disc then you
are then then the programs are going to
fight for the bandwidth and and
bandwidth is even going to deteriorating
or seek times are going to increase even
so okay very very short about what we
did with this fighting for bandwidth so
typically i'll just go to the examples
so here you see what happens in the
buffer manager when
oh no default policy in a database
system so the red so it clearly comes in
query one the red looks at a time where
it's actually waiting for for data Green
is when it's processing a block of data
and blue is when the the blockage in the
is cached in the buffer manager so let's
suppose that we have here five room for
five blocks so after you have read more
than five blocks you will swap out
blocks again so they will disappear from
this trace okay suppose a new query
comes in so what will happen is that
there are not two points on the disk
that are being accessed at the same time
concurrently right and so this means
that there is going to be more sick
times and actually your i/o throughput
will go down additionally that are
opportunities that are lost because
queries that need the same data are not
are not using it from the other because
everybody wants to start at start of
your of your scan range okay so they are
doing all this idle while they could
actually reuse higher of the other
systems of the other queries so data is
read actually many many times bandwidth
deteriorates and both later the--and
throughput of the system are bad and
these are the traits that you that you
that are produced so over time you plot
which this blocks are fetch time you see
multiple lines at the same time being
active okay so there are things that you
I've actually skip around these things
so this is elevator policy and you can
do all the scheduling stuff
elevator you get only you get only one
one trait at the same time you make sure
that I only know there's only one curse
or going through the data the effect is
that everybody that you get optimal
throughputs but latest years it's bad
because slow slow queries hold up fast
creation short Furies have to wait all
the way until the elevator each step
point so we did some work got
cooperative scans where we try to get a
scheduling policy that allows
cooperative scanning a group catch both
good throughput like you can get with
elevator algorithm but also gets nicely
latency so I would explain the algorithm
let's show how it works so suppose we
have a query that comes in but then a
second spirit comes in actually the
second query as a range that doesn't
overlap with the first query that's
because it's a very short query its
prioritized so actually the this query
gets its later the other query is
doesn't get the first query doesn't get
data for a short time but create to
quickly finishes Thank very one resumes
a query three comes in that has an
overlapping range both with query to and
with query one okay so the date of
create to it sees still in the cash
although it's the last data it would
read normally because at the end of its
range it starts reading that data right
away so the idea behind this algorithm
is that scans accept data in non
sequential order okay so it starts that
up there we
I'll let it work yup there we threw that
off two blocks had happened to be there
and then resumes here and query one is
now not accepting the blocks it would it
would usually try to read but it's a it
uses the blocks that are both
interesting to it and to carry three so
there are two users of this of these
blocks okay and after query three is
done also quickly the remaining blocks
are right right so you get these very
strange strange uses patterns where are
no sequential lines anymore and but if
you plot those three actually I didn't
discuss attached but you have the normal
policy and the elevator policy and the
relevance policy which is the
normalization point here and testing a
wide range of different scenarios fast
very slow queries right out of time she
said okay well I've always done but
yeah I will wrap up in two minutes ok so
the studies so this adaptive policy also
helps especially if you compared with
what usually happens to normal approach
both in latest in throughput you you get
you can significantly improve improve
the way your disk is used ok my
conclusion I presented this the work
that has been done in the entire
x-hundred project it is a new database
corner from CWI it use this blocked
sites right a model that makes itself
fast and so far that we did these io
optimizations to balance it more with
its with this based storage so we did
lightweight compression schemes and this
cooperative scheduling in storage
manager which get you the lightweight
compression in many benchmarks or in
both in IR and in data warehousing got
such a typical factor three of
compression and also speed and like you
saw here maybe you can be a factor too
if you do this of you in our we are
benchmarks will be so kind of affected
to both in latency and throughput of
your queries with this cooperative I are
scheduling so these things are useful
techniques for the system and yeah we
have tried this on data warehousing and
also an information retrieval workloads
so there is lots of future work and you
can find more literature here thank you
I'm sorry for being with late all right
thank you much place for coming
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>