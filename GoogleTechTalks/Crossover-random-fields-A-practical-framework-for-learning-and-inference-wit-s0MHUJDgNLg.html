<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Crossover random fields: A practical framework for learning and inference wit... | Coder Coacher - Coaching Coders</title><meta content="Crossover random fields: A practical framework for learning and inference wit... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Crossover random fields: A practical framework for learning and inference wit...</b></h2><h5 class="post__date">2008-09-11</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/s0MHUJDgNLg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a great pleasure for me to welcome
my friend Justin Domke from the
University of Maryland and he's actually
a graduate student who is almost on his
way to finishing over the past few years
he's been doing like really cool stuff
with basic vision problems and coming up
with like totally different solutions to
most of them so like 11 example I can
cite is his work on estimating 3d motion
and most people do it with like finding
point correspondences doing some
estimation and so on so what he does is
without finding any point
correspondences he can estimate motion
just from some probability distributions
and he actually ends up doing better
than hand clicked matches between images
so it's pretty impressive so this is one
of his latest cool T one of the latest
cool things that he has come up with
crossover random fields and at least
judging from his results and some
descriptions of his method I find that
this will be something which will be
really useful to a lot of people who
have come to this talk so without
further ado Justin thanks Angie so this
is going to be a talk on graphical
models and it was my assumption that
people's backgrounds would be kind of
varying so please feel free at any time
to jump in and ask questions in
principle this should be understandable
why people who have almost no background
so if you don't know graphical models
the problem that we want to solve is we
have we have some situation where we
have lots of uncertainty and you want to
represent some distribution over some
set of variables x1 to xn and then after
we have the distribution we're going to
manipulate it in some way to solve our
problems and in the simplest case a
Markov random field is nothing but a way
of writing a probability distribution in
a factorized form so instead of
representing all of p you represent it
as a product of factors sigh so you have
a product / see see here is clicks or
small subsets of variables say pairs of
variables are some small subset and then
you have some function defined on that
subset a positive function then you
multiply them all together and you
normalize and that's your probability
distribution over the
variables and this would be a Markov
random field or a factor graph depending
on certain technicalities now another
common case is a conditional random
field and the difference is we don't
want to represent just some joint
distribution we want to represent the
conditional distribution over Y given X
and it's very similar again you have a
product over factors defined over small
sets subsets of variables and the only
difference is now it's allowed each
factor to depend on X in some way so
examples of this the most common example
envision is probably stereo we're given
two input images you want to find the
disparity map why another example it
would be denoising where you observe a
noisy image and you want to predict a
clean image and finally there's the
problem of image labeling and this will
be actually the problem that i will i'll
present results for later so here you
observe some image and you want to
predict for each pixel what is its label
you have some small set of labels maybe
5 labels or 10 labels and you want to
produce the correct label for each pixel
in the image so what we're interested in
here is joint inference and learning the
learning problem is you're given some
set of data so here this is a set of
let's say labeled images you have an
image along with it with its label and
the learning problem is given that data
to fit some distribution then the
inference problem is you'll get some new
observed image X Prime and you want to
use the conditional distribution over
the labels to to solve your problems so
suppose that someone gives us a
distribution P and we have this
observation X Prime now what do we want
to do with the distribution what
questions do we want to ask and there
are two sort of famous inference
problems for graphical models the first
is map in inference so in map inference
you seek the the single labeling why
that has maximum probability given the
input and this is very intuitive and
it's probably the dominant inference
prints problem in in practice but
there's another very important inference
problem called marginalization and in
the marginalization what you do is you
seek the
the single variable conditional
distributions given all the ex-prime
that is you seek the marginal
distributions so the first thing to
notice about marginalization is that
it's something you have to do you do not
have even though it's very simple to
write down the conditional distribution
of one variable given X it's not
something that you just have you have to
run an algorithm to compute it and in
fact you can see it could be quite hard
because if if you want the conditional
that variable I has value of VI you have
to sum over all configurations that obey
that constraint and then if you have
many variables this is a sum with
exponentially many terms so in general
there's no reason to think that we could
do this some tractable but consider a
special case consider the case where the
variables are in a chain so what I'm
trying to draw here is each each node
here is representing a variable y 1 y 2
up to yn and the links represent the
fact that those two variables lie
together and a factor so this means we
can write the Joint Distribution is a
product where there's one factor over y1
y2 another factor over y 2 y 3 etc and
I've suppressed the dependence on X in
this notation to keep this slide simpler
so all right if you read the conditional
distribution in that way and you take
this sum it's not hard to see that you
can take the sum and you can you can
break it up into these two different
terms least one sum over y 1 2 I minus 1
and 1 sum over I + 1 to N and given
these two terms it's not hard to see
that you can compute them recursively if
you define alpha to be the sum over all
these variables up to i minus 1 you can
see that it has this simple recursive
relationship and so using that recursion
you can you can solve this efficiently
by dynamic programming so this is this
is I'm showing you the algorithm to
compute this the first of these two sums
but you can you there's a completely
equivalent algorithm for the other one
so all right this this has complexity
envy square it's NBD because there's n
variables and me values in this table so
there's envy values in the table and
there's another factor of V because you
have to sum over all the values in in
the previous variable so it's important
to note that this is quadratic in the
number of labels that each variable can
have
so that's how you do it for chains but
it's easy to also extend this algorithm
to trees but what if we have an image
which if we want to solve you know a
vision problem the simplest relationship
you'd think would be that each pixel it
depends somehow on its four closest
neighbors well if you have a grit a grid
clearly dynamic programming is hopeless
it's not going to work and in fact not
only is dynamic programming hopeless but
the problem in general is np-hard and
it's NP hard even to compute the
marginals within some constant factor so
in general graphs marginalization is a
very very tough problem but let's return
to doing marginals on tractable graphs
so I talked about here this is this is
your dynamic programming relationship
but you could you could equivalently
write this whereas instead of being
dynamic program you could write it as a
so-called message passing algorithm so
you have messages that are sent from
variable J to variable I which you
compute this way so in fact if you if
you run this this message passing
algorithm on a chain you recover exactly
the same algorithm as dynamic
programming but it works in in general
on trees now you can see that if when
you write the algorithm in this message
passing form there's nothing to prevent
you from running it on generic graphs
and graphs that are not trees or that
have loops and if you do this that's
what's known as loopy belief propagation
and in practice if you do that even
though the theory would say you could
not run in for insta on this graph in
practice you often get excellent results
now there's a couple of downsides the
first downside is that the algorithm may
not converge or if it does converge it
may converge to different local minima
but there's there's sort of a there's
been an explosion of research on
different approximate inference
algorithms in the last few years so this
is improving all the time so that's the
situation for doing inference but what
if we want to do learning so if we want
to learn some distribution it's natural
to think that if we know that we have to
use some approximate inference algorithm
the learning should somehow compensate
for that all right you'd think that
somehow the learning should be aware of
the defects and inference and
right to get the distribution that will
give you the best possible results and
there are a couple of papers in the
literature where people where people try
to do this and I'm not going to discuss
them because it gets very technical but
I think it's fair to say that there's
room for a different approach for for
learning and inference in high tree
width graphical models and that's what
I'm gonna talk about today alright so
the approach that I'm going to describe
for you it's based on two basic ideas
the first idea is new loss functions so
learning of graphical models is totally
dominated by what's known is maximum
likelihood our maximum conditional
likelihood learning and in this method
it's based on different loss functions
which are more purposive in a way and
the other idea is using a series of
tractable models so it may be impossible
to do inference on these grids but maybe
we can just fit a simple tractable model
and get some reasonable results so the
idea here is you use some simple
tractable model then you take the
results of inference on that model and
you feed them into a different model
right so you so let me talk a little bit
more about these these two ideas before
getting into the details so the idea of
the look the different loss functions is
here on the left if you're doing
learning p this would be what's known as
the maximum conditional likelihood
criterion so you would try to pick a
distribution P that maximizes that
quantity the sum over all data elements
the log probability of each element so
the simplest of these new loss functions
that I'm talking about is here on the
right hand side so you can see it's very
similar again you have the sum over the
data but the difference is now you have
the log probability of each individual
variable I so the difference here is
really that when you use the maximum
conditional likelihood criterion you try
to fit the whole joint distribution
whereas this criterion only tries to fit
marginals to be accurate and I'll talk
about this in much more detail so the
second idea the idea of using a series
of tracfone models is here so suppose
this is the model that you would like to
do in fritz on but you can't because
it's intractable so the basic idea is
okay let's first just fit a model
overscan
to give some results then we can do
inference here exactly by dynamic
programming and then let's take the
results of inference here the marginals
that are produced by this model and feed
them into another model now over columns
so you see the columns they cross over
the scan lines and you can continue you
can you can make as many models as you
like defined in different ways and what
the hope is is that if we define these
distributions q of y1 y2 etc
appropriately we can get marginals at
the end that closely approximate the
true marginals so the basic of the talk
will be discussing you know how to
actually do this so why bother and I
think that there's there's three
advantages of this relative two
approaches in the literature for
approximate loaning and inference the
first advantage is practablity so the
algorithm that I will describe we know
exactly how long it takes you everything
to do inference is just dynamic
programming and to do learning each
iteration takes the same complexity and
it's tough to analyze the the complexity
of the the methods in the literature for
approximately and inference because they
rely on convergence so it's tough to say
exactly how long that they take but it's
fair to say this is at least some
significant constant factor or something
faster the second advantage is you can
use these new loss functions which more
closely sort of represent our priorities
when we're doing learning an inference
and the third advantage is flexibility
and I'll talk about this more later but
these models can sort of be more
powerful than standard approaches so
okay learning criteria so first I'm
going to review maximum likelihood
learning so if you have some set of
samples X hat and these are sampled from
some true unknown distribution P and
maximum likelihood learning means we're
trying to pick a distribution Q in a set
of distributions big Q right so if if
you are optimizing over some parameters
depending upon how you set the
parameters you will represent different
distributions and presumably you cannot
represent all distributions you only
represent some you know small subset and
that set is cute
so maximum likelihood learning says that
you look for the distribution queue that
has the maximum product of probabilities
for all elements in the training data or
equivalently the maximum sum of log
probabilities so this is very standard
now if I asked you why you would use
maximum likelihood learning my guess is
that most of you would tell me a
justification something like the
following you would say alright what
you're really trying to do is you're
trying to find the most likely
distribution q given the training data
and we know by basis equation that
that's equal to the probability of the
training data given Q times some prior
over Q so if we assume a uniform prior
you're searching for the most likely
distribution given the data something
like this would be would be the common
justification but in my opinion this
justification should be viewed with
great skepticism for problems like
vision that are very top to model and
the reason is that this justification
really relies upon the assumption that p
is in the set that you're optimizing
over so when we when we make a graphical
model or whatever you make assumptions
you you pick neighborhoods you pick ways
to parameterize certain nodes and in
practice there's there's no reason to
believe that these assumptions that
you've made are correct and you will
find very often that the assumptions
you've made are either wrong or not
completely correct and if that's the
case this justification breaks down so
nevertheless maximum likelihood is not a
stupid thing to do and you can you can
have a different justification which
does not need this assumption and the
other justification is that when you're
doing maximum likelihood learning you're
trying to minimize KL divergence so if
you don't know KL divergence is callback
lie blur divergence it's it's something
that comes from information theory but
it's just basically it's a functional of
P and Q that tells you basically how
close q is to pee so if the KL
divergence was 0p would be equal to Q
anyway so this is the KL divergence and
you can see the term p log p is constant
so you can drop it and then because we
don't have the true distribution p we
only have data sample throw it for it
from it you can use the data as a sort
of Monte Carlo approximation
so when you when you do maximum
likelihood learning you're trying to
minimize the KL divergence to the true
distribution similarly if you're fitting
a conditional random field you're likely
to fit it by the maximum conditional
likelihood which is up here and this can
be seen as minimizing the expected KL
divergence so you have an expectation
over X and then you have the KL
divergence over Y and just using these
same tricks you drop a constant term and
then you use your data as a stand-in for
the distribution P which we don't have
the conditional likelihood falls out so
KL divergence is okay it's it's a
perfectly reasonable thing to minimize
KL divergence if you just want to fit a
distribution to your data but here once
we fit the distribution in the inference
stage we want to use it to compute
marginals right so once we fit the
distribution q the only thing that we're
really interested is in the marginals q
of why I given X whereas the maximum
likelihood or conditional likelihood are
trying to really make the whole joint
distribution accurate so so how do we if
we would only like to fit marginals it's
natural to think that there could be a
distribution q that has poor joint
accuracy but nevertheless manages to
give you good marginals and personally I
would take that distribution because you
never really see any aspect of the Joint
Distribution other than the marginals it
produces so the simplest case of a
univariate loss function is what I think
should be called the univariate
conditional likelihood so I know there's
too much information on these slides but
hopefully you get some sort of a feeling
for it the univariate conditional
likelihood says you minimize the
expected you would have variate KL
divergence so expectation over X the sum
of KL divergence is of individual
variables and you do some algebra and
you get this loss function a different
loss function you could have is the
quadratic loss so this is again we have
an expectation over X but now we want
the marginals of P to be close to the
marginals of Q and you measure how close
they are by taking the sum of squared
differences of marginal probabilities
right so if the marginals of PR always
equal to the marginals of q this will be
0 and again you just expand the
quadratic term
and this loss function will fall out so
so the reason that I'm bothering you
with this different loss function is to
emphasize the fact that there's no one
way to measure marginal accuracy using
something like the likelihood or the
univariate likelihood it's a fine thing
to do but the right loss function for
you it depends on what you want so
here's the last loss function is a
particularly purpose of one which is the
smooth univariate classification error
so this one's a little bit more tricky
but let's start with the second line
here very often if you're going to
produce marginals you're going to use
those marginals to predict the values of
individual variables so what you're
going to do is you're going to go to
each variable I and you're going to look
for the label that has maximum
probability you're going to for the
Maximizer of this so what we would like
to do is we would like those predictions
to be as accurate as they can be so here
I'm using ever since notation so inside
of these brackets the result of the
brackets is one if the inside expression
is true if it's zero otherwise so what
you're saying is you want to minimize
the number of predictions that you make
on the training data which are wrong so
we can compute this expression on the
second line perfectly fine as long as
you can produce marginals you can check
how often they're right the problem is
you would not be able to do an
optimization over that and you can't
optimize because it's discontinuous as
you change your marginal probabilities
the the Maximizer of this expression
changes discontinuously so you can sort
of relax this step function into a
sigmoid function with some control
parameter lambda and you get this so the
point of all this is that if you want to
use your model to produce marginals at
the end you should use a loss function
which measures the accuracy of marginals
and the particular loss function that
you want to use it depends on your
priorities there's many of them so this
brings me to the dirty little secret
about conditional random fields and the
secret is this take take this simple
conditional random field this is the
simplest sort of interesting case so we
have a product / pairs IJ where some
function sort of measures how much why I
and YJ like each other and then you have
a product over all variables I of how
much variable I wants to be
I I given the input image so these are
sort of the univariate potentials so a
question you could be asking is the
following and in fact probably some
people are thinking this you're thinking
why are you messing around with all
these graphical models because in the
approach as I've described it to you we
have to work really hard to fit a joint
distribution to the data in in the
learning stage and then after we have it
in the imprint stage we have to work
really hard again to get back from the
Joint Distribution to marginals so if
what you're interested in is marginals
why don't you just directly fit the
marginals is this is an excellent
question and the answer is well there's
theoretical reasons why you wouldn't
want to do that right so so this
function f you could you could do a
great job of fitting this directly by a
you know a boosted decision tree or
whatever your favorite classifier is and
the theoretical reason is well if you if
you were able to fit that you'd have to
sort of implicitly encode the process of
marginalizing out a conditional random
field anyway but regardless of that
theory in practice what you will find is
that doing exactly this just skipping
the conditional random field completely
can give you great results sometimes as
good or better than fitting a whole
conditional random field and if you if
you've tried graphical models you
probably know this it's not really a
secret so why does that happen I think
the major reason that this happens is
most approaches for learning conditional
random fields they require you to
represent your univariate potentials in
this sort of log linear form you have
some vector of weights which you you
take the you know dot product with the
input X and then that's the function f
so most approaches for fitting graphical
models they require some sort of a
simple representation like this whereas
if you if you skip the graphical model
completely you can use your favorite
classifier you can use a very
sophisticated technique and you can fit
something great so if you have to
simplify this function f in order to get
a tractable graphical model approach
that might not be something to what to
do so the lesson is we really do not
want to restrict the flexibility of
these functions f
so now I'll finally get to the model
that I'm describing today so here is the
basic idea of a so-called crossover
random field so what we do is we will
model the conditional distribution for a
bunch of layers l now this is a little
bit confusing at first blush so let me
tell you what this is not this is not
doing the bottom what we would vote and
this on the bottom is what I would sort
of like to do I would like to say that
the the conditional distribution for
layer L is given by completely
marginalizing out the variables at the
previous layer right so you have a set
of tracks well models the marginalized
out each of the hidden layers and then
you get to the end and this is what I
would like to do but the reason that we
cannot do it is because this summation
will be intractable you see we're
summing over every possible joint
configuration of the variables in the
previous layer which is a sum over
exponentially many terms so I will
assume that this conditional
distribution here is a tractable
distribution but even if it's tractable
doing this whole summation will not be
unless this has a simple form so this is
really the the central approximation
leading to a tractable approach here
rather than passing the whole
distribution from the previous layer we
pass the distribution only in a fully
factorized form so you take H about Y
given X which is a product of all the
marginal probabilities and you use that
instead so so once you substitute a
fully factorized form this will then
lead to a tractable way to do this
summation so it's important to note that
it's this is not equivalent to assuming
that the random variables in the
previous layer are equivalent there are
independent because of course they're
not independent the reason that it's not
equivalent is that when we fit the
distribution for the previous layer it's
fit to be used in this way
so how do we represent the conditional
distribution of one layer given the
previous layer and I use this equation
so I know this is a lot of symbols but
first look at the bottom which is our
familiar equation defining a conditional
random field you have a bunch of
pairwise interactions and then you have
the univariate potentials so the
conditional distribution of one layer
given the previous layer is almost the
same thing we again have these pairwise
interactions now just you know depending
on L but now we add this extra term fee
and what fee tells you is basically how
much does why I at layer L like this the
value at the previous layer at the same
position ok so so far you've seen all
the equations on this slide before this
is our the equation defining the
distribution at one layer given the
previous layer this is the summation
that we want to do so how do we actually
do that summation the answer is here and
it's I feel a little bit silly calling
this a theorem because it's actually
very very easy to show if you substitute
this expression in here and you do a
little bit of algebra this will fall out
so given the marginals at the previous
layer we can define the distribution
over the next layer by this now if you
wanted to you could also consider this
to be the equation defining the model
right you could you could skip all this
justification about a fully factorized
representation of the previous layer and
consider this to be the defining
equation so the point is if we have the
marginals at the previous layer we can
just do this single variant summation
and sort of absorb them in and then that
gives us a regular tractable conditional
random field over the next layer so how
do we do inference in principle I've
already told you but I think it's it's
worth giving it in a explicitly as an
algorithm so suppose that we have the
functions defining the crossover random
field and we have some observation image
X Prime so how do we compute the output
of the model well the thing that you
first do is you compute the marginals of
the first layer which is just defined by
a regular conditional random field
then what you do for every layer from 2
to the maximum what you do is you first
you absorb in if you will the marginals
from the previous layer then that gives
you a normal conditional random field
and then you compute the marginals of
that layer again by dynamic programming
all right so I've told you how to do
inference given the parameters defining
a model but how do we do learning so the
problem is we have the training data and
we want to fit a crossover random field
q2r more precisely we want to fit the
functions defining that model so here's
here's what we know already we know if
you give me the parameters of these
functions we can just using the
algorithm I just showed you we can
compute the output of the model that is
we can compute the marginals that the
model predicts then what we can do is we
can take the the output of the model the
marginals and we can plug them into any
of those univariate loss functions I
showed you earlier so we can measure how
well does the model with the current
parameters predict on the data set so
the problem is we can measure how well
the model predicts but how do we change
the parameters to make it predict better
or more formally what we want is the
gradients of the loss function with
respect to the parameters of the model
so how do you how do we get that
gradient well the first thing that I did
is of course I derives an algorithm and
the the authors of the previous paper
the previous paper is defining the
univariate conditional likelihood and
the smooth classification error they
provided algorithms for computing the
gradient in the case of you know single
layer conditional random fields and log
linear functions so it's possible to
extend these results through a great
deal of complexity to the situation that
I had here and here's a couple of pages
of that derivation and this is the first
thing I did and it was really a
technical nightmare and no one should
ever do this the reason is after I
implemented this I remembered something
I had read about a couple of years ago I
remembered something called automatic
differentiation and if you take nothing
else
from this talk take the following use
automatic differentiation and anybody in
this room ever use this tool Wow okay
I'm impressed so this is this is a
wonderful thing it's not to be confused
with symbolic differentiation using
symbolic differentiation using a
computer algebra system completely
different so suppose you have some
function like a C++ function that takes
n inputs and computes one output and you
would like the gradient of that function
so sometimes people wonder is it
possible to compute that gradient tract
ibly well the answer is yeah it's always
possible and you know this because you
know the chain rule so there are there
are software tools that will in fact
give you the gradient with with no extra
programming effort on your part and
they'll give it to you in the same
complexity as running the original
function so this is great so i
particularly i use this rad toolbox
reverse automatic differentiation by
david gay is it Sandia National Labs so
what it gives you is it gives you a new
object ad double new class and so what
all that you basically have to do is you
take all the doubles and your function
that computes the loss function and you
replace them with ad doubles then when
the function runs these objects will
sort of they overload all the standard
arithmetic operations like plus and
multiplication and they carefully sort
of record all the computations leading
to that loss function then you're able
to demand the gradient and it will
automatically sort of back propagate all
the information and give you the
gradient so this this works this works
much better better in fact this is this
is it has a better a better algorithmic
complexity than the algorithm that I
Drive by hand and much easier so the
bottom line if we want to do learning
how do you do it well we can compute the
marginals of the model just by dynamic
programming iterative video on each
level given all those marginals we can
compute the loss we can predict how well
it predicts on the training data then
just by automatic differentiation we get
the gradient of all these functions with
respect to the loss so we know how to
change the parameters of the model to
make it predict better so what we do for
learning is we do nothing but just plug
this gradient into a nonlinear
optimization
so notice in everything that I've told
you we never restricted what this
function f was that is the the
univariate potential so as long as we
can compute it and as long as it's
differentiable we can optimize it at the
same time that we optimize the rest of
the model and this is important all
right so now I will finally get to some
experimental results and i'm going to
only show results for the single problem
of pixel labeling so specifically this
is the the street scenes database now
originally I think this database
provides something like eight or nine
classes but here I'm only using five
classes so what you have is you have an
image and then corresponding to the
image you have labeled a set of labels
and so each pixel could be of one of
something like let's see five classes it
could be Street which I saw is gray
building which is red sky which is blue
trees which are green or cars which are
this sort of Pinkel pink or purple color
so the function f that I actually used
in these experiments here is a small
neural network so here X sub pie I that
denotes just the set of pixels in the 3
by 3 patch surrounding the pixel I in
the output so what you do is you do
nothing but you put that through a
nonlinear transformation you put it
through a nonlinear sigmoid function and
then you have a linear classifier on
that it's just a single layer neural
network all right so here is the result
for what you could call a zero layer
model so what this means is it's nothing
but just fitting this neural network on
every 3 by 3 patch in the training data
to predict the label at the center of
that patch and you can see that the
results are not very good these are the
results that you get if you now add
horizontal connections so this is just a
tractable conditional random field where
you only have interactions over scan
lines and you can see you get you get
quite a big boost so no we actually have
what's actually a new type of model
where we have we take the output from
the results on the scan lines and feed
it into a model defined over columns and
you can keep adding more layers I think
up to six here which is about when we
reach the point of diminishing returns
here's the result for a different image
and here's the last one
so these results I think are pretty good
considering they're all based on just an
analysis of three by three patches
notice here that in the output image
there's these barrels and the model is
confused and it classifies the barrels
car it sort of has no choice to do this
because it has to predict one of those
five classes for every pixel in the
image and none of those five classes is
barrel so it's it's guaranteed to get
these wrong and in practice you do find
that if there's some sort of a weird
object in the scene it inevitably tends
to think that it's a car I don't know
what that tells you so here's a sort of
quantitative analysis of how well this
does these are ROC curves so if you're
not familiar imagine that we want to
find cars in the database but what you
could do is you could pick a threshold
of point seven and you could see you
could say every time i compute a
marginal probability of point seven or
greater of being car I'm going to guess
that this pixel is car now if you do
that you're going to get a certain
fraction of true positives that means
you will of the car pixels in the data
there's a fraction of them that you
correctly guess so you'd like that to be
one and you also get a fraction of false
positives so this is non cars that you
incorrectly guess to be cars so you'd
like that to be zero and as you change
the threshold you sweep out a curve so
these are ROC curves and so you can see
that sky is pretty easy to predict and
doesn't in fact get very much benefit
from adding the different layers to the
model car meanwhile you can see with
just the neural network on every 3 by 3
patch doesn't do very well but we're
doing better as we add more layers
pretty much the same story for buildings
and roads roads pretty interesting Lee
after three layers you see no
improvement different classes seem to
require different amounts of
so the if you wanna look at
classification error this is what you
get if you just fit neural network you
get a classification error of almost
thirty percent but by the time we get to
about six layers it's a down to like
eleven percent or twelve percent on test
data alright so finally I have a couple
of movies that I made driving around
Maryland so this first movie this is the
results of the zero layer model just
just running a classifier and every 3 by
3 patch so this actually looks fairly
good but let me point out to you that
it's the results are param you can see
there's these pink pixels here meaning
that the model thinks it sees a car here
flying above the trees and see so it
doesn't do a very good job
all right so these are the results now
if we go to a one layer model yeah yeah
well I mean I mean this is a I mean do
you mean a CRF to find on a grid or a
seer after we should all right so you're
saying if instead of using a neural
network here I use a logistic regression
if you would yeah if you the reason I
use a neural network instead of a linear
classifier is that it's much better yeah
I have compared to quantitatively on a
difference well I have not been able to
make the question is do you mean a CRF
approach with approximate inference on a
grid or do you mean a CRF approach just
to find on scan lines okay great on a
great so that's this is it's a it's very
tough to actually get this to work if
you ever try it because it's actually
sort of an open problem to make learning
and inference work together for
intractable models right it is so so i
have tested these these these methods
for these are not standard there's
methods for joint approximate inference
in learning and i have tested them on
simple denoising problems and they do
genuinely work a little bit worse even
even if you use again a linear model in
this approach so i don't i don't think i
have tested a linear CRF versus this
model
myself reserved and coverage you have a
period of work dear have Wi-Fi workout
CR
yeah well okay first let me say this is
this is finally the result that you get
fitting a six-layer model yeah the thing
I would say most of people who are
actually using conditional random fields
for learning they learn by using
something called a pseudo likelihood and
the pseudo likelihood if you analyze it
in the terms that I had of the KL
divergence it's minimizing something
even weirder than KL divergence so that
comparing to sudo likelihood learning is
something I could definitely do because
you can learn by a suti likely exactly
even if you have intractable inference
but comparing to conditional likelihood
learning is very tough oh um there's a
couple things I wanted to say about
these results I would say that I can't
prove this quantitatively but just by
looking at the results on these images
versus test images from the original
database these results are a step down
there they're a little bit worse and I
think part of that part of that is just
because that's life in machine learning
but but the other reason is it seems
that this model has fit itself to very
delicate properties of the training data
so when I first made these movies to
create the the images I accidentally
used bicubic interpolation rather than
the bilinear interpolation that i have
used to create the original data and
when you run the route the the algorithm
on these by cubic lee interpolated
images the results are disastrous so it
seems that is fit itself very delicately
to the data so it I would I would
conjecture that it is also sort of
adjusted itself to the the colors and
you know color interpolation schemes of
the cameras in the original database but
but I can't I can't substantiate that
alright
so here again you see there's a
bicyclist which it looks like a car to
it alright so that's about it the only
thing I wanted to say is that for future
work the definitely the way forward here
is to make a better function f
regardless of what I told you I haven't
really stuck with my own philosophy
because no matter what you do a
classifier and three by three patches is
very limited in what it can accomplish
so certainly the I think you can get
much better results by fitting more
powerful models that analyze the image
in a better way yeah 3 matches that are
super pixels or with another
segmentation of image and the classifier
yeah I should do that once one has an
emissions classification you can use
that example guys are usually on the
ground well you know I think that the
fact that cars are on the ground is
really what this uses right because you
know there's not that much you can do to
recognize three by three I do not use
the position oh I will tell you if you
use the position it's an immense help
but I sort of felt that that was
cheating so I did not use the position
at all here but but if you notice that
if you train on three by three patches
you always find cars flying around in
the middle of the sky whereas when you
get here it doesn't happen and it
doesn't happen because it uses very
heavily the fact that cars tend to be
between the ground and the buildings and
the trees so it uses that context very
strongly the other thing is on the super
pixels I almost whatever the best
classifier you can build is you can
always take that and sort of put it into
this model right so whatever function
you have even if it involves super
pixels not individual pixels you could
there's still nothing to prevent you
from using that function f here which is
something you should I think you should
probably you know yeah okay yeah you
decide what your illness
oh well l max is I mean I don't you said
so to a point where is so right sort of
your eyeball I would you know I say
until you get to a point of diminishing
return but the reality is you get to a
point of diminishing sort of patients
with it getting slower and slower so
yeah I mean I don't I haven't checked
rigorously how much it improves on test
data but it seems that you really you
get a big benefit by adding the
horizontal interactions a big benefit
and just anecdotally you know it
decreases pretty rapidly James yeah yeah
this is a great point which is you know
you could use any tractable model so you
can do lots of interesting stuff you
could have you know diagonal
interactions or you could have weird
little ya little radial things or you
could even have a little multiscale
trees there's lots of multiscale models
in the literature any tractable model
and so my guess is that what you'd want
to use is you want to use as many as
possible not just the vertical and
horizontal interactions
let's see how did i initialize this so
the the functions sigh and feet that
which is all able to label interactions
those were all initiate initialized to
the diagonal matrix meaning a preference
of each label to itself yeah so this is
something I didn't talk about but doing
this doing this minimization is very
tough because it's a minimization over
you know two thousand training images
and to compute the gradient for any one
image takes like a half a second or
something so if you do the math you
can't actually use a standard I mean you
could but it would be very slow to use a
standard nonlinear optimization so i
actually used what's known as a
stochastic gradient method for learning
in fact if you know i used a natural
gradient is sort of a little bit of a
fancy stochastic gradient method so the
answer to your question is i run it as
long as i'm patient and then that's
convergent yeah yeah these results let's
see probably on the order of maybe five
hours on one pc so i don't know how
patient that is but not very much
anything else
just what revenge is there anything that
prevents the same analysis now on the
ground you got one category or ground
the grounds of the road and sidewalk of
death dog or a creek or a lake or river
maybe 10 or 15 different a friend could
you do the same thing that or for the
picture division supreme and glass and
windows another 10 15 categories let's
say well the immediate thing that
prevents you is the fact that inference
is n V squared so I mean they tell you
in school that polynomial algorithms are
fine but we know in reality that's it's
a real problem so there are there are
people that work on doing efficient
inference in algorithms that have many
labels so you can find it in the
literature people who define markov
random fields for stereo for example and
if you restrict these potentials to have
a certain form you can do in principle
of envy rather than 0 of n b squared and
if you use that trick here not it
actually works out you will
automatically get that same speed up in
learning which is a an advantage again
of automatic differentiation so I think
if you want to scale up too many more
classes maybe you could get to 20 just
by brute forcing it but eventually you
need to have some sort of a smart method
to combat that that quadratic increase
all right I guess that's it thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>