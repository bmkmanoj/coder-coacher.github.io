<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Mapping Methods for Efficient Similarity-Based Multimedia Re | Coder Coacher - Coaching Coders</title><meta content="Mapping Methods for Efficient Similarity-Based Multimedia Re - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Mapping Methods for Efficient Similarity-Based Multimedia Re</b></h2><h5 class="post__date">2009-07-30</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Muh-WmE2puU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's my project introduce my colleague
barrington today to introduce talk about
his recent research on similar based his
neighbor retrieval and application
database okay so thank you all thanks
Jim thing for inviting me and thanks to
everybody for coming including you guys
I'll try to look at everybody it seems
like the crowd is fairly small so let's
keep it interactive if you guys have any
question or some term is not familiar
just you know ask me at the same time i
have to say i'm not going to fully
describe every single thing that we did
in every single project because that
would probably take three hours if they
take me three hours when i first went
over the material so i'm gonna skip over
some of the details but if you think
that something is missing just ask me
and i will go over it also how is fully
says the second slide to make sure i
don't forget you know there's a lot of
people that they've collaborated with on
the projects that i'm going to talk
about i was a PhD student at boston
university like jim beam and i
definitely worked a lot with my advisor
stand scarf and michael advisor George
Kollias and then there is a lot of other
people professors post oak students and
so on the type of collaborated with on
the material that I'm going to talk
about today so basically just to kind of
start of the talk the topic here is
similarity based search in multimedia
databases so this is a pretty diverse
topic is not just a single problem it's
a family of problems it depends on
whether you want to search for video
audio or different types of data but at
the same time there are systems out
there are real systems that serve users
that need to perform this kind of search
operations all the time and doing it
efficiently is an important issue
because this can easily become a
computational bottleneck and at the same
time this is an active research area
there is a lot of work out there I mean
by lots of different people then we have
also done some work in very I'm going to
talk about and so it turns out that blue
a lot of times by just using certain
nice methods we can do a lot better than
just brute force search and especially
now working in the stuff that I'm going
to talk about today our strategies that
we found that in a lot of these problems
we can lots of times
find mapping methods that map the
original hard to deal with multimedia
objects two vectors that are easy to
deal with and then all of a sudden the
multimedia search problem becomes a
vector search problem that is much much
easier to control in to solve
efficiently so I'm actually going to
talk about three different projects that
we have been working on so the first
project is kind of the in the more
classic setup of nearest neighbor
retrieval where you have a database of
stuff like for example here we have a
database of images of handwritten digits
and then you have a query image for
which you want to find the nearest
neighbors in the database for purposes
such as let's say classification and
this part of the talk actually focuses
on how to do this task efficiently when
the distance measure that you use is not
a simple Euclidean distance measure but
a more complicated thing that is usually
more computationally expensive so it
turns out there is quite a lot of
computationally expensive distance
measures that we will talk about and we
have built a method that explicitly is
designed for these kind of measures so
just to throw out some of those measures
is things like dynamic time warping the
added distance kullback leibler distance
bipartite matching shaped contact
snatching things like that earthmovers
distance the second part of the talk
will address the problem of subsequence
matching efficient subsequence matching
in databases of time series and strings
so for example here you can see a very
coarse graphical representation of some
time series that's supposed to be a long
time series in the database then we have
a short query you would like to find the
subsequence in the database that is the
best match and you have the same version
of the problem for Strings for example
this could be at the bottom a DNA
sequence that has millions or billions
of letters and you submit a sequence up
here for which you would like to find
the best subsequence match and finally
the third part of the talk will address
will I will present a method we have
developed for efficient recognition in
domains where you have thousands of
different classes to recognize so
suppose that for example you guys would
like to for the google images product
you would like to have a system that
actually whenever you detect the face
you try to recognize if it's a certain
elaborately right a politician and
that's with an actor or so on so the
list of possible people you might want
to recognize good this will run to the
thousands or tens of thousands or
something like that so we have a method
that will show how to try to recognize
all these people in a more efficient way
than the typical way that just said okay
is a person a no is it person be no is
it person see no and as I said before
the common theme in all these three
problems is that we will describe
mapping methods that can reduce all this
kind of hard search problems into easier
problems that are for which there are
much more straightforward ways to deal
with so we start with the first problem
which is nearest neighbor retrieval
under computationally expensive distance
measures so the setup here is fairly
traditional you have a database of
objects so you can think that every one
of your objects can be an image or it
can be a time series or a string and
then you have a query object and you
want to find the kamo similar objects in
the database for that query object so
our focus here is on computationally
expensive distance measure so I always
find it easier by first explaining what
is not the computationally expensive
distance measure so let's consider the
Euclidean distance right so suppose that
here we have a vector in D dimensions
that is just basically a bunch of
numbers d numbers and we want to compare
this vector to this vector according to
the Euclidean distance so basically if
we have DD measures it takes OD time
right this linear to the number of
dimensions so this is what we consider
cheap I mean you cannot really do better
than that it's pretty hard to compare
two objects and unless you look at them
which already takes all the time now
here's an example of a computationally
expensive distance measure so the idea
is that if you want to compare two
strings to each other you cannot do the
same thing that you did with a Euclidean
distance right so you can know just
compare the first letter with the first
a second with a second and third with a
third and so on because this will not be
quite meaningful in order to have a
meaningful estimate of this distance
it's important to find an optimal
alignment that tells you what parts of
one string correspond to what parts of
the other string so this is for example
what the edit distance does that is
probably the most common distance to be
used for strings and because it has to
search for an optical alignment it is
super linear in time so the edit
distance in particular takes time that
is
direct to the length of the strings
another motivating application in
computer vision which is my primary area
is let's say you want to establish a
meaningful similarity measure between
these two images so in order to do that
it's really important to know that let's
say these two regions correspond to each
other and these two regions correspond
to each other and so on and just to give
a concrete example that was actually
motivation for my thesis work back about
eight years ago by lungi and Malik at
Berkeley the proposed the similarity
measure that they called shape context
matching that they applied on this kind
of data set where basically you have a
database of 60,000 images of handwritten
digits and basically the goal is to use
this kind of data set for classification
and see how well you can recognize new
test digits and they proposed the
measure that basically would extract a
hundred features from each image and
then it would use bipartite matching
which is a graph algorithm for finding
the optimal one-to-one correspondence
between features in one image and
features in the other image so the catch
is that bipartite matching is cubic to
the number of features so what was
really impressive and this is a very
very popular work in computer vision is
that basically shaped context matching
gave the best error rate ever so it was
just a simple nearest neighbor
classifier according to this distance
measure and they gave far better results
than some really really complicated
machine learning algorithms that were
applied to this data set the the problem
is that because exactly will this
measures computation expensive to
actually classify single 28 x 28 image
like the ones that are shown here it
took over an hour so to measure the
distances between a single image and
sixty thousand of these images in the
database it would actually take off in
an hour over an hour and I verified it
myself I implemented this as part of my
thesis I heavily optimized it and this
is pretty much the number that it took
and that their preview of the results by
applying the methods I will talk about
today we got this number down from 66
minutes about 5.2 seconds using indexing
methods and the error rate just went
slightly up from zero point 54 20 point
61 which at least back then we still
have been the best possible result now I
think they got it down to 0.4 using some
other methods but
again you can never win with these
things so and again as I mentioned more
examples of computationally expensive
business measures include dynamic time
warping the added distance the
earthmovers distance the Kalbach libor
distance and obviously these measures
are non-euclidean right we're not really
dealing with vectors let's say when we
measure the added business of strings
but it's important to mention that some
of these measures are also non-metric
right so for example dynamic time
warping and the carbon fiber distance
they don't obey the triangle inequality
actually yet the caliber globular
distance is also not symmetric so
essentially then this means that a big
part or a big fraction of indexing
methods that have been proposed out
there cannot be applied to these
datasets so most of the metal indexing
methods in the database community deal
with vector spaces so popular methods
like locality sensitive hashing or PCA
or KD trees VA files they basically they
assume that your data is vectors there
is also a family of methods that are
metric methods like different types of
trees vp3 xem trees many different types
of letter plus trees in it that
basically they are not designed for
vectors but they're designed for metric
spaces so they use the triangle
inequality to prune away candidate
matches and this way they speed up the
search now even when your space is
metric in practice again like pretty
much anything in database indexing some
in some cases that work really well some
other cases don't quite work well but
it's also important to mention that if
your space is non-metric then basically
these methods become in here into
heuristic and usually they break down
pretty badly so the method that I will
talk about it's actually a special case
of the embedding family so the family of
embedding methods so it's kind of useful
to take a brief look at how those
methods work so there is that you start
with your database objects or your space
of objects overall and you want to come
up with a magic function f that map's
every object to add a dimensional vector
right so we will talk about how to
construct this function so but the idea
is that once you have this mapping then
giving your query you also want to map
it in this D dimensional space and
instead of measuring distances
the original space which is
computationally expensive and this is
exactly what we want to avoid you
measure distances in the invading space
which is much faster so hopefully this
will give you quite a big gain in
efficiency of course the caveat is that
this embedding must preserve a big
amount of a similarity structure of the
original space so the nearest neighbors
that you get here should be ideal
behaviorally related to the nearest
neighbors that you would get in the
original space before actually talk
about a standard way to do this I should
mention probably many of you are
familiar with things like ISO map or
elderly local linear embeddings those
appendix are typically not good for
nearest neighbor retrieval because to
embed the query you typically need to
know the nearest neighbor of the query I
mean this kind of ebonics are extremely
useful for other things like
classification or visualization of
complex data and but it but they are not
really targeted for the problem of
efficient retrieval they're more
targeted for efficient visualization and
efficient classification in this complex
basis right so that is that if you can
afford to find the nearest neighbors and
then back ording to those to another
mapping that will give you some
information about the space those are
great things to do but if you just want
to find the nearest neighbors themselves
they don't really address that because
they do nearest neighbor is part of
their algorithm so a trick that has been
used in the database community for quite
a long time is what is called reference
object embeddings so suppose that out of
the entire database you just pick some
objects even randomly if you like so in
this case for example we pick three
objects just to illustrate the example
and then you define the following
function for every X that is an object
in your space you define a vector that's
composed just by that's basically you
form just by measuring the distance from
X to each of the reference objects for
example here we have three reference
objects so by measuring those three
distances we get the three-dimensional
vector so this is a kind of simplistic
but easy to visualize example where our
space is the set of geographic locations
in the US now admittedly this is an
extremely simple space it does not
compare to the more complicated spaces
we actually want to index but at the
same time it serves to show that this
of embeddings they convey some useful
information so suppose that here we
choose as a reference object Los Angeles
Lincoln Nebraska and Orlando Florida
right so suppose that you know nothing
else about the US except for the
distances from every place to these
three referenced objects say for these
five cities for example you just know
these three numbers you can still get
some useful information right you can
still tell for example that Sacramento
is much closer to Las Vegas than it is
to Washington DC so this kind of and an
example that shows the intuition behind
reference object in bed XD is that in
most faces even though they can get far
more weird and complicated on this space
it typically holds that similar objects
objects that are close to each other
they tend to have similar distances to
other objects so that's kind of the
information that reference object
embeddings capture so this kind of trick
has been used for at least 15 years as
far as I know possibly even more to
speed up nearest neighbor retrieval and
again like everything else in some cases
the work well in some cases the don't
work quite as well in our code in our
work basically our contribution is was
it's a new method for optimizing such
embeddings so basically the key
questions that we try to answer was
first of all what makes an embedding
good or bad so what is a good measure of
embedding quality and to if we agree on
such a measure of embedding quality how
can we actually optimize and embedding
according to this measure so and this is
the kind of thing that happens when you
have machine learning computer vision
people trying to do databases that we
still try to see it as machine learning
problems so it actually turned out that
our approach was a machine learning
approach for solving this database
problem we're essentially in our mind we
saw embedding these classifiers so of
course in order to treat them as
classifiers you have to come up with the
classification task so the
classification task is the following
suppose that I give you triples of
objects cue a and B so typically Q is a
query and they and be our database
objects and ask the question is Q closer
to a or to be this is a binary decision
problem right so it's either closer to a
or to be now if you're picky that can be
ties but we kind of ignore them because
they're really really raring you can
just ignore them pretty much
they don't really make much difference
at within our data sets so essentially
then for example you know if all you
know about the u.s. is these numbers
down here then suppose I asked you so
it's Sacramento closer to Las Vegas or
to Washington DC and you have to answer
just by using the embedding so in that
sense Aaron embedding defines an
estimator for this task or a classifier
that just answers by checking if f of Q
is closer to f of a or to F of B right
so in that sense our criteria is that
the good embedding should answer this
question correctly as often as possible
it turns out that it's no matter what
you do if you have a complicated enough
space you will always make mistakes with
your embedding but the idea that the
fewer those mistakes are the more this
embedding preserves the nearest neighbor
structuring in the original space for
example suppose that in the
theoretically ideal case you have any
betting that always answers this
question correctly then this embedding
perfectly preserves nearest neighbor
structure right it would never reverse
the ordering basically it really
preserve similarity structure in general
nearest neighbors father's neighbors
everything so overall by minimizing the
errors on these triples we can come up
with a good embedding so just to give a
mathematical definition for how this
estimator would work that is kind of
useful essentially if you have an
embedding f that map's view to a vector
space all we have to do is take the
distance between f of Q F of B and
subtract from that the distance between
f of q NF of a so basically if f of q is
closer to Ave Ave you say that then this
sensor will be positive so overall
positive results here mean that Q is
closer to a at least that's your
estimate and negative results would be
interpreted excuse closer to be now once
we set it up this way let's see what
happens if you're embedding is a single
reference object so for example suppose
that you're embedding is just distances
to link on the braska right and you want
to answer questions about the sacramento
closer to Las Vegas or to DC based on
just your distances to Lincoln so you
will get several wrong answers for
example according to this embedding if
you see here
looks like LA and New York are much
closer to each other than they are to
Detroit or Chicago but you will also get
a lot of the answers right for example
you will get the try that Chicago is
closer to Detroit than to either New
York or LA and statistically you will
get more right answers or wrong answers
right so this is a binary decision
problem so if you just flip the coin so
if you had no idea about the answer you
could still get it correctly fifty
percent of the times but the idea is
that here we don't make a totally random
estimate we actually use a little bit of
information so we'll make mistakes but
we should still do better than a random
guess so then basically we the other
thing to note is that if you have a
large space or a large database every
object in your database will give you
slightly different information so each
one of these objects in the database can
be used there is a reference object so
overall we can define lots of different
classifiers with classifiers by using
these objects so basically then we come
up with a fairly common machine learning
problem which is that we have many weak
classifiers and we want to combine them
into an optimized strong classifier so
for those of you who have a machine
learning background you know that the
standard answer to this is use a
boosting method and pretty much this is
what we do we use Erebus which is
probably the most popular boosting
method and I'm pretty sure we could use
other methods as well probably wouldn't
have made much of a difference so
essentially what we do is we tell
adaboost okay here's a bunch of triples
for which we have already spent the time
to compute the original distances so we
know for all those triples if q is
closer to a or to be and here are some
thousands of possible referenced objects
to use so each of those referenced
objects defines a weak classifier and
then we run at a boost which what it
does that it typically chooses a certain
number of this reference object so it
chooses a certain number of we
classifiers and it creates a linear
combination of these we classifiers and
it also sends a weight to each of the
weak classifiers so for the purpose of
this talk we can just use that abuse as
a black box we don't really worry about
how it does it is sufficient to know
that this is really a problem that that
it is designed to solve how to combine
many we classifiers into a good linear
combination and of course the goal is to
minimize the classification error on
triples which is exactly the
optimization criterion that we wanted to
how to use
so basically adaboost it produces a
classifier that is optimized for telling
you if you is closer to a order to be
but what you really want to do is to get
an embedding right so remember that each
of the wii classifiers that are chosen
by adaboost corresponds to our reference
object so if we just take those
referenced objects so let's say adaboost
shows us a hundred reference objects so
if we just define our embedding to be
those hundred referenced objects then
essentially this will be the embedding
that we want to use so we just take the
reference object and we concatenate them
here and we define what we call a boost
map embedding this is just a cheesy name
because we use that a boost and we
produce a mapping now once you map
objects to a vector space another
question is what is the right distance
to use between those vectors and it
turns out that the right distance to use
is a weighted Manhattan distance or l-1
distance where the waste that we use are
simply the weights that adaboost
assigned to each of the referenced
objects so the nice thing if we set it
up this way is this so suppose that I
take a triple qab where Q is closer to a
them to be okay then the classifier that
is constructed by adaboost will make a
mistake on this trip I think it will
tell you that Q is closer to be if and
only if this embedding under this
distance measure will also do the wrong
thing so basically the classifier will
tell you that you correctly the two is
closer to weigh them to be if and only
if the embedding really maps f of Q
closer to F of a than to F of B
according to this distance measure so
essentially if let's say you got really
really likely another boost produced a
perfect classifier this automatically
translates to a perfect embedding or
ever all if adaboost did a good job and
it really optimize this classification
error you will get the high-quality
embedding so once we have an embedding
then we actually ideally we want to use
it in some system to do something useful
so the way to use such embeddings
typically falls within the framework of
what is called filter and refine
retrieval so the idea is that the
embedding Maps objects into vectors
so at first by just comparing the
mapping of the query to the mappings of
the database objects you can produce a
small set of candidates right so even if
the set camp includes let's say one
person of the database that was still a
big gain over having to evaluate the
entire database and then if the refined
step you just compute the exact
distances to just those candidates so in
many of our datasets actually pretty
much in all the data says we have tried
the refines that pretty much takes all
the time in many cases actually it is
faster to measure 60,000 or 100,000
Euclidean or l-1 distances in this step
than to measure a single distance in
this step now again if this wasn't the
case there's also a lot of vector
indexing methods like again PCA lsh KD
trees that you could apply on top but we
found that just doing brute force in the
vector space was sufficient because the
main bottleneck was measuring distances
in the original space so here is the
examples of the results that we got on
the emmys data set of handwritten digits
as a reminder here shape context
matching that was based on bipartite
matching gap gave classification error
rate of zero point fifty four percent
but measuring the distance between the
query and 60,000 database objects takes
over an hour and here is a table of
results that we got with our method with
some competing methods I guess we can
highlight so the method that I just
described here without really any
changes it cuts it down from an hour to
a bit less than a minute oops sorry and
the classification rate just goes
slightly up from essentially the
difference between zero point 54a zero
point 58 is for test objects out of
10,000 so it wasn't really a big deal
actually a method a slight variation of
our method that I will not talk about
today because I just want to give a
brief overview so by doing some more
tricks in our method we get it down to
five seconds with an error rate of 0.6 t
one percent and it's important to
measure so VP trees are a well-known
metric method it works pretty poorly I
mean it just gives us a spirit factor of
three it cuts it down to 20 minutes
condensing is a method where you
basically say okay I have a huge data
set so I can just throw away
database objects and I do this in a way
that tries to preserve classification
accuracy so that actually it got us down
to about a minute but the error rate was
2.4% so much much higher than the zero
point fifty-four percent of before doing
condensing and also interestingly there
was a method by zhang and malik in cvpr
2003 that was actually explicitly
designed to speed a classification using
shape code context information so
essentially in that method the cost of
classifying a test pattern was
equivalent to measuring 50 shaped
context distances to database objects so
that method basically could classify a
query in 3.3 seconds and the error rate
by the error rate when they get pretty
high up to two point fifty five percent
and by applying our method which
basically it knows nothing about shape
context it just uses it as a black box
it uses the distance measure the black
box we got for the same amount of time
spent per query the error rate was 0.83
percent so it was kind of interesting
that using our method and applying this
to shape context as a black box we could
actually do better than the method
explicitly designed for that distance
measure and we actually got a similar
desire on another public benchmark data
set the UNEP and data set so basically
this data set every object is a time
series representing a character that you
write with a stylus on the PDA device
right so basically because you write
with a stylus it actually knows the
order in which you put each of the marks
so it produces a time series so here we
use dynamic time warping as the distance
measure which again is very similar to
the edit distance it's quadratic to the
length of the time series and just to
show their summary of the results here
by using brute force we get one point
nine percent error rate and it takes
about 12 seconds to classify single
character there was a method proposed by
Bauman and co-authors in family 2004
that again was explicitly designed for
speeding up online character recognition
this kind of data whose error rate was
2.9 and it but it basically speeded up
considerably the classification time 20
point 16
guns and by just applying Bush map to
this data set we got actually slightly
faster classification time actually less
than half the time and also better error
rate again there's tons more results in
my faces and in the journal papers that
we have produced on this topic ok so i
guess just to recap very quickly the
main contribution that we have made here
is that we propose a different methods
for optimizing embeddings that is based
on machine learning the formulation is
domain-independent so pretty much the
formulation doesn't care about what
distance measure you have to use as long
as the computationally expensive
distance measure this will just try to
map it to a vector space so as to
produce you to give you a way to do
efficient an efficient filter step and
find candy that's very fast in the
filter step and at the same time we
found that details work very well in
practice and in several cases get
actually outperformed methods that were
specifically built for specific distance
measures ok so that was the pretty much
the first part of the talk the second
problem that I'm going to talk about
today is efficient subsequence matching
of time series and strings so I'm
actually going to start with the problem
on strings so suppose for example that
here you have a at the bottom you have a
database that has long strings that come
let's say from DNA representations so
here you can easily have a stream whose
length is millions or billions of
letters and then at the top you have
something shorter so again it doesn't
have to be six letters it might be 50 or
100 or even a thousand or ten thousand
but typically it's much smaller than the
chromosome so it's part of the DNA that
lets say biologist wants to look up and
see what other parts in other DNA
sequences it is most related to so this
is what we call the subsequent matching
problem in strings so the added distance
is a pretty standard distance for
measuring business between strings
essentially says okay if I have a
sequence at the top and the sequence at
the bottom what is the smallest possible
number of insertions deletions
substitutions that I can apply to
convert this string into this string for
example here what we need to do is make
a substitution
for a to see and we need to delete t to
convert the top string to the bottom
string so the edit distance is too we
just need two operations another common
distance measure actually similarity
measure that you use for Strings is the
smith-waterman measure the main
difference between smith-waterman and
the edit distance is the Smith Smith
motor measure basically it doesn't
really try to align the whole string
with at the top with the whole string at
the bottom it just looks for parts at
overlap as much as possible and it kind
of works by giving bonus points whenever
you have matching letters and giving
penalty points whenever you have
mismatches or gaps for example in this
case this seems to be the optimal
overlap where basically if you match
this a GPC at the top with the tea sorry
attac at the bottom it turns out that ba
matches the a so that gives you a bonus
of two points the G is mismatched with
the T so that's a penalty of one point
you have a bonus of two points here you
have a gap here you have another bonus
of two points here so we're all the
smith-waterman measure would give you a
measure of four here for the similarity
so this is a similarity measure the
higher it is the more similar the two
strings are considered to be so although
these are different the Edit is not the
smith-waterman in terms of formulation
it turns out that the algorithm for
computing those is really really very
similar in it depends on dynamic
programming so the idea is that we
create a table where the x axis
corresponds to the long database string
and the short that the y axis
corresponds to the query string let's
say and then the key thing is that we
basically fill in in a for loop for
every I J position in this table we
compute the following answer what is the
best so for so basically this is I so I
would be three here and Jay would be
eight here so what we would like to
compute here is for the first three
letters of the query what is the best
matching string ending at the eighth
position okay so it turns out that
solving this problem
it depends on the answer that we have
already pre computed for this this and
this problem so essentially this is a
typical dynamic programming setup where
your original problem can be decomposed
into smaller problems you reuse the
solutions to the smaller problems so at
the end basically if you look at the top
row the top row basically tells you what
is the best subsequence match for the
whole query ending at this position
sorry or this position or disposition
and so on so at the end all you have to
do if you use the edit distance you just
scan the top row at the end then you see
where you got the smallest score so that
will show you where your subsequence
match occurred for smith-waterman
actually because they lost partial
matches you actually have to scan the
whole table and see where you get the
highest entry so that will show you
where the best subsequence match occurs
but the world these are really very
similar algorithms and for those of you
who are working with time series if any
computing the dynamic time warping
distance between time series is almost
there is again an extremely similar
algorithm it's exactly the same setup
now of course this algorithm works so if
you want to really find the subsequent
much of a query all you have to do is do
this algorithm but essentially this is
again the equivalent of brute force so
ideally we would like to do something
better than that we would like a faster
way to identify areas of interest areas
of possible matches in the database and
only consider those so the trick that we
use here is that it's kind of related to
the reference objects that I talked
about before but this is kind of
reference of this kernel trick this
trick yourself that we just introduced
and we have a paper at vldb in I guess
next month discussing this where we
extend this to the notion of a reference
sequence so let's see what happens here
so if we use what I talked about in the
first part with reference objects
essentially that would not be very
useful right if we use a reference
object then basically we have to do we
take the distance from the query to the
reference object that gives us a number
and they get then we take the distance
from the database sequence to the
reference object we gives us just a
number so essentially we have a
chromosome let's say with 1 billion
letters and you get just one number out
of it so obviously the single number is
going to be very useful right so the
difference between subsequence matching
and classical nearest neighbor search is
that here the match can occur anywhere
in the database sequence you can start
anywhere you can end anywhere so in a
way it's not sufficient to just map the
entire sequence into a number we need to
map every position into a separate
number so and the way to do it is pretty
much by taking the reference sequence
and computing the dynamic programming
algorithm soakin filling up the entries
in this table right the way we talked
about before so if we do this then at
the top row we will get for every
database position the cost of the best
match between the reference sequence and
a string ending at this position right
so this number here it will be the
embedding that we will use so for every
position for every database position we
will map that to that single number that
has been computed in this part of the
table for the query you will do
something similar we will actually match
the reference to the query using the
dynamic programming algorithm this
number here it will tell you what is the
best match between the reference
sequence and the suffix of the query
right that ends at the end of the query
so this number here will be the
embedding of the query now why would
this thing be any useful it actually
turns out that if we just use these
numbers we can obtain a bound for the
edit distance so basically the idea is
that suppose that I want to know for
this query what is a lower bound on the
possible edit distance between the query
and the best match ending here starting
anywhere before that but ending at this
position in the database so obviously I
can do this by running the full dynamic
programming algorithm but the idea is
that if I have pre computed this
embedding here for the database position
and if I compute this embedding for the
query subtracting these two quantities
will give me a lower bound for that for
example if it turns out that the
embedding of the query here is 10 and
they betting of the database here is 30
I know that you will never find the
match here that is closer than 20 in
terms of the edit distance in this
position so basically this way you can
prune out
a lot of database positions by just
using this inequality and i have to say
here that so our focus so far in our
first implementation of this method has
been on DNA sequences and an additional
constraint that we have used here that i
guess is not always present but it is
sometimes present is that I guess
biologists have indicated in their
publications that they mostly are
interested in queries where the database
the best match in the database is not
too different from the query and so
basically here we use the threads when
we say okay we will only consider cases
where the database match doesn't differ
by more than fifteen percent of the
query length from the query because in
several publications that we have seen
them with several biologists we have
actually talked or exchanged messages
with it seems that this is the kind of
query that the most lot they are most
interested in supporting so a simple
version of the algorithm that we perform
then is that essentially as
pre-processing we choose some reference
objects we pre compute the embedding of
every database position according to
each of the reference objects and then
given a query we pretty much do a filter
step as a nested for loop where for
every database position for every
reference object you see if using that
reference object you can prune away that
database position so this is a
simplified version of the algorithm that
works kind of reasonably well there is a
more complicated version that is
actually described in our upcoming paper
but essentially this is the essence that
by using these simple embeddings we can
prune out large parts of the database
now a topic here thats kind of subtle is
what is the right length for the
reference objects right so so far we
haven't really said anything about
whether the queries and the referenced
objects could be let's say ten letters
long or a thousand letters long so one
thing to notice is that the way we embed
the query essentially we ask the
question what is the best matching
suffix of the query for the given
reference sequence right so if let's say
the query is a thousand letters and the
reference sequence is ten letters this
number will pretty much only tell us
something about the last few letters of
the query right
so ideally you would like referenced
objects to have the same length as the
query more or less to give you
information about the entire query at
the same time if let's say your query is
a hundred or more letters and you start
using reference of Zotero this long what
you end up seeing is what is one of the
many symptoms of the curse of
dimensionality which is that pretty much
no matter what your queries and what
your reference object is mostly you get
the same distances or the distance is
not very enough and the problem with
that is that if the distance is don't
vary enough that the lower bounds that
you get here are not high enough right
so basically by just trying different
things we came up with the magic number
of 40 we just seem to work very well now
so what happens now if you use reference
objects of length 40 but your queries
really of length a thousand the answer
here is to break the query into chunks
so do whatever we talked about before
but do it with every chunk separately so
find close matches for every one of the
chunks and then the idea is that every
close match that you find for a chunk is
a candidate for a close match with the
entire square e sequence so at the end
you just go back and you evaluate the
area around those chunks to simplify the
good match for the entire sequence and
this approach is exact and it works
fairly well it actually outperform
competitors I'll show the results in a
bit but it turns out that if your query
is really large you can actually do
something better so here's the question
so suppose I have a really long query
let's say 10,000 letters and I break it
up into chunks of segments of 40 letters
each so what would happen if I only
chose a single one of those chunks and
I've tried to find the close match for
just that one and I ignore the rest of
the query how likely so if I was able to
find all that let's say I'm only
interested in matches that are within
fifteen percent of the query in terms of
the edit distance okay and suppose that
so fifteen percent of a query length of
ten thousand would be 1500 characters
evaded distance so now fifteen percent
for a chunk of 40 characters would be
six right so suppose that I get my chunk
and I find in the database all the sub
sequences that are within an edit
distance of
from that chunk so how likely are we to
have included in those chunks something
that actually include that is part of
the correct match for the entire query
so it actually turns out that by using
some fairly simple properties of
binomial distributions and under some
fairly realistic assumptions this the
chance of finding of hitting the match
that were really looking for for the
entire bigquery is over fifty percent
and which means and actually if we set
up a chunky use two chunks then the
chance of missing it is zero-point 25 if
you use ten chunks the chance of missing
the correct match is less than one over
1,000 so it turns out that actually if
you have really long queries but you
know that your magic with is within a
certain range of the query it's actually
you mostly don't need to look at the
hoch where you can just look at the huge
chunk so that will give you a very very
likely to be correct answer so the
experiments of your run here are with
human chromosome 21 whatever that is I
guess as far as I'm not a biologist so
this is just some words to me but
essentially this is a string of 35
million characters and it comes from web
from the human genome now as queries we
use strings from the mouse genome so we
looked for Strings in the mouse genome
that had matches within fifteen percent
at the human genome and that's what we
use the square ease and the similarity
measure we use here was smith-waterman
so pretty much everything I've talked
about before applies both for the edit
distance or for smith-waterman you can
kind of pick and choose depending what
you want to do so this tells us that we
get we tried with query lengths of 4200
two thousand and ten thousand so here
ARB sa it stands for the approximate
version of our method where basically if
you have a query of length 10,000 that
corresponds to 250 chunks of size 40 we
only use ten of those chunks at least
for the filters that we only look at the
inn to remain der of the query to
evaluate the to verify that we really
got the goods
match for the whole query so this is the
approximate version er BSA is the exact
version where basically you try all the
chunks bwt is one of the probably most
well-known competitors or a
state-of-the-art method for matching in
DNA for subsequence matching in DNA
databases and these are three different
parameters for blast which is by far the
most well known algorithm for matching
in DNA databases and the numbers next to
blast indicate the accuracy so notice
that blast does not necessarily provide
the guarantee that you will find what
you're looking for right but you can set
up its parameters to make sure that your
results are correct for let's say ninety
five percent of the queries or ninety
eight percent of the queries or even one
hundred percent of the queries so these
are at the three different settings that
we tried so we know that actually for
short queries of length 40 bw2 duck
shoot did much better than we did almost
an order of magnitude better once we go
to queries of like 200 and more actually
the exact version of our method which
guarantees finding the correct result
starts performing better the interesting
thing is that exactly because in the
approximate person we really don't you
really only need to look at 10 chunks it
doesn't matter how long your queries
that becomes much much faster than any
of the other methods for example for
queries of length 10,000 our method took
0.08 e six percent of the time we would
have taken for brute force search to
find a subsequence match here and for
comparison and our method was correct
ninety-nine point seventy five percent
of the times and again it's approximate
we look at ten chunks you can always get
unlucky it if we compare this to blast
with ninety-five percent accuracy it's
about 100 times lower so again this is
just one of the results that we got with
that we got many different results both
which myth Waterman NP edit distance we
also compared to other competitors like
Q grams that also turned out not to work
well when your queries become longer and
we have many of these results in our
paper that is actually posted on our web
pages ok are we doing timewise
okay 10 minutes so basically so this is
something with different strings with
this something now I mentioned before
that if you actually work with time
series computing the dynamic time
warping this is between time series it's
really really similar to compute the
edit distance between strings which
means that the seneschal we can apply
the same exact method with reference
sequence to time series so basically
we'll fast forward all the details but
again we tried again some results with
this and we had these results in the
paper we had in sigmod last year where
essentially let's say if we were willing
to sacrifice 10% of retrieval accuracy
in finding subsequence matches we could
about 50 times better than brute force
search okay and the last part of the
talk is going to be switching gears a
little bit so now I'm going to talk
about the facial recognition in cases
where you have a large number of classes
so again as an application or the
easiest example I can think of this face
recognition you might want to actually
recognize a large number of people in
cases where you want to recognize
celebrities for a search engine or let's
say you're in a surveillance system for
the police or the FBI and there is
thousands of missing people or suspected
criminals that you might want to spot
another case where multi-class
recognition is useful is for things like
hand and body pose estimation so we can
take a lot of different poses thousands
or even millions perhaps with our hands
on our bodies and being able to
recognize those could be very useful for
things like sign language recognition or
human computer interaction I guess many
of you may be familiar with the Nintendo
Wii and how it tries to estimate some
motion right it would be really cool if
you could just do things like that the
computer would really know what you do
and give the you know glow of death to
the opponent that you're playing with
and stuff like that so this is another
case where it would be nice to be able
to recognize a very large number of
different classes one common approach I
mean among many but this is just one
approach for doing this is what is
called one versus all classification
where basically you train a different
classifier for each one of the classes
so let's say if you want to recognize
Kobe Bryant you build a classifier that
just says okay is this Kobe Bryant or
somebody else and that's all it cares
about so if you want to recognize 10,000
people you build 10,000
of these classifiers and that sounds
like the actual people have done for
multi-class recognition although
probably not for ten thousand classes
partly because not many people have
actually tried to recognize 10,000
individuals and the plus is that each of
these classifiers can be trained using a
state-of-the-art method like boosting
methods or support vector machines and
people are happy with how well those
methods work but the question is so what
do you do when you have 10,000 classes
the standard approach is to just apply
each one of those classifiers in
sequence which is not very efficient so
we actually have a method that we can
apply on top of a specific ova based
method which is called join boost so
join puss was introduced by torralba
Murphy and Freeman from MIT in 2004 so
pretty much the boosting method by the
way just know how many of you are
familiar with boosting methods 12 guys
okay so basically the idea is that again
as I mentioned before in the description
of our embedding method i boosted
classifier is really a weighted linear
combination of weak classifiers right so
each of the little H is here is a way
classifier these guys are weights and
this is just an extra term that you can
net or subtract for every class to
introduce a bias for example for
computer vision a simple weak classifier
it could be something like sum up all
the intensities here a subtraction them
all they said the intensities from here
right so the idea is that this kind of
weak classifier is not very accurate but
again it should be better than random
and you can probably define thousands or
millions source of such weak classifiers
and then adaboost can find the best out
of them and combine them in a good
strong classifier now enjoyed boost the
main difference from the standard
adaboost or boosting paradigm is that
basically if you have 10,000 classes to
recognize you force all those
classifiers to use the same weak
classifiers so notice that let's say if
Y here stands for Kobe Bryant so this is
the classifier that recognizes Kobe
Bryant there is no y subscript for the
age so the way classifiers are shared
what is different so what makes this
classifier look for Kobe Bryant and not
plus a Michael Jordan is the alphas
right so the alphas the weight
are what makes everyone vs 0 class very
different from the other one they share
the same features the same week last
fires but they combine them in different
ways so basically then in the standard
approach if you build 10,000 of those
ova classifiers you just apply each of
them to the query and you decide what
person it is basically you try to find
the one versus all classifier that gives
the strongest response that is the most
coffee that yes this picture really
comes from my guy so in our work we just
presented that cvpr last month it's a
more efficient alternative for doing
this kind of task so how can you find
the winning classifier without trying
all of them so the summary of the
approach is that we will define a
mapping that will map every classifier
to a vector and every test image into a
vector in such a way that the winning
classifier will map to the nearest
neighbor of the query so let's see how
we do that so if this is the let's again
if this is the classifier for Kobe
Bryant this will be the mapping to a
vector of that classifier so basically
if we use D we classify so let's say if
we use a hundred we classifiers the
first hundred dimensions are just the
weight of those weak classifiers right
this is what makes this one versus all
class very unique the hundred and first
dimension would be this bias term and
the hundred so basically soph up to here
everything is straight forward we just
use the concepts that appear here this
extra dimension is that the measure that
we will use to just ensure that all our
let's say 10,000 one versus all
classifiers have the same norm right so
you can always if you give me vectors I
can always a set of vectors I can always
add another dimension to them to ensure
that they all expand that they cover the
surface of your hyper sphere ok now I'm
going to skip the math because of time
but this is it and you can see it in the
paper from how to do that now for the
query image I will also define mapping I
could actually define it in two stages
this is a preliminary mapping like we
call V original of the query we're
basically again if
we have a hundred we classifiers we
compute the responses of those weak
classifiers and this will give us the
first hundred dimensions of the query
then the second to last dimension is a
one and the last one is a zero now what
do we do it like this basically if you
want to compute the response of this
class fire on this query this is just a
dot product between the mapping of the
classifier and the mapping of the query
right if we take the dot product
basically every week classifier response
is lined up with a weight or that we
classifier right the bias term is lined
up with the one and this extra term that
we added this lines up with a zero so it
doesn't make any difference okay so
basically finding the winning classifier
is the same as finding for this
particular query vector the vector among
the data base vectors that maximizes the
dot product now the second step is to
take the square e vector and normalize
it so that it's norm is also the same as
the norm of these guys right remember
that we made sure by adding this
constant here that all these guys have
the same norm that we call n max so by
doing this normalization we ensure that
this guy also has Norman max notice that
by doing this we scale all the dot
products between the query vector and
the classifier vectors by the same
factor right so whatever was the winning
vector before is the winning vector
still so still the winning ova
classifier corresponds to the vector
that maximizes the dot product however
one last step is that once we have this
mapping now the V of the cloud the
mapping of the classifiers and the
mapping of the query all of them have
the same norm so they all live on the
surface of some hyper sphere that is
centered at the origin so in that case
maximizing the dot product is the same
as minimizing the Euclidean distance so
here we have actually mapped and
actually to our surprise we just
realized later that hey this is not
something that just works for joint
booster pro rata boost in any place
where you actually want to find a vector
in the database that maximizes the dot
product this way you can map that to
minimizing the Euclidean distance and
the nice thing about that is that well
how many methods do you know for index
endowed products personally I think I
know zero
and on the other hand for indexing the
euclidean distance there's tons of
different methods so you can just take
them and apply them oh really yeah okay
that we interesting to talk about
offline because the actual I was
actually looking the literature and just
wanting to know do those things assume
that they all have the same normal they
don't care about that right so if it's
that's the thing so cosine similarity
works only if everything has the same
norm so in our case we mapped everything
to the same norm but that was our own
mapping right if you just take the
original vectors they don't have the
same norm so you cannot apply this
cosine similarity yeah so basically the
idea is that on the other hand for
Euclidean this we can lower with that
there is tons and tons of methods that
you can use to index it so you can apply
pretty much any one of them and try to
gain some computational efficiency and I
will skip the details but basically we
just did a very simple method that's
based on PCA where we basically use PCA
we went to very low dimensional vectors
we found some candidates based on that
and that we evaluated each of those
candidates in the original space and
just to show some very quick results we
apply this on the data set of hand shape
images were compared to joint boost the
speed up factor was 120 and actually
compared to another method that we
presented two years ago that we call
class map it was still much faster that
method will give a speed of about 30 and
four phases actually this is kind of a
tricky result with the reviewers what it
turned out that we had we took basically
the largest face this latest we could
find which is the face recognition grand
challenge data set that has 535 classes
and there it didn't quite work very well
with we only got a speed-up of 1.6 and
for some drop in classification accuracy
so we actually wanted to look at what's
wrong and why does our method work that
bad we found that actually most of the
time was spent on computing the PC a
projection of the query which is part of
the indexing method that we use and
essentially that is a cost that has
nothing to do with a number of classes
right if instead of 530
class if you get let's say 10,000
classes this course would be the same
but everything else would scale up so if
we take out this projection cost and it
turns out that we get the speed-up of a
bit over an order of magnitude over
basically just trying every possible way
classify every possible ova classifier
to see which one wins so basically into
and this is pretty much the last slide
for future work here this is just a
pretty new thing we've been playing with
the indexing scheme that we use the
space on PCA is probably not the state
of the art so we feel that probably if
we tried more fancy indexing schemes
here we should get even better speed ups
and at the same time as I said before
this is a general method for indexing
dot products even in cases where they
don't all have the same nerve and that's
something that we're kind of interested
in finding applications for that we
found that many cases where you have
built databases using SVD let's say in
the Netflix contest where with this V do
you map every user to a vector and every
movie to a vector and that you want to
find basically the movie that the user
would like the best is the vector that
would maximize the dot product with a
user vector these are all cases where we
could apply this kind of reduction to
map it into a Euclidean indexing problem
okay so I guess this is pretty much it
and thanks very buddy for attending so
in questions using the distances from
so it seemed like first I'll grab you
take those distances from the exam
course and then you basically use them
as your feature vector then you cut your
calories consumed itself right but you
are typically only till joining the only
thing that you care about as far as
those distances our concern is the
ordering so if you take you to do those
distances that for example square then
we'll apply the law or whatever yeah it
wouldn't matter all right but human
height of distances them to change right
no we haven't but we could I mean this
is an extremely valid thing I always
said it at the back of my medal this
would be a nice thing to try we could
even use a hybrid distance by exactly
doing these things you could use things
where you part of the distance is not
hat and part of it is square toward any
power basically and yeah I mean yeah
right right right so basically I did
that essentially you could even say that
if you look at the level of the wii
classifier right that just looks at
distance to a single object right you
can square them you can do any tar
transformation to them right so in a way
that can be used to enhance the pool of
weak classifiers right and yeah so that
would make sense and I it wouldn't hurt
I mean most likely would give you better
results it's just that you know I had to
graduate move on to other here so
whenever it by this a with a title it
what makes it if I were to let's say
make a product this is one of the things
that would be the first to try to
further improve performance any other
questions
okay cool so thanks everybody for coming</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>