<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Matt Deans: The Exploration Ground Data System (xGDS) | Coder Coacher - Coaching Coders</title><meta content="Matt Deans: The Exploration Ground Data System (xGDS) - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Matt Deans: The Exploration Ground Data System (xGDS)</b></h2><h5 class="post__date">2013-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CrDdtVEJyCk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Matthew Dean's is not a wrestler from
the Midwest that's correct
nor is he dr. Matthews see Dean's who's
another NASA researcher who we have no
plans to bring out here from what I
understand but he does work on the
exploration ground data system which
from what I can tell and what I think is
interesting as well it has come into
play most recently I guess here in
Hawaii it says this is a great scam
instead of going to Mars where it's
really cold and hostile of life you get
a Hawaii and they test out your software
in Hawaii was Fiji close to you because
it was an American or have you sort of
had the Maldives
you know it's baffling and American
territory it's Devon island station
right so you can go to other countries
have you thought of testing your stuff
and I don't know Ibiza or I'm just
saying I could red team that for you um
okay but this is hi I'm Chris DiBona I
help look after the the the NASA Ames
specific relationship between Google and
that August institution and join me in
welcoming to our NASA speaker series dr.
Matthew Dean's so dr. Matthew Dean's not
math you see Dean's what's her middle
name that's really awkward
there's two matthew seed yes email Wow
okay well I don't do there's anything I
can add and we're really early imported
learning from you today so thanks for
coming
thank you for that excellent
introduction okay so I'm talk to you
today about the exploration ground data
system which is a research project has
been going on for several years now and
it's basically looking at how to build
software tools to help future NASA
missions get their get their work done
this is a picture of the moon this is
during the Apollo exploration of the
moon this is sort of the thing that NASA
is best known for is these sort of
really impressive images of this trip to
the there's pretty good evidence that
we've been to the moon there's there are
some debate about that but I'm one of
the people who's convinced by it and
NASA is currently thinking about
exploring going back to the moon going
back to the moon with robots going
deeper into space with human beings and
a lot has changed since the Apollo
missions and we're trying to figure out
how to incorporate some of that new
technology some that new thinking some
new approaches to to how NASA does its
exploration so nASA has of course been
many places since the Apollo era lots of
satellites have gone and taken
high-resolution data of the places that
we want to go in the future we have lots
of cool tools like the moon in Google
Earth that you know builds much better
representations of what the moon looks
like before we might go there so we have
a lot of opportunities to build up
better Maps than we had during the
Apollo era to plan our exploration and
of course we've been to the surface of
Mars and there are three Rovers there
now and there's been a lot of technology
developed on earth that's developed for
space like the the Dante walking robot
and the and the Zoe Rover and a bunch of
these NASA systems that have been
developed over the last several years
and these technologies were all designed
with an intent of improving the way that
NASA explores other planets and of
course missing from this slide is a
bunch of other stuff that's happening
right here on earth development of
mobile technology and smart phones and
Google Earth and all these great things
that we can leverage to do NASA's work
better so we're trying to build these
systems that basically help missions do
all the things that they have to do
before they go to the place that they're
going to explore while they're actually
carrying out their mission and then
after the mission is over and so we had
these themes of planning and planning
really takes place for
well before you actually go to the place
that you're going but it also happens
while you're doing your work in that
place so you're you're replanting and
you're thinking over again about your
how you're going to maximize your time
there wherever there is monitoring
happens from the minute you hit the
ground until the minute that you leave
and exploring your data and
understanding your data begins the
minute you start getting information
back from this place and it continues on
into perpetuity and so this is the
challenge here is to is to really look
at all these different phases of how you
how you how you explore and building
better tools I'm not going to dwell on
this architecture diagram because I want
to build it up piece by piece and I'm
gonna talk a little bit about how it all
comes together or how it all came
together but this is the picture that
I'm gonna end up with at the end and the
way that we do that is that we so NASA
has these technology programs and
Technology programs purpose is to build
these technologies but there's not
always a good driving use-case for the
building of the technology and we often
miss things like what do our users or
what might our future users really need
unless we can find a current day user
who can use the tools in the way that
we're trying to design them and then
understand better what they're getting
out of using them and what they need
those tool to do and then we iterate
with them and so we've gone off and
we've found our own customers to do this
work I'm gonna drill into each one of
these pictures and talk about what these
use cases are what these science teams
are trying to accomplish and then how
we're building the tools that help them
do that so I'll first talk about the k10
Rovers and these are robots that we have
built at NASA Ames and we have taken to
several different places around the
world to explore the Arctic and to
explore the desert in the southwest of
the US and either be too small Rovers
they're approximately the size of the
mer Rovers maybe a little smaller and
the idea that we had with this
experiment was to go off and do what we
were calling robotic reconnaissance and
the idea is that if you want to send
people back to the moon then a really
effective way to to carry out that
mission would be to first send robots
and gather a bunch more information
about what's there and then use that
information to
a more effective human mission where you
you have a lot more information to to
make that plan with and so we've done
that mission out in the in the desert
Southwest we sent the robots out we did
a bunch of exploring of the lava flow in
Arizona and then we followed that up
with the crew going out and I'll talk
about that in a moment with a human crew
going out in a crew vehicle and
exploring the same place that the robot
had been and using all that information
they got from the robot and we looked at
what's the what's the impact that we can
bring these are the k10 Rovers we have
two of them ones k10 red one is k10
black they have a wide variety of
instrumentation on board they have
cameras and inclination compasses and
GPS unit and laser light stripers for
obstacle avoidance there's a GigaPan at
the top of the mast that that's for
taking a wide-angle high-resolution
panoramas a lidar scanner on top of the
k10 red that box is a high-powered 3d
laser scanner builds very high
resolution 3d terrain model of the
environment a ground-penetrating radar
this box in the bottom of the k10 black
is a neutron spectrometer which works by
firing high-energy neutrons into the
ground and when they bounce off of
hydrogen atoms and bounce back up to the
detector they're going a lot slower than
they were when they were emitted by the
neutron source and you can detect this
presence of hydrogen under the surface
so we carry this wide variety of
scientific payloads and we've worked
with the science teams to understand
what those instruments are and how they
work and what they're telling them about
the science this picture on the on the
Left k10 black again is has a dynamic
cone finish our it's drilling into the
ground and it's using information about
how hard it is to drill into the ground
to get a better understanding of what
the surface properties are like like the
bearing strength of the soil and then on
the right hand side this that's a
microscopic imager which takes you know
a few tens of microns per pixel images
of the ground and again this is to get
very high resolution information about
what the soil types are what the rock
types are on the surface for the
geologists so the test objectives here
were to for us they were to test the
rover technologies themselves how well
does the rover get around how well does
it avoid obstacles how well does it get
to where
places that we wanted to go we also
wanted to understand things like what's
the value of robotic reconnaissance for
human mission
what's the cost-benefit look like for
doing a human mission to the moon
without robotic precursors or with
robotic precursors and then thirdly for
the science team the goal here was to do
geology they were trying to do
wide-scale geomorphology and understand
the geology of the site so the challenge
is that doing this are that we have a
real time science interaction with the
robot so unlike the Mars rovers where
you give it a set of things to do
tonight and then tomorrow it wakes up
and it does them and then at the end of
the day it dumps back all the data that
we collected for the day and then your
science team wakes up and they grab the
data that came in and they think about
it and they turn on it and they argue
about it and they decide what to do
tomorrow and then they send another
command up and the robot wakes up and
you sort of repeat this cycle and if you
are gonna put a robot on the moon you've
got real-time interaction you have one
second of light propagation time delay
not 40 minutes and you have continuous
operations because it's always daylight
on the moon for two weeks until the Sun
Goes Down and it's dark for two weeks so
how do you facilitate that real-time
science interaction to keep the
scientists in the loop all the time and
keep them aware of what's happening all
the time the second thing is real-time
access to that mission data at the sit
for the same reasons they have to be
incorporating the latest and greatest of
every single piece of data that comes in
and it's used to evaluate what's
happening update their hypotheses think
about what the most effective thing is
to do next the third challenge for us
was denied design these systems for non
roboticists I mean I work in a research
group of about 30 people who were all
roboticist and sometimes we build tools
for ourselves and we forget that we're
all expert users and that no one else
can really understand how these things
how these things work and so what we
needed to keep in mind was that the
robot is not the point the robot is a
means to an end the robot is the way for
the science the scientists to understand
what's going on and to sort of
facilitate their ability to remotely
explore this place and what that meant
was we were also designing for novice
users and so when when we do these
terrestrial analog missions we don't
have a lot of time to train the
scientists to use the interfaces they
show up they work with us for a few
weeks
and then they go back to their home
institutions and we have to have tools
that are easy for them to use so that
they can come in and spin up quickly and
then go off and use these things so one
of the first things that we
realized was a challenge for a non
roboticist was how do we tell the robot
where we wanted to go and what we wanted
to do and we hadn't really thought about
this before because we always just sort
of like you know drive a robot around on
our test yard and and and we understand
how to you know program scripts and
stuff like that so
crickets we had no idea actually how we
were going to do this the first idea we
had was to walk around with a GPS unit
and figure out where we were going and
then download the coordinates and given
to the robot and say now go do that but
that's kind of silly because then you
have to actually go the place where you
want the robot to go it didn't really
make sense so the obvious thing was to
draw a plan on a map and then figure out
how to convert that into a theme they
begin setting off to the robot so enter
Google Earth right so this is like
probably about six years ago or so
Google Earth was by then already
available it was an obvious thing for us
to leverage and use we didn't have to
build the UI we didn't have to develop
the map content we can just use this
this platform it was a great thing for
us to leverage and so the way that this
tool worked was that we would just
literally just draw a line we would open
up the you know add a polygon tool and
we would draw on the map and then we
would save that out to a KML file and
then we would you know run this little
script on it they would turn it into a
thing that the robot could actually
execute so the robot now has coordinates
and a coordinate frame that understands
and it's ready to go the problem is that
that kind of plan only has two things in
it it has a bunch of coordinates and it
has an order to visit them in but it
doesn't have any idea about what am I
supposed to do when I get there how long
is this going to take are there
resources involved none of that stuff
and so it became pretty obvious pretty
quickly this is a very limited way of
thinking about planning so what we
wanted to do is build a better planner
than that so we thought about what
actually needs to go into the
representation to the plan which is all
these other things that that was missing
the waypoints ordering objectives
timelines or tasks that need to be
accomplished the resources required to
do that and then separate from that
think about what is the right way to
edit and view that thing and it's not
necessarily to do it in the same place
where you're representing the thing so
when you're holding a hammer everything
looks like a nail
and there was one hook in the Google
Earth that was introduced
girl Earth 4 which was the the API for
just a very small number of things that
we could ask or tell Google Earth and
one of them was we could ask it where's
the center of the screen and so the way
that we built this system was that now
okay on the left-hand side of this
screen you've got this Python tkinter UI
panel and inside this Python script is a
full representation of all the things
that you care about in the plan and on
the right hand side you've got Google
Earth 4 and Google Earth 4 is I roll in
this design was just to show you a view
of the plan on the map so you you ask
Google Earth plays up in this file and
in fact that's not even an API call
because this is you tell the Mac open
file and then it knows that tell Google
Earth open that file and then Google
Earth goes and opens that link file and
that link file actually points to
something else that points to the actual
plan KML file and now we can in our plan
developing tool we can write out this
KML file to the disk and then Google
Earth will keep on pulling the file
system in getting new copies of it and
then inside of our planning tool we can
change what's in the plan and change the
kml view of what's in that plan and it
just keeps on getting loaded into Google
Earth and then periodically when we
needed to we would ask Google Earth
where's the center of the screen and it
would just come back with here's the
center of the screen and this is the
only API call that we needed in order to
build this tool so this is a tool that
we started calling for lack of a better
racket better item imagination we called
it Geoff's Google Earth props and the
idea was that you would go move the map
around until the crosshairs were in the
place where you wanted the robot to do
something and then you would configure
what you want the robot to do and then
click on the button on the left hand
side and that would insert a task into
the plan according to all the little
configuration buttons and pulldown menus
in the UI and the trickiest bit of that
was actually tasks that require an
orientation because if you want to take
a picture of something you can't just go
to an XY location and take a picture you
have to take a picture looking in the
right direction and so we even had this
little busy loop where one thread is
writing to the file and the other thread
is waiting for the user to to tell the
UI that just now pointed in the right
direction so we add this Waypoint here
it's asking to take a GigaPan image and
then we move the map around say take a
picture of this thing and it reorients
the waypoint to point it the thing we
want to take a picture of okay so this
is a much more effective tool much more
powerful much more useful and here's an
example of what one of these plans would
look like for that mission that we did
in the desert so we've got a timeline
key at the top now we know how long each
task takes what's the total duration of
the entire plan and this helps the
science team understand what do I have
time to get all this stuff done or when
will it be done we've got the list view
of stuff that that the robot needs to do
in the in the list at the bottom of that
left-hand side we've got a map D that
tells us where's it gonna go and what's
it gonna look at my guess there so this
was a big win and I won't get into all
the data displays and stuff like that
that we've built for the k10 Rover
because I'll have more better stuff
about that in just a moment but here is
where we wound up at the end of that
experiment so a couple of things that we
learned from that word that you know
building all these tools on open
standards and open source available
software was a great win for us because
it was a very small development effort
and we got up and running very quickly
and second thing is that using Google
Earth helped us greatly because we don't
have to write a lot of the code we focus
on writing the content it's familiar to
our users our science team users were
cut would show up and they're ready to
use Google Earth because they've already
used that they use it at home and you
know the fact that it's all based on KML
which is an open standard and easy to
write KML files made it easy for us to
integrate and it made and in this little
simple API of asking Google Earth you
know where's the Sun of the screen
helped us build this much more
sophisticated tool than you know
probably the people who built that API
didn't ever think that somebody would
use it in that way so the next project
I'll talk about is Desert Rats and this
is a mission that followed up the the
rover in the desert with a crew vehicle
with simulated astronauts who would
climb out of the back of the crew
vehicle and go off and do evey A's on
the ground and they would they would
carry sample bags and rock hammers and
they had a headset they could talk in
real-time to the people in the back room
they actually climbed through the back
of the suit so that rather than going
through an airlock there's a hole in the
back of their backpack and they slot
they slide in and then they shut the
door behind them
seals against the the back of the
vehicle and then they'd attach their
suit from the from the truck and they
get out and they walk around and the way
that they operated this mission was that
they had this idea of a tactical science
operation team and in a strategic
science operation team and the idea was
that you had people who were awake and
on console and working as the
crewmembers were driving around and
doing their epa's and they were there
sort of you know their reference back
homes talk about the plan and make sure
they weren't keeping on time and
resources not too high stuff but then
separately and those people would burn
out in eight hours they were burned out
they were done and then they would bring
on this second science team and that
that science team was like working a
swing ship they would get up and they
would show up at the meeting the
handover meeting and they would get a
whole bunch of data and it would be
asked to sift through it all and figure
out what happened in the last eight
hours while the crew was operating and
they come up with a new plan for
tomorrow based on everything that
happened and so they were coming in cold
and they had a lot to figure out quickly
so we had lots of moving parts we had
lots of console operators or 30 or 40
people all the time working on this
thing and we had this issue of the
overnight strategic team understanding
what happened and replanting everything
in just a couple of hours and they
weren't even round for when that thing
happened so our job was again to help
them build help build tools to help them
do this work better one of the first
things that we built for them was a
console log and the way that this worked
is that it would just pull up a little
interface on the web on a web browser
where they could actually let me
describe how they do it now what the way
that NASA does this now in the Mission
Operations Directorate at JSC otherwise
known as Mission Control the way the way
they often do this is they have
Microsoft Word documents with macros and
the coke little macro button and it
pre-populates a area on the board
document they type in what they want and
when they're done at the end of their
console ship they save that word
document to the file system and it
doesn't connect to anybody else's word
document and there's Word documents all
over the place and no one can integrate
them all look at them all they probably
you know pull up those records when they
need to like if an archive to go dig
around and figure out what happened in
the past but they don't really have a
you know streamlined integrated thing so
but we needed that because we needed we
needed the strategic team to have a
quick sense of what happened and they're
not going to go read a bunch of Word
documents in order to do that so we
built this console log that basically
every person loads up you know they
fight their web browser at the same
location and you authenticate as you and
it knows who you are it knows where what
console you're sitting at and when you
create a note it knows what time you
created that note and then you can type
kind of whatever you want and during
this mission we also had a naming
convention for all the data that came
back so if there's an image or a video
then there's a naming convention for the
data product and if you pasted the data
product name into your note then it
would automatically you know parse that
out was like a regular expression you
could tell that that's what that thing
was and then we create a link
automatically into the data archive so
you could have the the note that you
typed a validated product and the data
product itself all integrated into the
same into the same place so now anybody
who's on that strategic science team can
just show up open up this console log
and they can just chronologically just
read right through everything that
happen chronologically see it all in
order see all the images to help them
understand what happened and this helped
them greatly get their work done the
second thing is that we built these map
displays of all of the things that
happen so every place that the rover
drove is tracked with these little cyan
lines on the ground we have a complete
track of everywhere that it's been where
the vehicle is now or where it was
parked at the end of the day is in this
the icons there the vehicle icons every
data product that was captured during
the day we actually got two pieces of
information we got a piece of metadata
first and it would tell us what time was
it taken who took it where was it taken
all those quick kinds of things and then
the second thing was the actual image or
the actual video itself and so if we got
the metadata and we don't have the data
product yet we still know something is
there so we would put a place mark in
the map and we would make the icon for
it red which means it's not there I mean
this is where it came from but we don't
have the data yet and then the green
ones are ones where this is where it
came from and we do have the content so
if you click on that place mark you can
pull it up and see the video CD image
and it's ready to look at and of course
the plan so the plan for the day is the
yellow line and then the blue and the
place marks and everything that's
actually what happened one of the sort
of tricky things about this was that
there's this issue with with with Google
Earth which is if you
a whole bunch of placemarks in one place
which is what happens when a crew member
hops out of this vehicle and they walk
around they don't get very far on foot
and they take a lot of samples to take a
lot of images take a lot of video so
what happens is you try to look at this
stuff then you get this these place
marks kind of scatter as a pile of them
if you now you click on one of them and
you look at it and then when you move
your mouse focus away then the pile
classes back onto itself and you have no
idea which one one of the things that
you looked at and which ones are things
you didn't look at and so we had to come
up with a little bit better way of
organizing this information which was to
draw a bounding box around the entire
evie a so they would hop out of the
vehicle and they walk around they would
do what they do and then we would know
when do they start when do they end
what's all the data we care about from
that chunk of time what's the bounding
box that encapsulates all that
information and then build a much more
complex description for the bounding box
and now if you click on the box you can
sort through all the things that happen
chronologically rather than having to go
try to you know find each one of those
icons and click on it and then another
little plug here is that we in addition
to the those two interfaces I just show
which are using the earth client there
was a separate project going on at NASA
at the same time it was the Google NASA
Disaster Response project and they were
building a tool called share and this is
a couple of use of share looking at
wildfires and looking at responding to
building collapse of emergency and we
basically got to take this product that
was built for another project and figure
out how to plug it into our architecture
so that we can use it for science
purposes as well this is a image of the
of share used for Disaster Response in
Haiti and so we got this just plug
that's into our architecture and now
it's being used by a science team to do
field geology instead of by Disaster
Response crews to be doing disaster
response and so at the end of that
mission we have now more tools plugged
into our architecture now we have
planning tools tracking tools data
organization and search tools map
generating tools all this stuff so as a
result you know we built these tools
that did actually help people understand
very quickly what
and we and we moved away from having all
the data be generated by scripts and
dumped onto a file system and we moved
to a Django my sequel back to sort of
approach so everything can be generated
dynamically there's a lot cleaner or not
easier for us to organize the data and
keep it keep it straight and having that
streety science team was a really
powerful driver for us because we
thought we had a pretty idea of how to
build things that would help people get
this kind of thing done but we didn't
really know what we didn't know until we
tried to actually do that okay the next
project I'll talk about is the pavilion
like research project and this is a team
of people who are exploring lakes in
Canada and if this submarine looks
familiar to you if you've ever seen the
80s movie the abyss the company that
built these submarines is the same
company that built some of the hardware
that was used to film that movie both
the harbor in the movie as well as the
harbor that was used to you know
actually shoot the shoot at the film and
this is Darlene lamb she's the PI the
sign of that she's a PI I of the project
she is illuminologists and what they're
doing is studying the the bottom of this
lake there are these things that are
called microbialites I'll show them in
just one second and they're these rocks
that form actually when I was go ahead
and show you show you they're these
rocks that form over time and it's they
believe that they're that they're formed
in the presence of biology that there is
sediment being trapped by microbial mats
and that the conditions in the water the
fiscal conditions the chemical
conditions sedimentation rates
temperatures light all these things that
affect what types of microorganisms grow
and then the types of microorganisms
affect what shapes of rocks form as a
result and so their working hypothesis
is if we can figure out what conditions
in the lake cause these things to make
certain shapes of rocks then we can we
can look around and find the rocks and
then reverse engineer what were the
conditions that existed in this place
when this rock was formed and some of
them might've been formed very very long
ago and they can do things like carbon
date them to figure out when did this
form and then sort of recreate the
history of the lake so so they're
looking at my carbonate formation and
they're looking at Lake geology and
biology
and one of the cool things about working
with them is this is actually a science
different project so the k10 Rovers and
the Desert Rats these are things where
we were creating a science objective for
ourselves and then going through the
motion of science operations it wasn't
actually a brand-new scientific
investigation people had been to this
lava flow in Arizona before and they
already knew about it this is something
that no one really understands yet and
this this science team is trying to
really get new brand-new work done
they're a very distributed science team
there's lots of people from universities
in Canada the US NASA centers company
partners they're all over the place they
plan this thing far in advance rather
they don't just come together for a
quick field test season and start their
planning they plan for months before
they get out there they actually analyze
their data afterward they care about the
scientific data that they're that
they're collecting in the lake their
short trip to the lake is about getting
the data that they need in order to do
their work their work is to analyze that
data over months and write scientific
publications and get journals out and
that kind of stuff and it's a multi-year
campaign they've been doing this for
years and they need to use data that
they collected last year to plan for
this year so this is our old friend
Geoff's and it turned out for lots of
reasons that this was not sufficient for
what they needed to do so you know I
talked before about all the things that
Geoff's got us to add to our planning
tool but it didn't have rich interaction
didn't have a fast interaction it wasn't
platform independent you had to run it
on a Mac it was a thick client you had
to install software on your Mac in order
to run it so we wanted to make it easier
for them to anybody can use this thing
it's a server somewhere else you all
point your web browsers to it and you
can make plans and the last thing that
that it didn't have which ended up being
really could be a nightmare is version
control so if I make a plan and then I
copy it to a server and then somebody
else makes a plan and which version do
it do I need and do we have well you
know I'll have the same version all this
kind of stuff so this is much more of
like a you know Google Docs kind of
analogy we put the plan in a place where
anyone can get it they all have edit
access and then if anybody updates it
then you don't wonder about whether or
not you have the latest version so so we
built a web-based planner and this is
what it looks like it uses the earth
plug-in again rather than the desktop
Earth client and it has a lot of the
same things that Geoff's has it's just
and built in a web-based web-based form
so there's a map on left hand side you
can draw where you want to go you can
get estimates of the time line that that
will take by giving it an average
vehicle speed for example and you can
enter a bunch of other information about
the plan like who's who is planning it
what's the purpose of this plan give it
a name if you'd like and then rename the
waypoints and there's this set of this
box in the bottom right hand side here
is a list of the waypoints and the order
in which they're gonna happen and any
naming our notes about why you're doing
this thing or you can put a note in
about the leg between wait point to wait
point three you can say hey while you're
going from here to there watch out for
this stuff and so they can build these
really complex plans and save them onto
the server and they all have a much
better place to share them and then on
top of that it became much more obvious
that they need information in order to
plan they don't just need the map that's
the base layer in Google Earth they need
context they need to know what do we
know about this place what information
do we have why are we going there what
are we trying to look at and also
constraints where can we not go and and
where do we need to start an end each
each submarine mission so we built them
this map server and this is a really
dead simple concept the idea is that if
they need to get access to a bunch of
different maps then again it's a version
control and and sharing issue we don't
want to be emailing everybody copies of
maps and then any time the map changes I
have to worry about do I have the right
version of it we just built them a
basically a content management system
where we put all the maps into a
centralized server everyone points their
earth client or if you load the
web-based planner it already points
their points to this centralized server
all the map layers are available to
everyone and they're always current
they're always the the latest and
greatest and the right one and the nice
thing about it of course again is that
it's KML so anything that can be turned
into KML can be turned into a map in our
map server and so here's this a couple
of examples this is the bedrock geology
around pavilion Lake and this is
important because they're looking at
things like what's what are the
groundwater inputs and what's the
mineralogy what's the chemistry of that
groundwater and so if you know what kind
of rocks the groundwater flowed through
you know something about what's being
introduced into the lake here is another
one this is a sonar map of the lake that
was generated by an
and submersible that flew transect
surveys in the lake and collected sonar
data and they have two things that come
out one is the bathymetry the lake the
depth of the lake rendered into color
where blue is deep and ready shallow and
separately a backscatter map that looks
at the surface roughness of the bottom
of the lake and they can look for you
know little interesting looks and
crannies that way rather than just
looking at the absolute depth from here
to the bottom and they can also and
again it's KML you can embed anything
you want doesn't have to be a ground
overlay it can be placemarks with
imbedded description tags and links to
other information and so this is again
this is sonar back scatter data but now
rather than producing one map of the
entire thing we have little preview
images up here is an interesting thing
somebody found when they looked through
the data and we can cross-reference it
to where was it when the submarine
nowhere was the submarine when it took
that data where is that point in lake
and then drop all these points of
interests into the map and now a planner
can go in and say okay well I'm look you
know look through all these previews and
say these are the kinds of things I want
this plan to go visit and then build a
submarine plan to go visit the places
that are that are the the points of
interest that they want to go visit and
so the map server was very helpful and
then the third thing we did for them was
to do make some small modifications to
how the data archiving worked so the the
submarines for pavilion Lake have a
video camera on board and they
videotaped everything all the time and
then when the pilots flying around they
don't really have much in the way of
interfaces or controls or computing
they're they're in a they're in a
submarine that's like the size of a
business class airplane seat wrapped in
glass and I don't even know what steel
rubber and they don't have much room in
there besides the space that they need
for two hands and two feet to control
the submarine and then they have a
voice-activated microphone headset to
talk up and down so if they see
something cool generally what they do is
they just tell somebody on topside hey
this is really cool write this down I'm
looking at a tree with like a microbial
ID on it which is cool because they can
go sample the tree and carbon-date the
the wood and it helps them figure out
when the
Aquos form so what we did was we built a
system to post process the data that
came back and cross-referenced those
moments where the pilot said something
like this is really cool come back to
this and cross-reference that with the
video that was rolling at the same time
just just matching up timestamps and so
again building up a chronological list
of what happened by merging that data
afterward and this so this looks like
the Desert Rats console.log essentially
it's more or less the same type of thing
so at the end of that we had an
architecture that looked something like
this we had the vehicle tracking and the
logs and the data maps that we're
building in real time but we added this
map server and we added the web-based
planner for the distributed science team
to use and our search tools got a lot
better because we're working with a
science team that's using the search
tools for you know months and years so
some lessons that came out of that we
this web-based planning platform was
much more effective for the team the
data archives are something that the
scientists use throughout the year
it helps them tremendously with getting
their you know doing their scientific
analysis and the cool thing about them
too is that they did this work before
they work with us and then they did this
work with us helping them out and
building these tools and helping them
use them and we still don't have a
satisfactory methodology for quantifying
the impact that these tools have but
they make no bones about the fact that
it is absolutely night and day they used
to argue over where they were when they
saw this thing
okay so the last one I'll talk about is
resolve and resolve is a mission concept
for sending a rover to the South Pole
the moon and looking for water ice so we
know L cross the NASA a couple years ago
sent this impactor to South Pole the
moon and it's the plan was to impact the
moon with the upper stage of ascent our
rocket and then watch the plume and see
if you can detect water in the plume and
they could they detected water at the
South Pole of the moon but now they
don't know where is it what form is it
in how hard is it to get to what's how
it was the what's the distribution they
only know that there's one place where
this rocket crashed and there was water
in that one place and so this mission
concept is how do we go and map out the
South Pole of the moon with a robot that
has a neutron spectrometer on it
and to look for hydrogen as I described
before a narrow infrared spectrometer to
look for water absorption bands and this
kind of stuff and we did a field test in
Hawaii last summer in July of last
summer where we simulated a mission to
the South Pole to moon with this robot
okay so we're searching for polar
volatiles we're in a sort of a
prospecting mode and we're testing out
the result instruments these are
instruments that are designed
specifically for this purpose and we're
going through the motions of using them
in a way that we intend to use them to
see how well it works for us and so I
started to describe we have a near IR
spectrometer or neutron spectrometer and
then there's some analysis tools the
River has a drill and a couple sample
processing instruments to actually get
you know really high resolution really
quantitative information about what
exactly it drilled up and it's all
carried by a rover and the challenges
here are that we had a distributed team
and they're working again working in
real time the same argument for the k10
experiment I talked about before we're
going to the South Pole the moon we have
real-time communications the science
team needs to make quick decisions and
on top of that this is the concept here
is a very short duration mission like a
few days the South Pole of the moon is a
very hostile place if this mission is
not going to be a super expensive
sophisticated nuclear-powered thing that
can keep warm at night and all those
kinds of things
then it needs to be get in get the job
done and you know when the Sun Goes Down
that's it so they have to get a lot done
at a very short span of time in fact the
the MIRR Rovers didn't even get off the
lander before this mission would be over
so the job of the science team is to
quickly determine where's the water and
what to do about it sampling it they
need to map the area find out water
signature to decide where to drill and
then go back and drill in the right spot
and our job again was to help build
tools to help them do this so one of the
first things that we built for them was
a plotting tool and we sort of you know
borrowed the slippy map idea to make
slippy plots and the idea here was that
we pre compute and cache tiles of data
and when I see tiles I'll describe in a
second why that's in quotes but the idea
was to have a tool for them that they
could quickly zoom in and out of the
plot and slide the plots around and have
different variables plotted on different
plots that are synchronized all the time
so you can correlate instrument data
across multiple instruments very easily
the tiling scheme and again this is
really just about efficiency so if you
have a whole bunch of people all trying
to load these plots all at the same time
and every transaction with the server is
really cumbersome then it's not going to
scale well thought doesn't turned off
and so the idea here was that we for
every window window of time we would
compute an average rather than passing
you all the data from every time window
compute an average and compute standard
deviation minimum acts kind of stuff to
summarize the data in a time window and
that's some sample the time windows down
and the idea would be then that you can
if you zoom out on the map then you know
you'd have data where each plot is at
the same resolution in terms of the
screen resolution but the time
resolution is changing your zooming out
you're shrinking time scales but you're
only packing as much data into each time
slice at that scale as every other and
that keeps things efficient and then
that data was served to the client over
WebSockets so you would open up the plot
and the plot would say okay here's some
JavaScript that JavaScript and say okay
I need these tiles what I'm looking at
right now and then as you interact with
the plot and change what you're looking
at and zoom in and out or pan left and
right it just knows to go fetch the
right tiles that it needs for that so
this is what that looked like there's a
plot here of a couple of the payload
instruments there's a drill the orange
line is the actual force of the drill
bit the blue line is the depth of the
drill bit and the green at the top is
the neutron spectrometer looking for
hydrogen under the surface and the black
line at the very bottom is the near
infrared spectrometer and is looking for
water absorption bands in the infrared
spectrum and so when you get into it and
start drilling there's a point where the
force on bit sensor has a little blip
and that's when you know that the drill
touched the ground is a force detected
and so every every bit that the blue
line goes above that horizontal
reference line that's the actual depth
of the drill bit going into the ground
and then you can see that as the drill
starts to enter the ground that water
signal on the bottom
lot starts to rise and what that means
is that the drill is drilling into the
ground it's pulling up dirt from the
subsurface and that subsurface dirt a
dirt is what is wet as the drill gets
deeper and deeper that water sensor goes
away so that means that there's really
only a layer of water that stuff below
that that's coming up is dry again and
then when the drill bit came out of the
ground on the right-hand side that the
blue line crosses that red horizontal
line and the water signal goes up
well that was curious so we because
these are all cross-reference with time
we could go get the images and it turns
out that when the drill string came out
of the ground the borehole collapsed it
was like loose soil so when the borehole
collapsed that exposed some more wet
material and so the near IR spectrometer
was seeing that wet material the second
thing we built for them was raster Maps
and the idea was that the rover is
telling us where it is and the neutron
spectrometer is telling us whether or
not it's seeing water but there was no
place where these two pieces of
information were being reconciled so we
built a map generating server that would
dynamically create a grid map of the
neutron spectrometer data based on the
rover location and so this is a pretty
straightforward thing we get two pieces
of information we stick the hydrogen
signature into the map at the location
where the river says that it is and then
the cool thing about it was we could
build a real-time dynamic super overlay
so now we can build a tree of tiles and
point your Earth client at this and you
could load in a super overlay that was
changing in real time as the
spectrometer was mapping or mapping the
environment and so when you put it all
together it looks like this here's the
the rover where it's been the Purple
Line is the plan we've stopped following
the plan because we've seen something
interesting and now we drive around in
circles shouldn't say circles spirals
we're driving around on a mapping
pattern and we're filling in more and
more information around the site and so
now this is really helpful because now
the plot at the top is showing you what
was the neutron spectrometer value
versus time but that doesn't tell you
how to get back there the map you can
see exactly where that orange bright
spot is and that's where you want to go
send your Rover back and drill for that
water signature and then once that
framework is in place of course we could
put anything else in there so this is
the neutron spectrometer map but we can
take other
instruments like the near infrared
spectrometer and map the data that's
coming out of that as well and so this
is a different view of a different
signal that might indicate the presence
of water okay so for resolve we got
these great tools for real-time display
and the the real upshot of this was that
the science team was convinced this
these were people who actually worked on
El cross and so their experience of
working on a mission that only lasted
for a few hours and and that they were
not yet convinced that it was possible
to do this kind of this kind of
real-time science with the rover on the
South Pole the moon and I think that we
convinced them that these types of tools
with geospatial visualization and this
quick dynamic mapping that it actually
was possible for them to do this science
in real time and so at the end of that
experiment we had an architect's dragon
looked more like this now we've got all
the best tools from all the different
projects that we've supported and it all
comes together to make this big picture
so in conclusion XTS is a lot of things
it's map content management planning
tool real-time monitoring plots maps
notes in real time post-processing data
archives and it's all these tools for
browsing and searching the data
afterward it's been really helpful to
use Google tools to the Google products
in order to do that using cool Google
Earth and Earth plug-in platforms a the
sulfates integration we can get map
content from anybody and just plug it in
it reduces our development we don't have
to worry about writing a map UI we just
use the one that our exists and we can
focus on the content it reduces training
because we have people who come to work
with us and they're already familiar
with how these tools work and it it
really helps to support doing real-time
distribute collaboration because it's
all web-based that's easy to just get
everybody to to to coordinate and work
online and collaborate with scientist
has been really powerful for us because
it drives innovations that we otherwise
wouldn't know what we don't know until
we work with them and it helps them do
their work and it's difficult to
quantify how well it it helps them do
their work but it's they they're
convincing about the fact that it's it's
a huge benefit to them so some future
directions right now a couple things
were working on right now is that
planner tool I showed earlier it doesn't
really know
about what types of things you might
want to get done and we're working on a
extensible command dictionary framework
so you can write your own file that
describes all the different types of
tasks you might want to do how long each
one might at what might take what
resources that might need and then the
planner can dynamically generate all
these or interfaces based on that schema
and then if you generate a plan from
that then it saves it all out in a
format that encapsulates all that task
information we're working on telemetry
handling we've got we have a lot of
systems that send telemetry at a very
high rate for a long period of time and
so we're having to scale up in terms of
how we deal with all that data we also
want to be able to play it back
afterwards so right now we capture a lot
of data we make it visible but we can't
play it back as though it's happening in
real time and we're looking at that too
because that will help payload designers
and people who are working on these
tools that are downstream from those
systems be able to work with canned data
that's being sent back out in the same
way that it would be from the live
payload we're working on an analyst
notebook which is based on ipython
notebook if you're familiar with that
it's basically a way to to do you know
MATLAB or Mathematica style analysis
with a command-line interface and we're
integrating that with our website and
making a develop as available to
scientists to use and we're looking at
trying to be the open source release
with this we want to try to develop a
more generic set of i/o standards so
it's easier for people to just take our
system and use it if it's based on easy
formats to get in and out of the system
it'd be easier for people to go ahead
and use and on deploying that out to the
cloud and if if I knew that sounds
exciting to you and you want to help
come talk to us
yeah why Google and NASA world one
singer zinger first yeah so the question
was why Google Earth and not NASA
roadman when we started this project
NASA world one was still a relatively
young project and I started using Google
Earth on this because I actually was
more familiar with it sooner I'm
actually not even sure where Ness where
NASA worldment is now but NASA is a big
place lots of people working with lots
of different things and different where
we were and the things that we were
looking at when we made a decision about
how to do this Google Earth just was the
most obvious choice for us to use
so my first question is a non-technical
question you were talking about their
requirements for field study software
and one of the ones that you listed was
useful to novice users and that seems
obviously useful if your goal is to
support earth scientists on earth it
seems less obvious that that's a good
parallel to doing right yeah okay so
I'll try to summarize that question the
question was does our focus on novice
users and our focus on on easy-to-use
interfaces setcolor our research because
if we were doing this for a NASA mission
we would have time to train them but we
would have more more time with them and
they might require more sophistication
than you can put into an interface for a
novice user and in fact the planning
tools that they use for the Mars rivers
now are quite sophisticated and they
have experts who use them so I think
that those are all very good points I
think that the first thing I would say
is that it is an artifact of how we do
our work that we have to focus on
interfaces for novice users but this is
this is a thing that comes up all the
time in the world of looking at
terrestrial analogs for for space no
analog is actually a good analog for the
thing you're trying to do so that's one
way in which we know that the thing that
we're trying to do is does not is not
it's not a one-to-one match with what
they're trying to do but the other thing
is that the science team on the Mars
rovers are not expert users of the rover
planning software so their job is to
come up with what do you want the robot
to go do next and they come up with a
much higher level sort of intent
expression of intent
and then it's somebody else's job it's
the sequence generators and the
roboticists
at JPL who say okay how do we get the
rover to carry out the science teams
intent and those are the expert users
who use those more sophisticated
planning tools and I think there's
there's room for both of those things
yeah well I think that's a great
question I I wish that I had a running
log of all the feature requests bug
fixes nifty ideas that we've had over
over the last couple years and I think
that may be one benefit from my coming
here today would be to have a better
line of communication back to folks here
not like you said having to join our
team but even just having even just
having better connections to folks here
who work on some of these things where
we could actually say hey you know we're
using this tool and it's almost does
what we need but not quite that would be
a really helpful thing yes or you also
you might already are you also connected
yes so the question was a lot of this
seems to focus on planning and are we
working on things that are more about
the live operations of a mission yeah so
I think that that some of the tools I
talked about are I probably spend a lot
less time focusing on them but we do
have tools that are designed to to you
know build a real-time operating picture
of what's going on and try to keep the
scientists Rick in the loop and
situationally aware of what's happening
on that on the ground I mean it's a very
it's a critical that's a very critical
use case for these tools
do you have any any interesting process
for recommending usability of the tools
that people work and feel specialized
yes the question was do we have any
usability tools for what we're doing we
actually don't have any usability
experts on our team and I think that
would be a really useful thing to have
we are probably guilty of committing the
same sin that lots of developers commit
which is that we think about usability
as an afterthought and there are a few
people in our division few people who
work on projects in our in our building
who do have that experience and can
bring that experience to those projects
we we tend to take a much less
formalized approach and we have people
who sort of commit their novice interest
and intent to build the building better
user interfaces by just talking to the
science team about what's working what's
not what's confusing what is it watching
them work and watching them struggle and
seeing where their pain points are even
if they can't point them out themselves
and going off and fixing them but we
don't use any kind of formal UI
development approaches oh yeah no no
that's a good yeah sure it's a good
starting point absolutely how do you
weigh the balances between stuff that
has to work without connectivity versus
the stuff that you want to be connected
you know where's the especially in
future development where's that line
fall yeah that's a great question so the
question was about deploying to the
cloud or using network heavy stuff
versus stuff where connectivity you
might not even be available I didn't
talk about it again in this talk but we
do have some work we're looking at
things like how to put a lot more smarts
onto a mobile device maybe even one that
gets synced up with your services and
downloads a bunch of data maybe like
your plan and you know that kind of
stuff and then you go off and you use
your mobile device to capture data you
can record stuff on it videos images
notes whatever it tracks where you're
going all the time and then syncing that
back up again later when you're back
online and we've looked at that for
again for the disaster response project
in particular
and and we've been thinking about ways
to use that for field geology as well I
mean some of these places that we go we
have connectivity because we go we bring
a whole bunch of networking equipment
intro post you nuts and we set it all up
and we have connectivity because we make
connectivity but we have our own little
pocket of high-powered network
connectivity in the middle of nowhere
and so so yeah I think that we are
mostly focused on simulating places
where or simulating situations where we
have a rover on the surface McLennan and
we have the deep space network and we
have the infrastructure to talk to it
maybe not the same as you would if you
have you know your Ethernet cable in
your office but some connectivity back
to the rover at least when the DSN
dishes are pointed at it but but yeah
that's a very interesting use case of
Russert for us as well
is there an interface right now sure
yeah absolutely
absolutely I think that I mean and one
of the cool things about building these
tools the way that we do is that it's
very easy to just take stuff that we
have in our system and turn it into an
output unto an EPO product we do not
reach the product it's already pretty
much canned and ready to go for that
the tricky thing interestingly enough
ends up being about stuff like
proprietary data that the science team
needs to do their work and not get
scooped by scientists who aren't working
on the project so but we have actually
done that kind of work as well we've
we've had a lot of interaction with
outreach programs a lot of these
projects have their own outreach
programs and they ask for bits and
pieces they kind of pull examples out
and use them as outreach products but
yes it's actually a pretty easy thing to
facilitate because it's already in the
right format
all right thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>