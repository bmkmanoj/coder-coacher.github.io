<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Assistive Vision Technology for the Blind: Recognizing... | Coder Coacher - Coaching Coders</title><meta content="Assistive Vision Technology for the Blind: Recognizing... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Assistive Vision Technology for the Blind: Recognizing...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DCtDjtFQ94Q" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay this project is nicknamed Gro Z and
that spelled gr ozi and I'll explain
where that nickname comes from later
this is a collaboration with many
different people both at Cal IT - UC San
Diego and also University of Kentucky
and I'll describe the different parts of
that collaboration shortly but the focus
of the project is to develop a handheld
device which we call a mozi box and that
device will help blind and visually
impaired people locate objects in a
grocery store for example to prepare a
shopping list and take that to a grocery
store and use it to find products on the
shelf and just as a side comment this is
an interesting and different project for
my group for a couple reasons one is
because it actually involves a hardware
device which is something I rarely do
second the bulk of the research here
really the lead researcher on this as an
undergrad so this represents a big push
forward in my group to get undergrads
more involved in research another person
working on this project is Karolina
who's here
interning at Google so a little bit
about the motivations of the project the
idea here is to increase the
independence of people with low vision
or that are blind to perform grocery
shopping in a supermarket or store
certainly there are other solutions to
this a blind person could just order
groceries by phone or online and have
the groceries delivered but in some
cases the blind person might actually
want to go to the grocery store and have
the independence to locate the groceries
themselves and so the idea here is we
like ideally in the scope of the project
we'd like to cover everything from the
home of the user along the walking path
to the store and the navigating inside
the store to find the products and back
and that would include also the process
of payment at that
I'll counter so the the market so to
speak is the 1.3 million or so legally
blind people in the US but we also think
this could be useful to a much larger
group of visually impaired people that
could use some help locating products
and what can be a visually overwhelming
stimulus when you look at a grocery
aisle shelf there's really a lot of
things there and if you're visually
impaired that can still be a challenging
problem now there are two ways of
looking at the current state of the art
one is from a cold marketing perspective
grocery store managers think of blind
people as high-cost customers okay in
other words they're a pain the people
that have to help the blind customers
should be stocking shelves and and
packing boxes and so forth so if a blind
customer comes in and needs help that
takes that worker away from from this
other task but from another point of
view these grocery stores are under
selling to the blind customers so there
is demand from blind customers to
purchase these products and if there
were technology or just some kind of
solution more generally to help them in
this in this setting then grocery stores
could actually sell more to that market
so more broadly speaking the type of
device that we're making will have
applications outside the grocery store
for example one of the applications we
want to move into is an airport terminal
but from so from a pure computer vision
standpoint you can think of this
presentation as being about a mobile
device that runs computer vision
algorithms and that can do object
recognition but we've chosen this
specific problem setting of a grocery
store as a way of getting started in
this problem of navigating for the
visually impaired and okay let me show a
an overview of the system this is a
slide that Karolina actually put
together for us so there are three
stages
of this project there's the part that
takes place at home where we see the
user there in front of the computer with
online visually impaired accessible
website and the device itself the so
called mozi box is depicted there on top
of the CPU sitting there it's connected
to the computer by a USB port and at
this stage at home the user finds the
products online for example using one of
these online grocery stores and maybe
chooses 20 or so products to put on to
the shopping list and then a couple
things get uploaded to that device the
walking path to the store and then an
assortment of training images of the
different products because what we'd
like to do is spot those products on the
shelf so training images of all the
objects on the shopping list go on to
the mozi box then comes the outdoor
project which is something our
collaborators at University of Kentucky
are working on which is consists of
navigating crosswalks detecting visual
landmarks visual waypoints and so forth
and this so the idea here is this
Starbucks coffee logo you could in
principle have a GPS system that just
indicates where you're located the
problem is that this has bugs of area it
can be affected by the architecture and
the vicinity and it may not be
particularly accurate or you may not
have GPS available at all but to have
the feedback from visual waypoints can
help you know that you're moving in the
right direction so it's an extra
affirmation that you're along the
correct walking path so that part's
being done by a University of Kentucky
that's David mr. Hendricks tavini Asst
melodie Carswell the part that we're
focusing on is inside the store and
broadly speaking the problems inside the
store
include locating the aisle signs so
detecting the aisle signs and reading
them and then once you're inside an
aisle that you think contains the
products you want spotting the products
on the shelf and providing haptic or
tactile feedback through the Box to
direct your hand to that product on the
Shelf the mozi box itself is shown here
the size it's about the size of a
typical maybe if you took two flipped
cellphones and stacked them up it's
about that size so you can hold it in
your hand it has two orthogonal servos
on it with little plastic tabs and those
plastic tabs provide directional
feedback and they're inexpensive tabs
that all the hardware in the mozi box
and quantities of one is about $300 or
so the servos themselves are the type of
servos you'd find in a remote-control
car and that the mozi box itself is
actually the marriage of two different
projects there was one project called
the zig zag and that was a box developed
by John Miller at Cal I t2 and it had no
camera on it just a servo and the idea
there was that a so called remote
sighted guide a sighted person would be
sitting somewhere for example imagine a
person is sitting in the bleachers and a
an athletic field and then you have a
blind or blindfolded person on the track
holding this box with the servo and then
the remote sighted guide can set the
servo angle to direct the person around
the track and one of the motivations for
that besides helping visually impaired
people was actually helping first
responders at a disaster site where the
room might be filled with smoke and
they're able to get directional feedback
from an external source so the zigzag
was this servo part but for the haptic
feedback and then there was another
project called moves the mobile vision
system and the idea there was to take
the Intel OpenCV library and get it work
on a low-power low-cost mobile platform
so we put the two together and we got
mozi and grows II just became the
nickname for the application of mozi to
the grocery store so it doesn't actually
stand for anything anymore but that's
the nickname so the this is under
development in parallel so I won't say
too much about it except that it will
very shortly have the functionality to
run all the algorithms that I'm showing
because everything we're writing runs an
open CV but the simulation what I'm
showing our simulations that run on
regular PC and there will be some
challenges to optimize the code because
we can only get up to about 400
megahertz on this thing so that's just a
quick look at the mozi box now let me
jump into some of the different problems
that on the computer vision side of
things because this the broad picture of
the grow z project as I showed in those
different panels it encompasses the
website design the blind accessible
technology for finding the groceries the
haptic feedback so there are many
different issues besides the actual
computer vision part because at its
heart the computer vision part is really
using object recognition pattern
recognition things but that alone would
not make a very useful product at all so
melody Carswell at university of
kentucky is really critical in this and
so is John Miller at tality to to make
sure that we have volunteers from NFB
involved and lots of people in the
community that are testing this with us
and actually telling us what's stupid
and what works well and so forth so
there's really a lot of back-and-forth
here between the the computer vision
people and then actual members of the
blind community that are giving us
feedback what I'm focusing on in this
talk is really the computer vision part
and then maybe at the end we can talk
more about these these other issues that
that affect the user interface
so the first problem here
well what I'm showing here is is kind of
a mix of examples of processed data from
real supermarkets which we just recently
got permission to go into legally and
then also experiments from a convenience
store on campus as you might know
grocery stores are not thrilled about
having people come in and take pictures
and video because there's a lot of
intellectual property in a way present
inside that grocery store just in terms
of where the products are located on the
shelf and which products they're
choosing to sell we finally did find a
manager of a supermarket nearby campus
that's happy to help us so now we're
able to go in and take as many pictures
as we want as long as we call in advance
these pictures were taken before we had
that permission so we just snuck in and
took pictures and then ran out so the
idea here what's shown in the picture
here is a sample photograph taken
looking down an aisle and there's an
aisle sign it happens to be aisle 21 in
a Safeway supermarket and it has pasta
sauce import pasta canned meat rice
cakes and so one of the problem one of
the reasons you know I mentioned that
this mozi box has limited processing
power the reason that the processing
power is important at all is because one
of the biggest challenges here and
making this useful is just getting the
device pointed in the right direction to
begin with so we need this thing when
it's set to redial signs to run all the
time because if a blind person walks
into the aisle and they think they're
pointing in the right direction but
they're actually pointing straight down
the aisle instead of up enough to
contain the aisle sign this thing it
needs to be interactive enough to to
keep them updated about what's in the in
the frame so what we're shooting for is
about six frames per second and we have
a text-to-speech module on this box that
just speaks whenever it detects these
different words so I'll just say a
little bit about how the text detection
and recognition works we're using a
technique which was originally developed
at the smith-kettlewell Institute and
UCL a--
UCLA by alan you'll and chen and this a
Chanin you'll cv pro4 paper and that in
turn was based on a face detection
approach by viola and jones and so they
adapted that to work with text and we
adapted that to our problem and that's
based on what's called an atom boost
cascade and haar wavelets and the way
this works is you start with this huge
training phase where you take a whole
bunch of photographs of scenes that
contain aisle signs and scenes that
don't and you take all that data and you
get a human tool able to draw rectangles
around the boxes that contain aisle sign
text and so far this doesn't have to do
with what the text is saying it's just
the process of detecting things that
could be the words on aisle signs so you
get all that training data then you come
up with features that you use that
really in practice our convolution
kernels and for efficiency these
convolution kernels are just hard
wavelets that's just differences of
boxes that can be computed extremely
fast at different scales and these
little differences of boxes are able to
describe the textural features of text
so what you do is you use this learning
algorithm called adaboost in an
efficient implementation called a
cascade you train this thing for a
couple hours and then the result is that
you just put in a photograph of a scene
in a grocery store that may or may not
contain aisle signs and cascade
processes this in a very efficient
fashion and pops out rectangles that
could contain I'll sign text and as you
can see it is detecting the words on the
aisle sign it's not detecting the large
number above the aisle sign that says 21
because we didn't train
for that type of text you'll also notice
in the background there signs that say
yogurt and butter and there's a wavy
line passing through them and this is
kind of bizarre because some of you
might know about visual CAPTCHAs have
you heard of this so a visual CAPTCHAs
something
Yahoo uses these I think Google might
also is that right so these are designed
to be difficult for a computer to read
and easy for a human to read and they
it's really a case where something is
purposely made difficult to read to
throw off algorithms to to detect it and
it seems bizarre that a grocery store
would put something on the wall that's
effectively a visual CAPTCHA so we
perhaps it was there to to throw off
robots that are trying to spy on the
store we don't know what the motivation
was but we do know that two weeks later
when we came back those sign the wavy
lines were gone we didn't say anything
but for some reason they took them away
so these are this is an example of the
aisle sign detection and we so once we
detect the text we do some adaptive
threshold again feed it through optical
character recognition algorithm which
any of you have you who have used OCR
know that these algorithms are difficult
enough to use on regular text much less
text in an unconstrained environment so
in general it's very difficult to take
these thresholded letters and simply
produce the ASCII string that represents
these words but we have a big benefit
here in the sense that our universe or
our lexicon of words is highly
restricted to the case of grocery-store
terminology so in a way what you can you
can think of it like we have a low-level
OCR with a spellcheck that's highly
biased
toward grocery food aisle signs so in
its raw implementation it's not
terrifically usable right now but this
thing runs on the PC it's running at
about 10 frames per second and every
time it detects one of these rectangles
it binarize --is it runs it through OCR
does spellcheck and then this little
pre-recorded WAV file just says the word
so it's actually kind of obnoxious right
now but we're working on getting a
better interface so you can set the mozi
box to read aisle signs and just start
pointing it at things and that and the
speaker just talks and says what it's
seen and this concept goes back to the
smith-kettlewell so-called talking sign
concept which i think is maybe ten years
old that was based on a line-of-sight
infrared system and for street signs in
San Francisco so you could spot this
sign through an infrared beacon and then
you carry a box and it says the name of
the street so we're trying to do that
type of thing for food aisle signs just
using computer vision so this is a part
of the project that's actually being
done in grocery stores now with
permission but the recognition of the
products themselves themselves is
actually taking place in a small
convenience store on campus which is
called the sunshine store and the basic
reason we're doing that is because
grocery stores at this stage of the
project they're just way too big a
typical grocery store has about 25,000
square feet around 30,000 different
products and we do intend to do that at
some point and we have a we have
collaborators at evolution robotics
which is in Pasadena and they're
offering to lend us robot to use to help
collect training data and do the testing
and so forth at this stage we have this
great opportunity to use this small
convenience store for for beta testing
it has less than 2,000 square feet
you can think of it as a miniature
grocery store it's got about 4,000 items
in stock it has the drawback of not
having a bakery or produce but in all
other respects that represents a
miniature version of this of the
supermarket problem and the nice thing
is that it has regularly scheduled
maintenance hours so we can just go
there a couple times per month for a
couple hours at a time with students and
cameras digital video cameras and so
forth and collect all the data we want
so this is where our pilot testing is
taking place and specifically that's
where we're getting the testing data so
all the testing data is being captured
with a video camera and the target
resolution for this mozi box is a 1.3
megapixel camera so that's better than
VGA so all the testing data is being
captured like that the training data on
the other hand comes from the web so the
idea here is that so take the case of
this of this convenience store they sell
4,000 items the manager of the
convenience store gave us the inventory
a printout of the store inventory so
there's 4,000 items he gave us the UPC
or barcode for all the products plus a
textual description of what the product
is so all the things that could be in
the store are included on that list and
the nice thing about groceries compared
to say trying to solve the airport
terminal navigation problem or general
outdoor navigation problems is that
there are all these different online
grocery stores and in our case Google's
frugal website was extremely useful for
this purpose we can go find the training
images online and not just one but we
can find lots of different views of
these different products and of those
4,000 we picked a hundred and twenty to
build up our database and these are just
sample pictures of images that we
grabbed from Frugal for four different
types of prod
and this is what we use so one of the
artifacts of this project is a is a
large database of training and testing
images for all these groceries and I can
answer questions about that later if
you'd like but just to describe the the
nature of this of the data collection
task we use these nicknames of in vitro
and in situ to describe the different
modalities of data in vitro we use to
describe the clean images that we
capture in the lab or rather from the
web that typically have the products
professionally photographed at a stock
photo agency with a white background
labeled perfectly and then the in-situ
is on-site it's where the actual
products are located and those this
student Micheli mer ler painstakingly
went through 30 minutes of video
cropping all of these hundred twenty
different prod products in every fifth
frame by hand so for that entire list of
products he just clicked through every
single frame drew a perfect rectangle
around each product and even did the
outline of the mask so that we knew how
to separate the foreground from the
background
so the testing data over here this is
the in-situ side of things the in vitro
side it's really got two parts ideally
what we would do is just punch the UPC
code into some query just do a query
into a database punch in the UPC code
and pop out the appearance of the
product unfortunately that database is
not available or at least isn't
available for free yet so what we have
is a sort of two-step process you can go
to Google or the UPC
database lookup and punch it you can
just grab any product look at the the
UPC code and punch that into the Google
search field and it will automatically
query the UPC database and pop
a little textual product description
then you can write a script that grabs
that textual description does a Google
image query and then gets you things
that may or may not be that product and
that what we're doing there is
exploiting some redundancy that you if
you're searching for example for Sun
Chips this represents seven different
pictures of Sun ships or a neosporin box
that you might get out on those queries
but you'll also get lots of other stuff
in there that just happens to get
returned when you type in those words so
there's a sort of small-scale
content-based image retrieval problem
they're just in building up the database
so you can think of it as it's a
semi-automated
problem of taking that inventory list
from the store and doing the query to
get those images i've tried starting
around six months ago i started trying
to get direct access to these databases
through a one company that provides
these images to lots of different online
grocers and they're just ignoring me and
it's difficult to get that so my group
just came up with our own solution to
build up this database so this is what
some of the data looks like so the in
vitro or the online images these are
captured from Frugal and that's what the
masks look like the Micheli prepared by
hand and in situ images these are
captured with the in this case it was a
VGA resolution camera and they're
clearly not as high quality so this is
the type of challenge we have to try to
find things where there's training
images that look like that but testing
images look like this to give some idea
of the the number of different examples
we have over on the training data the
x-axis here is showing 120 different
products and the y-axis is showing how
many
each one we have and the mean is
somewhere it's about 5.8 so we have
close to an average of six examples per
training image per training product and
on the testing side where each of these
examples represents an appearance of
that particular product in a video frame
that was that had a box drawn around it
by Micheli so that has an average of
around nine examples per per item so
that that allows us for the purposes of
a kind of focused computer vision study
this then decouples into these two
problems where you can study object
recognition performance and you can also
study object localization so just by
taking these prepared training images
and the extracted testing images you can
create a database you can forget about
the particular application of groceries
and just treat it as testing and
training data you can produce ROC curves
and look at the difficulty of that
problem with respect to different types
of recognition algorithms similarly you
can study how well any given detection
algorithm does at locating the object
within the frame you could start with
the most basic question just asking is
the object in the frame at all all the
way down to localizing it within a
couple pixel and that stuff that we're
still in the process of studying so this
is a snapshot of some of the different
object detection and recognition
algorithms we're looking at we don't
expect any particular algorithm to solve
the whole problem we think that we'll
need a combination of algorithms
some of them include color histogram
matching which in this case we're just
using the chrominance channels to get
more and very more invariance to
illumination we can also use invariant
interest points so this exam
Balazs sift features from David Lowe we
may want to use something that like the
HAR like features and the adaboost
framework which traditionally was more
used for object detection but perhaps we
can think of this as an object detection
problem just multiplied by the length of
the shopping list so these this is just
a snapshot of different possible
algorithms we could also create new ones
and we're thinking of different ones but
one important thing to keep in mind is
that when the user goes into the store
we're not because we want this device to
work independently we're not expecting
it to stream video over a video and do
classification on some back-end server
we want it all to happen on that box
with a quick turnaround time so it's
highly interactive and and maximally
usable by that person and because of
that it's very important that okay the
point there is that we're not expecting
the person to be able to recognize any
of the 30,000 items in the store the
priority is only that they be able to
recognize what's on their shopping list
so in fact this device they're carrying
doesn't know what it's looking at most
of the time all we're expecting it to do
is to vibrate or activate in some way
when it sees an object that's on that
shopping list now another implication of
that is that we could do a lot of
processing offline on the PC when you
have all the power of the PC and the
internet available so when the user
prepares their particular shopping list
they might put 10 items on there which
when you're at your PC you could
optimize your object detection and
recognition algorithms to work
specifically for that set of objects
that you've picked okay this is
something we haven't yet explored what
we're doing at the moment is just
studying the way that these different
algorithms work in isolation so that we
know what we're dealing with
so the way that we what I'll do here is
just show a snapshot of the performance
of some of these different algorithms
that we're using the standard approach
here for quantifying recognition
performance called a receiver operating
characteristic or ROC curve so we're
plotting true positives against false
positives as a function of the threshold
used to declare a match or not
and in the case of color histogram
matching we have a lot of different ways
of quantifying the similarity between
color histograms so as I said that the
feature matching the example of feature
matching based approach we're using a
sift which finds interest points on on
the image and attempts to match it and
look for a geometrical consistency the
color histogram is actually looking at
histogram of two different chrominance
channels the C B and C are channels in
the chrominance plane and I have a
little demo video here that shows what
it looks like in the case of trying to
find at I'd box here the video camera
starts out pointing at breath mints and
and cough drops and deodorant and the
person scanning around and the box is
looking for Tide washing laundry
detergent so it's kind of slowed down
here but eventually the tide appears in
the window this is running at 25 frames
per second and it's capable of detecting
matching histograms of 5 different
scales and so this is just a simulation
here that the algorithm itself is not a
simulation it's actually running the
part that's directing the users hand to
the object is still a simulation because
we don't have the haptic feedback
working yet but the way that would work
is once the color histogram match is
above threshold the haptic feedback
would get activated and then direct the
users hand to that box
that mostly solved the problem but at
the end what's happening here is the
user is searching for the barcode and
then once the barcode is visible you can
verify that it is in fact the correct
product that last step yes stupid
question but if you're holding this box
in your hand that's directing it and how
can you grab a box in your hand
oh it's your hand oh yeah so I think in
practice the way it work is that you you
actually keep you zoom in until you're
touching the actual box but that is a
problem that remains open I but my
assumption was just that you you
essentially collided the box with the
with the object and then you pick it up
you might pick up the wrong thing in
which case then just saying your hand is
full because you're holding I did the
mozi box you have to hold it here that's
right yeah so that that's something we
still need to figure out in terms of
ergonomics and this barcode thing is is
a bit it's nice in terms of validating
that it's the correct product but one of
the other collaborations we have going
on as with and with a radio frequency ID
tag technology from Intel specifically
it's a VHF style rfid tag and it works
in close proximity about five to eight
centimeters and the idea is in the five
to ten year time range most grocery
products are going to have passive RF ID
tags on them and they're just close
range in order to be safe for humans
it's just a close range kind of thing
you can only read it from from the
range of a few centimeters but
eventually that will eliminate this
barcode part you'll still need the
computer vision to get you into the
neighbor to get your hand on the product
so we're going to use computer vision to
get you into the correct aisle by
reading the aisle sign and get you into
the neighborhood of the correct product
but then the hope is that you can wear
one of these right now it's the form
factor is a wristband so you could
either wear a wristband or just embed
that into the mozi box and have that
take care of that final validation step
so we're doing that collaboration with
with Intel research in Seattle right now
these are some examples of the ROC
curves we get for some some different
actual products on the Left where I'm
showing a training image that we got
from frugal of renew contact lens
solution and then that's a snapshot that
next to it is a picture from the actual
video so that's just cropped out from
the video so out of that's one of the
hundred and twenty objects so there were
a hundred and nineteen possible
distractor objects and the ROC curve was
produced for that based on true
positives and and false positives and in
this case you don't really have to pay
attention to these different color
coatings they're just different ways of
comparing color histograms but the ROC
curve in this case for the chi squared
distance between histograms is very good
this is something that has a very
distinct color histogram it's got a
bunch of white and then some blue and
green and that makes it very distinct
with respect to the other products yeah
you're comparing the estimates of the
quantum objects yeah but in
implementation you have to look at every
box every scale on an image that's
absolutely right so this this sub
problem this database that we created
that just has the the cropped out
products is artificial in the sense that
it's just object versus object in
practice you'll not only have the multi
scale but you'll also have clutter you
could have the floor fluorescent lights
that's something whether we like it or
not that's going to be that's going to
show up very soon this ROC curve is just
studying inter product confusion at this
point but you're right it's it's a it's
artificially separated so we can just
take off the shelf object recognition
problem algorithms and just study their
their performance essentially looking at
every that's right and you can see
sometimes it misfired at the completely
wrong scale but one thing to keep in
mind here and one of the reasons we're
not terribly worried about having the
distractors be things besides neatly
cropped other product images is that
there is a temporal element to this
there's temporal continuity in the
recognition which is something that's
not captured in this static database so
whether the aisle signs or the actual
product if you're looking at the correct
object then for several successive
frames that should continue to recognize
it as the right thing and as you
approach it the recognition should get
better now we do expect the occasional
fluke incorrectly recognized thing but
there will be a whole dimension of this
project which is to incorporate temporal
continuity and that's something we just
haven't gotten into yet here's another
good example the tide box has become our
poster child for this project because
it's such a nice product with it's an
attractive design
good color nice solid color histogram
it's gotten good sift features too I'll
show this again for sift matching so
that's the training image up there this
is an example of a test image down there
the ROC curves so good you almost can't
see it it's way up there in the top left
corner here's an art and Hammer box and
again it has a relatively good color
histogram so it's doing pretty well some
bad examples this is real real data so
things happen that we don't want to
happen there's a training image of a
skills bag there well in the convenience
store skill bags are at the very bottom
of the shelf and they're inside a
cardboard crate and that they're poorly
lit and I was the one capturing the data
here and I just pointed it down where
the skittle bags were and it was pretty
dark and you can hardly see anything
there but if you look there if you look
very close or if you do gamma correction
you actually see some skittles writing
there okay and this was a legitimately
cropped region that Micheli outlined and
it just doesn't work okay
but that's real you look in the data
that type of thing happens you could
argue it should be solved using better
illumination but that's a fact of life
we set this product project up that's
the kind of thing that's going to happen
it needs to be solved somehow
here's Dov I think this is a
antiperspirant so again we have an
illumination problem and if you look
closely that product because this is a
small convenience store a lot of
products are actually turned to the side
a little bit just to fit more stuff on
the shelf so in terms of color
histograms this has a couple problems so
much of it is white that it tends to get
confused with other things but the one
distinct part which would be the cap was
so dark that it didn't get a good enough
match yes
putting illumination in the mozu so that
we have and we this as a result of this
first data pass we did two things one we
decided we wanted to put an array of of
LEDs around the front of the device and
the other was to put a polarizer on the
lens because with the potato chip bags
the the glare was was usually the most
salient features that we extracted from
the potato chip bags were erroneous they
were just caused by reflection and when
we put the polarizer on it fix that so
these in retrospect that's just
completely obvious but you tend to
forget everything when you start a new
project so here's another one this is
Milano cookies and in this case it the
this is the kind of thing that would
work well with invariant feature
matching approaches but the color
histogram of a Pepperidge Farms cookie
bag is just not very distinctive at all
so this thing didn't perform well with
color histograms how does it do and get
soda cans the soup can sort of cans that
even more effective yes I guess they
have paper around them but the curvature
that's a good question so that the
curvature is really only an issue if the
if the cans are actually rotated if the
pose of the cam changes with respect to
the training images that were captured
the I don't have those ROC curves to
show but we do have internally the ROC
curves for every product all of those
120 so I could check afterwards to see
what they were but I think that they
worked well for the soup cans the soda
cans were more difficult because they're
behind glass in the refrigerated area so
they had more problems with reflect how
do you plan to indeed
problem of the poses again with respect
to the training data because that is
going to happen right yeah so there are
two ways one is through when you okay we
want to have highly heterogeneous
training data one way to get that
heterogeneity is that there are many
different online grocery stores who have
different pictures of the products so
when we looked for neosporin for example
we actually found eight unique pictures
of a neosporin box and they all had
slightly different poses it's actually
enough to build a 3d reconstruction of
that box sometimes you don't get that
though and I think that one excellent
way of building this heterogeneous
training set is to have the user
community share images so for example
you may have so Trader Joe's might sell
things that simply don't appear online
well you might have it in your pantry
though so if you have it in your pantry
you can capture an image of it you could
share it with the user community and
that's as good as grabbing it from the
web in fact it's better probably because
it's an actual photograph with the
device itself so right now it's it's a
bit more difficult I think then it could
be because we're forcing the training
data to come from this clean stock
photography domain and all the testing
data comes from the the noisy real world
but the idea you suggest with cans is
interesting because if you have domain
knowledge about a type of food that it's
cylindrical or square or crinkly plastic
you can use that and then generate
synthetic the way the grocery store
store stock these tonight all the soup
cans are stacked together so it's like
that's actually a cue that you're in the
soup aisle which is not in the crocodile
whereas the crocodile is also in boxes
yeah that's true and that brings up an
issue that there's a lot more than just
the aisle sign text that can tell you
where you are one of the ways that came
up was that some of the products that we
want to find are very small like there
was a
a toothbrush now it's tall and skinny
but toothbrushes tend to be close to
certain other products statistically
speaking so the challenge might be what
is the closest large object that's
correlated with the thing you're looking
for then download images of that onto
the device find that and then look for
the toothbrush so these are things that
are a little bit outside the scope of
the of the core computer vision problem
but these are machine learning problems
that I think the system will mean in
order to be successful so this is a
snapshot of sift features on the tide
box again our OC curve is so good you
can't see it the tide but for those of
you that ever worked in content-based
image retrieval the tide box is the
sunset so sunsets were the super easy
example back in the content-based image
retrieval days so tide boxes those are
it's really it's basically the informal
logo of our project that here's a Raisin
Bran box sift matching there's quite a
bit of pose variation here and that's
really the type of thing sift was built
for a Raisin Bran box is basically a
planar object with an interesting
pattern painted on it
ROC curve is very good for Raisin Bran
effectiveness if you could make the
training really really trivial for an
end user to do yeah
then they need to actually probably work
a lot better because the end user could
actually train it a couple of times in
the store that the user shops on yeah
and you know then your training lead and
your usage data is way to match so
closely I think you're you're absolutely
right and the nice thing about the
grocery store domain is that we would
encourage all the users of this system
to do exactly what you said and then
there's this UPC code that makes it all
so easy to index so if somebody in
lexington kentucky finds
these vegetable thins it has a unique
UPC code they can add that to the
database of course it's probably still a
good idea to do some quality control
make sure that it's the right thing but
that we can have this Clearing House
database of in situ and in vitro
training images that eventually will
make the system much more effective so I
think that we don't have that benefit
right now because we don't have any
users but I do foresee that it will get
much the the quality will go way up when
more people start using it so that's
here's some bad examples with sift
matching so this cheez-it bag again it's
very dark but if you if you stare at it
a little bit you'll see that the
cheez-it bag is not only reclined back
but it's also distorted a little bit so
that the word cheese is bent and in fact
the number of matching sift features was
zero it just didn't work at all
here's that skittles bag again again it
looks really dark but there actually are
these letters here that say skittles
know sift features fired on it here's
another problem this is pepto-bismol it
just so happened in the testing data
that the pepto-bismol appeared with
motion blur in a lot of these frames
some of the pepto-bismol some of the
frames containing pepto-bismol didn't
have motion blur and got recognized
correctly but when motion blur like that
happens it throws off the interest point
detector this happens to be one that
color histograms did work well on by the
way but in the case of the sift features
it only matched maybe one or two
different features on it yet to the
human eye these clearly looked quite
similar so motion blur is a fact of life
it's the kind of thing that gets worse
as you increase the frame rate of the
device
it's one of the trade-offs that we need
to make do we want a lower frame rate
with less risk of motion blur or a
higher frame rate and err on the side of
having the device make mistakes now and
then but constantly give feedback to the
user about what's being spotted so let
me say a little bit about future
directions one thing that's not
mentioned here but actually represents
the very first stage of the product
project maybe about three months of it
is something called a remote sighted
guide study I mentioned the remote
sighted guide earlier in the context of
a zigzag but what we're going to do in
that same convenience store is simulate
this whole experiment in a kind of
Wizard of Oz style where instead of
having the computer do it we're gonna
have a person sitting under the checkout
counter with a laptop so they're sitting
there with the laptop the user has the
mozi box but all that's doing is
transmitting the video to the user the
remote sighted guide that user has a
control panel which can do two things it
can set the servo angles and it can also
control the text-to-speech unit so we
plan to use this remote sighted guide
study to learn about usability and
protocols for directing people in an
environment like a grocery store how we
already have a lot of feedback that we
can import from the zig-zag studies
about what kind of protocols people
worked out for how much to turn do you
want to turn 90 degrees or 270 degrees
or what's the right way to get the
person to respond to the haptic feedback
so now we have those issues plus the
visual information provided by the
camera we also have text speech which
does exact it and have before so a big
part of this study will be will not use
computer vision at all it's just the
process of being a fly on the wall and
watching the way that
human operator would interact with a
blind or blindfolded person in the store
and then try to use that to to guide the
way we develop the automated system so
some of the things we're doing we're
expanding the core object recognition
ability bringing in new features with
that kind of combine sift with color
histogram type descriptors though I
should say it's not this is not really
where it we're not looking to do rocket
science in that regard we believe that
most of the object recognition
technology that's out there is actually
quite well suited to the Grows II
problem domain it's just a question of
putting it together in the right way and
making it usable we need to get these
algorithms poured into the mozi box and
we should we're actually aiming to have
the aisle sign reading working by mid
October and then in the coming months
start getting more and more product
recognition working on the device and
like minyan was asking we need to get
these do actual user studies to figure
out what this person is supposed to do
with their hands if they suppose they've
got a shopping cart and a cane and a
mozi box or shopping cart and a guide
dog and a mozi box and maybe something
else in their hand how much stuff what's
the right way to get their hand to that
box and so that's an interesting problem
and we we recently submitted a proposal
to knighter the National Institute for
disability and rehabilitation research
that would be a a three-year proposal
and so we're hoping to get that funded
we'll find out in October or November
this is just a snapshot of the
collaborators here and that the picture
shows John Miller who's a blind
researcher at Cal a key to and he's
teaching my students on how to use a
cane to cross a street and that's
Franklin gang from Qualcomm is watching
alone and as I mentioned that we have
collaborators at
Kentucky that are working on the outdoor
problem and then some machine learning
collaborators that are helping us with
the semi-supervised learning problem at
Max Planck Institute in Tuebingen and
then James Coughlin and John Braden at
smith-kettlewell are also advisers on
this project and that's it and I thank
you for your attention</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>