<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Research Scientist - ROMAIN THIBAUX | Coder Coacher - Coaching Coders</title><meta content="Research Scientist - ROMAIN THIBAUX - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Research Scientist - ROMAIN THIBAUX</b></h2><h5 class="post__date">2008-06-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/SIOYjfVEEFk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">was I'm very happy to be back here as an
intern in 2005 and I see that you know a
lot of things that's changed new
buildings new people and I'm actually
extra happy to be here because I needed
a new t-shirt all my old ones are you
know betterand and you know a little
interesting thing that happened after my
internship here was I went to in you
know intern at Microsoft and and the
contrast was very interesting people
here were saying you know it was 2005 I
remember the situation who was saying oh
my god
Microsoft is after us they're so big
there's so much money every company that
they've gone after they're dead
you know we're doomed and then I go to
Microsoft oh my god we're not Google and
oh you know where would they have so
much money and they're you know they're
in front of us so so you know that's
interesting to see a few years later
Google is still standing and Microsoft
still fighting so we'll see what happens
so today I'm gonna talk about my thesis
work and I'll first start by introducing
the field in general of nonparametric
Bayesian methods is probably unfamiliar
to a lot of you and then I'll hone in on
specifically what I did from even you
know further back if you want to say
what is this field it's the intersection
of computer science and statistics which
would be maybe a good definition of
machine learning in general but in
particular I'd like to emphasize the the
differences between the two fields one
is computer science one thing that has
done really well with you know the kind
of thing that students in computer
science get excited about first is all
these data structures lists and trees
and all the algorithms on them and with
statistics has been really good with is
how to make a good decision with core
data with uncertain noisy data how do
you do that and but was too
sistex has been working on is mostly
data that is you know d dimensional
vector so random a random variable that
is a real vector not complex data
structures like trees and and all that
so we're trying to bridge that gap with
these methods so nonparametric bayesian
methods to introduce them I'll start
with an example that is maybe the the
poster child for this domain and example
is clustering you have a number of data
points and let's say you know a simple
clustering problem where you want to
cluster points into you know into
roughly Gaussian bumps so here you have
a number of unknowns which are Theta the
cluster centers and then maybe you want
also to evaluate the relative size of
the different clusters if you don't
think they have all equal size so you
have pi is the proportion of each
cluster and the goal is to estimate that
from data so to complete the model you
need an observation probability which is
let me grab this laser observation
probability X I is drawn from a normal
centered at the particular cluster
center that excited that I item I
belongs to and with say here fixed
coherence so
oh it's just because this is
two-dimensional and D is D is the number
of clusters here okay maybe D should
mean dimension and so it right that's so
if you take a Bayesian approach to this
problem you will say all right type I
don't know these parameters I have
uncertainty about them so I will say
that I have a probability a prior
probability over these parameters and
now the goal is to recover these
parameters really you're doing inference
given the data use Bayes rule to recover
posterior probability over your
parameters so what you know I'll try to
give a picture of representation there's
really two things going on here there's
the model which is the particular
assumptions you made the number of
clusters and the particular shape of the
likelihood the observation model and
then there's the prior on that model and
all the space of possible ways that data
could be generated yet this prior and
these two things do kind of the same
thing they tend to restrict the the ways
in which you can explain how data is
generated and one of them is just a hard
constraint you can be anywhere here I
don't care but nowhere else and the
prior is a more smooth constraint I'd
rather you be here but you know if if
the data is really really says you're
here then okay you can you can go there
and you can view nonparametric bayesian
methods in two different ways the
traditional definition is it's doing
Bayesian inference but with an infinite
number of parameters that's one way to
see it but the other way to see it is
try to diminish the model try to get rid
of the model as much as possible and you
can never get rid of it completely
so here the the the number of clusters
is something that you might want to get
rid of because that's a hard constraint
and put all the work into the prior to
allow you to go to you know what would
be different models so one way to do
this is by using the directed process
darshan process is a random measure I'll
explain more in detail later with a
random measure
but you can imagine on some space here
the real line
and number of steaks with total mass
that sums to 1 and these these sticks
are random so their location and the
size of the sticks is random and and if
they have a particular distribution that
is the diversity process distribution
and they sum to 1 so therefore the this
little picket fence here is is a
probability distribution because it sums
to 1 and it's a random probability
distribution so what can you do with
something like that how do you you know
help solve the the clustering problem
from before so imagine you have a number
of data points called Y and one thing
you can do is to draw from this
distribution once you have but you know
the picket fence draw data points from
that distribution independently because
you have stick CA you have announced
your probability of falling in the same
group as the previous data point and
therefore data points will form groups
and you you know this patterns of ones
will be a clustering of your data the
advantage is that here the number of
clusters is random that as you grow your
data you will discover more and more
clusters and you have a coherent
probabilistic model over any number of
data points so the I won't go through
the whole history of the duration
process but you know just want to
highlight one part which is Escobar and
West who realized after you know this is
a definition and more understanding of
the dursa process and then finally it
was understood that really the dursa
process it's it's a very good tool to do
just that to get a clustering a random
clustering and here's the kinds of
applications that have
propped up with that use the directed
process in some way or another and for
instance things like protein folding
this is the case we're using direct
processes you can do density estimation
for the angles between two elements of a
protein and that's something that is
very important to be able to do the
protein folding because it it factors in
the the cost function that you need to
optimize to try to find what is the best
possible configuration of the protein
and so why is this so important to have
a random number of clusters after all
you could say well you know you could
use something like k-means and have a
fixed number of clusters and maybe if
you're not happy with that number or try
a few numbers around there and do a
little search that wouldn't be as
elegant but maybe that would that would
work but think of things like grammars
if you want to discover a grammar
you will need not just to guess what the
number of possible labels for four words
or phrases are but also how many rules
there are how many sub labels there are
for each particular label so now you
have lots and lots of questions of the
same type how many what is K and K means
that are all interrelated and there's no
way you can search over that space so in
in these cases were in more complex
models you you gain a lot by by going
this route so to summarize this is Bayes
rule and this really what we're using
the likelihood the prior and the
posterior is just the product of the two
so you have an important part which is
what is this prior and four primary
trick models where theta is just a
random vector and rdu are 2d then this
is something relatively simple but in
the case of nonparametric models you
will replace theta by G and
she is a stochastic process so something
like the diversity process is a more
complex object than just this data and
some of the examples of things that you
can build out of diversity policies of
things like directed trees of unbounded
depth and and fan-out so things like
grammars are a little bit like like this
thing partitions we've talked about that
binary infinite maestra and matrices we
will talk about this and then
distributions on on distributions that's
for instance the dirty really process
one of those all right so we're where I
come in in this story is in two places
it's a one is I worked on inference
algorithms for the directed process but
also I tried to go to cousins of the
dirty process you've seen all these
applications dirty process that's really
just with the dirty process and a few
variants that are at the core so maybe
there are things that we can do with
other types of processes and probability
theory has come up with a bunch of
interesting processes that maybe we can
use the properties of so here's one the
bit of process it looks the same on my
picture but it's not no it's no it's
like a different you have a bunch of
sticks also locations that rent are
random and Heights are random but it
doesn't sum to one and the sticks though
are between zero and one and what can
you do is something like this and I'll
explain you know more in detail what
exactly is this better process you can
do the the same thing you sample points
but instead of having a clustering you
will have a factor representation so
what I mean by that for each you know
think of these as features and with
probability equal to the height of that
stick I will have or not that feature
you get something like this so you know
if your if your Amazon maybe your
clients are not
into to be distinguished into a bunch of
groups they're separate
where you're pretty much the same when
you're in the same group and completely
different if you're not and not in the
same group which is what clustering is
about but maybe between this client and
this client they share the fact that
they like cooking and but also this guy
you know like said tonics and this guy
too etc so that's a richer structure at
the end but you can't really get with
right here you can have any of
probability yes yes
yes so you can view this as you can
belong to several groups at the same
time rather than just one or you can
think of it as features and saying I'm
represented by the sum of my features so
to introduce what the betta process is
I'll need to introduce a more general
family of things from probability theory
called levy processes that's going to
include the bitter process and it's
going to include other things and by
taking this higher level point of view
then we'll be able to see okay what is
common to the bit of process to this
other thing called the gamut process and
you know get a get a more general
understanding of these and do algorithms
that are common to all so first before I
introduce maybe prossies I introduced
that this is an example of levy process
that you're probably familiar with the
Poisson process and it's a very
important example and because from the
present process we will construct levy
processes later on so I will go through
the possible process make sure we all
understand so here is some intensity
function and the Poisson process is the
process that samples points
independently along the line that is you
so the the number of points that fall
into into an interval is Poisson
distributed hence the name and with what
parameter of pestilent of the Poisson
distribution is the integral of this
intensity so you can think of the
intensity as the you know the amount of
clouds that you'd have above-ground and
these points are the number of raindrops
that you get and so let's see so an
important property of the Poisson
process is that if you have two
intervals that are disjoint then the
number of points that fall into one or
the other is independent so it would be
you know if you're struck by a lightning
that doesn't make it more or less likely
that that's someone next to you will be
start by lightning - and one thing we'll
do is we'll talk about measures so here
we can view this set of points as a set
of point or you can view this as a
measure so what is the measure a measure
is a function that gives mass to sets
you give it for instance the set a and
and it tells you for because the the
mass of a is then the number of points
that are in here
okay so measures can be continuous for
instance the this intensity density f
you can make turn it into a measure by
calling it capital F and what is capital
F of a it's the integral of little F so
the amount of mass that F little F gives
to the set a so using this notation of
this terminology of measures you can
express this the fact that the number of
points here is Poisson processes Poisson
distributed with this much mass in a
slightly more compact way by saying that
this measure n which measures the number
of points in a set applied to the set a
is distributed is Poisson distributed
with parameter
which is the measure associated but
little F apply to cattle a any any
question here yes sorry see that's right
written that so big F of a is good let
me actually I yeah so a that's this
thing okay done so this is an example of
a personal process in two dimensions
works the same but just to have a
picture in mind what it looks like so
I've talked about discrete measures and
particularly you know the the sum of
points that fall in the set to be a
little more precise I'll you know made
more more clear but what this measure
looks like well draw it as a stick of
size one whenever there is a point here
so we can represent the opossum process
this way so let's call this pace Omega
so maybe the rail line here and little
Omega I is the is the red color okay my
alright if not all along change the
lights may be so little Omega I is the
position of these atoms and so this
point is little Omega a1 all right and
so you the the mass that falls into a is
the sum of one for all the the points
that there are na okay all right this is
easily generalized to different weights
than just one and the weights that is
associated to the set a is the sum of
the weights of the atoms that are here
so
I'm going slowly here but actually we're
very close to being able to define
living processes using this so levy
processes are discrete measures like
this that have the property just like
the Poisson process that the mass that
falls into a and the mass of false B and
if they are disjoint is an impenitence
that's the definition and now we'll give
a more constructive way to to build them
so there is something that it's
associated with a levee process by
theorem it's called the living measure
the levee measure is you can imagine
something like this it's a measure over
the the space Omega on which you want to
define your measure your random measure
plus an extra dimension and this extra
dimension is going to be the dimension
in which the weights of the points will
be drawn so this this this measure new
at the living measure I've you know
drawn it the big measure across
something here is just to make it
convenient to draw but can be more more
general measure and what you do with
this you draw a Poisson process with the
levee measure as base measure get a
bunch of points and now you look at this
sideways and you have you have a random
measure you you know you you join the
you make the sticks that go all the way
to Omega if you look at it you have the
same picture as we had before okay so
the mass that falls into a is the the
weight of the sticks that volunteer here
so now so a little detail here that
mention is here because it's a Poisson
process you will never have twice the
point if you draw another set of points
it won't never twice comment the same
position if you do want atoms that are
have a fixed position but
and weights you can just add that in and
also the third the third this height
that's the that's the intensity of the
living measure that's right so it's the
same picture as tu-tu-tu-tu-tu-tu this
one
yes let's say yeah same same picture so
now we're ready to define the betta
process and all we need by theorem to
define the better process is what is its
levy measure and also if it has
fixed-fixed atoms but what is the
distribution of those fixed atoms so
here it is and I'm not sure you want to
parse that but the so the livvie measure
to measure on Omega are the space on
which we want to define this measure and
P the intent the mass of these atoms and
if you look at this if you're familiar
with the beta distribution this looks
like a degenerate beta distribution
so hence the name better process also
the fixed atoms if you have any they are
better distributed so again that's the
name what does that look like looks like
this so this is the intensity of so this
is the levy measure again I've drawn you
know using a a uniform based measure b0
which is one of the parameters of the
better process but could be more general
that's right yes so the b0 can be mixed
continuous and discrete and the
continuous part is going to look like
this or you have a living measure I
associated with it and then the discrete
part you just take that separately and
you have a bunch of better distributed
atoms at fixed location
particularly this living measure is a
special it goes to infinity here and
it's not even integrable so what the
what that means is that you're going to
have an infinite number of points here
and but that's okay
because when you when you compute how
much mass there is in a particular set
the mass is still going to sum to
something finite because most of these
atoms are close to zero by the way the
the dursa process is not a living
process but almost up to random scale
and it has you know a similar behavior
it has an infinite number of little
sticks all right when I drew it I had a
funny member but really it has right yes
it's a theorem it's really the expected
mass contributed by a particular set in
that Omega cross P space is the the
integral of that so it's integral of
this
livvie measure but times X x times P
because you need to wait by the this by
how much weight is going to be given to
each point so if you do that if you
multiply this by P then this becomes
integrable and therefore you have finite
mass
that only the stick said any probability
and all the other real numbers have none
but what are you saying now is that all
the real numbers have a small stick you
just didn't draw the ones that are
nearly zero no I'm saying you're you're
both you're right on both ways so it is
true that the dursa confident number of
points that's still much less than the
uncountably infinite number of points
that are on the real line so most points
in some sense don't have mass and so the
same thing here yes yes so what do you
do with the bit of process you do this
which is what I sketched earlier for
each stick you will sample zero one
indicator variable to decide whether you
have or not this feature take this
terminology so here if I draw a bunch of
points here's the kinds of patterns that
I I see and I see that some features are
shared a lot some features are not
shared a lot Sarah so this this pattern
of points you could so if you wanted to
to do this algorithmically to draw this
number of points you have a little
problem is that the way I've defined it
you need to sample B first and then
given B sample all these guys but you
can't simple be completely because this
is an infinite object you have an
infinite list so instead what you can do
is ask what is the distribution of X one
marginally if I marginalize this object
and now what is the distribution of x2
given x1 and again marginalizing this
object and you can you can do that and
you get this thing called the Indian
buffet process which was actually
invented before we discovered the link
with the better process so history
didn't go the way I say and so given the
metaphor it goes as follows you have an
infinite so I have a big restaurant with
an infinite number of dishes and
customers come in and try a few dishes
and that's going to be which features
they have the first customer is going to
try a few dishes I'll say more precisely
with what distribution the customer too
is going to try a few dishes you know
make a decision whether to try the
dishes that have been tried already and
then maybe add some more new dishes that
no one has tried before
etc so the rule is that customer n plus
one tries each particular dish with
probability proportional to the number
of people who have tried it before so
the rich get richer and also will try a
Poisson distributed number of dishes of
new dishes that no one has seen before
and why a Poisson number that's related
to the fact that the levy process is
constructed from and plus all what some
process alright so what can you do now
with this that's an example of what it
looks like and and I've said this is the
same thing as the bunch of points that
that I showed sampled from the bitter
process but it doesn't look the same
it's just because I've ordered the
columns in the order in which features
are discovered so it looks biased like
this but if I randomize the order of the
columns and then it'll look the same a
so there are two things you can do it
what is an important property minute
maybe you're thinking about is
exchangeability
which is that if use permute the rows
here the distribution is the same and it
doesn't look that way because i've
ordered it in this particularly funny
way but if you shuffled the order of the
columns then it would be more clear that
the order of the rows is actually
exchangeable that's actually the
exchangeability is actually an important
property because when you have exchange
ability by definitely serum you know
that there exists some object that is
behind this and you know maybe if I'm
more time at the end I'll talk about
that and that
that's how we knew that there must be
something and you know now the game is
to find what is the subject turns out
it's the bitter process so here's a set
of examples of applications of a beta
process or Indian buffet process and so
this is more recent than the darshan
process of you know has less
applications but going there and it's
more specifically what you get out of
the connection between the dirty of the
meta process and the Indian buffet
process is this set of things size by
its construction which is a way to build
the bitter process in the order in which
the features are discovered by the
Indian buffet process so what kind of
merger of the two a way to sample the
beta processes that used in statistics
and they have algorithms to do that but
that things it's a much simpler way to
do it and then hierarchies a bit of
process we'll talk about that and and
new kinds of models of data so why why
new models of data just before I go into
hierarchies because when you have the
when you know about the Indian buffet
process and you know it's exchangeable
it's pretty hard to come up with a new
model that now would talk about counts
where you can observe a feature more
than once for a particular object you
have to reinvent another system of rules
like a new kind of restaurant and make
sure that it's exchangeable is pretty
hard but if you go the other way you
start from a process and you have
something that drawn independently from
it
you know that it's necessarily going to
be exchangeable so whatever you do is
going to be exchangeable so it's much
easier to go from this from this higher
level and and go the other way then then
history win so hierarchies have better
policies I need to hurry up now
so with what a hard key of better
proxies is
and before I started I should say why
did we even think of doing something
weird like this because Duras new policy
is also hierarchies have been defined
for them and then and they have been
very very useful and we'll see some uses
of that but already out there at the
outset just by an analogy we know that
doing hierarchies is going to be
important so hierarchy of better
policies is you have a better process be
whose base measure is drawn itself from
a better process what that means is that
the first the top level better process
is going to have these bunch of atoms at
random locations and this one now has a
base measure that is entirely discrete
and therefore it will have atoms at
exactly the same locations but there'll
be better distributed with mean the the
height of this stick so it'll be a kind
of fuzz dub version of this stuff level
one and then you can draw a samples from
this thing for instance yeah that's just
in math what I've just said so what
kinds of things could you think of doing
for instance if you have a bunch of
documents that are represented as the
set of words that it contain and each
you have a one if you have a particular
word so just the binary vector
what you want to do is to estimate what
is the probability of each particular
word and in the set of documents so
really you want to go after what is the
particular bit of process that this
thing is is drawn from if you have
several categories and you want to
estimate the probability of each word in
each category you could do this
independently and try to estimate maybe
with Laplace smoothing or something
the these probabilities but really they
are related if the is very common in
this category and some new category that
you've never seen before it's likely to
be also very very common and that's
important to find the relative size of
these probabilities very accurately if
you want to do classification if you
have a new document that you want to now
you want to say what is this probability
if I assume that it comes from sports or
for science from politics and you do
that's naive Bayes classification and
you want to recover what the most likely
category is then you want to know these
probabilities very precisely so you want
if there is information to be shared
between them you want to be able to
share them and that's something that you
could do with hierarchies of better
processes where you have a random base
measure that is you know a random center
around which each of these as is drawn
so each of these is a fuzzed up version
of the top level one so if you if you
use this compared to Laplace moving so
Laplace moving is a method we are to
estimate these probabilities you assume
that you have seen this this word before
in this category at least once and you
have not seen it at least once to make
sure that when you estimate the
probability just by the empirical counts
or empirical proportion of times they've
seen this particular word it doesn't go
too
which is doesn't make sense so if you
compare the probability that is given to
unseen document just on one category if
you if you have data from several
related categories then the the
probability is higher if you are so the
probability given to an seem data is
higher for the hierarchy of better
policies and that's really expected
because Laplace moving is not looking at
other categories whereas this is using
data from other categories so there's
there must be some information that is
drunk and if you if you are better at
predicting what documents will look like
in a particular category you should be
able to be better at classifying what is
the right category for for a particular
document yes so there's there's several
answers to your question one is what is
the number of hyper parameters if you're
if you're a Bayesian if you have you
know you put probabilities over
parameters you don't fix them but
there's always one or two parameters at
the top that you need to fix and that's
the what is the mass of the base measure
at the top level what is the
concentration parameter and the two
levels of the hierarchy so that's really
three parameters and now the other way
to answer your question is how many
things are you trying to estimate and
how many things do you have probability
over that's all the topic specific
feature probability for each word for
each category that's a lot of features
but observe that here because we're
talking about Lavy processes what
happens in a set that is in sets are
disjoint is independent therefore
inference on that particular feature is
unrelated to inference in that
particular feature so you can run
inference separately on a high key for
this feature and the hierarchy for this
feature
I don't know if I answered your question
on the line okay we get so so you you
optimized for these free parameters
no I so what I do is that I show two of
these parameters I can set using by
matching expectations of the flat model
where I have just just one level instead
of two levels here all right sue might
have one level then I can say for a
particular value of these two parameters
how many features do I expect to see and
how many distinct features do I expect
to see and so that's related to what
this Indian buffet process matrix looks
like and using these two parameters I
can match that to my data and and then
the last parameter I can fix that I just
one parameter now so I can go over a few
values and use cross-validation to
choose that particular sorry so how much
to gain how much of the red gain is
because you have a hierarchical way the
processes how much is because you share
he basically has more dignity I don't
know I have to you know that would be a
good experience to do I suspect if you
just merge all the classes together you
pretend that you have just data from one
class at some point is going to hurt if
you have too many classes now it's going
to look like you know some average of
all the classes not something very
specific whereas here if you if you use
a hierarchy and you add more and more
and more other classes it's not going to
make your estimate go towards the
average of all the classes is going to
to converge to you know something close
to your estimate if you had just one
class but just slanted towards the
average of all the classes yes so you
have the class ii classes is enough
given that you basically you infer me
what classes are no the set of classes
is given so so you're probably thinking
of using nonparametric bayesian methods
in the way that the directed process is
used where you have really a clustering
problem and you want to discover what is
which documents belong to which class if
yes so when I do this the choice oh so
you write right he's pretty funny yes
you don't have another process that
speaks out no I know I have some number
right right yes it's a much much less
ambitious application than if you wanted
to do what you're saying okay yes
right right and so these these types of
proxies are motivated by this but also
about the time you know by the type of
application that you're thinking but
here so why why did I start in this
direction because I want to make sure
that whatever I do is really really
scalable which is a question right now
on nonparametric bayesian methods how
much does it scale and the the the
difficulty is that really it's attacking
really hard problem it's like you know
nested clustering problems they're just
intrinsically hard so trying to go from
the other side try to find what is the
simplest thing that you can do to really
scale and you know try to make it more
complex that's what this is about
okay so if you if you use so this is an
experiment on the 20 news group data set
where you want to classify into one of I
don't remember like ten twelve
categories and and I compare hierarchies
of better policies to naive Bayes here
the the features that I used is all the
words in the document yes yeah the
question excuse me so if you compare
this to hierarchy code ritual into the
process I don't know if you so you are
thinking of using the hard key of dirty
process where you have again you observe
the the different classes but instead of
observing the representative document as
a binary vector of the words that it
contains it would be the multinomial
vector of the counts of each word and
then you have a dirty process over that
I haven't tried but I'll be interesting
and just so the words that are used here
is just cut at every space so you have a
lot a lot of junk a lot of features that
are extremely rare and that's why it's
particularly important to smooth smooth
them correctly otherwise they're just
gonna take over the estimate and the
naive Bayes here is instead of trying to
do so this is using the method I've
described with using cross-validation
and this is looking for all the possible
parameters for an a based on the test
set and trying to find what is the best
number and that's but you get but that's
really you know a case where you have a
lot of rare features and that's really
the the kind of places where it's kind
of method joints so inference I've
already talked about that you you can
because you have independence on
disjoint sets you can do inference
separately on each of the
of the features so computationally it's
very fast and all right so now I'll talk
a little bit about variance and maybe
there are questions this good time
otherwise you know things that you can
think of doing
yes question it's not that you lack
generality but if you go that far I
wonder why you have the P to the minus 1
here and 1 minus P to the C minus 1
there why not go all the way and end
another way a constant another
containing why is that P to the minus
wants a credit it's I think here it
wouldn't be that big a difference and in
the particular application I'm showing
for applications where you want to you
know do something more like clustering
where you want to guess the number of
features that's the more typical
application then your model and this one
are saying slightly different things
your model is saying because now you're
living measure becomes integrable you're
saying there is a finite number of
features and I just don't know you know
how many and what they are but at some
point if I see more and more data I'm
going to have I'm gonna know what the
features are whereas here it's saying
there's an infinite number of features
and the more data I see the more rare
features that will discover so it's you
know it's like two different assumption
but you could think of using that to P
to the minus a one and a half and still
if you multiply repeats integrable or
you could have people the minus half
that would still be that would be
anything greater than minus 1 would be
integrable because it would be the the
base measure would not be integrable by
itself but with what counts is the base
measure multiplied by P so you to so
then that would be incredible
alright so the kinds of things that you
would want also and I'll go fast here
let's see how much time do I have not
much so I'll I'll go fast another
example I'll won't really go into
details but you can think instead of
using a better process you can use a
gamut process and we've derived a lot of
things that we've derived the BRIT
process you've derived them also for the
gamut process things like the size by
its construction and hierarchies and all
that taken you can do that also
and yeah my policies are an example of a
process and that's why going to levy
prossies you know makes it obvious what
is the what is the relationship between
the two makes it much easier to go from
one to the other
it's not exactly the same but you know
it guides you a lot and what you can do
if the gamma process is have given this
parameter you observe a plus own number
of observations from that with parameter
this much and and now you have something
that's even more general than the indium
if a process is random binary matrices
because you have numbers here so an
example of image classification really
in this example I'm just going back to
the text case I transform this image
into a bunch of words quote-unquote by
taking sip descriptors which is
something standard and envision doing
vector quantization to get you a bunch
of visual words and then describe the
image as the factor of the binary vector
of you know words that it contains or
you know vector of counts of words that
it contains you can also so I've
described the gamma process now you can
also use the bit of process with a
geometric distribution instead of
Bernoulli distribution and that's also a
way to get counts and that has other
properties because the two metric is the
shape and the puzzle is more like well
bell curve so depending on your data and
one might be better than the other so
that's a gamma process again
this is equivalent to this is what is
behind a model that has been proposed at
nips last year mom will go into this and
summer is also in image classification
this is compared to support vector
machines so it it does pretty poorly on
the same set of features and we've tried
to reproduce as best as we could the
features from that paper that's the SEP
neck
it's then six but really the one thing
that is not shown in that graph is what
is the speed at which this computation
is done and this we're talking about
three minutes whereas here this is a one
versus all support vector machine so on
things like this is on the 15 scenes
categories you have to do 15 times the
support vector machine on all the data
and for things like the Caltech 101 data
set then you have to do 101 times also
the cost here is related to the number
I'm saying related I'm reluctant to give
a precise complexity because the the you
know packages for SVM have made a lot of
progress so I'm not sure what the latest
number is but it can be it can be just
linear and and these methods are linear
basis aim cost within constant factor of
night base that's pretty fast so this is
pretty expensive and yes that's right so
seeing that we said okay let's go to a
data set where there's really so many
classes so much data that SVM can't
compete and see whether we can do
anything and that that data set is the
80 million tiny images data set from
table Ferguson Freeman that's a very
interesting data set they've collected
eighty million in it images from the web
from various sources and reduce them to
size 32 by 32
and they have a an argument and you can
tell from yourself that even 32 by 32
which is really really small you can
tell a lot of stuff that's going on in
the image and and that that makes you
know also it more amenable to like
putting it on one computer but now you
have so what what how did they do this
they took the word annette database of
words in english to call it or the words
that were concrete and sent that to
search engine so now you have lots of
categories associated to all these words
so that's my fifty thousand categories
of something and and you know a million
images so these are examples and the
warnet so you know before we go into
classification we can't really think of
classifying and and be judged on exactly
recovering a beauty consultant from just
looking at you know the face of that
person so instead we use the word net
hierarchy of hyper names and and we
score a guess by how many nodes and the
hierarchy do we get right except there
except the root and we can also think of
using this hierarchy to have a hierarchy
of Libby Libby process of beta or gamma
processes that follows also this
hierarchy because it is expected that
images will be similar visually maybe
but you know at least that priori if if
they are from related categories and
that's roughly visible in this picture
is drawn from their websites of an
interesting picture that's all of this
data set compiled into one big poster
and the position of so each dot here is
the average of one category and the
position is related to so things that
are close and in the tree are close
together on the poster so you can see
that you know plants are here so there's
some some hint
visual similarity at least that's
associated to semantic similarity so
we're gonna use very simple color
features based on that poster ever see
that color at least is related and we
would like to use more advanced features
but for now that's all we were using we
just call it cut the HSV color space in
some way and the scores that we get so
if you were to guess one of the leaves
at random that's the kind of score that
you would get no I don't want to back up
now that's not a good time and so now
you can compare several things Laplace
smoothing with a gamma Plusle model to a
flat hierarchy of so flat would I call
flat hierarchy is the same as for the
text before it just one level and then
of using the word that hierarchy so you
see an improvement from from one to the
other and then the same thing but using
better geometric model and you see that
bit of geometric must be a better model
of what this data is and that's some
analysis an example but I'll just leave
it at that I think I'm late so thank you
very much oh yeah so imagine the tree of
all these hyper names like
generalization of words and you have the
this tree that whose leaves are all
these categories from which you have you
have data and now the score is if this
if this image I say it's it's actually
from this category then I find their
common ancestor in the tree and my score
is how many nodes in the tree do I have
in common so if I'm in the same category
all the nodes will be common so the
higher is the better
the thing is it's it's it varies
depending on this it's not a fixed size
hierarchy so like that's one example was
this thing yes so so these scores are
you know like you're not very deep but
this is very difficult data set and
using very very simple features though
so for image classification example
which paid us that figure exactly 213
the 15 scenes sorry
yes you do these HSB features again so
you know some like that is stepped I
don't want to have I want to have some
some kind of features that are that
correspond roughly to visual similarity
so here all the black part
I say that's one feature because I'm not
able to distinguish between this black
in this black people or something else I
do account so think of all these as bins
and I count how many pixels fall into
that bin so I have a each image is a 124
the number of colors here that mention
elector
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>