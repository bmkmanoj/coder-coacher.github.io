<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Music and Machine Learning Workshop: Automating Music Search and Recommendation | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Music and Machine Learning Workshop: Automating Music Search and Recommendation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Music and Machine Learning Workshop: Automating Music Search and Recommendation</b></h2><h5 class="post__date">2012-02-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/a6z0Kh8FXyU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">alright good morning everybody thanks
for inviting me doug is awesome workshop
let's get it started i'm going to talk
about music search the recommendation
today i'll try to keep it fairly high
level this morning just kind of some to
brutally at this time of the day but i'm
going to talk about is bringing the
right music to the right people at the
right time in place basically I've use
the following here as you probably all
know in the last 20 30 years things have
changed all out there from big studios
and pick CD stores to very small crack
studios and people's garages to online
uploading of music and now tens of
millions our hundreds of millions of
songs we live anywhere online as opposed
to maybe meet a couple thousand songs
being easily accessible it's east source
so because of that big change what you
have is a setting where you must have
accessibility to millions of songs that
are accessed by virtually anybody on the
planet right and so in your hand there
is all these very interesting devices
that are out there to consume music
cancel big challenges how do we get
music that we're interested in from this
very big pool on to our devices that you
know playing is music and so rightly
already here is how can I find the music
that I'm interested in we are just
ending up with you know Justin Bieber or
Britney Spears and also the time and
especially how can I do that when I
don't really have a good clue of artists
or song name sometimes because there's a
lot of people there listen to some have
your own control or ring home the first
successful dating a long time if they
want something I just playing saxophone
or something it's just October
thirty-first and there was some scary
how it music right how can you find that
type of music without knowing artist
things or some titles I want me to go is
you know building here algorithms I can
automatically listen to these millions
of sounds that are out there
automatically tag them with meaningful
words or meaningful descriptors IE index
the music that's out there and use it to
build a search engine visit the idea
here is is to be next music with all
kind of tags like a booty Shotter tags
but not just John it's also things like
you should be trying to fear when you
turn off the visitor a lot of the note
but this is with him so our time music
with tags married as mood as well
instruments user chipman like you know
and so the earlier years can I build an
algorithm that I feel using waveform and
this is really what I'm particularly
interested here because there's many
different ways that you could think
about associating all these type of
times you're using but we're really
interested in is and we just keep the
waveform for them either and how Peter
listen to the music and then
automatically tag it with appropriate
tag like for example jazz romantic piano
and before they in this case here and
then you know basically do that for any
song that you have in any large database
either a personal collection of music
that's out there for example myspace
then once you do that now you have an
index database or an okay to music you
can then use that to deal with queries
wide for the jinc simply by matching the
tax from the credit with antagonism use
that generate place for a radio station
right or you guys do things like reading
the system with the songs and given a
seat some what you can do is you can
recommend other songs from the database
and similar attacks right and use that
to generate a playlist of a radio
station and so in some sense that's how
like what what them but I think genius
does but it's live over here what we're
doing is we're busy now so entirely on
the way for my laughing genius probably
has some very different mechanisms that
they use most likely for example
collaborative filtering for the people's
plates and stuff like that other than
analyzing content of the song and Nora
has some type of music storage
functionality but as many of you throw
here know the limitations that they have
is that you have to kind of like see the
radio stations with artists names or
some titles and there of course never do
the songs that they could manually index
which is a lot less than number of songs
for example I do nicely already one of
the problems with genius is that I'm so
it works well for fairly well-known
music or no music that's you know the c2
by relative least an amount of people
but if you feed the genius let's say
with some music then you just recorded
your own garage then it's not going to
give you any recommendations in fact
that I destroyed you every day that some
very recent popular teenager music when
you would call it the genius agree
actually I'll be able to give you a
recommendation because it's only been
popular for like two weeks inside that
didn't have enough music group is you
talkin ta I'm in the memory surgery
that's what I was in this next time the
two cookies here so anyways I think here
is is that if you're building a system
that can and a lot it analyzes waveforms
and indexes music based on the way for
me to basically and they'll analyze any
song for which you have the env3 out
there because I just talked about you
can kinda like view with all kinds of
credits both praise driven by an artist
name or some kind of it also like just
natural language queries that don't
involve any artist or song titles or
seats on right anyways that's kind of
like the setup what's interesting here
with respect to VCU is that we actually
didn't onam eyes playlist survey i think
it was about two years ago based on one
on power auto-tagging sisters and I she
turns out if you are mice the whole
survey that get people vote in favor of
genius or
things that we generated with equal
probability which is actually a very
good result with in some sense like you
know for something that is genius we
provide recommendations for this is
someone I like you know the best
possible way of doing the recommendation
right you know the how songs coherent
playlist but being able to match that
with an auto dying system was actually a
very good result so there's certainly a
lot of future do that trying to
automatically update waveforms and
building that created a system of music
recommendation based on that of course
what you can do as well if you have it
available the mood music is popular in
all that you have a bunch of other
information about music an available
like images about the artists or song
lyrics or collaborative filtering
information or videos or blogs or things
like that when you have that information
available you may also think about
incorporating all that into your
recommendation engine we really want to
talk about that too much today but then
also you know a possible direction
consider for music for which that extra
information is available for example on
myspace and other sites like that okay
so that's kind of like that the high
level picture here now how do we build
is this black box this a lot of agri
well two main ingredients here are doing
some single processing and some machine
learning basically what we're doing at a
high level for starters is repaid you'll
be collecting a bunch of examples of
songs that characterizes our tag for
example rocks ohms and now what we want
to do is we want to is an entry an
algorithm that I recognized what is
typical to the acoustic content of these
rock songs and then extrapolate that we
use of so once the album has horrible
state fans for the drop songs it then
recognized that pattern in a new
somewhere they'd never seen before
inside that I show you a rock star
pattern so of course then the question
is like how do we how do we really do
that internally it's kind of like to
thank it for a first topic of this talk
here and so the next week several
different ways together to build these
models that recognize difficulties with
patterns and songs regenerative
approaches and discriminatory intent i
figure i'm not gonna spend too much time
on discussing all these different types
of versatile what i'll do is i'll give
you some fun example one approach here
that we were tongue which is different
from verses that for example darbar
tongue but my which i think that for
sure i'm gonna try to do is not really
like trying to say that this is the best
possible approach or anything like that
but i think what's interesting here is
that the little exercise of seeing how
the initial approach to possibly be
improved to take into account you know
temporal structure in music not going to
go to the world structure as in some
white and bone structure at least trying
to go beyond these fifty or 200
millisecond segments which is what islam
was long enough so again so this is not
the way to do it but at least it's an
example that will show you how it would
possibly little bit more of longer term
and grow mold room so what we're doing
here is we're first of all as I say
collecting a set of example songs it is
associated with a certain time for
example the rock type right and what
we're going to do here now we're going
to take a generative approach and we're
going to estimate the distribution of
the REO context for that tack alright so
we're basically going to try to
statistically more the typical audio
content is for songs that are associated
with that tag through some distribution
be an Oregon do that is by the English
at the song level so let's look at this
just at the song ever from how so
you have to model the statistical
distribution of the audio content of
just one song all the way this is
usually done using take the song you
chop the song into little pieces little
frames which are usually as I said can
be 50 milliseconds 200 milliseconds
something like that and for each of
these frames are going to extract
feature descriptor right the MSP diploma
each directors to be all kinds of the
very often in our services and elders
you know quite popular which is spectral
based features for each of these frames
you extract the future vector you
present that in some high dimensional in
judgment okay and so the nice thing you
do is basically mold that plot of a
feature vector with some statistical
model using expectation maximization or
even our favorite Calgary so now how do
you do this for attack while you
basically the exact same thing attack is
basically a commander and some
associated with so for each of these
song to extract the feature vectors you
throw them all together in one space
right and then you fit the shape of that
cloud with a statistical model by
running am and as a result you get
distribution of difficulty feature
vectors for the tag rock right as simple
as that there's some challenges with
this here like for example some
challenges are that if you have a lot of
song associated would attack you can
have a lot of data points here so that
the question is how to do this
efficiently they'll talk about this in a
second but basically that's kind of like
the approaching and once you have these
type of models for each tag in your
vocabulary and you can basically
annotate a new and seen song by just
taking the song chop it into pieces stir
it into a future clock on it as well and
then just kind of run this feature cloud
by each of the models that you have
available and then just apply some
simple Bayes rule allows you to come up
with difficult semantic annotation some
kind of like somatic histogram right all
the annotations are a tractor eyes the
song that consistent and never see now
back to the modeling so how this is an
example actually
this is just an example of such semantic
annotation vector four in the skills of
a top chili pepper some where you can
see that we can have vocabularies of
100-200 even thousands of times or more
organized in different categories and
then basically the height of each of
these bars here indicates how likely a
certain word is to be associated with
that song as a result of anyways what I
want to focus on is that distribution px
that we are using to model the audio
content for avian attack so the song
associated with tired and so for the
first kind of a most simple model that
we could use here is a gaussian mixture
model and so basically when we're going
to when we're modeling that feature club
with a gaussian mixture model but we're
basically doing is as I said we we take
the song we chop it into pieces but
we're somehow throwing all these pieces
which are 2282 maybe 200 milliseconds in
length throwing it into a bag and of
losing all the order information between
these pieces because all we're saying
here is that we're going to model this
cloud of feature vectors with a gaussian
mixture model the soul we're doing in
that case but so from the perspective of
this gaussian mixture model each of
these samples is drawn independently and
in no particular order from the gaussian
mixture model which means that if we
take all these frames here and we kind
of mess up their order then whatever you
obtain in the end is still equally
likely on an under the gaussian mixture
model right because it doesn't really
take an order of a string to account so
how can we do something about it so
while a first step to a right to some
temporal information to a pound could be
to glue let's say 200 of the strings 100
or 500 something like that but a good
number of these strains of consecutive
frames glue them together into sequences
of 5 to 10 seconds let's say and that's
what I've gone down here so I do these
together what I'm going to do that is
I'm going to mow the sequence of frames
that I obtained here as a sample from
pat's I'm horrible more specifically a
diagonal texture mobile or basically a
linear dynamic system what is what is
that type
basically a model with a hidden layer
and observe players so this model has
observation variables which basically
characterizes the instantaneous spectral
characteristics of each of these frames
here but there are conditioned upon a
hidden state variable X here which
evolves throughout time according to
this linear equation here so first thing
to observe is that this hidden state
variable X is a continuous variable so
we're not talking about discrete
variables here like in HMMs one of the
reasons why we chose that is because it
terms are often used in speech
recognition where it makes sense to
assume discrete States where each state
is associated with a phony in the
language for music it wasn't so clear to
us that musical phonemes was a
meaningful concept necessarily I mean
this could be debated but so in order to
accommodate that we chose for a model
with a continuous hidden state here
leading us through linear dynamic system
where continuous aronsdale he falls over
time through is a linear equation here
which basically models the temporal
evolution of my observation very wide
Rises instantaneous practical
characteristics like tempura and mitch
and ray is them so now there's one
problem though which is that if i would
now draw the entire song on the slide
here and i will start drawing random
like five to ten second frames Pratt
five to ten second sequences from that
song and it's not very likely that all
of these sequences could be modeled with
the same dynamic texture reason for that
mean is that sums are heterogeneous
right there could be you know it's our
solo and intro outro lovers of quarters
and and it's not very likely that five
to ten seconds sequences from the car
solo can be moved with the same dynamic
system than you know five to ten second
sample from
for example so therefore what we're
gonna do is we're going to model songs
with mixtures of these dynamic textures
more specifically let's say that so we
have a song here down here and what we
have is a mixture of dynamics lecture so
it's a mixture of of these via dynamic
systems that is obtained as model for
the Sun amazing the way to look at this
is by by realizing that every five to
ten second segment from my song is now
basically a random draw from one of
these for linear dynamic system so for
example in the intro these ten seconds
sequences are draws from one of the
dynamic systems that characterize my son
and then if you go further down into the
song by for example there's a verse
somewhere where the ten-second sequences
are drawn from another in your dynamic
system you go further down into the song
there's some drawn from another major
component alright so this is how the
modeling software but still we're not
holding the sounds with like 25 or 35
mixture of all so we're really trying to
find some structure in the sum where for
example early on in the song you may
have a verse that is modeled by the same
dynamic system then you know is towards
the end of the song for next verse or
something like that right one okay so
basically what we have here is a model
of music that does look beyond 200
milliseconds sequences or 200
millisecond frames and really looks at
the dynamics within at least five to ten
seconds sequences so of course now the
question is how do we estimate tag moms
well if you do the same thing again we
can just look at all the songs that are
associated with that tag in our training
set right then extractable sequences
from all the songs and thrown together
in one big bag and all the bag with my
tag mobile right it's basically the same
thing but now instead of extracting 200
millisecond seconds from all these songs
you're going to be extracting 10 second
sequences and all collection of all
these sequences with an overarching tag
model right now in this case is also a
dynamic pitch texture mixture model the
problem here is is that in order to do
the statistical estimation if you have a
lot of some associated with the tag
becomes very very expensive to just run
regular a.m. to fit this mixture of
dynamic textures at the tag level so
what we're doing is we're ever without
it is in two stages so first of all
we're just going to run standard p.m.
for each of these songs to estimate
mixture models for each of the songs and
then we're going to use an algorithm
which is called it's kind of like an
hierarchical am hollering that will
cluster all these mixture components
right into a model that has fewer major
components of course one thing if you do
is you can just a union of all these
some models kind of like all aggregated
together but then you end up with a
mixture model with a lot of mixture
components here like if you have a
thousand drop sounds associated with the
tag rock get 400 mixture components here
or 4000 to do inference with that is
just patiently very inefficient so
basically what we're doing is we're
we're looking at all the mixture
components we obtained here we're going
to cluster them into the most meaningful
ones where the mixture components in
your resulting model could be different
from any of the ones in here so you
could actually obtain novel cluster
centers here so to speak the way this
works mathematically is what we do is we
draw virtual samples from all of these
models and then basically implement an
order am step on top of them then it
turns out that because of the law of
large numbers you can actually integrate
out all the samples you draw from these
models and efficiently estimate this
higher level tag model from the
parameters of all of these song mental
models it's kind of like what happens
under the hood but the bottom line is so
this allows us to estimate tangles that
you take into account temporal dynamics
of music I'm not going to go into too
much more detail about this model here
for I can show you a couple quick
numbers on how these models form you
have to talk a little bit about how r
get my cleaning data how do i get my
thousand examples of rock songs for
example so one of the first ways we got
around this was
five years ago now was by conducting a
survey amongst undergraduate students in
the music department where basically was
kinda like a little mini Pandora so we
gave them let them listen to a song we
gave him a big survey and they I know
their music with tags from all kinds of
categories this was actually an
interesting experiment to realize how
much time and money it takes to collect
you in a small data set but anyway so we
had a data set at that point to call it
Cal 500 and so we had the first dataset
that we can evaluate our systems on the
way we evaluated here for example this
is just one metric that we're using is
by using single Tigers and see which
sums the system the automatic system
ranks in the top ten recommendation
results and look at how many of these
recommendations are actually allocated
with that tag in our cal 500 dataset
okay and again without giving you too
many more details about how the
experience were exactly calm down I mean
there's some cross validation all kind
of things although message I want to
give away is basically this small table
here which doesn't mention a lot of
other off of have your results and
things like that but which basically
compared to the models that we just
looked at here which is an GMM based
model a DTM based model and then to our
DTM based models the first thing to
observe is that the performance I mean
one is hot it is good closer to one is
better closer to zero is worse so you
can see that the performance actually
does go up with roughly ten percent by
going from these gmm models that look at
small frames up to these dynamic picture
models that look at five to ten second
sequences what you notice here is these
models are still than an extra mixtures
with their train in a different way for
example this is the DTM mo with the 4000
mixture components by just aggregating
everything after fitting the song models
and this is what is obtained by just
running em on all the songs together so
without doing this hierarchical TM type
of estimation and so what this tells us
is that the fact these numbers are lower
especially that this one here is lower
tells us that
hierarchical type of estimation
procedure you're actually regularizing
the estimate as well in some way and
that's why Foreman's actually goes up or
goes down you go down the table here
that's some kind of interesting results
as well from machine learning
perspective that anon for computational
reasons is a good idea to use this
hierarchical estimation procedure but
also for regularization type purposes it
actually seems to be a good thing to do
that all right any questions so far it's
my time you are just yo que about 27
minutes sample us about to hold up the
time anything okay alright so basically
so we see here that using a tempura
balls really doesn't make a difference
so that will certainly an interesting
result now back to this how 500 dataset
and trying to by the way just for
information this data set was collected
like about five years ago so nowadays
there's be a lot of our efforts that are
being done that go far beyond databases
of only five hundred songs anyways
what's nice about this data set here is
that it's hardly annotated so for every
song we really have a clear indicator
from the person who took the survey as
whatever tag applies or not which is
valuable in certain ways but still is
very expensive to go like this so what
we did next to try to collect data a
slightly larger scale is resorted to
crowdsourcing on human computation try
to turn the time that people spend
playing games into some musical data
collection on the back end so what we
did is we designed this online music
game that's it incorporated into
Facebook actually and so the idea is
that people kind of you know go spend
five or ten minutes playing this game
this is mostly geared towards casual
gamers and so so what happens in this
game is that when people enter the game
on Facebook there's a song playing which
you should be hearing now because
there's no no speak
dear Motty music is my battery it's
easier for me to explain it actually so
while the song is playing there's a
whole bunch of people that are playing
the game at the same time they're like
in the same room at the same time so to
speak and so they get like a little game
that's presented to them in the middle
of the screen that somehow tries to ask
them some questions about music there's
a way around here that try to pull
people for the knowledge about the music
which you'd only usable you to
entertainment a little bit but the games
are important are the ones that are
played right after a song starts playing
where for example a bunch of jars are
shown to do people are playing the game
and they have to click on the genre they
think is most appropriate and then once
the timer is up here they can actually
see what article click done by seeing
how these things TRO just be like these
things row has other people have clicked
on it and then based on the agreement of
the crowd that you actually gain or
point if you agree with the majority of
the people in kinder room there so
anyways I'm it turned out we implemented
this game on facebook and in about think
it was a matter of three to six months
roughly we had more than 8,000
registered players about 150,000 of
these game around plate and so this
surname is allowed us to collect is a to
collect a good amount of tagged data but
of course and the question is how good
is the party of this type of data are
compared to doing like a survey and a
controlled environment with students
with a musical background etc etc right
so anyways in order to evaluate that
what we do is we use that data to train
our search engine or better and train
our army veremos you cannot eventually
using the search engine so specifically
what we did is we used the data
extracted from the herded game to train
our machine learning models and we
tested the predictions of our moles on
the hell 500 data sets Thanks ok and we
compare that to something that we
thought was probably you know a fairly
viable alternative which is compared to
a mobile app restraint on Pandora
which is they thought I was annotated by
musical experts right because the big
question here is can an online proud of
just casual players provided the data is
as good as data provided by musical
expert constrain amusing alienation
system I was kind of the question now
how do we get this data your brother
died isn't really publicly available in
any way but it turns out that you can
actually crawl some of their data from
some of their web pages this was an
effort that was conducted entirely by
dr. pol while he was at Swarthmore so we
actually have this data set available as
well for people who want to use it if
will used to be called swap 10k it's now
called the Calvin k dataset so it is
probably available if you're interested
in using it so we use these two datasets
to train to any parent machine learning
systems and then tested both on our
golden Cala 500 data and as it turns out
over time as more and more songs were
analyzed by heard it turned out that the
performance of machine learning systems
trained from her went up and up and up
over time and in the end we obtain a
performance about ninety six percent of
the performance of the expert training
system which we consider as being pretty
much confirming that we can extract
useful data or image as usable from just
casual gamers of music experts and this
band here is so we actually did a couple
experiments and then average starts a
result so that's why we have error bars
here and these error bars are shaded
until the point that this performance is
no longer significantly different from
expert performance here based on some
care one-tailed tests with equal to zero
it was that so one of the things that we
started realizing while we're doing this
was that if we are going to use data
from the game to train a machine
learning system then we may as well use
the machine learning system to feed data
into the game and maybe present that day
time to the game that we believe is
going to be the best data to improve the
machine learning system faster or what's
known as active learning can we actively
integrate the game and a machine
learning system to kind of like making
machine learning system more intelligent
quicker since I only have a few minutes
left I'm going to kind of jump to the
details here but the idea is pretty much
done for this so what we have to do here
is we have to somehow figure out as to
which song should we feed next into the
game which song should be analyzed next
in order to update our machine learning
holos more effectively in the next round
of mobile updates and so we can put an
approach which is actually fairly simple
as a first step approach with it isn't
an approach that uses the current models
to suggest the next songs to be fed into
the game and if you do that if you
engage in an active learning loop like
that it actually turns out that the
performance of the her today's the day
the day by extracted from the
performance from the that can be
obtained by playing machine learning
models of the data that was extracted
from her tits so far in time actually
goes up quicker in fact it reaches this
96 performance in about half of the time
of you know doing some random sampling
strategy it also reaches this
statistically significant point in about
half the time or have the number of
analyzed sounds as when you just do a
random sampling strategy so basically
what this tells us is that doing some
kind of active learning like that really
allows us to extract useful data from
casual gamers more efficiently and with
less human effort and if you just do
some kind of you know random feeling of
data to begin a couple more things here
so everything follows from US stocks
that we have
don't mess with future music so one
thing that is interesting by inviting is
gaming to pandora is that they should
provide you with demographic information
so as a result of that we can actually
start analyzing whether it is any
different between mellow rock as judged
by a person from one demographic
category versus another demographic
category and so you can actually start
personalizing some of these allocation
moles so that's one evening and the
other thing that I think is kind of
interesting as well here is basically
how how do we bring the recommendations
to people now so one of the rocks were
trying to pursue is called zero click
recommendation which is basically
instead of requiring a person to
actively provide queries as to what
they're doing or what they're feeling
like in order to construct playlists can
you actually use the sensors in a smart
phone to automatically detect what a
person is doing or how they're feeling
and then you know provide
recommendations based on that so
basically you know your phone is in your
pockets there is a headset pointed to it
and you kind of like listening to music
that changes depending on what your
phone things that you're doing or
feeling light so may think that this
sounds a little bit futuristic but the
thing is that you know in some sense all
these the signals from the sensors in
the phone at least accelerometer signals
for example or just one dimensional
signal so you can and I've tried to
analyze them with similar techniques as
the ones you used to analyze music with
right I think you know having worked on
music we have a competitive advantage to
try to you know build something right so
anyway so let's say the system detects
it is running and play some energetic
music if they're just now we have late
at night and you know play some mellower
type music so anyway so that kind of
like completes my talk as in like you
know right music how do we you know
figure out what romantic music or jazz
music is bringing to the right people
can be personalized music search and
recommendations and also coming off
we kind of provide these recommendations
at the right time the right place all
right to kind of provide music to people
in different circumstances from this
very extensive pool of possibilities
that's out there I certainly have
excessively tank a lot of PhD students a
master student and other graduate
students that work with me on this and
transit funding from all kinds of
sources a very central mention here
thank you very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>