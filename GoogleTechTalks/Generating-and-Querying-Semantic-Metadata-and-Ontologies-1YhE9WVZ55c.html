<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Generating and Querying Semantic Metadata and Ontologies | Coder Coacher - Coaching Coders</title><meta content="Generating and Querying Semantic Metadata and Ontologies - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Generating and Querying Semantic Metadata and Ontologies</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1YhE9WVZ55c" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay gaydar thanks for the introduction
as you see here I want to talk today
about some aspects of on the one hand
generating semantic media data and on
the one other hand how to query this
kind of metadata from a more user
oriented point of view what's the
structure of my talk I will start with
some motivation why these topics are
important from our point of view and
then I will have two main technical
topics today one is how to extend the
well-known MediaWiki environment with
some semantic aspects and what kind of
edit values you get from adding these
kind of semantic aspects and the second
part of my talk will address how to
provide natural language interfaces for
growing knowledge bases and how to
support the adaptation of these very
interfaces to different kind of domains
because that is typically one bottleneck
you have when you provide these natural
language interfaces and then I will
conclude with some outlook on future
topics we are investigating ok let's
start with the motivation well when you
think of the kind of applications a lot
of people are interested in integration
is always a very important issue so you
see all these environments popping up to
integrate some component applications
into more bigger applications we have
these recent developments like Yahoo
pipes on the one hand or Google mashup
editor on the other hand but basically
it's some kind of manual tasks you have
to do in order to bring up that kind of
application and what you see in the
background are typically rather simply
structure there are data models that are
the basis for bringing these kind of
applications together what we currently
address is what we call the notion of
content integration where we are
interested to bring together content
from various resources and these
resources combine various kinds of data
model so you have
my structured sauces you might also have
some textual sources you might also have
multimedia sources and what is from our
point of view and very important aspect
to really be able to do that integration
on a more content oriented level is that
you need some kind of semantic metadata
to describe these two various kind of
sources and therefore have a better
basis of how to define the relationship
between these different kind of sources
so that is the integration aspect that
will be addressed by looking into that
semantic metadata area and then the
second aspect years are when you think
of how to access our information sources
well we are all familiar with these
keyword based search and one could say
well when you talk about what you could
tell these are called these short tail
queries in these very popular domains
like music and that kind of stuff that
works very well but when you go to the
other domains that you could call these
long tail queries where you have
specific domains not these very big user
communities there is some advantage of
providing a little bit more of semantics
to describe these kind of domains and
then being able to do some better kind
of very answering in these domains and
that it will be the second aspect I will
address in my talk today trust to give
you an example what kind of application
we are currently investigating in one of
these you funded projects we are
involved in in Kaltura one partner is
the United Nations in Rome and they have
a big department that is addressing
fishery and agriculture issues and for
example they are developing some kind of
fishery assessment system in order to be
able to answer queries about why you see
for example that examines the stock of
tuna is depleting and that these are the
kind of issues they have to address by
getting a lot of queries from all kind
of countries that are supporting that
United Nations organisation in Rome what
we have there is a typical situation is
that you have a law
of different kind of resources that are
available we have some documents about
fish species who have databases that
describe the current situation in
various oceans for example and what's
going on in different kind of countries
that live to some extent on on fish
industry then you have our
organizational aspect being described so
we have really a very heterogeneous kind
of a collection of resources and in
order to come up with application still
one of the major bottlenecks how to come
up with all these semantic descriptions
and therefore one challenge is how can
you provide environments that are usable
for a lot of people in order to provide
some kind of semantic information
without having first to study for five
years all kinds of logics doing a PhD
and the Semantic Web area and then being
able to provide that kind of data so how
to provide a path that is accessible for
a lot of people in the end and then the
second point is how could you come up
with user interfaces based on natural
language so that they are rather easy to
use in the end and then you have all
these problems of how to handle these
ambiguity issues when you pose these and
natural language queries and also you
have to handle the effort of how to move
from one domain to the next one because
if that ever is to high in the end it
will not be feasible to use that kind of
natural language technology and then
several you have to think about how to
reduce the effort of making that
adaptation step from one domain to the
next one and that is part of that Oracle
system that we have developed in Carlson
that will indicate some of these aspects
okay so let's come to the first part of
my talk how to use these rather
straightforward the wiki environments in
order to enhance them in a way that you
can really provide semantic metadata in
these in these environments okay here
you see my running example that's
related to Croatia in Europe and the
example is that you talk about a
specific island in Croatia called branch
and then you see the
typically you are familiar with in
Wikipedia so branches related somehow to
Croatia it's somehow related to a tragic
see for example it's also somehow
related to tourism because that island
is to some extent living on tourism but
also to some extent on fishing and what
you see here is the typical structure
you have these individual pages and that
are linked to each other it's a human
reader you can put in much more semantic
relationships when reading that page
compared to what a machine is typically
able to do so when we think of these
kind of links well we have perhaps and
then we have all these kind of other
entities libraries are tourism fishing
and from reading that page as a human
reader we are well aware well that we
have something like it belongs to
relationship between Prussia on the one
hand and Croatia on the other hand or
that prat word some is located in the
Adriatic Sea or that we have as some
cities are located on that island or
their posh lives for example on tourism
and then the question is could be
bringing some more semantic stuff into
the basic MediaWiki system in order to
be able to capture these kind of
semantics that are very easily catching
up by an human reader but are not really
available on the machine level when you
interpret these cuddling's and that is
the basic idea then of how we approach
that kind of scenario we have in essence
two type of extensions one is that we
are able to type these links that will
approach to tourism for example and
second extension is that we also type
these attributes that are round so when
you talk about the number we had
inhabitants of a city that you can
really be precise what kind of meaning
that attribute has in the end so that is
the standard syntax we are using and
that makes the ibiki environment to
define these links here between Pratt's
on the one hand and Croatia on the other
hand and we speak as far as possible to
the available syntax in order to make as
less modifications as possible in order
to come up with our awesome
the extensions and here you see the two
type of extensions that are added in the
end so we precisely define the type of
link between products on the one hand
and Croatia on the other end so we know
that they are now we have defined a
belongs to relationship between proud
and grayza but the user is free what
kind of a name he wants to use for that
relationship so there is no predefined
ontology that to some extent preoccupies
what you are allowed to do in the end we
are still free here to say belongs to or
who could also say his part of or his
associated worth but really he appeared
the freedom that in order to support
that bottom-up definition of semantic
relationships the user is free to choose
the name he really prefers you have a
question I did not get your point what
was your question yeah
no that is not an is that's exactly not
an is a relationship here you see the
texts are in the conventional Wikipedia
environment would be Pratt is a Croatian
Island and what you have seen on the
page before here you see is that for
whatever reason a link was defined
between Pratt's on the one hand and
Croatia on the other hand so and now we
can decide what kind of link do we want
to define between proud and Croatia and
here we decided because that link
belongs to we could also have chosen
other kinds of links but that is the
decision of the user what kind of link
he wants to introduce in order to relate
these two elements there are a different
kind of textual representation that
might end up in the same kind of
semantic link or that might end up in
different kind of seaman dealings so
that's the decision of the user who is
typing in their text and making that
decision what kind of type yeah the user
is inputting that kind of text in the
usual way with these slightly modified
syntax but then you are free to say well
I call that a belongs to link okay
well my examples are you have that
specific page describing Brad yeah that
defines the context you are in and then
you define the links you think are
relevant to describe that entity on a
more precise level okay and the second
extension is that you are also able to
precisely define the kind of attributes
that are used to further characterize
your entities so here you have that
attribute that describes how many how
much square kilometers are associated
with that a specific Island when you
think of that kind of stuff on in an RDF
context then you see that in the end you
generate two tribbles out of these two
specifications so one triple is that
belongs to relationship between prod and
Risa and the other triply is that a Aria
attribute that relates Braj to that
specific number of square kilometers in
theatres so that is the underlying
semantic representation that is used
what we exploit in that context is a
rather one easy straightforward
one-to-one relationship between elements
you find in semantic MediaWiki on the
one hand that is what you see here on
the right hand side we have articles you
have attributes you have links and on
the left-hand side you see two which
modeling element in the owl ontology
language these wiki elements are really
mapped so for example an article is
mapped to an individual so that would be
that that correspondence or anything of
that type link we have referred to that
example belongs to that is then mapped
to an object property instance or when
you think of that attribute value that
is mapped to a datatype property so we
have a rather straightforward one-to-one
mapping between these elements you find
in the wiki environment to the formal
representation that is based on some
subset of that of that our language and
that is not a surprise because the kind
of extensions we have defined had been
tailored in a way that we can really
have that rather straightforward mapping
to the to the
okay so what is now the benefit of
adding that kind of semantic description
you see at the top the straight forward
a page as you find it in Wikipedia 2d
and what comes on top now is what we
call the fake box at the lower part so
here you see all these specific facts
that had been provided by by the user so
here for example arch now has that
belongs to relationship to Croatia so
that is one of these faiths
then other could be that proud lives on
tourism and lives on on fishing so now
you see here in that box at the lower
part of the page what are all the facts
that have been specified by the user for
describing what he thinks is interesting
to know about protein in that context we
also have some additional features in
the background so that you have all
these conversions available from square
meters to square miles owned and that
kind of stuff what might be more
importantly is that you can also to some
kind of semantic browsing so you always
see here these icons that you can click
on and that provides you with the
ability to browse around in the content
of your wiki system based on the kind of
relationship you are interested in so
here we would say well we now know that
prowl moves on tourism and there are
other entities that ought to live on
tourism so that we know well maybe there
are other towns or other islands around
and when you would click on that icon
you would see something like that as the
next page so you see again that prachi
is one of these elements that live on
tourism but as you see also other cities
like Rio or Dubai that are specified to
live on tourism so that provides with a
very flexible browsing environment based
on these on these type things so that is
one immediate benefit you get by typing
in these links that you can then browse
around in that collection of X in the
end a second important aspect is that by
exploiting these extensions
we are also able to generate some of
these summary tables in a consistent and
very straightforward way you are all
aware that you find these summary pages
that in the end are manually maintained
so what are the cities in California or
what are the cities in Germany so
somebody has to input that and when you
change one of these basic facts you
really have to think about how to modify
your aggregation table and that is
something one really would like to get
rid of because that is a manual task a
lot of effort and consistency is always
an issue so what we provide in order to
get rid of that situation is what we
call inline queries so you can really
put in these queries into your text that
you enter on your on your wiki page so
they are embedded into these asked tags
and then you can specify your query by
exploiting a very restricted career
language here you see that you can
specify a conjunction of atomic
conditions so here we specify that we
are interested in countries that should
be located in Africa and have some range
of population around so we have that
type of contract to query here available
and then you have a in addition the
opportunity to define our to define what
kind of descriptive elements we want to
have in the answer set so how do you
describe the countries you are
interested in by their population of
other borders so when you then would
activate that kind of query the system
would automatically generate these kind
of tables so that you see well Central
African Republic has around 4 million
people there and there are the other
countries that have a border shared with
them and by having that table generated
you are always sure that if one of the
underlying facts has been modified that
table will reflect that modification so
you have consistency guaranteed and you
have trust to specify one set query and
not to maintain the table manually so
that's again a very immediate benefit
you
from that kind of a pause so from our
point of view that is one way of really
taking an environment where a lot of
people are familiars enhancing it with
small semantic ingredients but having
then an immediate payoff by getting
these kind of additional benefits more
or less for free in the end and I think
it's very important that you really show
that people get immediate benefit so
otherwise they are not motivated to
first invest three months of specifying
all that semantic stuff and then have
the first and it value a based on that
but when you specify these links on the
first page you are entering you
immediately get the benefit from that
and that is one experience we have made
that that is a very good way of really
attracting people to really use these
kind of kind of features in the end of
course there are still a lot of open
issues and some of those topics are
current web that we are currently
addressing so one question is when you
leave that freedom in that bottom-up
process that you can choose the naming
of the links on an individual basis the
question is is there some support
available to generate some kind of
agreement so that a lot of people would
use that belongs to type of link instead
of naming it in a different way and
clearly you can then exploit some
techniques that are known from
information extraction and ontology
learning in the background to handle
synonym problems and hominem problems
and that kind of stuff so that is one
way of providing additional support for
that for that process second aspect is
what I've shown here is the situation
that you enter new page and then you are
use these semantic extensions to provide
that semantic information but they are
already a lot of pages around nobody
would like to do the manual process of
enhancing these already available pages
with that semantic stuff so one idea is
how could you improve or how could you
support a process that at least some of
these links are generated in a
semi-automatic way
I will address that aspect in a minute
what kind of approach we have it
available to generate some of these
links based on the available information
then third aspect is user interface
aspect how you can make the user
interface still more comfortable and one
aspect is for example auto completion
aspects also using some guy kind of
templates that use such as fill-in in
order to provide these kind of links and
then an ongoing discussion is what kind
of additional expressiveness one might
add to what we have already available so
what you have currently seen is that we
have these binary links so we can only
relate to entities to each other
but sometimes it would be nice to have
also in a railings available in order to
cope with other aspects so that is one
issue that is currently investigated
also it might be interesting to add some
more semantic description to these links
so that you could say well one
relationship is symmetric it's a
symmetric relationship but we are
currently investigating that very
carefully because the experience is as
soon as your semantic model becomes too
complex people are not comfortable
anymore to handle it in the end and
therefore we really think about what
kind of additional elements we want to
add to what we have currently available
so that is ongoing discussion what might
be a useful idea in that context okay so
let's address one of these aspect how
you could improve the generation of
these semantic links in one aspect is
how could you exploit some learning
approaches in that context in order to
generate some of these type things in
the end what is specific in that
situation when you want to learn
something from a Wikipedia environment
is that you have no redundant
information in the end because you have
one page describing one specific entity
and there you find all relevant
information that's completely different
to going to the web where you find these
millions of pages are relating to the
same kind of effect so you need some
approach
being able to handle data in that
specific situation what we have done as
a solution is that on the one hand we
rely on a pattern based approach you
will see in a minute how that works in
the end so that is one design decision
we have made and the second decision is
in order to cope with that
haffley redundant environment we do that
in an iterative way so we start with
some bootstrapping and then improve the
kind of things that system is generating
in an iterative way and in that way we
are able to cope somehow miss that kind
of challenge in the end ok so what is
the basic idea you see here I stand a
page from Wikipedia about that nice fish
blue troll in that context what we
explored in that context is that we just
want to learn new relationships that are
related to the entity that is described
on the page so blue chill is the anchor
and then we want to learn what kind of
links can we extract from that page that
this is driving blue cell for example
and we can specify on our system in what
kind of links are we currently
interested so you here see for exam the
exam that we are somehow related in the
relationship where that fishery is
really living in so a relationship
between the fish and the area where you
find that fish so that is the kind of
specification you give for that learning
process what are the kind of
relationships I'm I'm interested in and
then what we exploit in our iterative
approach is that we exploit a rather
simple notion of context and you see
what we use as the context here is are
some tokens in front of the link and
some tokens that are coming behind the
link so for example we're talking about
kripak we have some tokens behind Trebek
and some tokens in front of credit so
that is the kind of very simple context
notion that we are exploiting in that
learning process so how is that now done
in the end in that bootstrapping
in iterative approach
you start with some very few seed
elements so you see here that one seed
element would be blue gel and crab egg
and then you go to your Wikipedia pages
in order to see what kind of patterns
you might find on these Wikipedia pages
so as you saw you find these explicit
link elements and then the context by
some takes part in front of it and some
takes part behind it so that is that
simple notion of context we are using
that and from these contexts we generate
some generalized patterns that is done
in a way as it is known from inductive
logic programming that you generalize
the kind of descriptions in order to
come up with some more generic
descriptions clearly you need some
heuristics in order to guide that
generalization process so that that the
patterns that are generated in the end
are not becoming too generic so we have
some restrictions on the number of wild
cards that are allowed in these patterns
and also we have some restrictions on
the number of examples that are still
available to support that kind of
generic pattern so there are a
combination of some heuristics to our
guide that generalization process then
you use these patterns and then you go
again to your wiki pages and then you
find additional instances that meet
these generalized patterns and then you
have done the first iteration of your
bootstrapping process and then you have
additional instances and then you can
again go to the next iteration and see
what slightly modified contexts are then
generated based on these new instances
who have found so that is the kind of
cyclic approach we are using there we
are also currently investigating to
supplement that cycle based on Wikipedia
by going with these generalized patterns
also to the web to get some information
from the web then to filled out what you
find from the web because then you get a
lot of information so there is some
ranking involved
and by that you can feed in some
additional instances into that iterative
process and the results are rather
promising when you combine these two
kind of learning scenarios in the end
some the challenges we are addressing
here is that you have really these
situation that a lot that you do not
have a lot of instances available and
therefore we use that pattern based
approach in that iterative way also one
advantage of that approach is that you
get rid of a lot of that natural
language processing so we do not have to
analyze the complete page about that
feature in that example because we do
not analyze all these sentences we just
analyze that part of the sentence that
is in front of the link and that comes
behind the link so that reduces the
effort of that you have to invest into
that natural language analysis
considerably so that is also one
advantage of using that that pattern
based approach by filtering these
patterns and ranking them we came to
some extend guide the precision recall
ratio so when we come up with rather
generalized patterns then of course we
will have a rather high recall but low
precision so by putting in some
filtering function that ranks these
children are generated patterns we are
able to guide the system to some extent
with respect to these notions of
precision and recall what we have also
found in experiments is a very good
indicator of the quality of your pattern
is a very simple measure to trust look
at the number of instances you find that
supported pattern that is a good
indicator about the quality of that
pattern and what you can also exploit as
additional information is that you
exploit some background knowledge so
that you exploit for example some typing
information of the elements that show up
in the patterns and that again improves
your your result in the end so that is
one way of generating some of these
semantic links in a semi-automatic
in order to get rid some of get rid of
some of that may not stuff okay so that
was the the big heat environment stuff
let me now move to the second part of my
talk on the Tetra language interface
aspect well why is that interesting
missing in a lot of application domains
it might be really nice to have some
more flexible way of varying the content
of the systems we have available and one
way of doing that is provides some kind
of natural language interface for
addressing that clearly they are always
these well-known issues associated when
you come up with these natural language
interfaces one is how to cope with all
these ambiguity issues that pop up
immediately also the question is how can
you come up with a large coverage of the
kind of questions that you are able to
understand at the end so that the users
are not too restricted in the kind of
way they are allowed to specify the
natural language query clearly then you
need some robustness because you never
know what people are typing in when they
use these natural language queries and
also always the question is how much
effort do you need when you have that
infrastructure available to adapt that
to a specific application domain and
that is to really also one of the of the
bottlenecks that are around so wise
natural language interfaces may be to
some extent more easy task compared to
text understanding when you compare the
kind of challenges well on the one hand
typically these natural language
interfaces use rather short sentences
that do not have these very long
sentences that you have to analyze when
you analyze text in the end then we have
here our assumption that we do not
handle this cause phenomena that are
typically very valid when you do text
analysis clearly when you think of a
very nice interface that this cause
handling would be a nice feature but
that is currently not part of our system
and makes life much more much more easy
in that context so there are some
assumptions you might put into that
interface
development that makes life a little bit
more easy compared to natural language
processing so the just to show you what
our kind of queries we are able to
handle in the end so that is taken from
some geographic domain and test system
so you could ask related to some Yoruba
and geography which River flows through
more cities than the Rhine so that you
can compare different rivers that are
around I will not go into the detail to
try see that in the background you have
to generate rather complicated logical
expressions to really cope with the
semantics of these kind of of Glarus in
the end what we have available air as a
system is that system are named Oracle
and that comes with two kind of
functionalities one is the end-user
functionality so you pose your your
query that is then interpreted as you
have just seen it then there are these
logical representations generated and
then you submit that logical
representation of the query to your
knowledge base also the domain ontology
exploited in order to come up with the
answer and then that answer is shown to
the user so that is the kind of end user
cycle but you also need is that
development cycle and there you see here
the developer of the system because you
have to come up with all these Mexicans
who need in the background to do all
that kind of natural language processing
in order to understand the meaning of
the query in order to be able to map
that to that formal logical
representation in the background
whatever this has later on one specific
design in cretin of the Oracle system is
that we have the lexicon splitted up in
two parts one is a domain independent
lexicon that you have to trust develop
once and then you have these
domain-specific extensions that have to
be generated for each domain you want or
to address in the end and I will come
back to that issue later on ok so what
is the kind of basic approach we use for
building up the meanings or
these are natural language queries so
here the query is rich River flows
through cards where you see here that we
use a compositional approach so we
analyze syntactic elements like that
verb phrase here for example that
extension of that verb phrase relating
to some prepositional aspects so by
analyzing these individual syntactic
elements and composing them in a
bottom-up way but if we come up then
with the complete semantic
representation of that query so that is
that compositional approach that is one
of the well-known approaches in computer
linguistics to define the meaning of the
sentences in in our case the meaning of
these of these queries in the end okay
what is the specific aspect we address
when we talk about adaptation of the
system to specific domains so what I
have already mentioned is that we have
that separation between the domain
independent and the domain specific
lexicon the domain independent lexicon
is tailored to the kind of words you
find in your queries that have a concept
meeting so that these are the words that
are Maul as independent from the
specific domain you are currently
working in so you can define that kind
of lexicon once and then reuse it in
several kind of different domains in the
end what we also exploit here in there
in the background is that you can refer
to some foundational categories being
defined by ontologies in the background
and what we exploit here for example is
that foundational ontology dolce that is
developed by Aldo and his group in Rome
in order to come up with these basic
definitions of terms that are then
independent on a specific application
domain what is then to be done is when
you move to a specific domain like in
our case into that Geographic domain
with rivers and cities and countries and
borders you have to come up with that
domain specific part of the like
and that is typically the bottleneck to
some extent because you have to develop
that for each specific domain and the
question is how much training and how
much know how do you need in the end to
be able to define that lexicon so do you
need a PhD in computer linguistic to do
that and then you have a bottleneck
because not a lot of people around there
well are there other ways of generating
that kind that kind of lexicon and what
we have developed here is an approach
that people that do not really have that
expertise in computer linguistics are
still able to do that Indian so that you
have much more people available that are
able to support that that adaptation
step in the end so how is that done in
the end so in essence we rely on two
basic ingredients to support that
development process on the one hand we
have our ontology that defines the kind
of relations we are interested in so
here for example we have that flows
through relationship that relates River
to cities that is part of these
relations definitions we have specified
in our ontology so there are the
predefined links that are relevant for
our domain and then the second question
is how is that reflected on the
linguistic level in the kind of lexical
elements you have to define for your
lexicon so you find here that notion of
predicate argument structures that are
well-defined for verbs in that context
so you know that flow that comes with a
subject and then it comes with a
prepositional complement that defines
through which object that that element
is really is really flowing and then the
question is in order to generate the
lexicon how do you come up with that
mapping between these relations we have
defined in your knowledge base and how
to relate them to these sub
categorizations frames that are known
from linguistics and here you see the
kind of simple mapping that is then
provided you see that relationship flows
through between rivers and city and here
you see
that sub categorization frame that is
now specified by the user to capture the
meaning of that flow verb coming with a
subject and then you see here the
subject is the river and then we have
that complement that prepositional
complement and that is in that case
defined through object and that is
mapped to cities of you know that that
compliment is related to cities by
having that rather simple interface
available our experience is that really
uses that Trust got some basic knowledge
about these linguistic elements are
really able to come up with these
mappings and therefore generate that
that lexicon in the background
clearly you would assume that way in
case someone having five years of
experience and a PhD in that area will
come up with better structure for a
lexicon yes to some extent but the good
message is that the quality of that
lexicon that is generated by relying on
these people that have trust a very
limited training in computer linguistics
has still good quality so it's
comparable not on the same level but
it's a quality that you can really
exploit in the end the second thing we
are using in order to come up with broad
coverage with our grammar
is that we first specify some of these
mappings and then we ask users to post
queries and for each query that fails
because some linguistic construct was
not defined in the end we explored that
iterative approach that then we define
specifically tailored to that query that
missing element for our lexicon and in
that way we can iterative Li Pradhan the
scope of coverage we are able to do
because it's it experience shows it's
very difficult to imagine from the very
beginning what would be the kind of
syntactic concerns people will use to
pose these kind of queries and by then
seeing concrete examples you get a very
good guideline what kind of soon Technic
elements to further end into your
lexicon so that is an iterative approach
guided
by the kind of various uses opposing to
your system in the end and that shows
that that is a very good strategy to
come up with a good coverage of the type
of queries users want to pose to your
system in the end okay and so what we
have learned here is that we can provide
a translation of these natural language
queries into that logical representation
to really post them then as queries to
you to the knowledge base in the end
that we are able to provide an
adaptation mechanisms that is usable by
non experts that need a little bit of
training but not these many years of
training so that more people are really
able to support that adaptation process
and when you then compare in some
studies that we have done what are the
results when you have chosen the experts
compared to what are the results when
you have chosen these non experts are
you have rather comparable results in
the end and by having that iterative
approach we are also able to address
that coverage problems in a suitable way
so that is a rather promising from our
point of view ok let's come to the
conclusion but what we have address in
both scenarios both in that wiki
scenario and in the natural language
scenario is how to come up with some
kind of techniques that more people are
able to provide the kind of content you
need in the end to come up with your
solutions so that is in the wiki
environment to be related to all these
web 2.0 technologies how to bring in
communities to specify content in the
end and that is related when you talk
about these interfaces how to come up
with all these lexicon descriptions that
you need in the in the end so from our
point of view that semantic extension of
media wiki is a really nice way of
providing an environment where people
are really able to come up with a lot of
semantic descriptions and by exporting
these facts I have shown in that fake
boxer that are associated with these
Wikipedia pages you can feed them into
all kind of RDF repository
and therefore generate a lot of rdf
based effects in the background and also
you have seen by providing that a
relation extraction technique you are
able to generate some of these effects
in a semi-automatic way and with respect
to these natural language interfaces
really we need these semantic models in
the background but we have some ways of
making that adaptation process more easy
and also usable by by non-expert users
in the end so what are topics we are
currently investigating in order to
improve the kind of solutions we have
available currently clearly we are still
investigating how to further improve
that preach between the Semantic Web
world on the one hand and these
community-based approaches from the web
2.0 world and we see rather promising
steps by combining for example these
wiki environments as we have made now
available with these ontology
engineering environments to come up with
a very smooth transition from this very
lightweight approaches to these more
more heavyweight approaches we also sync
by adding some semantic descriptions we
could also provide better means of
integrating that a huge amount of
information that is around in all the
social spaces but it are still
disconnected to some extent and probably
providing some more semantics we think
there is a lot of opportunity to provide
better integration what we always try to
do is to come up with solutions that are
to some extent easy to use so that our
people not forced to have several years
of logic training computer linguistic
training to really do that kind of stuff
and also to exploit some learning
mechanisms in the background to get rid
of some of the of the manual stuff that
we are doing currently and clearly
scalability is always an issue both from
a point of view how to come up with a
lot of semantic effects Indian so how
can you really support that large-scale
knowledge acquisition and on the other
end really when you talk for example
about natural language interfaces
efficiency and scale
ability is always an issue because you
have to have means that you are really
able to come up with efficient solutions
in that context so we have some of the
solutions around but really there is
room for for improvement in all these
different kind of aspects okay thanks
for your attention clearly what I
presented is based on work of a lot of
people in my group and if you are
interested just go to the to the web
page years of our groups in Karlsruhe
and that you find all these kind of
relevant literature but it also might be
interesting for you go to the homepage
of our AFP Institute because there you
see a completely semantic portal that
has in the background all these semantic
models around so that you can export are
the effects about what checked people
whatever we are doing there so that is
one example of a semantically supported
portal and then you can pick a lot of X
plus by going to that portal okay thanks
again and I'm open for some kind of
questions yes please
when you were querying uh okay you had a
category and your query I'm wondering if
that category sorry I'm wondering if
that category is a specific is is
special in the system or if it's just a
tag that a user defines and also if if
that's enough or if you needed a
hierarchy or a list of labels or
something for are you see when you go
into that geographic domain we have an
ontology in the background available
that describes some basic concepts of
that geographic domain so that you know
some characteristics of countries what
kind of relationships they might have we
have seen that in that adaptation
process that we have some predefined
links in my exam that was river to city
so we have the notion of River available
we have the notions of cities available
and we know what kind of care services
they have available so there is some
semantic model about your domain
available in the background in order to
support these kind of analyses in the
end okay so each element has like it one
domain that it applies to like country
has a country has a country and then all
those relations are specific to that
domain well we know think of ontology
engineering YouTube you rely on some
reuse of already predefined ontology
so you might find some module about
these Geographic concepts that could be
reused in a lot of different kind of
applications in the end you know you do
not have to start from scratch every
time you move to an a new domain in the
end so that is the typical reuse you
have as part of your ontology annealing
process Thanks
listen and for the domain-specific
lexical building the structure how much
is the volume work it's like me so like
we have like for geography like I guess
historical figure and every kind of
structure like these kind of like like
like something you trying to ourselves
to somewhere to do or or like it's like
how much the volume of the are you see
clearly what I've described here was one
example system we have developed
addressing that Geographic scenario that
has not a worldwide coverage so that is
currently more or less related to
Germany and some Geographic elements
that are related there and then we did a
specific case study to see when you take
people that get just a very limited
amount of training in that linguistic
area and compare the results with what
my experience people in my team are are
delivering in the end and then we
compared the results are based on these
two kind of lexicons that had been
generated that was the result that was
rather promising that the quality of the
lexicon of this non expert was really
fine not that good as my PhD students
are doing that but enough quality to
really deploy the system that is the
kind of message but we do not have
available a system that would now cover
all of Europe or all of us that would
still be an effort to do work in thangka
yes please I had a question as you
looking at natural language queries and
things like that nature do do we think
as we provide more semantic information
behind content that that people will
start building an intuition of different
types more longtail more deeper queries
you that they will build the intuition
to build and actually write and then
those queries for example like the
borders like you know right now I don't
even as a user I don't even think of
even asking that question because I know
there's not an answer
but as we add more you see that if we
add more semantic data that we can ask a
question like how many you know how many
countries have odd numbers
borders and also have rivers running
from the north and south I mean like
these type of queries may not you know
we don't think of them but you know yeah
you see what I've said in the beginning
you can distinguish these short tail and
long tail queries so we are currently a
little bit biased to that short tail
scenarios where a lot of people are
using that we use these keywords and we
do that every day our 24 hours and so
we're a little bit biased to these to
these kind of signals but when you go to
more specific domains people are really
interested in these more complex
relationships how things are really
related to each other and then you need
that kind of expressiveness at the user
interface to really be able to address
these kind of issues from my point of
view that will pop up art readily
because people they are well able to
handle these shorter queries and the
scenarios but now only see well there
are other domains where we have
additional needs and they are currently
not well covered by these keeper based
approaches and therefore I think not
between three or six months but in our
midterm process I see that that kind of
issues will pop up and then you need
these kind of solutions in the end so it
seems what you've described is a is a
shallow domain-specific lexicon are
their core semantics lexicons that for
example know that a river has water and
lake has water that you build this on
top of do those exist or is that just
something else that is outside the
domain itself or you see we have that
separation between that domain-specific
and the domain independent one yeah only
one investment has to be put into a
first developing that domain independent
exit right for example what the other
sources are at a city it would know a
rivers and water source and lake Israel
yeah it depends on the application
domain so envision later on how much
effort you put first place into that
domain independent excellent because
that can be a huge effort or you can
limit that okay yeah and therefore you
have to have some idea what would be in
the second step the application domains
you want to address it
to see how much effort is worthwhile to
be put first place in the development of
that domain independent Lexy and this is
part of your cycle where they can view
queries and say hmm people are starting
to ask about water maybe we'll just link
water to the other entities or things
like that
you see currently we have separated that
process so that the main specific part
is part of that iteration because we
think that core part has to have some
really good quality so the question is
whether you are really willing to let
other people modify that you know so
currently that is a rather control
scenario weather experts are really
looking into that kind of thing but they
can get feedback from these iterations
then we see that these kind of queries
are not well handled and they see that
it's not due to deficiencies in the
domain-specific lexicon but in the
kernel and then they have to address
that thank you hi I'm a little bit
curious about I mean maybe I sort of
missed something huge but above the
relationship between the two portions of
your talk on the ontology construction
sort of with your MediaWiki stuff on one
hand of the natural language stuff on
the other have you considered or tried
out being able to generate some of the
you know underlying metadata or
underlying semantic structures and so
forth directly like using machine
learning or whatever from the
semantically enriched wiki and or
Wikipedia you know vanilla Wikipedia or
because it seems like there's a lot of
there's a lot of manual effort you know
even facilitated but that goes into them
to the ontology construction then a lot
more manual effort that goes into the
natural language construction I was
wondering if you can you know leverage
one for the other we see that high
potential are having synergy between
these two areas currently what I have
shown that the rocket system is not
really integrated with our semantic
media making me he stuff why I put these
two topics into one talk was that they
both address their challenge how to come
up with environments that are supporting
a lot of people in providing the kind of
semantics you need in the end to come up
with the solutions and that wiki in
mint would be one component in building
up these background and semantic
information that you then might exploit
for example in your natural language
environments in the end and what you
have seen in that pattern based approach
for relation learning is one concrete
solution we have can be available to
black in some learning mechanisms
already into that wiki environment but
there is room for a lot of improvement
and there are a lot of techniques around
in microbe and all around the world with
learning stuff and that is some
incremental process of plugging in more
of these learning stuff but clearly that
is very promising okay yeah that's
that's what I mean just a quick
technical question are you allowed to in
your relation form for annotating the
wikis it have belongs to Croatia for
instance can you insert an entire binary
relation in there sir or does it always
have to reference the the page topic
well you see what we have currently
focused on is that you start from your
page and then you define the link
between the entity that is just wrapped
on the page and the other things that
are then related to - that kind of stuff
but the mechanism in the end is not
biased to have that directed notion you
see because in the end you generate
these facts in the background and there
you might link other things to each
other not only proj to croatia in the
example but currently the kind of
environment because you are on that
specific page is somehow tailored to
that situation but the underlying
techniques are rather generic
and if you if you guys want to meet him
maybe after lunch or so on please let me
know I'll be happy to set it up okay
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>