<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoGraphic Technology Day 24: Quantization, image... | Coder Coacher - Coaching Coders</title><meta content="PhotoGraphic Technology Day 24: Quantization, image... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoGraphic Technology Day 24: Quantization, image...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/tLBDNN2o5lU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay welcome everybody to photo tech
number 24 today we have interesting kind
of two-part talk by Lance Williams whose
a Googler Diego rother who's a Google
intern from University of Minnesota we
can talk to us about quantization
decolonization texture super resolution
and interesting stuff like that and
we'll start with Lance hi thanks for
coming Diego rother has been doing some
interesting work on image texture this
summer and I thought that I would try to
frame that and provide some history some
background for it by describing some
very old work that I was involved in the
vision of image texture that Diego is
going to present has to do with kind of
interpolating pixels and there's another
part of this that's really very
important which has to do with the
structure of images groups of pixels
pixels are not uncorrelated because when
we sample them we usually insist on
there being band-limited so i'm going to
talk about some very low-level signal
processing considerations that have to
do with sampled images and some work
that was done at Apple quite a while ago
this this work was presented at an IM a
workshop in film restoration in February
2006 but it was the subject of an apple
patent that was filed in 1996 it's
fairly evident when signal processing
class develops the idea of sampling and
reconstruction but the idea that time
happens in discrete intervals is usually
combined with the assumption at the
outset that the samples themselves are
continuous that is to say that the the
samples of the signal have continuous
values and if that assumption is made
then the Nyquist reconstruction sync
reconstruction is obviously correct
usually the subject of quantization is
into
later and it's introduced as a
non-linearity that can be characterized
its effect on the signal can be
characterized but it can't be inverted I
think that if you consider quantization
in the context of sampling and and kind
of develop these ideas in concert you
can achieve a kind of different
understanding of quantization and I
think it's possible to get improved
results in reconstructing quantized
signals and that's true across the board
now I'm presenting this this is kind of
Forgotten lore this is not something
that it turns out that I discovered
other people that observed it earlier
but it's not widely known it's not
widely appreciated and I think it should
be much more so so notionally we have
the idea of a sampled and quantized
signal at the top you have a bunch of
direct Delta distributions that
integrate to the value of the amplitude
of the samples and the samples are of a
signal that might be a kind of a step or
square wave sort of thing but in the
second panel we show the quantization
bounds for each of those samples what we
know about the signal at an instant in
time is not some value that it's assumed
but rather a range of values that it
passes through so at the bottom there's
a hand-drawn signal that is a smoother
than the quantized step that just
represents another signal that
notionally might have passed through the
same quantization bounds and it's it's
quite a bit different than a set of
samples that are arbitrarily supposed to
exist at the midpoint of the
quantization range so it's because of
the way sampling theory is developed you
know as as a compromise on instantaneous
continuous samples that
interpolation is the mode by which
sample signals are reconstructed in the
ideal continuous case it's obviously
correct to interpolate the points but
these points that we've got in these
quantization ranges aren't really points
that the signal necessarily pass through
so in fact we're enforcing constraints
that represent things that we don't
actually know about the signal so i'm
not going to represent that aliasing can
be inverted it cannot but there's a dual
way of understanding some of the
phenomena that are associated with
aliasing and that is as quantization if
we parameterize the outline of this
raster letter R and trace around it as a
function of some independent parameter T
that takes us around the perimeter then
we can divide the contour into X of T
and Y of T and those outlines are
clearly quantized to the raster
boundaries and what I've Illustrated
here is a smooth reconstruction which
keeps the samples within their
quantization bounds but otherwise we're
asking for the smoothest signal and I
think you'll agree that the are on the
right is a more satisfactory are than
the one on the left it's also satisfying
that it reek want eise's exactly to that
outline so this is an idempotent process
so what are we really doing here well
let's consider the interpolation case
for a moment suppose we had a bunch of
samples and we wanted to interpolate
them well that means we want to pass a
curve through them they're obviously an
infinite number of possible curves that
can pass through a set of points so we
select one by typically selecting an
objective function that that curve must
satisfy the objective function that's
been used in the examples in these
slides is the objective of the natural
cubic spline
which is to minimize the integral of
second derivative squared it's not so
easy to frame sync interpolation in
these terms but a lot of us particularly
those in graphics are perfectly happy to
represent things in terms of low order
polynomial splines this is satisfactory
for a lot of purposes and we could
imagine a sink like interpolation of
this contour that's certainly not
difficult to envision but the point here
is that we can satisfy the objective of
minimizing the integral of second
derivative squared and observe only the
constraint that we keep the points
within the quantization bounds we're
doing exactly the same thing that we
would be doing if we were interpolating
the points we're satisfying that
objective but we're now we're just
satisfying a different set of
constraints we're keeping the points
inside their bounds instead of
arbitrarily forcing them to interpolate
a value at the midpoint of the
quantization range which is just in my
opinion mistaken so that's the 1d case
and there a lot of 1d signals of
interest but this also applies in the
bivariate case this is a image of a
picture of a sphere that's represented
in 16 gray levels and if we had this
picture and wanted to reconstruct it
with more grey levels let's say we had
eight bits available we could do that in
the same way that we reconstructed the
outline of the are we can ask for the
smoothest function that stays within the
quantization bounce now I don't know how
visible they are on the screen here but
there are some artifacts attached to
this reconstruction around the perimeter
of the sphere there's some kind of faint
sort of halos radiating from the outline
and those artifacts are actually caused
by the background of the sphere in its
unreconstructed form that vertical ramp
is a bunch of bars and where those
constant value bars hit the perimeter of
the sphere
there's a little kind of ringing sort of
artifact and what's happening is that in
trying to minimize the objective we've
got some points that rather than being
within the same quantization band have
many quantization levels apart but the
optimization still wants to pull this
one down and push this one up and that's
what causes these artifacts but having
observed that it's fairly clear what
we'd like to do to fix this which is
we'd like to take the outline of this
sphere and process it like the outline
of the are by creating a boundary
wherever contrast edges exist we can get
this kind of smooth boundary and we can
get this kind of smooth interpolation so
that's a geometric representation of the
sampled image and it is kind of the
limiting case of the sort of texture
sort of structure that we need to
interpolate texture in the ways that
Diego is going to describe we need some
kind of structure for the boundaries of
our different sort of structure of our
different pixel textures the textures
are correlated we have to be able to cut
those correlations with by determining
the sort of discontinuities that we've
redrawn the outline of the arwen and in
that case we have a representation for
image structure image boundaries and
contours and that we have an independent
representation of the texture between
those boundaries and that is like a
geometric recreation of the image and
it's got a lot of flexibility and power
I should mention that I discovered after
devising the scheme that aveda zakkour
had published it in reducing blocking
artifacts in transforming image coding
that was rather narrowly framed so
that's why it took me a while to find it
she used projection on the convex sets
to achieve her result
and some other correspondence to the
same I Tripoli transactions criticized
that method and recommended constrained
optimization instead we use constrained
optimization pete laguna wits wrote the
thin plate spline routines with which
these examples were realized and despite
the fact that there's some lore on this
topic i think that this is an
underappreciated approach to
reconstructing quantized signals thank
you
okay hey I will talk about my work that
I we will last during the summer
yes
ok
I can shout for a while necessary hey so
i will talk about okay so i will talk
about this work that we did during the
summer and use it as an excuse to talk
about some fundamental algorithms in
image processing in particular in
texture synthesis and texture transfer
and you may wonder why this is there
this is that is there because how many
of you have seen this movie before okay
great that is right because I I felt
that the best way to explain and my work
was to start from the end and go
backwards like like in this movie so
questions so a these architectures that
we generated I will tell you later how
just from this from these from those
samples we generated the movie shot i'm
going to show you hmm it's showing in my
screen sure
yep
so that is the original picture for
generating that was asphalt and it's the
first frame that you see in the video
and using only that frame this frame we
can generate all this movie by keeping
the statistics of each frame constant a
different example is this one this image
is was taken from google earth and it's
some desert in i think in colorado
so as you can see these textures look
real at every scale you don't get
blaring when when you zoom in
so one one more thing that we can do is
to elucidate detail I will show you
another video for that so in this case
you will probably recognize a place it
was taken from google earth also the
first frames and the detail was
elucidated so of course this party
synthetic it's it's doesn't come from
the satellite now it will zoom again in
the budding deGrasse this time you will
have to wait a little bit
so okay those are our results now let me
tell you how we need that
so this is a rough outline of what we do
first we including the the top part of
the pyramids those frames taken from
google earth and then for the last frame
the last real image that we got we
associated a segmentation that divided
that into different classes that that
much tells us that this is grass this is
path this is some kind of stone and for
each of those classes we have an
associated picture these are pictures
that contrary to those the those who
were not taken in place these were taken
somewhere else these two actually were
taken in place because it was easy but
this one was is grass taken somewhere
else okay and not only that we have to
tell the algorithm that these textures
look like that at this scale so that the
algorithm will propagate this part up
the detail it will down sample the
images to the correct scale that we want
and it will also push down their details
from this image to get the images that
that you saw in the picture in the video
will be that for each frame until we get
here where we have another keyframe we
call these keyframes and so these are
details corresponding to each of the
textures this you may recognize is that
a certain Colorado that we put to add it
into this texture and going into more
detail how we do this this jump so we
start with the picture and it's
segmentation at the upper level okay
thank you so we crop the central part
and we double the resolution of that to
get these two images then we have to
split the image using the segmentation
into each of the three classes that will
be processed separately and then after a
step that I will tell you later we have
to transfer the texture from the image
on the samples to the images where we
need them okay but before that we have
to match the color for these two to work
better for this step to work whether the
texture transfer we have to match the
color and match it again to to recover
the original color in there in the frame
after that we just put the parts
together to get the finished frame which
has detection now and we proceed to the
next frame of a of the video going into
a little more detail you can stop me at
any time if you have questions ok a in
that I cheated in that case because I
chose one that was like this it there is
no way the only information you have
here is this image and at that
resolution you cannot guess the
direction of the weeks so unless you
know something else you will have to
guess something to get the orientation
of bricks from the orientation the
boundaries
yeah but for example in this case if you
ask me how to put bricks in here where
we have two boundaries you have to get
something but anyway probably unless you
know the place any orientation will look
good in a case like that a so the first
step shown there in the arrow which was
to recover to double the resolution of
the labels we can have done what Lance
just explained but with some something
simpler that works in this particular
case if you don't do something like that
you will get a boundary like this which
won't look good when you zoom in we
wanted something like this and for that
what we was too smooth independently
each region this is a profile of a cut
around here so with smooth independently
each region and then assign two to each
pixel the label that has higher value
the highest value okay that's how we
constructed the labels then for the
texture transfer what we did was use a
generalization of the algorithm proposed
by a frozen lung and that algorithm
works by first contract constructing a
dictionary for that we scan all the
training samples the training texture
with a window and we apply the function
from that neighborhood to get some value
that will start here and we store also
the corresponding Center color
associated with and in this way we
construct the dictionary but by moving
that window all over the sample once we
have that dictionary we want to transfer
this texture to this image that was
computed by interpolating from the upper
level to add the detail to make it look
like this and so for that what we do is
we move this window all over the image
in this in this holder and at each pixel
we take a neighborhood around it we
apply the same faction that will give us
a value we look for the closest match in
the dictionary we get in that way the
color that we
put here in this pixel and we move to
the next pixel that's a whole algorithm
to transfer the texture okay this this
is this is really important this
algorithm is fundamentally evening much
processing so if you have questions
nearest a Euclidean distance yeah this
is a vector you get an elevator from
here just yes a in their algorithm I
think they use 9 or 11 we are using much
smaller we are using 66 white 6m so what
what the use was simply a concatenation
of the values in the castle part of a of
the window meaning the part that was
already touched by a by the algorithm so
they use this part here and they waited
it using a Gaussian a window and because
they use only the causal parts they are
not they cannot use the user input this
this would be the user input for the for
the textural transfer so you want this
part to guide the synthesis here but
since they only use the pixels that they
already over roll they cannot make use
of this user input so this is relatively
slow because searching in this
dictionary for when when this the
dimension of these vectors is high it's
very slow and this tends to blur the
details of the images your 20 texture
over the perfect acacia tree the next
part in the training texture would be
the best match for
if you just tell if you just cut square
on you and you would like that you mean
well what is the if you looking for the
best match of the best match will be the
next part in the training texture okay
you're ahead of me I will tell you that
so for for some pictures in particular
for natural textures that are the one we
are interested in Isaac means propose a
what you are saying right instead of
looking at this point in the dictionary
you take the you use the neighbors the
information that your neighbors are
providing so let's say this pixel was
feeling from this position and so this
pixel vote for this pixel here because
it's in the same relationship here and
here so these pixel votes for this these
two were filled by by these two so these
two vote for this and the same for that
one vote for that one so instead of
searching the whole dictionary you only
have to a search for the match in these
three candidates and that's much much
faster now you compare this neighborhood
with these three neighborhoods okay when
you do that i forgot to say something
the a chic means first initialize this
with random random location so he will
for each location here he will assign a
random location in the in the training
sample okay and then do this if you this
you end up with blocks like this right
instead of coping one pixel or an pixels
all over you will copy continuous blocks
okay the lines are only to show the
blocks right you will get blog sir be
right and that has the advantage has two
advantages first it maintains structure
for example flowers here they are not
cut into small pieces they are kept a
complete you don't get blaring because
of that a but also because this
boundaries are irregular are hard to
notice and this is very fast
and another advantage is since a chic
means uses non castle windows he is
using user input user input which we
need for that to work we need this part
to guide the texture that we are
creating and he showed that if you
repeat that past several times you get
better performance because these blocks
then tend to and large and small blocks
tend to disappear another improvement
that was suggested by hertzmann is to
use instead of looking for one patch or
the other using the two different method
was to use both of them at the same time
and keep the best one and they define
the best one as of course if you just
measure the Euclidean distance to the
patch patches in the dictionary a frozen
loom will be better than ash means
because these patches are include there
but they say if this is not to a to
worse too much worse than they patch
found by eros then you should use a
let's use that because you will maintain
bigger blocks the important part in my
in my view of this is that they observe
that a lot of image operations can be
framed as special cases of almost this
same framework like image denoising
texture synthesis and transfer
super-resolution almost in painting the
difference in all those operations is
how you define the function that maps
from the patch to the space where you do
the comparison between budgets so that's
important part how to define this
function this function must encode all
the information that you need to predict
the color of the central pixels so for
example one thing that is important to
predict the that color would be the
colors are of a pixel surrounding that
you want to
there is an edge crossing that pixel you
want to know the gradient in what
direction it goes but if there are
things that don't provide you with
information you shouldn't include them
because then it's harder to find me the
dictionary you are overfitting there are
other problems so the things that we
used to to define this function we first
use that we are very sensitive to the
two changes in the luminosity but we are
not as sensitive to changes in the color
in the chroma so we gave different
weights to those parts the the the
luminosities has a bigger weight and
also we have to define whether we're to
use a causal versus a non-causal windows
for example when we first a in the first
pass of of the algorithm when we are
searching here since this part don't how
doesn't have high frequencies if we
search for this patch like it is we we
were biasing ourselves to find patches
in the dictionary that don't have high
frequencies so instead of growing up the
texture we want we will grow some
blurred a picture and so for that we
have to use this part down sample we
want to find some patch in the
dictionary that when done sample looks
like this but for this part we want to
keep the continuity of the higher part
if there is an edge coming here we don't
want to down sample if we want to
continue the edge so we have to use as I
will tell you here we have to use a both
parts so our final algorithm has three
passes the first one we do a pass of F
rosenblum using only the mint color of
the patch on the luminance grey gradient
and that is very fast because the
dimension of of that of that function is
very low it's only five in the second
pass we use a chic means which is very
fast and for that we use
we down sample the color into and then
we use you cannot see but we use only
the castle part of the illuminance to
keep the high frequencies but only in
the luminance part because it's what you
can perceive and in the third pass we
use a without sample the color of the
patch of the whole patch but we only
include the yq components of the day I Q
component of the Y IQ and then include
the whole luminous a that impulses much
more coherence that a means doesn't
care about it because he doesn't had to
show all the frames he he only shows
that the last frame but we need more
coherence between the frames and that
produces better results than either a
chic mean or eras but runs in the same
time as a chic means so it's much faster
and the last step that I have to tell
you about this but I said it's important
to equalize the colors before searching
the dictionary otherwise for example in
this case if i'm looking for a patch
here and I look in this dictionary
instead of searching the whole
dictionary what it will find so on these
these parts are very much there are very
dark and so we are wasting almost a
whole dictionary so for that to work
well we have to equalize the colors
before doing the textures transfer and
undo the transformation after detection
transfer and the last thing is that if
we just apply that there will be very
small differences between consecutive
frames and when you show those frames
one after the other you will notice that
so what we do is now we go in this
direction and we take this frame and put
it in the center of a frame above ok but
some people may complain that that will
modify the data that you are giving but
the modification is so small that I
guess you couldn't notice anything in
the video i show you
we don't blur the edges of that square
or do anything else okay motivation a so
what do you do beyond the available
resolution in some cases if you have
some external source that can tell you
this is a this is grass or this is a
street or this is corn then you can
apply what I showed you another detail
according to the National Agricultural
statistical service services ten percent
of the whole US area is called by these
four crops just so those are regular
textures that we can predict thank you
yes a ashik means is particularly
adapted to natural texture so for
example regular textures like bricks in
the path i showed in the test yellow
bricks it's a worse case example for for
a means but you can substitute
ashik means for something else and apply
the rest of the algorithm and should
work yes I don't understand how you
could run faster having three steps to a
two different types of algorithms rather
than just one of anyone okay
26 a fine sure not because sorry okay
how can we run faster than ever sent
along if we if one of our stages is
precisely efforts and loom the thing is
if you want only to to do that you will
have to use our windows are much bigger
you will have to use this kind of window
size that we use here which is six by
six or more and if you do that in the
Air Force and loom you will have to
search every time in the dictionary and
that will be very slow if you use six by
six you will have 36 pixels each one has
three colors so it's 100-something
dimensional vector okay but what we are
doing is we in the first stage we only
use five dimensional vectors so the
search in the dictionary is very fast
and then we apply we apply this that
it's very fast as minces works fast
with bigger windows does it make sense
the second one is six by six anyway yeah
but this is fast we do us in the same
order as this we don't do faster than
this but this is already fast we do
faster that image analogies that I
mentioned before which has a a forest
inside so run slow
yep what all the grass look the same
with you hey you can have different
dictionaries for different grasses hey
so you can include them as different
classes if you can get a segmentation
that we tell you this is this kind of
class and this is this kind of grass
then you can make each one look like
like they should there's where you're
not looking straight down but you have
something coming at you and very close
very far far away concern like too
jagged are you there
sure I mean if you can if you can paint
different surfaces but not just with the
texture by telling it this is a material
that looks like this at this scale and
looks like this at this scale then maybe
you can put that into the graphic card
and make it look as it true that each
kale using using this that's that's of
course one application plans reply to
your question even if you use the same
canonical everywhere you've got
different colors of grass and different
shades of grass yeah in also so from
here you can see that even here you have
diversity that can be used to generate
different grasses if you want yeah yes
picture
I think the bottleneck is to to get a
good segmentation if you get a good
segmentation then you can do this yeah I
think it
yeah yeah I try this with a textures
that how different statistics are
different levels and it's like this
wheel uniform I see statistics and you
will get something that looks fractal it
looks good but it if you know what do
you expect for example in grass you know
that at some point you don't get more
grass you get a leaf or sales or
something so but it will look fractal
yes
now in this case we we simply did by
hand but we know that there are a sec
Sam segmentations provided by
agricultural services that say this is
gone this is wheat but this wasn't that
wasn't the focus of our talk well if you
can get sad one thing one possibility is
to use textural classification but your
segmentation will be as good as your
texture classification algorithm yeah
yes you can what you're doing there is
kind of immersing the segmentation in
the in the matching in the dictionary so
but everything depends on how good is
your desk limitation that you can get if
you can get a really good segmentation
from an external source then it's better
to use that now if you if you don't then
you can give that dust today to the
dictionary but you may get surprised yes
the lair
we let me go to that slide so here no
yes we since you know how picture
assigned to these layers the the last
layer you have a sinus this one not the
last but the first one that you find
when you will deeper so you probably the
texture from from here up I mean you for
this one
so the layers when I start here I know
that I have texture for this layer and
for this layer I don't use this one
because i know that this one should
integrate to this one so if it doesn't
then there is a problem there or the
animator the one who designing the
texture wanted it to look like this and
not like this integrated also if i
integrate this one let's say i integrate
this one to get to hear those are like
nine of times this will reduce 500 times
so it's word to use this one that it's
much bigger not the one immediately
below because i don't have picture here
yeah yeah i feel from this I feel all
the ones that I on how but down sampling
accordingly right I then just use that
picture i use that one here but i don't
sample about you here and so on
that picture of the author's instead of
the grass pitcher what sort of is it
just a fractal or is it try to reproduce
like a in a particular case with these
parameters yeah so in this particular
case here we use a very small function
there so it will depend a lot of that in
this particular case you will get a
fractal kind of texture from that if you
include more if you include bigger
blocks there you may get parts of faces
and go deeper with that okay thank you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>