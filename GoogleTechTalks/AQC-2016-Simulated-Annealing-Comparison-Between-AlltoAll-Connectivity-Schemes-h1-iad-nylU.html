<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC 2016 - Simulated Annealing Comparison Between All-to-All Connectivity Schemes | Coder Coacher - Coaching Coders</title><meta content="AQC 2016 - Simulated Annealing Comparison Between All-to-All Connectivity Schemes - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AQC 2016 - Simulated Annealing Comparison Between All-to-All Connectivity Schemes</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/h1-iad-nylU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everybody and thank you for
the organizers for giving me the
opportunity to give this talk so today
what I'll talk about is using simulated
quantum annealing to compare some of
these all-to-all connectivity schemes
that are now being proposed so I'd like
to first thank Wolfgang for actually
doing a great job of laying the
groundwork so my job will now be easier
and this work was done in collaboration
with Walter Vinci and Daniel xodar at
USC and all the details which I'll be
sort of skimming over you can find on
this archived reference so as we all
know right many interesting optimization
problems can be mapped to izing systems
but of course the price we have to pay
is usually that these Ising systems
involve all-to-all interactions or
long-range interactions all right so
here I just show the case of having 12
logical spins and they're all connected
to each other and of course the problem
with this is that if we if I'm going to
build a device that tries to implement
this graph and hardware is going to be
impossible because I can only really do
local connections and of course the
solution that we've all known for a long
time is that the trick is just to map
the logical problem to the physical
connectivity graph that I have available
to me so Vikki Troy many many years ago
taught us how to do this at least for
chimera right and at least that's the
case I will focus on right and what she
taught us was that if I have n logical
spins that are fully connected I can
minor embed that on an L by L chimera
graph where L is equal to 4 n where I
essentially what I do is I take the
logical spin and I just extended it into
a chain ok and this chain has a length
of the ceiling of n over 4 plus 1 and I
give you here at least a sketch of what
this looks like for this N equals 12
problem and so for example you can see
that this or maybe it's hard to see the
spin 7 is now there's 4 copies of it and
what's important is that I actually have
to couple
these spins for magnetically to
essentially ensure that they point along
the same direction right or if you like
I'm essentially trying to penalize
energetically this chain from breaking
okay so I might use this language in the
stock where I talk about the logical
spin being broken which just means that
my chain is no longer either fully up or
fully down so this is the scheme that
we've all been working on for or at
least using for a long time but now we
have the Leichner hog solar scheme which
was first introduced last year and
Wolfgang just described it to us so let
me very briefly describe it again so the
idea is that you take your logical
couplings ji J's and you map them to
local fields okay so now since you had n
times n minus 1 over 2 couplers you now
have n times n minus 1 over 2 physical
spins and essentially the value of your
logical coupling becomes the value of
the local field on that physical span
now in order to make this mapping work
what you need to do is introduce four
body interactions at least in the
original implemented and essentially
what this four body term will do is that
it will essentially energetically
favored physical states that can be
mapped to logical States right and what
like neuronal showed us is that you can
essentially use this to build this very
familiar triangular lattice which
essentially implements your logical your
original logical problem and what's of
course what's nice about this is that it
fits very nicely on a two-dimensional
layout now when an error occurs right so
here an error is some spin flip
essentially you will violate one of
these four body terms and essentially
you can measure that and so then you
would know that an error occurred
somewhere so what's our dilemma our
dilemma is that let's say you want to
build your quantum annealer
you want to pick which which
architecture to use you want to pick do
you want to pick the minor embedding
that way or do you want to do the lhz
way so what's good about both math
methods is that we know that if you make
the penalties or the constraints very
very strong
then they decouple if you like the the
consistent physical states from the
error States or the spurious states
right so in this way you can at least
you know have a good control and remove
at least or push away energetically the
the bad states in the Ising spectrum so
only in the classical spectrum the bad
is that of course you're the standard
driver right doesn't care or you know
distinguish between the consistent and
the spurious states so your quantum
spectrum actually is a mess right it
mixes these two states and this makes
life very difficult right and in
particular serve the main focus or the
main point I want to push in this talk
is that when an error occurs in the
middle of your evolution this error
evolves to something highly non-trivial
by the end of the evolution okay
and this is why or is a big reason why
fault tolerance for adiabatic quantum
computing or quantum annealing is very
very difficult this is why we haven't
been successful so far and I really like
always to cite this paper by Kevin Mohan
and Robin because I think they did a
great job of sort of just laying all the
various issues that we have with fault
tolerance for accuracy
okay so what I'd like to do is at least
I would like to sort of come up with a
model for the errors in quantum
annealing that will be relevant to us
right so we all know for example we can
have diabetic errors and this is just
simply because we're evolving too
quickly and of course it's easy to
overcome because you just need to evolve
more slowly or have a larger annealing
time TF but of course you can also have
errors because you're interacting with
some thermal baths and here of course
the longer your anneal is the more time
you're interacting with the bath so
there's a problem for today's talk I'm
going to restrict myself to at least
what I think to be the more
benign error model for quantum annealing
and that's if you like weak coupling to
a thermal bath and what does what does
recup link to a thermal bath mean it
means that essentially the decoherence
is happening in the instantaneous energy
eigenbasis so let me describe what that
means for us today so here for
simplicity I'm just going to consider
two energy levels so let's say this is
my ground state and this is the first
excited state and essentially
decoherence in the energy eigenbasis
will just mean that I'm going to have
thermal excitation in relation
relaxation between these energy
eigenstates so I have here the energy
the gap or the instantaneous gap is
going to be delta T and the key thing to
remember is that thermal excitation ZAR
suppressed by e to the minus beta where
beats as the inverse temperature times
the instantaneous gap so what does this
mean it means that early in the
evolution when the gap is large thermal
excitation ZAR suppressed as the gap
gets smaller you start getting more
thermal excitations when the gap gets to
its minimum you get lots and lots of
thermal excitation and then when the gap
opens up again maybe if you're lucky
you'll get some thermal relaxation but
rarely does the thermal relaxation sort
of bring bring you back to the ground
state
unless of course I mean you wait a
really long time here and again I
emphasize when this thermal excitation
happens here in the middle it can evolve
or that error can evolve to something
highly non-trivial at the end so I want
to use this error model to compare now
the minor embedding scheme to the LHC
scheme so let's just do sort of some
number counting so if I want to consider
a spin system of eight spins so logical
logical eight spins in the minor
embedding scheme this corresponds to
having twenty-four physical spins and in
the LHC scheme it corresponds to having
twenty eight spins and if I want to
consider sixteen logical spins I'm
already up to 80 physical spins for the
minor embedding and 120 for LHC so
trying to do a microscopic similar
raishin is definitely impossible even
doing a master equation approach is not
feasible but what we've learned at least
a lot at this conference is that quantum
Monte Carlo is a good sort of
substitution right and the way I like to
think at least of quantum Monte Carlo
when it comes to trying to use it as a
model of a good of a physical quantum
annealer is that you can think of the
quantum Monte Carlo updates as sort of
pushing you towards the instantaneous
gift State and that's kind of like your
decoy hearing and the instantaneous
energy eigenbasis right so our method of
choice for today is going to be
simulated quantum annealing which we've
heard a lot about in this conference so
let's think about simulated quantum
annealing and the kinds of errors you
can have you can imagine essentially
doing your simulated quantum annealing
simulation with not enough number of
sweeps and in that case essentially
you're not allowing your system to
follow if you like the instantaneous
Gibbs State so that's kind of like the
diabetic errors and it's also
essentially you're not giving your
system time to thermalize and then of
course you have thermal fluctuations and
these I'm going to think about as
thermal errors now what's important is
that in your simulated quantum annealing
again when these errors happen serve in
the middle they don't appear as random
single spinners by the end of your
evolution and if you're still not
convinced about using sqa as a model for
quantum annealing at least it's been
successfully used at least in capturing
many of the features of the d-wave
processors so that's just yet another
reason that maybe will be okay for doing
this comparison okay so let's think
about what errors look like for minor
embedding so here I have an ideal chain
so here my red will mean that the spins
are in their correct orientation if my
error model was just random errors I
would just sprinkle some
errors and there would be in principle
no pattern to them but of course when we
do quantum annealing with chains usually
what we find is that the errors appear
as domain walls okay so definitely this
does not look like that now in practice
what do we do when we're doing minor
embedding when we have broken chains
like this what we just do is we just
take a majority vote on the chain to
determine whether the the logical spin
is pointing up or down so now let's do
the same thing for the LHC scheme so
let's first consider what earth look
like when I have random errors so here
I've sprinkled four spin flips and what
you should notice is that when there's
an error it's accompanied by essentially
violating four constraints surrounding
that earth okay now of course if the
errors are very very sparse you can sort
of see these errors by and correct them
right but actually people have been
working on this and they've actually
developed very very good error
correction schemes when you have random
errors but when we run our sqa
simulations these are not the kind of
errors we see okay instead what we see
are very very few violated constraints
okay so essentially we see no violet
constraints except let's say one here
and one here and then you're led to
wonder okay which spins do I have to
flip to actually get those kind of
constraint violations so here I just
picked or showed one set of these so for
example I can get this this constraint
to be violated by essentially flipping
all of these spins and I can get this
constraint to be violated by flipping
all those spins but if you stare at this
long enough you'll realize that this is
actually not the unique spin or flip
error configuration that will give these
constraint violation pattern so here's
another example so this is the the one I
showed you just now and here's another
one that will actually give you the
exact same constraint violations
and so you're led to ask yourself well
which one is the more likely error right
then actually there's many many more so
it's not just these two so for our work
we're going to consider several decoding
strategies so one is the simplest thing
you could sort of think of which is
essentially the equivalent or what I
think is the equivalent of the majority
vote for the minor embedding so what
you're going to do is you're just going
to take a random spanning tree on your
on your logical yep
you're gonna yeah take a random spanning
tree on your physical graph use that to
map to a logical state and then
essentially do this many many times and
then do a majority vote on those logical
States another way more sophisticated
way would be to actually find all the
possible errors that correspond to the
constraint violations that you found and
pick the one that actually requires the
fewest number of spin flips okay
now actually you can think of this as
being sort of the poor man's version of
of essentially setting the initial
Murray temperature to zero artificially
ideally you would want to actually do
this decoding at the Nishimura
temperature of course then there's the
very nice wait three parity check method
of post ascii and fresco which i will
point you to that paper for more details
now what's important is that essentially
these methods only really work very well
for random errors okay so now there's
the question of and this is in some
sense the big question how do you
compare the minor embedding scheme and
the LHC scheme okay and how do you
compare them fairly okay now this I
think will be very controversial but
this is what we did okay
we assumed we use where we're gonna use
the exact same parameters for both
methods using simulated quantum
annealing what does that mean it means
we're going to use the same temperature
for both we're going to use the same
number of sweeps or both the same type
of spin updates and the same annealing
schedule okay and the only optimization
we're going to do is on the penalty
strength
right so we're gonna run both methods
with large number of penalties and pick
the penalty that maximizes the
performance of each method individually
okay but this is not the unique way to
do this comparison right for example as
Wolfgang pointed out there could be
benefits of optimizing the annealing
schedule for both of them separately and
of course you can imagine that if you
were going to build a quantum annealer
based on the minor embedding method or a
quantum annealer based on the lhz method
you might actually have different sort
of capabilities based on which
architecture you use and that's
something in principle you should use in
the comparison but for us today we're
going to stick to this so let me show
you some results so here is a single
instance right so this is an eight
logical spin problem
the ji J's are picked randomly from 0.1
0.2 all the way to 1 and what I show on
this plot is the probability of have
reaching the ground state for different
penalty strengths and there are
different curves here there are lhz
curves with no decoding minor embedding
curves no decoding and then LHC curves
with a majority vote so with this with
the random spanning trees and then when
our embedding with a majority votes on
the chains so let me point out a few
important parts so one is that at least
for this instance you see that majority
vote or the decoding helps both methods
only really for small penalties and for
high penalties both methods essentially
or the younowknow decoding and with
decoding give similar results but also
you'll notice that there's actually the
peak and the success probability is at
some intermediate penalty strengths so
actually having very large penalties is
not optimal and this is actually
something we've all sort of learned for
example on the d-wave processors now
notice that of course here the minor
embedding method seems to outperform the
lhz method but you might say well both
of them are not doing great you know the
maximum here is not even as your
five so what's going on so for example
you can that was for ten thousand sweeps
now you could do it at a hundred
thousand sweeps and what you see is that
essentially both of them their
performance improves but the minor
imbedding still seems to outperform the
LHC now you might say okay maybe this is
just an artifact or maybe this is just
you know this particular instance that
you you know cherry picked so what we
did was we generated 100 instances right
and ran all of them using again the same
sq8 parameters and this is the plot you
get when you plot the minor embedding
success probability on this axis the LHC
success probably or this axis and
essentially anything to the right of
this line means the minor embedding wins
and again unfortunately or fortunately
you see that the minor betting is
overwhelmingly favored by LHC so what I
showed you in the previous plot was not
a fluke but now you might say okay look
you know there are much more
sophisticated decoding strategies for
lhz so maybe you should use those
absolutely
so we now do things with a minimum
weight decoding which turns out to be
actually as good and usually better than
the the parity check method of pastels
key and press cool but of course I want
to emphasize that this minimum weight
decoding is actually not efficient
whereas their method is so now if you
use that and now what I'm going to do is
I'm going to color code the dots
according to the LHC penalty strength
the optimum energy penalty strength now
you see things are much more balanced it
looks like right and one thing you
should notice is that it notice like lhz
does very very well when its optimum
penalty strength is very small okay
let's go to larger system sizes and now
if I go to larger system sizes it looks
like things are no longer so balanced
anymore
more and more of the instances are in
favor of minor embedding although for
these sqa parameters they're still doing
or both of them are not doing so great
now
so you might wonder though well what
about all these dots you know there's
something interesting happening there
and it actually turns out that this is
actually artifact of these small problem
sizes it turns out that you know this
minimum weight decoding when the penalty
strength is zero it essentially helps
lhz find the ground state very very
quickly because essentially the errors
are so dominant in the in the LHC scheme
and the minor embed the month the MW d
MW d decoding essentially does its own
optimization and it turns out that the
ground state of the optimizes of the
optimization it is doing is the same
ground state as the ground state of the
logical problem and that's why it works
so this is actually an artefact that
actually very quickly disappears as the
system size grows so at least the
conclusion at this point is that it
seems that as the problem size grows LMZ
becomes less and less competitive so now
this question of the gap came up in the
earlier talk and it's of course very
very hard to determine this but there's
a very nice arc or there's a very nice I
don't know question that my colleague
etai Han brought up which I would like
to sort of use and it's the following
so consider in the LHC case the zero
problem so you essentially have the LHC
scheme but you've set the logical
problem to zero so all you have now is n
times n minus one over two spins coupled
with four body terms this is an instance
of for exercise okay so you might ask
yourself well what do we know about X or
set and quantum annealing and so if you
do the literature search it turns out
people know that 3 X or set exhibits
exponential an exponential closing of
the energy gap and I point you to these
three papers so while this doesn't prove
that the LHC scheme will always have an
exponentially small gap the question I
want to pose to you the audience is what
is the loophole because I don't know it
so with that let me just bring up the
conclusions of my talk so first if
you're going to make sort of statements
about the performance of various schemes
on quantum annealing it's important to
have in relative or use a relevant error
model so at least when you use sqa it
seems that the LHC performance is not as
favorable as minor embedding and in our
study which I teased I haven't shown you
in detail we at least attributed this
difference in performance is really due
to the fact that LHC has a harder time
to thermalize using sqa and an open
question I would say is that is this
because of the worst gaps or the worse
gap structure but now is this the end of
the story and I want to emphasize that
no it is not
ok lhz has a lot of very very nice
properties right then we shouldn't
forget those all right so one it lends
itself to very nice decoding strategies
right and you can imagine that if
there's some innovative young person in
the audience who can think of something
better then maybe you can imagine having
a dramatic improvement in the success
probability of LHC of course you know
both lhz and minor embedding can be
further optimized right and I think you
know really you have to optimize both
completely to maybe be as fair as
possible now there are other advantages
of the LHC scheme which I've not
highlighted in this work and Wolfgang
pointed this out which is that in order
to implement your logical problem in LHD
you only need really precise control of
the local fields ok and in many
implementations of quantum systems we
have much better control of local fields
than we do of couplings now why is this
good this is good because we know that
as our izing problems get bigger and
bigger this phenomenon of j chaos
becomes a bigger and bigger problem
and so precision is important all right
so the fact that you can possibly
control your local fields much better
than you can control your couplings
might be a very important thing
okay but there are also advantages to
minor embedding which I didn't highlight
in this work right so we all know that
you know when you get your your machine
from d-wave the holes in the graph okay
and you don't send it back just because
they're holes in the graph mainly
because we know for example that if
there's imperfections in the graph we
can snake or graph our our our our
chains around those holes right but what
if I had imperfections in the LHC scheme
right if I punched holes in their graph
I mean does the scheme still work and I
don't have I don't know but I wonder if
this rerouting idea from this simon
benjamin it all paper is the answer and
then let me end with one final thought
and this is my personal opinion and you
can ignore it if you wish my feeling is
that right if we're going to implement
all - all connectivity schemes we can't
just do it on the classical part alone
it has to be done in conjunction with
the driver right and I think ETA and
Federico my colleagues at ice I sort of
started along this line even though they
were I think thinking about maybe
different things in this work and each I
continue it with Marcello and I think
maybe we need to sort of really think
about doing both together now with that
thank you
so let's start with a few questions hi
thanks first for the very interesting
talk my experience different point that
you said last that the decoding strategy
can drastically improve like the success
probability after a life cease scheme
that is something that I observed by
just tweaking the minor problem and
belief propagation just a little bit and
it would just be an interesting question
to you for your intuition would you say
that the sub power like performance is
due to the LHC or due to the decoding
what would you say if I had to guess I
would say it's a place for the
simulations we did it's due to the LHC
and the reason is because what I see
really when I do these simulations on
for example you just you put a you know
you plot where the constraint violations
happen they're usually very few
constraints that are violated but what
this means is that essentially a large
number of spins have to be flipped to
just get those you know tiny number and
I think this is the problem is that
essentially the errors look like a large
order n number of spin flips and then I
don't think any decoding scheme at least
at that point will help much yeah yes I
mean great presentation and I have a
comment but it's it's really less to do
with the core body of your talk but
something you mentioned at the beginning
about the thermalization can you flip it
back to that slide sure let's see what's
the easiest way so this is errors in and
the is it this one or is it the next one
the one with the diagram yeah this one
yeah yeah so you had sort of described
that you have these potential thermal
excitations and they're very fast when
the gap is the minimum and then you know
the likelihood of you know then getting
thermally saved is very unlikely unless
you wait a long long time which is
actually like a very non-physical way of
thinking about this right because for
every transition probability you have
going up you have the same one going
down right at the caster's because your
population in the excited state
it's at zero right yeah the moment you
get high population in the excited state
or even let's say a 50/50 split then you
your rates are basically the same going
up or down okay so you're right okay so
when it's okay so when it's 50/50 yes
right
but now the pace so let's say what
happens here is that you ended up
pushing thermally a lot but of course
here when the minimum gap is small you
also get lots of diabetic errs right so
let's say let's just say for the sake of
the argument then now you actually even
have more population up here than down
here okay so you would think that at
this point as the gap is opening up
again
you should get lots and lots of thermal
relaxation right but the problem is is
that of course at this point what
typically happens is that your your
Sigma X term or your driver is very
small right the energy eigenstates are
now getting very very close to being
computational basis states and at least
if you have like a de phasing bath model
the essentially the the relaxation rate
has become suppressed because
essentially the bath you know if you
wedge because there's proportional to
essentially you know energy eigenstate
one the bath operator energy I can stay
to and if it's something like
computational basis Sigma Z
computational basis this is going to be
essentially very close to zero so this
is what I mean the relaxation here at
least if you use like you know the weak
coupling master equation will tell you
that the relaxation here is very very
small because of other terms and the
rates absolutely okay any more questions
I think we have time for three more
questions
yeah actually to follow up that question
so is there any way of changing your
simulated point annealing algorithm to
have sort of constant bit flip rates
with some temperature model or is that
not possible within the context of
quantum annealing I can't flip
relaxation well I mean so I mean quite
so let's say you know if you started in
a very high energy state in quantum
water Carling you ran it right it should
thermally relax if if the temperature is
low enough yeah so maybe I'm not
understanding your question
well he was talking about relaxation
after the gap yeah and the coupling to
the outside world should be the same so
even if the X term is small you should
still have potentially significant bit
flip relaxation so at least I think that
intuition is not true in the weak
coupling limit right so so okay so in
quantum Monte Carlo for example we know
that so at least in the path integral
approach for example spin updates become
very inefficient when your Hamiltonian
becomes essentially just the icing one
okay and in the past people have said
this is actually a good thing when you
want to use quantum what Carl to model
things like d-wave because we know
there's this freezing region right so
when the quantum Monte Carlo the spin
updates become less efficient this is
good because it's sort of capturing the
freezing phenomena on d-wave processors
right now in the master equation
approach the same freezing happens okay
for the de phasing bath because the
rates are proportional to essentially
the overlap of you know the the one
energy eigenstate the second energy
eigenstate and this the bath term right
and if it's a de phasing bath and your
energy eigenstates are essentially calm
or very close to being computational
basic basis states they were foggin all
so that's why that overlap is very small
and both the thermal excitation and
relaxation rates here are essentially
zero so in the weak coupling limit even
a master equation approach will tell you
that essentially the thermal dynamics
becomes sort of suppressed because of
that so as we known yesterday in the
panel discussion the Quan Anila may not
be passed for found the exact solution
so I don't know whether or not you have
compare the performance of the two mess
or for a approximate solution okay so so
we didn't look at whether either of them
finds or like just yeah what is the
lowest energy state it found we didn't
do that but what we did do is we
actually compared how close each one of
them was the thermal State okay and I
can tell you enough and you can ask well
how did you find the thermal State for
120
or larger spins what we did was we
essentially ran parallel tempering on
the classical problem for many many
sweeps and we said okay hopefully it's
thermalized and that's good enough and
we compared the distance of you know lhz
minor embedding
to the thermal state and we found that
minor embedding was actually closer to a
thermal state
so yeah but we didn't do for example a
sort of a residual energy comparison
although if I had yeah maybe I won't say
anything so let's thank you game time</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>