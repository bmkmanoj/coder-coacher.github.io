<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>What Do Those Images Have In Common? | Coder Coacher - Coaching Coders</title><meta content="What Do Those Images Have In Common? - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>What Do Those Images Have In Common?</b></h2><h5 class="post__date">2008-03-27</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/YoOULXBa2wg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">anisha todorova ch who is a here
visiting from UIUC he's a postdoc there
with our you wit with David Forsyth or
which is of course up on the screen so
I'll turn it over to as sin Disha be
about a 45 minute talk and then we'll
take questions at the end or would you
like questions in the middle yeah I
would like questions in the middle this
year so feel free to bring up questions
or in the talk of unity okay thanks
thanks andraia for organizing this for
me so this is a joint work with
Professor Andrew kucha and I'm their
paws dog the main topic of this talk
would be given an arbitrary set of
images to discover like recurring
patterns so these patterns in fact are
objects in 2d and in this particular
example we were able to extract these
two patterns as the most occuring things
so one could even you know just start
anticipating what the entire talk is
about is that things that recur in in
arbitrary set of images may have some
perceptual value to them we can even
relate some semantics to them so in this
case like canonical views of a horse or
any object that recurs in the object in
the image side so here's the outline of
the talk I will break I will talk about
like object recognition throughout the
talk and first I will give you the
review than some other contributions
that we we've made in the last three
years the talk will focus on
unsupervised the object recognition and
then in the end we will just cover the
supervised case just to show that the
entire method covers both ends are
supervised and unsupervised settings so
in this review I will briefly go over
the standard modules of an object
recognition system just to point out
what are the main contributions to this
work so standard recognition system
consists of some training images with
which we usually communicate the nature
of object or object at agree that we
want to learn then we represent images
using some suitable features have object
representation and then given a
previously unseen image the object is
either recognized or even segmented or
we even have a short sentence as an
explanation for instance the faces in
front of a board and so on and so I will
go in the following slides over each of
these modules and just briefly review
what what what is the state of art so
current state of the art says that
categories must be defined by a user so
the category must occur in every
training image and we have this kind of
broad division in to supervise and
weekly sepoys training sets we are in
weekly supervised once we eliminate the
manual segmentation of objects but still
we require labels for each image with
respect to the category the damage
contains and also in addition to that we
need to prepare a background so training
images our background category this is
an ill-defined problem because
background is defined with respect to
the categories that are present so it's
not defined with respect to intrinsic
properties but with properties that are
not in those that we want to learn as
for a feature extraction the following
module the approaches use key points
edges or regions and I will excuse me
and I will talk more
about the the features that we use and
features that we use our region's and
wine and zone so it'll be in the
following slides as for object
representation it is widely accepted
that a spatial information is useful and
to encode like spatial relationships
between some features of an object one
may use either a planar graph
representation or her code
representation there are both you know
advantages and disadvantages to each
approach so here I will just briefly
talk about like a hierarchical approach
where this is my PhD work where the idea
was to find a structure in absorbance in
the image and and these that structure
would be like clerical graph model by AC
entry we are hidden variables in fact we
present objects object parts that
generate the observables and whenever we
need to learn a graphical model there
are two things related that we want to
learn that is to find out what is the
model structure so how many hidden
variables we have at what level they are
how many levels we have in that
hierarchy and also PDF that characterize
each variable in the model learning
these two things usually is
computationally feasible so people have
approximate approaches iterative
approaches and that's the one that I
also proposed for my PhD in the end we
end up with some Lauren structure of the
model and PDFs of random variables and
in that we obtain the model of the
object as a forest of bc and directing
of cyclic grass or trees and so ok so
that'll be just an illustration of our
other of one Oracle model so the main
problem with current approaches is that
they usually have only limited goals so
either
they're just aimed at at doing image
classification or just you know some
kind of constraint set of set of goals
whereas object recognition has so many
other things on the list like explaining
the location of an object and in the
context of other objects that also we
want to delineate the boundaries of a
oven of a recognized object and so on
another problem with current methods is
that they usually use approximate
inference methods because the fancy
stuff that they proposed to represent
objects or two strike features and so on
turns out to be computationally
infeasible so for instance the model
that I presented just the slide ago it
has this problem so I had to use
variational inference and whenever you
use variational inference then in a way
you throw away all the nice theoretical
like initial start that this needs to be
a nice caracal model where things are
causal and then with variational
inference you just break all these
causality connections and n treat all
variables independent and so on the mcmc
also has a problem of slow convergence
as long so what people usually do in
terms of object representation to
resolve these inference problems they
come up with a fixed structure so in
constellation wall like from Caltech you
have that objects have six parts and why
six parts it's a magic number that they
come up with so maybe it's justifiable
four phases that we have six parts but
then for elephants or for forest or for
you know any any other type of object
how can I claim that there are only six
perceptually valid parts her hair color
approaches usually fixed the number of
levels for instance uncle joe says that
objects have three levels and why three
levels and he cannot explain and or
maybe a branching factor usually that
you have a binary splitting of so for
instance in
pyramid approaches pyramid is a regular
structure where the image is binary
split into smaller parts and so on so
this branching factor is also fixed as a
power to number and so on and finally we
have these problems that usually
approaches require large sets that need
to be labeled this involves a large
human labor and suddenly this background
category is an issue so all these
deficiencies of probably work I try to
address in my work and whether I
succeeded or not I hope you'll be happy
with the solution so so here we go with
with the unsupervised single category
recognition this is just simplification
of the entire approaches to illustrate
the main methodology used but the most
like all these techniques are in fact
used to handle multiple categories which
I will eventually come come to to that
point okay so what is the problem
statement given arbitrary images each
containing more than zero category
instances but some images may not
contain category instances at all
determine if a category is present in
that image set and if yes learn the
model of the category so it's just
single category that we are after and
then given a new image recognized in
segments all occurrences of dollar on
category so this would be the setup
given a completely arbitrary set of
images you know with some random things
try to see what is the pattern what is
the theme that recurs in that side and
in this particular example the patterns
are faces that that are most frequent
here so we would like to learn faces so
each image may contain more than you
know or 10 or equal to 0 or category
instances and so in the set we don't
have background images we don't have
labeled image
is and also what is important that the
entire site is very small so we want to
learn faces only on six training images
not like hundred and six but only six so
since there is no user to define what
the category is for it for us we need to
come up with some mathematical
definition so we reason that patterns
that recur in the set are in a very
important because objects are structured
and everything that does not repeat in
the image search should be clutter and
as such should not be learned so things
that recur and are similar in our given
arbitrary imageset we call categories
and one should make a distinction that
category for us in our approach doesn't
mean semantically well defined category
but it's just recurring pattern to the
pattern with the certain properties and
properties that we want to examine our
photometric geometric and topological
photometric would be like color contrast
geometric Aryan shape and an topological
would mean that if an eye is embedded
within a face that's important
information so containment is important
and also spatial layout that the hand is
usually below a head and so on so
spatially out our relationship solace
important to us so for instance using
this definition we might say that
elongated shapes is a category and it's
a ballad category for us so we have
elongated shapes in the fence and also
in legs then other things other patterns
that repeat in this arbitrary set are
also categories for us why not so this
is my manual simulation it's not the
result actual results so don't get
confused this is just to illustrate the
concept but so so we examine these
properties and as these patterns become
larger and larger they we believe
are gaining more and more semantic
meaning and eventually the largest
objects that have similar photometric
geometry and topological properties we
extract and it turns out that they
really mean something meaningful so
perceptually valid and I will show that
in in the following slides this
definition is recursive which means that
a more complex category can be expressed
in terms of its subcategories so if I
have a category horse then then a
subcategory of horse our legs over our
horse and then I have a cow as a
category and then the subcategory of a
cow is a leg and then legs over horses
and cows can be shared so we want to
capture all these relationships okay so
that's the setup in the so the training
set is arbitrary and then the following
module is feature extraction so we
believe that using regions is good and
it has its advantages over using key
points and edges regions facilitate
modeling of fundamental characteristics
of objects objects are cohesive and I
want to capture the deformation I cannot
capture it by just random points next I
want to capture the information that I
is contained within a face and you know
this containment cannot be encoded by
key points regions are higher
dimensional than these local features
and in a sense they include edges so if
somebody is like a great advocate of
using edges you might as well use
regions because regional boundaries give
you edges and also very important for
our goals is that once you discover a
right configuration of regions that are
features of your object you immediately
get recognition and segmentation and why
because region boundaries coincide with
object boundaries or object pond park
boundaries so you don't not only get
this segmentation of an object that you
also get
segmentation of its parts and the sub
parts and so so so that's the motivation
for using regions but the problem with
regions is that low-level segmentation
which gives us regions is usually in
stable so in this particular example we
have like two exactly the same you know
persons and and completely different
segmentations so so the the communities
usually that's my sons and my
interpretation kind of afraid to use
regions although all of us are aware of
of its advantages but just because how
to address this problem this is a huge
problem and an end but then we we just
reason that if we somehow managed to
address this problem then we will gain
all the advantages of using regions so
one should not go away from from from
regions just because there is this
problem it's the ability if you have a
top-down information that can guide your
algorithm your recognition algorithm and
address this instability then you will
get a very powerful feature
representation of images and your object
so what is the problem with this
instability that adjacent regions may
split and merge so in this particular
example we have a large region for a
like this side of a face and it got
split into two here regions and so on so
this splitting and merging is in fact
the how the instability in fact
represents itself to to the algorithm so
so our way to addresses is exactly to
explicitly account for splitting and
merging of regions
so here we have a slide that illustrates
our image representation so we use
low-level segmentation to give us
regions low level segmentations finds
homogeneous regions but then you may ask
how do you define homogeneity
homogeneity is defined as like the level
of variance of for instance intensity of
pixels within the region should be less
than a certain threshold we are to set a
threshold you know nobody knows so we
just use exhaustively all possible
thresholds from zero to 255 and so we
get a multiscale segmentation we are at
each level of contrast we get a
segmentation so all these regions that
we generate can be organized in a tree
structure where a larger region contains
small regions that are embedded within
it and and so on so this would be a
segmentation tree which we are each node
is a region and hierarchical links
represent the bedding of regions in it
so since each node is a region we can
associate a list of region properties
like contrast area and so on to each
node and these properties we define
relative to parent properties to achieve
scale and rotation in plane invariance
so for instance area is not 50 pixels
but it's a percentage in the parent area
so like can I so if a pig-faced goes
through changes in scale you don't want
to capture the the absolute area of an
eye you want to capture the ratio of
Anna and the face through scales in
addition to capturing hierarchical
relationships we capture both like
spatially out relationships since we use
regions intuitively it's not quite clear
hard to define neighbours for regions
and in this slide I illustrate a case we
are these two regions are close to each
other and yet they see each other only
through a gap and perceptually they
should not be declared as regions where
these two are even farther away but
since there is no narrow gap between
them they should be in a way regions
because they are close to each other so
region neighborhood relationships are
you know difficult to define and they
depend not only on distances between
regions but also on the spatial layout
of other regions in the neighborhood to
capture that configuration of regions in
a in a nice way rigorous like
mathematical way we use a voronoi
diagram that we generalize for regions
so voronoi diagram is yeah okay so blue
would mean a region and red would be
exactly that I was about to explain a
voronoi deck where annoyed polygons over
regions so black tiny lines depict
voronoi polygons for each pixel along
the boundary since regions are exposed
to each other via their boundaries it
makes sense than to compute generalized
voronoi diagram over boundary pixels so
not so I don't care about pixels inside
the region I just care about pixels on
the boundary i compute for every
possible point here voronoi polygons and
then find the union of all these small
polygons and there'll be generalized
Veronica Liga and then if two polygons
share a part of their boundary there
they reach their neighbors and then so i
can use voronoi diagram to define the
neighborhood relationships but at the
same time I can also measure the
strength of their relationship so we do
that in as symmetrically so the strength
viewed by this region would be the
length of this shared part / the
parameter of the entire voronoi polygon
and the same here but since the
parameter here is smaller so this region
would find this one as a like a more
important neighbor than this larger
region for this one which is intuitive
kind of justified like that if I have a
large region and there is a small region
in its neighborhood neighborhood it may
not mean it may not be as important
whereas I have a small region and a big
region next to it so it should be more
important to seen by this one so this is
our cpr 08 paper how to also like define
neighborhood relationships between
regions and then given a our
segmentation tree we establish
neighborhood relationships among sibling
regions and get a graph structure so
each node is a region and the structure
of the graph defines the relationships
so it's a rich representation that
allows us to do a lot of things so what
we want to do to discover repeating
patterns and these patterns in fact
appear as many similar sub graphs in our
image graphs so how to detect them to
discover them we do graph matching so
the idea is to find common sub graphs in
the image graphs in this particular case
like given two images with two faces we
manage to match them in this way and
matching also gives some random clutter
but this is ok we will be able to
eliminate that if this clutter doesn't
repeat in the images and if it is if it
repeats then it should be associated
with the category that we learn so if a
face always appears in front of a
blackboard than blackboard together with
the face should be a category for us
because we don't have any supervision to
make distinction between the blackboard
dinner interface ok so how to do
matching the main idea is to match
regions in these two graphs if their
intrinsic properties are similar and
intrinsic properties by that I mean
color shape area and so
of course all that relative to their
parents and again the same should call
for their sub regions and the same
should call for their neighbor regions
so it's optimization over intrinsic
properties and her echo and neighbor
relationships and the result the actual
common sub graph that we obtain should
be should preserve original topology of
the image graphs so matching is the
place where we address instability of
regions I certainly would not have time
to explain all the details in 45 minutes
they stalk but the the idea is that
because we have splitting and merging of
regions you may not expect that region
in one graph should match only to one
region another graph because this
corresponding region may may be split by
the segmentation algorithm so you want
to allow many too many matching so that
one node can match many nodes in the
other graph and vice versa so you want
to have one-to-one many-to-many one too
many all these combinations so to do
that we augment each graph by their
mergers and and and there is some nice
way how to do it it's not like
exponential that all possible
combinations we augment but I don't have
time to explain that and again since due
to changes in scale some parts in the
sub graph for instance here eyes on the
for instance third level and they're up
I is on the fourth level so I want to to
achieve matching so that I'm not fixing
that regions that the same level are
bound to match I want to allow
flexibility that different regions the
different levels can still match and to
do that I introduce these auxiliary
links that would guide the search
algorithm for good matches by
introducing transitive closure of the
entire tree
in the end I get this so if you notice
that this face here was split into three
parts and because of a merger we managed
to match this good face with this one so
at the root of this result this would be
an example of one too many matching the
node here one that represents the face
match these three notes here so we got a
nice matching result this would be like
the formulation of the matching
algorithm given two graphs find the x
direction which is a subset of the
Cartesian product between the sets of
two nodes of two node sides of two
graphs so that the initial topography is
preserved and also the we get the
maximum similarity measure between the
little grass where this is the the
function certainly heuristic alida find
because this low level of extracting
information we don't know what the face
is and whether there is a face in the
image so so the how to define this
function what is similar is it's only at
this level heuristic ok so I don't have
time for this this is a detail how the
the matching actually works and those
who are interested can just read the
edges TV paper that recently got
published on that complexity is and
squared which we would like to reduce
but any reduction would involve higher
level information so it's a problem this
would be an example of our result so
given two graphs and their hair color
neighbor relationships we match both
regions and we match their relationships
so this is the end result that we have
ok so finally once we have
the similarity measures computed for
each possible combination of nodes and
their sub graphs we need to to extract
the most useful information there are
various ways to do that and in each
paper that we can publish so far we have
a different way to do that so in this
particular example a slide I just showed
one possible case and that is to just
plot a histogram of all similarity
measures between all possible pairs of
sub graphs in all possible images and
then compute the frequency of this
particular measure and then extract the
mode that is most prominent if mode
exists it means that categories are
present which means so a similarity
measure is high so sub graphs are
similar and at the same time their
frequency is high so something similar
recurs in the image side so this is what
we are after and so most similar and
most frequent a bunch of sub graphs
should be our extracted category and
this is the result that we get finally
once we extracted instances patterns
that repeat we need to learn their model
so it's a question how to learn a model
of structured data you know usually the
what people address is to model like
vectors and things like that but how to
find a good model of a bunch of graphs
so our solution is to do graph Union
because sometimes the face or any object
in question can be occluded so i may
have detected only a part of that object
also due to instability of segmentation
some parts may be missing so the the
best idea is to just join all these
graphs into the union and and this is
just a symbolic representation of
learning a graph union we align all
these graphs like this and then project
them into a structure like this so graph
union
would be like the union of all these sub
graphs we are one peer and may have a
lot of children and one child may have a
lot of parents so this is not the street
tree structure but just a dag I hear a
void I avoided depicting the
neighborhood relationships because the
picture will be quite quite look
cluttered but so you just visualize so
imagine that we also learn a neighbor
relationships here so this would be our
structure of a model representing faces
in this case so to get back to to the
beginning of this talk I explained them
so for my PhD work i had this
hierarchical model where I would have to
learn simultaneously structure and PDF
of variables in the model so the work
with these graphs and and and everything
that I presented so far would amount to
that structure learning so what I did in
okay what I did in my postdoctoral work
is to avoid the computational difficulty
of extracting structure of a graphical
model use graph theory to do that and
then once i have the structure then I
have a very simple problem just to learn
PDFs so given a structure of my model I
just can you know just learn some you
know joint PDFs on that on that
structure each node represents a cluster
of all possible regions that I extracted
and the pdfs of of that cluster is easy
to compute and so given a previously
unseen image and the model we match them
and get these results so matching the
graph representation of the image with
the model that we have learned gives the
simultaneous recognition and
segmentation so I avoid all these post
processing steps that
usually are present in the state of art
that once you detect an object then you
kind of develop these probabilistic maps
and over the location of an object then
you need to threshold these
probabilistic maps and nobody guarantees
you that the thresholds that you learned
on training sets are the same as in
there in the test image so so this gives
us more valid object boundaries this is
an illustration of a model that we learn
for wise men horses so given an
arbitrary set of images are in which
courses appear and and some of the you
know horses are in different poses this
illustrates that we can handle
articulation some small changes in in
rotation and soon we scale and this
would be a result so these are test
images by matching the images with a
model we we get immediately segmentation
recognition and segmentation is good on
jagged board and other usual challenges
that are encountered in object
segmentation parts where we missed two
segments are low contrast obviously
regions where the initial low level
segment or didn't even give us the
boundary so this background is you know
kind of eating up the part of a horse so
that's why we couldn't even you know get
that edge of a horse so low contrast
regions that merge with the background
is still a problem for us but we are
trying to address this problem that's a
that's a problem related to the segment
or low level segment or that you use but
not inherent to the high level things
that I present okay so the same
methodology can be used for addressing
texture analysis so this will be very
brief I don't have time to go over the
details this is our ICCB 07
paper are based on on the idea that
extra represents a repeated the spatial
repetition of some textural elements
that we call texels so what people did
in the past is just to locate these
structural elements and represent them
as points dr. elements are usually very
simple these are like templates like
polka dots or some rectangular very
simple things and what approaches
require some constraint in terms of
placement of textile seed of the
irregular or near regular we address
more complex setting the taxes are real
objects with the some structure within
them subtext all structure texts may
include each other and so the the
problem statement here is to given an
image of a texture we want to identify
texels learn their model and given a new
image of the same texture try to segment
out the texels so so the contribution of
this paper is that we were the first to
segmental texels this problem is
difficult because if you can see we need
to learn a model from only partial views
of so we need to learn the model of
entire text from only partial views of
at exel well certainly some texts may
have been fully visible but some of them
are partially visible but in general we
can do that if there is no texel that is
entirely visible only we can learn that
only from partial views these are the
results using the the same techniques
that have shown and this is I would say
the most impressive one some of the B's
we miss just because of this low
contrast we still have the problem with
the low sig mentor so just given a this
arbitrary image we don't know what what
the image even represents we just know
that it's a texture with some repeated
elements we try to search for similar
sub graphs of the image graph and this
is the result that we get
finally my multiple category recognition
so the training set here is more complex
we have multiple occurrences of multiple
categories / image certainly you know
category instances may may experience
variations in scale viewpoint and
illumination we want to address
articulation self occlusion suddenly
clutter and again it's an arbitrary set
so in some of the images mean we may not
even have category instances zero
occurrences categories are not defined
by the user they are unlabeled and again
we have a very small set let's say 10
images so on 10 images I want to learn
horses if they repeat in the set or
cause if they repeat inside or in this
case since in every image you have like
a writer horse combination I i may have
extracted only a writer horse as a
separate category but I will explain how
then to handle that case so problem
statement given an arbitrary set we
discover all categories present learn
their models and then given a previously
unseen image we detect recognized
segments all all of that together in
without any post processing steps so
this is simultaneously done but I think
the most important contribution of this
work this is I ccv 07 also that we are
able to to give a semantic explanation
of recognition so standard you know
approaches they just label okay there is
a face or there is a course in damage
but we are able to summarize what we see
in the image we can give a short like
brief sentence that says in this image
we recognize the writer horse why
because there is a horse and a writer in
the image and why because there is a
tail and legs and legs are shared by
cows and and so on and there are some
unshare parts like horns and so so we
can give a by our representation that I
will
like briefly explained in the following
slides what is share what is not shared
what is detected at what level of
hierarchy is detected so we can give an
explanation in terms of so the
explanation about recognized categories
in terms of simpler subcategories okay
so so this work questions what is the
most efficient model of multiple
categories so if we want to handle
thousands of categories what will be the
best way to to have their representation
and there are various approaches I will
just give them so what we want to do is
to to capture the the fact that usually
objects are can be viewed as
configuration of parts so parts can be
shared so that's another thing that we
want to capture so horses here and cows
share legs and camels make sure legs
with cows and horses but there are
things that are not shared like hams and
like heads and so this representation is
efficient because usually object parts
experience small variations so they're
learning is more robust and also they
occur more frequently in the world so
legs occur more frequently than horses
by default so it's it it's guaranteed
that the model would be more accurate
when you learn horses because they're
more available of them so more data will
another thing are sharing of parts allow
sub linear complexity so if you want to
learn a camel you don't need to start
learning legs from the beginning you can
just add more and more more complex
categories and you can have already
learned parts and attach them to so this
model may not be learned like in a batch
approach it can be learned online as you
add more images you can just augment the
model and some unsure parts are
important for the follow-ups again the
model is you know completely arbitrary
structured we don't fix the number of
layers we don't fix the number of parts
and so on and we certainly have exact
learning of structure we don't use any
mcmc or approximate the inference so
okay so this is just an overview given a
like a set of images we represent those
images as graphs and then we discover
similar things in them cluster them and
then extend the neighborhood and
clerical relationships between regions
to the relationships between clusters so
this cluster would mean a node in this
grammar of categories and since legs are
embedded in this cluster so this legs
may be embedded here we then extend the
link and that's how we construct the
model again given a previously unseen
image by matching the image with the
model we get immediate recognition
segmentation and also explanation so
here here's just illustration of results
this is a training set of UIUC called
animals data set that I prepared it's
available on my website the the set has
six major categories like deer cows
horses and these get cheap and so these
categories are very similar and in a way
this data set complements standard
benchmarks because because it has like
images containing very similar
categories so it requires the system to
have high resolve a billet II between
the categories which is a difficult you
know thing to achieve it's very easy to
make a distinction between a face and a
piano but how would you recognize you
know like a house and a car with how can
you you know just discriminate between
the two categories so it's it's a
difficult problem these are our results
are some of the animals we miss why
because they're sub graph does not you
know a set of the the threshold of
recognition for
but if we tweak that threshold you know
put it up and down then we will be able
to even recognize these parts princess
dogs have never appeared in the in the
training set but we are still able to
recognize dog's legs so if my threshold
of like false alarm and recalling
whatever threads will be used for this
recognition if it were a little bit
lower the lower then we would just say
okay we recognize legs and then you know
like by by using that information you
know we could maybe infer that there is
another animal that we were not able to
learn from the training site think so so
these are some of the results again the
leg here is missed because of the low
contrast with the background and
certainly the model learns on shared
parts these are some just positive
results on this data set we actually
recall about you know in mid 70s okay so
this is a another like application where
the entire approach can be used this is
our new like ECC be submission just to
illustrate the full power of it given a
randomly extracted frames from a video
sequence we were able to summarize that
video sequence since the model is
generative we can just synthesize
project each you know node as a region
just and so none of these instances
occurred in the set we probably thickly
synthesize these objects that I'll learn
from the side if you can see dogs has
never been fully observed and yet we
were able to kind of learn an entire
object other as a dog so the question
here why do we learn four categories
it's our level of sensitivity to the
differences between categories if our
level of sensitivity were a little bit
lower we would have learned one you know
one node for
all possible poses of a person but in
this particular example we use the very
you know high sensitivity level so
that's why we learn four categories in
this particular image sent another
example that a bunch of you know images
that this is a video of man dancing we
can generate generated 2d sketch so if
you can see none of these poses appeared
in the in the original you know frames
but we probabilistically generate the
most probable poles of a category that
we learn again for instance these two
guys never appeared together in the in
none of the frames but we summarized it
we learned that these two should appear
and again due to regions and their
hierarchy we are able to produce summer
is a different length so if somebody is
interested only in the you know high
level you know just abstraction we can
generate generate this type of a summary
this would be like one possible summary
this would be the second summer and so
on but we can increase the level of
detail that we put in our summary but by
that we also increase the length of the
summary so we can you know change these
parameters ok so again so finally this
part of the talk would be to address a
supervised setting so so far I've been
talking about unsupervised extraction of
categories and their learning so what
actual results we achieve for supervised
setting just to compare the entire
method with the standard methods that
that are you know supervisors are weakly
supervised so this is our CPR 08 paper
which deals with the problem of
supervised dimage categorization so
these two categories of images are
examples from caltech 256 this category
is labeled a horse and this category is
labeled a wagon and as you can see it's
very difficult to classify so given an
image how can I say
this is you know an image that bill so
this category or this is an image that
belongs to this category and why because
there are a lot of things that are
shared between these categories so in
these in these particular set horses or
prominent but here also of course is a
prominent so I'm stuck then you know
wheels are here and also wheels are here
but what is not in this set of images is
those unshared parts that they learned
and this would be the wagon top since
this test image doesn't have a wagon top
it is correctly labeled as a horse
category so we learned so in this paper
we present an approach how to make use
of the model that learns shared and
unshared part and how to weight the
relevance of each part to the regulation
of of their parent category so this
would be results on a caltech 256 so we
so the the latest result is from the
sermons group so we increase their
performance published at I cecilio seven
by nine percent and this would be our
result for Caltech 101 so not only that
we improve like on 30 categories but
also like for every possible number of
training examples per category so so the
entire approach obviously is good for
unsupervised approaches but also for
supervised sidings summary so to to
speed up the entire summary like the
main message that that should be taken
from from this talk is that regions are
good regions allow us to have rich
representations of images and these
representations can you know encode
capture spatial relationships like
photometric geometry properties of
objects
but but when you use that kind of
representation one needs to overcome the
the associated problems with
segmentation instability but once your
come that you have all the power that
that you can get from regions and the
power is that you you may learn or
perceptual valid categories from
completely arbitrary sets you can you
know have nice summaries of videos you
can have variable length summaries and
at the same time if somebody wants a
supervised case which you know is valid
for instance in a surveillance video why
would you want a method to account that
there is a puzzle the possibility for
dolphins to occur that may be
unrealistic so in a more constraint
settings this approach can still work
because supervised of you know settings
is quite valid and real okay and again
we can certainly extract the texture
elements called texels i showed you only
the example of a frontal view texture
but the same approach can be applied to
different views of the texture that's
our future work oh thanks to Narendra
who Jake who is my advisor inside thank
you so any question and we'll pass the
microphone for for questions
I'm used to seeing two and a half D to
signify a representation where you have
images that have a clue ssion priority
but they also in general have scaled you
call this 2.1 d because the texels don't
scale 2.1 d was named by the mumford
like in the 80s and it is a case where
objects are thin plates that are and
there they are overlaying each other on
the surface so this gives us occlusion
but there are no 3d effects because they
are thin and by thin I mean they are
with a smaller so much smaller than the
distance from the camera so the 3d
effects can be neglected so I ignore
shadowing effects and like inter like
these 3d effects that come out from
occlusion of real objects so I'd never
heard of this formulation I'm kind of
curious how is Mumford's 2.1 d idea
distinct from matter ins dead leaves
models from the 70s that's exactly that
he called that so that leaves model is
2.1 d yeah
so I was just wondering could you apply
the same framework like say if I didn't
want to rely on segmentations but I
wanted to use local features whatever
could you do that so the the entire
approach I must say realize a lot on
topological information of features so
the the fact that something is embedded
within a larger region is very important
and that allows us to handle like small
training sets and for instance for four
phases we are able to learn the entire
carrier faces only on six images because
topological information is very
important I don't know how to capture
topologically you know that kind of
information with bitcoins again special
relationships are you know with points
are defined only with respect to their
distances we believe that regions give
us a better more perceptually valid
neighborhood relationships because
neighbors are not just neighbors because
they are close to each other but because
also the neighborhood allows it to be
neighbors could you elaborate a little
more on the graph matching approach that
you know seems to be the crux of the
mean why this works yeah so what is good
properties of the initial segmentations
that you start with how do you overcome
which constraints help you exactly
overcome the fact that the regions are
so unstable like during matching do you
have extra hierarchical you know
transitive properties do you have issues
I mean little more on yeah so there is a
larger you know like volume of
literature on on image matching we are
images are represented as grass so it
will be difficult for me to just to to
give a like a Crocs or short course on
that but the main idea is to so let me
go to that slide that I skipped a little
bit
so in in this particular example so
suppose we have an ideal case so just
ignore for the time being instability of
region so given two possible graphs we
want to match them so this is so how to
do that you need to find good pairs of
regions so what i do i form association
graph which is like all possible pairs
combination appears like Rose would be
notes from one graph and calls would
represent notes from another graph so
you form a matrix of possible peers and
then since this matching needs to
preserve original topology you allow
connections between those pairs that
preserve that topology so so in this
particular example v2 and v1 v2 and v1
are a valid match and also this is a
valid match because v3 is a child and v3
prime is a child so if a node matches
then their children should also match
and by that you allow possible you know
like like combinations of matches other
things you know this this combination is
not connected with code because it will
violate the original topology once you
have this association graph constructed
then there is a theorem that the
matching result is a max click on this
association graph and it's straight
forward Max click is a you know an
algorithm that requires n squared and
why because I start with the association
graph that has n squared notes
yeah yeah max click there is no exact
algorithm for max league but unlike like
an CMC and other approximation
algorithms these there are like good let
there is no approximately it converges
very fast to a solution that that is
global so it's so it's a solving a
complex convex problem so it's it's
guaranteed that it'll end up after a
certain number of iterations to global
source
yeah I have a question about like you
said topological information is really
important in your model but like for the
case of the animals they are the
background is almost always the same
they always stand in a grass field why
doesn't the background become part of
your model so I didn't show the entire
model you'll learn definitely other
nodes which represent some parts of the
background that do repeat so in images
where there is no grass you this
information helps you to say that there
are things that are unsure between a
grass cow combination and a cow and then
that's how you can extract that cow is
different from grass cow combination and
since we learn the frequency of
occurrence of of grass cow accumulation
since this combination has lower
frequency than its not extracted in that
histogram so that's how I eliminate so
just pure on frequency of co-occurrence
of of different categories I think
that's all that we have time for Kimmy
let's be thank the speaker again yeah
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>