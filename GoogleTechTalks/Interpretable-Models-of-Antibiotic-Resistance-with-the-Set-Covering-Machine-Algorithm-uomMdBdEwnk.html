<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Interpretable Models of Antibiotic Resistance with the Set Covering Machine Algorithm | Coder Coacher - Coaching Coders</title><meta content="Interpretable Models of Antibiotic Resistance with the Set Covering Machine Algorithm - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Interpretable Models of Antibiotic Resistance with the Set Covering Machine Algorithm</b></h2><h5 class="post__date">2017-02-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/uomMdBdEwnk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">extra coming this is Alexander he's come
down from Montreal on this nice snowy
weekend and Monday to present to us on
some work he also presented at nips this
year and machine learning for sparse
models of antibiotic resistance so here
we go right thank you thanks for the
invitation really excited to be here
thanks for showing you up despite the
bad weather so I'm a PhD student at
Laval University
I'm supervised by Francois Laviolette
and Norma Jean from the department of
computer science and also I'm Co
supervised by Jaco Bay which is in the
department of molecular medicine so I'm
a member of a lab that's called the owl
and also the new big data research
center at Laval University alright so
this talk is going to be about my PhD
project which is using machine learning
to predict antibiotic resistance so I'll
first introduce what is antibiotic
resistance and then why we would like to
predict it and I'll show how we do it
and I'll show real results on actual
real biological data so what is
antibiotic resistance it's just a state
in which bacteria become able to survive
in the presence of an antibiotic and
this usually occurs due to genome
modification so for example you could
have an antibiotic that inhibits the
protein synthesis in the bacteria and
protein are essential for a cell to
survive so by binding to the ribosome
this antibiotic prevents it from working
but if there happens to be some mutation
in the ribosome then the antibiotic
cannot bind anymore and the bacteria
become resistant to the antibiotic so
this is one way it could happen the
other way is horizontal gene transfer so
bacteria can actually exchange genetic
material like genes that make you more
fit to survive in the presence of
antibiotics so I have this little video
which is from the Harvard Medical School
and it really shows you the development
of antibiotic resistance
in real-time so this was taken over a
time lapse of eleven days and what you
see is that there's this giant petri
dish with increasing concentrations of
antibiotics so in the center there's a
thousand more times there's a thousand
times one antibiotic and then the second
rectangle and in the first one there's
just no antibiotic so you see that the
bacteria spreads in the region where
there's no antibiotic but then hits the
wall off of this antibiotic and at some
point a mutant shows up and then it's
able to survive in the presence of the
antibiotic and it will just spread and
you see that there are more mutants okay
so these will spread out into the region
with a little bit of antibiotic and then
they keep growing towards the the region
where there's a thousand times more
antibiotic and so this happened over
eleven days so you can see that if you
misuse antibiotics or if we use
antibiotics when they're not required
we're favoring the development of
resistance and populations of bacteria
will become resistant given the
sufficient antibiotic pressure so right
now you see the bacteria are just making
their way into the 1,000 times more
concentrated region okay so I thought
this video was quite fascinating when I
first saw it so it really stresses the
importance of using antibiotics properly
so there's this little cartoon which
shows it actually illustrates horizontal
gene transfer so there's one bacterium
which is part of the antibiotic
resistance and it tells the other one
put this in your genome and even if
initially it won't be able to harm you
okay so this is just saying that
bacteria can actually exchange genetic
material which makes them resistant to
an antibiotic so why would we like to
predict antibiotic resistance well it
would definitely be helpful to use
antibiotics properly if we had a way of
screening rapidly if an antibiotic will
be effective against some bacterial
infection okay so think of having some
patients showing up in the hospital and
you take some samples from biological
sample and using a machine learning
model you're able to predict in a few
minutes if some antibiotic will be
effective to treat this
okay so if you were able to do that you
wouldn't have to go through what is
called
empiric therapy where you give a very
general broad-spectrum antibiotic to a
patient and then you make a culture and
you wait a few days and at some point
you say oh it's this type of bacteria
I'll use this antibiotic okay so this is
really actually an application of
personalized medicine where we would
design treatment plans for patients and
at the same time if we had these machine
learning models in hand we could look at
how they're making their predictions and
try to extract some new biological
knowledge out of them so this is the
point of what we're doing and our
typical setting is a bacterial genome
wide Association Association study so we
have two sets of bacteria four different
antibiotics we have resistant ones and
sensitive ones and then we're feeding
these bacteria into a learning algorithm
and trying to obtain a model which is
just some function that we could query
with new bacteria and it will make a
prediction
so is it sensitive or resistant to the
antibiotic and the way we approach this
is as a typical supervised learning
problem so we have a some training set
which we call s and it contains examples
which are pairs of X's and Y's where X
is a genome so it's just any string of
DNA nucleotides and those four letters
right here and why it's just a label
saying is the example resistant or is it
sensitive to the antibiotic and it's a
zero or one it's just a binary label and
D is an unobserved data generating
distribution we don't actually observe
this distribution we just assume that
our examples come from this distribution
and we'll try to find a model that
models as properly as possible how this
distribution generates the outputs why
according to the input X so first we
need to define a suitable representation
for genomes machine learning algorithms
typically work on vectorial data so
we'll try to derive a vectorial
representation for genomes and we'll use
D as a dimension of that vector
representation so Phi is just a function
that map's a gene
to a vector and then once we have this
type of representation we'll try to find
a predictor which is a function that
takes as input any vectorial
representation of a genome and just
outputs zero or one okay and we want the
accuracy of this model to be good so we
want to minimize the probability of
making a prediction error for any
example that is drawn from the same
distribution so we're really trying to
model how the healthy outputs are
generated according to the input yes
so for bacterial genomes are about five
million base pairs and I'll show I'll
describe the feature representation that
we use later on but we're always going
to be dealing with extremely high
dimensional data yes in this case it's
whole genomes yeah whole bacterial
genomes so there are many types of
learning algorithms that could be used
to address this problem because it's
just a regular supervised learning
problem in our case we make the choice
to require interpretable models and
interpretability is hard to define so
I'll just give you different examples
and why I think some algorithms would be
more interesting in this case than
others so we have neural networks which
are a great type of algorithm however
you see that we have the input layer so
these are the input features what we
actually observe and that we have some
hidden parameters and the input features
yet mixed up they're connected to each
of these of these hidden units and then
those are all connected to another layer
and at some point we get a prediction so
it's quite hard to understand why the
neural net will actually hide how the
neural network is using the input
features to make the prediction we also
have support vector machines which look
at the data in the vector space and try
to find some hyperplane that separates
the two classes so you could draw some
kind of interpretation out of this by
looking at the coefficients of each
feature looking at which coefficients
are the highest but there still could be
some coefficients that are actually
cancelling features out so it's pretty
hard to understand the true importance
of each feature whereas when you have
models like decision trees
which can model less complex function
functions but you can really understand
why the model is making a prediction so
you see this decision tree the first
thing you do when you have a new example
you ask the first question which is r1
and then if it says true you branch to
the left and if that's false you branch
to the right and at some point you end
up in a leaf and you just predict the
label that this leaf says okay so this
type of model rule based model is pretty
interesting in our context because we're
then able to validate the models that we
obtain using literature or using lab
experiments and then it's easier to get
the actual community the biological
community to use a model if it
understands how its making predictions
and we could also detect some biases if
we're learning something that's
irrelevant all right but the models also
need to be sparse here's a decision tree
there's a lot of rules in there and it
you would get quite a headache to try to
understand how it's making predictions
okay so we're really going to try to
have rule based models that are as
sparse as possible and this is what
we'll call interpretable and in our case
since we'll be dealing with extremely
high dimensional data and very few
examples for example we'll have millions
of features but a few hundred examples
because genomes are getting cheaper to
sequence but they're still pretty
expensive you won't have ten millions of
genomes for a study okay so we'll be
dealing with this type of data and then
in this case it's quite dangerous to do
something that's called overfitting and
this is when you learn a function that
works really well on your training data
but when you look at some new testing
data it actually doesn't work at all
okay works very bad poorly in this case
you see there are there's a green wiggly
line that fits the data points very
tightly so we could say that this would
be overfitting because you see there is
some red dots in the blue dots and those
are likely to be outliers and it's
actually considering them as red
examples so it's fitting fixed fitting
the data very tightly
whereas this black line is smoother
probably it results from a higher level
of regular
that's what we call it in the machine
learning literature so you see that it's
likely to to work better on unseen data
because it's not so fit according to
these noisy points that we see here
alright so we'll be looking for
algorithms that have a high level of
regularization now the data
representation that we use for genomes
is a k-mer profile so kima is just a
sequence it's a small string of k
nucleotides so there are four to the
power k possible sequences in that case
and we use we found that sequences of
length 31 actually work really well on
our bacterial genomes so we're using we
actually have 4 to the power 31
sequences which is approximately equal
to 4 times 10 to the 18 ok so you can
see that that's a huge feature space and
would cost billions of dollars to have
as many genomes as that right so input
so you should be shocked by that in
practice we don't actually need to
consider k-mers that we don't observe in
our data because if a camera is not
present in a training set then there's
no there's no way it's gonna be a good
feature to separate your two classes so
what we do is we define the set K which
contains all the cameras that are
observed in our in our training set and
it's it's a lot smaller but it could
still be huge in our case it's going to
be of the order of tens of millions and
we then represent each genome by a
binary vector which is just it's just a
vector of zeros and ones and it has a 1
for for a k-mer if it's present in the
genome and a 0 otherwise it's a sliding
window of 1 so I'll show this in the
example actually so suppose that this is
a set of cameras that you found in your
training set and X is a small genome
then you see this vector representation
there's a 1 for the first camera because
it's the actual first sub-string
and then there's a one for the third
gamer because it's the actual it's the
substring that starts right here at the
second position
okay so we're really using a sliding
window on the genome so you can see how
we get a lot of features because
bacterial genomes are typically of the
order of five million base pairs all
right so this is the actual number of
genomes with respect to the number of K
mares in the data sets that I will be
showing results on later on so they're
colored by species actually actually but
that's not important here what's
interesting is that we'll have a maximum
of five thousand genomes and a maximum
of 40 mil about 45 million k-mers okay
so you see how there's a huge imbalance
between the number of features and a
number of examples and this is why this
particular situation is very challenging
now the algorithm that we used is called
the set covering machine it was proposed
by Muhammad Shah and John Shaw Taylor in
2002 so it takes as input your data set
which are the label the the features and
the labels but also a set of boolean
valued rules and these rules are a
function that takes as input the vector
representation of a genome and then say
true or false and in our case we'll be
considering a presence and absence rule
for each camera so we have two x number
of cameras rows and so the objective of
the set covering machine is to find the
shortest logical combination of these
rules that most accurately predicts the
labels when we're trying to model so the
logical combinations that we'll consider
our logical and and logical ORS so
conjunctions and disjunctions
so here's a little schematic of that you
see we have some candidate rules a data
set and then we feed this into the set
coloring machine algorithm and it
selects some rules that are the most
effective as for predicting the labels
and then combines them with a logical
operator which in this case is a logical
end and you take any new genome and feed
this fit it through this logical
combination of rules and it will output
true or false and this is the prediction
you'll get so formally we want if we
have a candidates with set of candidate
rule R and we're trying to minimize what
we call the empirical race
which is the probability of
misclassifying an example in your
training set while using the smallest
number of rules and unfortunately this
problem is np-hard it reduces to the
minimum set cover problem and this is
why the algorithm is called the set
covering machine so to solve to actually
train this algorithm we use an algorithm
which is really similar to fatales
algorithm for the minimum set cover
problem okay so the algorithm is as
follows
I'll explain it for the conjunction case
the logical or case can be derived by
the Morgan's law just take the negative
of the conjunction okay so it's a greedy
algorithm so we start with an empty
model and then we'll be extending it
greedily during the training so we start
with an empty conjunction then we
compute some utility score for each of
the rules and I'll show you what this
utility score is on the next slide but
then once we've computed that we select
the rule with the greatest utility add
it to the model we remove all the
examples for which this rule predicts
false and I'll give you the motivation
for that in just a second and then we
keep going we keep doing that over and
over until there are no more egative
exact negative examples to remove or we
have performed as situations and s here
is a hyper parameter so you would tune
it to its it's a regularization
parameter that allows you to do early
stopping okay to limit the complexity of
the model by doing a certain number of
iterations so the motivation for
removing all the examples that are
classified as negative by a rule is just
that we're learning a logical end so if
at some point there is one rule that's
as false then the entire conjunction
will be false okay we cannot recover if
we make a mistake we cannot recover it
by adding another rule to the model so
since we really want sparse models we
really don't want to keep considering
these examples for which we can't do we
can't fix a mistake okay so this is
really why the set of any machine is
such yield such sparse models as you'll
see later on and the utility score that
we use to actually select the best rule
at each iteration is the following so
each rule RI we compute a score UI which
is which is actually a combination of
the number of negative examples that are
correctly classified by the rule minus P
times the number of positive examples
that are misclassified by the rule okay
so you're just doing a trade-off between
those two terms and you have a hyper
parameter little P and this you you tune
based on your data for example by
cross-validation okay so you you can
increase for example the importance of
making prediction errors on the positive
examples by playing with this parameter
now what's really interesting about this
algorithm for our settings is that it's
a running time complexity is linear in
the number of examples and the number of
features so that's really desirable if
you're going to be training machine
learning algorithm from such high
dimensional data and we've been able to
come up with an out-of-court implemented
of that algorithm so this means that the
data is kept on disk and it's just
loaded in small blocks when we're
training the algorithm so you need to
try to present this implementation so
it's called cover for k-mer covers we
use a set covering machine it's
available on github there's a link at
the end if you're interested so there
are two key elements to having an
out-of-court implementation of this
algorithm the first of all the first one
is having a compressed data
representation okay so at first we were
storing our data in a matrix like that
so we have one line for each genome and
each line is basically the camer profile
of a genome so there's a one if the
camera is there in a zero otherwise and
storing this itself is quite inefficient
because we use a byte to store binary
values right so you're losing seven bits
so you're actually storing more useless
data that useful than useful data okay
so what we realize is that we can oh I
forgot to mention but the yellow lines
are the positive examples and then the
white lines are the negative examples
okay so resistant insensitive but then
if we want to have an space-efficient
representation of this data we just take
the columns and then we pack these bits
into
teachers okay so suppose we have three
bit integers okay we would take the the
first three lines of each column and
then combine them in an integer which
actually gives four in this case okay so
this small matrix is equivalent to that
one it's just you can't do random access
to the to its elements anymore
okay and we also store this matrix in
hdf5
which has built-in gzip compression so
there's an additional layer of
compression over that and then the
second thing that's very important is
we're able to learn from the compressed
data without actually uncompressing it
so what you can realize is that to
compute this p i term which is in the
utility score it's the number of
positive examples on which each rule
makes a mistake so this number is equal
to the number of positive examples minus
the number of ones in this yellow region
for each rule for each Kaymer
okay so you could get the number of
correctly classified positive examples
by counting the number of ones and
what's pretty interesting is that there
exists an atomic CPU instruction that's
called the pop count so in one cpu
instruction it gives you the number of
bits that are set to one in an integer
so we're actually computing this using
one CPU cycle for each Kaymer instead of
64 if we're packing these bits and in to
64-bit integers okay so this really made
the difference between taking 48 hours
to train the algorithm in a few minutes
okay so this is how we implemented it
and this is open source it's on github
if you're interested so we have very
high dimensional data and few examples
so machine learning theorists would be
wondering can you really expect good
generalization so good accuracy on
unseen data in this setting so a typical
type of analysis is to try to derive an
upper bound on the prediction error of
your algorithm for any unseen data okay
and this bound holds with some high
probability okay so here we have that
the risk which is the expected error
rate will be small or equal to some
Epsilon with probable
t1 minus Delta and if we use a typical
type of analysis which is an Occam's
razor bound that basically says that if
two solutions have similar accuracies
then the simplest one is it's probably
the good one okay now if we look if we
use this kind of bound in our setting we
so M is the number of training examples
R is the number of errors we make on the
training set and cardinality of H is
just the number of rules in our model
okay so you see that this term right
here is going to be huge so it's 2 times
4 to the power K choose number of rules
so basically this is going to dominate
your bound and it's gonna dominate even
if you make very few errors so this is
saying don't expect good generalization
there's no way you can have a
performance guarantee for that algorithm
so this seems like a bad result but
actually the set covering machine
algorithm has an interesting property
which is sample compression so that
means that every model you learn with
the set covering machine can be
represented as a small can be actually
represented by a small set of examples
and a small message so if you have a
conjunction which is a logical end of
rules you will need examples that
contain these cameras and for the
message you would need the position of
the camera in the genome and if it's a
presence or absence rule okay so the
message is very small so actually if we
look at the generalization performance
in this setting we see that we really
don't have this big community realtor
many more instead we have the sum of the
lengths this is a number of nucleotides
in all the genomes that are in the set
Zi which we call a compression set okay
so this is saying that if you're able to
find a conjunction or a disjunction that
is using very few rules but makes very
few mistakes on the training set then
you could still have a good you can
still be pretty confident that it's
going to work well on unseen data okay
so then we could be optimistic of the
results that we get on real biological
data okay so this is also interesting
because as you can see the bound doesn't
depend
keh anymore which is the size of the
gamers and increasing K increases the
size of the feature space exponentially
so you see that we can learn from
exponentially more complex feature
spaces without actually with actually
having good generalization performance
guarantees right so you know for the
theory I'll show you real results now we
applied this method to a big database
that's called the Patric database it's
from the University of Chicago and they
basically collect a lot of bacterial
genomes with some metadata and they
aggregate it all into one place so we
looked at this database and considered
that a data set was a combination of
species and antibiotic and then for each
of those combinations that had at least
50 resistant and 50 sensitive isolates
we downloaded them so we downloaded a
total of approximately 12,000 genomes we
had in total 36 data set for five
different species and these are the data
sets you see here so the y-axis is in
tens of millions okay and the x-axis is
it's just a count of genomes in each
data set so you see that we really have
some data sets where we have a small
number of genomes and like 45 million
k-mers so this imbalance is super high
now how do our models behave in terms of
accuracy on unseen data so this is
really leaving out a set of data and
then training the model on some part of
the data and then predicting on that
left out set of data we compared here we
compare our SC emitted set going machine
to a baseline method okay this is a
dummy predictor that just predicts the
most abundant class in a training set
okay so if they're more resistant then
sensitive will just say resistant for
any future example so basically we
really want to be above that diagonal or
else we're not learning something
relevant here we see that most of the
time we are so this is a good result
sometimes we're not and what's also
interesting is that sometimes the
accuracy of the baseline predictor is
extremely high which
really means that we have a strong class
imbalance in our data sets sometimes
they are much more resistant than
sensitive isolates up to at least 10%
sometimes okay so we're happy that we're
above this diagonal line and for
actually 26 out of those 36 data sets we
have more than 90% accuracy so this
means that 90% of the time we are
classifying an example correctly now
another interesting result is that the
accuracy when you compare the accuracy
and a number of cameras you see that
they're not really related so the number
of cameras gives you the size of the
feature space so it's pretty interesting
to see that we can have extremely high
accuracies about 95% with features big
with feature spaces of 45 million
cameras yes some of them are redundant
but they're pretty long so we don't
always have perfect matches but there is
some big part of redundancy and there
are some features that are actually
equivalent like cameras that overlap the
same gene will necessarily be present or
absent at the same time it would depend
on the species for example tuberculosis
are very similar you could say maybe 90%
of the genome would be similar but but
there's different kinds of mismatches
between each examples so you can't say
this big block of chimera is there all
the time so which was discarded so about
above 90 percent similarity I'd say
there are and we dropped them out of our
analysis we don't consider them same
thing for cameras that only occur in one
genome we just discard them good so this
really points out that the set covering
machine can avoid overfitting when we
have very few examples with respect to
the number of
features and here I'm showing the
running time with respect to the number
of genomes we expected to be linear
which is what we see and the point of
showing this is to show you that it
takes between 8 seconds to 18 minutes to
train the algorithm on my laptop on
these big data sets each point in this
in this
ok right
usually we you could use
cross-validation to choose these
parameters you mean choosing the value
of K according to the testing set for
example well we're using a procedure for
example cross-validation to select them
so we select them based on the
performance average performance on the
validation sets and then apply them on
the testing sets so I since the examples
are from the same distribution if your
if your performance is similar on the
validation set and on the testing set
then I don't think you would be
overfitting
exactly yes yes yes yes I did not
mention it we are doing cross-validation
so we take our data said leave out some
testing data and from this training data
we break it up into five parts and then
we leave one out train on the four
remaining parts and then predict on the
the fifth one and then we do this for
every block we select the best hyper
parameters there then we fix them we
retrain on the training data and then we
predict on the left out testing data so
we've never seen the test data to select
the parameters and that's really
important so something that's also
interesting that I I'm not showing here
is that you could use this bound to
estimate the performance of a set of
parameters and use this to replace
cross-validation so you would just train
the model on your training set use this
bound to get some idea of how well it's
gonna work on unseen data and then pick
the combination of parameters with the
best performance according to this bound
s is the number of iterations so you're
doing early stopping you could say I
have a maximum of 10 rules but for me
based on my cross-validation score seven
seems to be the best so we use 31
because people that were working with
bacterial genome comparisons usually use
that value we drive it it gave very good
results and then just to make sure we
weren't actually missing out on some
information we tried to cross validate
from K equals 15 to K equals 91 and then
we tried multiple values in that range
and we saw that it didn't change change
much and the accuracy of the model
actually I have a slide about this at
the end if you're interested and so
right now I just show a comparison to a
baseline method but it's interesting to
look at how this compares to other
learning elk
so we compare it to decision trees using
the cart algorithm to sparse linear
support vector machines so in SVM with
the l1 penalty then too dense support
vector machines so just regular support
vector machines with an l2 norm and then
to a kernel method so a polynomial
kernel support vector machine which is
interesting in this case because it's
like consider if you it's like
considering conjunctions of Kaymer's so
if the degree of the polynomial kernel
is 3 and you're considering every
possible combination of 3 k-mers okay so
it's quite like using all the
conjunctions in the data so for the 3
first algorithms we had to use feature
selection because the implementations
that we had did not allow to to fit the
whole data sets into memory for the SCM
however we had are out of core
implementation so we were able to
consider all the features and for the
kernel method while the the running time
complexity only depends on the number of
examples so here we really consider all
the features so I'm going to show the
error rate and the number of cameras
used by each algorithm on 17 different
data sets but so the what's important is
really a big picture here so this what
I'm showing right now is the number of
cameras that were used by the model over
ten repetitions of the experiment so
this is why you have decimal values okay
so it's an average where we took a we
randomly split the data into a training
and a testing set so you see that the
set covering machine here uses three
point three rules whereas the sparse
support vector machine uses 20,000 okay
and this is really because our set
covering machine is a greedy learning
algorithm okay so it starts with an
empty solution and just adds rules if it
helps to decrease D the actual error
rate okay so we see that pretty much all
the time the set covering machine gives
us partial solutions and that we are
using less than five rules in every case
so we really have small models here and
now looking at the error rates on unseen
data you see that generally the set
covering machine has a smaller or equal
error rate
okay or if it's not the best it's very
close from the best for example 2.72
26.7% vs. 26.1%
okay so you have very sparse models and
this doesn't seem to be detrimental to
you the accuracy of the models yes yes
yes because we okay good questions so we
are setting the maximum number of rules
and then we are at each point we're
saving the model evaluating its
cross-validation score and then we pick
the one that had the best
cross-validation score so it's like
considered cross validating the value of
s but we have it for free basically
alright so now I told you the models
that we obtained were interpretable and
this is actual models that we learned
using this algorithm and what we found
interesting is that this took us maybe a
few hours of computation and we
recovered some resistance mechanisms
that were reported in the literature
actual resistance mechanisms so here
these little rounded rectangles are
antibiotics and these circles are rules
okay so when the border is double like
in this case it's an absence rule so it
means that it's the absence of the camer
that is useful for the model and when
it's a single border like this one it
means that it's the presence okay and
the the weight on the edges gives you
the importance of each rule according to
the algorithm and the number in the
circle is the number of equivalent
optimal choices that the algorithm had
because k-mers are correlated it uses it
uses the best rule based on some score
but they're often more than one optimal
choice it just picks one randomly but we
still report the number of rules and
I'll tell you why in just a second
in the absence of cambered 15 it's
guaranteed
yes what it means is that in the absence
of this scammer or the 14 other
equivalent cameras it's not the camera
number 15 it means that if this one is
absent the model will predict resistant
no because we can have mislabeled
examples this rule could be like you
need to look at the error rate of the
model to know how accurate this model is
so what we did is that we use the tool
called Blast
it's basically some search engine for
DNA you put in the DNA sequence and it
looks up the sequence and annotated
genomes and it tell you tells you where
where it's found so we colored these
circles based on the annotations that we
found so for example for isoniazid we
found that it's the absence of a k-mer
in the CAD G gene that's responsible for
the resistance and there's actually
publications about this it's been
validated in the lab so looking again at
rifampicin you see that it's two k-mers
that hit in the RNA polymerase which is
what i was talking about in the
beginning this is the the ribosome which
is responsible for creating a protein ok
so I think the most interesting rule
here is this one the yellow one it's
actually you see that there are 616
equivalent gamers and that it's the
presence of these gamers that's
responsible that's a good predictor of
resistance and in this case if you look
at all those gamers you see that they
really you could assemble them and they
covered the entire gene okay so it's the
presence of the gene that's responsible
for the resistance and this is why we're
showing the number of equivalent gamers
because we're not using them for
prediction but it could give you
additional insight on what's the type of
mechanisms that you're identifying and
when we looked at the literature we
found
that this gene is actually a known
marker of cross resistance between these
three antibiotics so when it's present
the bacteria will be resistant to the
three antibiotics here so and this was
done without any bias it's a completely
untargeted we just recovered that yes
yes yes and I'll show you on the next
slide
check out going beyond cameras with
cameras because you could try to use
those cameras to understand which type
of biological mechanism you identified
here what we did is we took the 15
equivalent cameras for the eysan eysan
model and we i'm showing them on the
gene so this is the position on the cat
g gene and each line corresponds to a
k-mer
and if you look at the point that they
all overlap this is the only position
that they all overlap and it's a known
hotspot for resistance conferring
mutations for as an eyes it and so here
what's below that black line is the
variants that we have served in our data
and they are all known to cause
resistance to the antibiotic and the
wild-type is actually the sequence that
you find in bacteria that are sensitive
to the antibiotic and what is i think is
fascinating here is that the algorithm
is very is a greedy algorithm so it
instead of selecting each of those
variants individually it said well i
could capture all those variants using a
single rule so using one rule which is
the absence of the wild-type it models
all those different variants that we
have in our data yes so we get really
sparse models using this method
that's right in this case all the models
I was showing I forgot to mention but
they're logical or x' so it's if this
one is present or this one is present
well I guess in that case your point
would be right since you would need to
have either of those so if you have more
mutations you have you're more likely to
develop the resistance if it was a
logical and then you the more rules you
have the less likely it would be that a
bacterium would be resistant because you
need all these are also be true for it
to be the case I do not think that it's
urea right I think that mutations are
not I wouldn't know I think they are
definitely not independent and that they
must have different probabilities of
occurring but I don't have enough
biological knowledge to elaborate on
that well I think in your data set
they'll be correlated because you're on
the pressure right to survive in the
presence of the antibiotic so if some
set of some combination of mutations is
important then they'll be selected for
and they'll always be present together
you're right yeah in that case the
bacteria will just die and they wouldn't
survive right um yes
good question actually we we had genomic
data and that's pretty much why I think
since the the bacteria the the
resistance mechanisms are mostly genomic
you have all the information you need by
looking at the genome if you were
looking at protein sequences you would
know if some mutations are synonymous
for example if you switch something is
it equivalent in terms of protein but we
just didn't have this type of
Datta yeah and there's no alternative
splicing in bacteria so it's a it's a
lot simpler than in humans so by looking
at the genome you still get a good idea
of what's going on
and so yes we we also created a
web-based platforms so microbiologists
and anyone who's interested can go and
look at these models and maybe try to
draw some insight out of them so this is
available at this URL you have different
species different antibiotics you have
the metrics that measure the accuracy of
our models and then you could look at
the models like this one and they're
annotated according to some according to
blast the the annotation search engine I
was talking about earlier and then you
could click on them and you have access
to all the information about this gene
all right so to conclude we have used a
set covering machine to learn very
sparse models from extremely high
features feature spaces high dimensional
feature spaces with very few examples we
have an algorithm that's extremely
scalable in this case with our out of
core implementation it was shown to
generalize well to real biological
datasets and the models that we obtained
were interpretable and we made no
assumption on the type of organism that
we were studying here so as part this
future or future work we are applying
this to human genomes and in this case
we have much higher much bigger datasets
we are applying this right now to 1,000
human exomes and this is about 5.5
terabyte of data so this is the biggest
data set that we fuse the algorithm on
for now we're also looking at extensions
to consider unlabeled data because we
have a lot of tuberculosis genome but we
don't have antibiotic measurements for
cement for all the antibiotics but I
think there's still some relevant
information in there and we are
definitely looking to consider
abundances instead of just the presence
or absence of k-mers and the key to
doing this is realizing that cameras
don't a cure 1,000 times in the genome
they mostly occur one two or three times
right so you don't need to consider a
big range of thresholds in this case
so that will be it thank you very much
for attending this talk and these are
the people who work and have worked on
this project with me in our lab this is
the URL if you're interested in looking
at the code and we have a paper about
this in BMC genomics if you're
interesting in knowing more thank you
it's a good question it I think it would
be better to have a general model for
multiple species when we first did this
analysis we were we didn't actually know
that we were told by microbiologist that
the resistance mechanisms might be
different different based on the species
so we just tried to learn to separate
them right now we're trying to learn
more complex models like tree based
models instead of just logical
conjunctions and then we're trying to
apply those to combinations of species I
think that they will have a more complex
structure that will be more appropriate
for modeling multi species mechanisms
yes yes we only had seed if data so
these results I showed you from two
different studies this was in our paper
and the Patrick data was after that so
there's we didn't get I think there is
seed of date and Patrick but there was
not enough based on our 50 so we had a
criteria on where we said we need 50
examples in a tray of each class in the
training set and this filtered out seed
if in this case but in in our paper we
had I think five seed of datasets that
were given to us by a microbiologist at
me
the university well we can't tell until
we validate them right yes well in this
case we have for every throw mice in
resistance we have the absence of
cameras in Midian and synthase and
what's so actually this is undocumented
and what's interesting is that it's a
synonymous mutation so if you look at
the DNA it's different if you look at
the protein it's the same but there's
still translation of RNA into protein
and sometimes different codons could be
more rare than others and this is why it
might have an effect on resistance but
this is very hypothetical we have no
validation right now there was
fine-grained data but we found that it
was very often noisy so what we did is
we took the official thresholds from DB
regulatory agents
agencies like the you cast for example
and we just said everything that's above
the threshold DB public threshold is
resistant and everything that's below it
is sensitive if we were to do to
consider the actual minimum inhibitory
concentration values I think would be
better to look at interval data to say
is it within some interval because if
you test a bacterium and you retest it
the value can change quite significantly</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>