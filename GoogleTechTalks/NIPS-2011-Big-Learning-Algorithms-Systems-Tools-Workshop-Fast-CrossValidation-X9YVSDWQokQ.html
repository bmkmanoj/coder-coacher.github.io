<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Fast Cross-Validation... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Fast Cross-Validation... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Fast Cross-Validation...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/X9YVSDWQokQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay yeah hi everyone can you meet ya
okay I'm Tomic looga and today I'm going
to end fast cross-validation where I
sequential analysis so this is joint
work with my colleagues from the team
ready then you paint me a picture brown
so let's never look cross validation is
something everyone here in the room has
used before and but you all know that
it's unfortunately also very
time-consuming so here's an example of
our current project so we use fitted QE
generation in the reinforcement learning
context even with a very small parameter
with 10 x 10 we get nearly half we had
half a million regression problems and
would be cool to have some tool that
points these numbers down to a
reasonable size as you also might
imagine directly optimizing the error
landscape to avoid calculations is often
difficult due to noise so our new
approaches that we use increasing
subsets of the training data and this
has some nice properties if we use
smaller subsets subsets the training
time smaller that's cool more training
data error to this model gets better
error estimates and make what we will
see in the next slide is that the
relative correct the behavior of the
parameter configuration converges so
let's have a look whether this claim 3d
is right so what can we see here this is
on the x-axis we see the Sigma of a
Gaussian kernel on the y-axis the class
education error of some classifiers
which has been learned or increasing
training data sizes so and generally the
first thing that we observe that we have
some Sigma's that are nearly never the
best optimal parameter and the other
thing that we can see that these optimal
parameters that attained kept the haps
the best training error this behavior of
these Sigma's converges but what
also see is that with small training
data size some not an optimal parameter
might be chosen and so we have to take
care that we not stopped prematurely so
and this is here an average over 500
repetitions and what we can see here is
the nine individual runs and as you see
it it's much more noisy the error
landscape but the overall behavior that
we have observed in this average case
stays the same so essentially what we
can see is our individual runs are noisy
but we can see the over a tendency of
the parameters we have a lot of
underperforming parameter configurations
and we can estimate the correct
parameters on a sufficiently large
subset of the data so how can we exploit
this now our idea is to transform there
is a point five errors of these
configurations into a binary top of
flops team with robots testing on the
way drop significantly loser
configurations while a test from the
sequential analysis framework and
furthermore implement an early stopping
procedure which decides whether we have
reached these table to toes where we
have seen enough data to really
estimates a correct parameter there's
optimal parameter so how do we do this
here's one step of our fast cross
validation procedure we learn on a
specific training data set size and can
calculate the residuals on the remaining
data set and gather these interview in
this point wise performing matrix then
using the friedman test we can see and
we honor them according to the average
mean square error and with the Friedman
has we can test whether we some
configurations act significantly
different then the top performing ones
and so we can categorize the
configurations into top and flop
configurations and copy this data to
this
trace metrics in the binary form and
then we can use these past behavior of
all the configurations in these
sequential at a level analysis testing
framework to identify significant loser
configurations so how is this done
basically this is a graphical depiction
after his test basically it's test which
looks whether a binary random variable
is has a success probability of pi 1 or
bigger as s PI 1 or smaller or equal to
PI 0 and we can see here how this is
done we actually just you know to you
this cumulative sums of the traces so a
0 means that we go one step to the right
and one means going one step Tywin and
upwards and if the trace hits these
regions then we say it's either a loser
or winner region and after we have done
this we can categorize or label each of
the remaining configurations as a
significant loser or not so in this case
the first 21 CR dropped and to see
whether we have reached this stable
configuration as a stable region where
we can really estimate the optimal
parameter configuration use another test
cochran cue test and we look with the
given window size with a sliding window
here in this case it's a window size of
for whether the remaining configurations
act similar so and in this case we can
see that this first is this
reconfiguration here significantly
differently behaved than the other
remaining configuration so we haven't
reached this table the part that we have
seen in this motivation slide so the
procedural says we have to go all we
need more data to really get to this
stable parameter estimation poem so how
do we choose these parameters for a
sequential and for our sequential
analysis test we have seen that
we cannot really use individual traces
to determine whether configuration is
the top performer and the overall sense
so we have to look at the over at all
remaining configurations so therefore we
fix this point here to the maximum
number of steps and since we want a
really aggressive tech aggressive test
we say okay maximize this region here
and this gives us and gives us the PI 1
and PI 0 that we can readily plaque into
the sequential analysis framework and
have a test which given a specific
significance level beta and alpha has
these properties denoted here in this
optimization problem so okay what does
this mean for our procedures we have to
keep in mind that the first crushes are
the most difficult configurations for
the first was validation are those
configuration which our first flop
configuration and then change to top
configuration to winner configurations
and therefore we have to look how the
test act on these or behaves for these
these kind of configurations and we have
done a simulation study where we
simulate these switching configurations
so to say by binary random variables
which have such success probability
ranging from point zero point 1 2.5 and
at a given change point changes this
success probability to a straight one so
going from a flop configuration to a top
configuration what we can see here are
the the ratios or the probability that
these specific configurations would have
been dropped and the nice thing is that
we have some kind of security zone where
no configuration is dropped through
where you have no false positives false
negative so and actually we can also
Rufus if the change point lies in this
region here then our algorithm has a
show to sell securities or maybe you
have false negative rate of zero so this
is pretty nice so the experimenter can
just fix these significance level alpha
and better and we can calculate the PI 0
and PI 1 with this optimization problem
that we have seen before and you can
estimate with this our conservative or
aggressive the procedures so here's one
one example run off the past
cross-validation what do we see here on
the x-axis we see the steps of the fast
pass validation on the y-axis we see the
configurations here we have roughly 600
configurations the red power means that
the configuration is a flop at this step
a black bar means that it's the top
configuration at this step and gray
means that these configurations have
been flagged as a significant loserness
drop are dropped so and what we can see
here is the that after two steps nearly
half of the configurations are dropped
so yeah get a nice speed increase here
and these blow-ups here these small
blow-ups shows us the window of the
early stopping were and we see the pay
do a graphical depiction so to say of
the behavior of this early stopping rule
so the procedure goes on until all
remaining configurations turn out to be
flops so nearly all configurations which
are kept here at least step 7 behaved
similar in the past and therefore the
early stopping who said ok we have
salvaged some stable region and we can
now estimate the correct optimal
parameter so that's really work we have
done some experiments on a
classification and seven regression data
sets for each data set we use half for
the parameter estimation have for the
test error estimation we used to support
vector machine or regression and Connor
logistic regression with a Gaussian
kernel of and 610 parameter
configurations and we estimate the
optimal parameter once was through
10-fold cross-validation and once with a
fast cross validation procedure was ten
steps repeaters with different splits 50
times to get the statistic for this and
we compare the test error difference and
the relative speed factor gain so let's
have a look first at the test error so
on the left side we see the results for
the classification data and on the right
for the regression data set for the
regression we have scared the regular
sense so that these numbers are
comparable generally we see that the
error difference absolute error
difference between the first cross
validation in the full cross validation
never exceeds all point 02 so this is
quite nice letting and what do we get
from this so here we see the speed
increase over speed increase as before
on the left side the classification data
arrives on regression later we see for
classification we get to speed up
depending on the data set and on the
method that is use of ranges between 20
and 124 one data set here and for the
regression we get something around 40
and 100 so why is there this pious
upwards in this regression case remember
we are I mean we have to categorize each
configuration into either top off lock
and if we find this regression case we
can calculate these top of flops on the
residuals so we have more information
than in the class education case where
we just have whether we have categorized
data appalling correctly or not and so
this test here in this regression case
can be much stricter and can drop or
select more info configurations as flops
so and therefore we get a higher speed
increase because we can drop more
configuration
on the way so basically that's it just
to sum it up so our motivation was to
estimate the correct correct parameters
on a sufficiently large subset of the
data we has transformed so cross
validation move into a race of
configurations which get em alerted on
linear increasing subsets of the data
and at each step of this race we
transform the test errors on individual
data points also Rene many
configurations into binary top of flops
key with robust testings robust tests on
the way we drop significant losses
configurations with test from your
sequential analysis framework and
reapply distribution free testing
techniques to decide whether we have
seen enough data to work correctly
estimate the right of the optimal avatar
so basically that's it from my side so
thank you for your attention and I'm
looking forward for lively discussions
yeah
yeah I think it's the same for a normal
cross-validation there you would have
the same problems that you have each run
this non deterministic thing and
therefore yeah we don't lose anything I
think because if you use a normal
cross-validation then you have the same
problem so
Frank
mmm
yeah sounds like an interesting idea so
and I think we are doing it more in a as
you said a sequential way and we unroll
it so to say and but using benders why
not
until defective you might qualify
now the bit right
No
but when they
yeah you should talk
so we haven't done any other experience
we just use this linearly increasing
thing so but perhaps it might be also a
good idea to use other schemes yeah
where do seems like you're assuming that
a configuration is it has an intrinsic
goodness and we're trying to ask me from
small samples no we know that the
goodness of the configuration depend on
sample size fits all summer if you
reject shape of learning curves which
and
no but yeah I think you have to keep in
mind that if you have a learning problem
at hand then you just have a limited
amount of data and for this we just can
use this amount of data to look at the
shape and the final shape will be the
maximum number of data so basically this
is what we want to explore to buy
linearly increasing subsets of the data
approximates this last curve so to say
soon as the structure of the last curve
yeah yeah yeah so this is somehow it
twice thumbs that you specifically
created to look at like this because if
we it has some some smaller structure
that you can just see at a specific site
training set size and this is so
behavior that we can see in this that
you can actually see here so if you have
just 100 data points then we the
cross-validation fix fixes its optimal
parameter to this global structure and
if we have more data at hand then we see
the fine grain structure and can we
estimate this optimal parameter reliably
yeah
from small hmm okay
it is
from
but one of years
the whole point of machine learning is
where our algorithms to themselves at
the complexity of the problem and
denominator always come back to hear
about take you everything I don't really
I guess people are</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>