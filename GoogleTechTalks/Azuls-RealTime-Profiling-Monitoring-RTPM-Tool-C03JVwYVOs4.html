<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Azul's Real-Time Profiling &amp; Monitoring (RTPM) Tool | Coder Coacher - Coaching Coders</title><meta content="Azul's Real-Time Profiling &amp; Monitoring (RTPM) Tool - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Azul's Real-Time Profiling &amp; Monitoring (RTPM) Tool</b></h2><h5 class="post__date">2008-02-14</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/C03JVwYVOs4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello I'd like to welcome cliff click
who is a distinguished vm hacker at azul
systems he's been working on hot spot
for what about 10 years now yeah yeah
bout 10 years he's used to work at Sun
was one of the chief designers of hot
spot there and now he works it Azul and
he's one of the chief designers of hot
stops hot spot there he's going to talk
to us today about the monitoring tools
and techniques that they use that excuse
me that they use it as well cliff okay
so in case people don't know I'm going
to go through some old slides that I
brought that aren't really terribly
appropriate just talk about what Azul
isn't and you know what we do and why we
have the tools and then I'm going to
attempt a live demo of the tool that's
unscripted by me in any way and so you
know after some showcasing of the basic
stuff will let people ask for you know
what buttons to push and we'll go track
down stuff with the tool and see how it
works ok so so what is the sole systems
the like I said these slides are a
little bit inappropriate but they're
what I have a pose for the moment we
sell a piece of hardware for running
very large java vm or lots and lots of
small java vm sort of any mix in between
the hardware supports many hundreds of
cpus that are shared using cache
coherent shared memory hundreds of
gigabytes of heap the the cpu is our own
little custom cpu it's a standard three
adjust risk we have little hardware
tweaks thrown in that makes some of the
stuff we do very cheap so amongst other
things we do is that we can sustain
allocation rates on the order of say
fifty gigabytes a second of allocation
that's not megabytes a second gigabytes
a second of allocation with essentially
no pauses ever so this is you know
concurrent GC at a very high throughput
rate and the way we do that is we have a
read barrier in hardware so like I said
I'm going to I'm just go touch lightly
and what we do it's traditionally been
felt that it's too expensive to do read
barrier and software so we did it in
hardware that hardware read Barry I
won't go in a lot of detail
it but basically let's just use a very
simple a very simple GC algorithm that
does concurrent relocation that's sort
of the big one and that lets us run you
know moving objects in the heap
compaction of the heap and that kind of
stuff all entirely concurrent with the
java threads running and that's where we
get to do stuff like you know 50
gigabytes a second of garbage collection
you know recovery without killing the
mutator threads I'll killing the running
java threats let me see skits little
stuff here we one of our big ticket
selling items here is what we call pause
lyst GC and it's actually very short
pause times so for for you know during
the course of a day you might see a
pause on the order of somewhere between
a max pause between 100 and 200
milliseconds all your typical paws will
be 20 or 30 milliseconds they'll come
every few minutes you'll never get a
full GC pause of 30 seconds or 10
minutes no matter what there's no stop
the world phases needed there's no you
know the individual threads only stop
for brief periods of time and we do this
by running a lot of GC load on
background CPUs we have lots of lots of
CPUs so we can do a lot of garbage
collection games in the background and
then when we get into the monitoring
tool I'll show you a bunch of stuff
about how the you know GC monitors and
what we can do with this stuff is wrong
direction um gosh the rest of stuff sort
of doesn't matter like I said the slides
are old here so so let me recap real
quick we sell an appliance it runs java
on the appliance but it does so in a way
that's very gosh i should start yeah i'm
really an unscripted talks are bad this
way uh okay so we sell an appliance that
runs java but the way it works is that
you you launch your java vm on your
running let's get this up while we're
looking here you won your java vm on
your on your existing cpu and we just
shipped the bike codes over to our
appliance run the java their shift the
results back what
running on your original machine is
nothing more than like a cisco router or
we call the proxy just a packet forward
or takes the io ships it over our box we
do all the work their ship the answers
back so in this case this box we're
showing here is a standard Linux machine
it's a two cpu intel looks like totally
normal blade I've just run a 64
warehouse jdb because I've been Dorking
with command line flags here I've
specified for no good reason a 200
gigabyte heap so so here is MIM commit
of 200 gigabytes this is going to be 64
warehouses and run for like two minutes
not very exciting up top Oh jvb I'm
sorry JB is a job as a very standard
Java benchmark the way you're supposed
to run it is run it with as many CPU
many threads many warehouses doing work
as you have CPUs what I have up top is a
perf bar showing a 768 CPU azul machine
which ever to run this with 768
warehouses would take an hour to get set
up so i don't want to do that so i'm
going to run it with like 64 because i
can do that in a few minutes and in
theory what you're really supposed to do
is run adding one warehouse until you
double the peak CPUs and then you look
at the tail off p max and twice max or
whatever but it takes about a couple
days to run an official where jbb score
on our big box so this is instead as i'm
just running just loading two warehouses
will take off at 64 up here there's
going to be one cpu is actually doing
the loading and the rest are doing
background GC cycles periodically and
there's a bunch of compiler threads
thrown in there that i'll show you in a
minute I get to the monitoring tool but
this is a perfe Green is a cpu busy red
is Colonel and it runs off to the right
of the screen to see if I can flip over
here this is my bad connection there go
okay there's the right hand side that's
the other half the perf bar so you can
see it's just too big to show everything
let's go share the memory thing I
nobody's get back to okay so mostly I
don't care about that except i'm going
to start here
did it take off here any second okay
we're getting bad BP and leg here oh
it's trying to go somewhere here it's
it's uh oh no we're hat we're getting
bad that you can see it's going for like
the 64 CPUs all once and then that they
drop down and VPNs giving me as one a
second peak so let's let's um let's skip
that for a minute and let me pull up
this other guy will come back round to
this guy in a second here so the tool
that I'm going to talk about is our Lexi
let's go to hear this is our arty p.m.
real-time profile management whatever
its acronym de jour and this is a java
web browser hooked into the running JVM
so you can attach any web browser into
the existing JVM and we'll look at all
the stuff i can do while though je viens
running none of the stuff i do has any
overhead on the JVM and that's mostly
because James do collect a ton of
internal information and then they they
don't typically export it we've just
drugged most of the information out we
found this very useful so we kept adding
things and could drag out that didn't
cost us anything and in the end we have
a whole lot of cool stuff we can get out
of a running JVM without any interesting
overhead on the running JVM so somewhere
back over here on the other window you
can't see here down below I have an old
jboss window and I have oh by the way
I'm am running the local internal
engineering build of things so this is
alive and a lot of ways me you're on
Grindr on this guy so this is jboss with
grindr the jboss windows up here is
already Ryan I just fired off a grinder
load and and then I'm here is my RT p.m.
start up window I have the command line
for jboss this is me calling out my
version of Java on the Linux side and
then some jboss arguments
use locked collections is how we work
around bugs in jboss where they fail to
properly lock/unlock java to
unsynchronized Java collections and then
you know other stuff I have say for
instance all the hotspot internal flags
I can look at there's no end of them
here here's c1 and c2 you'll hear me
talk about c1 and c2 a lot that server
and the dash point compile you guys know
anything about hotspot and that's your
desk client compilers some people so
dash clients the light weight fast
compiler dash servers the heavyweight
slow compiler we have it in a tiered
configuration we run client we profile
using client compiler and recompile
using the server compiler using the
statistics gathered from the client
compiler I can look at the tix running
jboss most of them are caught between GC
and fill and stack trace because i'm
running a very small heap on GC so i'm
doing endless GC cycles in the
background if i go find somebody who's
actually got some code compiled here i
can get you know disassembled assembly
language code with ticks profiling ticks
on the left and you know where a memory
and here's the actual machine
instructions this came out of the
heavyweight server compiler jet that's
our internal semi language for this
particular giant method son I owe CTF
encode whatever this is the code that is
generated by the heavyweight jit for
that method yeah
in this method that's yeah yeah which
probably means that we bail out real
quick out of the method you came in you
took some stall cycles getting into the
method and then you did a little bit of
work on your belly they're all spread
out there you go okay so there's lots of
ticks down in here we come in here a
long time if I were to guess I guess
there's some Luke going on here that's
doing something UTF encode I'm totally
going to guess here but this looks a
whole lot like load of character store
bite and search and loops I suspect he's
spinning through a java string pulling
16-bit characters out encoding them
8-bit bytes riding back out to a byte
array somewhere and he's looking for
some Terminator character that's just a
guess based on the name and you know the
kind of coding style I see here big
unrolled loop and on and on and on it
goes so here's push frame are 12 resolve
patch call that's a standard function
call resolving patches an unresolved
function call to see if I got a resolved
function call in here I don't saw in
line unrolled loop oh well collie call
our info car set encoder gets called
mostly by string got encoding which gets
called by SunNet double protocol which
called by which gets called by and so on
and so forth so I have call path and
annotated with the tix that's a compiler
profiling stuff there we roll down here
I see here so here's the running threads
i just clicked on the threads list
here's all my threads that are currently
running this jboss most of them are
probably not doing a whole lot this guy
is doing something he was at the time I
looked this is completely live so I can
refresh and see things change see
nothing so nothing Jo he did change he
switched her lock I get a stack trace on
the live thread he's a middle fill and
stack trace well that's where all my
time is going ok that's fine if I look
down in here and say so was he got a
legal argument section calling date
parsh here's date dot parse as the
method itself here's the compiled code
for it it's not really what I want I
want to know what he why he he fail to
whatever its there's the code for date
parts i really like you know why he
federal and start storing exceptions so
I really want to do is see if I get a
medium stuck tree so he's going to have
moved so medium stack trace is break out
of every bit of thing the server
compiler guy that's the c2 won't have a
whole lot of information here because
he's optimized it away see here here is
that this pointer for this frame so we
start at the top we're stuck in a socket
right came from Java net socket output
stream right which came from this right
call which came from internal output
buffer blah blah blah blah blah blah
blah which came from which game and so
on so forth some are down in here I
start seeing stuff like here's at this
pointer it has an actual object here is
a local variable which has a one it's
nine you know real flush at the Java
level and so on I can drill in on an
object here is this instance of this
object it's got an array of characters
which is no it's got a thousand
bite-size write buffer thing it's got a
field called lock it's got whatever
things it has I can drill and on each of
these other things too oh my heads turn
spinning doing that okay so let me come
back around here let's pick a new
top-level thing here's monitors so
what's up here at monitors monitors is
there hot locks contended locks compile
law code cache lock these are internal
vm locks but this is a java level lock
if I drill in on him I can see here is a
typical blocking call to that lock so
that is my highest level hot lock and
the reason i can't keep more than a few
cpus busy running this version of jboss
is because this lock is contended i have
you know there's other locks in here so
this is actually not a very big example
i need to get something with more
threads and more locks here's freemarker
that cash two-thirds of the time you
come in on somebody's get template call
and you take this lock cash that anymore
you cash storage and block and one-third
of time you come in on this call it's
interesting
memory it's drilling here real quick so
I've given myself a 4 gigabyte heap
currently using only 500 Meg's I've run
a thousand GC cycles since i started
jboss a couple hours ago if I get a GC
summary it will see a summary of the
last 50 cycles of the lasso to go last
20 minutes lots of stuff including
allocation rate and the size of the heap
so here is I'm doing bi 30u Meg's a
second on this jboss there have been no
thread delays see what else we there
been pauses the largest pause in the
last 20 minutes has been seventy three
milliseconds and that's in the young Gen
if I go to the old gen collection the
largest pause has been 16 milliseconds
and so forth allocated overall time I
could see I've had two billion two
gigabytes of car raise allocated overall
time and 500 megabytes of byte arrays
but a more interesting one hears yeah
that's that's all things so let me let
me be clear here I intended to be a live
demos and people ask questions I'm going
to steer around and drive and look at
stuff and poke at it I can run different
applications I don't have a huge
selection but I can you know if I have
it i'll run it i have a lot of the
random we need java benchmarks floating
around jboss is sort of the more
realistic thing i have here and then you
know really is sort of like how would
you use this to go do something
interesting so for instance here is
what's currently live in my heat I got
63 megabytes worth byte arrays and if I
drill down in the byte arrays I have
points to information I see that forty
megabytes of that comes out of jar files
another 15 Meg's at a zip file so this
is my class files I've lived in my class
files as byte arrays of the Java level
so that's not going away it's not going
to get smaller or bigger it's nothing I
can do about it without loading the
class file so it's maybe not interesting
look at byte array but if I collect
comnet spective dot whatever I have 50
megabytes live in my heap of query entry
log things of which most are pointed to
from java.lang.object array so if i come
down to here at java.lang.object array i
see that most of these come out of
arraylist and some are rooted that's
what unknown is they're currently live
on somebody stack if I jump to a ray
list where to ray let's go I see that
most of the array lists come out of well
they come out of combat net spective
does something so somebody is making
piles of array lists and a hanging on to
them somewhere in jboss and all this net
spective stuff our pile of arraylists
being hung on to that hang on to a huge
ton of comm net spective dot axiom dot
whatever it is so i don't know anything
about jboss but i can tell you where the
memory goes and you know who's pointing
to whom buried era there in the heat
let's see the compiler is at this point
i'm going to be totally idle either
compilers are idle and settings is not
gonna have anything interesting yeah a
couple on-off legs for things that
actually cost so um pretty much
everything I can show you here let's go
to I owe for second is all gathered sort
of free of charge from the point of view
of the vm it doesn't slow it down at all
and it's always available it's always on
you attached with the vienna you
attached with a browser and you start
surfing around this would be true in
like a production vm it was a week old
you know you had a weird crash or
performance anomaly on your running vm
if I still up and running you can attach
to it look at it so one of things i did
here is I looked I see oh I got 25
million calls to stat and 15 million
calls to stop mode and I can see how
often they're coming in where we go here
they're moving rate is like 10,000 calls
a minute of stat the appliance is you
know running the bike codes on its side
and the proxy is running on standard
Linux or solaris or whatever OS you have
on your side so the appliance really has
to do a caching of like standard i/o
stuff in order to get performance
otherwise it's an RPC call to and from
the proxy so he has a cash for lots of
stuff including like a name cash since I
can get stats on how well the name cash
is doing who's hitting and missing the
name cash I can take a look at like
here's open sockets I've got a bunch of
sockets open for for exactly this
browser right here there's another
socket open I don't know where that one
came from and then jboss opens a bunch
of sockets because he's listening for
the grinder process which is throwing
transactions at jboss get open file
handles a lot of jar files are open we
hundred lines of jar files what else is
open here is an open file oh here go
local data there's a log files open too
yeah no no so let's go back over here
let's see where did this guy here we go
let's kill this guy so I'm going to turn
on some engineering flags here to make
my life easier except that except that I
can't type and the Oh VPN come back come
back come back oh god oh man oh no this
is disastrous
I'm picking a different port number if
you were doing this in production you
have the ports come out auto configured
on launch but I'm running an engineering
machine on an engineering set up and we
don't have the management stuff running
on the sky so in fact I'm going to uh
yeah you can't see I can't see ok look
it looks good ok maybe assuming that the
typing wasn't so bad
that looks bad yeah yeah you can see 1 2
3 5 i'm aiming to get when I 1 2 3 4 1 2
3 5 except i probably can't spell swx 20
21 1 2 3 4 s to X 21 to 35 then dead
miss type something here are did I fail
to open oh come on why just why did this
work and this one I no damage is a live
demo so the answer is that I can open up
more I can open up as many as I want I
mean every every process is his own
thing
oh you want to aggregate across the
whole thing so right so if you go to the
management console thing which i wonder
if i can get up here he will aggregate
across all of them so this is definitely
me being unscripted because i want the
faster to put in here so these are two
different linux box is launching against
the same appliance okay so this is in
theory going to be okay this this is a
startup of jb be suspect that WV that
mean calls do it calls run warehouse
calls yada yada yada yada yada and up
and up and we go home ec he's busy
running I'm sorry um how many the big
appliance um you know 100 200 I don't
know in production of people that may be
run more than a hundred but I don't know
some of the guys have filled up with the
kind of Suisse's got a couple big boxes
really full if you use a developer box
that's going to be like a quarter that
size so about two hundred CPUs you can
expect to have say twenty to thirty jvms
on their really without any trouble you
know without knocking heads on each
other and running out of resources so so
at this point let me start taking
questions from the audience about what
to go look at here and people interested
yeah what's good boss can you speak up
or reason I dare information about what
about my information up virtual calls
right so um so hot spot by default
heavily optimizes virtual calls and
without doing it without zool doing
anything special there that's just hot
spy most virtual calls in fact call a
single target and the runtime the
compiler knows it and they prove it and
then they make that go to a single
target and they in line and then you
want actually see the call at all to
just be gone for an interesting fraction
of call
all's they aren't able to in line but
they are able to show that there's a
single very common target in which case
city what's called an inline cash it's a
one entry cash built right in the code
where the key is the expected target and
the value is a subroutine call to that
target that would just be in line in the
code if you go to the profiling
information here let Mesa go let me go
do this before my GV be runs out if I go
look at profiling information here not
drugged-out properly because this is an
engineering build is a list of all the
targets that we went to but the server
compiler knows what those targets are
although the display coming out here is
not showing it I've turned off funny
flags for that and that's just you know
we're in the mid release cycle kind of
thing but that information certainly in
the machine and its intended to be drug
out but the usual answer here is you
have a single target and if you go look
at the server compilers code he will
have in mind that call you won't ever
see the call it'll just be part of the
code this yeah I can go look at the code
here so I don't know which version I got
till I jump into the assembly I got here
and in fact I got server code and the
server code is very short and there were
subroutine calls in the byte codes but
there are no subroutine calls in here
till they get down to these things
called uncommon trap dad's our paths
where the server compiler cut off
compilation because profiling data
revealed you never went down that path
so he inlined all the calls that were to
be had in the main piece of the code
here and the last thing he's going to do
somewhere down here is jump to a return
if I can find one buried in here 1308
here it is so he's got a loop going on
here's a branch to a 0 18 from here to
here as a loop and then somewhere near
he bails out of the loop and just as a
store in return and there's no function
calls now so probably had a loop around
some little bite parsing thing he
inlined all those function calls and
disappeared them and and so what they
are doesn't matter we take they got the
mic first
it exists but it doesn't work that's
cool surely you want the green light
instead try it again maybe it's got good
are we getting nowhere so systems often
to have both short-term and long-term
behaviors where things will happen you
know once every few hours or something
like that do you have ways of tying into
things like that so you can look and say
it see well show me what happens when I
arrived here as a result of an uncommon
event as a reverse as a common event
right something like that um so the
management tool which I have failed to
get the password for has a lot of that
kind of stuff built in including trend
line diagrams that are scrolling by time
things you can watch you know heat
growth events and the like that span
hours and days the this tool is using
get hdb you get and you could certainly
arrange to have somebody running gits
against a running JVM on a regular basis
if you wanted to write your own version
with tool and extract out what was going
on you know on an arranged to have your
own profiling bits there so there's
nothing magic here you just you can do
you can do a URL hacking and go get what
you want out of the VM and because
there's security issues and we sell a
lot to banks there's in fact password
games that have to be played if you turn
on all the right we want things to be
secure mode but if you're inside your
own firewall they use do URL hacking you
can get what you want out of it
so not to the source code I don't have
sores we have debated doing stuff like
chatting byte codes but that's sort of
you know go share something yeah so um
so here is the client compilers version
of come back oh now we're done
you have a job
yes in fact you can get the source for
it but they haven't been compiled by it
and in this case I am looking at
shutdown errors on my my exciting
version of vm which i have Bigley hacked
locking and I expected shutdown errors
which is annoying which is why we hung
there let me um yeah let's just relaunch
see what happens here so what I was
trying to show there is the assembly and
the bike codes are side by side I mean
I'll flip between them here in a second
they're gonna be meeting this guy back
up so let's let's just get coming fresh
ok here we go tix create a random string
so here's the assembly with you know
performance counter tix here is client
compiler byte codes so get static aight
i'll 0 to a a load all these are
standard bytecodes invoke static of this
function call and and so on and so forth
see what's this guy so here is that call
I just jumped to the version of random
here which is random dot next int random
about next and do a little math there's
an ad remainder and return the value so
it's a you know random it's been in line
so there's no assembly for that routine
the server compiler here's his he
doesn't have the profiling data on him
so I know these are all going to be 0
but he will say he got in line into the
caller so at this point the server
compiler code for this guy has been in
line this call got in mind and I'll here
go here guys this this thing's been
invoked 5,000 times there's a back edge
in the middle worth 134,000 times
somewhere here he's spinning in a loop
if I can find it reached 130 thousand
times reached reached this is I return
to this invoke static so here's the
client compiler in vault in line invoke
static here's the client depth one
here's the client compiler
too so there's a problem with looking at
bike cards is because of your interested
in the in lining and the client compiler
on the server compiler are going to do
different levels of in lining and I'm
only gonna get clutch profile info on
the Klang compilers in learning levels
it's just sort of the nature of the
beast but yeah but I'm looking at the
byte codes here and I'm looking at the
assembly here so does that answer your
question right so yeah the hard part
there is you have to give me the path to
the source code at the time you launch
the vm or at the time you run the
browser here we have the you know the
line number tables and all but I don't
have a good way to bring out line
numbers in any interesting way unless I
have a path to your source code as well
but something we could do it's actually
an interesting idea oh here we go here's
jbb running now here's 64 threads all
busy hammering away doing whatever
they're doing trans actually manager to
go calls calls calls and so on and
here's a lock let's see if I got a hot
lock display looks a little better now
no interesting locks in gbb these are
all internal vm locks yeah I can't see
it because it's an internal vm lock
here's somebody's locking object but
very very rarely transaction manager go
occasionally locks some object people
interested looking at other stuff here
io or GC cycles or you said this is
comes with basically with no costs in
your vm didn't quite hear out how much
of this is in the standard hotspot p.m.
and how much of that is custom so right
so about a third of what you see is
computed inside a standard hotspot vm
but it's not brought out in any
convenient way so hot spot internally
will profile and he will internally have
sort of the high-level profile bits
available to them about what methods are
high and you'll have some amount of the
GC cycle times
available to him for his DC heuristic
Tracking's hot spot right now doesn't
ship with a tiered system so you don't
get a server client split you get
profiling in the interpreter and the
server side or essentially no profiling
and then you run the client beyond that
they don't they don't that informations
inconvenient to get for them that we've
done custom hacks for so for instance
per thread dumps or something that's
inconvenient to get an existing hotspot
completely we've completely redone how
we stop and start and run threads and
that makes it easy for us to pick an
individual thread get a stack trace a
very cheap get us back trace and have it
take off and keep going that's hard to
do in the existing hotspot
implementation the garbage collector
doesn't doesn't do points to information
in the existing hotspot that's something
we hacked in require major GC hack to
get at that the tick profiling
information as far as I know that's not
easily available in hotspot they do
performance counting which is different
from we do both both the hardware tick
counters and the software counting and
the profiling so you can't get at the
tix like I can with the code now I can
spit out the code as a hospital engineer
on an existing hospitai using white
magic flags but I can't tell you where
on the code what piece of its hot the
hot locks thing again I don't know that
that's convenient to get out of an
existing hotspot I haven't looked at
what sun's been doing I know they've
been thinking about that stuff for a
while but you know we hack our locking
idioms a fair amount to make it cheap to
gather that information and then once
you go to gather of course you want to
bring it out so we can look at it so a
lot of was showing off here require deep
vm hacks that I don't believe son has
done sort of related to the source code
question any given sequence of
instructions may be derived from a bunch
of different sources all sort of
interleaved is there some way to look at
a sequence and say oh you know these
these memory references are coming from
that module and
those ALU operations are you know since
I'm on an old-time compiler hack I can
often look at the source code in the
generated code and tell you what you
went to what but that's not going to
help you help you unless you're you know
a compiler hack it's hard to do ok the
VMS that the program render halt here is
this let's try again the hard part with
that one is the level of Optimus
optimization is such that there's not a
very direct correlation between one of
the next right so you know it's gonna be
really hard for me to say or explain to
you how I got to where I'm at from where
I began with unless it's very simple I
take a very simple loop you look at the
code you can guess oh I know what that's
doing but as you add complexity you
start in lining and gets big you know
how do you make heads or tails of it I
don't know it's you want aggregate
information you want to come back out
and say okay I'm calling you know
whatever dot random a thousand times a
second we were wrong things try this one
you know I'm calling whatever stocking
it stock a billion times a second why am
i doing that and then you know figure
out from there what's going on you know
you can go to the threads and get poor
man's poor man's profiling by simply
drilling on a thread over and over again
so here's you know integer.valueof
inside loads knit table okay well if I
run a few thread dumps here and nothing
changes this is my common path on the
main thread and okay it's the startup
times mains busy constructing 64
warehouses when it takes off and main
goes idle I can look at the worker
threads and see what what they're doing
that way I can get you know field names
out of the generated code when they're
actually available so here's compiler
too he's he's that's the server compiler
he knows that what at the Java local
bite low double local one byte code
level is hashmap value iterator so I can
tell with the Java locals are if I'm
elected by codes where's my field names
here's the interpreter it's a giant
frames they're all 34 in trees oh here's
a string huh another string results to
expect at jb b dot whatever so yeah it's
it's always a case that you have you
know the the the difficulty of looking
at code and figuring out where all the
parts came from this case integer dot
value is probably pretty quick these two
instructions i happen to know our how
we're doing tracking of lifetimes of
generated code and then here's an
allocation idiom nughty lab there's a
set up for it and here is integer dot in
it not in line and then a branch to
return does that answer what you know
what goes on there I hard to say right
if you knew what your code was supposed
to be doing you can usefully tell
looking at the generated code at least I
can I don't that one's tough yeah it's
sometimes useful just to know this yes
instead of the instructions as a result
of those five I know what you're asking
yeah there's a lot of like instruction
scheduling going on and a lot of
interleaving of you know one thing or
another and so it all kind of gets
blended together and it becomes really
hard to tell who's doing what for where
so here's a pile of loads here's the lv
be by ways are read barrier instruction
so i've done a load 24 offset it from
register five mr. five was registered
zeros was at this pointer i loaded a
feel from this pointer field office at
24 the compiler knows what that field
name is because that yeah because he but
I don't have that brought out in any way
he doesn't track the name but he knows
at compile time same thing here I loaded
a different field I did Rand the two
weed barriers but they got scheduled so
the loads overlapped so there's some
scheduling going on there and as I go
further down the line I'll see cases
where the scheduling will be sort of
egregious there'll be hundreds of
instructions apart between pieces of
some inline function and it'll become
really hard to track
I've looked at such information when I'm
getting around to is I've looked at
information in the past it's less useful
than you think unrelated question very
often with sophisticated optimizations
if the source cut does something a
little bit wrong you won't get to take
advantage of those optimizations and so
yeah there can be cliffs in the
performance of the code if ya you
introduce some new piece of nationality
yeah sudden things get worse yeah do you
have any AIDS to try and diagnose those
sorts of problems no as a compiler hack
I know what I know it when I see it but
not how to explain to somebody else if
you turn on or off your your new thing
you can often see the where the time
went didn't go right but you don't know
why the optimization didn't kick in that
one's harder to do one of the things
that does happen in this kind of
environment is that we are looking for
optimizations that are very robust
because people aren't expected to be
experts and you're not expected to go
tune the heck out of things either so we
really need the optimizations to work
when they work and not when they don't
to try and avoid the cliffs that you're
talking about having said that there's a
very obvious set of cliffs that show up
in micro benchmarks that people fall
over all the time and I don't have a
good answer for although I can tell you
exactly how they appear and that's when
where you run a hot loop a million times
the body loop consists of a function
call to a particular target the first
time you run that loop you're going to
one target the compiler knows it and he
says AHA in line any in lyons and he
unrolls and strips to loop clean and you
get some really fast number the next
time you call that loop you go to a
different target well now that one
function Collins I'm going to two
different targets and the compiler
refuses the in line now at that point
you lose all this great optimization so
in this kind of a thing you're on the
loop once the first targets fast in the
second one slow it looks like the second
function call is a slow one but if you
reverse the war
the calls again the second function call
be slow but it'll be the the other
function call so it's unrelated to the
function call a target it's related the
fact that you went to two targets first
target is fast two or more targets is
slow and that lends a tricky one to get
right when you're writing writing micro
benchmarks other than that the
optimizations are a pretty darn robust
they like the inlining we in line very
heavily you won't kill inlining by
adding one more layer of indirection
will continue to inline sufficiently
that you get the good performance
benefits out of inlining that has by way
been one of the issues have had with
other compilers in the past where at
some point you made the function a
little bit too big and he quits inlining
from that point to the end even trivial
things like you know accessor functions
and and then performance definitely
falls off a cliff but the RM on
heuristics they're they're very robust
against that kind of stuff that's why I
just totally a hotspot general thing
that's nothing to do with azules year or
not
so if a lot of stunned looks back there
no one no one's uh let's see here are
people interested in looking at more at
the GC side of things or at the IO stats
I've picked up here get this probably
this guy's toast again yeah he's peas
run to the end and it killed himself if
you have a way to see how many cycles
are spent in c1 compiled code versus e
to compile code yeah but it's not the
answer is always not much in c1 okay
because as soon as you get much in c1 it
triggers a c2 right so if you have a
very short run time a program then yeah
you'll spend an interesting fraction of
that short amount in c1 because that's
although further agos but as soon as the
runtime starts to stretch out from
seconds to minutes it's all on c2 code
so that'll you don't have any
applications that have a large enough
method footprint that they just spend a
lot of time in the jet and NC 1 code
other things that would cause code C to
compile code to get invalidated and
recompile we do see code that where C 2
gets invalidated and then recompile but
in the end you end up in situ code again
after the invalidation go around do
people know I'm talking about their the
server compiler makes he's very
aggressive optimizations and as part of
them he will make assumptions that so
far upheld true like I've never seen
this value be no before so instead of
putting the explicit null check which
the Java semantics required me to test
for null here I'll put an implicit null
check where I simply load through it as
a memory reference and if I get the
hardware trap it says I took an old
memory reference then I tested for know
but the recovery from that's painful
slow so I don't want to do that very
often so if I've never seen a null I'll
go ahead and omit the fast version and
that holds true then i win I don't have
to do a branch on null test right but if
it ever fails then I have to do the
recovery it's expensive than a recompile
up method knowing that I've seen it all
now and i get rid of the null test so
the sea to the server compiler may go a
couple go rounds compiling the same
method as it discovers the boundary the
profile of what's typically taken in the
execution of your program
having said that you know after a few
minutes of phase change in your program
start up to steady state or steady state
one to state say to just typically after
a few minutes you're all on C to code
and that will show up in the tick
profile you'll click on the hot things
that'll be c1 methods for a while and
then the how things will be all see two
methods so do you have any story about
how real customers use the tool to
resolve their real issues um yeah so
right so see how far I can go here so a
lot of them some of the issues show up
in how you scale so people are trying to
run more thread so that show you jboss
is running basically four cpus and she
choked on that lock okay now that you
know he's choked on that lock you can go
fix it and you'll figure out how you can
stripe the lock or you're going to
shorten the whole time or change how
you're doing to get rid of the lock and
then you go back and run again okay now
you're using eight threads let's say and
you're on a different lock and you go
find that lock and now you can go fix it
and so on and then you discover you're
out of threads all the threads are busy
but your thread pulls too small why'd
you tell that and go look at your
threads listen they're all they're all
running on umrah blocked but you know
you're throughputs limited by 0 count of
threads okay increase the size your
thread pool and so on that's sort of one
obvious way another thing that happens
is people will use our garbage collector
to fix egregious GC pause time issues
that they have but it usually it just
means they made the problem go away so
we have a lot of profiling data on
garbage collection because we have
people who are very interested in
knowing that they're not seeing big
pauses they have you know soft real-time
issues but that's usually not an issue
for us you can tune the GC if you want
to to within a hair breadth of you know
killing it but it's nothing like the
kind of profiling that typically goes on
on a regular vm if someone's up against
the 2gig limit our four gig limit or
whatever the the Linux limit is usually
they've profiled extremely heavily on
the garbage claims of a dozen flags 50
flags 100g cflags I've seen huge command
lines you wouldn't believe
all those problems typically go away so
they are very interested in the GC
information but it's not a problem at
that point we have had people stare I
like the IO stuff figuring out that they
are doing a lot of i/o if you scale the
thing up of course you scale your
program up you get different bottlenecks
and at some point you know the logging
that was cheap when you were running on
four and eight cores becomes the
bottleneck when you're running 16 32
whatever and you find that out by go
looking at the i/o side and seeing oh my
god i'm i'm coughing up a megabyte a
second of log file or you know 20
megabytes a second or something like
that I've got examples where it was
literally a gigabyte a second of log
file and that was that was the block
that was the you know the bottleneck
let's see here people doing the thread
dumps that / not the detailed ones but
the light just one line per function
call we're looking at how a thread got
here like what the heck are you doing
here kind of things you have this red
get in there it's usually a concurrency
race issue they didn't understand how
they had a race to error somehow and the
problem was they have two threads going
in once modifying one's reading and they
thought it was a single threaded piece
of code and they get a stack dumped back
out ah now they know why
so for me personally I use a lot of the
code generated side of things to do you
know compiler hacks for performance
the only one I did was this is his do i
do client hacks for individual client
machine digital client targets the only
hack i did was prefetching simple
straited loops long-running simplest
righted lose which has broad
applicability somebody walking down a
giant array will prefetch all the way
down based on that we get like like
stream the stream venture give a dollar
version of stream benchmark we get like
eighty five gigabytes a second
sustaining memory bandwidth which puts
us in the top 20 supercomputer list this
is interesting thing and I didn't do any
hack for that one I i did this other
hack for this other customer and oh look
stream got huge that's it no i don't do
any other hacks the hacks are all
generic
it's actually three now so I think we're
about on target here yeah no way go for
it classic fortran code is very very
predictable and java as originally
envisioned was it sort of take the world
as a stew pot and stir that kind of
thing hahaha and I wonder in practice
when people solve your this is sort of a
follow-on when people solved their
performance problems and then change
their systems in some way how dynamic is
the performance behavior if they solve
their performance problem do they come
back you know a week later and say oops
we have a performance problem again or
de fixes tend to be pretty stable um I
think the fixes tend to be stable
because of the the expectation is I mean
when you say about performance problem
it's usually I need to get some
increment more and that's going to be
reasonably stable over time if the
answer is I need to get the most i can
ever get and more is always better then
then I have a different kind of
performance problem right and that's
different beast but if I'm if I'm not
needing my current SLA my service level
objectives of you know ninety five
percent of my transactions complete and
second and I'm not far off probably a
few fixes and i'll be on and then i'll
be there and that'll be it they'll be
done and that will be stable that's one
of the reasons that people like leave
these servers up in the server room
forever once they got it is it's like
you know hands off don't touch and one
of the things that's true for us that's
a little more difficult for other folks
is that we're very stable under load
because we typically have a spare cpu
for every thread that's runnable so as
you pile on threads during peak load
they all get to run and then you don't
have issues where the guy holding a lot
gets context switched out by yet another
runnable thread and there's not enough
cpu to go around and so eventually you
have to context switch back let that guy
finish up and get his lockout and
there's a whole lot of stability and
performance that comes out under load
there but that's that's just the wonder
of having extra cores to throw around
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>