<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Learning and Recognizing Visual Object Categories | Coder Coacher - Coaching Coders</title><meta content="Learning and Recognizing Visual Object Categories - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Learning and Recognizing Visual Object Categories</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/w2C-WffS-AE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay let's get started the speaker today
is dan hunt locker dan is a computer
vision researcher is on the faculty of
the University of Cornell when I'll
University I should know better than
that over there for six years as to a
student in fact dan has also been
researcher manager Xerox PARC and a
software go for engineering manager at
financial financial software company and
actually usually one of the world one of
those wants so he's been in fact when
David Hartman Park researcher and in
fact right now he's got a cross
appointment at quenelle between the
business school and the science faculty
dan has won a number of number of awards
for his teaching and he's especially
very good at getting undergraduates
interested and involved in research one
of the undergraduates who he worked with
this way co-wrote papers with royalty
as an undergraduate was John Feinberg
who some of you may have heard because
it was his research later on when he was
at IBM in the statistical structure of
web links that led to people called a
grinning page to develop some of the
early algorithms so anyway I've done Dan
for getting up to 20 years now I've
actually worked with him half cornell
abbott park and of the financial
software company and very happy to
introduce him at google thanks so i just
got done with a semester teaching in the
business school and and i alternate in
the falls i teach in the business school
in the springs I teach in computer
science and one thing about business
school teaching is it's it's pretty
interactive so i'll expect you all to
stop and ask me questions and if not
i'll start cold calling on people in the
audience that's sort of a business
school tradition start asking questions
dick's already looking at me saying oh
man what questions is Sutton luck are
going to come up with but but I trust in
this audience people will keep me
entertained with questions but my goal
is to get through as few slides as
possible in my talk in general that's
sort of that it's more fun for me that
way so what I wanted to talk about today
is some work we've been doing in my
group on object category recognition and
this is a in the computer vision
research community this is sort of seems
to be the hot topic du jour there are a
lot of people working on this sort of
problem lately so if you look at kind of
the history of object recognition
computer vision it tended to be focused
on very specific instances of objects
like not just recognizing bicycles but
recognizing my particular bicycle and
maybe not even the model of bicycle I
have but the ones with you know the
scratches in the mud in the exact places
that they are on my bicycle and in the
last five years or so there's been a
bunch of work done on recognizing
categories of objects in the computer
vision community and most of these are
and the ones that I'll talk about today
or what I would characterize is being
visual category
that is their categories of object like
a bicycle where there's still quite a
bit of variability in the appearance of
these different bicycles but there's
still what I would say defined visually
now if you start to think about a
category like a chair for example which
is defined more functionally there are
some chairs that look the same visually
but things like this strange egg that
this person is squatting on it's the
functional role that defines it as a
chair and that's going to be beyond the
kinds of algorithmic approaches that
that that that we're taking and then of
course you can get to these very
abstract categories like vehicles where
it becomes much more semantic and so
I'll be talking about things where its
category recognition for these sort of
visually defined categories now within
the task of object recognition and
vision there to sort of complimentary
things that one can do one is usually
referred to as classification which is
determining whether an image has some
particular object category in it or not
and then localization is telling where
those things are in the image and so if
you're thinking about things like
searching images looking you know for
photographs you have that happen to have
cars in them or whatever classification
alone is enough you don't really
necessarily need to localize where those
are in the image on the other hand if
you're thinking of applications where
you actually want to use the image data
to interact with the world or you want
to do photo editing or things like that
then you care about actually being able
to localize the object as well and not
just not just do detection or
classification and i'll be talking
largely about localization kinds of
problems today in terms of imagery
everything that that that we've been
doing in my group for years now has to
do with recognition just from a single
two-dimensional image so we're not using
anything like stereo vision for depth or
temporal information that you would get
from a video sequence and so what I'll
talk about is this localization problem
for visual object categories in a single
two-dimensional image
so when thinking about doing recognition
from visual data their various cues that
you can use for doing recognition and
generally they tend to get divided into
two broad categories in the recognition
community under the names of appearance
in geometry so appearances is patterns
you can think of it like service surface
markings if you yep it's an interesting
question so question is is
classification a necessary precondition
for localization most of the
localization methods will sort of
classify as part of the localization
process but the converse definitely
isn't the case there are classification
methods that have no idea where the
object is in the image but but the
localization methods don't for example
first classify and then try to localize
they don't say somebody told me the
objects in the image now I have to tell
where it is they'll sort of do
simultaneous classification localization
so but they but they do end up
classifying in the process of localizing
and in fact the problem of localizing if
someone told you the object is
definitely in the image is an easier
problem than sort of doing simultaneous
classification log now someone says
here's a picture of the car tell me
where it is rather than here's a picture
what's in it and then you know if
there's a car where is it so but don't
apologize for asking questions that's
good otherwise I'm gonna have to start
asking questions so so in terms of
recognition cues appearance you know I
sort of like to think of Tiger fur is
kind of a canonical example right it's
it's sort of a pattern of intensities
and there's very little in terms of
shape I mean a tiger can change its
shape a lot but from even just a little
patch of fur you recognize it and that's
sort of a canonical example of
appearance whereas geometry is more both
shape based things and often spatial
configurations of shapes or features so
a face is a pretty good canonical
example where you think of the you know
eyes being located above the nose which
is located above the mouth and so forth
and so it's sort of a configuration or
an array of parts and there tend to be
some sort of canonical arrangement to
those parts except in the case of
something like a Picasso painting where
the arrangement somewhat distorted so
and if you look in the vision community
you know these are problems people have
been working on for about 45 years now
which started very much looking at
geometry and then there was a whole sort
of time period looking at appearance and
the stuff all talked about and what's
been going on recently in the community
is trying to use those two cues together
so in using appearance in geometry
there's a sort of that I think a good
illustrative example of some work that's
largely been going on in Pietro piranhas
group at Caltech which they call the
constellation of parts models and the
idea is that they're going to detect
certain kinds of features and the
easiest way to think of these features
is just think of them as corners but
corners where you don't know the angle
in the corner that is they're looking at
things where there's a fine invariants
so if you start looking at the the shape
of things you can't actually preserve
the corner you just know that there's a
corner you don't know if it's 90 degrees
or 45 degrees or what and so they'll
detect these little features and then
they have a Gaussian spatial model that
says given these detected features what
kind of configuration of feature
locations correspond to the object so
it's a two-stage process where you first
detect features and these little dots
here are various features that they've
detected and then you say well which
feature is sort of fit some kind of
Gaussian spatial model and this is a
picture from one of their papers it's
supposed to indicate the Gaussian
spatial model by these little cross
section cross sections this is sort of
like a level set of the location
Gaussian so this says how much
uncertainty is there in the location of
this feature and if you look at pretty
much all the work that's been done an
object recognition on these kind of
generic category recognition problems
they all follow this general form where
your first detecting features and then
given the future
trying to match those two a spatial
model and in contrast the the work that
we've been doing doesn't do any sort of
separate feature detection we're trying
to do this is sort of one overall
optimization problem and and here's a
good illustration of the problem with
feature detection so here is actually a
relatively decent corner detector and an
incredibly simple scene right i mean if
you can't find the corners reliably in
something like this it's hard to imagine
how you're going to do it in real-world
scenes and you can probably you know
even from the back of the room see for
example there's some corners here and
here that are missing there's some extra
corners that have appeared in various
places these corners didn't get
detective and so forth and the issue is
one that sometimes in the in the
computer vision community we call the
aperture problem it's basically when you
don't have enough context a lot of these
corners become very difficult to see so
the contrast here is actually very low
and part of the way you know that
there's a corner here is that you see
these edges converging on each other
from further away and if i zoom in and
just look at a little local patch of
this image like over here this corner
which is pretty evident here is pretty
much gone here because you don't and I'm
still giving you quite a bit of context
here right if I zoomed in even further
you really wouldn't see the corner up
there and so when you're trying to make
these decisions locally about detecting
features using just the information in
some small region you're somehow
throwing away a lot of the information
that's important for doing detection so
the idea is that instead of trying to
first detect features based on local
information and then look at the
configuration of features since this
feature detection process is noisy can
we somehow recast this is a problem
where there's a single overall process
going on so does that any any questions
so far oh we're getting dangerously
close to me asking questions um it'll
get more technical a minute and then
hopefully people will have questions so
so this idea is not a new idea um and in
fact if you look at a picture like that
you should immediately say 1973
right I mean it's sort of this sort of
the cyber view of the world version 1
point 0 back about 30 years ago so there
was a paper published back in the in the
70s by fischler no longer on what they
called pictorial structure models and
the idea was that these local local sort
of regions like in a face things like
the eyes and the nose and the mouth and
the hairline and the the sides of the
face that this notion of these of these
local parts is an important notion and
that those parts have some sort of
pictorial or appearance characteristics
to them and then there's some structural
relations between them but what's
different between this and the approach
that I just described is that they
didn't envision doing a process of
detecting eyes and nose and mouth first
they envisioned doing one overall sort
of process and the way I like to think
about it is say you took this this sort
of abstract face model and you dropped
it down on top of an image and you start
wiggling the pieces of this thing around
and you say where is this thing the
happiest in the image and what makes it
happy is well if any of these local
pictorial pieces are over some piece of
the image that look a lot like say you
know if the eye thing is over something
that looks a lot like an eye it's going
to say hey I'm happy and so it has a
degree of happiness based on how much
the image locally where you drop it down
looks like an eye and then these Springs
want to sort of pull the pieces into
some overall spatial configuration and
if you're stretching the springs a lot
they get unhappy so you can think of
sort of plopping this sort of model down
in an image and wiggling it around until
it gets the happiest and what's going to
make it happier is when you move it
around so that the eyes are on top of
things that look I like in the image and
when you move it around so that these
Springs aren't stretched too much now
that's a very physical sort of
description and the problem is it's
great to you know you can get a nice
sort of intuitive notion of what this
sort of recognition of
things like faces might be without doing
feature detection but that's not a very
computational statement drop the thing
down in the image and wiggle it around
until it looks happy so uh and in fact
this sort of approach pretty much got
got buried in history because of the
fact that that nobody was able to really
come up with reasonable computational
approaches so what we've been doing is
investigating how to take this kind of
view of things where so you can you can
think of feature detection right it's
almost sort of an efficiency hack right
if I process the image first and say
these are all the things that look like
eyes now I know where to start putting
down this model I'll put it down at the
places that look like eyes but if I
somehow could take this sort of model
and efficiently try possibilities
without having to say where their eyes
are not that would be less prone to
error because I'd be able to use the
context instead of trying to find 1i
alone I'd have the context of the other
eyes and the nose and the other things
that are connected to it by Springs if I
tried to solve this as one problem
overall so that's sort of the the basic
idea and so what we've done over the
last four or five years is develop a set
of efficient algorithms that can solve
matching for these pictorial structure
models and their Gaussian spatial models
so they they share with with this work
from piranhas lab that i talked about
mentioned a moment ago the fact that
there's some sort of gaussian model of
the relative positions of the parts of
the object and and you can think of
those as being spring-like models the
algorithms all make use of dynamic
programming techniques to get a lot of
efficiency and I guess if I was going to
stand on one of my soap boxes for a
minute i would say that is computer
science educators one of the places i
feel like we sort of don't do a good job
is teaching people enough about dynamic
programming that you know divide and
conquer is all over the place in a
computer sign
curriculum and yet dynamic programming
to me is almost always the best tool to
use for just about any problem so and I
think it's just because it's harder to
teach dynamic programming so and then
we've also looked at learning these
models from examples and and we actually
get better recognition accuracy on some
standard data sets that are used in the
computer vision community than these
techniques that rely on feature
detection and of course that's what you
would hope since we're using the full
context instead of trying to detect the
features alone so what I wanted to do is
talk a little bit about some of these
techniques talk a little bit about how
we learn these things from examples and
in general talk about the problem a
little bit touch on some issues about
learning these sorts of models in images
so it's very important when you try to
learn models for recognition that you
don't presume to too much supervision in
the training process so there's a
paradigm that that's coming to be used
pretty widely in the computer vision
communities days of a sort of weak
supervision where people give you class
labels for the images so they'll say
this image contains you know a car or
contains a truck or contains a bicycle
but they don't say where it is and
that's sort of labeling is a lot easier
to get labeled data for you know through
a lot of these photo sharing and photo
tagging sites and various things I mean
you can get a fair amount of information
about what images contain but to
actually get someone to painstakingly go
and sort of say you know here's the
wheel on this car and the next we forget
it you're not going to get data like
that so this weekly supervised process
is something that's pretty important so
so let me talk a little bit about the
mathematical definition of the model and
then kind of sketch how the algorithms
work so so we're going to model an
object here using a graphical model
where so we'll think about the parts as
being nodes in a graph will think about
these Springs that connect pairs of
parts as being edges and
for each edge we'll just have a go seein
model of the relative spatial
relationships between those two parts so
if the parts are just allowed to
translate around in the image then I
just have a Gaussian / x + y right which
over the delta x and delta y positions
of the two parts simple two dimensional
Gaussian and that collection of Gaussian
pairwise models gives me an overall
spatial prior on configurations of the
parts so if I sort of drop one of these
models down on a completely blank image
like this where there's you know sort of
no preference from the individual
detectors because nothing here looks
like an eye or looks like era looks like
mouth because it's on a white background
this thing will still reach a sort of
have a resting configuration where it's
the happiest which is the one that's the
highest probability spatial
configuration and you know so that will
specify a separation of the parts from
each other and then the spatial
configuration is taken over
configurations and by configurations we
just mean that there's some sort of set
of parameters that describe the location
of each individual part and in the case
and in a lot of the cases we'll look at
these will be translation rotation and
scale in the image point but the
simplest one to think about is where
it's just translation in the image plane
because it's 2d and it's sort of just
like the image itself and then in these
models so this is one of the seven ages
seven nodes and nine edges and of course
you could build a complete graph here
where you had all the edges all 21
possible edges between pairs and then
you would have one of these sort of full
joint Gaussian models like like have
been used in some other techniques so
you can think about our models in some
senses encoding less information in that
they're not trying to necessarily model
all the spatial relationships and that's
going to be critical to being able to
compute these things efficiently that
the graph structure is going to matter
yeah so that's yeah so that the question
was how do you how do you decide how
much to represent in terms of the shape
and basically the way we do that is the
computational complexity is going to
turn out to depend very directly on the
graph structure for example the graph
structure where a tree you can do this
stuff very efficiently there's certain
other kinds of graph structures where
it's pretty efficient and so usually
what we do is so if you've picked a
topology to the graph then you can
actually build the best say tree
efficiently by finding the sets of parts
that I have the most dependable spatial
relationship with respect to each other
and trying to decide sort of how much
structure to put into the graph there
aren't any good automated techniques for
that because it's sort of this model
selection problem and machine learning
right where it you're trying to sort of
compare two different models that have
different numbers of constraints in them
in some sense the more constraints you
put in the more power of the model has
to represent things it should be until
you get to overfitting so you have so
many zillions of parameters that should
do better and better so what we tend to
do is look for a sort of diminishing
returns where you add more structure and
the performance starts to asymptote but
it's not it's not theoretically as
justified as as the fixed model
yeah so usually what we're going to do
is is is the graph that will choose its
complement is still almost the complete
graph so the graphs that will basically
the graphs that will take have low tree
width and so that still leaves a lot of
edges that that are in the compliment so
so for doing image detection basically
they're sort of tooth so we have this
prior which is the the spatial model the
distribution over spatial configurations
and then we have a likelihood model and
this is sort of a big cheat and it's a
big cheat that everybody pretty much
makes in the computer vision community
except one one person and we actually if
I have time at the end I'll talk about
how we use some of it so as a guy yali
amid at University of Chicago was a
statistician who has actually some nice
techniques for dealing with this cheat
so here the cheat is that we're going to
model the likelihood as a product of
likelihoods for the individual parts so
what we're saying is that the appearance
of the parts is independent of one
another the spatial structure will
encode all of the dependencies between
parts but you know so for example if if
we dropped two parts down on the same
place in the image they're clearly not
independent of each other they're
explaining the same pixels um but
they're even you know subtler effects
like you know if the lighting changes
the parts will change color together so
we're not accounting for differences in
appearance that that would be modeled by
some dependency between the parts so we
can do spatial dependence between the
parts but on the appearance it's
independent and that's basically to get
the problem to factor in a way where we
have a computationally tractable
solution and pretty much everyone doing
these multi-part kind of models in the
computer vision community makes this
cheat y'all iam it has a nice way of
dealing with this for the case of
overlapping parts um which which I may
get a chance to touch on so but given
this sort of model where you you have
the graphical model you have the prior
and likelihood then there are two things
we can look at we can look at the total
evidence overall configure
patience so in some sense you can think
about the configuration variable which
is telling you where all the parts are
as a nuisance variable it's sort of for
each configuration you've got some
probability which is you know how good
does the spring model or the prior say
the thing is and how good does the
appearance model or the likelihood say
things are and then you just sort of
want to integrate out over that variable
and that's the view that you take if
you're trying to do detection without
localization right you just sort of want
to say what's the total evidence in the
image for this thing or the other thing
you can do if you're interested in
localization is you can take the quality
of the best configuration so these are
map estimation kinds of problems and
then the maximizing configuration tells
you where the parts are now the problem
is in sort of viewing these two things
that way where you sort of say that the
total evidence integrating out over this
location variable will be my view if all
I want to do is classify the image as
having a bicycle or not I don't care
about location when I care about
location I want to take the max the
problem is this is a sort of brittle
view for localization because the
maximum probability thing may not be so
great because your models always have
some inaccuracies in them and so much of
what we've done from sort of the
statistical point of view over the last
couple of years is to is to look at this
view of things for solving that problem
by basically do using sampling
techniques where we sample high
probability configurations instead of
taking the Maximizer and though that I
will I think get time to touch on so so
I've been put put up a few equations
here let me try to explain more in
pictures so this slide I want to stay on
until people get because this will sort
of explain the approach so you know
notation may or may not make sense but
if this doesn't make sense to ask
questions so so the first thing that we
do is we we have some image and let's
say we have a very very simple model of
a motorbike which is just a front and a
rear wheel model and some spring that
says how far apart those
things ought to be and and and just for
concreteness here these front and rear
rear models they're just going to be the
probability of an edge at each location
in some little template so the the
contrast from the projector kind of
washed this one out a bit but you can
probably so hopefully see a little bit
of variation in these so that it's sort
of brighter along here because that's
the highest likelihood place for the
edge of the wheel it's pretty dim out
there because the wheels sort of aren't
that big and this is sort of a little
bit fat because there tend to be a bunch
of edges from spokes and other sorts of
things in there and this front wheel
model actually has a fork kind of going
up there there's a higher probability
sort of region there from the fork of
the bike now obviously this is a very
impoverished model just probability of
an edge of each pixel and if we were
going to use edge based models we do
something more intelligent like at least
use the orientation of the edges and so
forth but the things get to
multidimensional to show on a screen so
so so we have those probability maps and
basically i can take this and i can plop
it down on the image and in every
position as i move it around it's going
to have some level of happiness in terms
of you know if i take this thing and I
plop it right down on there there's a
whole bunch of edges right in a circle
this thing is going to be wildly happy
yep it's wildly happy it's really bright
right at that location of the image
whereas if I plop it down at some
location up here it's going to be pretty
unhappy not so happy so what each of
these are these are quality maps that
say how well does this part explain the
image when I locally when I place it
down there and not surprisingly for this
rear wheel these two locations both look
really really good but there are a lot
of other locations actually you know
there's a lot of stuff down here where
their edges that you know turn out to to
look pretty happy to that model and
there's sort of a placement here in the
middle of the bike that looks pretty
good when you place the front we're mock
front wheel model down it's a little
more selective because it also wants to
explain this fork piece that you can't
quite see there but believe me it's
there and so it's much happier here and
it's still pretty happy here in the
middle of the bike as there's some
stuff up like that but on the rear wheel
it's not very happy because there's not
anything that looks very for quite so
the idea is that instead of doing
feature detection right what would a
feature detector do it would basically
threshold this map and say yep here's a
few places that could be rear wheels and
you know here's a couple places that
could be front wheels and you would just
have this binary threshold adan to say
these are the locations where these
things might be and instead what we want
to do is not threshold those maps we
want to use the whole quality map and we
want to combine these things together
directly in some fashion and only do
threshold at the end when we actually
need to make a decision so back you know
a gazillion years ago when I was a
graduate student I used to have that
this is typical MIT randomness so we
yeah
yeah absolutely so so in fact the
featured right so if what a feature
detector would do it's true is it
wouldn't just threshold because you'd
get these blobby regions you do some
neck non maximum suppression like thing
to not only threshold but also take the
peaks of the stuff that we're over the
threshold sure absolutely but but but
and this is a good place to roll up the
sleeves and and and and and and and have
a debate if you want because so we're
taking a very extreme view but it's
actually an extreme view that I'm going
to I'm going to argue for pretty I mean
I'll have to be pretty bloody to get me
to stop arguing for it which is um let
me put it this way no one's got me
bloody enough yet so it's true that you
could that you don't just want to
threshold this because you get these big
blobby regions so non maximum
suppression or some other technique for
picking not only stuff over threshold
but but but a local maximum and then
it's true that you don't have to just
have that output that's the locations
that satisfy that you can actually have
the strength at those locations and so
you can know that this is actually a
better match than that is because it's a
higher peak but still fundamental to
these things is that you're creating
this sparse representation and the
question is why that is for example in
this case it's a kind of drop dead
stupid trivial image and it's pretty
unambiguous that the things that match
well locally are also going to match
well globally but you know if someone
were standing over part of this wheel
for instance then this wouldn't be real
bright here but and so if i did my nice
threshold and non maximum pressure and
so forth i wouldn't find anything there
and yet when i consider these two things
together and i know that there's a
spatial model tying them together then
the overall configuration in fact may
still explain this thing
very well and so but the only way i
would argue that you can get that is not
is to make no decisions at the beginning
because as soon as you start trying to
sort of build some sparse representation
here in the absence of the kind of
context that you get from relating these
parts to one another then you're in
you're in danger of having made the
wrong decision and then it's hard to go
back on it so so so we want to keep sort
of as much data as we had my art my
here's what I would argue is that the
only reason you want to throw stuff out
early is because you think it's too
expensive to keep it around because
otherwise why not and what I'm going to
do is show you how it's not to expense
it that's sort of the that's that's the
sales pitch but but I should let you
punch back now because I gave one long
punish so I know you have
you
okay so so just what dick was saying is
he's been on both sides of this debate
and and it's okay for me to take an
extreme view but but by his tone of
voice he doesn't believe it's really
going to work
well
yes so I do presume I have these part
models and the question is where did
they come from and basically where they
come from is a an incredibly brute force
process where we start with a bunch of
patches that we just sample it random
out of a set of training images that we
know are positive exemplars so they must
contain a motorcycle someplace in this
in this case of motorcycles and then we
look for you know individual patches
that sort of seem to occur a lot and
then we try lots and lots of pairs and
triples of these patches and look at how
will they explain the training data so
it's just sort of there's a very brute
force process that builds kind of an
initial model and then we do an e/m
procedure to sort of improve on that
initial model but it sits there is this
sort of weekly supervised learning
process where someone just said here's a
whole bunch of things that have
motorbikes in them somewhere and then
and then we learn these models from that
but yes we do need the part models too
so so that's why in my talk where I said
without the detection of features I'm a
firm believer in features these local
sort of descriptors are very important
but but what I'm not a believer in is
actually detecting the features
individually I think instead of
detecting them one should say well how
happy you know how happy is that part of
the image to be characterized as being
an instance of that feature and then
given these happiness maps combine those
together in some way and only make
decisions at the end and so so what this
is illustrating is is is is sort of the
first step in combining these maps
together so the problem is that if I
want to take this map in this map and
combine them I have some spatial
uncertainty right it's not like i can
just sort of take this map and and and
and sort of translate it over and plop
it down on another map and multiply the
two together because i have a spring
there it's not like I know exactly where
these two parts are with respect to each
other I just know that they're probably
here and as I move them apart I know how
much less happy I am and so what this is
is it's the it's a transform of this map
that accounts for that
spatial uncertainty and having done this
transform I now can actually take these
two maps and multiply them together
directly and the key thing is that we
can do this transform very efficiently
this transform turns out to be so this
is basically a lower envelope operation
turns out to be a variant of a distance
transform for those of you who are sort
of image processing types who've looked
at distance transforms so we can we can
do this thing really fast and of course
I made the critical mistake of not
taking out of my bag so this is the one
thing about having questions along the
way as I never know how long I've been
talking for so I got a bit like so
because what this is doing if you think
about what's going on with a spring
model what's going on with a spring
model is that as I stretch these things
apart it's getting sake what in fact a
quadratic is a pretty good spring model
right as I'm pulling stuff apart it's
getting quadratically less and less
happy so I don't want to just sort of
blur this thing out in some arbitrary
way I want to actually encode the real
underlying Gaussian spatial model or
spring or spring cost model um it it it
can't in some cases it's a convolution
with the Gaussian but in this case or
what I'm trying to do is actually this
this minimization it isn't actually
convolutional to go see it's it's
basically instead of a normal
convolution which is some product it's
over the min some semi ring instead of
the sum product so basically you take
the the sum in a convolution replace it
with a min they take the product and
replace it with a salt and that's what
that operation actually is it's
sometimes called the min convolution
because it looks like convolution except
with a minute
ah you alone that was such a while I
swear I didn't plant that question this
is a markov random field model and so
this is it's a probabilistic model where
there so it's a graph graphical model
where they're undirected edges so it's
looking at correlations but thing
between things rather than causality
which is the sort of traditional
directed graphical models and and those
are markov random fields so all a markov
random field is is a collection of
random variables where you have
correlational connections between
certain pairs of the random variables
rather than causal connections and you
know the nice thing about sort of
viewing these things so MRFs are just
amazingly powerful I just people you
know one sort of footnote if people if
people don't know about Markov random
fields and if you spend any of your life
looking at models where you have several
random variables where there are
correlations between certain subsets of
the random variables you should go look
at markov random fields they're really a
very nice formalism because the tendency
if you don't know about MRFs is just to
throw this into one gigantic model right
where you look at all possible
correlations between things like build
something like a complete gaussian model
and the MRF swear you just look at
certain subsets of the graph of the
complete graph can can be very useful
and in these graphs reach ability in the
graph basically corresponds to
conditional dependency right so if i
take this node and i take it out then
these notes are all conditionally
independent from one another
yeah yeah actually so we just take so so
the the spatial model is an mrf is a
mark of random field if we just sample
from that we get really good human body
configurations for example in a human
body model or really good face
configurations in a face model and
that's the sort of oh I mean these are
generative models so there are way a way
of sort of testing how good your spatial
model is is to sample from it and if you
start getting things that don't look
like faces for example in your face
model then you understand where there
are weaknesses in the underlying model
so so the human body i think is the
easiest illustration of this so these
are a kinematic so the great thing about
human body is a tree as a natural model
because the the kinematic structure is
is a tree structure the skeleton forms a
tree sewing this sort of mrf thing we
have the parts are these sort of simple
body parts and those are the nodes in
the graph the joints between connected
pairs of body parts of the edges and
we're going to look at the 2d image of a
joint and so the spatial configuration
that will have looking at a 2d images so
say this is a torso and an upper leg
attached to it we'll look at the
relative position orientation and a
scale of these two things so we get for
shortening of the parts so the nice
thing about trees and this is a
factorization that's been known for a
long time is that in a tree this spatial
prior factors very cleanly in flac
factors into a product over the edges or
the connections between the pairs of
parts and a product over the individual
components and in fact because in our
spatial models we're only modeling
relative location and not absolute
location this denominator disappears
because we're not saying that it's you
know more likely that the leg be here in
the image than here in the image what
we're saying is it's more likely that
the leg be here with respect to the
torso so it's only the pairwise
components
matter and in this sort of a tree
structure you can sort of directly apply
that the traditional Viterbi algorithm
where instead of recursing on the length
of a chain like you do inverter BIA
recurse on the depth of the tree and
that's again something that's been known
for a long time the problem is in this
sort of world of human body models it's
not all that practical to apply butter
be here because the great thing about
Viterbi it's linear in the number of
elements but that's just a number of
nodes in the tree here and it's
quadratic in the product of the state
spaces avella it's quadratic in the
state space size of the two of elements
right so there's a sort of trellis in
the Viterbi algorithm you get something
quadratic in the state state set size
and the problem is that our state set
here is locations / part these are
position orientation and scale so
there's maybe five or ten million of
them per part and five or ten million
squared is just not a very happy number
when you're trying to do things that say
video rates so so this is a place where
there are a bunch of fast methods that
you can use because we're only concerned
about relative locations instead of
really modeling the cross product of
locations of pairs of parts all you
really care about is that's just
proportional to some function of the
difference between them and so as Dan
was suggesting you can use convolution
or FFT like operations in fact because
these things are Gaussian you can do
something better which is binomial
filtering which which is doesn't have
the log and doesn't have some of FFTs
annoying sort of boundary condition
things for finding the best match or the
map estimate there these new methods for
doing linear time lower envelope
calculations or min convolutions that
we've developed over the last few years
and that's the sort of picture I showed
him as the example which is which is one
of these may not have been min or lower
min convolution or lower envelope
calculations
so so in doing the best match and I
wasn't going to talk about the
algorithmic details any more than that
because their papers on that stuff so if
someone's interested I can point you
with those so when doing the best match
right we're considering all possible
spatial configurations in an implicit
sense right but most of them are being
eliminated because it's a dynamic
programming solution right it's like
Viterbi is a dynamic programming
algorithm there's a exponentially large
space but most things don't have to be
considered but the problem with finding
the best match which is what Viterbi is
doing for us is that these models are
very impoverished and I think you know
in general any models that you're going
to have for interpreting image data or
you know many other kinds of noisy data
the model is never going to be right and
so as soon as you have model error
finding the best match is a very
dangerous thing to do what you really
want to do is find some sort of
population of fairly good matches and
try to be able to consider those in a
reasonable way so here's a case where
the min-cost match is sort of clearly a
little strange this is a very simple
appearance model which is a binary
silhouette just as an example and it's
perfectly happy to place this arm down
here instead of placing it up overhead
because this stuff looks as armed like
as that stuff up there does and this
model is just the models just trying to
explain the image data underneath it
it's not trying to explain the whole
image right because there might be other
stuff in the background here that bit so
there's sort of no way in this kind of
model to really get it to prefer this
sort of thing except maybe in our
spatial prior to have lots of pictures
of people standing like this right the
reason it preferred this is because this
is closer to a sort of default configure
it you know we we took a bunch of
pictures of people to build that that
spatial model that so this spatial model
here was actually built from a bunch of
training images of just people sort of
standing in different kind of standing
poses and that was sort of the average
image and so when you apply that sort of
model to something like this having this
arm down just looks
better than than having it up so the
answer in this kind of formalism to that
kind of problem is to that's interesting
so there's some disappearing stuff yes
powerpoint so so that the way we deal
with this problem is actually to sample
is to is to look at this sort of total
evidence view where you're viewing
configuration is a nuisance parameter to
integrate out so we actually from the
statistical or Markov random field model
sense we compute the the actual a
posterior distribution because of the
fact that it's factored into this simple
product we can actually compute the
thing and then we can sample from the
posterior distribution so this first is
just this same example and these are a
bunch of samples that are high posterior
probability now if in fact I just had no
input image then it would be back to
your question before about if we just
sort of sampled from the prior with
these things look look human-like and
then you'd sort of get people standing
like this but these are samples from the
posterior where we are taking this
appearance information into account and
you know here's the thing that was a map
estimate that's high probability it gets
sampled but here are some other high
probability samples and this one
actually looks a whole lot a whole lot
better so one of the things that that
that that we've been doing quite a bit
of in these models is sampling and in
fact there's some work that that came
out of Dave Forsyth who's at university
of illinois now and andrew's the sermon
at Oxford together with a student of
theirs David Ava raman on where they
used our algorithmic techniques for
doing the markov random field modeling
and sampling from the probability
distribution and they put a lot of more
effort into developing good appearance
models for the parts right this is a
pretty cruddy appearance model it's just
a binary silhouette and they have some
really impressive if you look at a devil
ramens website he has like this
minute video of Michelle Kwan in a
championship in her championship skating
appearance and it tracks all the body
parts through the whole nine minute
video it's really quite amazing and so
these algorithms are now being used by
stuff I didn't do so i can say that it's
the best but but being used by
techniques that really do a good job of
you know pretty low resolution video
stuff from sporting events where you
know you don't you don't have great
great resolution in the video so i don't
know if dick if this starts to answer
the you know does this stuff work or not
but so it's not that we get it's it's
it's not that we get away from having
the prior so the prior is part of our
model and it's basically intuitively
this is sort of the idea it's dead we
want to use the prior to bias the set of
solutions right that's what priors are
supposed to do um and the problem is
when you come when you do map estimation
right just sort of compute the best
match your you're picking one solution
that maximizes the posterior
distribution right you're taking the
prior into account in forming a
posterior you're picking a Maximizer and
instead what we do is we sample so so
you can think of that whole posterior
distribution for every configuration it
says how happy you know what's the
what's the posterior probability and we
just sample a bunch of posterior high
posterior probability configurations yep
you could be M best and you do some sort
of smart averaging it can be that you
have some other information that allows
you to select among those things with a
hypothesizing test kind of paradigm in a
bunch of what we've been doing lately we
basically do is sort of simple
hypothesizing we use this as I patha
size or do some very simple testing
the end um but but the the the the key
thing in my mind that actually makes
this stuff work is that is that it's not
based you know in these kinds of videos
if you tried to actually you know detect
where her legs and stuff were on every
frame and then imposed the spatial model
after the fact that the results wouldn't
be very good that's right it's basically
what you get is more robustness by
making decisions at the end yeah so the
prior model comes from some you know
this is the perennial machine learning
problem somebody gives me a set of
positive exemplars and I learn a model
from it and if it's a good set for my
test cases then I get a pretty good
model and
right so right so one thing that we have
done with with with we've looked quite a
bit of human motion and and for the
human motion data we have developed some
models with with mocap one of the things
about human motion is that you you
really want to augment these three
models a little bit so I mean the thing
about human motion is that you have to
maintain balance and in maintaining
balance there's essentially what balance
is essentially from a geometric point of
view with some sort of symmetry right
around the center of gravity so the tree
model only accounts for the kinematic
constraints and so what we actually end
up doing for the human body model is
that you know the nice thing about
graphical models is you can have nodes
in the graph that don't correspond to
actual parts in the real world so we
have latent variable in this model that
essentially is measuring the degree to
which the model is balanced and that
actually helps a lot for for these kinds
of things where balance is really
critical but again you learn that
balance parameter from a bunch of
training data so probably I should more
or less wrap up um so um so let me just
sort of and then I can take take some
questions offline so what we've been
working on is doing detection of
localization without doing the separate
detection of individual features and
although I didn't actually get to it in
the talk but there are papers on this
stuff so but for a bunch of common data
sets that people are using in the vision
community these days we actually get
more accurate detection of these kinds
of generic models like people and
bicycles and so forth then the best
techniques that do localization and use
feature detection as a separate step and
then the spatial structure stuff I
didn't actually get to talk about so
I'll just briefly
Shin here as I started to before I think
in response to Craig's question so the
tree model is a place where you can
naturally apply the Viterbi algorithm
but it it doesn't have a whole lot of
constraint in it and there's another
sort of family of models that we've been
working with where you can apply a sort
of variant of Viterbi and in this family
of models so a star graph is just a
depth one tree right you've got the root
and you've got all the nodes that are
one away from the root and there's a way
to generalize a star graph which is
instead of having one note at the root
you can have two nodes at the root that
are connected to all of the nodes so now
you no longer have a tree you have
something where you have a pair of nodes
connected to every node or you could
have a triple of nodes that are
connected to every node and of course as
you increase the size of that set
eventually you get up to the complete
graph and that family which are which
are called k fans because they're sort
of their fans or star graphs where
you're building bigger and bigger clicks
in the middle that family turns out to
be a family where we can sort of
naturally look at these kinds of
questions about how much spatial
constraint you want to put in the model
so just in terms of where things are
going on these kinds of problems both in
what we're doing in our group and what's
going on in the computer vision
community more generally the vision
community really sort of took some data
sets and beat them to death past death
so one direction in which things are
going in the vision community is things
with larger and larger numbers of object
categories in them but the direction
that we've been taking in our work is
actually not so much going to bigger and
bigger sets of categories you know sort
of staying with tens of categories but
actually looking at much more
complicated scenes so rather than
pictures where you know there's a
motorbike filling eighty percent of the
picture dealing with scenes where there
are lots of different objects in them
and much more view variation than in
in the kind of example image from the
motorbike and once you start looking at
those more complex images there's a lot
of opportunity to use contextual
information at the scene level right so
everything I've been talking about about
this pictorial structures approach to
recognition is using context instead of
detecting the features individually you
use the contextual information at the
object level and use that to help you
you can also use contextual information
at the seam level if you found two cars
that are sort of lined up with each
other than other car like things that
are lined up with that are much much
much more likely to be a car because
cars tend to drive on roadways the same
thing with people so as you start
looking at more complicated scenes you
can again take this whole thing one
level higher up instead of detecting in
the cars you can have a car model and
you can say how car like is each part of
the image and then you can use
constraints at the same level to say
there's a bunch of car like stuff in
some organization you have a prior model
on the scene and that that makes it more
likely that you'll detect the individual
cars so and in general in the
recognition community right now there's
a lot more work going on I'm starting to
use scene context and not just doing
detection of individual objects so
that's that's pretty much what I had to
say if they're other it's been good
we've gotten questions along the way but
dick must have a cluster
how about four
funnily process where you kind of narrow
down your options on the way up and
rebirth
why is that bad so that's a good
question so questions why how could this
possibly be faster you know both
biologically and from an algorithmic
perspective you want to sort of narrow
things down and have sort of top-down
and bottom-up kinds of constraints so
here's how I think about what we're
doing here basically what we're doing is
we're using the top down information in
constructing essentially richer and
richer versions of these maps so when I
go back to this sort of picture and I
sort of construct this compound map that
is the appearance for the rear wheel but
taking the position information for the
front wheel into account this map is
used top-down information and
constructing it use the bottom of
information from here and use the top
down information from the spatial model
to build that so so basically what what
we're doing is we're bringing top-down
and bottom-up information together in
this process and the fact that um and
essentially the detection of individual
features I would argue in fact is the
wrong wave or a bad way of winnowing
things down because it tends to end you
up with a combined of problem and that's
why these things end up taking longer I
think if you do feature detection
because if you if you detect the
individual features but you know you're
making mistakes so you know you're going
to have extra features that you can't
account for and you know you're going to
have missing features that you're gonna
have to sort of go back and try to look
for you now have this nasty
combinatorial problem because you're
essentially doing subset matching
because it's a subset of the image
features because some of them might be
extra and it's a subset of the model
features because some of them might be
missing and those kind of combinatorial
subset matching problems are just
unpleasant
whereas if you stay in this sort of
representation this kind of dense
representation where you are definitely
when this representation is much more
window down than that one because it
took the spatial constraint into account
but it just we don't win oh it down in
the sense of the data structures we
winnow it down in a sense of the
information that's present in the same
data structure so I would argue what
we're doing actually is so I think
you're right about the way these things
happen and it's just it you know too
many of us have too much computer
science training where we think ah
combinatorial algorithm good thing and
the problem is when it turns into a
subset matching problem it's not a good
thing you are better off dealing with
lots of pickles a lots of subsets that's
a very good way of that's a better way
of summarizing it then so anything else
great
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>