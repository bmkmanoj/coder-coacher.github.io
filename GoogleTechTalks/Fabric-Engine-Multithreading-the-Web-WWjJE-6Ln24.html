<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Fabric Engine: Multithreading the Web | Coder Coacher - Coaching Coders</title><meta content="Fabric Engine: Multithreading the Web - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Fabric Engine: Multithreading the Web</b></h2><h5 class="post__date">2011-07-07</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/WWjJE-6Ln24" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">hello people my name's Philip Taylor I
work for a company called fabric
technologies and we're building a web
technology for multi-threading web
applications and this is Peter's a lead
engineer so before we talk about
anything we're working on we want to
sort of provide some background and
explain so the motivations and sort of
what part of a what brought us to start
working on technology for the for our
web applications where we typically work
in a high-performance software and what
we've been aware of is the changing
hardware trends over time what were the
different CP manufacturers of producing
each year has been changing
significantly and especially in the last
three to five years so here's a
comparison of two different CPUs the
Intel Pentium 4 which was released 2009
ran out
3.8 gigahertz and it was a very complex
CPU very fast ran at a very high
frequency but next to it is the Intel
Core i7 processor which was released
fairly recently and runs at roughly the
same frequency actually runs at a
slightly slower frequency the key
distinction here is that the core i7
processor has 4 cores built into it and
it's it's a process of built around a
different paradigm instead of trying to
speed up make one processor core go as
fast as possible they've introduced
multiple cores and actually they're
running the cores that are slightly
slower frequency but instead by
utilizing all of these different cores
we get higher performance overall so
this is going stiffin forward a bit
further just recently this this
processor was announced by Intel it's a
it's called Knights corner it's 50 core
isn't it and I don't actually know how
fast what frequency it runs at but this
is sort of a peek at the future like
let's look forward 5 to 10 years and
that this could be what we're looking at
in terms of a consumer-grade processor
this the sort of transition is what
we've seen actually happen over the last
ten years from sort of more basic CPUs
that
using consumer devices and this graphs
this is taken from an article in dr.
Dobbs Journal
a guy called herbs I wrote this article
and this graph sort of plots the changes
in harbor over time I mean it's got the
Pentium 4 processor mentioned there and
it's contrasting different different
properties of a CPU architecture one of
them is the frequency and we can see the
frequency is that dark blue line and for
for a decade the decades prior to 2005
we sir pretty consistent increase in
frequency of CPU processors but things
have changed over the time
there's due to physical constraints
actually increasing the processor
frequency just isn't isn't the solution
to the speeding up of processors in the
future and instead what we've seen is
actually leveling off CPU frequencies
and and conversely the other green the
other one is important to notice is the
green line and this shows actually what
is it number 10 this will be the only
thing which is remain constant is of
course more expensive silicon and as you
can see Moore's law continues to be true
to this day but if you look at the pink
line at the bottom of the graph which is
the actual numerical performance of
these processors it's pretty much
leveled out and I think this graph when
does it stop at their 2005 or we'd like
basically the last ten years the America
performance on a single core has
basically remained constant and I think
what we're looking at this graph and
saying well what happens if we
extrapolate this graph forward a few
more years what is this Delta between
sort of the gain the performance gained
by a single core versus the performance
gained by multiple cores and actually as
we can expect the Delta between sort of
the the power we can extract from the
multi-core processor is going to be
significantly more
utilize all of the cores available
and there's been some other development
happening recently as well so this is a
shot of the GeForce GTX to do and what I
wanted to show here was sort of a
contrast training okay we've been
looking at Intel CPUs and here is a
NVIDIA GPU and what we can see there's
some some numbers it's got 240
processing cores and as we know these
are different kind of course but there's
still lots of them this is a massively
data parallel processing engine it's
it's capable of processing vast
quantities of data very quickly and it's
just another computational resource that
we have in consumer grade machines today
one of the things which is really
interesting about working with GPU
processors which I'm sure many of you
have here before is the GPU processors
there are a tremendous number of
processing cores but the cores actually
are not running at a clock speed which
is very comparable with the CPU on your
machine which makes a lot of sense but
one of the things that implies is that
really if you want to be using these
processors in any meaningful way you
really have to be able to split up your
problem into lots and lots and lots of
things that you can do in parallel as
opposed to having single tasks we know
that of course one other thing here is
that as you're aware is that the actual
memory architectures of these chips are
fairly sophisticated and it's a
programming model that a lot of
programmers are not even used to working
with and this is a graph pulled off a
blog comparing the floating-point
capabilities of the Intel chips versus
the NVIDIA GPUs and the ATI GPUs now
this is the kind of graph that Nvidia
and ATI love to see because it makes
their processing capabilities look
extremely even impressive it's a
different form of processing capability
as known as general purposes there as
their CPUs but what it shows is that to
really leverage all of the computational
power and a machine you really have to
be able to take advantage of the GPU the
GPU has evolved well beyond just a
graphics processing unit and that's just
another compute resource and so the
to heterogeneous computing comes in
where you actually have to divide your
tasks into ones that can be solved on a
GPU versus one step need to be solved in
CPU and it's kind of a another step
sideways so there's lots of changes
hardware changes happening in hardware
and one of the more impressive changes
between happening of the last year is
the mobile chipset front what we're
looking at here is some some changes in
chip architecture of the last few years
as they've packed more and more cores
into a smaller chip here we have a
dual-core cortex a9 this is running to
iron chips but it's also got a GeForce
GPU on it so it's not running an NVIDIA
GeForce chip this GeForce chip can run
OpenCL code and it's a fully fledged
graphics processor and this is the kind
of capabilities we didn't see anything
except for sort of fairly high end
gaming machines five years ago so what
we're seeing is that while we're
noticing these big trends happening on
the main sort of processing architecture
from the mobile chipsets are catching up
really fast almost every single well all
of the nature mobile chip manufacturers
have announced quad-core chips chip
designs and these will be coming on the
market towards the end of this this year
now at the same time we've got the
markets are changing in terms of the
software seven or eight years ago all
the developer had to the think about was
building software that would run on an
x86 chip on a single core on Windows
operating system and now we're done they
could basically target a huge market
just like that but things have changed
we've got operating systems such as OS X
have become much more prominent they had
a huge gain in market share there's a
new operating system such as Android
coming on the market which have their
own peculiarities and cue necks and and
iOS so first software developer not only
is the hardware changing becoming as a
huge technical challenge to handle it's
also the software platforms and actually
making software that can run on a
variety of there's another technical
challenge which developers today are
facing so this is basically leaving
leading to an overall increased
complexity in software development this
complexity really relates to people
trying to build high-performance
software anything which really
effectively uses the hardware
capabilities we've got many core
architectures coming out and people need
to be able to write software which takes
advantage of these cores we've got
different kinds of cores in the same
same and devices and GPUs and CPUs and
this is leading to developers actually
having to work with multiple languages
and things like OpenCL CUDA C++ Java
there's there's just a lot of different
tools that people actually need to be
very competent with to be able to access
all of this these capabilities and the
hard way Frank fragmentation is a
serious problem
things like ARM chips versus x86 chips
the same same machine code who actually
won't run under two different processors
and then the software fragmentation as
soon as you're accessing operating
system services or using operating
system services then you actually have
to start paying attention to that
operating system you're running on and
this is a massive challenge for
developers too so we see actually the
the work that a developer has to do in
the next few years to actually leverage
all of this hardware is extremely
complex and much more complex and what
they had to do six or seven years ago
okay now we're going to talk about web
technologies more now and web
technologies have some massive benefits
over native code applications and one of
the reasons is because they use platform
neutral technologies throughout web web
content can run on pretty much any
device which is capable of rendering
HTML and but it sort of comes at a
fairly heavy performance price and
that's what we want to talk about now so
we did a lot of research just looking
around instead of data and statistics
trying to find content that would
compliment is taught and this is a graph
which plots over time the speed of
various scripting engines and what's
nice is that the graph starts way back
at 2007 and we could see the red line
there actually represents ActionScript
so it's a proprietary language developed
by Adobe and all these other bars are
actually representing the speeds of
various scripting engines and what we
can see is a massive arms race they'd
mostly enough by Google but it's
actually pushed a lot of the developers
and vendors to really speed up their
JavaScript engine significantly this
this particular test does show chrome
way up front which is good but we're not
going to go into that little discussion
that they've sort of a takeaway point
here is that we have seen massive
advances in the processing speed of
JavaScript engines but this is sort of
the downer slide like this is this is
kind of kind of comparing although we've
seen huge performance increases in
JavaScript when we start comparing it to
native code it's still pales in
comparison so this is actually a gret
this is the same test I can give you the
link to that article if you want to have
a look at it but basically comparing a
variety of different JavaScript engines
versus native plan
plug-in architecture developed by Google
and but is running native code in the
browser
so the top seven bars represent various
scripting engines and the fastest one in
this this chart is actually the chrome
11 v8 engine and then we've got the low
three bars which is basically the native
client running the same code written I
think and C and on Linux windows and OS
X and anyway you can see we're not gonna
go into the numbers too much but there's
still a massive Delta between the two so
in the web community today there's sort
of there's a lot of optimism around
javascript and web technologies and
because we've seen so much advanced
advances in the performance of
JavaScript I think some people are still
expecting that eventually the gap
between native code in JavaScript will
disappear completely and what we're
saying is well it has sped up a lot but
the gap is still there and in some cases
it will speed up to this close enough to
native code that people were in mind but
in many cases will be well behind and
the gap today is still significant
maybe I'll talk specifically to the
slide so if we actually think about
what's happening in for instance a
JavaScript runtime my JavaScript
runtimes are extremely sophisticated
this is a lot of this I think is thanks
to Google's v8 effort and other
scripting other JavaScript buttons such
as Firefox those have been playing
catch-up not doing a really good job
with it as well obviously what's going
on behind the scenes to make it possible
to achieve these increases in runtimes
are a lot of static and dynamic analysis
within the engine itself and in cases
where you're actually able to see that
for a certain part of the code the type
structure doesn't change you're actually
able to just compile it to fix native
code which you can just execute directly
on the processor that works really
really great especially for things like
isolated unit tests for you we know that
there's no complexity to thing but one
of the problems with javascript is
javascript has a weak type system if you
write your code perfectly then it can
compile it's a really tight native code
but if you make any mistakes and doing
it it can suddenly fall back on weak
type application because we all know
basically just means the code is being
interpreted at that point the in
comparison of course we all know that a
compiled language like C or C++ we know
absolutely everything exactly how memory
is going to be used we can optimize it
specifically for for the entire program
for instance compile it directly to
native code and this is why we're seeing
the performance numbers which we saw on
the last slide there's some other things
which sort of add to the complexity of
the situation in a scripting language
like JavaScript you don't have very much
control over the actual memory
architecture for this it's a way that
the memory is the data is laid out in
memory you can for instance have
problems where because it uses a much
more complicated structure to represent
arrays or something like that you lose
the performance that your caches on your
processors would otherwise give you
things like that and there's just very
little control from the scripting
language of that kind of thing and just
talking about the concept that if you if
you were to write very clean JavaScript
code and in theory you're the engine
could actually analyze this code and
produce it extremely efficient justed
code of
the machine code but the the the the
performance metric we looking at before
was a quick so it's not like it's not a
sloppily written test it wasn't written
it's not your sort of run-of-the-mill
JavaScript you find online this stuff
should be really tightly written so this
kind of should be actually an ideal case
for the optimizer to produce the fastest
code possible many real-world tests
would expect much worse book and Delta
between the two languages this is kind
of summarize the position we're at we've
seen lots of increases in the
performance of JavaScript over the last
few years this is great this is good
everybody this is good for web
development it's expanded the
capabilities with browser to build much
more sophisticated web applications it's
still playing catch-up on single
threaded native code and you know
there's debate with maybe it will get
catch-up completely but at the same time
we're seeing if you're writing native
code today then you should be writing a
multi-threaded native code if you really
care about performance so the Delta even
between native code and multi-threaded
native code is actually pretty pretty
pretty strong and this is sort of where
in the industries that we've been
working in official fix and video games
yeah we've been realizing you must take
advantage of multicores
to create a distinctive advantage and on
the right guy here the red graph the red
bar which is supposedly the best one
will have been told never to use red for
a good thing is code which uses the
vectorization vectorized code which runs
on GPUs or these yeah if you can
actually get your problems to be solved
on a GPU you've got a massive
performance increase again and I do look
the best select scenario is actually
where you using both multi-threaded CPU
code and using vectorized code whenever
possible
so now we're kind of talk a little bit
more about well we've talked about the
hardware ever talk about JavaScript of
that there how do you take advantage of
these new architectures what is it what
are the best practices in terms of
making the best use of the hardware
available so there's a concept called
task level parallelism and this is your
sort of person structure he method to
utilizing multi-core architecture what
you do is you take your program you
broken break into pieces and try to
solve each piece in a different core you
try to isolate your task so that this is
not too much
inter dependencies between them ideally
to erode inter dependencies and then you
can basically solve these tasks and
parallel then you do that a few more
times and if you can spread if you can
break your program down on interest
enough tasks and you can actually
distribute these tests across all your
cores there's a few challenges to this
approach one of them is not all problems
can be easily broken down the
granularity to which you can break down
your application defines the limit to
the number of cores you can actually
utilize at any one time which is that's
a heavy problem and often the longest
running task you have will end up
becoming the bottleneck of your
application there's another data level
parallelism which is where you take one
long large piece of data and you divide
it you you have an operation if you want
to do on this data and you divide the
data to pieces and you actually allocate
the different pieces to different cores
and this actually scales very well for
scales to many many core architectures
because usually you're working on an
updated edit you can slice into hundreds
of pieces and allocate them to different
course and this opposed to be the way
that the GPU excuse code so the task
level parallelism is really great and
it's a very it's a good first step in
terms of multi-threading
it really only goes so far and then dead
level parallelism is a way to scale
across many many cores
it also has a drawback that it tends
limiting the kind of problems you can
solve with them not all problems can be
solved using dem level parallelism lots
of problems don't involve large datasets
lots of problems are just actually
better you better solved single core for
months so we will talk about high
performance software there's a few
things that we can sort of conclusions
from the previous slides high
performance software really must run
native code there can't be a an
intermediary layer between your
exceeding code and the actual chip this
intermediate layer does have a pretty
heavy performance price and we've seen
from the numbers of this it's not
insignificant it really must utilize the
multi-core CPUs if you've got a quad
core chip and runni running on one core
that's a 75% wastage of the CPU cycles
and we extrapolate that fraction of
wastage will get worse and worse over
time you must utilize the GPUs if the
GPU is going to represent a huge
advantage in terms of computation well
it's something that not utilizing it
you're just leaving a lot of processing
on the table and due to the fact that we
have such rapid advances and hardware
technology today you really have to be
able to build a system which can can
adapt to the changing hardware this is
if you're building a software today and
you want it to be running for the next 5
years you can't assume the structure of
the CPU is going to be running on so you
have to be able to build something which
which will is adaptive you can't hard
code it to any sort of particular CPU
architecture or GPU architecture and of
course due to the fragmentation in the
software platforms you really have to be
able to
feel something's cross-play this is a
huge challenge on its own so there's
other aspects here we leveraged me in
Internet today seems like a no-brainer
there's massive amounts of data and
computation resources available online
and if you're building something that is
pushing the limits of performance then
you must consider using these extra
resources and this kind of list defines
a very technically challenging problem
set and there isn't many companies in
the world who have engineers capable of
solving these problems and this is what
spurred us to start building fabric the
fabric is really about high performance
web applications when we say high
performance we need high performance in
the raw form of the gems we don't mean
relative to JavaScript performance we
don't mean relative to what you can get
today you mean very high performance if
you're building a desktop application
what would be the performance benchmarks
you'd expect to hit and if we can get
the same benchmarks in a web application
we have truly high performance web
applications now I'm going to spin a bit
more time selling why would you even
care we can browse the internet we can
look up Wikipedia what else do we need
to do in a web browser well if we think
forward then the web browser can become
more than just a tall the viewing
content it can actually become a way for
accessing applications online and
there's a multitude of uses things like
you've got a web browser usually
camera and so things like augmented
reality become it could become a part of
your web webpage viewing sports online
we've got things like the Kinect device
imagine if you could connect your
Microsoft Kinect device and actually
throw a web application be accessing
that and building websites which take
advantage of these cool new devices
content creation is a big big area which
is always using the maximum amount of
CPU power available from things like
video editing 3d animation and motion
capture I'll be showing some demos
around content creation in the well
isn't there heavyweight just off
occasions which you use to do things
like engineering or fluid flow analysis
yeah but they're also having the same
challenges as everybody else they're
trying to use that all the core CPUs and
if we can use the same power image in
the browser there's actually a lot of
secondary benefits to building a web
application things like collaborative
work plus where multiple people could
connect to work with the same data at
the same time and then this health
and Sciences where the abilities for
multiple people and different locations
actually work with the same piece of
data is super important web applications
that can be used to communicate things
like an MRI scan information to patients
in remote locations or even with doctors
and other hospitals
web technologies have a lot to offer
everybody when it comes to building
applications today and of course in
business we think of happenings like a
financial analytics software share
prices so enough of that we're going to
show a bit of a demo now on what we've
what we're up to before we start doing
the demo I want to say with the stage
we're at we've been developing a fabric
for about a year and a half
we're at the alpha stage now so we've
got a first customers coming on board
what we're showing you is just a
snapshot in time we're working pretty
hard on it every day so in a month we're
gonna have cooler demos and things keep
progressing very quickly
so the first time I'm going to show is
actually um something called an n body
simulation and the reason we we picked
this because without an academic test
it's a sort of a stress test your your
GPU so what this is this is actually
utilizing your GPU to calculate your
galaxies forming over time every single
point in this in this large
constellation is actually exerting a
gravitational pull and every other point
and it's an interesting problem to look
at it because it's numerically with it
the algorithm is super simple but it's
numerically an incredibly heavy weight
and it's also an algorithm which is
solved by lots of people on using
different platforms so it provides a
good benchmark placed up there so what
we have is a true web application it's
using web tools and everything for for
the interface we have a render region
here just a second and this is we can
interact with the scene and we can
actually zoom through it these controls
here although I can't see so well
there's some parameters there for the
size of the of each of the stars but
it's the same content that would
normally be you would normally build a
desktop demo app for you'd use a Goliath
UI and you actually knock it up but
we've provided tools that you can
actually build this demo and run it in a
browser and then and then actually
distribute it to infinitely none for the
numbers of people all they have to do is
install the plugin yeah and actually we
based this demo on an on a demo from
Nvidia so it's the sort of a this is
very similar we actually just made sure
that we used the web technology for the
UI but apart from that most of the code
is exactly the same
in a short video now I could have showed
you this demo running on this machine
but I've got a nice quad-core machine at
home so I wanted to show it running on a
more high performance machine and you
should have a video recording software
that I'm using which isn't obviously the
best we got a slightly jerky playback
but it is actually running super quickly
it runs at about 40 frames per second on
my machine without video recording
software it's a basically a flocking
demo so all of these particles are
looking at neighboring particles and
avoiding collisions and things like this
on my quad-core machine I get a very
healthy CP utilization
there's acceleration structures being
used to make sure we optimize all the
lookups but if we look closely those
particles just pause this for a second
if you look closely at those particles
they're not actually they're avoiding
Caleta colliding with each other and
they're doing sort of the basic flocking
rules it was a simple test to try it's
it's just a first that's the beginnings
of a crowd simulator and there's a crowd
simulator it could be used to say things
like simulate the performance of
buildings or large-scale
architectural projects where they want
to see well what happens when we put
11,000 people walking through a building
how how much congestion we get certain
places and these are real-world problems
that people are working on today so real
world statistics is that in a
traditional single threaded application
wouldn't run at a fraction at this speed
even if it's written really efficiently
and with the numbers what we're getting
11,000 agents running at 40 frames per
second without video recording going
that's actually the same there's good
performance you expect from a
handwritten well-written desktop
application there's no performance
difference at all but the same
application will run on on Windows Linux
and OS X and even Android without
actually needing to change anything as
we go forward I'm just sort of
highlighting here the fact that it's
we're using web controls this is using
we use jQuery just to layout the user
interface these buttons are just jQuery
buttons and we sort of use standard CSS
just to build a UI so HTML provides a
really good
really rich toolset for building user
interfaces and JavaScript provides a
really rich language for doing event
handling and that's what we do use those
web technologies as much as possible I'm
just putting a breakpoint in this file
and just refreshing the page because I
want to show you guys that it really is
doing things on load it's not actually a
pre-built demo as that as the page is
loading and a dependency graph is being
built and we actually doing a runtime
compilation the compile happens onload
rather than at execution time it's a
compiled onload so we are actually front
loading the work in the same way when
you compile an offline application
you're actually pre processing code as
much as possible so execution time
actually runs as fast as possible
press PLAY because the source code being
pulled in here called flocking KL and I
just includes the code that actually
runs when there for it on each particle
in the flock and as I press play
basically the use of that file got
pulled in compile and now it's actually
part of our graph and this running
just a second I don't have a lot of
space up here okay so this is a video I
can show it fullscreen okay so this is
actually before I get into this our
background is physical fixing games so
we just decided to do some quick tests
with motion capture so motion capture is
a process where you have these a roomful
of cameras there's a really large room
and these cameras capture ultraviolet
light
you put these actors in the room and you
put them light dots on them and as they
move around you capture all of this
motion so you're capturing large
quantities of data very high resolution
cameras running at very high frequencies
and all of us data is correlated
together to try and triangulate the
position of each of these dots once all
these dots triangulated you still have a
lot more work to do and that's sort of
where we're coming in so we're working
with Viacom we run a server it's a
Windows server and basically it's
running a live set all of us data is
being captured they're triangulating the
dots and then they actually project
transmitting that solved data across the
network stream and we're actually
connecting directly to live network
stream and processing it further so what
we're looking at is the vikon software
it's called blade we can see the actors
they're doing their thing now we're back
on our Linux machine so the Linux
machine is connected to the Windows
machine and we're actually receiving the
dots and we're at this point we're just
doing a very basic display of the dots
this was this is our first test I have a
video that we I wasn't able to show
today but it shows from this point we
take the the dots and we actually
triangulate bone transforms we
triangulate transfers
burn transforms for all the bones in the
body and then from there we actually
drive a full connector rig including
inverse kinematics and and and other
tools and from that we actually deform a
mesh so the architecture of fabric means
that the developer who built this
doesn't know a whole lot about
multi-threading he didn't actually have
to know anything about multi-threading
the way he structured it fabric takes
care of multi-threading it's actually a
very efficient client for the vikon data
stream and it's also super flexible
because it runs in a browser so they can
actually connect to their vikon stream
anywhere from any device at all
and and the system is efficient enough
that we actually expect to get fully
rigged characters with hair and anything
else running on them in real time in a
browser so I'll get a bunch of demos
they will cover different things this is
what our alpha testers are getting to
play with today I'm gonna go up and show
some other demos basically those these
are the ones that the beta testers don't
actually get access to so what we have
here is just a HD video stream being
played back so we just use a standard
off-the-shelf video decoder software we
we have a system called extensions which
enables us to Lincoln libraries from
other from third-party tools and
actually then use them to stream and
data so the vikon stream is actually
using a vikon SDK here we're using the
NPM pic playback SDK stream this data in
and then it's it's in our engine running
in the browser and we can actually do
things with it like we're just using
OpenGL here to do filtering it did not
work oh I don't know what's happened
there I am sorry
I'd have to show you the other demo yeah
ok someone break the filters
okay this video once is playing in the
scene we actually will go into that
further how our SDK works but we're
building at all what we call a a scene
graph so scene graph in a Buhl's you to
construct 3d scenes what we consider is
that we're building a low level
architecture for doing high performance
computation the browser but is its
domain agnostic it doesn't have anything
built into it for doing any particular
problem solving just the concept of
processing data on top of that we can
actually build architectures for solving
very specific problems and in this case
we're solving we're bringing in video
we're actually putting it on a sphere
and then that sphere is being rented
from a camera so we're using a lot of
OpenGL and we're actually just doing
some simple processing in the previous
demo I would have shown you it's
actually fairly basic GLSL shaders for
doing things like edge detection in that
case we're because we're doing all the
processing on the GPU our actual engine
isn't doing a whole lot it's just sort
of setting up the GLSL shaders and
running them so it's not actually a
great showcase of what our system is
doing you could actually do some fairly
similar things using WebGL but I think
our system makes it a whole lot faster
so we're going to talk a bit more about
how our engine does these things and
what sort of what how we've gotten to
where we are
so what fabric is is it's a dependency
graph it's basically from within your
running web application you construct
and manipulate a dependency graph a
dependency graph is this collection of
nodes those nodes contain data and
they're connected together with
dependencies and the dependencies
basically define an ordering of
execution of the nodes the actual
modification of the data as it
propagates through the graph is affected
through operators and these operators
are written in a language we've been
developing called KL and this language
it's compiled statically so it takes
advantage of all the optimizations you
find in a statically compiled language
but it's being compiled onload which
makes means that there's actually
platform neutral stuff the web
application is still a platform mutual
web application if you look if you run
the same demo on different CPU
architectures different machine code is
generated but it's generated on load so
we the code kale coders has loaded in
much the same way JavaScript code is
loaded JavaScript code is loaded as a
web application has loads and from
within your JavaScript you actually
start saying I care we're going to be
constructing this graph it's got these
nodes in it and it needs these this code
compiled and these operators attached we
use a compiler architecture called LLVM
and LOV m is a really cool great project
and it's being used by a lot of
different companies today the reason why
we started using it was because it's
actually got very fast compile times
we knew that absolutely it was important
that we could compile and run our load
application super quickly and also has a
great back-end architecture for the
optimization stage so we knew that the
code would be generating would be
competitive with with sort of any other
traditionally built application and sort
of the secret sauce of the way that
we've been building it's the glue stuff
that basically you can define your graph
you can define your operators and once
you've assembled them all the execution
engine just runs
and so design considerations when
building a system is that by defining
things using a dependency graph we've
we've automatically defined independent
tasks which can be run on different CPU
of course so we the graph implicitly
defines task based decomposition of your
work workload and that works very well
for each node we actually have data
stored on the node and what we have is a
concept where a node can store a bunch
of data but that data can be a
homogeneous data set a large chunk of
data so things like images and measures
and this processing operator
architecture and pieces so we can
actually slice up that data set actually
allocated to different cores so at the
same time we've got tasks based
distribution of work at each at each
node we can actually decompose that even
further and break it into separate data
chunks and allocate that to different
cores so that's where the system scales
very well for very many numbers of cores
at the same time we're actually also
working with the GPU so we can actually
push work to the GPU and let the CPU get
on with doing other things so we're
actually utilizing the GPU whenever
possible
at the moment we use it mostly for
rendering but we're actually working on
instead of utilizing the GPU more and
more Furyk's if generic number crunching
problems that's pretty much it for us
today any questions
yeah sure I was actually expecting that
soon I don't quite have enough room to
do things so the KO language Peter
designed it when we first started
working on fabric
we were actually supporting C++ in the
browser and we just knew that that
wasn't a long-term solution and while we
looked at things like I mean Google's
been working on things that Native
Client to try and make C++ secure in the
browser it's also very that's a huge
problem with this old as well so we
decided that actually it was easier to
develop design at develop language which
actually solved the problems we were
looking to solve and nothing else it's
actually a fairly simple language so cut
bounding box is a nice simple file so
this this is an operator which runs on a
piece of geometry to determine the
bounding box or the geometry just
looking at the maximum values so you can
see the language we developed as
strongly typed all the types actually
have to be defined in advance and but in
many ways it's very similar to
JavaScript it's a very JavaScript like
syntax from it many things things like
for loops and conditionals it's actually
the intention is that a developer who's
familiar with JavaScript actually pick
up KL and within a couple of days be
fairly familiar and be able to write
code in fact I've been working with
JavaScript and kale so much that when I
switch back and forth I actually always
have to sort of tell myself like a
strong typing here but that's pretty
much the only difference
questions like you know Health Park when
we take the language design to get away
from the strong types because that is
one thing that JavaScript programmers
are not used to having to deal with it's
a lot of type inference you can do in a
language which is really nice which is
one of the directions that I'm sort of
really interested in in taking this
language but the point of the language
is not to be as sophisticated as a
language like C++ obviously what we're
looking for is something which enables
people to do high-performance
computation without actually having the
humongous learning curve involved
because it's people who have specific
problems performance issues which they
need to sort out and they don't want to
jump too far from the web application
development paradigm to do so so I mean
Kay L stands for kernel language so it's
not a full language it doesn't actually
have to have as many features as a full
programming language like JavaScript or
C++ it really is just for the developing
kernels of code that run were none nodes
in a very well-defined context
yeah sure so the question is how would
if you're building a website web page or
web application which is using fabric
technologies basically what extra stuff
do you have to do like how do you
integrate it so I can show you it's one
of the demos that we've been looking at
just so you can see open up the flocking
demo and effectively at the top of our
page and you would have seen this I
think I stepped through it a bit too
quickly and then talked but we specify a
bunch of JavaScript files and we're
looking at compacting a lot of stuff we
can actually merge our map files into
one file that's a longer term issue but
effectively we have a bunch of 3mf here
we specify our scene graph just our
system so what the scene graph is it's a
layer of code which obstructs the
dependency graph we see that a lot of
people just want to build like if we
step through so the first file that gets
included is wrappers and J is fabric J's
and when the application when the web
page loads we actually invoke our plugin
we construct an instance of our plugin
so we're running things through an NPAPI
plug-in actually I didn't mention that
earlier yeah everything's running in a
plugin the first thing that has to
happen when we start constructing a
scene is just construct the camera okay
so as we can see here there's cameras
and lads being configured and the fabric
and the fabric namespace actually
initially sets up the context and
prepares the plugin for actually
constructing scene graphs
the scene graph is just one application
of fabric it could be used in a
completely non in a completely different
domain where 3d rendering is of no
interest whatsoever and then of course
it uses a different set of code and you
have a much
different set of files being loaded how
would we hear them if they had a
question okay do you guys have any
questions remotely that's an interesting
question yeah so the question is there
are some security problems with GL
drivers which are a bit of a concern for
WebGL like what's our answer to that in
fabric that's a really interesting
question right now we're exposing the GL
bindings directly so we have the same
security issues we would like to be able
to expose things at a little bit of a
higher issue but one thing to keep in
mind with fabric is that there are sort
of there's the idea of having a consumer
web application running fabric to
produce a high-performance component but
there's also people who we see as being
more initial customers to the product or
people who are installing a specific
application or something like that so we
can we can govern the hardware in the
first stages of what we're doing so
basically the long the short of it is
that yeah that's the sort of run boy
what is the best solution to us I mean I
know you guys working on WebGL too
similar problem what do you do about
about the problems with graphics card
and security how do you manage the GPU
memory stuff like that either
these are big things but again yeah the
typical initial fabric applications are
gonna be well ok yeah this machine is
actually going to have a specific nvidia
card or something like that and you're
basically installing this is an
application delivery platform for a
specific application as opposed to a
general web consumer think that said
we're looking forward to ok what happens
if it actually does become something
where people go to a web site and oh
yeah I know they've got some
which is you know these questions he
answered this is one of the reasons that
we're using kale to do the high
performance code instead of using C++
there's nothing wrong with using C++ if
it's something that you're asking people
to install on the computer if they want
to use the application but we liked
something better than that in the long
term we'd like the possibility of being
able to have untrusted high performance
code running for sure it does for the
high performance code but for the for
the GL issue you're specifically
answering that's still sort of an open
question aren't as well to tell you the
honest truth what we'd like more than
anything else is we'd like to just have
access to the GL context that would
simplify everything tremendously for us
right leave that to the browser the
browser is extremely good at handling
security so one of the things that we're
doing is we don't actually access any
resources like people can't load files
off of disks and stuff like that
obviously right we get all of our data
through htb streams coming from the
browser that's great because then we in
here at the browser security model we'd
really like to be able to do the same
thing with the rendering pipeline for
sure it's a feature request so just to
explain the reason why kale provides a
security layer is that language like C++
exposes pointers and pointers are
basically addresses in memory and and
they're really like they're a loaded gun
for a lot of developers and by exposing
such things in the browser you actually
create massive security holes so by
developing a language like kale we it's
intentional that it's pointless you
never actually access memory directly
you never need to allocate memory or
deallocate memory a lot of the memory in
that all of the memory management is
handled using using nkl automatically
behind the scenes for you we do expose
the system called extensions which
enables you to Lincoln library and when
you're running when you're building an
extension you are writing it in C++ and
you can do whatever you want there and
we use the extensions to connect to
things like the fact on the store
anymore sort of video streams
so the question is we've introduced a
new language and so how does that affect
the tools that people use I the first
thing is is that Kol is compiled by the
browser as it loads so there's no
specific tools required to work with
kale it's just a text file like
JavaScript so we use the same editors to
edit JavaScript and kale at the same
time once the application is running we
actually do have a debugger running that
we can analyze the running application
and provide tools to actually understand
the performance and sort of identify
bottlenecks and things are there I'll
show you the debugger just quickly and
get back up it's come to this
demo
you don't want me to do that I can dance
I can have tango for you guys I'm just
opening up the console firstly we we can
log stuff to the console and here we
have a debugger now as Peter said it is
a preliminary stages and we're still
working on things it was actually a
project that's as high on our priority
list to rewrite but what we are seeing
is a visual representation of the
dependency graph that got constructed
over here actually we have the flock and
a flock they are one particle system
like a flock is actually broken down
into multiple dependency graph nodes
each dependency graph node effectively
defines sort of an execution context so
problem so all of the particles live in
one node but there's other data that
gets associated with the particle system
like for example it's bounding box which
is stored in a separate node and this
enables us to schedule the execution
according to the notes that you're
running and it's the same graph system
we're building actually abstracts that
away so as a developer you don't
actually need to know that one block has
made up of more sub nodes by clicking on
this flock attributes DG node I can see
I mean immediately sort of all of the
members which are assigned to it and
inputs and outputs that actually says
dependencies
what's dependent what do we depend on
wants to pin out on us
we can also debug running operators and
the fact that we're running a compiler
in the browser gives us a lot of
flexibility like we can actually run
modify code as it's running and I'm just
reread yet the code as long as the code
that doesn't syntactically break
anything basically the flock will just
change behavior so we see this kind of
like it can continue in Visual Studio
what we don't try to do is propagate
those changes back into your source code
because we're not writing stuff to disk
so we use it as just a way to test
running coded and fine-tuned it changes
back so in terms of debugging at a lower
level which is kind of an interesting
question for a performance-based
language this is something which I've
thought about there's actually
absolutely nothing and giving us from
basically doing brakeman reflection from
the LVM environment back into the
browser which will be really fun to do
when you think it's that point but yeah
absolutely I mean LVM is just a generic
compiler Farragut's yeah we can
absolutely have to defense you clean up
but we're actually I'm suspicious of
there's certain cases where I will
definitely be useful but I think for a
lot of the problems that people are
gonna be solving analytic tools are
probably going to be the seducible this
being able to do useful profiling is
going to be a tremendously helpful thing
and debugging multi-core applications in
general is a very difficult problem
space so yeah those tentacles tend to be
a little bit more useful for my
experience
well instead of that's part of our work
in progress we doing what our goal is to
abstract the GPU to the point where the
developer doesn't need to concern
themselves with it there our goal is
that developers don't need to know a lot
about GPUs
that's our goal from our Civic business
perspective and we're not going to talk
much about the technical side of things
just because it's stuff that we we're
still developing at moment so any code
that we use the GP for we explicitly
manage the GP so we do expose the full
OpenCL api from within chaos and of
course we also expose the full OpenGL
API so you can manage your GPU manually
just in the same way you would in a
desktop application what's nice is that
because we expose this the full open GL
API right up to opengl 4.0 we got access
to things like geometry shaders
tessellation shader is really the latest
and greatest hardware and this is
something we can do because we're not
constrained by standards bodies we're
not actually in there
we're not trying to hit some lowest
common denominator weight we can
actually push as hard as we like on this
stuff and for a lot of customers we're
talking to they do have a very powerful
GPU in their machine and they would like
to use all of it so there's no reason
for us not to do them
yeah yeah absolutely I mean yeah so the
question is are there any limitations to
the parallelism that you can extract
from on the operator graph yeah of
course I mean there are a lot of
different multi-threading models that
are have different sort of epic
cassadee's depending on a problem domain
you're working in for the problem that
means that we're sort of targeting like
building things around like high-end
scene grabs and stuff like got to create
for instance we're finding this actually
works really well there's there's
there's already a lot of task based and
database parallelism in these scenes if
you consider for instance other types of
applications like applique you know
we've had some questions from people
around for instance using actor based
models for for multi a little bit larger
at that point and that definitely would
be an alternative model that we could
use in this case but that's it so it's
kind of difficult to do static analysis
on that kind of stuff which is what are
the really big benefits we get from
taking the operator drop approach to
things one of the things which is really
really powerful with what we've done
here is that we have not just all the
source code for all of the operators but
we also have a full description of
what's necessary to decide how to
optimally multi-thread things based on
the currently available hardware so in a
sense you have basically a transportable
description of a computational problem
and it varies very specifically
computational problems this is something
I don't think Phil mentioned we're not
expecting somebody to write their whole
application and fabric and Caleb more
that you know a web application which is
95% JavaScript and all the other web
technologies and stuff like that has
specific parts of it which for which the
existing high-performance
features of JavaScript just aren't
sufficient things like web workers and
stuff like that are not really very good
for doing database parallelism
what we have is we have this way of
representing a computational problem
which needs to be solved and sort of
like a package that can actually be
transported from you can be described
within the web browser targeted for the
specific machine that is running on it's
very easy for us to do a lot of static
analysis on it that's one thing we're
already doing a lot of difficult are the
best run it based on what hardware is
available based on other computational
resources which can be made available
right yeah but yeah absolutely there's
no there's no question it's like there's
a whole bunch there's a bunch of server
cloud-based multi-threading models which
yeah I mean they they're trying to solve
different problems basically there's
also an important element that fabric is
really just a computation engine that's
running in the system it we leverage the
browser as much as possible whenever we
can so the browser has a very powerful
event model for caching events and
propagating through through the web
application so we actually catch the
brown and the browser events and
translate them and use them in the 3d
scene graph so we actually translate
sort of 2d web the browser events into
3d events and we actually have built up
an event model in our 3d scene graph
which mimics very closely how the
browser Dom events work including things
like bubbling and supply this so and
then we actually do propagate events
back into JavaScript so the idea is that
we use JavaScript exactly for what it
was designed for event handling and
things like manipulation and any systems
where we are key interacting with a
running graph we're using the JavaScript
engine a lot to do modifications to the
running graph so if you're going to be
painting strokes on a mesh or making
changes to the graph every time you make
a change structural change to the
dependency graph it's running of that
changes a few made in JavaScript and in
the next evaluation that comes through
on the engine those new changes are in
place we don't allow any sort of
structural changes to the graph while
it's running we sit there between frames
evaluation we allow changes in the
structure
yeah yeah we don't actually we we're up
to the technical level that we're aiming
at is actually a web developer so we
wanted to make it possible for a web
developer who's not even familiar with
things like memory management of any
kind can actually build a high
performance application an application
which which is competitive against a
well-written
desktop application so a lot of these
issues like memory management is that
the first goal was doctors usually keep
their abstracted away from the developer
for multiple reasons one of them being
security but the second one being
simplicity there's a button LVM where
it's generating absolutely incorrect
code it's referencing pointers the wrong
way
absolutely yeah I don't see that as
being a different scope of potential
security issues beyond stuff that's
exist for instance yeah I mean obviously
the jet in v8 for instance could have
exactly the same problems right we don't
actually so yeah we're hoping the fact
that you know I didn't write LLVM will
inspire a little bit more confidence
it's a large public project right
there's a lot of a lot of people
auditing it
yeah it's pretty crazy if yeah if
there's anything this is sort of an
aside note that will sort of inspire how
stable LVM can be it's how rarely they
actually do bug-fix releases which is
like never good I've never seen so many
units before in my life absolutely and
that's something that we actually yeah
so the question was like well it is yeah
I was pointing out that we could
actually pipe the output of LLVM to
Native Client so essentially we could
target Native Client as the platform as
opposed to a browser plugin well that's
that tell honestly that we'd like that
problems native clients would have been
a bit behind like they've sort of
they've been reiterating I'm sort of it
I don't know if you guys are reckon
Native Client here if you are great to
talk to but the no it's like there's
been a lot of iteration on it and it's
sort of like we've been trying to push
ahead and they've been sort of honestly
lagging a little bit and so we're like
ok well they're fundamentally what we
have with fabric internally which would
be sort of interesting as developers is
that it's actually a client-server
architecture right so you actually need
just a message pipe to work with the
fabric or it's actually an it's truly
client-server you can actually have
multiple clients working with the same
core which is nice a whole bunch of
different possibilities but we actually
did that intentionally cuz we originally
had more of an object base like methods
and properties in her face which is what
Native Client was using back in the day
when we were first looking at it and
then at one point we were finding first
of all that the performance was
absolutely abysmal for talking to the
browser this way and the other thing was
that I think that they found exactly the
same thing in the Native Client team so
we basically said like ok let's go back
to the drawing board like let's make
this so that they can decide to
architect things and whatever way that
they want to and it's not going to kill
us because we like eventually to make it
so that we can just apply on the native
client to say hey look once again
Google's done although
for us they've sorted out the security
issues stuff like that right right so we
actually did like the pass and then run
your verifier yeah absolutely that's all
you get actually enable just usually as
a flag it's like you read an extra
secure mode yeah that's an interesting
idea
we've always been looking at native
plant fairly closely for last year and a
half and tracking its progress and our
intention would be to support Native
Client if possible it does actually
eliminate one of the big problems that
we have bringing a plug-in to market
which is plug and install friction and
weed that's exactly why it was developed
right so we actually hope that one day
we can actually take advantage of Native
Client I think that's about it for today
thanks guys for listening</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>