<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Whats Eating Scientific Data?  21st Century Approaches to | Coder Coacher - Coaching Coders</title><meta content="Whats Eating Scientific Data?  21st Century Approaches to - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Whats Eating Scientific Data?  21st Century Approaches to</b></h2><h5 class="post__date">2009-07-09</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/Zjyi8NG1SjY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">introducing Nico Adams and Jim Downey
from University of Cambridge chemists
who work on semantic problems and
chemistry which is very interesting to
me because I've worked on similar stuff
in biology it's a really huge problem
it's a potential solution to these
problems demica very interesting and
they're going to tell us about what's
scientific approaches thank you very
much for the introduction David and also
thank you very much too elenco for
setting this up and inviting us he
unfortunately couldn't be with us today
but yes we would like to try and give
you an overview of some of our work
that's going on in Cambridge we are
working in the general area of semantic
technologies as applied to science
technology medicine our particular
specialization is chemistry and we care
about chemistry chemistry of course is
the central physical science which sits
at the interface of many high-value
industries and high impact other
sciences such as bioscience
pharmaceutical research and so on and so
on and so on and one thing that we're
seeing more and more is that exciting
scientific new progress product
development is mainly mainly happens at
the interfaces of a number of different
science chemistry being in the middle
but a reinterpretation of that from an
infirmity shins point of view simply is
that product development in chemistry
new chemical discovery very often our
data integration and data mashup
problems and of course the one thing
that signs and therefore so chemistry do
there are three areas of activity will
the time its data production its data
analysis and its date of publication
that is just simply what science does
and in chemistry the sort of typical
scientific discovery cycle is a little
bit a little bit like this you start off
with an idea which you develop you get
funded you then develop an experimental
plan you carry out your experiments you
record the results of the experiments
you process the data that comes out of
it you publish the data and then you
read a little bit and you use
your own data and other people's data to
generate more ideas and you go around
the cycle again now virtually all stages
of that cycle involve data of course but
virtually also all stages of that cycle
are disjoint it is very very difficult
to bridge data from one side to another
side of along a route and the main
reason that they're disjoint is simply
has to do with the way in which science
and chemistry in particular at the
moment produces data and publishes data
and communicates data so if we look at
data production in the laboratory and in
silico we see the following things in
the laboratory what we do is we carry
out an experiment and we record the
experiment and the data that we generate
in the experiment in an in a notebook
that can be an electronic laboratory
notebook or it can be it can be on paper
but in any case what we're doing is
we're producing in essence a compound
document we're producing a document that
contains empirical observations and data
all mixed into one however the way in
which we treat that document is entirely
determined by the semantics of the
document as opposed to the semantics of
the data that is also contained in the
document so the document if you will
determines the semantics of how we
handle our we handle data it's at the
same for scientific publication so
here's a typical scientific paper that
scientific paper actually contains text
it comes it contains an awful lot of
data that data isn't entirely visible to
a human we see that sort of thing but of
course because the semantics of dealing
with are the semantics of the document
what a computer sees in essence is this
and we're really and a machine will find
it almost impossible our priority to
discover data in the sort of mess and
let alone understand the data understand
in inverted commas and act on it so the
way in which we publish and communicate
an exchange data in science quite often
leads to data interoperability death
another facet of that data into a
probability death and of that negligence
in terms of dealing with scientific data
is the use of PDF across scientific
communications so even if we produce a
digital document in which we could in
essence this
other data we then usually convert it to
PDF for the purposes of communication
and for the purposes of distribution and
what that in effect means is that we
take potentially discoverable data in a
document we converted to a set of
graphical objects without any related
semantics between them and then we're
expecting a computer to try and sort of
rediscover that data again and as one of
my colleagues p tomorrow rust says it's
rather like taking a cow converting it
to hamburg and then trying to convert
the hamburger back to account and of
course my science most scientists are
entirely unconscious of that process but
from a scientific point of view from
scholarly communications point of view
PDF is not a good thing it's just
another manifestation of that
callousness with which we handle
scientific data the second phenomenon
and the supplies to data in in in silica
if you will is that we quite often
produce data in a particular vernacular
so here is a representation of an enzyme
near many days associated with the
influenza virus that's a representation
of data that humans understand but of
course the underlying data looks
something like this it it is structured
data it is in some form of markup
however it is markup which only one
particular type of software can actually
deal with one particular soft type of
software can understand and it is markup
which is not universally used by all
software system so if you will dub
markup is vernacular we have structured
vernacular but these D sorts of
vernaculars to lead to in essence data
interoperability death if you really do
work on the assumption that you know we
need to mash up that sort of data with
data from other domain sources the need
for mashing up really becomes clearer
and clearer as you look across the whole
of the pharmaceutical life science film
and personal care industry very few
pharmaceutical companies maintain a
whole discovery pipeline in-house these
days and by that I mean a discovery
pipeline from fundamental research going
right through to the clinic rather what
they've done is they've chopped up the
discovery pipeline and they are starting
to outsource activities across that
pipeline into smaller companies that
specialize in a number of these
activities
what it means though is that those
pharmaceutical companies need to have a
means to actually export the data out of
their own out of their own system into
the companies they outsource to and the
the companies that are being outsourced
to need to get data back into their
clients into their client systems so
data integration it really really is
depressing and data interpret eerily is
a pressing thing and it leads to a
phenomena such as the formation of the
pistoi Airlines for example which is a
conglomerate of a number of very large
from players in the pharmaceutical arena
that tries to standardize data formats
across the industry now you may argue
how successful about how you may argue
about how successful that will be but in
any case it's going on and the lessons
from it lesson from it is that data
interoperability is not about
serendipitous reuse of data that it is
essential for integrated data
environments and we've just taken a day
out from a conference here in Silicon
Valley semantic technologies and that
conference certainly seem to be
suggesting that this integrated data
environment is starting to happen on the
web now we're starting to see more and
more and more mashups more and more and
very large scale mashups across very
large parts of the web so we do require
exchange formats and we do require or at
least so we believe computer plant
ologies now you scratch your head for a
while and you think well we'll have a
scene a situation before where data is
in lots of silos where it doesn't have
meaning where it's hard to discover and
of course the answer is the world wide
web on the world wide web what you have
is a situation about basically anybody
can say anything about any topic and
what that means is that data can be in
different documents and different
locations on different servers but it's
all data that may or may not pertain to
one single resource in this case we're
using a polymer as a an example and will
reassign tists really would like to do
is we would like to be able to pull that
data together not just from the world
wide web but you know where scientists
have got collections of documents in our
computers in the information systems
that we use and so on and so on and we
would like to bring that data together
to discover it and to mash it up and to
reuse it so if you think about that the
world the fact that the World Wide Web
has the same sort of problems that we
face
signs of data enter probability data
discovery unstructured data you might
ask yourself well as the world wide web
also evolved solutions that might
address some of those problems and the
answer is again so we believe at least
yes data discovery and data structuring
in the web is facilitated by extensible
markup language data integration is
facilitated by a technology called the
resource description framework and I'm
going to explain that in a minute and
meaning and true data and our
probability can be facilitated by the
use of ontologies if you if you if you
decide to go then double root so let's
examine some of these technologies in in
slightly more detail in Cambridge and
actually even before our time in
Cambridge we've been busy developing
chemical markup language chemical markup
language was developed and is still
being developed by Peter Marie rust and
Henry sepa Anna caters for wide range of
chemical data it describes molecular
structure but also physical chemical
properties spectral information preslava
free computation on information and so
on and so on the whole gamut of data the
DD encounter in chemistry here's a very
simple example of what CML could look
like it's a description of a molecule
there on the left and you see that the
whole document talks about a molecule so
we've got the molecule element as a
container the document contains Adam
arrays and bond arrays which in turn
contain atoms and bonds and literally
just lists of atoms and bonds occurring
in the molecule and the first item is an
atom of elemental carbon it has an ID
and an X and the y coordinate which and
this sort of representation in essence
allows you to construct that sort of
representation there on the left and
that Adam there is connected by the
first one somewhere there molecule you
can extend these sorts of rigorous and
complete descriptions of small molecules
to fuzzier materials such as polymers
and nanostructured objects and that some
something we've been working on recently
in cambridge i'm not going to i'm not
going to go into how this document works
but we cannot
the introduced fuzziness and stochastic
computability into XML and we can treat
attributes in an XML document as
computable free variables if we if we
want to which is kind of a new concept
on the side for sure yeah I'm not
familiar with it but when you say
chemical information for Paloma stand
we're now running into an ontology
problem because when the protein
chemists a polymer they mean something
completely different that I mean when I
in in in in biochemistry in protein
chemistry a polymer is just a single
macro molecule with a large degree of
polymerization of amino acids in
material science which is my original
prominence of polymer is an ensemble of
molecules all of which have got slightly
different lengths slightly different
architectures slightly different
properties and exactly and exactly and
and none of the fire none of the
representation we've had up until now
can carry with that stochastic will
cater for that stochastic nature
eponymous yes exactly exactly exactly
and we haven't been able to describe
that sort of stuff let alone search for
it or search over it as well and this is
exactly what polymer markup language
polymer markup language gives you so
this is just a very short run through of
the sort of XML tools that for for
chemistry there are many other markup
languages which are pertinent there are
there's analytical markup language
mathematical markup language
thermochemical markup language all these
sort of things in to operate quite
nicely with CML and help you to deal
with scientific data and to soar with
chemical data and to structure chemical
data now data interoperability is
something that rdf caters for rdf stands
for resource description framework and
the name is programmatic in the sense
that it is a descriptive framework for
describing resources a resource in
internet
because everything that can be named or
addressed that can be real physical
objects or metaphysical ideas even and
of course rdf make statements in very
simple triple in a very simple triple
form almost human language subject
predicate object forms and the reason
for that is of course that a triple is
the smallest irreducible representation
of data that there is an Excel
spreadsheet in essence is a collection
of triples so column heading polystyrene
row heading has temp has glass
transition temperature value in the cell
90 degrees that's in essence a triple
now what makes RDF special is not those
triple statements but rather the fact
that every part of the triple has the
URI associated with it and well that of
course means is that you can take
statements from different domains and
bring them together and mash them up and
mash them up in these sorts of triples
so you can you can you can construct
these graph representations of your
knowledge across different domains and
across different namespaces the
namespace in the URI codes in essence in
essence for the domain the analogy
talking about Excel spreadsheets to this
is taking every cell in your Excel
spreadsheet putting it into a different
document somewhere on the server but
because it has its whole its old address
space whether by deploying one query you
could in essence bring the spreadsheet
back together if it was expressed in an
rdf so here's a way to yeah that's right
that's absolutely right so here's a way
in which chemical data is actually very
trivial can be expressed in rdf to
civilization as and three years so we've
removed the the urls for better
readability but you see the triples
we're talking about benzene again
benzene is a molecule which has a
property here which is a boiling point
and the boiling point has a value or
voidable one and has a unit of degree
centigrade
and by the way that warning phone also
has a measurement condition which is a
measurement pressure which in turn has a
body of a hand that's a way of actually
describing minimum information
requirements as well you see how we
which ain't ripples together and how we
chain information about objects together
so the benzene or the molecule benzene
is connected to a property which in turn
is connected to a measurement condition
which is specified as a measurement
pressure and so on and so on and so on
now there's a lot of chemical data now
in RDF coming on stream in the form of
dbpedia for example there's RDF tied up
in molecules dotnet and there's also
freeways which contains a significant
amount of chemical data right now foot
right there is one small pointer for
true data interoperability the ability
to just mash up and by matching grass
really isn't enough I've got concepts in
my document here such as a boiling point
and a measurement pressure and the
notion of a molecule and really if you
want a machine to discover data
associate well the machine to understand
data associated with those concepts and
to understand whether it can if it finds
those concept the same concept in two
different documents with data associated
with them whether it really can bring
those two together it needs to
understand while others concepts the
same do I mean the same thing when I
talk about a polymer and do you mean the
same thing when you talk about a polymer
so what are the accident ization then of
course for that you might want to
consider the use of ontologies to
clarify that relationship and in
Cambridge what we've done is we've
developed a set of ontology is called
collectively known as Kim Kim axiom they
are a number of interoperable ontology
modules came axiom chem domain which is
one model in this clarifies a number of
fundamental relationships in the
chemistry in a chemistry domains the for
example it deals with the relationships
between molecules and substances that is
of importance when we're talking about
the sorts of things that we've just been
talking about polymers versus
macromolecules and what mine
standing of those things is it deals
with identify as what our identifies for
a molecule how do they relate to a
substance it deals with roles so what do
I mean by asset do I mean a molecule
that acts as an asset or do I mean acid
the substance in a bottle that sits on
my bench and that is sulfuric acid these
those sorts of things k max in pali then
is an ontology that allows you to talk
about the properties of these molecules
and substances that can axiom chem
domain d defines and chem acts and
metrology is an ontology that allows you
to speak about how these properties were
measured and of course we've also got
ontology because we are polymer chemists
originally that deal with concepts in
the polymer and the polymer domain
there's also an ontology at the moment
for chemical metadata which is under
development now here's a summary of
those ontology stay were developed with
the use case of describing chemical data
in mind there are other Impala G's in
the chemical domain which do different
things so their ontology of chemical
reactions the ontology zuv chemical
compounds and their biological functions
and so on and kim axiom into operates
quite nicely with all of these
ontologies so between them they actually
cover a wide area of chemistry the
platform has gone at some secure some
significant interest internationally
were working with University of Ottawa
there was society of chemistry and the
European bioinformatics Institute to
actually between us come up with a solid
platform and maintainable platform for
the further development and maintenance
of these ontologies and we're at the
moment expanding chem axiom to also deal
with data from computational chemistry
codes and crystallography now how do
these ontology work well here's an
example of the formal computable
definition of a chemical substance and
basically what this document does is
defines a chemical substance as the
Union class so we were doing set theory
here and we're modeling the world in
terms of classes and a chemical
substance and does this document is
defined as the Union class of the class
of everything that is composed of
molecules
so substances bulk substances in the
bottle are composed of molecules and the
class of everything that is composed of
other substances and that gives you a
recursion and therefore allows you to
model mixtures and formulations and all
that sort of thing and the class of
everything that our chemical elements so
the Union class of all of that is
defined here as a chemical substance now
we will always argue about these these
these definitions and the axioms that at
least I now have a computational way of
actually checking what it is that I mean
by substance and therefore what the data
that I've associated with that concept
means an interoperability now comes from
this combination of data in rdf and
ontology so here is more data about
benzene and rdf it's the same thing
we're saying that benzene only this time
we're talking about benzene not the
molecule but benzene the substance
they're saying that benzene the
substance has a property which is a
density and a boiling point and the
boiling point has a value and it was
measured by a method measurement method
called vomitoria here by the way you see
the mixing of various namespaces and
various your eyes and actions because
you see that cue names are drawn from
different domains so that's the rdf now
then i can look up what some of those
terms in my rdf mean so i said that the
boiling point was measured by a bully
ama tree while here is the definition of
the Bergama tree in comics and metrology
it uses an instrument called an EVO do
metra it is it is a it is a thermo
chemical analysis technique and one
axiom that isn't shown is that it
returns values of dimension temperature
so if now someone in their documents or
when their data collection had a boiling
point that was measured by balama tree
but were the unit of the value that of
the boiling point that they gave was
centimeters then I could immediately use
this ontology to check well actually is
their understanding and my understanding
of the boiling point at the same and
therefore can I bring the data greener
or do I need to be careful yeah that's
right right right
all right now we've talked an awful lot
about Marco and that markup is all nice
and well but the question and should be
asking immediately is dude how do i get
that mark up into my documents and into
my information sources nobody wants to
have to order that sort of stuff by hand
how do I do it and there are two answers
to the question the first one is you
introduce mark up our posture re so you
have a document you've written it in the
normal way and you then try and
retrospectively mark the document up
there's an awful lot of legacy data that
we have where that is the only way of
getting markup into documents to do that
we have developed a software robot
called Oscar which we call the journal
eating robot and literally what Oscar
does it takes journals documents other
scientific publications and it tries to
read it to pasture to pasture text and
using a natural language processing work
flow to extract entities actions and so
on from the tanks so here's an example
of the type here is the beginning of a
chemical paper here's an example of the
type of markup that Oscar returns it
discovers chemical compounds that
discovers certain ontology terms I can
discover reactions yes yes right um the
question the question was a chemical
papers quite often quite often a very
dense and that there's ambiguous meaning
in a lot of those in a lot of the
sentences and how do we ensure that in
our natural language processing I can
come up with the right sort of
ontological association with the entity
that it may have discovered the answer
to that mainly is that that
disambiguation disambiguation is
something that we don't touch or
possibly can so we deal with so we deal
with entities with pure chemical
entities at the moment and a pure
chemical entity is relatively easy to
identify and unambiguous relay
said is always going to be a leg asset
in terms of actions how these entities
are connected to other entities that's a
more difficult problem and that's
something that we're working on but we
haven't touched it we haven't touched it
so far so right now we're just literally
trying to get data out and we're trying
to get it entities out and they're
disambiguation problems are relatively
yes yeah so so so Oscar Oscar is
relatively well Oscar can be relatively
demain agnostic in the sense that it
relies on a number of external resources
such as external anthologies jim has
pointed out that we can retrain Oscar
also the natural language processing
that's actually going on in Oscar is not
he's not particularly deep so we have
discovery of entities in terms of either
lexical matching or in terms of
probabilities right we can work out
probabilities of whether something is a
chemical entity or not and that's about
as far as it as it goes we we do this
using Ingram's and and the sort of thing
and it so yeah sure and an animal most
animal animal muslim so the way this the
way this works out then is here's an
example of Oscar markup and it's I've
taken the first sentence of that paper
that I've just shown you and the first
sentence racer legassik coated magnetite
has been encapsulated in biocompatible
magnetic nanoparticles by simply a
margin above evaporation method once
Oscars finished passing that sentence
and it starts to look a little bit like
this and as you start reading here fully
acid
we have discovered it on the basis of
civil simple lexical look up so one of
the resources that Oscar use is actually
a lexicon of a very large lexicon of
chemical terms can be in this case and
we associate the information that we
have discovered it through looker carry
it with with together with the markup
we've also associated structure mark up
with was a term oleic acid which we get
from our chemical ontology here so we've
got a smattering and we've got an
interesting yes however there is another
entity that sentence called magnet side
now magnatag we didn't have in our in
our dictionary and magnified has
actually been discovered as a chemical
entity using an Engram school with a
given probability and we associated
together with the concept you're
embedding that's right yes yes that's
right because I mean that's that's one
way of ensuring so later if someone is
actually processing yeah you can have
priors yeah I don't believe in grass
veteran they'll just throw that's really
believe yeah that's right yes that's
right also this is this is annotation in
line Oscar also produce a standard of
annotation so if we didn't want to have
the or have multiple annotations or we
chopped up your entities a different you
you you changed your tokenization in
essence you could you could you could
you could deal with that we can not only
do that for chemical entities we're also
starting to be able to do it for
procedures for example here's a
synthesis procedure that contains a
number of different entities such as
reagents chemicals that also contains
solvents that contains quantities and
that contains actions we can discover
and mark up all these sorts of different
different types of objects and we can we
can hold them in CML and once we've done
that and we actually throw the natural
language away you can still understand
what's going on in the document we're
taking a reactant in a certain amount
dissolving it in a solvent of a certain
amount adding more reactants with
stirring filtering evaporating adding
salt and filtering and drying it and of
course once you have that information in
that
form and marked up in XML you can then
pass that on to synthesis robots and two
other information systems and you can
searching and discovery and so on now I
wanted to give you a quick demo of what
a scar actually looks like if our AV
system doesn't doesn't scupper us right
now here we go so here's here is a
typical here's a typical scientific
paper as you would read it in any
journal the only thing and as it gets
published in the web on the web by ASTM
publishers the only thing that we've
done is we've taken all of the graphical
ghaffar that the publishers usually put
in and we can now actually you see it
contains a number of chemical entities
it contains data it contains all the
usual things that you'd expect in a
scientific paper introductions
experimentals and so on and we can now
actually ask Oscar to pass this thing
and it'll go away and think about it and
after a while it will have it will have
come back with the markup it will have
discovered chemical entity as you see
it's discovered a heck of a lot of
polymers here it's discovered not just
the names but also abbreviations and it
will sometimes we'll be able to
associate a structure with the discover
chemical entity so here we've got water
for example we do get a structural
representation of the thing we have got
more complex molecules where we can do
that as well but NVR formamide that
doesn't seem to work but here's a vinyl
acetate for example and here you see
again the sort of markup that we've
associated with it in the text if I
click on it I can get to more
information about the molecule such as
synonyms smiles in chickies then I can
hook through to other information
sources where I could find out more
about about the yeah yes or so that yes
now it interacts with was the resources
that Oscar uses and it adds it to a list
of agencies
we have an ongoing recovery that's right
yeah okay so this is this is Oscar and
of course now this is this oops sorry
this is introduction this is an
introduction a posteriori but it would
of course be much better because this
sort of information archaeology is lossy
to do it our priori and if we do it our
priority to do it our pre or we've
developed authoring tools that plug
right into the authoring environment
that most scientists use which happen or
chemists use at least which happens to
be word and which we watch which will
want you as you introduce chemistry into
the document and introduce markup so
here's the plug up in action we can
introduce chemistry into word documents
for example by importing a CML file into
the document or by actually selecting
molecules from a gallery of molecules it
turns out many chemists in their
day-to-day lives only work on a small
subject as subset of molecules and so we
can deposit them in a gallery and then
as I write about them they can choose it
and introduce it into the word document
Oh another alternative is that what will
actually once you type and as it as it
discovers Kemp potential chemical
entities that will mark these things up
using the smart act technology you can
then confirm that indeed yes this is a
chemical entity by converting it to
something we call a chem zone and
ultimately what this has done is it is
deposited a CML document into the word
Oh XML document we can now have got to
document to chemicals in a molecule we
can look at those things in a molecule
browser which is part of the document
it's not so useful if you've got only
two molecules but the typical chemical
paper contains twenty to several hundred
molecules that sort of thing that comes
really valuable the representation as
you will see in a minute is actually the
graphical representation in the document
is discover just decoupled from the
underlying data so we can change the
representation we've just changed
benzene two phenyl hydride and now two
c6h6 but it all points back to the same
CML document here's a here
he right but I see that URL or her URI
as really a reference to document about
them yes right to molecule that's right
don't really use that no doubles doubles
doubles Jeff that was just said that was
just an example I'm flying this thing so
what this one of the demonstration was
running what also showed was we
introduced the second benzene molecule
which was independent of independent of
the first one we could change the
representation of that one independent
of the first one so this is copy by copy
by value but we can also deal with copy
by reference so we can couple several
representations to one underlying
document that's going forward there's
more information again forwarder that
website will be releasing it very soon
as hopefully as open source open source
called on codeplex so it is a plug-in it
is indeed a plug-in forward young so the
next question you should be asking
myself well dude were all my data
sources now I can do all the mark-up I
can introduce the data where do i get my
chemical data from and that is a very
very good question chemical data is very
different from other types of scientific
data in the sense that very large
businesses have been built a top of
chemical data and business interests are
very interested is actually very hard to
get to get hold of open kemah chemistry
data it is stifling the progress of
science at the moment we are hoping that
the increasing the increasing drive
towards state integration the need for
data integration together with the
increasing availability of the semantic
technology stacked and other
technologies that we've just been
demonstrating will ultimately drive the
uptake of those technologies yeah and it
is it is indeed a universal problem but
is a problem that together I think we as
scientists and information will the more
forward-thinking information providers
are cracking slowly Oh surely cracking
so the next thing is but dude how do I
publish semantic linked data on the web
and with that question I would really
like to hand over to Jim downing who has
developed a fantastic system for data
publication on the web and he's gonna
conclude the talk by telling us about
that thanks Nico
so snicker said I'm going to I'm going
to kind of give you a quick introduction
to a tool we're developing called lens
feel than what lens field is is it's a
system for the generation management and
publication of semantically rich linked
data and of at the top level it fits
together like this we take a software
build metaphor approach to the data
processing we produce a linked a simple
linked data web application and provide
automatic methods for deploying the data
with this application that provides
sparkle endpoints and other bits to make
the data more machine discoverable yeah
yeah and we allow we kind of help with
the management of the of the whole
process and of the data and by
automating and helping with their backup
procedures and version control but just
to step back for a moment nico nico
alluded to it earlier but why do we want
to do this in the first place so why we
actually care about linked data well
this is kind of going to my simple model
of science from 30,000 feet and it's
about observing observing the world
forming hypotheses forming a model using
your model to make predictions comparing
the predictions against experiments and
refining your refining your model now at
its best science is a virtuous cycle
where people can build on each other's
results they can share and collaborate
on their hypotheses and build them all
together but at the moment this really
isn't happening and we think that linked
data could really reduce the kind of
friction in the cycle and and a lot of
the disconnects when people produce
research they don't produce data so
people can't observe that data to build
for the hypothesis now linked data can
help sure with with the data size of
things but it can't really helped so
much with sharing and collaborating on
models but that's starting to happen as
well for example as a site called my
experiment dog which has developed and
is run by the University of Manchester
in Southampton and that basically
provides some scientific that's Carol
goals yeah
right that's great what it basically
does is it allows allows scientists
specifically data processing in
informatics to share their informatics
workflows so here we can see that you
can you can share your workflow other
people can download it and run it
validated against their data you can tag
it and follow people and all that kind
of jazz and importantly going back to
Nico's availability problem here we can
have clear statements of open licenses
so it's clearly what we wanted to get
linked data we wanted to see hammer how
far we could push it but we were really
having problems with deploying these
tools and introducing these technologies
in our in our department even the PhDs
and postdocs are all very very smart had
problems with semantic technologies then
new to them and when you're introducing
something new you really want kind of
that Python factor of simple things
being eat simple and complex things
being possible kind of you put in a
little bit of effort you get an awful
lot of bang for your buck in the first
few days and semantic technologies
really aren't like that and it's really
sticky part of that is because of the
sorry huge variety of technologies you
need to know this is a really really big
technology stat and somebody who comes
out of a PhD in the lab is not going to
know about how to get a web app working
on the server let alone about what HTTP
is let alone about content negotiation
Sparkle RDF but there's a much more
fundamental problem that actually causes
the really long delays which is just
remodeling the world as graphs when
you've been used to tabulated data's and
relational databases and this kind of
stuff but the reason you actually bother
with these kind of complex technologies
is that you've got faith that in the
long run you're going to get off an
awful lot back you're going to get a lot
more capability you're not going to
stick yourself to some simple
abstraction that's going to kind of
peter out once you get down the line of
it or isn't going to network getting
network effectiveness that kind of thing
so we really wanted to come up with some
kind of hybrid that let people have
simple tools and abstractions
they were learning but once they got to
be experts and started to dig into these
things just got out the way and we
didn't want to put a limiting
abstraction on to them so that it was
you know we made it simple for them but
it would limit them later on so we're
really trying to come up with that
abstraction so the first thing we went
for is we built a client server server
app that then kind of dealt with all the
semantic bits and dealt with the sparkle
and that was all kind of going okay but
we really what we realized actually the
data publication is such a small part of
the scientist working process quite work
and practice that it really wasn't
compelling for them we realized that if
we wanted to kind of hit the sweet spot
for getting these semantic technologies
into their into their thinking and into
their workflows we really had to reach
in reach into their data management
reach into their data processing so the
kind of the environment we were starting
to work in that case is that as nico
pointed out there's a paucity of good
quality public data in chemistry and
pubchem and chemspider both starting to
go places with that but it's still it's
still early days and we're hoping that
you know through these tools will aid
the course it's also the fact that
people data management is a really rare
skill people don't come out of PhDs
knowing about what to do with their data
really you know they they probably know
that it's important to back up and
they'll know how to burn a DVD but
certainly it's certainly the kind of
information management you need to be a
good good data producer isn't there
automation software like the workflows
engines that was behind that they're my
experiment at all that's called taverna
they're starting to get adopted in
places but they're not the right
solution for everything and you know and
it's still early days in the software
that's kind of that's an artifact of
this disconnect that's happened in East
science / cyber infrastructure that
we've had computer by and large we've
had computational people developing
infrastructure and we've had scientists
doing science and it's been hard to get
the scientists to use the infrastructure
or the infrastructure people to actually
own make it useful for scientists it's
starting to happen but again it's not
there yet so for all these reasons we
needed a plan B we needed to really help
them get from the raw data help them to
do it right help him to get the quality
in there and then we could think about
getting the publishing the link data at
the end we needed one of these so we
started develop it it was lens field so
what next few slides are kind of lens
fields design aims what I tried to
achieve achieved with it and what we're
trying to achieve as we go forward with
lens field and we want to kind of it
promote best practice we want to give
you default ways of doing the right
thing and making it really easy so for
that from that point of view we version
version control your data alongside the
scripts that you write in the code you
write to version two to process the data
so that you can go back at the end of
your PhD if you need to rerun any any
results you can go back and your work
and work out why your script produced
the results that way if you fail to
version control either stitching it back
together would be a nightmare and at the
same time you know so we build the whole
thing on top of mercurial which gives us
nice nice version control system and
waste of shame Decker it's a distributed
version control yeah M it might it might
well prove hippie I think that I think
the main thing that the distributed
version control gives us is it gives us
flexibility in the architectures we
build you know you can have centralized
systems using DC via CBS but you don't
need to and so we've got at the moment
we have a really simple abstraction that
just allows you a snapshot which takes
you through an interactive query of
making sure everything's either under
version control or ignored and it's all
clean and then we have backup which does
a push to a central server so we can
have so for example if you have a
directory at some court raw data that
you want sort in a version control that
then maybe there's something you run a
simulation and takes two days daenerys
100 gigabytes of data you don't want to
put that in your version
would you know you have all the
instructions to recreate that data you
would go through in the step and say do
not snap drawn or do not back up exactly
on the other hand if that if it was an
indeterminate process you'd need to know
what that run was and you might want to
keep it yeah absolutely you've got the
flexibility there and the backup which
pushes all of your changes to a backup
server and that's the other thing that
then mercurial can give us we don't have
to have one central server you can push
backups to one server you can share the
data by pushing it to another on the
data processing side I've mentioned this
build abstraction and I really wanted to
focus on describing what was describing
what the process was rather than
scripting what was meant to be happening
and especially because we're not going
to be able to wrap everything from the
start with this system you know half
these programs are running on a Fortran
that only runs on the vax down the hall
and half of them is running with little
scripts in whatever language that the
scientists learn when they were doing
their PhD and and so on and so on so it
was really important that whilst we do
want to automate as much as we can the
initially we just had to be able to
describe it that's that you know that's
the minimum when it comes to the link
data we were just aware that it had to
go out it had to be kind of invisible on
day one we had to do it automatically as
far as possible we've put a lot of
effort into writing converters from
those vernacular formats into into CML
and from there into RDF against the owl
ontology is that we've got through that
we're promoting the adoption of these
standard otology so there will be a
corpus of data out there basically it's
minimizing the cost the scientists of
producing this stuff we had a big
non-functional requirement though as
well which is that we're trying to get a
new tool and we're trying to get people
to adopt it and scientists are fairly
resistant to any kind of new product
productivity tool with it there doesn't
give them productivity and that the
whole point of this is it has to reward
the first user on the first day they use
it there was no way we could say use
this for a month and then you'll find
your phone
better than having vision controlled use
your data there was no way we could say
well use this because when a hundred of
you are using it you'll find you can all
share your data it'll be cool and and
the real thing that we found that people
went for in this kind of zero point
benefit is the idea of having your data
and process backed up is very attractive
and the other thing will come onto is
that we can do some fairly fast
visualization that they're not able to
do well they don't regularly do at the
moment we can make that very easy for
the other part of it was that them to
try and reduce the barriers to adoption
with the tool it meant that we couldn't
really get the users to come to the tool
we needed to take the tool to the users
and that meant running on the
workstations there's an awful lot of
info data processing happens on the
workstation still or even if it's
happening in it in the cloud or in a
distributed cluster it's being launched
from the workstations and that's where
the data management happens and so yes
we've made it made it we've made it as a
workstation running out just to come on
to the compulsory a bit of layer cake as
I've mentioned there's some version
control which is happening in mercurial
m and there's a whole load of Java
libraries we have linked data RDF
infrastructure is all in a package
called sesame that provides a kind of
primary abstraction and the triple store
and all this kind of stuff and we have a
wealth of functionality in existing Java
libraries the chemical information
community have been primarily in Java
through the chemical development toolkit
cdk and with jumbo and with a lot of
other developments so we couldn't
abandon that that legacy the rest of
lens field is built in a language called
closure which is a lisp that runs on the
JVM it's a functional lisp and the real
my first reason for choosing it was that
it was functional I had this brief
dalliance with her line last year but
the lack of Java interoperability it was
clear that that wasn't going anywhere
although the distributed computing model
in Erlang was absolutely fantastic
so might better my belief in about the
functional programming languages is that
as multi-core as basically this computer
scale with cause now instead of as
processor speed it's going to be
important if you want to make this stuff
accessible to programmers who aren't
like the top 2% that you've got to have
an abstraction that stops them hanging
themselves it's why memory management is
why Java and.net took off and I think
functional languages are going to be
what helped multi-core take off so it
isn't really that I've got a big thing
about functional languages I didn't Lee
a CS degree and do right no Haskell's
also strongly type which is you know so
last year and so closures also dynamics
as it helps us stay cool and another
feature of closure being a lisp is
something that I'm not going to claim
any great expertise in but it has had
some interesting properties that were
hoping to in the in the CODIS data kind
of environment that we've taken some
advantage of in the in the built
structure so diagrammatically the way
you describe one of these data processes
this describes a build we constructed
for a chemical engineering collaboration
we had in the University and they'd done
some Gaussian calculations stretcher
determination and thermochemistry
calculations they got a lot of output
files from that process they came along
with 200 output files and we stick this
together in a few minutes and did the
data processing were able to kind of
jump through their data really fast what
we want is we say well what we want is
to visualize this stuff in a web
application this link data application
to do that we need some rgf and we
probably need some pretty pictures to
get the pretty pictures we need the CML
to get the art if we need the CML as
well and to get the CML we use a
converter and these raw converters we
had in place already which was fortunate
but it wouldn't be hard to kind of do
these do these kind of migrations
incidentally said the way you express
the builders in terms of these pink
arrows you are you're a function of your
precursor and the function that produces
you and the data conversion isn't isn't
the order of it you don't care about as
a user
the system works out the order and kind
of evaluate suit for you what it looks
like in in in lens Field it looks like
this I'm not going to go into the syntax
it doesn't really matter except to point
out there isn't an awful lot of it
that's the main thing and this isn't a
file that gets passed and read in and
then runs a system this is the program
and that's been the the neat thing about
having these this macro based language
around we didn't have to write a
separate component to read the stuff
yeah yeah yeah so are strongly strongly
inspired by making us so the way these
things these things get expanded is the
kind of that source macro expands out to
a set of files it's on the left hand
side with the gaussian files and the CML
product gets expanded out to a set of
output files from the input files and
the conversion as the conversions run
they can start to put their processing
information into this build structure so
the build stretch is kind of a stretch
is getting expanded and enriched as you
go along files might get dropped out if
it's a repeat run like make you don't
have to redo redo files they're already
built the point of keeping that log
information is that by the end of the
room when everything's been evaluated
we've got all of the provenance of the
run as it happened as well as the data
that we've got from the files so we've
got this and using the URIs it's all
it's all mashable up and so we've got a
complete build stretch we've got a
complete record of what of the process
and we can convert that to rdf and we
can drop it out into the triple stores
bar we're going to skip very quickly
through the oh so mighty interest is how
the workflows automagically yep by a
leading certain aspects of the workflows
specific accent aspects and generalizing
yep
to go from HTML page to a PDF page
that's generic rule that just calls or
whatever application yep and then when
you describe your workflow you say I
have an HTML and yeah we haven't we have
an auto conf that generates can generate
this we can you know we have we can have
a registry of converters and you can
build up all of the paths and you can do
path optimization in all this kind of
stuff and for standard flows that's
great if somebody had needed to do some
heuristic calculation on the CML then
all of a sudden the auto conf isn't so
valuable so it's something we've done
because it was interesting so I've shown
kind of just straightforward data
conversion straightforward may we can do
other things as well and we have a
system called Crystal Eye which I
possibly don't have time to show what
should I show it anyway ok so what
crystal I does is it then it crawls the
scholarly publication literature and it
looks for crystallography information
follows links tries to work out the link
text follows it to the crystallography
files so for example here we've got acta
crystallographica re which is fairly
unique type of publication it only deals
in publishing crystallography reports
and but it shows that we can drill down
we can have a look at the structures
that were published an article in that
issue of the publication we've
compressed a little bit there we can go
and see a larger report of it if it
loads know yet though again there we go
and we've got these neat visualizations
allowed to spin the molecules around
when they when the laptops working this
is the work of Nick day who's also
working on this lens fuel system there
and this was just to say that
we'll be moving the publication crawling
and grubbing the supporting information
and we'll be moving this under
lanceville the next few months believe
it has judge the journals the seed yeah
and it works through the articles I mean
this is admittedly a really brittle
system it's also one of these things
where we're adding some frosting on the
top we really hope the publishers are
going to start doing this soon we'd
really start to start having data feeds
in at and we'd like to start them them
to start publishing you know Ari is a
way of describing a boundary ransom
resources on the on the kind of the
global graph in RDF I haven't seen what
plus do most probably not most probably
the semantic publication of saying
here's the supporting information isn't
and the reason for that is that M you
don't want to irritate your authors
that's one of the big things about being
a publisher the people who are doing the
best and this that we've built this
system to harvest nature chemistry which
is a new public new title and very very
good publishing their data they go to a
lot of efforts to get it into the right
formats they publish CML right off the
web page and they're not quite Samantha
fully semantic yet they've got bits of
rdfa for the bibliographic but that
takes such an amount of manual effort
for them to do that because they can't
demand that their authors do it all the
time so that they're the best but it
really does require am getting getting
data publication into the into the
authoring workflow because really the
authors probably lost that those
molecular representations NMR spectra
regularly aren't recorded in open
standards and submitted with the journal
article they'll print them out paste
them into their book and so on and so on
and we can also use it for EM for
automating our natural language
pipelines as well this is the Oscar
opsin tool that Nicko showed earlier we
can use it to for example download we
can use lens vil for example to download
pubchem abstracts grab out the structure
and it's
asians and reaction annotations convert
them into convert them into the standard
formats into a li f and then visualize
them as well this is just an example of
the visualizations that people found
really useful it's very simple but the
ability to be able to throw in your data
as soon as it comes out to see a list of
all your data and then to be able to
come up and have a quick look at the 3d
representation and that all of the
parameters is really a strong win on the
kind of day one win so just as a summary
and what we've read what I think we've
managed to achieve already with lens
field is that we've got this idea that
we've got this day one win the
scientists we mainly through backup and
visualization it was a matter of seeing
what what caught with people and the
long-term benefits through having that
provenance information and the version
control the link data publication
they're going to be the longer term ones
so what I hope we've managed to show you
in our hour is that then there are
chemical markup languages like c ml and
pml available we've got a whole RDF tool
stack for turning these into semantic
data turning them into linked data on
the web and the ontology to really add a
new and nascent thing but there's some
traction there and that's this is going
places and that we're moving into the
publication as well with tools like
Lancefield we're supporting that a lot
better but the reason we're doing all of
this is that data availability is still
a problem in chemistry and that's the
one we hope to crack these are the folks
that worked on the tools we've been
sharing today and on the ontology and
these are sort of our collaborators well
so many thanks to all of those thank you
very much
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>