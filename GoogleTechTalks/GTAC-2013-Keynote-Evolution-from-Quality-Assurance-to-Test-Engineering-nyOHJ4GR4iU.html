<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2013 Keynote: Evolution from Quality Assurance to Test Engineering | Coder Coacher - Coaching Coders</title><meta content="GTAC 2013 Keynote: Evolution from Quality Assurance to Test Engineering - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2013 Keynote: Evolution from Quality Assurance to Test Engineering</b></h2><h5 class="post__date">2013-04-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/nyOHJ4GR4iU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">with that I'd like to kick it off and
hand it off to Ari so all right thank
you thanks very much Tony for the for
the introduction so we are four minutes
ahead of schedule I am going to do my
best to fix that I want to welcome all
of you guys to New York to Google New
York I am from the from the New York
office here I'm part of the test and
engineering organization specifically
with ads but before I get started I
really want to give a shout out to our
folks from Boston any of you from Boston
our prayers and thoughts are with you
guys as you go through this as the city
and me personally as the city that went
through 9/11 we completely understand
the pain that everybody is going through
and has to endure we're right there with
you our thoughts are with you so I'd
like to start by telling you guys a
story I hope you'll bear with me there's
quite a few stories in my in my
presentation my story starts in the
mid-1990s right um just to give you guys
some context about the mid-1990s here's
what they look like music was
distributed on discs you literally had
to carry a pile of disks around Wi-Fi
didn't exist right so if you wanted to
connect to the internet you had to find
a cable if you were lucky you found one
of those blue ones because that might
actually give you reasonable
connectivity if you were unlucky you had
one of those beasts over there called a
modem that connected you to the Internet
you actually had to dial up to the
internet you had to wait 45 seconds
before you had connectivity if you're
even luckier you had a cell phone on you
might actually have to get straps and
strap it onto your back because that's
what that thing looked like you had to
carry it around right um and that's what
a laptop looked like they had tiny
little screens and they had these bricks
I guess they still have bricks today but
they had tiny little screens if you were
lucky might actually be in color Google
at the time which is the figment of
imagination and a couple of grad
students and here's actually what they
looked like way back when for me I was a
software developer I've been a software
developer my entire career when I
graduated from
software development is quite simple you
spend a couple of months collecting
requirements from your customer you
write them down in books this big then
you spend a couple more months banging
out software right then you take all
that software you take the requirements
you give it to some other group and that
group then spends a few months
validating whether the software that you
built matches the requirements that were
written on many months ago the fact that
something might have actually changed in
those few months is completely
irrelevant right you got the
requirements you build some software and
if it matches and matches doesn't match
doesn't match we had a customer
financial services that wanted to build
a web-based application pretty advanced
at the time right um in order to take
some data that they had this particular
company invested about a hundred years
building up their intellectual property
much of it sat on 3x5 index cards so the
project was to take data off of 3x5
index cards stick it on to a database
and then distribute it to the world over
the web there were I don't want to say
risk-averse because I they were building
an application that was meant to be
deployed on the web but they didn't
understand what sort of application they
wanted to build we went up to them and
asked them okay what do you want what do
you want us to build we're happy to
build it for you tell us your
requirements they turned around we said
we don't really know so we had it we had
a project manager at the time who said
maybe we should try this thing called
rapid iterations to try to help the
customer understand we asked them what
do you mean by that what our rapid
iterations right um he said well let's
build the application let's do a build
and deploy to some sort of QA or our
user acceptance environment on a weekly
basis we'll show it to the customer on a
weekly basis the customer will tell us
whether they think it's reasonable or
not reasonable and then we'll make
adjustments along the way this literally
blew our minds right what are you
talking about how is it that we're going
to possibly start writing software
without volumes of documentation how are
we going to do Quality Assurance how are
we going to know that we're doing the
right thing the guy said don't worry
about it let's just try it we'll see
what happens all of us thought this was
a pretty neat idea so we we decided to
go for it right we decided the Thursday
night would be build night nobody leaves
the office until the build is done on
Thursday night Friday morning the QA
group comes in validates it Monday we
show it to the customer customer tells
us we're on the right track or not
repeat sounded pretty easy right so we
started we Bank cut some code out for a
week we get there on Thursday afternoon
four or five o'clock we said okay let's
just start doing a build let's see what
happens to make a long story short we
were there until Friday morning trying
to get the environment working right
it's a long night took us a couple hours
just to get the code to compile all
right the first time we actually built
the thing then it took us a couple more
hours to get it working in the
environment right on the environment
that we wanted to deploy into didn't
match the environment that we were
developing against all sorts of crazy
stuff right so finally Friday we sleep
Monday we showed to customer customers
happy all the application did was log in
log out how much could we actually get
done in a week customer was able to log
in customer was able to log out thumbs
up we're all good to go all right second
Monday morning we as an engineering
group got together and said look if
we're going to go through this every
single week we're never going to get
done right actually the main motivation
is that we all wanted to go out on
Thursday nights like we had lives and
this thing was getting in the way of our
lives right so we wanted to go out
Thursday night what are we going to do
to make sure that this week is not going
to be a disaster we thought okay maybe
we do a trial run on Wednesday we did a
trial run on Wednesday we didn't go out
on Wednesday night we were there until
midnight getting the thing to run but
Thursday night we had a good time wasn't
so bad we only had to clean up the mess
that we made on Thursday rather than
cleaning up a week's worth of mess so
you can see where this is going right
all of you guys um understand what
software development looks like today
rapid in durations continuous builds all
that good stuff right but at the time it
really felt like we were pioneers in a
brand-new industry right
we ended up and you guys see you guys
see the end story behind all of this
right um we ended up launching the
product amazingly the product is still
there today hopefully all the code that
I wrote is long
gone four or five times over that's what
it's going to take to read the thing out
but the product is still there today the
customer is extremely happy but as
importantly by the end of the project we
ended up with an extremely high
development velocity from a test
perspective because we ended up
automating the tests we end up
automating the builds we end up
automating the environment the
environment creation that was necessary
to deploy our code we ended up with an
extremely high development in velocity
at the end the only quality we had to
worry about was for the features that we
developed in that particular cycle
because all of our regression made sure
that we didn't break anything in the
past right and we also ended up with a
whole bunch of artifacts right a lots of
small tests a collection of medium-size
tests and handful of manual large tests
that we had to run because browser
automation and certainly environment
virtualization didn't exist at the time
to enable us to automate browsers but we
learned a couple of things first of all
making testing easy so we as developers
could do it was really really really
important right because then we could
build a feature write a test for it
and be done we didn't have to wait for
somebody to come along at a later time
to test it for us also what we learned
is if you hold us developers accountable
for the pain associated with building
deploying and testing we would do the
right thing from the beginning rather
than waiting for somebody else to come
and clean up our mess right fortunately
all the stuff that we built for this we
were skunkworks shell scripts and stuff
like that all the artifacts are long
gone right the artifacts that we built
were unique for that particular
environment there's only really one kind
of browser available at the time really
one kind of operating system that this
was deployed onto so the artifacts that
we built
weren't really useful in the long term
they were useful for that particular
project but the thought process that
went into my mind something clicked
right I'm just like it clicked with many
of you guys this is the right way to
develop software right so we really felt
like we were pioneers at the time and
it's a feeling that that I think I share
with many of you guys that are
in the audience right and many of you
guys that are watching this presentation
we were at the cusp all of us right
we're at the cusp of something that was
pretty cool the industry changed a whole
new discipline within computer science
evolved something that we within Google
called test engineering and it
proliferated it really proliferated over
the last 15 years I wasn't the only one
excuse me I wasn't the only one many of
you guys went through the same exact
evolution at the same time luckily for
me and for all of us many of you also
built artifacts that could be reused by
other projects right think of all the
technologies that exist today that we
have available to us to make all of this
stuff work right a continuous build
engine unit test frameworks all sorts of
desktop and browser automation
technologies virtualization which
enables us to build really cheap
environments and have those environments
be you can bring up an instantiate
invite you can instantiate an
environment you can hydrate it with data
you can run some tests against it then
you can destroy that environment at a
relatively low cost right think about
all the different processes that are
available to us today best practices
agile right dogfooding something that we
do extensively within Google we have an
app we have the opportunity today to
take an application deploy it to
production and make that new version
available to a small subset of our
audience maybe trusted users maybe 1%
maybe only users that are internal to
your company that can tolerate issues
right within Google we call that
dogfooding
and so we open up our application at
least new features in the application to
a subset of our audience before we turn
around and expose it to the to the
external world that way if there are
issues we know about them and we expose
them only to a limited audience there
also
all sorts of knowledge and information
sharing all of you guys are here at this
conference to learn right the
conference's exist about this particular
topic that's great this is not the only
conference there is lots of them lots of
mechanisms for information sharing today
right books videos all sorts of other
things right industry many companies
like Google many of your companies
have dedicated organizations that focus
on test engineering and also focus on
moving that industry forward right uh
but probably something that I'm most
proud of and and we're going to hear
from some of these folks today there's
also an entire realm within academia
that's focused on test engineering today
so not only are we as industry as
members of the industry focusing on this
particular topic
there's now a fountain of information a
fountain of research a fountain of
students right that are coming out of
universities having worked on test
engineering full-time and we've invited
several members of the academia to come
and talk at G tach this year to help us
understand the kind of research the
count of groundbreaking research that's
happening in universities and other
other institutions like that that enable
the next generation the next wave of
information the next wave of test
engineering to happen so just like I
felt like a pioneer many many years ago
oh so last century right um we've
evolved many of you guys have felt the
same exact thing right many of you guys
were at the forefront of this particular
industry we've evolved as an industry
think like I think myself that all the
tools that I created for that particular
project are long gone
right if we had to deal with only the
tools that that we built for that
particular project we'd never be here
today so I want to thank the many of you
that also participated in this
particular industry to take the
knowledge to take the learnings and
build the tools we've really evolved
right we've gone from pioneers to a
well-defined
industry that we're all proud to be
members of today so G Tech is a little
bit about sharing as well right what I'd
like to do is share about how Google
goes about doing test engineering right
we're also going to hear there's several
talks including the one immediately
after mine where you guys get the
opportunity to share about what test
engineering looks like about your
particular companies I'd love to hear
that but I'll go first I'll share what
Google is doing about test engineering
some of the best practices that we have
in place
des and some of the challenges that we
face today that I'd love to see softer
first a little bit of background about
what Google looks like it is a company
Google Speed Google scale we have
roughly 15,000 engineers and roughly 40
different offices around the world at
any given time it's about 5,000 projects
under active development we have a
single monolithic source code tree with
all sorts of mixed language mixed code
mixed stuff 100 million lines of code
roughly 50% of which roughly half of it
changes on a monthly basis so it's very
active right we spend a lot of time
actually curating the source code we do
a lot of deletes within Google we don't
lift things rot development happens at
one branch right there's head and
there's head that's it that's all we got
all submissions are done at head all
releases are done at head all builds are
done at head all tests are run at head
everything runs at head as a result we
we one of the benefits is that is that
we don't have to support a thousand
different versions of libraries for
different applications everything runs
at hit and I'll talk about the
consequences of that in a little while
all the builds have done from source
right we actually do not pull in any
binary artifacts into our binaries every
single build straight to the source
right so even a small build actually in
my first day at Google our first week at
Google I built a simple web server that
just responded to her to a request I
built the thing I was expecting the
thing to build have a couple of data
files a couple of dot class files done
right next thing I know there's a
hundred thousand files that go into
building this thing right so even the
smallest build as a result of me
building that website I just want to
give a little bit of context to that I
picked up advanced monitoring I picked
up advanced instrumentation I picked up
advanced metrics a lot of infrastructure
that went into that little web server
just to respond to a simple request
right but that means that even the
smallest builds can have roughly a
hundred thousand artifacts associated
with them it's pretty complicated on a
given day we have about fifty five
hundred submissions to the source code
repository roughly average
that's roughly 20 per second if we were
doing a verge development across the 24
hour period but we're not so at peak
which roughly corresponds to about three
minutes before people go home we have 60
plus submits per second
I'll per minute roughly one per second
that translates to about a hundred
million test cases run per day so that's
what Google looks like so I'd like to
spend a couple of minutes talking about
what test engineering at Google looks
like there's really two parts to that
they're the tools and the people but
first a little bit of context Google's
focus is really about engineering
productivity as a whole the test
engineering group or the discipline is
part of this greater discipline so as
part of engineering productivity we
focus on three things
developer infrastructure which is really
about building tools that every single
developer within Google uses it's about
test engineering which primarily focuses
on test automation but also is involved
in in lots of different areas and I'll
show you examples along the way and also
release engineering which focuses on
building tools and and optimize
processes repeatable processes for
getting information at the door but our
mission is pretty simple we want to
develop it build it test it release it
and measure it better faster stronger
throw in all your all your favorite
superlatives there so first let me start
with the build system itself right the
longer the better that's kind of a bad
thing if you look for if you look at
things from a developer productivity we
want to avoid this scenario so Google's
build system is really optimized for a
whole bunch of different things but
primarily performance right so what we
do for performance is parallel eyes as
much as we can all the builds first
thing we do when somebody does a build
is analyze all the dependencies what can
run in parallel then we send those
requests off to our cloud which it's got
thousands and thousands and thousands
machines just standing by waiting to
receive orders to build information
massive performance right we can get to
roughly 60 files we average about 57
actions every second on a 24-hour basis
through our through our cloud we also
optimize for correctness
McLean is a bug right you should never
have to make clean if you have to make
clean there's a bug in the build system
there's something that's not defined
there's something that's not declared
there's something that's incorrect right
our build system imposes that
correctness on everything that we do
increment ality also fits in right we
want to do the least amount of work that
has two consequences first we want to
figure out the least amount of work that
needs to be done for that particular
developer that's pretty well understood
and all the build systems around the
industry do that but we also want to
make sure that across Google as an
enterprise we do the least amount of
work that means if my colleagues over in
the other side of the world let's say
Mountain View have done a build at a
particular revision and I then do a
build of that same revision and the same
file same target here in New York I
should not have to repeat my comp there
the compilation they built it I should
be able to reuse it so our cloud
actually caches the output of all the
builds and I benefit from somebody
else's build of the same target at that
same revision we also want to simplify
use by developers so our build system is
multifunctional we I can build a file I
can run that particular target I can
test that target I can run source code
coverage for that particular target and
then I can do a dependency analysis on
that particular target all using the
same commands just by changing arguments
the same basic command right I don't
have to go and do crazy things and the
same applies for multi-language I as a
builder don't have to know whether it's
a Java target a Python target a shell
target a C++ target a JavaScript target
I just know if I want to just run the
test against it I run that target and
run tests against it pretty simple
pretty powerful there's a lots of talks
and I've provided these URLs that go
into a lot more detail about our build
system feel free to go check them out
some performance numbers are about our
build system this talks about
incremental 'ti about 90% of our builds
run less than 20 seconds it's only when
you get in the 90th percentile and
higher door builds take a lot longer
than that because the caching doesn't
kick in you're building something from
scratch
nobody else built our cash anywhere from
89% to 98% effective depending on on
what you want to do there are actually
two levels of caches one on my local
workstation so if I - if I run two
builds in a row my second build
leverages the first build I don't have
to go back to to the cloud to download
all that information but let's say the
first time around I run my build 89% of
the time I will be leveraging off of
somebody else's build the great part and
I'll talk about this at the continuous
build engine our continuous build engine
is sitting there building stuff at every
single change right i as an individual
developer can take advantage of the
cache that's been primed by our
continuous build system build
performance over time has also decreased
even though the amount of code we've
typically had to build has increased
this is a direct result of test
engineering or just straight-up
engineering against the build system to
make it more efficient better analysis
of dependencies better analysis of
parallelization but also just plain old
faster computers that we can run our
builds against we talked a little bit
about test execution just like builds
tests also happen in extreme power so if
a particular test suite has 4 test cases
as this one does this example does will
run all four them in parallel more so
will actually start the execution of
tests as soon as the build for that
particular test is done right so test1
test2 test3 in test for need to be built
if tests for the build for test 4 gets
done first the execution for test 4 will
will start first
if test 4 fails we don't even bother
waiting for the other three to report in
test 4 fails therefore the entire test
suite fails we report that right away
right so the developer can start taking
action this might actually happen before
test1 test2 and test3 even finish
building our continuous build system
ties these two tools together widely
adopted within Google there's one
continuous build system that accounts
for the vast majority of all the builds
that we have within Google this is what
is test this is what its user interface
looks like there's two key components of
this continuous build system that I'd
like to talk about first is the before
and after submit test abilities and the
second one is about flaky tests and the
consequences thereof the first one is
actually really key like I learned in my
first project if you make it really easy
for developers to run tests and hold
them accountable to that they will run
tests and check code that's actually
correct into the repository you don't
have to find out about it after so our
continuous build system has what's known
as a pre submit I can take some code
that I'm sitting and editing on my
workstation hand it over to the
continuous build environment and say
take this merge it in with the last
known green and tell me whether I'm
going to break the build or not that's
great right I don't have to set up my
workstation in any special way to make
tests running possible is just a generic
workstation right it's going to be
running my code and changes are going to
be running the same exact environment
that the continuous build engine uses to
determine whether tests are correct or
not correct so that's fantastic we
actually take that to an even even more
extreme level as soon as code is
submitted for a code review within
Google the code review process not only
sends it to a human being another human
being to go take a look at but it also
sends it to the continuous build system
to evaluate we don't wait for the
developer to ask the continuous build
system whether the code is okay we just
do it proactively so roughly the same
amount of time likely maybe even faster
they'll hear from their peer in terms of
a code review your code looks good your
code is horrible throw it out start from
scratch or anywhere in between it also
hear back from the continuous build
system your tests pass your tests fail
anywhere in between what tests fail what
the logs are right they'll get
notification they'll also get a great
email and this was this was a tool that
was built by by my colleagues over in
Zurich they'll get an email that says
your incremental code that you just
added only has 23 percent test coverage
for example I pick 23 percent out of a
hat right so we do incremental test
coverage a source code coverage right
there dynamically on behalf of the user
right in an attempt and I'll be nice in
an attempt to notify the developer about
what level of coverage their incremental
code has
right okay I'll stop being nice it's
really about shaming them into believing
that they should be writing more tests
for stuff that they're writing but right
they get to stuff all proactively it's
right there and the code reviewer also
sees it so here's an incentive for the
developers right right off the bat to do
the right thing our continuous build
system also handles fine-grained
dependencies that's one way of lowering
compute costs for us right like I said
100 million tests a day that's pretty
expensive to run all of them we only run
the tests that are actually necessary we
know all the dependencies through the
build system we know what test is
impacted by what particular change will
only run the tests that are appropriate
for that particular change flaky tests
endless pain for us flaky tests flaky
test is defined as a test that is
non-deterministic given a fixed
environment particular revision of code
particular set of changes you run the
test numerous times and the test gives
you different results right lots of
possibilities for what causes flaky
tests right environmental issues code
issues test code issues itself right etc
all right you can imagine if if there's
a routine that returns a random number
and the unit tests that that tested
tries to check that the number is below
a particular threshold will maybe you
know if it's testing a 50/50
it will pass half the time and fail half
the time so non-deterministic flaky
tests are shown in this particular
display by the scattering of reds by the
way the the the solid line of red that's
a that's a failure that was submitted by
a particular user
that's the second-to-last row from the
bottom but the miscellaneous Reds that
you see there are all flaky tests
they're tests that are non-deterministic
why is this a problem well first of all
if a developer is making a change right
and they submit their their change into
the pre submit queue and the pre-summit
queue fails the developer does not know
whether it was the change that they
introduced or if it was a flaky test
that caused that particular failure
right worst case they go and they start
trying to find out what's going on
within their within their code that
caused this particular problem we would
love
to be a hundred percent correct around
test passing and failing because that
would enable us to go to the next level
of automation if we detect a failure we
reject we roll back that submit that
would be fantastic right but we can't do
that because of flaky tests so we do
three things our build system keeps
track of flaky information and that
particular tool was built by my
colleagues over in London right to
measure understand and present back to
the developers the level of flakiness
associated with a particular test right
it's pretty easy to see the different
patterns with flaky tests as opposed to
real failures so we do statistical
analysis and try to come up with some
particular numbers right we try to get
the developers to fix them so by letting
them know their tests are flaky by
shaming them into letting them know that
their tests are flaky we get the
developers to fix them if we can't we do
the the least popular thing it was just
retry tests - over and over and over
again until we determine that the test
is flaky or we we determined that the
test is is is okay
we look for a pass as you can imagine as
I showed before our submits into the
source code repository are not uniform
over 24-hour period so we could take
advantage of the overnight hours to do
incremental source code computation for
all of our projects we also do extra
analysis of flaky tests overnight so
that's our continuous build system some
statistics about it number of submits
per day SEL within Google is called the
change list
it's corresponds to a particular change
that was committed into our repository
linear growth over time that's fantastic
we hire more developers more developers
write more code we have more submits
over time the good news is that those
developers are not only writing code the
writing tests so the number of seconds
that we need to use to compute code test
runs for every single submit is also
going up over time that's fantastic
right developers are writing more tests
for any given change we run more tests
we're happy to do that but you know I
have kids that are going through
geometry today so I understand this when
you take a straight line x straight line
you end up with a quadratic quadratics
are bad
right there is no way even with all the
data centers that Google has there is no
way that we at Google can keep up with
quadratic growth across all of our test
cases right I remember we actually had a
conversation our developer tools group
had a conversation with our data center
people they went up to them and they
said we need more machines to run tests
the data center people being data center
people were quite blunt they said you
have a choice to make
you can run your tests we have enough
computers to run your tests we'll have
to shut down all the production
applications you guys okay with that
test our production what you guys want
to do yeah so that's where we are today
we we are heavily applying engineering
to solve a brute-force problem this is a
theme that comes up over and over and
over again and I've seen this over the
last 15 or 20 years when you can apply
brute force the easiest thing to do is
apply brute force but brute force times
out after some time and time you just
can't scale right you have to start
thinking about better ways of doing
things and that's what we're doing today
so maybe we don't run the test at every
single change we run them periodically
and then we'll run all sorts of
interesting culprit finding to try to
figure out where in between that the
breakage was there's actually a talk
tomorrow about culprit finding within
Google so let me talk a little bit about
the people the people that make this all
happen within Google there are two roles
se TS and tes within the test
engineering group at Google we within
test engineering do a handful of things
we create processes and plans that would
help test the particular product we we
think of best ways to maximize coverage
but we also are highly involved in
building a lot of the tools that that I
talked about in fact many of the tools
that I demonstrated were built by the
test engineering folks here at Google
I'll show you guys some examples and
there are some links that you can go to
to to read more about this first one
probably making up roughly 50% of what
we do is really focus on flaky tests
like I described earlier the problems
with flaky tests right there they're a
huge cost and they're huge time sink how
do you eliminate flaky tests you build
hermetic environment so you can run your
tests in
hermetic environment ourself isolated
environments instantiated tests are run
against them and then deleted the the
more hermetic they are the less external
dependencies they are the better the
cheaper it is to create that environment
the faster the test will run right so a
lot of the engineering that we do is
about building consistent reproducible
hermetic environments so we can run
multiple of those environments and run
tests at the same time right for a
typical application we might be running
five ten fifteen twenty instances of
that application at any given time in
parallel with different revisions of
source code being tested against that
application that's pretty common right
we need separate environments that don't
interact with one another and don't
interfere with all the testing that goes
on there so that's roughly 50% of time
what we do and there's a link over there
that has more information
another one is metrics if you can't
measure it you can't really understand
it or control it right so we try to to
measure lots of interesting things we're
Google we love data we collect lots of
interesting data and presented to
developers as a way again to get the
engineering versus to do the right thing
and when I talk about developers I
actually mean us because we're
developers as well so we collect metrics
for test engineering in addition to
collecting metrics on the developer
engineers are the the the product
engineers the developers on product
first one and I'll kind of go around the
slides starting with the upper left-hand
corner this is positive reinforcement
about incremental source code coverage
this was built by my colleagues in
London they had this idea that for every
single submit we measure the incremental
source code coverage and if it's good
source code coverage we give credit to
the to the person who submitted it so
here it is we have these look at
displays scattered all around the
offices if any of you guys are going on
tours look around there's a whole
handful of them around the fourth floor
here they're constantly being refreshed
with update metrics so that's an example
of a kind of metric that we put up there
these these look up displays scroll
through lots of different information if
a person does a great job with
incremental source code coverage and
they submit a change that's got high
coverage they get their picture up there
it's right there in the middle
nice and big with lots of green around
it the group that this was deployed into
started a friendly competition of who
can get the highest green most number of
greens get their face up there as often
as possible
fantastic loving it this utility alone
raised source code coverage for four
applications by a significant amount
right bottom left hand corner is latency
metrics so we measure constant
performance of application make that
information available right we don't
wait until the application is in
production to find out that a
significant performance problems are
about to hit right we measure we show it
we move on well you have to show build
status and that's that green project X
status that passed today last time
standpoint not there are some groups who
shall remain nameless that don't adhere
to the or don't respond to the positive
reinforcement that's on the the top
left-hand corner we had a problem with a
particular group where non high-priority
bugs like the the P 3s and P fours or
piling up and the group was just not
motivated to go and fix them so we brain
start about this we tried all the
positive reinforcement giving them kudos
for fixing for fixing bugs rewarding
them recognizing them doing all the good
things didn't make a difference so we
went the other way public shame
humiliation we put this utility together
we made a black on purpose black as ugly
right with lots of red around it excusa
Tory put up the worst offender we're
sitting there in the morning a bunch of
us we're sitting there was nice and
quiet we're sitting there typing away
the next thing we know somebody comes in
and I'll give you the the g-rated
version right because this is you know
g'tok is a family-friendly sort of
conference why is my picture up there
comes up to us why is my picture up
there was a lot more colorful but
you have a lot of bugs what do I do to
get my picture from out from there go
fix your bugs the entire day we hear
furious typing lots of banging lots of
other choice words picture disappears by
late afternoon next thing we don't why
is my picture up there from the second
person who's up you have a lot of bugs
go fix them needless to say a few weeks
after that problem solved
so in a summary write for test
engineering we really look for creative
ways to introduce cultural change so
things are done correctly right up front
with our development groups so the
impact of test engineering and you know
we're not engaged with every single
product within Google there's only about
eight hundred nine about hundred of us
or so against the population of fifteen
thousand engineers so we're not engaged
with every single group but the groups
that we do engage with first of all are
really happy to have test engineering
support but we also have demonstra
for example google plus a hundred new
features released in the first 90 days
right it's roughly on average more than
a release a day search we can go from an
idea to production in 48 hours less than
48 hours and chrome the browser six
weeks regular schedule get really tester
so that's the impact of test engineering
I also want to pause at this point I
know Tony addressed this but really it's
test engineering the discipline of test
engineering within Google that sponsors
this conference here in in year out
there's lots of volunteers from the test
engineering discipline that helped put
it together this year and the test
engineering discipline funds at year
over year so I want to say thank you to
to the various members that organized
this conference and the various
individuals and executives that form
test engineering but also made this
particular conference happen so now it's
your turn right over the next couple of
days we're going to hear many of your
presentations about what test
engineering at your particular companies
looks like maybe you call it something
different love to hear the names we'd
love to hear your best practices we'd
love to hear your challenges right what
can we learn from each other to improve
all of our product
so I have a question are we done are we
done with test engineering right
here we were 15 20 years ago we clearly
had a lot of work to do we were all
pioneers in a brand new space are we
done we kind of have awesome tools all
this stuff works we're able to develop
software so the question that I have are
we about coffee right do we need to
hunker down and drink more coffee and
continue engineering against Tenace
engineering or is it about beer do we
pick up some beer now Samuel Adams again
a shout out to my friends over in Boston
fantastic beer should we grab some beer
and go celebrate be done are we done
with this particular discipline I think
the answer is no as much as I would love
to get together and drink some beer it's
only about 10 o'clock in the morning
it's a little early we're not in Ireland
coffee's still good but we have a lot of
work to do because the world is changing
underneath us and that's really why many
of us are here today stake some
statistics from YouTube 72 hours of
video are uploaded every single minute
to YouTube for a billion hours of videos
are watched every single month so while
developers are sitting there watching
videos while they're waiting for the
build systems 2011 more than 1 trillion
views 140 views for every single person
on the earth that's even assuming that
every single person on the earth has
internet access right most interestingly
70% of the traffic comes from outside
the United States that's on the media
side on the mobile side 25% of traffic
to YouTube comes from mobile devices so
I started thinking about this and said
is this really a problem right that's
great it's great that YouTube has all
these fantastic statistics showing the
change is this a problem so I looked at
various different sources on the web the
source for this particular graph is up
there mobile versus desktop traffic
desktop is that 85% mobile is at 15%
we all apply the 80/20 rule I don't know
this looks to me like we're kind of done
we figured out the desktop we figured
out all of its issues with the desktop
we know how to mock out all of its
inputs we know how to mock out all of
its output we know how to analyze the
output we have image comparison we have
great tools for understanding if
applications are working well it looks
like we're done right then you know Eric
Schmidt former CEO current chairman of
Google and and his buddy Jared Cohen
wrote a book was actually just published
today I got my copy yesterday
fortunately I got a chance to read
through some of it called the new
digital age
that talks about some of the changes
that are underfoot and I'd like to tell
you a little bit about it the first
thing he says is there's only about two
billion people that are on the Internet
today
but he predicts that in the next few
years five billion more the incremental
will come online and he's seeing some
early indicators that this is true first
he visited Iraq in 2009 the first thing
he noticed in this war ravaged country
trash wasn't picked up reliable water
was not evenly available across the
country the first thing that they got up
and running one of the highest priority
items in Iraq was getting a mobile
network so they can exchange information
from one another one of the first things
that they got up and running like to
tell you about another story in in
Africa right again as he was traveling
the world in in the Congo fishermen
actually fisherwomen on used fish they
used to take their fish to the market if
they were lucky they sold them they were
unlucky the fish went bad they throw
them out using mobile devices they
actually leave the fish in the river
they catch the fish but they leave them
in the river until they have a customer
ready to pick them up they receive
notification on their mobile device they
take the fish out of the water they give
it straight to the customer no loss in
inventory maximize their opportunity and
this actually created a semi economy
there if somebody has fish that another
Fisher person once
they can now exchange it among
themselves using mobile technology lots
of interesting stories so are we done
again is Eric right so I started looking
to more data and and this one is again
the source is on the bottom 74% of the
developed world is already using the net
but only 26% of the developing world is
using the net and if you average it out
relative to population the average is
35% much closer to the developing world
graph the developing world line than the
developed world because there's a lot
more people in the developing world that
still have to get connected right so
let's revisit these graphs again looking
at them from different geographical
perspective let's take South Korea right
South Korea is highly advanced right
this story their desktop versus mobile
tells a very different story right much
closer to 60/40 than it is worldwide
let's take another step and look at
India the story in India is radically
different it's a lot more mobile stuff
going on then there is desktop stuff
right and this trend is only going to
continue growing so mobile is already
here today it's something that we have
to address today but there's lots of
challenges with media and mobile and
I'll run through them right platforms
tremendous amount of variability and
platforms lots of hardware lots of
different operating systems the
operating themselves may not be as
stable as you would love them to be
right and unlike on the web where I as
an application developer had complete
control of the stack top to bottom in my
production environment that's no longer
true on the mobile side right if I'm an
app developer I now have to deal with
other applications that are on the
device my application the operating
system all sorts of different
combinations and permutations are going
on right the hardware is different the
platforms are different input input
there's no longer just a keyboard and
mouse right there's tremendous amount of
variability and input these days across
all the different devices there are
cameras right there are GPS receivers
there's
touchscreen there's gestures there's
physical buttons there's voice
recognition you can tell the device what
to do on the output side right there's
all sorts of devices resolutions on
languages right all sorts of issues
around that also the network is no
longer consistent the same application
that's running on the web at least you
can assume that if the application is
running it can talk back home it managed
to get downloaded somehow offline is
getting better and better every single
day but really how many desktops are
offline right mobile that's no longer
true the same application that's running
can experience within minutes lots of
network no network medium network
network bandwidth network latency all
sorts of variability on the network side
that have to be dealt with in some sort
of an interesting way on the media side
the variability and qualities as high
codecs will this media work on this
particular device will it work well will
it Street will it be real-time how do I
understand video quality right any of
you have taken compressed videos and
looked at them image by image you'll see
that the image are radically not good
right um
the but that's okay because our brain
does a great job of taking a sequence of
them and putting them together sound our
ears are not as forgiving if there's a
pop in the noise right instantly our
brains will determine that this is bad
software delivery mechanisms have
changed we now have to again go back to
the world where we're downloading
software weight allowing the user to
download software right and and if we
want to release a new version we no
longer can do it instantaneously and
give access to all of our developers
right we have to wait for the developer
to download it so again we have to deal
with multiple different versions at the
same time worse there is now an
intermediary between our users and us
right the app stores are their security
privacy we have to go back to a world
where malware and viruses and all these
other things are a plague also device
sharing right is is at play so these are
all the challenges and again I feel like
we're pioneers again coming back to the
same feeling that I had when I started
my little adventure in the nineteen
I feel like we're about to begin on a
brand new adventure right we're all
pioneers in this brand-new industry
having to solve roughly similar problems
that we solve many many many moons ago
all of us collectively have to solve
them but in a brand new space and if we
think mobile and media are it today
right the next generation of devices is
upon us we've got one of these guys
right we have to wear how do you test
this it's a whole new challenge right
the number of inputs that are available
to this particular device are enormous
cars how do you test cars with all their
inputs and outputs and all the decisions
that they get to make and all the other
devices right so I'd like to close by
saying all of us are pioneers all of us
are in this adventure together
all right let's put on our thinking caps
let's share let's collaborate coffee
beer later but coffee now and figure out
what the next wave of test engineering
is going to look like for these new
devices both for the media and for the
mobile devices today and for the
next-generation devices tomorrow same
sort of adventure that many votes have
gone through starting on the desktop
let's take all of our thinking for the
desktop and apply it for all this new
world thank you very much
I hope you enjoyed this
Wow
that was uh was fantastic Ari thank you
thank you for that I learned some really
important things in this talk one is Ari
is going around taking pictures of all
their sleeping people sleeping at their
desk
you'll see that yeah okay so watch out
if you're sleeping he may take your
picture and the other thing I noticed at
this graph it is graphs is there's a
quadratic decrease in productivity at
Google right around January who would
figure this end of December January so
hopefully all of you got some really
good notes
you know I joke around about these
things but there are a lot of great
thoughts in here and a lot about the the
challenges that are going to be there
going forward and all the problems that
we need to solve that's why all of you
are here if you're not spending time
during the breaks talking to each other
you're not getting the most out of this
opportunity so we really want everybody
here to be talking everybody else so
please do that um two quick things we
have transcribers that are over in this
corner everybody's done a great job not
standing between where I am and over
there just be cognizant please don't do
that otherwise they can't get the
transcription correct and we also have
somebody signing over here which is
great so thank you so hopefully I just
didn't have some bad word or something
like that but that's thank you I grew up
near Gallaudet College</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>