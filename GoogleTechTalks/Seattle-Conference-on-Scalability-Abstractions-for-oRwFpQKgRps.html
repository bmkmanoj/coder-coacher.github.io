<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability: Abstractions for... | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability: Abstractions for... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability: Abstractions for...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/oRwFpQKgRps" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our first figure of the day is Jeff Dean
who's a keynote speaker jeff has been
with Google for the last few years and
he's built he's been an an architect of
pretty much every single system that we
have google in search and crawling and
indexing and advertising systems and
he's a Google fellow he's also from the
Seattle area he is a he did his PhD with
Craig chambers angeles PhD on on
compiler optimizations for lot scale for
object-oriented programming languages
one invite jeff so welcome everyone so
the plan is a lot of what you see and
use on a daily basis from google is our
products which are you know nice things
but what I'm gonna talk about today is
basically the underlying systems
infrastructure and also a little bit
about the underlying computing platform
so you can understand what sort of
computing systems we're building on top
of just to give you a flavor of what
sits beneath all these different
products and so I'm not really one for
mission statements but I actually like
this one because it's pretty broad it
means we'll never run out of ringing of
things to do so how can that be bad so
the question is if you're trying to take
all the world's information and organize
it what does that mean so if you think
about just the web there are today tens
of billions possibly hundreds of
billions of web pages in the world you
know each one is about 10 kilobytes of
data and that gives you on the order of
hundreds of terabytes of data that you
need to be able to organize and search
over quickly things like that and then
there's all the other kinds of data that
is both you know private data for
individual users things like email
things like broadcast media now online
videos online pictures so all those
kinds of things add up to an awful lot
of data
world and it's growing substantially and
most of its in digital form these days
but something that's not we'd like to
take stuff that's not in digital form
and make make it accessible in digital
form and the web web search although
that's our sort of starting point as a
company is just a tiny point tiny tiny
fraction of what we're trying to do we
we take this pretty seriously you know
we started with web search and then we
gradually added various other kinds of
things now we do a lot of things with
geographic data that's kind of an
interesting area these days with
satellite imagery and various kinds of
pictures of cities and so on we do a lot
of community oriented applications now
where we allow people to talk to other
people organize their own email search
their email and so on so there's lots of
things there and they all place
different kinds of demands on the
systems infrastructure they all have
slightly different requirements and so
on but the goal is basically to build
systems infrastructure that allows you
to quickly and rapidly with small teams
build some of these interesting products
so one thing we've certainly seen over
the years is that we seem to always need
more computers than we have and there
are several reasons for that one is as
traffic grows over time even if you
don't do anything else you need more
computational power just to handle more
and more requests the second thing is as
you increase the size of your index or
you index more kinds of documents or
things like that you need more computers
to deal with the increasing scope of
data for the same number of requests and
finally as you try to improve the
quality of your ranking algorithms you
try to apply more expensive ranking
algorithms on every query you need more
computational power just to perform
searches over that same amount of data
with better algorithms and so the
product of those things means that you
essentially need lots of computers and
you always seem to need more of them
than you have so the goal in terms of
our systems infrastructure is we want to
create a set of tools and
that allow sorry I put a button on the
back here that allow people to make it
easy to build products and we're heavily
focused on price performance we don't
care about the ultimate performance in a
single machine as I'll talk about in a
minute we want to make it easy to use
lots and lots of machines for people
that will enable us to build better
products we can have larger indices we
can update them more often we can have
more more responsive queries we can have
faster development cycles at all okay so
let me talk a little bit about the
hardware that underlies our computing
systems these days so the basic
philosophy we have is that we have
chosen to build on very low cost
commodity pcs that's where you got the
volumes of purchasing in the marketplace
the drive costs down so that you can buy
you know insane amounts of dis storage
for really cheap prices these days you
can get really fast computers and sit
under your desktop for you know not much
money and it's at that price point that
you get really good performance per
dollar and we just build lots of them so
in part that's because we don't really
care about how fast an individual
machine is you know if you can buy the
fastest machine today it's going to cost
you a significant premium to buying the
one that is just a little bit slower and
we'd rather buy the one that's just a
little bit slower because most of our
problems don't fit on a single machine
anyway so you already have to figure out
how to partition things across multiple
machines and we have lots of inherent
parallelism in most of our applications
so for example there's both across
requests parallel them so you're
handling many requests per second from
different users those are fairly easy to
paralyse across different machines and
you also have within request parallelism
where you can take a large document
large index of billions of documents and
partitioner didn't do many pieces of
millions of documents each each machine
can deal with a smaller index of you
know on the order of a million documents
and so there's parallels in both across
different requests and within the same
request so it's pretty easy to figure
out how to in our case
paralyzed computations across different
machines which means you just want lots
of performance per dollar the other
thing is you could spend more on sort of
more reliable machines things you know
higher-end servers tend to have features
like rated disks redundant power
supplies things like that which are very
nice features to have but you pay a lot
of money for those features and at our
scale even machines that are
ultra-reliable that have these things
these features are going to fail anyway
so you already have to deal with this in
software in some form it's just a matter
of how often do you have to deal with it
and if i can buy twice as many machines
versus buying you know the same number
of half the number of reliable machines
that much rather have twice as many
machines because they're not you know
half the reliability okay some
gratuitous pictures this is when I when
Larry and Sergey started the company I
started the project that it was a
research project to Stanford and their
their advisors apparently wouldn't buy
them any computers so what they would do
is I'd go down to the loading dock at
Stanford and volunteer to receive
shipments that other research groups had
ordered of machines other research
groups had ordered and volunteer to set
them up and then they would kind of live
on the float they'd kind of hold on to
it for a little longer and use it a
drawback with that is I think they had
like 10 computers and nine different
processor types and operating systems in
here which is a little more
heterogeneous than you might like but
clearly their lessons have played out
well in designing the first versions of
our machines so when we were first
starting out we needed machines quickly
and we decided that we would manufacture
our own machines because it was too
expensive to buy other ones so we would
buy motherboards and just drive and kind
of assemble the parts each one of these
trays has for motherboards so for
machines basically and eight deaths each
machine had two disks there's four in
the front kind of laid on top of the
motherboard and then some wires to kind
of
keep the keep them off the motherboards
and then there's a row of four that are
more neatly organized in the back
there's reset switches in the front and
they're all sitting on this tray and to
protect the tray from the motherboard
the motherboards from the tray there's a
thin layer of cork below the motherboard
so these were affectionately known as
cork boards they also had some cabling
issues which kind of so our next design
we decided we would omit the cork and we
might be better served by putting all
the connectors on the front of the
machine rather than snaked it back to
the back so this was around 2000 data
centers were interesting in those days
they lots of people would buy kind of
high-end Sun machines and put them in in
cages and data centers and data centers
charged by the square foot which was an
interesting pricing model so our
incentives as far as we could tell from
reading the contract were to pack as
many machines we possibly could in those
square feet and they didn't actually
charge you for power which was kind of
nice so we sometimes had to help them
out a little bit on their cooling so we
bought a little fan at Target eventually
another scale we had to require was
moving out of bankrupt data centers and
into new ones because the pricing model
seems to have not worked for a lot of
those data centers so you know you get
pretty good at building pretty large
scale clusters and deploying them
rapidly basically you can pre wire all
these things in a rack level the racks
are on wheels you just kind of wheel
them in and then you have to cook hook
up the interac networking and away you
go this is kind of our current
generation is it looks pretty similar
it's basically commodity pcs usually
with dual cpu chips with now two cores
per per chip so typically for processor
fair machine
you know hard drives they run a version
of Linux with a very tiny number of
patches we've found useful for our
particular platform and then a bunch of
custom in a software and that's kind of
one thing to point out is typically our
networks have you know we have a cluster
of perhaps thousands of tens of
thousands of machines connected together
with a central switch and we have a
bunch of machines in Iraq that you share
a switch for that rack and then thats
wit that racks which hooks up to a
central switch for the cluster and
there's less bandwidth available than
you would then full bisection bandwidth
for all the machines so one bottleneck
in our system is talking outside of Iraq
is less efficient than talking inside
Iraq so we do a bunch of software things
to kind of help mitigate that in some
ways just to give you a flavor of the
kinds of things that can happen in a
cluster well I asked one of our ops
people have put together this list of
what kinds of things actually happen
this is in the first year of a new
cluster after that they get a little bit
more reliable than this but you know
this just gives you a flavor of some bad
things that can happen and some of these
are you know you'll lose 40 machines for
a few minutes some of these are you
might lose several thousand machines for
you know a while you have lots of
individual machine failures many more
hard drive failures than that so that's
that's the delightful platform we build
on okay so what I'm going to talk about
mostly in this talk is three pieces of
infrastructure we've built over the past
few years to sort of allow us to use
that computing platform in a sort of
reliable and reasonably efficient way
the first thing is if you have a bunch
of computers with disks you'd probably
like to get little store stuff on them
and you'd like a sort of distributed
file system so that you can have a you
know centralized a common name space
over all this data
so one thing we decided early on was our
file system requirements were a little
bit different than typical file system
objectives in particular we want to have
really large read and write bandwidth we
want thousands of clients be able to
talk to thousands of machines in a file
system and get really good io bandwidth
for reads and writes it needs to be
reliable instead sort of the cluster
level of machines we're mostly dealing
with fairly large files most of our
systems kind of take you know small
things like web pages and put a bunch of
them in a file so we have you know files
that are many gigabytes typically rather
than lots and lots of tiny little files
and we need sort of efficient
distributed operation which means we
don't want to have a central bottleneck
in the file system much as we can help
it one thing that really helps us is we
basically control we're able to link in
sort of some client-side code into our
file system to put some of the logic of
how to deal with this distributing
nature the file system into our client
applications rather than having to have
it work at OS level or have everything
proxied through a server that we control
okay so the basic idea of gfs is we have
a special machine called a master that
deals with all the metadata the file
names and keeping track of a mapping
from file names to chunk locations we
break at the master level we break files
into 64 megabytes chunks which is a
pretty large block size by file system
standards the actual chunks on disk use
Linux files so those are stored with 8k
file system blocks or whatever it is but
at the master level it keeps track of
things at this 64 megabyte level and
then the individual the actual data for
files is stored spread across a bunch of
what we call chunk server processes
stored on local disks on the machines
and every chunk is typically replicated
three times on three different machines
and typically we try to spread out the
chunks across different racks so that if
you lose a rack you don't lose all three
copies of your
that of a particular chunk so the master
manages metadata clients talk to the
master when they open a file that's the
master says yes the file exists and here
the three replicas for the six different
chunks in the file gives you 18 machine
locations and away you go and then the
clients talk directly to your chunk
servers to read and write files and
occasionally they'll talk to a chunk
server the charger we will say I don't
have that chunk anymore go talk to the
master again and the master is also
responsible for when noticing when a
machine dies and then rewrap the gating
any chunks of that machine had to make
sure they're bringing them back up to
the full desired level of replication so
that's systems been pretty stable for a
while and is sort of running on almost
all of our machines that in various
clusters we have probably several
hundred gfs clusters some of them have
you know upwards of 5,000 machines you
often get pretty large collections or
clients talking to the filesystem and
you get pretty high bandwidth rates out
of these file systems as you're
processing large amount of data you have
maybe 10,000 clients talking to the
chunk servers in that file system and it
all sort of works in the presence of dis
failing and and machines going down a
racks going down so on okay so now that
we were able to store data it's often
useful to be able to compute over it and
in the early days of Google we would
basically have some large data set maybe
a bunch of documents we'd crawled and
then we would need to write some phase
that say counted how often the every
word occurs and so we would take the
files and we would sort of write some
hand partitioning coat write some code
to partition the problem into you know a
bunch of chunks of those files and we
would write some code to actually do the
work of counting word frequencies which
is you know that much
and then we would have a bunch of code
in there to deal with checkpointing the
state of this computation and what
happens when a machine fails how do you
recover and to all the messy details of
running computations on this sort of
slightly unreliable computing hardware
sort of obscured the real computations
we're trying to do which were often
fairly simple things like count how
often words occur or cluster documents
by content checksum or something like
that so MapReduce is a system we came up
with after writing several of these
phases that sort of abstracts away a lot
of the messy details allows you to
express your computation in this
particular programming style and then
the library can deal with what happens
when machines fail and so on ok I've
said most of us so the basic idea is
you're going to have some input data
which is you can think of as a set of
key value pairs or input records of some
form and the crawl if crawled pages
example the key is the URL of the page
and the value might be the contents of
the page so the map phase you're going
to process those input records and
produce some sort of intermediate key
value pairs that your computation is
trying to extract from that input data
or summarized from that in potato or
whatever and then there's a reduced
phase where you can specify for the same
intermediate key how do you want to
combine different values that may have
occurred from different records or
multiple value from the same record how
do you want to combine them into your
final output data so the user basically
writes these two simple functions map
and reduce and the underlying library
then takes care of it and the user
provides a little specification of what
input data they're supposed to be
processing and so on so let's look at an
example here the key and value are a URL
and the contents and then the map
function since we're trying to count
word frequencies is just going to for
every word in that text we're going to
like split it its paces and emit key
value pairs that are each word
occurrence and one
simple enough the reduce computation our
function in this case will be very
simple it's basically going to get
invoked for each unique word and for
each unique intermediate key word in
this case and then it's going to get the
sequence of values that the map function
generated for those things in this case
it's just going to have the value of one
many times one for each word occurrence
and it's going to add up all those
things and emit a final count a final
table final count for that word and then
the library takes care of applying the
reduce function to every unique
intermediate key and away you go now
this is a really simple example but you
can do a much more interesting things
with it like produce inverted indices
you can you know do training for machine
learning systems various things it turns
out that you can express a fairly wide
variety problems in this map and reduce
style so some of the things the library
does for you are makes your computation
pretty fast one of the things that does
is it tries to put computation for
particular chunks of your data onto the
machines that have that data or push it
close to those machines so remember I
said we have limited sort of cross rack
bandwidth so it will actually push
computation too close to where the data
is because the computation is typically
do you know size of a binary and the the
data is typically much much larger and
so you can often get you know thousands
of machines reading their data
conceptually off of the distributed file
system but really it's just reading off
the local chunk server in most cases you
can spend a lot of effort on the sorting
algorithms and the sorting library
inside MapReduce because you know you
can tune that a lot and then everyone
who uses this library benefits the
system deals with machine failures I'll
talk about that in a minute can deal
with bad records like if you are using
some third-party library that crashes on
you know some random record in some
deterministic way you can effectively
set it up to
after its tried that a few times it can
skip that record so your computation
will actually complete if that's what
you want or you can say that some I
don't want to skip record stuck you have
your choice it's pretty easy to use it
deals with what happens when you want
your computation to go faster you can
just add more machines generally and
will run things in with wider degrees of
parallelism and it also gives you some
monitoring and sort of central status
pages that you can look at a zoo
computation is progressing that's kind
of a standard for across all these
different kinds of jobs are trying to
write um so it's being used kind of
basically all over Google I have a grass
later about how many different MapReduce
programs there are but it's basically a
batch oriented computational model
that's bit proved pretty useful until is
some testament to this if you look at
the number of different MapReduce
programs in Google source tree over time
those early numbers are when we were
basically trying to rewrite our
production indexing system which have a
sequence of maybe eight or ten different
phases to you know take raw documents on
desk and ultimately end up with a a
final inverted index and other data
structures for serving and so that was
when we first started using MapReduce we
didn't think it would actually be useful
for things other than our indexing
system but people found out it's
actually pretty easy to use and it would
take care of a lot of problems they were
finding in you know writing other kinds
of computations if you take the
derivative of that graph this is the
number of new MapReduce programs per
month at some testament to it actually
being easy to use every summer we have a
bunch of summer interns come in
and most of them don't have any
experience with writing distributed
computation through parallel
computations but they seem to be able to
check in map reductions in to our source
tree I don't know if they work but they
they do get checked in and they seem to
run a lot of jobs so here's just some
stats about it how much it's being used
these days you know the typical
MapReduce finishes in 10 to 15 minutes
but it uses you know several hundred
machines that's just kind of you know we
run 3,000 of these computations per day
we have a bunch of deaths per job that's
fine most of those are caused by some
job that repeatedly crashes over and
over not so much that every job has five
worker deaths someone also pointed out
that work or death seems bad when Google
is hiring so fast I assure you these are
all is they're all machines so a little
bit of how the computation is actually
staged the basically the map produce
program has two kinds of workers there's
a master who is responsible for sort of
coordinating the activity dealing with
what happens when machines fail knowing
which other workers have done which
pieces of work which tasks so it
basically knows the input data that's
provided by the user and breaks it into
a bunch of tasks those tasks are then
parceled out to free workers so it says
oh I have a free worker I'll tell it to
do map task 12 typically you want to
have many more map tasks than you have
worker machines because that allows you
two things one is better load balancing
if one map task turns out to be slow
then the other machines just do a little
bit more work and the slow guy kind of
turns away for a little bit longer on
one of the map tasks it also makes
recovery a lot faster so if you have
each worker doing 100 tasks and one
worker dies then 100 other machines can
each pick up one piece of work and
recover very quickly from that machine
dime
the other thing we allow the user to do
is to partition the intermediate keys
typically with a very simple function to
like fingerprint the keys and and do the
mod n so that you can apply the reduce
function in parallel so you end up
spreading the intermediate keys and
computation of reduce over a bunch of
machines and then you end up having to
do the shuffle so the master keeps track
of which reduced workers have to talk to
which map workers for to get
intermediate data from those machines
the intermediate data is written to
local disk on the map workers basically
buffered there then it's transferred
across the network just by doing our pcs
and then we shuffle it once we have all
the data from all the map workers
because we can't sort of apply the
reduce function until we have finished
all the map tasks because they guarantee
we make for the client is that we're
when you process intermediate key you're
going to see all the values that
intermediate key and so that's basically
how it works and then we once we have
all the data and we've sorted it grouped
it together which we do by sorting and
you apply the users reduce function to
each unique key and away you go so the
status pages I alluded to beautiful
pieces of red and green so the the green
indicates map tasks that are currently
in progress this is sort of when a job
is just starting up you see in the upper
left we have 323 workers no deaths so
far morale we've split things into
thirteen thousand ish map tasks those
are called shards we've started working
on 323 of them our total input is a
little bit shy of a terabyte we've done
about a gigabyte so far and we've
decided to partition the reduce function
across 500 different machines we have
500 reduce partitions in this case this
is actually some phase of our indexing
pipeline um so as map tasks complete we
start shuffling the intermedia data from
the completed map tasks as those same
workers are doing it other map tasks
that are in progress so you basically
pipeline the shuffling of the data with
the computation of the map tasks and now
you see we have more workers we have
1707 workers we've had a death but
doesn't really matter fine and
eventually we get up to one hundred
percent map task complete all the
shuffling is now down the red is a
shuffling and now we're starting to
apply the users reduced function which
is the blue and you know that proceeds
in parallel across 500 machines we've
asked for for our reduced partitioning
and we get close to the end and now we
have all but two of them are done all
but one of them is done and finally
they're all done one thing you notice is
that stragglers are kind of a problem
sometimes you end up with a slow machine
that isn't due to data dependence in the
actual work that's doing it's just slow
for some other reason it could be you
know the lots of other jobs are running
on that machine so it's you're getting
less of the CPU it could be maybe it has
a bad local disk and so instead of
reading at 20 megabits a second it's
reading at one megabyte a second because
the disk controller keeps through trying
things there's all kinds of reasons
we've actually had all kinds of weird
things in one of our first platforms
that use hyper threaded processors the
BIOS manufacturer hadn't really thought
about what would happen when two chips
for reading and writing processor
registers and at the same time and so
they had a race condition where it would
read processor status register doodle
some bits and then write it back in and
you'd end up with in four percent of the
time the machine rebooted you'd end up
with the processor cache is disabled
because it would stomp the bit that says
please enable the processor caches so
that was kind of annoying because you'd
reboot the machine and you reboot 100
machines and four of them would come up
flow you reboot them again for other
ones we come up slow and a machine
without caches is a working machine but
it's like 30 times is slow it's very
annoying
mix the jobs do you run one job so if
you have a dependent MapReduce we
typically would run that won the
completion and then run the next one but
we are any leaving lots of independent
computations like on the same cluster so
there might be you know hundreds of
users running different map reduces on
the same cluster you know random jobs
taking network bandwidth taking CPU from
you so it's pretty important to deal
with Jaguars so I mentioned some of
these locality the shuffle stage gets
pipeline one thing we do to deal with
stragglers is towards the end of the
computation we will start off multiple
copies of the last few map paths for the
last few reduce tasks and whichever one
finishes first wins that actually brings
in the job completion time tremendously
because you'll typically get scheduled
on a machine that's that's fast that's
not as loaded as the slow guy and it
will be able to complete things more
quickly now we also compress
intermediate data because our
environment is more CPU rich than
Network rich so it makes sense for us to
do fairly lightweight compression on the
intermediate data just to avoid bit in
Iraq transfers now it's proven to be
pretty useful and we have a paper about
it that has a lot more details about it
if you're interested you search for
MapReduce you'll be able to find it okay
the third half of the talk so over time
we found that a lot of applications
wanted a interface to storage that was a
little bit higher level than just a raw
file system they wanted to be able to
process lots of different kinds of
structured semi-structured data where
you had maybe different pieces of data
would become available at different
times but they all were kind of related
by some some key in our crawling system
for example you have URLs as kind of the
natural key to tie everything together
and then you have various kinds of data
like you might have some small metadata
saying when did I last crawl this URL
you might have other things like the
actual contents of the page the last
time you crawl over the last few times
you crawled it and then you have other
things that are being run kind of
asynchronously where York extracting
links from these pages maybe you're
running a pagerank computation over the
graph structure you've extracted from
all these pages and you want to update
the page rank value for this page so
these are all kind of tied together with
the the URL and we have other systems
that have kind of natural keys for
organizing data where you have for
example per user data you have user
preferences you have you want to be able
to keep track of recent queries done by
this users so you can show them in their
search history that kind of thing and
geographic data tends to have a natural
organizational point where you want to
organize around you know a particular
region of the earth earth surface and
you have satellite imagery you have
vector map data you have maybe user
annotations that you've allowed users to
make about different points of the earth
so on um so we really need something
looks kind of like a database what we
needed to scale the really large amounts
of data you know we have you know
hundreds of terabytes of raw web content
we have you know lots of satellite
imagery data and so on so we want
something that has this structure to
semi-structured API and it's kind of
like a database so you could use a
database like Oracle or something the
problem is the scales really large even
if you could buy something it would be
really expensive and it would solve the
problem for that particular application
and then we have lots of applications
that are like this so the next time you
wanted to solve not for URLs but for
satellite imagery you then have to go
spend a lot more money also we can kind
of integrate it with our file system and
have a little bit tighter integration of
how the system deals with you know
compressing data how it stores data on
disk and get some nice advantages from
that so we basically decided to build
something like this ourselves we decided
we didn't really need full database
functionality so we don't support joins
we don't support full sequel queries or
something we have a fairly simple API
that allows you to get at
data with the following model it's
basically a multi-level map that I'll
describe in a minute it's designed to be
fault tolerant and persistence so once
you've written data into the system it
basically is persistent it's scalable so
we have systems with several thousand
servers serving a set of tables in
particular what we call a big table sell
the it's pretty and a lot of those cells
support pretty high volumes of reads and
writes we initially did it more for
batch style things like our crawling
system but more recently it's been used
a lot and user facing applications where
latency is a much bigger concern where
you want this operation to finish in 10
milliseconds and and you need that to
happen it's important to us that it be
self managing so you can you know it
deals with machine values of course but
also you want to be able to add another
500 machines the cell and then have it
take advantage of the extra capacity
that those machines should offer and
sort of load balance across the
available machines that it does have so
the basic data model is that you have
rows and columns think of it as kind of
a really big spreadsheet and a lot of
our applications actually wanted to be
able to look at multiple values across
time so in our crawling system is useful
to be able to keep several versions of
crawled contents so you can look at how
much is this page changing you know from
one day to the next is it something
that's completely static is it changing
only a little bit as a changing a lot so
we actually allow a third dimension of
time where you can set up a particular
column to keep you know all versions of
data that you've written in there one
version you can say I want to keep all
the versions in the last two weeks that
kind of thing and it turns out you know
if you squint at this abstraction right
a lot of our applications can make use
of this thing because it's a pretty
generic abstraction
so the way we actually take this and
distribute it across lots of machines is
you can think of the table as a sorted
sequence of rows we actually think it's
important that the users be able to get
at sorted sequences of rows rather than
just a random ordering like a distribute
hash table or something so we break
these tablets these tables into what we
call tablets which are just contiguous
regions of rows that are roughly a few
hundred megabytes in size and we have a
serving machine that's going to be
responsible for on the order of 100
tablets you know for the same reasons
that we want a map worker to deal with
you know many different map tasks it
helps recovery so if one machine fails
you can quickly have each of 100 other
machines pick up responsibility for one
tablet one of those hundred tablets and
you recover pretty quickly when a
machine fails it also gives you finer
granularity load balancing so if you
notice a machine he's overloaded you can
move one tablet at a time away from it
until the load imbalance kind of goes
away so initially we have tablets we
have two tablets in this table at the
moment eventually that bottom tablet
gets enough data that we decide to split
it and so we basically pick a row that's
roughly in the middle of the two of the
tablet it seems to split the amount of
data and roughly in half and then we
partition that in two separate tablets
that are now independent and we can move
one of them away from this machine put
it on some other machine so on we
actually also do merges which are more
complicated because you have to sort of
stage things a little bit in the case of
a split the thing you're trying to split
is one tablet and that's on one machine
in the case to emerge those two things
or independent entities and you have to
sort of pre stage things to get the
tablet on the same machine before you
can kind of blow them together okay the
system structure we have like a lot of
our systems you have a master that is
responsible for basically metadata
operations and load balancing we have
tablets servers that serve data and then
on
neath that we build on top of a lot of
the other infrastructure at some of
which i've talked about gfs we have a
cluster scheduling system where you can
insert jobs into the cluster and you say
i want to run a hundred tasks on
different machines for my job and it
will take care of allocating resources
to you it takes care of handling
failover when a machine fails it will
restart that task on a different machine
gfs obviously we store the underlying
state for the table in gfs and we have a
distributed block service so that that's
highly available and reliable so that
for example the master wants to we want
to have hot spares for the masters of
the master goes down we bring another
one up or a quickly so we actually just
start two processes they each try to
grab a distributed block and this lock
service to say I'm the master one of
them will get it and then the other one
will basically continuously try to
acquire that lock so if the first master
fails then the other one sort of takes
over fairly quickly as soon as I can
grab the lock and then we have a client
library again we can link that into
applications and the client library when
it needs to open a table the the
metadata for each table is stored in the
lock service and then it talks directly
to the tablet service to read and write
data occasionally docks in the master
but unlike gfs most of the metadata
operations most of the location
information to find out where particular
piece of data is actually handled by the
tablet servers and spread out over all
the tablets servers so you only need to
talk to the master if you want to create
a new table or something like that she's
pretty rare so the state in our tablet
is basically we have a mutation log of
mutations that have been added to this
tablet so when a right comes in we
append to that log then we buffer that
right in memory so that we have
efficient access to it when i read comes
in we look in our in memory buffer and
we also maybe look in some compacted
representations of the log that we've
stored on disk
that are organized sorted by key
sometimes you need to look at multiple
of those you can also specify that some
columns are mapped in memory so you
might have a bunch of files mapped in
memory to represent things so as I said
a compaction happens when they're two
kinds of compaction is one is you
buffered up a bunch of updates in memory
and now your memory is getting full so
you're going to take that state in
memory for this tablet write it out into
a on disk immutable form and then you
can flush your memory and serve the
state off of those disk files instead of
off from memory the other thing is
eventually you build up too many of
these files and you need to reduce the
number of files by and so you basically
pick a bunch of them you merge them all
together and then you produce one file
for the tablet that represents the state
up to a certain point of the log so you
always have the state of the last major
compaction and then you have some number
of other files that are minor compaction
and then you have the piece of log that
has not yet been compacted that's kind
of the state of the tablet we also allow
the ability to segregate columns from
other columns in the on-disk
representation under user control so one
thing is very useful is if you have some
columns that are very small and you want
to iterate over them independently of
all the other ones the contents here is
quite large it's basically all the
contents of all the webpages we've
trawled and sometimes you might want to
iterate just over the page rank and bank
language values for the page which are
you know a few bytes each so we allow
segregation of columns into what we call
locality groups you can say this one
should go in this locality group listen
sugar in this locality group and then if
you need to scan and you only scan the
language of page rank columns your PI 0
is proportional with the data and those
columns rather than all the columns so
one tricky thing in the system is how do
you actually locate a particular piece
of data
so what we actually do is we store
tables that are themselves big table
tables and those tables point to the
tablets and the other tables in the
other machines so there's a
bootstrapping table with we store in our
lock service which is a pointer to the
meta 0 table that meta 0 table has one
row for every tablet in the meta one
table every row in the meta one table
points to the actual location of a
tablet of a tablet in a real user table
so if I need to find a row in this user
table on the right I go to the meta one
table and scan forward to find the right
row for that tablet for that find the
entry in the meta one table which will
point me at that thing if I don't
actually have know where that tablet is
then I go back to the meta 0 table and
find out where that meta one tablet is
and if I don't have the meta 0 location
then I read that out of the
bootstrapping table it seems to work
pretty well and if you apply a little
bit of sort of prefetching and caching
then you generally just need to go
directly to the machine on the right and
sometimes need to do a lookup in the
metal one table most of the other stuff
is cached completely
so we now have you know about 500 of
these cells where a cell is a master and
a bunch of tablet servers the biggest
one is managing you know three terabytes
three petabytes of data and it's in
action active use for a lot of
Frederick's most projects these days are
building on top of something on BigTable
rather than building directly on raw JFS
files so in terms of what i think are
our challenges are in terms of where we
want to take our infrastructure these
days i think a lot of these tools work
pretty well at the single cluster level
we're pretty happy with those where we
have issues is in terms of we have lots
and lots of clusters distributed around
the world and we don't really have a
single system that deals with data in
all those clusters or computation in all
those clusters so one thing that would
be really nice is to have a single
global name space for all of our data
right now we have different gfs cells
and those are separate namespaces so if
you copy files from one gfs cell to
another there's no automatic system that
knows that that connection of this data
it was originally copied from this data
and it's another source if you need to
read it we really need some way of
keeping track of that kind of thing so
that you can more automatically make
more replicas of data across different
clusters for example and we'd like them
more in a more automated fashion migrate
data and computation across clusters we
sort of do that migration today within a
cluster but not very effectively across
clusters once you have that you have
lots of consistency issues that are
mostly tied into wide area replication
and network partitions of this cluster
is now offline for a few hours for
maintenance or something or it's
partitioned but i'm still getting
requests in both sides of the partition
i need in a lot of cases i need to be
able to do something reasonable continue
operating in some limited mode on both
sides of that partition I can't
reasonably say to all the user requests
that show up in the the non quorum side
of the partition that well sorry you
can't
do anything it'd be better to show for
example a show users their email but
maybe say some update some email
messages may not have shown up or
something rather than completely not
showing them anything so basically
looking at how can we build systems that
are spanning multiple clusters and are a
much larger scale of what we've built
today I will briefly talk about the
kinds of so a lot of what I've talked
about is the stuff that sits underneath
but the end goal is to build interesting
products interesting features of
products so I'll talk a little bit about
the kinds of things you can do given
this infrastructure it's actually a
pretty nice environment because we have
built this infrastructure and it's
pretty easy to write you know random
MapReduce interns have a lot of fun
writing map produces and doing
interesting computations and we have a
lot of interesting data so you know one
of the things I think we believe pretty
strongly in is that the more data you
have the better you can make your
systems this is actually an example of
all the various the top one is the
correct spelling of Britney Spears all
the other ones are misspellings of
Britney Spears that were detected by our
spelling correction system to be
misspellings and corrected to the
correct spelling and you see is a very
long tail of potential misspellings and
the more data you have our spelling
correction system is trained on a
collection of documents and queries and
the more data you have the better the
system is going to work because you're
going to see more examples of of
misspellings and corrected things that
have been corrected to and so on
actually there's one here those briny
Spears that may refer to pickles I'm not
certain that may be a problem the other
thing you can do is you know put
together little demos of interesting
things so this is looking at query
frequency it's a little bit old data
taken from an old system so is this is
beginning of 2002 to the middle of 2003
so it spans 18 months the Freak
see of queries for a variety of cases of
queries to google com so you see for
example the Eclipse case there's always
some number of queries containing
eclipse because Mitsubishi makes a car
called the Eclipse but occasionally you
see a big blip in Eclipse queries and
actually if you look at the frequency of
individual queries in those days you can
figure out of it to lunar or solar
eclipse which it's kind of cool the
every 28 days regular as clockwork we
seem to get a big spike in queries for
full moon just kind of cool you wouldn't
have expected that but there it is you
may not have expected watermelon to
exhibit seasonal trends but it does it's
kind of low in the winter you know
there's a big spike in the middle of
summer any guesses fourth of July right
it's a popular picnic food trying to
figure out what to do with all this
watermelon you've gotten I don't know if
it's actually the fourth or the fifth of
July probably before World Series you
kind of expect a big spike in October
for the baseball world series but
there's actually two other World Series
that happen the Little League World
Series and the college world series so I
think you know this is kind of a toy and
kind of fun to play with there's
actually a public version of this called
trends google com that you can do your
own queries so we basically make this
data available as long as they're
sufficient unique users to not have any
privacy issues the the data in the upper
right would be useful for perhaps as a
signal to our ranking algorithms right
if someone does a query in say july four
World Series it might be more useful to
show them College World Series pages
than sort of Major League Baseball
october world series pages so even
though this is kind of an interesting
toy there's always lots of interesting
data that you can imagine including in
things like improving our search quality
improving ranking summer olympics is
kind of interesting because this was a
winter olympics year
there was no Summer Olympics that year
this is just people pining for
gymnastics instead of ice skating or
something and you also see interesting
things that happen when a new word
enters the lexicon so opteron is an AMD
processor that was introduced a few
years ago they first you know there was
no queries for it before and then they
announced that they were going to be
building this processor and so people
did a little few queries nothing much
happened until they actually released it
and then after they released it to drop
back down to a much higher level than
before well and that's you know
potentially interesting things so let me
talk about one more kind of application
that sits on top of this infrastructure
which is machine translation this is
translation of human lying one human
language to another arabic to english
english to arabic chinese english so on
so the there most of the translation
systems work as a set of rules that have
been handcrafted by people over many
many years and in the early 90s there
was a group at IBM research that looked
at what would happen if you train the
system by training it on lots of data
where you had translated versions of one
sentence and another side the same
sentence than the other language and you
basically just looked at probability
distributions of if the word hello
occurs I have a high probability of
seeing Bulger or in French for example
and basically if you have enough
training data you can basically build a
probabilistic model of words and phrases
that tell you how to translate text
without actually having any handcrafted
rules so the more data you have the
better this will work you don't have
much data at all it doesn't work very
well at all but the basic idea is to
build a model that given the source
language sentence tells you what is the
probability of all possible target
language sentences and you pick the one
with the highest probability clearly
that's a very large search space so you
do a lot of pruning along the way to
help guide the search and so for
training you basically have some amount
of what we call parallel aligned corpora
where you can get for example
want one big source of it is United
Nations documents which were translated
into six languages literally sentence by
sentence it tends to give you a
translation system a little bit of a
United Nations bureaucrat kind of twinge
to it but so you want to use that data
and all the other data you can get to
kind of hope it washes out but the basic
idea is you find these parallel corpora
and then you build these language models
one thing that really helps is in
addition to these aligned corpora which
you can't generally get as much data of
those as you would like you can clean up
a lot of the translations by having a
very large language model of how often
every five word sequence in the target
language occurs for example so if you
take all your English documents in the
world you're trying to translate chinese
to english you have some amount of
chinese and english parallel data that
you've trained your one of you
translation models on but another thing
that helps is to have an enormous
language model of english word for word
and phrase frequencies and now if you
take 10 billion english documents you
process them and then you have to
candidate translations for a sentence
and one of them has a five word phrase
that has occurred 43 times and the other
one has a phrase that has never occurred
you're probably more likely to prefer
the one that's occurred 43 times it kind
of makes the translations flow a lot
more naturally and then you actually
just need to look up lots of
probabilities so you know I don't know
much about machine translation but I
know how to look up data in a large
distributed system so I I worked with
our machine translation folks to they
need to do about a million lookups per
sentence in this state and the state is
hundreds of gigabytes so it's kind of
fun good problem and you see that if you
have more data the your translation
quality is better this is a contest run
by the National Institute of Standards
it measures the percentage overlap for
your translations with a set of four
human translations of the same documents
and at the high levels here it's
actually reasonably readable you can
definitely get the gist of
what the documents about it's not
perfect translation but even if you took
a human translator another human
translator and compared them they would
only get it like eighty percent because
human translations are not one hundred
percent overlap either the important
thing to notice is the more data you
have the better your system work so you
can train the system on less and less
data to see what the effect of having
more data is and every doubling in the
amount of training day you have
basically gives you a little bit higher
score and that trend seems to bear out
as long as far as we can see so that's
kind of cool one of the things that I
think having the right infrastructure
helps with is that if your
infrastructure solves a lot of annoying
problems that are commonly repeated by
seeing and need to be solved by lots of
independent groups it allows those
groups to be more productive in building
sort of real products which is what you
really care about in improving existing
products so we tend to have people work
in pretty small teams work on you know
some of the teams build interesting
infrastructure other ones build you know
interesting products on top of that
infrastructure and we let people kind of
play around with different things like
the thing that showed query frequencies
over time is something someone played
with just as a side demo but it's
actually useful data in some cases for
improving search quality all kinds of
things one of the things that I think is
interesting about about Google is that
we have a pretty broad range of problems
and the set of problems we work on spans
very low level things like hardware and
networking how can we build large
clusters of machines out can we network
them connect them together a better
bisection bandwidth how can we design
data centers to be more efficient for
cooling all the way up through you know
how can you build you know interesting
distributed systems on top of this
slightly unreliable hardware how can you
build good data structures and
algorithms for a variety of problems
information retrieval and machine
learning and statistics and so on and
then all the way up to user interfaces
and you know product design all those
kinds of things are integrated and a lot
of the teams span lots of these
different problems it's kind of fun
so we've written a few papers about some
of these systems that i'll just point
you at if you want to they go into a lot
more detail about some of the inner
workings and I'll take questions yeah
yes so one of the things we've moved to
is rather than so the question is does
BigTable give you any SL azor our
latency guarantees about different kinds
operations one of the things we've moved
to is having groups of operations people
manage collections of big table cells so
we have what we call service cells that
rather than having each individual group
run their own big table cell and as part
of the service agreement you can say I
have this much data i need you know i
need this kind of access to it I needs
to be you know three nines less than 10
milliseconds and you can basically say
say that okay okay I think we have don't
have time for any more questions because
I've babbled on too long but i'll be
around at the brakes and stuff so okay</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>