<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Google Internet Summit 2009: Networks and Statistics Session | Coder Coacher - Coaching Coders</title><meta content="Google Internet Summit 2009: Networks and Statistics Session - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Google Internet Summit 2009: Networks and Statistics Session</b></h2><h5 class="post__date">2009-05-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/MFFMsfQEJ1I" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we will move on now to the first part of
our agenda and that's to look at the
internet from a statistical point of
view and to do that i'd like to invite
Steven Stewart to come and share this
session the Steven I'm not going to try
to give a price ease of your background
you're welcome to take a few minutes to
do that but I will tell you that steven
has been a bastion of development in my
world anyway on the ipv6 side on
measurement and other activities that
are so critical to the net and so I
appreciate very much to even your taking
the time to join us this morning then
I'll turn this over to you thanks thanks
and ingredients everyone we had planned
a a view of the a statistical view of
the internet starting with the very
large and then getting down to very
small and Alan I believe was going to
handle our view of the very large well
thanks for having me here my name is
Alan Walden I work with telegraphy
research which is a market research
company focused on global
telecommunications based in Washington
DC although I am based in Bratislava
Slovakia i'm going to talk about the the
internet at its core basic level
globally which is the under the undersea
cable system largely as well as some of
the capacity available internationally
around the world so I have a few slides
I want to run through that touch on some
of these issues the first slide in here
is a map showing the current undersea
cables around the world they're scaled
by the capacity the lit capacity how
much is in equipped and is currently
active honest systems so you can see
between the US and Europe about 12 to 13
high-capacity systems in in place across
specific about eight several to South
America and in the intra asian area
between singapore and japan with quite a
few systems all very high capacity
this is it this is changing quite
rapidly actually the way this map looks
in the next year you're going to see new
cables to Greenland to to East Africa
even some two islands in the South
Pacific so you're really seeing
expansion of the cable in structure to
some very far flung places at the same
time you're seeing new cables on any of
these routes that are already in place
you're seeing five new cables between
Europe and the Middle East we had one
across the Pacific clash you were going
to have 223 more across the Pacific in
the next 18 months as well so it's it's
a clearly a bobbing rapidly there's a
bit of a cable boom right now underway
about 16 new cables coming in service
just in this year alone generally we're
seeing is 4pi repairs you can do as many
as eight but it's it's more more costly
so kids are going for a four to five
fire pair system usually and it can
handle usually 96 228 waves at ten
teenpink in big waves it it can change
over time it's possible but right right
right now most the cables that are
currently in place you know they can't
do more than 128 waves per fiber pair
yeah exactly I'm trying to figure out
whether the people who are watching this
are going to hear questions without
benefit of a microphone what's the
situation they won't all right so we
need a microphone a handheld okay so
yeah if we need if we're going to do QA
we need to use microphones okay thanks
ok Oh
this was a nice image showing the
capacity growth over time I've it's a
it's an envelope to open it up to see
what's inside what you what you would
have seen here ash always didn't work is
i use a mac to make this so alright so
this this was showing on the person that
i sent which which did did work actually
was the increase in the capacity over
time for the pat for the past five years
seven years so what we've been seeing
actually it's not an envelope but a
relatively slow growth of capacity
between chazzan 2005 what there was a
much need for new capacity because there
was so much capacity of the cable that
was installed during the tall combine
with other early 2000s that there wasn't
any need for new capacity but in the
past few years it's it's done this last
year alone across the pacific let's
split capacity grew 121 percent that's a
result of one new cable but also several
cables were upgraded so they either had
new by repairs lit on them or had it
more waves added to existing we let
fiber repairs so hopefully the next one
going to work hold my breath ah so so
this is another image that shows the
same five routes major global routes
that we're on on the previous slide that
didn't come through but uh so this is
the the lid capacity versus the
potential capacity of installed cables
at the end of this past year so you can
see roughly how much is lit across the
Atlantic its twenty five percent Pacific
is 30 the one that might seem worrisome
is the Europe to Asia route which are
cables that go from Europe through Asia
down through the Red Sea across the
Middle East it's yeah they are quite
utilized the good news is there's about
five new cables are going to come online
next 12 months
which will add up which will add the
potential for about think 18 terabits
extra so right now we're at a total of I
think three terabits potential on this
cables so clearly the the Europe to Asia
Brad's going to get plenty of new
capacity in the net in the next few
years it's a very important question
because the question was how much does
it cost to lie two wavelengths across
the Atlantic thinking card costs to the
wavelength card costs it's about two
hundred thousand dollars that's been
coming down a lot if you buying in bulk
you can get it down to a hundred
thousand for a pair on either end that
doesn't include the cost of the back
hall from the station into the city pop
but the other point here is is even even
though it seems that it's relatively
under underutilized with only 25 percent
via the Atlantic is is lit up with
potential even though the potential is
39 terabits at current growth rates this
could be exhausted by 2020 14 we think
which means the time of start
considering building a new cable could
be coming up rather rather soon because
once the bestie is gone you don't need
one cable you need several cables
so the last slide I wanted to touch on
here was just showing the data we have
on a public Internet capacity country to
country this is this is the aggregate
totals of the various backbone operators
just but just but just the international
links between various countries and it's
only only showing the the routes that
are over 30 30 30 gigabits and aggregate
so so obviously the the internet does go
to Africa in the Middle East just not
it's not shown right here on the data
set what's important about the bot ways
this looks now is I think everybody
knows the Internet's becoming more
regionally focused in the way traffic is
routed how capacity is growing if you
look at how how the clients are linked
it's still largely through the United
States South South America goes all
through the United States to get to New
in the rest of the world asia largely is
still going through the u.s. to get
anywhere you know there is a growing
amount of capacity between Europe and
Asia directly both subsidy and
terrestrially across Russia but it's
really small compared to what is in
place across the Pacific and it's much
more expensive as well right now to go
on cables via the Middle East or a cross
cross Russia that's all one word fun
right now there's any questions yeah
thank you so two questions what is the
relative cost of terrestrial versus
undersea cables and the other is are you
seeing any change due to the economic
changes in global economic conditions in
the in this segment in tables in terms
of the cost it's going to vary by the
region obviously and how much capacity
is installed so it's really hard for me
to state different the differences can
be a cable going across Russia at a cost
that's far more expensive to maintain
operate than won't going across the
United States for instance the price is
probably ten times as high across Russia
than going across United States the
question about us about the impact of
the current I come a crisis on existing
cable projects and cable builds so far
we've not seen in any projects being
being cancelled at all most cables are
still being being built I think there's
a new project that's coming along it
could have a hard time getting getting
funding but just in the past three
months we've seen I think three projects
announced some some pretty big projects
that and we're now sand had contract
sign so clerk quickly these are
long-term investments that are being
made so there's a there's a new cable
announced that goes up the coast of West
Africa it's going to be going to service
in 2011 it says 600 look into a pagar
cable that was announced just last month
so the money is still coming in coming
in to to the cable sector yes is
basically raw big carrying capacity
respective of protocol versus the
internet have you parsed it in this
presentation in the second question
which is a perennial one is how much of
this is sort of unlit purposely by some
of the owners of the cable systems to
drive up demand right on the what we're
showing here is just the layer 3 layer 3
internet capacity but the previous
slides are showing raw capacity the
potential capacity of the system
and I can just go back and show this so
this is this is showing the potential
capacity in red here in this slide so
yeah that is not being being lit current
currently utilized but I don't think
he'll owners are withholding it for any
reason they've been adding capacity
rapidly the past few years so I don't
think there's any planner now to to keep
pipe airs on the unlit they're trying to
trying to serve demand as fast they can
I I don't have it no yes so for the
systems that are going in now I as I
understand that these are usually done
by consortiums and who is active in
these consortium who is actually paying
for this stuff it's a good question it's
we are shall seeing some activity among
consortiums but also seeing private
operators still in some parts of the
world I have private cables go in and
smaller markets where there's an
opportunity to really make a large
profit usually it's a higher risk as
well the confession that we're seeing
now are smaller consortiums such as the
one that Google's involved with with the
cross the Pacific the unity project a
template six-member seven members so
you're still seeing the traditional
telcos largely leading a lot of the
investments so you're seeing verizon
AT&amp;amp;T still being very active as well as
carriers in Indian and the Middle East
loving the new builds did air so sorry
telecom telkom Egypt Tata Communications
reliance of India as well are building a
lot of cables no you know Google's were
the first one to that I can think of to
to take a direct very large role in
cable ownership and building the cable
you have non care entities as large
owners of capacity on cables definitely
definitely but in terms of being at the
forefront of trying to build and as a
design a system and invest in it it
still largely the liquid carriers switch
over to Tom spreads it now since a week
and time for questions at the end and
having wreaked havoc on this whole setup
once here we go again we're now trying
to we're going from the macroscopic now
we're going to talk about the network
that's been built out of this stuff ok
I'm going to show you some data on pack
oh that's okay i'm tom leighton i'm the
chief scientist and co-founder of akamai
technologies and a professor of
mathematics at MIT and i'm going to show
you some data on packet loss latency and
throughput at various points in the
internet so we'll start with some packet
loss and latency and the data I'm going
to show you here is it two nines that
means it's sampling it the worst one
percent of performance and the idea here
is it well basic if you take averages
things look pretty good you know module
of getting into China but otherwise
things are pretty good and people like
to think of their applications running
in a data center at five nines and the
internet is far from that and to sort of
get a feel for that this data is at two
nines so the 99th percentile of
performance and it's sort of what you'd
expect you know if you're going in the
United States you know things aren't so
bad so if you're going from San
Francisco to Chicago you know you can do
60 milliseconds this is data taken
across up a two-week interval in the
internet nothing dramatic happened
during that time at two nines you got
six percent packet loss and you don't
really have anything that you would call
you know hours of degraded performance
you know things are pretty good now i'm
going to show you data on internet
that's just you know doing a ping from
between the pairs of points the internet
Akamai has an overlay on top of the
internet and that will show you what is
capable with the the hardware that's
there okay now you don't get access to
all the wires your packets follow paths
determined by bgp and what the networks
you know want the pack of the routes to
be taken and we have we have our servers
in 900 networks we can sort of control
how we map our data through the internet
and this set is the best you could hope
to do
you know at 29 is 51 milliseconds so not
a big difference within the u.s. this is
round trip time ping time so say you've
got on to South America and you start to
see some differences packet loss not bad
at two nines you still have no packet
loss pretty good but you can start to
see that you're Layton sees are a lot
higher than what you might think of us
optimal going to South America so you're
paying 300 milliseconds versus maybe
some kind of notion of optimal 180 and
of course if you were to go into China
which is maybe one of the worst places
to try to go pack a loss only at six
twelve sixteen percent is pretty nasty
at two nines you have a hard time doing
two nines in the China but you see
dramatic Layton sees they're very bad
and look at the hours of degraded
performance basically every work day in
China during the whole work day it's
it's very poor behavior a form is going
into China yeah there's a couple of
reasons it's when traffic increases and
there's very poor connectivity between
the major carriers you know CNC in China
Telecom and of course there's the Great
Firewall you know at the perimeter of
the country filtering traffic coming in
and as traffic goes up it becomes less
effective and things get really slow
yeah two week period yeah it's pretty
bad in fact there's degraded there's
unusable unusable is a subjective
measure as close to 10 hours a day
during the weekdays we probably could
you know it's not the hours you want to
use probably degraded is based on packet
loss and latency compared to optimal and
there's an absolute amount of latency
versus and a percentage of latency so if
you go from two milliseconds or four
milliseconds that's a big percentage but
it's non material and then packet loss
is phone in as well and it's somewhat of
a subjective you know measure that if
you're running an application across
that link it would be degraded means it
starts you start noticing it slow and
not so easy and there's an unusable
measure which I didn't include here
which means it really is unusable you
shouldn't expect your employees to be
doing it
so these are ICMP echo request ICMP echo
response type measures that's correct
and the same for akamai also is that
just routed through your fabric instead
that's correct and do you prioritize
ICMP traffic differently from the rest
of the traffic at all uh no you don't
know exact equal so okay do you know
about the relative I mean do you have
statistics on the relative difference of
handling for priority for icmp versus
into traffic I don't know if we do or
not so I don't know the answers on that
this is going through the Great Firewall
cuz we have to leave the country so
we're not doing any cashing on other
stuff you would think of occu- doing
this is just transport layer and it's
tunneling in this case the Akamai scores
would be tunneling through other waka my
servers and other networks along the way
typically out of China are probably
making to stop overs on the way to
Rock'em our servers now if you look at
Europe it's a similar story but because
the geographies are closer within Europe
you know things are actually pretty good
you know you could go london to
amsterdam and you see there's a here's
an example we get a big difference in
percentage of latency but it is not
material because you know it's only
talking eight milliseconds yet to be
doing a lot of round trips for that
matters you know unless a nice place
would be going London Milan you know
you're looking at six percent packet
loss at 29 s not so bad you see no
degraded hours effectively and the
latency difference is percentage-wise a
lot but not necessarily material if you
go to Asia within sight Asia things are
pretty pretty nasty Tokyo to Singapore
whoops won't see if I can get that green
is better than red and yellows in
between in terms of performance of
Singapore would be a good location and
now here you got two nines at twelve
percent packet loss and if you in this
case you go to Seoul here you have large
Layton sees not so bad packet loss down
to Sydney sort of
between again big lake so much in packet
loss so Asia is not so nice to be going
you know city to city in Asia and partly
gots cuz the geography spread across
that was all that data to a normal two
week period these are some events that
occurred in the internet where things
are not so normal across this is a
little over about a week and a half
period where were the we had the
earthquake in taiwan a lot of those
cables we just saw had disruptions this
is data taken to run an application and
it's being tested from several points in
asia back into the United States the
application running in the US the red
line is what was happening on the
internet the green line is the same
application that's running on using this
overlay Network you talk of my runs for
its customers and you can see of course
when the earthquake hits the application
was taking a few seconds to run go list
over a minute there's an eight hour
period where it totally fails of course
many sites were just completely down
many transactions could not be run for
even longer than eight hours when that
earthquake did I think we this is
several points in Asia including Taiwan
but there were several locations and
Asia were impact because the cables you
know of course we just saw go into one
location fan out and I got away from the
mic here sorry and then you you see the
problem and you see you know as cables
get fixed things get better but it was
it was nasty for a while this is the
same event and now we're looking at
packet loss and you can see that the
packet loss quickly spikes over sixty
percent something happens to make things
better a day later and it's down to
fifty percent and then maybe you can see
the events when cables are being fixed
and life is getting better but you're
looking at a two week period we're still
over ten percent packet loss and now
this is not two nines anymore this is
just average behavior so you can see you
know the fragility of the system if you
knock out a bunch of the cables the
internet does not perform very well we
saw the same thing with the Mideast
cable cut this again is showing latency
in milliseconds in this case something
that was a baseline of 200 milliseconds
quickly spiking to over 500 milliseconds
and then as you know repairs happen
slowly getting back to normal and this
is showing a two-week period green again
is Akamai if you there is capability to
get bandwidth out if you had control of
all the infrastructure so it's not an
infrastructure problem as much as it is
a how it's used you know the routing
protocols here's packet loss after the
middies cable cut and this is again is
going from the u.s. into I think it's
into India through that territory into
Middle East now I'm going to shift gears
a little bit and talk about bandwidth
throughput and this is measured from
real data as we're delivering you know
movies and software downloads to
end-users and what we're measuring here
is an average overall IPS that we see
now I peas are very loosely correlated
with end users many end users can be
behind the same IP you know the firewall
here you can have dynamic IP space so
one user might see a lot of IPs so it's
not a one-to-one correlation with end
users but it's you know an approximation
and this is showing the actual delivery
rate averaged over IPS that we see and
we see pretty much every IP in the
internet what was delivered the other
night yep sorry all right let's see if
this whole I got a master okay so
globally now the average delivery rate
and megabits per second one and a half
Meg's this is averaged over I believe q4
of last year and we publish actually
these statistics in much greater detail
and that's not blue report that's
sitting on your desk and that has the
report from q4 we now have one online
that's the q1 report and we make this
public so that you can see how fast
average bandwidth or throughput is to
end users and then you see it broken up
by down by continent now here I'm going
to give you a little bit finer grain
view the green is showing in the three
major continents the fraction of ips
that we
liver too at over five megabits a second
so make this might be classified today
as high broadband it's good enough to
get DVD quality live people call it HD
but HD would really be you know blu-ray
would be much higher the yellow is the
percentage of IPs that we see between
two and five megs so maybe we'd call
that today broadband that's at least TV
quality then the orange slice is those
between two hundred 26 k + 2 megabits a
second at the upper end of that you can
get TB quality to lower end it's pretty
lousy you really not look in a good
video and then there's a small red slice
that's below 256k so we've seen a rapid
increase in improvement of achieving
last mile bandwidth so you may have a 30
meg pipe but will count you hear much
lower if we can't deliver to you at 30
max and that includes all the things in
the internet yeah these numbers seem
higher than I would have expected and
I'm wondering if you could explain how
you measure it yes so how the numbers
might change if it was weighted by
traffic volume as opposed to IP for
example like what's the median here and
also how much it changes week Dave
versus weekend where people might have
slower connections on weekends because
we're at home right I think if we waited
it by bit traffic the numbers would skew
more towards a higher throughput rate
because this we're measuring the
throughputs based on large file delivery
think of it as a movie a video or a big
software download and of course that
would dominate the total so maybe it
would be the same i don't see that
skewing it making it less to show less
throughput available to end users we
measure it by you know statistics on our
servers it says we delivered a hundred
kilobyte file in this much time and
that's reported analog line to this IP
address and then we look at us for an IP
address what is the maximum delivery
rate from the sample we took now it is
true that if we had one sample point for
this IP address that said it was very
high throughput five Meg
and one that said it was lower one meg
will report the five maybe cuz that's we
achieved so that may be accounting for
some of your intuition there so we know
that that IP we can deliver to at five
Meg because we did we delivered a big
file to it at that speed in terms of
weekend versus daytime I don't know the
answer to that that's that's an
interesting question quick question yeah
is this using the Akamai infrastructure
is it the big guy internet no this
weights both it's using the Akamai
infrastructure the question is is this
using Akamai or the Internet to make the
measurement and the answer is both this
is an Akamai server which is generally
sitting very close to the end user so
we're not going through San Francisco to
Tokyo here if we have an IP address in
Tokyo odds are very good the Akamai
servers in Tokyo in that is p in fact
that's true around the world so you
wouldn't see this performance if you
were you know delivering big files from
a few data centers you see much worse
performance because delivery rate very
much slows down as you go across longer
distances in the Internet yeah thank you
so we're always told that in North
America we're behind in terms of
broadband deployment is this because a
lot of this is from people at work all
right we noticed far behind us bateau
well I can't tell you for sure this is
because people at work I think it's
believed that there's much higher
bandwidth and worked in the home so
there's a lot of broadband moving into
the homes now we have not segmented
these IPS by home or work and I don't
think we have the ability to do that but
as you can see there's certain countries
in fact you look at that report you'll
see Korea huge you know high broadband
rates and you can see that in the report
based upon the IPS in fact let me just
show one last at here this is showing
how many IPS per capita so if you took
the population the United States you can
now buy x point 35 and you would see how
many ip's we saw and did this
measurement against in the North America
and it gives you a feel of course North
America has more I piece per capita than
SE Asia does
that gives you some way of maybe putting
this into context the bottom line here
is though that the last mile
connectivity is pretty good and it is
reachable if you're serving from nearby
you can get today good broadband to you
know at least half of the IEPs today
which we loosely correlate to half the
end users so I'm back I'm Steven Stewart
and for my brief introduction I've
started a few networking groups here at
Google my current internal title is Iron
Chef networking which I like because it
sounds a lot better than loose cannon
but I currently work in in production
netops which is the group that builds
and operates our network that faces the
capital I Internet I'm going to talk
about a project that we stood that we
recently launched called measurement lab
it is our attempt to help so I'll make a
brash statement it's our attempt to help
repair the damage that that has happened
in the relationship between academia and
industry there are a lot of very good
relationships in that you know that can
be characterized as you know academia
and industry working together but we
found that in the internet space in the
capital I measurement of the internet
space there's a lot of progress to be
made mainly in the area of helping
academic tools get industry style
deployment in the capital I internet we
we like a lot of the tools individually
but when they are deployed at scale they
have problems reaching users so on and
so forth so who is it um it's this set
of folks why and
again this the not to date myself but we
used to we used to call I guess what's
now called / dotting we used to call it
surviving flash crowd and so we would
like to have academic tools deployed an
infrastructure that is capable of
surviving flash crowds the focus of em
lab is active measurement of
infrastructure this is in contrast to
the kind of numbers that we actually
collect as a that we Google will collect
or that Akamai would collect as a as the
serving end of serving infrastructure
here we want researchers to have the
tools to develop a detailed view of what
happens when they interact with the user
on the internet and ideally if we can
help them with the marketing with lots
of users on the internet the probably
the best known tool on this in terms of
public notoriety is glasnost which was
the tool that was used to determine
whether comcast was interfering with
with bittorrent and they got / studded
with vigor once people figured out that
they could use it to determine whether
comcast was interfering with with
bittorrent our goal here is network
transparency you know we are we are
trying to bring data to debates that
haven't had a lot of data basically all
you know all of the all of the
discussion of network neutrality on
Capitol Hill for instance seems to be
conducted without the benefit of data so
you want to know what it is think
planetlab it is run just like a planet
lab install would be run except we have
a minimum baseline for what kind of
machines and what kind of connectivity
are deployed we have you know three
fairly modern servers you know
multi-core decent amount of memory all
connected by gigabit to the Internet
so that we can do the best that we can
to ensure that the server end of the
measurement platform is not the
bottleneck in taking any in taking any
measurements and that's our our current
deployment scorecard just brought our
fifth North American note up last night
so I hastily went in and corrected the
slides so I'm going to present some
results from mm lab it's important to
note that these are the results with
researchers who are using this platform
as a means to make their tools visible
to end users the mm lab would not be
without these folks and the others that
we hope to recruit to turn this into a
first-class clearinghouse for running
tools that would collect active
measurement data from users in the
public Internet NDT brief show of hands
if you're familiar with it well goodness
you should all go and test your home
broadband right now this is more
sophisticated than the the speed test
stuff that you would find from from some
of the broadband providers who you know
they want to you know basically just you
know throw up a little speedometer to
help you decide to use their broadband
connectivity rather than someone else's
there's a lot of what we hope is helpful
configuration advice that goes along
with conducting this test to help
educate users on what it means to you
know for instance tune buffers and what
it means to get in and tune TCP we
actually in this tool try to detect and
advise on problems like duplex
mismatches its proposed that a
non-significant a non-trivial amount of
internet performance woes are due to
duplex mismatches in home networks that
people have no idea how to detect or
correct the this is one of the some
reviews of what NDT he tries to
to tell us about what the internet looks
like it's rewarding it's important to
remember that all these results are from
the people who sell or the set of people
who self-select to use this test and we
see that we have a lot of cable DSL
users elected to come use this and we
think but can't prove obviously that
they might be attracted by glasnost to
determine if they're good torrent is
being blocked but because we have all
these tools in one place they're
encouraged to use other tools so we're
hoping to get a lot of cross-pollination
from having all these collected in one
place where users can self-select to do
a variety of tests rather than just the
one that attracted into the site to
begin with so here for the people who
used NDT the bottleneck link in most
cases was determined to be a cable modem
one of the other things that we collect
we integrated the the web 100 Colonel
patches into the planet lab OS in order
to be able to collect web 100 exit
statistics from TCP sessions and so and
to be able to to pull the kernel for
detailed information at the program 50
the performance of TCP during sessions
so during this NDT 10 second test we
pulled the congestion window versus time
at 5 millisecond increments and so what
we see here is slow start to to the to
the users 64k buffer and then we start
to see events that that bring you know
the congestion window way down you know
here you see you know slow start trying
to to bring it back up until you reach
the threshold where congestion avoidance
takes over and we you know we believe
that the the differing size of these
peaks in the sawtooth indicates
differing rates of of cross traffic
interfering with this particular TCP
session we can stack that up against a
TCP dump and see where we transmits
indicate that we've lost so on and so
forth so you know as we collect more and
more
this data we expect that it's going to
be I'll make the rash statement a
treasure trove of data into the poor
months the performance of TCP in real
live end-user broadband networks glass
noticed as mentioned was developed to
determine whether BitTorrent was being
manipulated what these folks found was
that twenty-five percent of users who
came to to test had their service
degraded class mist is also able to
compare BitTorrent protocol transfers
with reference flows that are not
BitTorrent and what they found was that
sixty percent of flows that have
differentiated performance sixty percent
of them were throttled 2.5 the
throughput of reference flows and forty
percent of them verbum were reduced to
two tenths of the throughput of a
reference low and there's a lot of
detail by ice P by continent by slicing
and dicing the results that's coming up
in there glad and their saw speed paper
papo to is one of our newer tools that
focuses on measuring available bandwidth
this is different from looking at TCP
and making a determination based on a
large TCP flow like a lot of speed tests
do it's actually using you know this raw
protocol datagrams to try to assess how
much available bandwidth there is in a
in a home network connection and they're
starting to come in with some results
that indicate what kind of through PG
says these are results prize p this one
here and it's an eye chart even for me
this red one here is telecom italia and
you can see their their their results
are all kind of clustered below 15
megabytes per second
be you know one of the things that we
are very much looking forward to doing
is correlating these results with other
tools to determine what was TCP
throughput in a test that's reasonably
adjacent to a path load test and if
they've got you know downstream and
upstream is clearly less you might you
might this is comcast here this time so
you might infer that this might be the
with the docsis through you roll out and
people might be coming to test diff
furthest is one of the ones that were
most excited to get going this tests for
discrimination looking at at flows of
different types getting back to the you
know is ICMP punished or elevated in
priority these are tools to try to
detect what kind of policies are being
in being subjected or being you know
packets are being subjected to policies
that cause the flows to be buffered or
police or or what have you some of their
preliminary results are one of the one
of the examples of bad behavior in the
network neutrality debate is whether
skype and vonage would be punished
relative to the native VoIP offering of
a provider and one of the one of the
preliminary results here's that that's
not happening in the internet today the
they have got results that show that
Comcast residential traffic shaping was
detected but per customer not per
application so we're seeing you know the
behavior of policing or shaping across
all flows associated with a residential
network customer but not targeting
specific applications at least not yet
and that is the end of my presentation
questions are from what standards walk
you can't into in put this into an
appropriate standards for and I'm
thinking particularly at least in a lot
of the ones i play with particularly
under the injian rubric you've got a lot
of people who are basically developing
specifications that sort of work against
you so having these available for
agencies that look at things like net
neutrality types of discrimination might
be a good thing and so are you
considering them so in well you tell me
if I've answered the question we are
planning on having one of the pre one of
the conditions for deploying a tool in
this network and NM lab is that the tool
is open sourced and that all the data
are made available eventually we prefer
that the the data are made available
kind of as its measured or as its
collected for researchers who have
concerns about access to the data before
they've written a paper we've you know
we take a page from our brethren and
astronomy where because this is kind of
like a telescope right you know
telescope the points up at the sky is a
big shared resource that people book
time on and this isn't terribly
different the goal though is to get all
of the data into the hands of
researchers whether they were the ones
who concede to the experiment and
collected the data or not so that you
know post post collection analysis can
be done with a different eye toward you
know answering different questions than
those that were necessarily the
questions when the data were collected
in fact we're trying to turn this into a
more general platform that's that's
constantly throwing off data rather than
the data that's just conceived of a
single experiment but the
as far as you getting into the hands of
standards bodies and whatnot it's really
the researchers that drive that you know
we we want them to take these results to
standards bodies into you know referee
journals and whatnot yeah well all the
comment is that regulatory agencies
could drive it too and we have one of
the one of the participants in EM lab is
New America Foundation whose open
Technology Institute is very much
interested in acting as a bridge between
the researchers and the regulatory
agencies to you know as I mentioned in
the beginning bring data to debates that
have not had data before and in fact we
have some of these results that i just
showed from the researchers have been
taken to regulatory agencies to try to
you know to try to bring data to these
debates yes one last question in that
and Greg can come up and so for the not
discrimination of skype voltage have you
tried to differentiate between video and
audio traffic to see if there is
actually discrimination against video
traffic one of our we're putting the
finishing touches on a call for for
tools and right now we have tools on the
site that are very much focused on TCP
as the delivery mechanism for for data
right then they you know they slice and
dice the TCP protocol to find out its
performance we would like to you know if
you will Commission tools that would do
the same for for rtsp as a video
delivery protocol for sip as a as a
means of doing void the some of the
challenges are you know how do you how
do you bridge this platform with the
PSTN for example you know can we you
know can we you know sign up for an 800
number that people can call from the PST
and to interact with the server that I'm
going to play some pre-recorded message
asks the user to input some DTMF tones
in order to assess the performance of of
the VoIP half of the call you know as an
example for people who are trying to
develop products in that space that data
might be really useful then you know
once you've done that how do you scale
so that you can have 5800 numbers from
different providers and provide the same
service to collect same data but that's
you know the that kind of operation at
scale is something that the research
community hasn't really had access to
and we're trying to expand their minds
with regard to what tools that could
deploy knowing that they have a platform
out there that's you know pops deployed
worldwide no minimum of a gigabyte of
connectivity you know to do with as they
please cetera et cetera Greg I'm an old
friend events fov I've been doing stuff
in network since before the internet
Yeah right so it's my i'm honored to say
a few words to this group about what my
team and I are doing this is just a
brief slice of what we're doing i hope
you might find interesting I apologize
for not putting up some micro second
resolution graphs of of large-scale
Google traffic I will do that at some
point but I just didn't have anything
appropriately sanitized for today so I
hope that the snapshot you're going to
get will actually be useful burp is our
cute little name for burst packet
analysis I think in Toms graph he
mentioned one point some links that had
six
package rub a six percent package up
makes people to scream around here and
so does one percent they'd like to see
zero but nobody's going to pay for that
so we live with on a tight balancing act
between buffersize the cost of gear the
amount of traffic we can send through
the links and acceptable packet truck
you drop a packet you lose a transaction
or latency goes up etc etc so why work
on this well everyone is sensitive to
packet packet drop seem to be caused by
bursts netops says that every one person
is bad please make it smooth our
internal Enterprise congestion seems to
be caused by microbursts these are
bursts whose duration is shorter than an
RTT so the congestion avoidance
mechanisms don't really see them in real
time and cans and of course you never
can put on buffers that are big enough
to handle all the traffic we're trying
to put through so what is the right size
for a buffer that you can afford that
will help you manage the burst well
nobody really knows because until we
started building special purpose that
sniffers you couldn't get a picture of
one of these things and the measurements
that you see at the end nodes don't tell
you what's happening in the core of the
network so so there was when we started
some of these projects there was this
feeling that if you could just do
traffic's moving on everything to smooth
everything out that life would be
wonderful so how much smoothness do you
need and if you had ultimate smooth
traffic would it help so I decided to
learn a little bit more about that and
it's takes the form of trying to use
statistical means to be able to predict
packet drop as a function of the traffic
that's being put on the link and the
buffer size that's where it started
that's not where it ends but that's what
we're doing so in order to do that
here's a overview with procedure first
we got time series from applications
from an individual application as well
as from
aggregate of many many thousands of
applications running over an aggregation
length and for the next few slides I'm
going to show two applications that we
measure both are trying to do a constant
packet rate file transfers very long
duration like a file transfer that takes
a few days using two methods the XYZ
that's not its real name but the XYZ
method was bulk UDP transfer app where
the rate control is being done out of
the application not the kernel or the
protocol stack the other one to disk q
disk is the linux name for queuing line
discipline that goes in the kernel and
it has a traffic shaper and there's
another application that we measured
which is a competitive in some sense
with XYZ so we looked at both of those
trying to do the same thing but using
different means what do we do we take a
look at the time series and we extract
the pmf from it which gives us the first
statistics general traffic statistics
then we spent quite a few months working
on change point detection change point
detection is a big deal if you're a
statistician what it means is if you're
looking at a traffic flow there's some
distribution that describes the packet
arrival rate and the bursts well what is
it and has it changed in the last few
milliseconds if you can determine that
it's changed then you've perfected the
art of change point detection that's the
technical word for that so we look at
our time series we try to determine the
underlying distribution but of course
there's more than one underlying
distribution so we work real hard on
figuring out which distribution applied
during which segment of time and where
the change points are so we've gotten
pretty good at that we've gone through
hundreds of methods right so I put up a
little graph that shows for the change
points was it doesn't tell you the other
99 methods it didn't really work so well
so we produce the distributions and
along the way we've noticed that these
change point detection methods are
actually pretty good for anomaly
detection so for instance we have a lot
of dashboards and monitors within Google
that are trying to observe when
something goes wrong and go get lots of
time series from a lot of different
sources in the network SNMP logs you
know all kinds of stuff there's quite a
large number of time see resources and
of course they all have underlying
distributions and if you can detect a
change point you may be well on your way
to saying hey something's gone wrong and
so we're where the team is actually
working on that with some pretty good
results having extracted distributions
we analyze them so one of the ideas was
this let's suppose I know that a filed
server inside Google has ten fifteen
thousand active connections and I want
to simulate that Orson or try to figure
out what the burst activity is of all of
those active connections with respect to
switch but for some place well what we
can do is what we decided to try to do
is to get a traffic distribution from
one client all right measure it and then
use a convolution technique to figure
out what the distribution is for 10,000
clients however many you want and that
seems to work so if I want to figure out
what the traffic and the burst dynamics
are for an aggregation length that's got
a few million connections on well I can
take samples from different single
servers say within Google or any place
and I can use this convolution technique
to figure out what the aggregate
cumulative distribution is when I stick
a million of them together and if I do
that then this last technique the
modeling technique can help us answer
some questions about packet drop with
respect to certain size buffer so that's
what this is all about so I'll show you
a few things too that we were able to do
we took the the two applications that i
already described we extracted two
distributions and initially we use a
closed form solution for what is the
probability of an overflow during the
next time window our time windows were
one millisecond so for as a function of
buffer size now that's the probability
within the next time window if you were
starting from an empty buffer alright so
this would
is not particularly useful and I gave
this presentation everybody so watch the
bag at drop rate per second and yada
yada yada well the closed form solutions
we were using don't yield that answered
most people with Intuit simulation and
we wanted to try to find some technique
that would give us the answer without
running a simulation right so that's the
that's what motivated the development of
the state-space model so first I'll show
you where we started and then I'll
explain how the state-space model works
and then I'll have to stop so here are
some pictures of the raw data from the
two apps this particular one you can see
that a plateau is it runs for a period
of time fairly constant although it's
bursty within its constant pneus
threatened be a word for that and then
it changed then it changes so what it's
really doing this server that we were
measuring is constantly changing the
number of flows that it's running
periodically and so what you're seeing
is the server changing its mind and
trying to maintain a constant constant
packet rate during each segment of time
and so the green blip over there is
highlighted in the next slide it's a
close up of what was happening right
around the green bar for this
application and you can see then it's
all over the place or it appears to be
all over the place well actually it is
all over the place but compared to the
next application it's actually pretty
good the top one is XYZ that's the UDP a
constant rate bit rate in user space the
other one Cuba cutest this is what this
is how the Linux kernel implements
constant packet rate shaping because of
the fact that it really likes to burst
it's really the best i can do is take
the burst and spread them out over time
those peaks are the the sort of natural
64 kilobyte bursts that's inherent in
the stack plus the driver and what this
thing is doing is controlling how many
of those blobs per second puts out so if
what you were really trying to do is to
minimize the bursts that you see when
you get a thousand of guys doing the
cutest call coming together it's not
going to look too good because the
probability that several of them tried
to do some
first at the same time is pretty good
right so when you scale things up these
bursts get worse and worse and worse so
that is you can read that off on the
right where the standard deviation of
the XYZ is much less than the standard
deviation for the other one now I'm
going to zip through another couple of
graphs because the results are really
sort of obvious the graphs really show
that under the constraints of the closed
form analysis that we did based on these
distributions the top guy is less likely
much less likely to overflow its buffers
during the same time window twice the
second one that's fairly obvious result
but there is this oh right this is the
point if your traffic has high variance
compared to low variance and you want to
minimize the drop rate then you have to
run at a lower utilization and so these
are wonderful hand-drawn graphs there's
some time going backwards effects in the
red thing that some people get a giggle
out of but this is a notional diagram so
if you can solve the problems that I've
been outlining here you can then
determine where you want to adjust the
utilization on your link if you want to
control the packet drop well if you're
getting ridiculously high deep burst and
you want to keep the packet drop rate
down very very low obviously you must
have a very low utilization and so the
vp of operations will look at the
utilization figures and say hey you're
running this thing at twenty percent I'm
paying for it I don't want to do that to
figure out how to run it at ninety
percent everybody wants you to run a
very high utilization and then these
bursts just kill you and the other thing
I can mention because I didn't have a
graph that was suitable to show is that
I haven't said anything yet about qos
what happens in qos is the high priority
traffic which is bursty always goes
through the low priority traffic is
clipped to some limit and when the high
priority traffic comes in it's the best
effort traffic that gets slammed because
we don't drop packets for the high
priority traffic so bursts on the high
priority traffic cause the best effort
traffic that look terrible and so you
can think of this as collateral damage
and it's pretty much invisible too well
most of the tools and most of the
protocol stacks that is the problem and
of course as I already said it's
unacceptable to run the overall
utilization on the link low enough so
that this doesn't happen that's the
problem oh I should also say that I've
got my own ideas what should be done
about this I'm not talking about those
today but you can probably sense that
working on some very careful measurement
tools and gathering data like this is
intended to support the engineering
decisions associated with what we do
going forward in terms of what's but
precise what card where to buy what to
do in the protocol stacks all this stuff
is related and it's fun to work on this
these are I want spending time on these
this simply showed that the XYZ method
was better than the other method if
someone really wants to look at the at
the the charts week we can do this but
the results were for this they were you
know they wish they showed that well
there was a difference so how to do
better than this to actually get rate
packet drop rates here's a simple little
experiment you have a and B sending to
see they're going through a switch
they're running a test program that we
have around that simulates the RPC
traffic so they're sending random one
megabyte RPC messages to see and the
load is set so that it's going to
overflow the switch right and I haven't
drawn the measurement equipment but all
the links are tapped so we have a trace
of packets going into the switch and
trace the packets coming out of the
switch and this is just absolutely
hilarious to me right because you detect
congestion when you see the packet go in
the switch but not come out you know
switch as Mouse packet rep and so we've
done this on a fairly large scale so
that for some of the switches inside of
our infrastructure we can do this for
all of the links going in and out of a
switch not just a B and C right
and with that and micro second or less
than one microsecond resolution data you
can really start a get a good picture a
real picture of what's going on with
these microbrews so when we do that and
we've developed the software to look at
the stuff that identifies the dropped
packets figured out the drop packet rate
and then we extract packet distributions
as I was illustrating on the previous
few slides for the inputs and the
outputs and the drops at pretty good
resolution so this shows for this nearly
trivial case the top thing is the is the
distributions for the two flows the
middle trace is the distribution of the
aggregate flow the sum of the two and
I'm showing the measured aggregate
distribution as well as what we computed
it would be using the convolution
technique and because the the things are
overlapping each other it's hard to see
what the differences is so on in the
bottom we're showing what the error term
is between aggregated and the computed
distribution it's quite good the theory
the mathematical theory is quite sound
so we think this is going to scale up to
you know a gazillion flow is whatever we
want the beauty of it is that if you
believe that the next thing I'm going to
show you is worth it this lets you
synthesize the statistical properties of
a nearly arbitrary combination of flows
this that's so without running a
simulation this is really kind of sweet
so the way this works is we use some
results from trellis coding theory to
build a state space diagram where the
states are labeled vertically s1 through
and it's whatever it is each one of
those states represents we you can think
of it as number of packets in a buffer
so at the top at zero it's empty and at
the bottom where it says overflow that's
an N plus first state we add to capture
the statistics for when the thing is
overflowed so for each one of those
states you can read out directly from
the distribution
of that we've calculated according to
their previous slides you can read out
how to compute the transition graphs in
other words an arrow from the left hand
side to the right hand side is the
probability that if you're in that state
you're going to go to the next state on
the right which is either you know more
buffers or no change or a little bit
less and how many packets you might drop
on along that that transition this is
also assuming drain rate so if no
packets are coming in we assume in the
time period the drain rate are going out
of the buffer so the buffer is draining
that's how so we're assuming that the
buffers are drained that well summary
pick a number anyway so once you do that
you can then do a lot of calculations
with this oh I should also say that the
theory behind this assumes certain
properties of the time series one is
stationarity and the other one is that
the random variables are independent and
that's really a problem it's it's where
we're sort of like these distributions
are not perfectly station stationary
meeting that from one time one sample to
the next the the distribution may not be
exactly right and that's certainly true
for this case where we only had two
flows we think that is less of an issue
for thousands of flows right so that's
one copy out second caveat is
independent random variables well if you
think that there's a random variable
describing the traffic dynamics for one
flow ask yourself is it independent well
no it's not because we already talked
about collateral damage the fact that
the TCP slow start and all these things
can act in synchronization there are all
kinds of things that make them not
perfectly independent but for a large
enough number of slows their independent
enough so that this trellis state-space
diagram is approximately correct right
in other words good enough right after
all we're using Ethernet you know we
might as well use something like this
because it's
that's good enough no flow control yeah
what the heck you know cheesy people fix
it so so for this nearly trivial example
the red line at the bottom shows the
measured packet drop rate which was
constant and the we were very
pessimistic and predicting in the
calculated packet drop rate but we
believe that for the size of this
experiment that says we're sort of on
the right track so obviously what we're
doing is expanding this acquiring more
time series especially on the really big
links scaling this up to millions of
flows I'm also interested in measuring
something called packet dispersion all
of the TTP's texts in the world try to
send now first back to back packets well
that's what goes into the switch what
comes out are they still back to back
when they come out or not well there are
many theories on that as there are
people that understand this problem so I
am to you know make some pictures we try
to aunt we're trying to answer that and
I also mentioned that Kos is another
destabilizing factor especially if one
of the things you're trying to do in the
place where you get the most complaints
are from the people to have the best
effort traffic we're getting slammed by
the other burst right so the interaction
with Kos is also important and we also
have links very high bandwidth links
with low utilization in other words only
sixty percent they still get package
drops and I hoped that we would have
some micro second data to show on those
today but the equipment is still being
set up so we will publish that in fact
there's some time series that we got on
some servers I did this a request from
some researchers and at the University
of Barcelona and they agreed to make
those time series available to other
people and I think other people would be
interesting interested when they publish
their paper so when we get some more
really high bandwidth
I'm going to say to make it available
you know for people to work out because
I think it shows up some issues with
respect to qos microbursts and
congestion that I think are of interest
to everyone small medium and large
that's all I have thanks very much I
thought everything was perfectly obvious
finda play that's a challenge a really
stellar stuff there I'm blown away I got
distracted up several epochs back in
your in your talk we're going to go back
no no no when you talked about eighty
percent versus 90-odd percent yeah and
people not wanting to pay for only
eighty percent utilization and it it
strikes me that we all have grown up the
whole industry has grown up with percent
utilization as one metric and then
jitter and other things as other metrics
and it's that percent utilization on
which you get sucked into saying well
we're underutilizing and therefore we
got to try to raise that and and with
the other statistics suffer this is a
see if I can frameless question right
it's not only to you but to anybody else
who lives in there would it be possible
to replace those particular dimensions
with some other dimensions that
aggregate and or slice and dice the
space better so that it's quite evident
that as you raise utilization you're
making some other aspect much poorer
using the risk and so forth so that we
can change the vocabulary well this is a
social engineering problem not a
technical problem and second part of my
answer is that since I'm mostly studying
Google internal enterprise traffic there
are things that we can do don't really
work on the internet in particular if I
could show you some real traffic graphs
you'll see there's a diurnal pattern
with you know peeps during the day and
there's weekly patterns and there's
monthly patterns all these are
detectable so
I normally you see the best effort
traffic if this is the graph right
imagine the ground and you break it out
by QSR you see best effort down here
fairly constant alright and then you see
the diurnal load traffic going like this
obviously what we could do oh so the
utilization is less during the trunk
than it is during the peak right so
obviously what we really ought to be
doing is raising the traffic shaping
limit on the best effort perfect during
the trough so we can go up and go down
and so then you increase the average
utilization tada right can't do that on
the internet right because the internet
because you don't have that control
right so I can do something with traffic
utilization on our links with a
two-hundred-thousand-dollar remember
that line cards at which you can't do on
the internet so that's a long about way
saying I don't really have a good idea
on what you can do for in utilization
third thing I would say comes from high
performance computing you want to make a
parallel computer you want to make it
really go fast it's not going to go past
all the time you pay for a lot of idle
machines so that when you hit the people
in the computation they could all be
running in parallel that's what you pay
for you want a racing car gets lousy
mileage and so some of the trade-off
between not running at one hundred
percent utilization all the time is so
that you can handle the peaks in the
traffic right and that so it seems to me
that that what's what one can do is try
to fix it so the peaks can be spread out
over over say more links or alternate
routes you take it takes some traffic
during the peak times you send it over
routes that are not optimal more hops
something like that in order to be able
to run the whole thing at a higher
utilization spread traffic around but
that requires dynamic adjustments to
routes okay which make our existing
route routing algorithms look simple and
of course they're not simple in other
words i think i can imagine things that
can be done gap filling hole filling and
sets at her but i don't think it's going
to
easy to do especially with multi-vendor
multi equipment multi ISP alternatives
and kind of thing I think it would be
tough so the answer is I got some ideas
I don't see how to make them work yet
Thanks ordinarily we would have had all
the panelists up here for more Q&amp;amp;A but
it's now 10 15 according to my Ronald
McDonald watch and we were trying to
schedule a break for a half an hour now
I'd like to schedule a break for 15
minutes honestly and restart at
ten-thirty part of the reason for trying
to stay on schedule is that we have Eric
Schmidt coming right after lunch and its
really really important for you to get
back here from lunch to be here when
eric gets here I have 115 otherwise you
know I'll be in some jeopardy if the
room is empty so out of courtesy and
hope that you would come back so let's
take a break until ten-thirty and will
resume with the next panel and let's
thank this panel very very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>