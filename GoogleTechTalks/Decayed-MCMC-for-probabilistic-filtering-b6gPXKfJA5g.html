<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Decayed MCMC for probabilistic filtering | Coder Coacher - Coaching Coders</title><meta content="Decayed MCMC for probabilistic filtering - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Decayed MCMC for probabilistic filtering</b></h2><h5 class="post__date">2008-03-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/b6gPXKfJA5g" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">is my pleasure to injuries Bhaskar
Bhaskar
residents together Berkeley and these
two here to talk about DMC thanks mark
so this talk is going to be on an
algorithm for decade mcmc filtering let
me begin with a motivational problem so
showing up okay so so there's a standard
problem of here's a user and using
surfing the web and what is this user
want to does she want to buy something
to read something interesting just to be
amused and let's we can write this you
can see this is a problem with intent
estimation over time where you have a
user whose intent is unobservable
evolves over time and we have some
algorithm which we call intent estimator
which sees things the user does what
they click on what queries they type and
over time maintains an estimate of what
their intent is are they looking to buy
something say or are they just surfing
and once you have this estimate I guess
you can use this to decide various
actions such as what what ads to show
what search results to show and so on so
I and this is a feedback process
obviously because the intended from one
time step sort of effects what you think
in the next time step so I've drawn it
that way to sort of show the relation to
a much more general problem state
estimation so here instead of a user
there's an environment with the hidden
state evolving over time there's a state
estimator now receiving noisy
observations from this environment and
we want to maintain an estimate of this
hidden state given all the observations
we've seen so far and when we have such
an estimate we can use it to answer
queries and make decisions what what to
do so I'm going to talk about what goes
in there and I'm going to be talking
about a special case probabilistic
filtering so here we're going to make
various assumptions first we assume that
the environment has a state X T which
evolves according to a Markov chain p of
xt given xt minus 1 we assume that the
observations YT depend on the only on
the hidden state at time t xt
and we assume that what we want as a
state estimate is a probability
distribution over the hidden state as
opposed to je sais just storing the most
likely hidden state so ideally we want
to have the posterior probability
distribution of the hidden state given
all the observations so far and we may
or may not be able to achieve this for
four particular cases but we would like
to do you have an approximation of that
and there are some well-known advantages
of doing it this way which is that if we
maintain a distribution we can keep
track of many hypotheses if we're unsure
what the true state is we can quantify
how uncertain we are about things so so
going back to this example you can
imagine that there's a the hidden state
is the users intent which we could model
using say you know how how likely they
are to click on things how how sort of
willing they are to focus on reading
something long and so on and we can have
a model that of also a time of how that
evolved over time and models of what
they what they would do as a result like
with the pillar they would click on ads
and so on and I know that I guess people
who have studied these problems very
much so here's a couple of other
examples here's a this is the picture of
room with a large compute cluster and so
here we might have various questions
like a keeping track of overtime which
components have failed should be running
Diagnostics in any part of the system
should should we take any particular
actions to fix things and again this
this can be naturally modeled by having
hidden state being the status of the
various components hard drives network
switches and so on observations being
say throughput at various sensor nodes
in the network job completion rate of
machines and so on second example this
is a picture from the of the winning
entry from the DARPA two thousand five
grand challenge and controlling a an
autonomous vehicle like this requires
being able to answer various questions
like where am i right now is there a
card my blind spot this is this instant
do I need to refuel soon
and again it's you can model something
like this by having a state consisting
of your position velocity and also
location of any other vehicles
pedestrians fuel remaining and so on and
you get observations from various
sensors like camera gps rangefinder data
among other things right so so this is
really general problem state estimation
in specifically probabilistic filtering
and in this talk I'm going to be
presenting an algorithm for this problem
so first I will talk about existing
algorithms there are many existing
algorithms I will focus on two of them
specifically exact algorithms this is
the so called Bayesian update and one
particular approximate algorithm called
particle filtering which is sort of the
closest analog to the algorithm that we
will be introduced so before doing that
let me say what we're aiming for with
this filtering update so recall that
there's going to be an environment
producing observations why and at each
time we have a previous estimate of the
distribution of the the state at time t
minus 1 and we want to take that
together with the observation and
produce distribution of the state at
time T so what do we want from from from
from that box over there well first we
want efficiency specifically the the
complexity of what happens at each time
step should be independent of of tea so
in other words if the time we take to
integrate a new observation should not
grow with the number of observations
we've seen so far and the second thing
is it should be it should in some sense
have a mild dependence on on how
complicated the the underlying
probability stick model is and I've
chosen to say it should just be
polynomial in the model description I'll
be a bit more specific about what i mean
by by polynomial and model description
in a few slides beyond efficiency we
also want accuracy so we want one way of
saying that is that for all times the
estimated distribution of our algorithm
is close to the true the true posterior
given given the observations so far so
let me just for concreteness give us a
very simple example so let's say that we
have a situation where the states and
observations
both just small finite sets say the
numbers from 1 through n and let's say
we have the following Markov chain so
the way it evolves is that if you have
xt being some number then XD plus 1 is
XT plus or minus k with probably 1 over
K squared for all possible k so in other
words it sort of moves around
symmetrically and it's it's less likely
to move along distance so that's the the
state Markov chain and the observations
let's say that we most of the time you
just see what the true state is and
every now and then we can an outlier
that's that is just uniformly random and
of course we don't know which are the
outliers so we don't know if we saw the
true state or not so here's here's a
picture of a sample run and the top row
we have the hidden states which we don't
know of course and the bottom row there
are observations for 5 5 6 and 11 and a
filtering algorithm has to answer the
question given these observations what
is the probability of X 5 the state at
time five given these observations 4556
11 so just for intuition I mean if you
look at this observation sequence there
are kind of two possibilities one is
that 11 is a is just an outlier an
observation outlier and really the state
has stayed around five or six for this
entire sequence second possibility is
that maybe it really did jump to 11 at
the last step and so you can imagine
that the distribution looks something
like this if there are two modes one at
six in one at eleven so this is the kind
of thing that a filtering algorithm
would do okay so let me start with the
the exact solution to find the true
posterior distribution so let's say
we're given p of x t minus 1 the
distribution over the hidden state at
time t minus 1 given the observations up
to t minus 1 and now we have this new
observation YT so there are two two
steps the first step is to propagate
forward and find the distribution of XT
the state at time T given given
observations after t minus 1 and this is
the summation or integration if X is
continuous and secondly there's a
conditioning step using Bayes rule where
we incorporate the effect of the teeth
observation
yeah so this is completely general it
doesn't it applies to any distributions
for the for the state Markov chain in
the observations but whether or not it's
efficient depends on the specific
distributions we have so the first case
where it's efficient is known as the
hidden Markov model ubiquitous model
probabilistic model so here we assume
that the state space is this finite set
say 1 through n with transitions given
by a matrix the transition distribution
given by an N by n matrix and here the
the exact algorithm I showed in the
previous slide is called the forward
algorithm and it can be done in time n
squared so in this sense it's polynomial
because to describe the model takes you
know an n-by-n matrix and the time the
update time is also n squared so that's
efficient second widely known case where
it's efficient is linear system so here
we have the state space is now a
continuous state space it's consists of
real vectors and dimensional and the
distributions are all linear Gaussian
and we will always in this case we will
always maintain the filtering
distribution is a Gaussian with a mean
and covariance and here again it turns
out that you can do the update that is
update the mean and covariance in time
and cubed where n is the dimension so
again it's polynomial because to
represent the distribution takes already
takes n squared numbers so n cubed is
still the polynomial but that's where
the good news ends because those are the
only tractable cases so it turns out
that many popular families of
probabilistic models are all intractable
for this for this exact filtering update
so this includes dynamic bayesian
networks mixtures of gaussians hybrid
models of n various sorts undirected
graphical models and I won't go into all
of these but let me just talk about one
dynamic bayesian networks so I on the
right here I've shown a dynamic busy
network that the basic idea is that for
DB ends the state X is divided in two
variables let's say x1 through xn in
this case and we have some
the dependency model is local in some
sense so let me give an example so let's
say we have X being x1 through xn where
this represents a model of propagation
of failures in network so let's say that
X is 1 if machine I is down otherwise
it's 0 right and let's say the machines
are arranged in this sort of a ring kind
of topology so that you can imagine
failures propagating from a machine to
two connected machines in a network so
if machine I fails then in the next step
machines i minus 1 and i plus 1 and more
likely to to fail so we can represent
this using this graph where the arrow is
basically a mean dependencies so you
know this XT plus 12 depends on X T 1 xt
2 xt xt 3 so the thing about this model
is that even though there are 2 to the N
states for the for the for all the
machines you can describe the model
using just of n over n bits of n numbers
because we just need to say how each
machine depends on each machines failure
depends on its itself and its neighbors
in the previous step so in this sense
you can describe the model using you
know polynomial in n numbers but can you
do filtering in in polynomial in n time
and the answer is no so unfortunately
because it turns out that even though
the model can be represented in this
compact form the posterior distribution
over time results in all the machines
becoming correlated and so it requires 2
to the N parameters to represent roughly
so so even writing it down takes two to
the end so the filtering update has to
take that long and this is borne out by
you know you can you can easily verify
this so I've shown a graph here where
the x axis is the instant size of that
of that model I showed in the previous
slide and the y axis is log filtering
update time and the green line is how
long the exact filtering update takes so
the fact that it's a straight line on
this log scale means that it's
exponential time so so that's bad and
meanwhile the blue line is our algorithm
decayed mcmc and i haven't described it
yet but you can see it's it's sub
exponential in fact alternative
polynomial and so it is practical on
that on that class of models and I'll be
describing how that happens okay so so
far we have exact filtering which is
intractable for most models let's talk
about particle filtering so in exact
filtering if you remember there are
these two steps you have the previous
distribution in a new observation and we
propagate in condition so that those are
the two steps it happen on each
filtering update and what during these
steps we are always representing the
distribution by either a probability
vector or a set of sufficient statistics
in the case of a Gaussian say so
particle filtering is basically that
except we represent the estimate by a
set of samples instead or particles as
they are called so it's a Monte Carlo
method in that we represent a
distribution by samples from it and
basically it's you just have to change
the propagation and conditioning
operations to be sample based so let me
just have a show a quick sketch of how
that works so in this situation we have
three observations so far for five and
five and we have unknown States x1 x2 x3
and so at any point in particle
filtering we have a set of particles so
I've shown the squares up there each of
which represents a guess about the
current state or sample about the
current state x3 so here we have
particles that we have two particles at
51 at four and one at six so it's sort
of a distribution peaked it at five
basically and so the way it works is
that when you see a new observation we
propagate forward each particle that is
for each particle we sample from the
transition distribution conditional on
it and then we incorporate the evidence
by re waiting each particle based on how
much it matches the evidence by the
likelihood so that makes some of the
squares bigger there and then finally
resampling so here we now have five six
six and six reflecting that the
distribution is things its most likely
six so that's how each step goes in
particles will train and so as you can
see the the complexity of that depends
on how many particles you have basically
so if you have a fixed number of
particles then
then the update complexity just depends
on that it doesn't depend say on how
much how many observations you had so
far so it's independent of T so that's
that's good and secondly it's also often
it's polynomial time in the model
description so for example in that busy
network example I'd shown earlier you
can show that the particle filtering
update is polynomial in in the in the
size of the DB n rather than in the
number of states so that's good but
whatever the accuracy so so what is
known about particle filtering is
consistency so to be precise let's let's
fix a time step T let's say T equals 100
and let em be the number of particles so
what we can show is that if you imagine
running particle filtering with M
particles up to time step T and let
consider the air that you accrue by
doing this so this is a random quantity
and as the the number of particle goes
to infinity this this random quantity
goes to zero almost surely so that's
good that's certainly necessary for the
for the algorithm to be reasonable but
but it's important to do not read more
into the it and it's saying so the point
is that this is for a fixed time step T
and it's lettin em go to infinity but
really when we run particle filtering we
have a fixed finite em and we're letting
T should have go so we're interested in
that behavior and what can happen is
that with the finite number of particles
particle filter can lose track of the
mode of the distribution so I had shown
so I've shown this example right where
you see this observation six then you
push those particles forward and usually
that's that seems reasonable but now
suppose that what had happened between
time three and four is that first of all
the state jumped to 15 and we got an
observation outlier so we saw six anyway
so this is obviously an unlikely event
but it's going to happen every now and
then just due to chance so the problem
is that particle filtering takes a long
time to recover from such an error so
suppose for example that we on the next
step we see we see 15 right the thing is
that particle filtering has a hard time
changing its particles that it has to
match that 15 it
to propagate them through the through
the transition distribution so it can
take a while so even though we see
multiple observations suggesting the
particle is a very wrong it can take a
while for them to just sort of stumble
onto the two more essentially they're
they're doing almost a random walk till
they find the mode and this example
points to the to the fact I mean the
reason for this example is that
essentially particle filtering is
recursive that is the samples you have a
time T are going to be reused at time T
plus one which we uses the series the
computation but it also means that any
mistakes you made a time T will
propagate into the future so if you
guessed wrongly in your particles then
you take a while to recover from that
from that mistake and the example I
showed might have seemed kind of
contrived but in fact it's ubiquitous in
high dimensional models this is like
probably the biggest problem that people
have in practice with particle filtering
and sort of intuitively what the problem
is that when you have a higher
dimensional model each particle is like
a guess about the entire state and the
more dimensions you have the more likely
you are to not guess correctly and
here's a graph showing what can happen
so X is time and why is the error and
the green line is particle filtering so
you can see that as this characteristic
behavior where it's sometimes it's
correct but then it makes a mistake it
tends to stay it takes a long time to
recover so it sort of stays far from
zero for a while before coming back down
and it has a couple of such excursions
and our algorithm is more the errors are
more independently spread out because
when it doesn't it's not recursive as we
will see and so it doesn't it will
recover faster and that sense from
errors right so so where we are is that
there are these two existing algorithms
exact which is interactable in many
cases of interest and particle filtering
which could take a long time to recover
from from errors and so our algorithm
was motivated by these two where these
two problems
so our algorithm is an mcmc algorithm so
mcmc you are markov chain monte carlo is
it's a family of probability in
inference algorithms and the setting in
MC MC is that you have some distribution
pi of x over some x and assume we can't
compute PI directly as early and maybe
we can't even sample from it directly so
mcmc methods are based on saying okay
we're going to set up instead a markov
chain k whose stationary distribution is
pi so in other words if you run em run
this Markov chain for a long time you
get samples approximately from PI given
such a Markov chain you can run it for a
long time get these samples and then use
the samples as a Monte Carlo estimate so
we can answer any question we may have
bought pie by averaging over the samples
instead and a special case of MC MC is
called Gibbs sampling so this is when
the target distribution is X is over an
X which is divided in two variables so X
is X 1 through X T and the idea and
gibbs sampling is that x is like this
vector at each time we're going to pick
one of the components of X so say I and
then we're going to flip the value of x
I conditional on everything else and
then we're going to leave everything
that's alone so so we just repeatedly
pick a component flip it pick another
component flip it and so on that's how
the Markov chain goes and it's a fact
that the stationary distribution of this
process is pi you can sort of show that
with little bit of algebra and so this
is an mcmc algorithm for pie whether or
not it's practical will depend on
whether we can do that second step
efficiently that is can we sample X the
i'th component given all the other
components of X so we we are going to
use a gibbs sampling algorithm for for
filtering and before describing it let
me just set up some some terminology so
there are going to be to Markov chains
sort of floating around here first of
all there's the the environment itself
which has hidden state and that's
evolving a going to a Markov chain I'll
call that the physical mark
I've seen p of x t plus 1 given xt the
state space is the hidden state of the
system and secondly there's the the so
called computational Markov chain which
is the mcmc markov chain so here the
state space of this Markov chain is
entire state two directories so X 1
through X T that's what each state of
this computational Markov chain looks
like and the goal is to is going to be
to make the stationary distribution of
the computational Markov chain be
precisely the distribution the posterior
distribution given given the observation
so far p of x given y 1 through t so how
would that look well so here's here's a
filtering situation so let's say we've
seen these observations and to see how a
Markov chain works we have to say harder
harder update how to simulate from it so
let's say the current the current state
of the computational Markov chain is on
the second row there for 55 65 so how do
we how do we flip this well this is the
standard Gibbs sampling recipe so
randomly pick one of the components say
the fourth one and now resample just
that hidden state given everything else
so that's that's what we have to do on
each step now one optimization we can
make right away is that although we it
looks like we have to compute the
conditional distribution of that that
hidden state given everything in fact
because of the the Markovian assumption
really we it only depends on the
previous and next hidden state and on
the current observation just those three
variables so computing that that
sampling distribution is is not bad in
that sense so that's good and then what
we're going to be doing is we're going
to be running Gibbs sampling getting
getting a you know a sequence of state
trajectories and from each one we're
going to just save the very last the
very last hidden state because
ultimately we care about the
distribution just of that hidden state
so we can throw away everything else so
so that's that's a perfectly valid
filtering algorithms but is it is it a
good one and the answer is no so so if
you look at the the gibbs sampling step
right sample are uniformly and flip flip
acts
value so intuitively on because we care
about getting a good estimate of of XT
the teeth hidden state we need to flip
that at least a constant number of times
and that's going to take T steps right
there so it's not good because we don't
want an algorithm whose whose time is
growing with t and so the idea with the
algorithm is that since we only care
about the marginal distribution of X T
we should be we should sample that more
often because that's the only
distribution we actually want to be
accurate and the thing that supports
this is that Markov chains have
exponential forgetting which which says
that the state K steps in the past in a
Markov chain has an exponentially small
effect on the current state on the
distribution of the current state so
this this is a property of the physical
Markov chain and what this means is that
we don't need to have a very accurate
estimate of the distribution of the you
know of the the state hundred steps ago
same so the algorithms going to be
sampling the distant past less often and
the way you can do this is by just
changing Gibbs sampling so instead of
flipping a randomly chosen a uniformly
randomly chosen time step we're going to
choose from this decay distribution
which is biased towards the reason past
and what we want from this decay
distribution well it has to be positive
for all for all the times so we should
have some probability of being able to
sample any time no matter how far back
it was that's for correctness and it
should also intuitively it should the
case lower than the forgetting rate of
the physical Markov chain and so it
turns out that a polynomial decay will
end up doing all those things so we use
this decade DF k is k to the minus alpha
for some alpha bigger than one so for
example of alpha was two then dfk would
be 1 over k squared so we'd be sampling
things k steps in the past with
probability 1 over k squared
so yeah I mean ideally you can do that
but the forgetting rate might be hard to
compute it may also vary with the
observation sequence in different parts
of the change so a polynomial will
dominate any exponential yeah yeah not
strictly slower but yeah no faster so
related question I mean you're not
talking about Constance here but you
could imagine a situation where even
though the dudes forgetting rig is
exponential is sort of close the
constants in that make it very very
little DK single produced three states a
yeah and and then if you choose your
cave to minus alpha with a different
constant you could be really under under
sampling say the state the the third
step back you mean you give you choose
the wrong alpha know what you don't
right so if you're you assuming that you
can shed because it's an expensive decay
choose a polynomial you are happy but do
you ignoring the factor the constant
factor in the in order first the recent
states actually want the function would
be yeah don't so it could be it could be
that even though in terms of sort of
asymptotics is this is a good choice in
terms of the sort of a short-term memory
it's a very bad choice you could be
arbitrarily bad actually right cause you
couldn't the constant and if you allow
the model to become arbitrarily
deterministic then this can be
arbitrarily bad in that sense yes this
is true and yeah that's a limitation and
I'll sort of talk more about that but
yeah yeah the choice of alpha and
general is obviously I mean a problem
right so anyway there we have this
algorithm and so the question is does it
is it a efficient so first property is
consistency which says that let's say
you're fixed evidence sequence let n be
the number of samples as I'm goes to
infinity error goes to 0 so this is a
way you definitely need this property
and it's
pretty straightforward for for the final
case at least but that's not really
enough because that you know ultimately
we don't be running our MC MC to
infinity so how long does it take to get
an accurate answer so when we have this
question about MC MC algorithms usually
analyze it in terms of two things first
of all how long does each step of the
mcmc take each step of the computational
Markov chain and secondly what is the
mixing time that is how many steps will
be need of that computational Markov
chain where the mixing time is defined
as s such that no matter what no matter
where you start the Markov chain off
after s steps it's sort of spread out
all over the state space to within
epsilon of the stationary distribution
ok so if we sort of apply that to two
decayed mcmc the problem we run into is
that intuitively to mix that is to have
an accurate estimate of the distribution
of the the state trajectory you have to
flip each hidden state at least once and
so already that's linear in the length
of the observation sequence so that's
not good so so to dream this we we
extended the notion of mixing time so we
define what's called the marginal mixing
time so the idea roughly is that we're
going to we are going to run the Markov
chains just long enough so that the
distribution of the the teeth state XT
is accurate and the estimate of the
other distributions can be quite bad
that's that's fine so this node this is
called the marginal mixing time and
given this definition the main result
that we were able to show is that for a
particular environment model the
marginal mixing time of the khaid MC MC
for any observation sequence is a 0 of 1
we're in particular it doesn't it can be
bounded independently of the length of
the observation sequence so that's what
we want we don't want to be growing with
with T in that case and I won't go into
the details of the proof but it involves
extending to this marginal case where
is techniques from from MC MC theory
such as parts coupling and log Sobel of
bounds in the paper but let me talk
about that constant factor so as you're
saying the you know there's this
constant factor which depends instead of
how deterministic the physical Markov
chain is and so if it's very
deterministic then that's going to make
the the decayed mcmc chain also mix
slower and I have some ideas for for for
dealing with that okay well let me talk
about some practical considerations so
okay I've sort of been vague about what
you do with this Markov chain I mean
I've said you run it and you take
samples from it but how exactly does
that work so given a time budget say how
many change do you run which samples
from those chains should use and how to
initialize the chains so the most so the
knife thing you could do for MC MC
algorithm is to say well I'm gonna run
the chain for a while take the last
sample another chain take a sample run
the chain and so on so here you are
getting iid samples from approximately
the stationary distribution you hope but
no one actually does this because it's
extremely wasteful in throwing away most
of your samples so one thing that people
often do is one long change is run a
single chain for very long and take you
know all the samples beyond beyond some
burned in point and a third thing that
people do is something in between where
you run many chains but you take
multiple samples from each chain so
there's actually some debate between two
and three in the statistics community
which is which is better in general for
our setting there's some particular
constraints in the problem there which
is that we will be running mcmc
repeatedly unrelated problems if we're
filtering so that is you know at time T
we run mcmc given the evidence up to
time T at time T plus 1 we get a new
observation run mcmc again so we will
have these related problems and because
of that that suggests having parallel
change so at time T we're going to run k
computational Markov chains for some
parameter K and get some samples from
each one at time T plus 1 use the extent
the final sample of each chain from the
previous time step extend that by one by
one hidden state and you
is that initialize k new chains so the
hope is that if the model is not
completely noisy then the sample from
the last sample from the previous street
from the previous time step will be
somewhere near a high probability region
of the for the next step and so the hope
is by having multiple chains like this
we encourage is exploring multiple modes
of the distribution so this might this
can help to alleviate the problems with
with determinism some slightly okay so
let me just talk about those graphs
again so here this was if you remember a
graph where the x axis is the instant
size Ferdie bien y axis is log filtering
time so exact is exponential and decayed
mcmc is a polynomial there and so with
DB ends it turns out that so you can do
each step of decayed mcmc that is the
gibbs sampling step you can do that in
polynomial time I haven't really said
how that works but it's basically
standard graphical model influence
techniques and also the marginal mixing
time is often polynomial time in n in
the in the graph size so that explains
the 3ds graph so now if the particle
filtering example so there is so this is
self in time I'm not sure come with
respect to what the tolerance parameter
on the actual estimate so I ran this um
I ran decayed mcmc until like until
multiple runs if it were giving an
answer that was within like point two
five of the truth that's that's where
the y-axis comes okay and so if you
remember with particle filtering the
issue was this correlated error business
where once you make a mistake it takes a
while to recover and so why is this well
suppose the KD mcmc in particle
filtering our sample based but in the
mcmc the samples represent the entire
state
actually so what this means is that you
can in the in the light of new
observations you can go back in time and
say well that sample that I have 50
steps ago is wrong and I should flip it
it has the ability to do that and it's
non recursive and and in particle
filtering on the other hand the samples
represent only the most recent state and
it only resampled the current state it
can't go back in time and change its
mind and it's recursive which is good in
the sense of reusing computation but as
we saw it has this the potential to to
make errors that are that should have
lost for a long time okay so that about
wraps it up so in conclusion I've talked
about this algorithm decayed mcmc that
the sample based filtering algorithm
it's widely applicable and like exact
methods and it scales well with respect
to to both the history length and the
model size so the various topics for
future work one is to extend models with
more determinism I think they're various
ideas here like integrating with with
logical inference like sad methods and
also with with sampling a different sort
of time granularities the different
variables like sampling the ones that
evolve more slowly at a different time
scale a second thing is the trade-offs
with particle filters so neither of
these algorithms dominates the other but
I think you can get some more insight as
to which one will do well in what sort
of model and finally I'm think the idea
of focusing your your sampling on around
the variables that you actually care
about is more general than just
filtering it applies any very situation
when you have a large probability model
and we have a query about some small
piece of it you'd like to focus your
sampling around things that affect that
small piece so I think that's another
fruitful direction yep can you give an
example
um I mean if like the the graph there
was from a linear Gaussian where the the
observations were pretty the observation
model was pretty low variance and that
tends to cause problems for particle
filtering because it sort of becomes
overly sure and you get like sample
collapse problems basically so if on the
other hand yo model where it's extremely
the state transitions a pretty
deterministic that's bad for dmca ncua
particle filtering doesn't care about
that so much yeah particle beam is
navigation we going to mix so it's okay
i was comparison so what exactly did the
compared like was it the same running
time for particle filtering and your
thing or was it the same number of
samples or how we do hi I mean I should
I guess ideally you should compare a
running time I did number of samples
because they both scale with that like
linearly ladder is much cheaper uh I
mean it's not much cheaper I mean it's
uh because I mean in another case well
you don't have any burn off then you
shall burn off oh when I say it's I mean
at each sample is in each sort of flip
of decayed mcmc versus each particle in
particular train okay so at the end of
the party that had more samples to
generate its distribution than ftse
because some flips em since you cannot
use right like the additional burn off
yeah I mean that's a small amount so or
everything I call also working in the
past mcmc sure you know all the police
would happen to the past you mean I had
previous time steps yeah they don't
generate new sample they're not
generating samples here yeah it's uh
yeah it's it's it's not using them in
that sense except in the initialization
as I as I talked about they kind of get
you know you use to set the initial
thing but yeah yeah so um thank you for
listening and see there any questions
yep so your holes win
you're single sir fairly noisy and
you're unlike processes repurposed be
waiting for a long time and not really
know the transition having five stages
back because you're not getting a strong
signal something is there any way you
could say gosh things are looking worse
now right Mike might like it has gone
down maybe it's time to do more sampling
right so dynamically adjust how much
sampling you do based on yeah yeah and
you that's I guess one of the strengths
because I mean it's not like in
particular doing where you have you
can't let go back and change the number
of particles you used 10 steps ago but
you can like run mcmc for a bit longer
yeah I mean in their Diagnostics for
convergence that you could consider
using mcmc convergence Diagnostics it's
not just running longer you can also
change the Alpha right as long as things
seems fine been fine I don't need to
explore the past because I'm pretty sure
that I'm have been doing well but once
you kind of things go down the
probabilities are smooth and well maybe
I should look more the test evidence and
kind of yeah be more aggressive with
heart with alpha yeah yeah I mean the
other setting of alpha is yeah it means
something that I haven't thought about
very much but definitely can set that
adaptively distribution over the initial
state to see if it's not strongly
multimodal and maybe you know paying
attention to that as well because
personal if it's stronger bimodal and
and it's reasonably deterministic later
on you can sort of go down to pass it
might be worth really sorting out
commercials I mean the initial sort of
physical state of the yeah I mean
that'll to some extent happen
automatically if you if you imagine
initializing say I mean that haven't we
talked about how you initialize the
chain but if you initialize it from like
the prior before extinctions you'll get
like half I mean you get some Sam
solution modes yes then you should
perhaps if and the distribution is
multimodal strongly by lowes maybe you
should be ready to revisit that initial
guess
she has later on you should've outside
early on I see I mean you're saying that
I mean yeah this is this place the whole
determinism issue right like if you're
if you're stuck in a mode then you kind
of it's hard to flip this one one thing
because it'll be sort of inconsistent
with what's before and after it so my I
mean my feeling with through these very
deterministic models is that you have to
in a sense you have to flip the mode all
at once for the entire sequence you can
I mean if you have another variable that
encodes which mode you're in then you'd
like to sort of flip that all at once or
four large portion of the sequence like
this is you know like corresponds to
blocking in give something in general
okay got it backwards where you go to
observation things looking bad you just
start choosing values from the from the
current back you know the past or a ways
and they try to patch that into what if
your previous low so you kind of get
over the the local minima yeah yeah that
so in other words in sort of not take
not take into account you'll value nor
do a gibbs sampling step but some other
sort of more complicated well do you get
sampling look Paige go into the recent
past and then and then seed you can find
something that will hook onto that we're
going to share but there isn't future no
well is it well three samples you only
look at the future really on the past
when you're very good order if you could
prove anything about a sampling except
where you start clipping from the
present backwards and yeah keep moving
backwards and so you get no show then
you get no changed and everything back
beyond it you ready sort of sample that
space already nothing interesting is
yeah you cannot you can speed up get
something by sort of being never about
that like if you know in advance I mean
the set of crook yeah yeah it depends on
the specific I mean but algorithmically
you that there's improvement so that you
can certainly do like if you know that
if the thing is the set of indices you
sample is independent of like the values
you sample for the flips so you can kind
of use that to ignore a lot of your
sample or to not do a lot of your
samples if they're never going to matter
for the distribution of the current
state go back to another leak question
and insulin so that this because the
mechanism using you reset when you start
punking you extend the existing send
samples first and then and then set
inside your chains from there yeah so
this kind of imposes a certain kind of
inertia in this into the into the sand
point that makes a little bit of cursive
yeah like I am and so I mean in one way
that goes against your critic critique
of a purely recursive method but only
the hand for these bimodal situations
where you were we're early on you made a
decision am I going to the left or to
the right and now I'm in this corridor
and I deterministically going down to
one way for some that corridor which is
by the way many of the sort of particle
filtering examples you see right up from
robots going down cars at CMU so Horace
definitely tell you it happened right
exactly yeah you know but basically I
just bimodal situations with a lot of
determinism so I'm wondering whether you
can say something more precise about the
inertia imposed in your sampling by this
myth and we in relation to the kind of
energy you have in particle filtering
that helps you with a situation so I
don't know what yeah I mean I i think
this method is sort of like yeah it's
sort of when you have the initialization
it start it's sort of it's starting to
look like so the in-between particle
filtering indicating cnc like you can
sort of interplay between the two
methods perhaps I don't know what you
can say precisely I mean
I guess you'd have to look at the
particle filtering convergence pounds
and see how they depend in the
parameters of the model ultimately what
I mean in that bimodal case you know
that parallel chains here so you would
you would hope that if you have enough
paddle chains you get you get particles
in both modes at least I remember I
their clothes here probably more than I
but there are particles which are
approaches that insert new particles
right to help handle to the surprise
effect and getting out of these getting
in the wrong mode and there are some
sort of corollary here is sort of
starting new chains that would help you
to handle anything weird sort of you
know game yeah I'm yourself to say okay
all these traces are actually yeah I I
yeah I bet there's I mean like yeah
there are all these mcmc methods that
they're all these methods to improve
particle filtering with some mcmc moves
in addition to the like don t of
particles in yeah you could similarly uh
yeah you could certainly start in your
chains if you if you thought I was
necessary non-synthetic no unfortunately
I mean we didn't have yeah we wouldn't
so we did it on like DBN examples in
linear gaussians but yeah that was it
and that would be nice of course to
apply to your problems
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>