<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Seattle Conference on Scalability 2008: Chapel | Coder Coacher - Coaching Coders</title><meta content="Seattle Conference on Scalability 2008: Chapel - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Seattle Conference on Scalability 2008: Chapel</b></h2><h5 class="post__date">2008-06-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/dK8IdrJrYtE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">our next speaker is Brad Chamberlain
brad is a principal engineer at cray
where he works on parallel programming
models focusing primarily on the design
and implementation of the chapel
parallel language in his role as
technical lead for the project Brad
received his PhD in computer science and
engineering from the University of
Washington in 2001 where his work
focused on design and implementation of
the zippel ZPL parallel array language
primary particularly on implementing and
generalizing its region concept Brad
here is going to talk to us today about
chapel thanks man my own yeah okay so
thanks for much everybody for coming
this morning again my name is Brad
Chamberlain so one of the casualties of
us switching from the two track to the
one track format is I'd about 30 minutes
worth of jokes which I've had to
completely cut out of my talk so that's
the only one that survived and it's all
downhill from here so scalability I work
for crank and scalability is obviously a
crucial part of crazed business we build
large machines we want to allow our
users to get high performance in these
large machines and scalability is
obviously crucial for that my specific
job at Cray is that I want to help
enable our users to program our machines
more productively and I do that through
innovation language design some
particular I'm working on this language
called chapel which I'll be telling you
about today and for the purposes of my
work we define productivity as a
combination of performance plus
programmability plus portability plus
robustness so that's what we're trying
to do today this is kind of the thesis
of my talk and I'll spend a lot of time
sort of defending this thesis now we'll
come back to it so it'll probably make
more sense a second time around if you
look at the way people program hpc
machines today I sort of hold that there
are three main limiters to scalability
the first is that most of the models we
use today have very restricted
programming and execution models and the
single program multiple data or SPM
deprogram model which you'll hear more
about in a bit is one of the big factors
here the second is that our programming
models tend to expose a lot of the low
level implementation mechanisms
to the user and the third is just a
general lack of programmability and I
define this as not only the ability to
write code but also to read it to modify
it to knit maintain it experiment with
it sort of all the things you do with
piece of software over its life cycle in
the mainstream software engineering
industry these are very important in hpc
they tend to be sort of ignored a lot
more I think so as I said we're working
on this language called chapel it's a
new parallel anguish we're developing at
cray there's sort of five main themes
that drive our design the first is we
want to be a language that supports
general parallel programming so I think
of this is if you have a parallel
problem you want to write up you should
be able to write it up in Chapel there
shouldn't be a point where you say well
I've kind of run into chapels
limitations now I'm going to switch back
to the old way that I used to program so
we think of this general parallel
programming is being support for data
parallelism task parallelism and then
arbitrary nesting of those types of
Harold's and within one another we want
to be able to sort of express pearls
amid general levels of software so
executable level function level
iteration level statement level
parallelism and also to target general
levels of hardware parallelism so across
nodes across scores within a node
parallelism within a core if you have
that capability the second theme in our
design is to reduce the gap between
mainstream programming languages and
parallel programming languages so you
know most students are coming out of
school today probably learning something
like Java or maybe c-sharp pearl matlab
and then if they join the HPC community
we sort of put them through this hazing
ritual where we say well we program in
Fortran and MPI you know welcome to the
family and I think there's sort of this
sense of you know they've been
teleported back 20 30 40 years and don't
understand what's happened exactly so we
want to we want to fix that and at the
same time we want to not sort of
ostracize traditional HPC programmers we
don't want to come up with a model that
a C or Fortran program would look at and
say well I have no idea how to use this
because I just have never really
programmed in anything like Java for
example and then these last three themes
I'll talk about more of my talk so I'm
not going to really define them here but
the first is we want to have support for
global view abstractions the second is
we want to have control over locality in
the program and the third is we want
what we call a multiresolution design to
the language again I'll define those as
I go
so here's the way my tacos there's some
great X up there which isn't showing up
which says I've just given you the
motivation for chapel next I'm going to
talk a little bit about this notion of
the global view programming model what
it means and how it plays into
scalability I'm going to give you a
brief language overview I'll probably
have to skip through some of that
material at high level but I'll make it
available to you afterwards to look
through and i'll do some wrap up at the
end and in particular try to tie chapel
into the question whether or not it
would be appropriate for more of a data
center style program all given sort of
that we're here at the google conference
okay so what do i mean by this term
global view let me just start by taking
a very simple problem this is probably
one of the simplest interesting parallel
codes you could write you want to apply
a three-point stencil to a vector so in
particular I want to take this blue
vector and sort of maybe take the
average of each elements to nearest
neighbors and assign them to the
interior portions of my yellow vector
here and I would say that if I describe
this problem to you mentally the image
you'd probably get in your head is this
thing on the left and that represents
the global view of the problem the way
we think about it now the way people
program this in practice today is using
what we call a fragmented model and the
distinction here is that when you're
programming your code you're thinking
about what one process or what one
thread does you're not thinking about
the problem as a whole so the sort of
logical average these two vectors gets
fragmented essentially so the way that
this tends to work is let's say I'm
running on three processors indicated by
these red bars here I'm going to take my
vectors and first of all chop them up so
that each processor owns a piece of that
vector then I typically for these kind
of stencil style computations i allocate
a little bit of extra space to cash
values from neighboring processors I
insert some communication to update
those caches and only at that point can
I do the original computation I wanted
to do the averaging of the nearest
neighbors but of course is still I'm
writing about that on a process by
process basis as opposed to sort of
again at the conceptual global view so
let's look at some code to implement
this this is some code on the left that
uses a global view model in fact I think
this is all legal Chapel code so i
declare my problem size n to be a
thousand say i declared to raise a and B
to be of size of thousand where I
allocate a floating point value for each
element then I have a for all loop that
iterates over the
elements and does the averaging okay so
the main thing to note here is I've done
all this in terms of my global problem
size n global index face conceptually
the way the control flow here goes is I
sort of have a single conceptual thread
entering main and then when i hit
concepts in language which create
carlton like this for all loop i'll get
parallelism and it'll conceptually
contract back to a single thread at the
end again okay so in contrast using ESP
md model single program multiple data
which is one of the main fragmented
models people use in practice I would
define my global problem size again but
then I would compute what's the local
portion that each processor owns and so
I'll take that global problem size and /
our number of processors I'll then
allocate my local arrays over that local
size rather than the global size again
adding one to get these neighboring
values I insert some communication and
then i have the loop at the bottom
itself which again i'm expressing in
terms of the local problem size so
there's a little bit more detail
management that has to happen here a
little bit more complexity and in fact
this is a little bit error prone so the
first time i wrote this code i gave us
talk five or six times and finally
colleagues said you know you've got a
bug in your code there there's a
boundary condition on the sort of
extreme processors in that bottom loop
down there that you have to take care of
and so this purple code is sort of fixed
for that and these kind of problems tend
to creep in these codes all the time the
other thing to note here is that the
execution model is you've got sort of k
threads entering main executing the
whole code maybe taking different paths
through control flow but by and large
you have this sort of single static
amount of parallelism for your entire
code in this programming model one other
note on this before i go on is that I've
written this code sort of assuming that
my problem size n divides evenly by my
number of processors I could write a
more general version that require more
effort and so in practice people often
make these kind of simplifying
assumptions that make their code more
brittle than it perhaps ought to be just
because it's it's easier okay so this is
that same code implement an MPI which is
kind of the de facto standard for
parallel programming today it's pretty
much the same thing we saw just with
more arguments and things to take care
of the reality of the situation the
previous slide was more like a pseudo
code implementation so the main thing I
want to point out here is that you know
you could all learn this you could all
program in this
but this is we started with a pretty
example a pretty simple example here and
if we go to a higher dimensional array
say a 3d stencil rather than 1d stencil
then the amount of communication you
have to do expand geometrically so this
is an example from a benchmark called
the nafs multigrid benchmark or mg
protection stencil where we take a
weighted average of a 3 x 3 x 3 set of
points combined it down to one element
and if you program this up in fortran
plus mpi it results in this code here
again lots of detail management lots of
sort of chopping up these arrays doing
the communication so on and so forth in
Chapel that code looks like this again
because of the fact that we're a global
view language we don't have to
explicitly manage all of those details
in line with the expression of the
algorithm that's taken care of by more
advanced users and/or the compiler and
or the runtime I'm not going to walk you
through this code in detail this being a
scalability conference you would of
course like to see some results that
sort of show that Chapel is amazingly
scalable the unfortunate truth is that
this is very much a work in progress and
so I don't have those figures to show
you today however the concepts that I'm
showing you here these distributed
arrays are based very heavily on our
previous work in ZPL which was sort of
the data parallel subset of Chapel
essentially and I do have results for
that so let me walk you through those
before I get a deeper into chapel so
this is that same computations EPL it's
expressed a little differently you can
see the 27 points the stencil here and
the four weights of my cube over here
it's still concise compared to the
fortran plus MPI but there are some
differences from chapel and this is the
this the speed up we got for the CPL
code this is probably in the late 90s on
a crate III ii so a pretty small machine
scale compared to what a lot of people
are using today but i think the lessons
here still apply now this is a speed-up
graph meaning that higher is better this
dotted line should have represents ideal
linear speed-up and the CPL figures are
in yellow and green we had two different
versions we were looking at where's the
fortran plus MP is in the blue so the
nice thing here is not only was the CPL
code shorter more readable more
maintainable but it also performed
better and so why is ups why is that the
main reason that CPL scales better in
this case is running on the
preity 3e using a message passing
protocol like MPI is not the ideal way
to communicate it's very portable way to
communicate on a lot of plot of
different machines but the crate III had
better ways to communicate using single
sided messaging called a mem library and
because Cpl didn't express the semantics
of what implementation mechanism you
used for communication in the code the
copilot could map to ship am on the t3e
and then I more generic cluster that
only supported NP I could map the MPI
and do sort of civ excuse me similarly
to what you would expect for mpi the
other interesting thing is that CPL
actually performs better at the smaller
scales as well when you give new
language talks especially the HPC
community people say well for train gets
great performance why would we ever want
to use anything different we're going to
lose performance if we go to your
language and that's not necessarily true
if you design your language well I just
want to point out that this result is a
little bit old there are other languages
that have gotten similar kinds of
results showing that by taking
communication semantics out of the
language you get better scalability I
use this because I own the work I guess
one other note here is that the CPL
program again because of the global view
expression of the computation supports
an arbitrary problem size which you can
specify runtime arbitrary number of
processors which you can specify at
runtime and you can use a 1d or 2d or 3d
day to decomposition dynamically the MPI
binary for the kinds of simplifying
assumption arguments I made at the
beginning of the talk assume is the
power of two problem size power of two
number of processors both those are
specified statically it always used a 3d
data decomposition so it's it's a much
bigger code it's a much more much less
dynamic code as well in terms of what
you can do with it again you could
rewrite the code to relax those
assumptions but you know what cost and
performance it will cost in development
effort in the fragility of your code and
so on and so forth so this is sort of
again summarizing the difference in code
size here and just I guess I've made
this point probably by now the CPL is
shorter two largest em because you don't
have to put all this communication code
in by hand you don't do with all this
sort of manual bookkeeping of your
arrays as you distribute them across
multiple processors and of course code
size is always a strange metric my
argument is not only as it shorter it's
also easier to write read maintain and
modify okay just so to kind of summarize
this introductory section the advantages
of a fragmented model like
mpi are that it's a fairly
straightforward model of execution you
could pick it up pretty easily if you're
a computer science programmer it's
relatively easy to implement you get
reasonable performance on commodity
clusters it's fairly portable and
ubiquitous and frankly the best thing
you can say about is if you look at sort
of the major advances in hpc over the
past decade or so almost all of the
scientific work that's been accomplished
has been done in these fragmented or
SpMT models so they have to get a lot of
credit for that at the same time it's
not what I want to be programming in 5
10 20 40 years from now so there's some
big disadvantages this sort of
cooperating executable model of
parallelism is sort of a single level
parallelism and architectures have many
levels of perils and algorithms have
many levels of parallelism I want to be
able to exploit those things as I
mentioned it fails to abstract away a
lot of important architectural and
implementing mechanisms and I think it
obfuscates the algorithm with lots and
lots of low-level details which makes
your code more error-prone more brittle
and has referred to MPI often being
called the assembly language of parallel
computing because you're sort of
managing all the explicit movement of
data much as you would an assembly
language hoisting it out of memory into
registers and back again okay just to
kind of wrap this up so what people use
to program today they in hpc they have
communication libraries there's some
shared memory models there are some p
goss languages which are kind of a next
generation of language that kind of
falls in between MPI and chapel and so
the bad news here if you have my point
of view is that almost all of these are
fragmented or SPM d programming models
with the exception of the shared memory
models their global view but kind of
trivially because they assume shared
memory so they don't have a notion of
here and there and expensive and cheap
which is very very important as you get
onto these huge system scales okay so
they tend not to scale up as well okay
so going back to my main thesis
statement I said that one of the main
scalability limiters in these HPC
program models is restricting the
programming and execution models and
again I think this limits the
applicability of these program models to
the multiple levels of perils in that
exists naturally in hardware and in
software they expose a lot of low level
implementation mechanisms for example
MPI says too much about how
communication should be done as opposed
to simply saying what should be
indicated and or when it should be
communicated and it also most of the
models we have today bind the language
too tightly to a particular
implementation and hardware so a lot of
people do these things where they have
an MPI code for the internode
parallelism and OpenMP code for the
inter corporal ISM and then directives
for intra core parallelism as you sort
of have these three different
programming notations all of which are
talking about parallel computing I'd
rather have a language and that's what
chapels strives to do which uses a
single notation for parallelism and can
map it to any of those levels and then
finally lack of programmability lack of
programmability is obviously bad in
general but I argue that if you're going
to these really large scale machines and
problems you're not just going to get
great performance the first time you do
it you need the ability to modify your
code experiment with it try different
things to knit and to the extent that
your programming model makes it really
really hard or frustrating to do that
you're perhaps using the wrong
programming model all right so let me
give you a bit of a language overview
now and I'll break this down by spending
a little bit of time on just the base
language sort of the scalar language on
which we're building then I'll go into
some of the parallel features forecast
Krell ilysm task parallelism and data
parallel Keith you know I make my career
saying the word parallelism and I still
can't say tasking data parallelism and
then I'll talk about some of the
locality features we have for reasoning
about where you are on the machine and
then I'll wrap up at the end okay so at
a very high level chapels design was
very intentionally intended to be a
block structured imperative programming
language and the reason for this is that
while there might be advantages to
functional languages or more declarative
languages or other sort of interesting
program models the fact of the matter is
that the HPC community and in fact the
mainstream community is very rooted in
sort of block structured imperative
languages and so we felt that was
important design theme we also
intentionally decided not to create
chapel as an extension to an existing
language so it's not c plus something or
Java plus something and this frustrates
a lot of people because they think it
would be easier to learn maybe if we did
that and that may be true but there I
think there are a lot of reasons not to
do that one is political if you choose a
sea-based language you maybe turn off
your for trainer java or matlab
communities another is that you know
parallel programming at these scales is
very different than sequential
programming and so taking a sequential
language that wasn't designed with this
from in mind and modifying it is not
easy to do and it also tends to cause
programmers to sort of stick to their
old bad habits which may be made sense
in the sequential world but don't make
sense of scale so instead what we did
was look at languages either developed
in the industry or academia maybe they
caught on maybe they didn't and take
aspects from those that made sense and
try to create a new coherent language
out of those I will go through this
whole list of sort of concepts that we
borrowed or learn from from previous
languages but again these slides will be
made available to you so you can look
over them on your own time so this is my
one slide overview to sort of the base
scalar language that we're building on
syntactically speaking we're adopting
sort of this sea by which I mean C C++
Java and to an extent pearl family of
syntax whenever it's possible and use a
useful for us and this is just sort of
reflecting the fact that you know by and
large more and more of this sort of new
languages coming out tend to fall into
this family of syntax might as well sort
of build on that familiarity that said
we do have some pretty major departures
probably the biggest one is our
Declaration and our casting style syntax
uses more of a Pascal or modular base
syntax there I think are good reasons
for this I'm not going to make that
argument today but we could talk about
it afterwards if anyone's interested we
also use a pretty different syntax for
generics then you would see in like a
C++ for example and our for loop syntax
tends to be different as well due to the
parallel features in the language in
terms of the main language elements we
have more or less standard scalar types
expressions and statements you would
find sort of what you're used to finding
in most languages we support
object-oriented programming using both
value-based classes more like a C++
style class and reference based classes
more like a Java style class but we very
intentionally made this not an
object-oriented language of pure object
orange because we again we didn't want
to disenfranchise to sort of see in
Fortran programmers that are today the
bread and butter of hpc so it's there if
you want it and you like it and you use
it and you don't have to use it if you
don't want to it's not a pointer based
language we tried to very much restrict
the opportunities for aliasing in the
language in fact the ref
space classes are one of the main ways
that you get a licensing in the language
and because of lack of pointers we have
argument in tents which are somewhat
similar to fortran or ADA where you sort
of talked about copying things in or out
or just passing them by const to a
function this last list down here is
sort of a grab bag of base language
features but I really really like and if
you ask me to design any other language
I would try to argue to put these in the
first is iterators and not in the sort
of C++ or Java style but in the clue or
Ruby style a kind of function that
yields values sort of creating the
stream of values coming out of it the
second is tuples I think tuples are a
very simple concept and they make code
so much more beautiful than you know for
example function that can only return
one value we have a latent type scheme
so you can leave off the types of
variables of arguments of class members
these will be inferred through some
fairly simple static rules so you still
end up with a statically typed program
but you don't have to be typing type
information over and over again if you
just kind of in an explorer exploratory
programming mode we have a very rich
compile-time language it allows you to
do a lot of compiler transformations in
the language we have configuration
variables which are kind of meant to
solve the whole command line parsing
problem so I actually add command line
arguments to my programs which I never
do and see okay so moving on to the
parallel features I'm going to skip
through these pretty quickly because I
think the more interesting features for
scalability are the other data parallel
features so let me just say at a high
level this first slide talks about ways
that we create peril ism begin creates a
task sinks waits on a bunch of tasks to
complete that's all I'm going to say we
have ways of coordinating between tasks
these are in a data-centric manner we
have sink and single assignment
variables which are a way of tasks kind
of making sure that someone else has
written or read a result we have atomic
sections which support transaction
against memory using either hardware or
software transactional memory techniques
VJ i'll be talking more about
transactional memory in his talk later
on today and then we have a few ways of
creating structured tasks using sort of
a compound statement style co begin and
a loop style co for all so again i
purposely sort of flitted through that
quickly in the interest of time again
look through these slides if you have
questions I'd be happy to talk to about
an offline let's talk a little bit more
in depth about the data parallel
features in language
Chapel as ZPL before it has this notion
of a first-class language concept that
represents an index set you can think of
this is representing kind of the size
and shape of an array without
representing any of the data that would
be stored in the array so kind of like
the silhouette of the array if you were
to squint your eyes at it we have a
pretty rich set of these you can have
integer tuple style index sets which
give you sort of your traditional
rectilinear arrays as well as stride it
in sparse subsets of a rectangular
arrays and then we have anonymous
indices anonymous excuse me anonymous
domains which allow you to do sort of
graph style computation and we have
domains / arbitrary values which is kind
of more of a hash table or dictionary
style index set so here I'm doing a
domain of indices which happen to be
strings this slide just shows the
different declaration sin taxes for
these I declare these because their
first class object I can declare them as
variables named them give my type and
value I can reassign them over time so
they really are first-class one of the
main things you do with these domains of
course is you use them to declare arrays
so here for example I'm saying take my
dense domain and give me a dense array
where for every index in that index set
I allocate a complex value here I do the
same thing for my sparse domain saying
for every element there give me a real
floating point value you can also do
this for the graph and dictionary style
domains you can use these domains to
iterate over your index set so I can say
for all IJ in the striated domain update
the dense array at IJ using the sparse
array value at IJ so I'm iterating over
this index set in the middle reading
from my sparse array and then
incrementing into the corresponding
values of the dense array again you can
iterate over the even though I don't
show it here you can iterate over the
graph or dictionary style domains and
arrays as well domains also support a
richer notion of slicing than you would
see in like a Fortran style language so
I can index into an array using an
entire domain which says give me that
incomplete sub array of elements and do
this computation on those elements so
this is actually the same computation
that we saw on the previous slide and
then domains also are our mechanism
through reallocating raised in language
if you reassign a domain it causes the
array to be reallocated and any
overlapping values to be preserved by
default
so here for example i make my strider
domain twice as started in the column
dimension and add in every other element
i add maybe an equator to my sparse
tween over here to get new non zeros
along those indices and the arrays get
every allocated automatically for you ok
so moving on from data perils and to
locality we have a notion in the
language of a look how and this is an
architectural unit of locality so it has
the ability to store things in memory to
do processing it is that all threads
within a locale have more or less
uniform access to local memory within a
locale and remote memory is going to be
more expensive so on the quantity
machine you can think of a multi-core
processor or an spm sorry an SMP node as
being a locale um so what do you do with
these locales well when you run your
program you specify how many locales you
want to run it on so eight for example
that will be passed off to whatever
machine manager you're using that'll
actually allocate those resources for
you and then within the code you have
access to a few variables that talk
about the number of locales the index
space of those locales and you get an
array of the locales which represents
sort of here the Machine resources that
you're running on you can't duel out
with these locales but it gives you the
ability to sort of talk about where
things are running and where things
should be allocated given this sort of
initial array of locales you can then
create your own views of the locale set
for example here every shaped it into a
two-dimensional set of locales here I've
broken it up into two disjoint sets of
locales and maybe I'm going to run
different tasks on and then we use these
on clauses in the language to indicate
where tasks should execute we can do
this either in a data-driven manner
saying execute these tasks on the
locales that own these elements of the
array or we can name the machine
resources explicitly by indexing into
locales array or specifying one of the
other low calories that I created on the
previous slide this allows me to steer
not only where computation runs but also
where data should be allocated and let's
me worry a lot about locality in my in
my language if I don't trust the
compiler and the run time to do
something intelligent for me the other
thing we use these little kyle's for is
these domains and arrays that we've been
computing on can be distributed across
locales much as they were in zpl so here
for example i can say distribute this
domain D in a block manner to my
computational grid which is this
two-dimensional view of my locales
and that's going to have an implication
for how the arrays are partitioned up as
well in addition it also tells me sort
of a default work allocation when I
start iterating over this domain or
using it to slice arrays things like
that so these distributions specify a
mapping of indices to locales how each
locale should store its local indices
and its array elements and it also
implement l's how the parallel
operations on domains and race should be
implemented so you know what should each
core on each node do if I do a for all
loop over this locale sorry over this
domain or this array of values okay I am
finding out that I am over time as often
is the case so let me let's say let me
just think about what I'm going to do
let me tell you what I'm not going to
talk about all of these concepts lead to
what we call this multiresolution
language design which is we want you to
be able to program at a high level when
it suits your needs and dive down closer
and closer to the machine when it suits
your needs so distributions are a
high-level way of programming on clauses
are very low-level way of programming we
think a language needs to give you both
in order to be scalable to wrap up I was
going to tell you what do we work on
from day to day you could probably guess
I was going to talk about our prototype
implementation and what its status was
you can read about this in your own time
and I was going to serve talk about how
do you HPC computations compare and
contrast with data center concerns as I
understand it and pose the question as
to whether or not Chapel would be a good
data center computation language or not
I think some things very appropriate
other things would require some new
challenges I'm not an expert in data
center computation so I sort of
mentioned here that this could be a
potentially interesting collaboration if
people are interested in looking at it
if you don't work on data center
applications but something else looks
interesting to you we're open to other
collaborations as well uh here's a
summary which I won't repeat you've
probably heard it by now this is our
team and this is how you contact me if
you if i don't answer your question the
next five or ten minutes so thanks very
much
you
Oh</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>