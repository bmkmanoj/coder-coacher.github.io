<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Combining Parallelism, Virtualization, Heterogeneity and Reliability: Some cu... | Coder Coacher - Coaching Coders</title><meta content="Combining Parallelism, Virtualization, Heterogeneity and Reliability: Some cu... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Combining Parallelism, Virtualization, Heterogeneity and Reliability: Some cu...</b></h2><h5 class="post__date">2008-10-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/zimQwvTkLf0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">everybody it's my great pleasure to
introduce
lessons I've known Peter since my second
year university in 92 and worked with
him for about he might be about seven
years from 2002 early last year he's
going to give us an overview of the
interesting stuff they're doing at
computer science at the end unit thank
you Bill well my slides are available
here you see the hyper hyper link and if
there'll be probably sections we have to
skip over and stuff like that and there
are embedded hyperlinks so this might be
useful some people so I'm part of the
department of computer science and we've
recently formed ourselves into a group
computer systems group and i'll just
give an overview of what we could be
covering today ok we have openmp virtual
apg clusters Numa simulation tools these
are kind of three topics which we could
easily spend half an hour on each so
with that and the overview plus small
typical computing that really only this
a choice of one so I'll just wondering
what the audience what which one of
OpenMP virtualize HBC cluster and Yuma
simulation tools you'd prefer or we
could try and do too and skim over them
virtualize HP sees clusters okay all
right so this is a computer system group
and in order to found our group we
decided to go photograph of ourselves
that's quite recent in fact up until
that time I didn't realize how dark our
clothing all my colleagues seemed to
wear but anyway we're five academics who
there's Emma he he works in robotics and
this is real robotics not sort of
software agents which I'm made in the
state of calling robots once to him and
he has submarines which swim and a swarm
and they talk to each other and he also
has autonomous aircraft as indicated
below bioengineering is a new new
projects with IBM and my colleagues
Alistair and dill and Stephen Blackburn
are involved and basically the
application is ion channels within cells
these are the things that are
responsible the transfer electrolytes in
and out of cells and in biology they are
extremely important things to study now
they're looking for tools they're
looking for algorithms are looking for
implementations are looking at GPUs and
cell processor and they're also looking
at extending x10 to implement these
algorithms from a higher level stephen
blackpool and also works in runtime
systems for modern language object
oriented languages and he does lots and
lots of work on that as well performance
analysis this is where we're bill it was
a long member of this project Alistair
Rendell leads this and toy doesn't lead
the next one I've been leading the work
in simulating we have we've developed a
spark simulator which I won't get to
tell you about now but also as part of
that stealing Blackburn
a benchmarking benchmarking of Java his
conclusion and that of his colleagues is
that the standard Java benchmarking
suites a way out of date they're
appropriate for five or ten years ago
and a very large projects that he's been
involved with the cup of project which
is part of and is doing proper
benchmarking on on workloads that are
representative real day uses okay so
parallel computing this is the the CC
Numa project there's computational
chemistry on non-uniform memory access
and I'll stir in Dell leads that and
bill myself were involved in some of
that as well bill bill worked on the
interval arithmetic which is part of
that looking at the harnessing the
celebrate when add water and engine and
the GPUs for doing scientific
computations this cluster open NP and
I'll stir in dells the main player but I
also work in that area as well and and
also on trying to use a service-oriented
architecture for traditional high
performance computing okay so operating
systems and I guess this is what we'll
concentrate on today how to use
virtualization you can junction with
high-performance computing and in
particular clusters and my colleague
Eric mccreath is or third done some
linux kernel development
ok we have a teaching program but I
think we'll skip over that it's but
we're just part of that we were able
support I reach it use our teaching to
support our research activities as well
ok so I'll just talk a little bit about
the the clusters because that is related
to the how we're using virtualization
because this is this is the platform
that we that we use so we have a kind of
a project which which encompasses this
work or the Jabberwocky project and the
idea of the Jabberwocky is that it's
like a the mythical monster are made of
parts of many different things you it's
got all these different sub clusters and
if anyone recognize these name these
this is from Lewis carrison curls poem
and there's a there's a hyperlink to
that as well and is growing and recently
we've included a new penny small
infiniband clusters made them from Sun
dual-mode service quad-core service i
should say ok well before getting into
what we do i just would like to give you
the motivating scenario and the idea is
you have a compute center and this
computer center is serving more of the
traditional scientific engineering
applications rather than commercial
applications although it could be just
as well commercial applications in in
several respects as well your compute
center is going to be heterogeneous
inevitably and the basic model is that
you have multi clusters of homogeneous
subclasses of different kinds different
processing elements different
interconnexion networks and
the idea is what we want to develop
runtime environments which will run a
parallel job and according to the
particular needs on the and the demands
on the on the multi cluster and be able
to move that jobs as conditions change
move them intelligently the various
purposes so you if a a sub class to
which is better for the current job
becomes available you can migrate it to
that you might be interested or so just
in simply load balancing and you might
be also interested in consolidating once
a job becomes fragmented over over the
multi cluster you might want to bring
them together when the process has
become available okay so the approach is
basically we we developed the
infrastructure to monitor the job with
low overhead as it runs determine at
runtime its its characteristics both in
terms of computationally in terms of
memory in terms of communication well if
we're going to move them we need some
mechanism for that and virtualization
because it encapsulates what's going on
much better than the process level is is
an obvious candidate for that
virtualization is also important because
the users of the data center will have
their own virtualized environment and
many codes will only run on particular
version of Linux and GMC and whatever
and they can customize their environment
which is also important in this context
so there are the two advantages of
virtualization the question is can you
make this efficient and can you make use
of migration successfully
okay so looking at the first problem can
you can you have communication running
efficiently under virtualized
environments and that brings us into a
more general problem we have Kostas made
of multiple CPUs I don't mean SM peed in
the strict additional sense this could
be multi-core systems typically they'll
have multiple network interfaces and
it's a rough coincidence but there's
this at present a parity between the
number of interfaces and the number of
CPUs so the idea is can you make use of
giving each CPU for example dedicated
interface and will that scale okay so on
an hour machine we have this particular
kind of opteron motherboard and we had
four cpus and while there are six slots
and because we need to keep one for
control we can use four of those slots
for dedicated communication channels the
gigabit ethernet in principle of course
it doesn't matter whether it's commit
InfiniBand or whatever but one advantage
of the infinite of ethernet is that it's
widely used and it's quite reliable and
can you use this in conjunction with
virtualization
okay so are just a little bit about
background in den virtualization people
are speaking of its advantages we have a
renaissance and virtualization over the
last few years and it's Zen isn't the
only one but it happens to be the
particular one we're working on but most
people are being pessimistic about the
performance overhead introduced by
virtualization ok so in virtualization
basically you have the hypervisor which
normally you'll have the direct access
to the devices and the communication is
slowed down because when you want to
communicate from virtual virtualized
operating system which is also called a
guest it has to communicate first to the
hypervisor transfer pages over to it
then it through a virtual interface then
it goes out for the physical interface
and then obviously you're going to get
some overhead in that process ok so
under then there's several guest
operating operating system but once it's
it's primarily on the machine and can
access the hard way it sits beneath the
hypervisor as well but it is special
because it's the one that actually does
the communication and does the device
access which is domains your drive to
domain ok so
advantages manages all of the cluster
and we get this data transfer which is
potentially slower okay well just
something that that's relevant to this
when we talk about performance is
basically how linux or for that matter
any other operating system under
multiple cpus trans processes tcp/ip
okay there are several papers on this
and if you go to my home page and look
my paper I've got references to the
papers that done studies of this but
basically once you've got a single
stream TCP connection you can logically
break that down into into sub sub
messages and send those individual sets
of packets over different interfaces
that's typically called channel bonding
has anyone heard of channel bonding few
people to anyone heard anything good
about it well I haven't and all of our
experiments or the show that basically
sucks a more successful strategy is
connection parallel and that you try on
use independent messages and you can you
can get much better performance and
that's also confirmed by our experiments
as well
such as large
yeah yeah
okay that's a very interesting question
is it was basically that with open mpi
when you have a large message it can
actually make use of multiple
communication channels that's that's
very correct and indeed we'll we'll
actually see some results and we'll see
how good the performance is it's a very
very clever thing about open the
implementation how they've done this it
does work okay so what we have here is
with multiple interfaces it's what work
that I began with Richard I examined and
and David Barr that two years ago and
it's carried on by sultan muhammad atif
and we've got a paper in hpc 06 and
basically this is not the only way you
can do then but you can have one
virtualized known each cpu and what you
can do in Zen is that not the normal in
way of connecting it is that the law on
the guest goes through as Enbridge and
that basically goes out to the
physically thought but there's possible
to configure it differently for example
guest to Z naught can be connected to a
different send bridge and can go on out
on the physical ethernet one so in this
scenario you could have guessed to and
guest naught both communicating at the
same time for separate physical
interfaces alternately you could have
guessed naught and guest norden guess
one they could be communicating for the
same interface and of course you'll get
contention for that physical resource in
this case okay now we want to compare
this with native linux because that's
going to be a baseline and of course you
can do the same kind of thing with
native linux with eighth one directly
connected to a process which is under
domain for example a process to instead
of gifts to in a process Nord both
aren't running under Devane
naught and these could go through
separate interfaces okay so now we can
get to see some results and apologize
for these tables they look a bit awkward
but this is latency in microseconds with
short messages and low is good high as
bad okay we have two versions of of
native under two different MPI's and
when you're using multiple interfaces as
from your question earlier the two
versions MPI's work very differently
openmpi basically will take a single
message large message and try and and
and break it up and use both interfaces
simultaneously so for example if you
have what we have is to cluster nodes
here for processes communicating to
their opposite pair on the other side
each potentially can be using a
different interface to communicate so
for example with four pairs we have the
four processes that all trying to
communicate and this is megabytes per
second this is the performance that
we're getting for large message message
science now MPIC eight works differently
what you have to do is buy just in the
routing tables you can and vision
multiply RP addresses you can get
separate MPI processes running on domain
naught to use separate interfaces bit of
a trick but it's explained in our paper
now exported interfaces is one of these
n configurations and what you can do in
Zen is give a physical interface to one
of the guests domains you can bypass
this in efficient communication
shared bridges the default send
configuration and there you'll use it
only using a single interface separate
bridges you can potentially be using all
interfaces so what do we see well
firstly we can see a significant
difference between the two versions of
MPI our native and that's that's again
quite interesting secondly you can see
that the exported interface is quite
often does well it quite often
outperforms the native not by a huge
factor but it's basically boils back for
Linux tcp/ip stack processing here
you've simply tlie separated the stack
processing between the four processes
you effectively paralyzed that much more
efficiently you're getting much less
cash pollutions you're not getting any
contention on locks which what you're
having under the native linux running
four separate processes okay you'll see
that the shared bridge doesn't scale
very well in fact since you're using
only one interface that's not surprising
separate bridges you get some
performance but again it's not
spectacular I'll just explain the osu
benchmarks latency is ping-pong
bandwidth is where you're trying to
shove data down one channel in one
Direction's as hard as you can
bi-directional bandwidth is trying to
move data simultaneously on both sides
of in both directions across a physical
Ethernet link and because Ethan it's
bi-directional you can see some
improvement in by bandwidth over over
bandwidth
okay well that's using the the
interfaces between cluster nodes how
about communication within it within a
node well here we've got pairs of
processes like on the diagram previously
for example whoops we could have guessed
01 communicating for guests 00 that's
going to go through the Zen bridgnorth
through this path here and of course we
don't expect that to be very fast okay
so using separate bridges so the shared
bridges the separate bridges actually
makes it makes it much worse shared
bridges though this is the bandwidth of
the native operating system that's
running on the shared memory transport
it's very fast export it interfaces
that's running going through the
ethernet interfaces and loop back mode
and you can see the difference between
between loopback mode and the native
shared memory transport and you can see
that the virtualized we are experiencing
a significant performance penalty in our
communication now when we go to
application level benchmarks then we do
see that the separate bridges can be
quite bad the shed bridges in fact isn't
as bad as you might expect from the
micro benchmark and for some reason the
exported interface is actually not on
that graph but i believe it's better
than than the shared bridge
okay so what about when we have a truly
parallel benchmark across cluster nodes
now we have four four nodes and we have
the dominant communication is going on
within a note or but there's some
communication going across between the
nodes so what happens under that
situation well again the baseline of
native linux which is in blue then we
can see that the difference isn't that
no longer that convincing and in
particular if we look at exported
interfaces a lot of places the penalty
is small and even in this in it for Lu
it's negative Mohammed also devised to
accommodate interfaces which can only
run under NP i CH and it also performs
very well now the exported interfaces is
not my gradable whereas this one is
these others are migrated global
configurations so there are that makes
them of more interest so there doesn't
seem to be a compelling case for um for
improving communication performance
under Zen for then guest domains on the
same physical host from these results
and that's an optimistic results which i
haven't seen people quote before okay
nonetheless we did that benchmark too
late if we've done this about a year
earlier we perhaps would not have wasted
so much time on trying to do it I'll
just go through this quickly because
it's a bit of a diversion but there are
a few lessons that we learned here okay
then socket came out last year and Susan
McIntosh from IBM she wasn't the
original author but she she took it up
project up and made ready for
distribution and it's a pseudo socket
form of communication it's only one way
soccer this no pole select and we
thought that in open mpi inside you can
have various
inside NPR you can vary by transfer
layers it can be for example tcp/ip
inferring penny penny candy for shared
memory we were thought we'd create our
own one which is a hot version of tcp/ip
but using Zen sockets when you coulda
tect that the virtualized processes were
running on the same physical host and in
principle that look like it be possible
to migrate this because this has a
built-in mechanism that it for socket
breaks which would happen if you migrate
the virtual machine it would try and
reconnect automatically okay well
Mohammed spend a good six months on this
and working with open mpi internals is
not at all nice so my recommendation is
don't go there unless you're very very
brave and in the end it was only bro bus
for blocking MPI non-blocking because it
relies on all these callback mechanisms
which are very very touchy it's very
very hard to debug and then the
performance wasn't particularly
impressive then socket by it saw
community is quite fast but once you put
it inside this BTL and you had this
complicated protocols that you're
following it slows it down quite a lot
so there must be a better way but we
believe that as an open open issue okay
so now we'll get to the scheduling
framework which is kind of actually what
the hummers should have been working on
all the time okay so we have this
virtualized multi cluster with all these
sub Koster's on them and we have a file
system which is NFS mounted in fact a
ovaries have proven better than NFS for
this purpose and we're using that ok so
the scenario is that each parallel job
runs on its own vm we don't have VMS
with mold cpus and the reason for this
is it enables my great ability to be
most easily facilitated if you've got a
two cpu
a guest operating system that it might
it's harder to migrate and simply one
okay well if you read Zen literature the
migration down time that is the time
that the virtual machine if you're
trying to contact it actually stops
responding is quite short only 75
milliseconds but that isn't the actual
overhead of migration and one of the
things we wanted to do is actually find
out how long it would take a parallel
job if you're running it and you've
migrated one node okay so what we would
like to do is we couldn't run our jobs
on this subclassed and be able to
migrate it if if the conditions and the
clusters changed we see an opportunity
to move it to a better place well how do
we do this we have to capture on the
flyer that the runtime characteristics
of the job under then the mechanism that
we you have to use a xeno profile which
is the Zen version of 0 profile and for
basically CPU characteristics that's
what you can use to capture using the
hut hardware event counters currently
you can only capture four at a time on
the AMD and then you have to manually
change the calendars for example you
could count CPU cache floating point
operations cash top level cache misses
second level cache misses and something
else but if you want to measure
different different events then you
would have to manually change the
counters and of course that's not going
to work so waiting on multiplexing which
is expected soon to come into Zeno
profile in which case you could just
multiplex over live sets of event
counters so that gives you the CPU and
memory characteristics for the
communication characteristic is
relatively easy to it put a profiler
OpenMP is designed to put a profile
filing level and you can basically get
statistics on the number of course
particular functions the average message
length that you used on those calls
and in that way you can easily build up
a communication profile using that
information you can in principle
determine whether it's your job is
fighting point limited it with its
memory limited with communication
limited and choose a sub cluster which
may be a little bit better to run it on
at that time one slight problem is that
then deliberately tries to make this
hard for you to actually find what
physical host you're running on and what
physical hosts are elsewhere but there
are ways that you can bypass that okay
alright so basically in diagrammatic
form this is simply what we covered on
the previous slide but you have your MP
application and you have a profile built
into the MPI runtime that goes into your
data store you also have the Oprah filer
getting the information resource manager
that author gets extracted and then
sensed under global data store this runs
on domain 0 and which also runs a
scheduler the scheduler can control
where the virtual machines are running
and can automatically perform the
migration and it'll also use information
about the about the physical host in
order to guide its decisions ok so
migration experiments if you can decide
whether it's it's better to move your
job to a different sub cluster you also
need to have an idea of how long this
job is going to run for and how long
it's going to take you to the migration
see whether it's worthwhile if you
believe the job is going to a
short-lived job is going to finish soon
then you're better off leaving it
leaving at it where it is for example ok
so it could vary from the virtual
machine memory size and the apparel jobs
memory footprint and how communication
intensive so this is some preliminary
results on the high-performance learning
benchmark we've used this because you
can easily change the memory footprint
of the job and a result is that doesn't
really matter the memory footprint of
the job which suggests that with xen
migration you're just migrating all the
pages whether they're in use or not
which is inefficient I've recently
visited University of Toronto they have
a project called the snow flock project
now it takes us 15 to 20 seconds to
migrated machine as measured by the
increase of execution time of a parallel
job they have got it down to sub one
second
which is a pretty astounding result but
this is not migration this is cloning
which is slightly easier problem
nonetheless many of those techniques
they use could be applied because I
suspect that when Zen migration is doing
a lot of useless page copying when you
migrate the machine okay so this is my
PhD muhammad's work on virtualized
clusters and this is going to keep him
busy for the next two years or at least
until the individual is scholarship
monies run out and hope that that won't
be shorter than two years because I
think it's a very big in challenging
task okay well we do have some time just
the multi-core stuff I'd like to get to
but that's only like five minutes worth
there are any questions about about the
virtualized kosters work or any
suggestions we will really like some
feedback a few if you have any
suggestions yes
okay the question was what kind of
scheduling algorithm would we use well
that's that's a very good question and
where we're going to start off with
first come first serve but that is just
the basic thing that you get within with
inside a typical application to just
Maui Muhammad's already a prototype to
all this invitation and I believe he's
using Maui but the thing is that for
what we want to do is different to the
scheduling that you you get out of these
default scheduling infrastructure
because it is a choice that it's where
does change a lot not so much when is
the is the main issue so that's really
something but Mohammed and I will have
to think about a lot more the scheduling
algorithms two years
another question
now
yes that was that's the snow flock
people the university of toronto isle de
lara and Michael Britten Oh lead that
project and andre's I forget his surname
now I met them last week he's he's the
crack free XD student who got then
running that was for migration that's
that's simply muchy sorry that was for
cloning so cloning is in some sense
easier because you're just making a copy
well migration in a sense you're making
copy as well but what they can do with
cloning is they're not cleaning one at a
time they're cloning our whole heap at
them at a time and they're using
Ethernet broadcast for example to do
that and using all these these
techniques but yes there their work is
very it's very good and this is the
other the other thing they freeze the
virtual machine before they copy it
which is again a different different in
terms of Zen than simply migrating it so
there are differences but there is still
a lot we can learn from from their
techniques and I think it's a very
interesting achievement what they've
their bunis
okay well we might just just have a
quick look at open in P then on clusters
because what I would like to do is to
show you that what give you an idea what
the state of the art rather than go into
the details well people been talking
about costs are open to pee and how we
have to have high-level language
programming language moles for clusters
and how terrible NP is I can't
understand that did valium pee on myself
but then I've had ten years experience
in so I'm not the typical person who
suddenly confronted with the job of
having to paralyze a big application ok
so we've to PhD students jia chi and jin
wang and alistair rindels also she
survives gnite supervised g and we have
to cluster DSM's this is the first
cluster openmp product came out last
year and Jin has developed another
nervous system so Omni is the compiler
and basically it compiles down the
original code into well you can
visualize it as intermediate level code
with course to a a software distributed
shared memory library runtime library
and that's called Daniel ok so
basically they're working on trying to
get performances acceptable using
various different methods that's
basically the thrust of this this
project of both of their things and
using different approaches genius
concentrating on the run time system gia
is more less using what this class
troponin p is but he is looking at using
InfiniBand and whether these fancy
features infinite band can cut down the
page fault overhead which is a donde the
head of these things so we're just with
the page fault what happens is that each
process is the whole shared memory space
in in theory but each in each process on
a cluster doesn't necessarily have all
the global global address space mapped
into its memory when it first touches a
page it will get a page fault and it
will have to fetch the page from other
parts the cluster in that case and
that's what gives you all your overhead
okay so this is kind of I guess the last
client techniques you can try before
sort of saying all is this approach
viable is it not so it'll be interesting
to see what comes it comes comes out
okay well I'll skip through all of this
will just get to the results okay so we
want to compare MPI which is a baseline
for the nest peril benchmarks these are
tough benchmarks they're not easy things
to paralyze all this some of them aren't
with cluster open and p versions so
these are actually different codes there
are actually different algorithms and in
some sense in comparison isn't fair
because a lot more effort has actually
gone into these algorithms the MPI
versions in the openmp okay
embarrassingly parallel or no big deal
it's designed so that's going to scale
pretty damn well and shame on you if you
can't get that to scale but here we have
the SP benchmark and we're talking about
a small cluster okay will open in p just
scales and it's a little bit better on
beat
II but you can see cluster OpenMP
doesn't it does NP I'd as well on the Lu
benchmark but Castro MP doesn't have it
and NP I at least Class C does well an
integer sort which is a very short
benchmark it also does well and
conjugate gradient which is rather less
short but as you can see cluster open p
doesn't perform very well okay well
these are also preliminary results but
in fact these are all on infiniband does
InfiniBand help you much well it helps
you a little bit but in terms of
actually getting a speed up and with
parallel computing if you don't get
speed up your wasted your time ok this
is no prize it's not getting a speed up
in fact they gin and juice as I said oh
we improve our speed up by 25 percent
and I see are you still under one and
they say yes or I say we'll keep keep
working on it so even on a small scale
clusters can we improve these systems
enough to get to get speed up that's the
interesting questions that we'll see in
the next two years the other issue with
cluster openmp is memory usage so one
reason why we run computational clusters
the memory requirements for your
parallel for your total job is too big
to fit in one node if that's the case
you can run and distribute memory over
cluster and then with enough nodes it
will fit into memory and it will go much
faster that's another reason for
paralyzation well unfortunately the
cluster open in P it has a master node
which has to keep a copy of the whole
memory memory space it may not be up to
date at any time of course but it has to
keep a copy and that does not scale now
there's no fundamental reason for that
except that you if you're going to try
and do it otherwise you're going to pay
more in performance and you're going to
do a lot more work
however on the slave nodes which is the
the other the other nodes the memory
performances you do pay a hit once you
get up to a distributed openmp version
but then it does scale okay
so we might just get close to the end
and I'll just like to talk about briefly
my ideas about multi-core computing and
some ID and some research directions and
possibly get some suggestions from you
okay now this is a slide which from a
person that bill will remember fondly
Robin Stanton put up a long time ago
when we were visiting fidget so we had a
collaborative project and 90s the
fujitsu working with the parallel
computers it was a great experience
being on that project bill came aboard
on the sort of last four years like it
ran forth about 13 years there's a long
project he came on board the last 13
years and i believe he actually had a
small involved with even before then and
after year one i was involved so anyway
this is this I don't know who who
thought this up that this is this graph
here represents well people in parallel
computing what are their beliefs should
you try and get speed by increasing your
parallelism or should you just simply
lays Leroy on moore's law to get to
processor speed up now this I first saw
this in the early will just towards the
mid 90s and the killer micros were
coming out and indeed they were true
believers but it seemed to be the days
of the agnostic sand even the heretics
were starting to get a bit of a bit of a
show then because more ores was just
doing for commodity processes amazing
thing and just by increasing the clock
speed you could get your increases in
performance but
so that should be mid-2000s multi-core
is changed again and I've always been a
true believer and I think the day of the
true believers and other two believers
has at last come and indeed if you look
at the ultrasparc t2 which which I'll
talk about in the next slide it could
even be the death of the phonetics and I
might join them the original de Graaff
didn't have any any art pointing down
here but I believe that you could have
the Luddites down there okay well sort
of one of the nice things that happened
to me this year's that I dream of
getting a t2 cool threads processes for
my universities for teaching and
research and I've been lobbying firstly
food professor Christina fantasy who
works at the University Queensland and
in sun labs and then once she got Steve
helber which is that who is the director
of research at some labs on my side the
dream pretty swiftly became a reality so
it's what machine is also called the not
Niagara you will see that that's
recognized that it's Niagara Falls and
it's quite a nice photo if this damn
tourists hadn't gotten the way of the
camera it'd be a very good shot in fact
anyway I'm calling our machine Mavericks
and our spark on research Network our
sparks they've been mostly donated by
Sun where we've had pollo we have Auto
we have Alcatraz and we had Fremont and
I'll seeing what the Bay Area has this
water theme so who's heard of Mavericks
yeah what's Mavericks big ways and where
is it
it's off the coast it's actually pretty
well adjacent to hear so one of the
biggest waves in the world and in case
you're wondering the scale of this thing
those little dots their answers their
boat loads full of half dozen people
standing up now there are actually two
surface on the wave one's about here
one's about there they're just ahead of
the white water now you can get a
different appreciation for this and
Niagara Falls if you go under the
journey on it before so you can feel the
ground shaking from the falls well it
said that the seismographic berkeley can
pick up each individual wave breaking
when it's this size so I think it's
quite a Fitz in quite well with the thin
so that's why I call it Mavericks okay
so of course we can't let our students
loose on on our research networks the
system administrators said that we all
have it would like let use but what we
can do is use logic domain and we can
export a virtualized machine it's not
exactly the same as then of it it's it's
a similar kind of thing that's called
parliament and Australia is a very poor
country for lots of water moving
unfortunately it's getting poorer and
porous as the years pass in fact but
there's one on falls which is the
highest waterfalls even though it's not
necessarily in terms of throughput it's
nothing to compare with the original but
I'm afraid that's the best Australia can
do ok so these are the kind of
suggestions I've also looked into the
hardware threading you've got eight true
CPUs ate you ate weddings how effect is
that very cheap architectural trick of
using Hardware threadings update ways
how much does it scale well my
preliminary experiments on very simple
benchmarks suggested scales extremely
well to four and up to wait you still
get benefit and that's on on a simple
benchmarks just memory copy parallel
memory copy or parallel vector reduction
we could evaluate L Dom's we can
investigate Blass which is quite an
evening thing and a haskell project
past Haskell is very memory tensive
programming language the teacher is
prickly well suited to that well if we
get time we could upgrade our simulator
to the t2 it currently molds the v12 80
which is a traditional SNP over that
five years ago operating system issues
large cow modest course simulation and
if we can look at some of the issues
that I didn't get to talk to but some of
the issues that Andrew over whose PhD
student at Bill Nye super have been
supervising have come up with a very
difficult much surprisingly difficult
topic of parallel process simulation
when you look deeply into it and another
thing which ties into the topic we
taught today can you use heterogeneous
multi-core to small amount of areas on
the edge of your chip to a specialized
processors for commute to aid
virtualization and communication under
virtualization and eliminate cheaply the
old heads that we saw before hi another
alien you graduate okay well that's
basically brings me to the end other any
questions
Peter can you remember that that t1
processes are as Oracle had very poor
floating point yes almond still 22 is a
22 Billa the t2 is better so you act
each wheel CPU has a real floating-point
unit Oh excellent so that's much nicer
however the threads have to share it the
8 threads on that cpu to have to share
it so you're you're really only looking
at the 16 floating-point operations per
clock but certainly a lot better
couldn't be worse it's not really meant
to be floating point intensive but it's
it's really designed it's designed for
for example it's got support for
encryption and specialized specialized
things for that it's not a scientific
machine even though we would like to
explore how useful is for scientific
computations
thanks better thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>