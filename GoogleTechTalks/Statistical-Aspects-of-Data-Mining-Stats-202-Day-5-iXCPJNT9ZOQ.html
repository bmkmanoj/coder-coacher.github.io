<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Statistical Aspects of Data Mining (Stats 202) Day 5 | Coder Coacher - Coaching Coders</title><meta content="Statistical Aspects of Data Mining (Stats 202) Day 5 - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Statistical Aspects of Data Mining (Stats 202) Day 5</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/iXCPJNT9ZOQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">this is lecture five for data mining um
if you're playing along with the
homework assignments now a few of you
turned in homeworks to me so I don't
have any resources to grade homeworks
for for Googlers but I will put the
solutions up so you can look at the
solution to those and I'm going to put
up the chapter 3 homework actually it's
technically chapter 3 part 1 homework
because I think some of that will spill
on to the next one and also though that
doesn't pertain to you but it is the
chapter 3 homework is linked from the
same place as the first homework now you
just have to scroll down to see the
first homework and so if you're
interested in following along with the
homeworks I will post solutions to that
so last time we're into chapter 3 which
is exploring data now chapter 3 it's
divided into section 2 basically deals
with summary statistics and section 3
deals with visualization and I want to
start with the visualization and again I
think I said this last time but I
encourage you to also start with
visualization so the first time you get
data the first thing to do isn't to
compute a mean for every column or
compute the median for every column or
anything like that the first thing to do
is to sort of maybe make some plots and
sort of see what's going on in the data
because you might realize that half of
the data is corrupted once you make the
plot so I usually start with the
visualization and so we're going to do
that as we go through this chapter and
you can see here's some quotes from the
book about visualization I think the
last point is probably the most
important one the goal is the
interpretation of the visualizing for
information by a person and the
formation of a mental model of the
information and that's true not only for
you as you're exploring the data but for
as you're presenting it to other people
you want them to be able to form a
mental model of the data so this simple
toy example I started off with which was
just these 40 exam scores and with 40
numbers you know you just look and you
see okay I see 40 numbers you can't
really see any anything about the
distribution what is are there any
patterns in the distribution you don't
really see anything so of course you can
imagine with 40 million numbers you're
going to see even less at least 40
numbers I can put them all on the same
screen but that doesn't mean I can
really see anything interesting about
the distribution so the first thing we
did which was what your book talked
about was making a histogram which
basically just takes the numeric
attribute bins and in two equally spaced
bins and gives the counts for each one
and I'm just going to go through these
commands again because I'm going to need
some of these objects as I expand these
things so the first
thing was to read in the data which of
course will give me an error because I
have not set my working directory and
I'm just going to use my desktop again
today as a working directory but usually
when you write scripts you can um set it
to be whatever usually set it to be
whatever directory the script is in but
of course then if you move it you have
to change it so let's see I set working
directory to be my desktop and just tell
Windows that the slash does actually go
the other way so I'm going to leave it
as my desktop all day today and now I
can read in let's see read in the data
set using the read dot CSV again this
one doesn't have a header I'm going to
show you one later on today that does
have a header but that one does not and
then we made a histogram and again the
hist automatically returns a histogram
and I set the bins to go you see her I
said breaks 120 to 200 by 10 and so that
was a histogram and with that I can see
a lot more than just 40 numbers I can
see that what was going on is that you
had a lot of people that did bad a lot
of people that did well and not too many
people in the middle so there was sort
of this this bifurcation or this bimodal
nature of the distribution which you
certainly couldn't see by just looking
at 40 numbers and you certainly wouldn't
see it if you looked at 40 million
numbers now whether or not this is real
you know well let's just sort of see if
it persists as you get more data because
it could sort of be an artifact of how I
did the bidding but it's quite possible
it's real and if you had a whole lot of
data then you could be more confident
that it was real
so that was the histogram then we said
sometimes you don't want to have to make
these bars these solid bars you want to
make just points and connect them by
lines and obviously you can you can
display the same information as the
histogram with points connected by lines
and we said that was called a frequency
polygon or a relative frequency polygon
and there's no function in are
necessarily for doing that but the way
we did it was we took the histogram
function and we sort of stole different
parts of it right so the first thing I
do my histogram and I basically make a
histogram but I say plot equal false
okay so it's not going to plot anything
but it's going to give me back some
things that I want in particular the
counts basically are the numbers
indicating the counts in each category
and then the breaks they
basically the 120 130 140 etc all right
there right so now I have the counts and
I have the breaks and when it comes down
to the histogram there's nothing more
than a plot of this on the x-axis and
this on the y-axis now actually what I
want to do is I want to use the
midpoints of these because that would be
the point in the middle of the bar and
then I'll use those counts and so that's
going to be how I make my polygon and
the last thing here let's see it I
already called plot no I didn't call
plot yet so let me call plot plot the
arguments to plot are basically x and y
right so maybe I'll this right so when
you call plot is a nice generic function
in Y first argument is a second argument
that's why these are vectors they have
to be of the same length obviously
because it's going to make a scatter
plot with that and so that's what I did
here my x-values are 115 comma breaks
plus 5 so that would be 115 125 135 etc
and then the Y values are the counts but
I added a 0 at either end because I'd
like to make the frequency polygon go
back down to 0 and then the only other
thing left is to connect those points
with lines and so I did that by calling
the lines command here and then it makes
my polygon have lines on it
and I think last time I put these up on
the same screen along with the histogram
and you can see it's nothing more than
the histogram except I put points in the
middle of bars and I connect it back
down to 0 at either end so that was our
frequency polygon and I said one of the
advantages to that was that you can put
multiple groups on the same plot and
I'll do one of those today before I do
that I just want to finish up the last
thing I did last time was to talk about
the empirical cumulative distribution
function so you know the the histogram
the polygons are nice for looking at the
distribution but if you want to look at
the cumulative distribution that's going
to be a monotone increasing or at least
non decreasing plot starting at 0 ending
at 100% and for each value on the x axis
you have the percent that is less than
that value so technically speaking the
cumulative distribution function gives
you the probability that a point is less
than a value whereas the empirical
cumulative distribution function is
based on the data so it gives you the
percent of points which are less than
that value but of course the empirical
cumulative distribution function is an
estimate of the
real chemo distribution function
sometimes this pot is called an ogive
although that's not in the book and
someone wrote a really nice function ECD
F&amp;amp;R that makes these for you and so I
think that's actually how I did this one
right and I just call it here I change
some of the defaults just to make it
look a little bit nicer but the actual
computation is fine because how do you
do this basically every time you see a
score you go up by 1 over N and the
places where you see a little bit bigger
jump there's probably two of the same
value in the data set so as it goes up
by two over N and then you're left at
100% or 1.0 on this graph and so how can
you use this well you can say you know
what is the median well trace 50% over
and say okay it's about there like 165
what is the 90th percentile traits 90%
over it's down here like around 190 what
percent of people got less than 180 look
at 180 traced over and get 0.6 so that's
sort of useful but where this graph is
going to be really useful which is what
I'll show you in a second is when you
want to compare multiple groups and
these group these graphs are really
useful for comparing multiple groups
because right now it's just a summary of
the percentiles which I could just
calculate but for comparing groups this
graph will be very useful so let's see I
think that catches us up to where we
were last time yeah so any questions on
any of that
that's all from last time okay so I
didn't want to go through that just
because some of those objects I'm going
to reuse today so I have them in R so
the question is now what if I get sort
of a second exam right if I'm teaching a
class I gave one exam now I gave a
second exam and so I have second exam
scores so the question you know that you
might ask is the second exam harder than
the first or easier than the first now
because you're comparing two groups of
numbers it's not a simple question like
just comparing two numbers right I mean
obviously there's people here that are
higher than some of the scores from
before there's some of the scores from
before that are higher than some of the
numbers here so it's not sort of you
know apples to apples comparison there's
also three people here that didn't even
take the exam right there were 40
squares before now there's 37 scores so
the question of is this exam harder you
know we have to sort of think about how
we're going to answer that question so
the short answer is that I'm going to go
through some of the plots I made before
and just sort of put them on the same
graph now the histogram would be tricky
right because you can't really have bars
on top of each other
moreover you should make sure
use the histogram has to of course be
relative frequency because if this one
has 37 and the other one had 40 you
don't want to compare counts to counts
you want to compare relative frequencies
to relative frequencies so I should
divide the total counts by 37 here and
by 40 before but again it's hard to put
histograms on the same graph so polygons
are even better and you can use a
different color type line for each group
and then give a legend and I'll show you
how to do that and so that's sort of
useful for comparing the two the two
groups but what's even better is going
to be to make the empirical cumulative
distribution function and put both of
those on the same graph and again you
can also give a legend for both of those
so I'll show you how to do this and I'll
mention some cases in which these will
be useful so the first thing is to make
the polygons on the same graph and give
a legend for that so first of all let me
um you know I never made a histogram for
these new scores so let me read in these
new scores
let's see close this guy so the new
scores are here so this is more more
exam scores and it's just again a single
column and the last score is 132 which
is the last score then I'm going to make
a histogram out of these new things but
I'm going to again say plot equal false
because I do not want to plot it I
simply want to get the counts and the
break points out of there so that's the
same as before again I don't want to
overwrite my old count so I'm going to
call these things new counts and I'm
going to call these new breaks and if
you notice the only thing I really did
different here is that this the x-axis
you know I'm going from 100 to 200 by 10
before I started I think at 120 but
there's some people that got lower so if
you were trying to automate this here I
just hard-coded these numbers but in
reality you would somehow choose the min
and the max or let it do it for you and
just make sure you're keeping track of
what it does and so I have new counts
and new breaks right so the new breaks
should go 100 110 etc and the new counts
tell me how many are in each one okay so
that's enough to make a polygon so let
me do that I basically plot the new
breaks on the x-axis the new counts on
the y-axis and there's my polygon right
so the only thing I need to do now is
connect these points by lines now the
one thing you might wonder about is I
had to set the Y limit arguing
and the reason I had to do that right I
set why Limit going from zero to 0.3 the
reason I had to do that are we'll set
the y limit by default but if you look
at the old polygon right this one the
frequency went up to 12 so the relative
frequency would be 12 over 40 so I need
to make sure my plot which is going to
later have this one on it also goes up
as high as 12 over 40 so that's why I
set this one to go from 0 to 0.3 and
then the last thing to do is to connect
those points by lines again the lines
command works just like the plot command
you give it x and y the only thing is of
course this one makes lines and this one
makes points also this one just adds to
the previous plot so when you call this
one you get to set you know a lot of
different parameters like the labels the
margins the x-axis label the y-axis
label the title where's this one you
don't get to set as many parameters
because it's just going to add to this
one ok and by the way if you make lines
that are out of the limits on this one
it doesn't really complain it just
doesn't draw them so I'm going to add
the lines to this polygon and then I'm
kind of halfway done because I have the
exam 2 scores plotted on my polygon
there we go and you notice one other
thing here I'll say PCH sorry wrong
thing LT y equal to this is line type 1
is the default one is Solid 2 is a
dotted line now I will use a so here
I'll just show you the the one up here
oh I will use different colors but I
don't know you know sometimes people
print things out on black and white so
you don't want to just use color to
differentiate sometimes you also like to
use different line types so you could do
thick versus thin here I made a dotted
line and so you see this is basically
the distribution of exam scores for the
second exam you know we have a little
bit more mass over here a few people did
really bad on the last exam but most of
the mass is concentrated here and so
it's going to be interesting then is to
overlay the first exam and see how that
compares and so that's exactly what the
next set of commands do on the next
slide basically you know I still have
the breaks and the counts in there from
before so I'm going to add points with
the breaks and line so points
is sort of like lot except it's not
going to create a new plot it's just
going to put points there and then lines
of course the same thing so it's going
to put these points in these lines on
top of the same graph I'm going to make
it blue
and the default line type which is solid
so I have a black dotted lines now I'm
going to have a blue dotted line so I
have a black dotted line now I have a
blue solid line notice in each of these
here I took the counts divided by 40 and
the previous one I took the counts
divided by 37 because I'm trying to make
this a relative frequency and not
frequency right I want to scale these
things to be on 0 1 to make it a fair
comparison because one class sorry one
exam had 37 people taking the other one
had 40 people taking it so let me just
push in these last few commands so this
should just add points right it's just
like the command plot but it's just
going to add to my graph as opposed to
create a new one so I've added these
blue points and now simply connect them
by lines solid lines I do that and
there's the solid lines and then finally
a legend the legend command is sort of a
very nice command to use you see here I
put a legend and tells me that exam 2 is
the dotted black one exam one is the
blue solid one the legend command is
pretty intuitive to use in terms of the
arguments that it takes basically you
give it X Y these are the X Y
coordinates of where you want the legend
to be and you might want to move it
around you know so it doesn't run into
your graph or you might just want to
create extra space on one side of your
graph to sort of put it in there then
you tell it what two things or what
three things or what vector of things
you want to assign the legend for so
this will be literally the text that
appears on the graph then the color this
would be a vector exam two is black exam
one was blue so I give it that vector
the line type exam - I used a dotted
line exam want to use a solid line and
for both of them I use the plotting
character equal to 19 so instead of
giving it a vector of 19 comma 19 I just
say 19 it'll make them all 19 but you
could make out a vector if you wanted to
use different types of dots for the two
different types of lines and so then
here you see the final product and
there's your little legend which looks
better here on my higher resolution
and you can compare the two the to exam
one and exam two now it's not that easy
to compare them right it's not that easy
to see what's going on because you know
well more people here on you know in for
exam one than on exam two but then this
is really low here in suratul here and
what you really want to say is you know
is one of them harder than the other
right that people do better on one exam
than the other and it is kind of hard to
compare when you're looking at the whole
distribution like this you might make a
case that exam two is harder right
because it certainly has more mass here
more mass here you know and less mass
here but then it has more mass here and
this so it's a little bit hard to
compare so there's other ways if you
want to just compare two distributions
and one of those will be to plot the
empirical cumulative distribution
function which I'll show you next but
this is one way to sort of look at both
of the distributions on the same plot
and the nice thing is you know here I
notice that exam one is sort of bimodal
where exam two really is not at all
although it does have sort of a few
people that did really bad out there
okay any questions on making the
polygons on the same graph okay so like
I said this is sort of useful for
comparing the two exams but what would
be really a lot better is to make the
empirical cumulative distribution
function on the same graph so that's
what I do here in 11 and the first
command here basically exactly as it was
before I don't think I changed anything
here so this is just going to give me
the empirical CDF for the first exam
right and that's what it was so I think
the only thing I might have done
differently I go from 100 to 200 in
anticipation of putting the second exam
on there which has some lower scores so
go from 100 to 200 that's the empirical
human distribution function for the
first exam and what I want to do then is
basically add the second one so now the
function e CDF remember this thing I
have to call it here I call it within
plot and I'll create a new plot with
that it turns out I can also call it
within lines that actually is legal it
also works whoever wrote this function
gave it the capability of being called
within lines and so doing this is simply
going to add to this plot I'm going to
plot the more exam scores where this was
the first exam this is the second exam
again verticals equals true I like you
to put the vertical lines this is the
dew points I don't like
put points like false horizontal lines
or red vertical lines are red lwd that
one is the line with LW d equals line
width 4 is pretty thick the default is 1
which will be thin so this one's going
to be a red thick line whereas the
default is going to be a black thin line
and then when I put my legend I'll say
that I'll say exam 1 exam 2 exam 1 is
black exam 2 is red the line width I
give it a vector write exam 1 is line
with 1 which was the default exam 2 line
with 4 which is what I specified here so
that should do the trick and then the
interesting thing is going to be what we
can say about this graph once we make it
so I do this that gives me the red line
and then I do this that gives me the
legend and now I can order the legend go
hello where's my legend there it is
ok so now I have the 2 exam scores I
have their empirical chemo distribution
functions plotted on the same graph the
Y skill is always going to go from 0 to
1 but you just have to make sure you get
the X scale so you get all of them and
so you see you know here's one here's
those 3 people that do really bad on
exam 2 right 1 2 3 those are the 3 1
over n jumps there and then you know you
can see now the interesting thing is
that the red line is is always above the
black line ok so does that tell me that
exam 2 has higher scores or lower scores
lower scores which is a little bit
counterintuitive right the fact that the
red line is on top means the scores are
lower and you have to be a little bit
careful with that but if you think about
it it makes sense right because if you
look at any anything here right like 140
well for the exam on a small percent got
less than 140 but for the red line a
larger percent got less than so they're
always pound it
we're always plotting the cumulative
percent less than right and so that's
what happens is like that being on top
means you have lower numbers over all
right what's the percent that got less
than 180 well a higher percent got less
than 180 on exam 2 which means they have
consistently lower scores so what this
is showing
you is that every percentile every
empirical percentile for exam two is
lower than the corresponding one for
exam one right every single percentile
so in this case the terminology is one
distribution is stochastically larger
than the other distribution right it's
it's every percentile you line it up you
get a larger value so this is sort of
you know end of end of story right you
don't have to really worry you know well
some exam 2 scores were higher than some
exam once which doesn't really matter
every percentile for exam 2 is lower so
I can say sort of unequivocally exam 2
as a harder exam ok the cost another way
of looking at this might be to say that
the red line is to the left yes I think
that's that's more intuitive the left is
right there both here in the red line
the red line is shifted to the left so
it would be as if I had you know a
distribution here and this was the mean
and then exam 2 shifts to the left okay
and the nice thing is if the
distribution doesn't change shape these
two lines will never cross if you just
do a shift of location it'll exactly
shift the location here the lines will
never cross right okay but um if the
lines do cross then the story is more
interesting right if you say well I
don't know more interesting or more
unappealing or something but they always
start at 0 and ended 100% right so this
is call this 100% okay so if the next
line crosses that's suppose here I can
try and match the colors suppose the red
line here you know started off higher
right but then at some point it crossed
okay at some point it crossed
so now certainly I can't say one exam is
sort of you know just harder than the
other but I do have to say that you know
all for the exam to you know this part
of the distribution is consistently to
the left but the very highest you know
like maybe the top 10% of exam 2 people
actually did better and so you can't
really give one answer the question
which exam is harder because the top you
know maybe the top 10% of exam 2 scores
are actually higher than the top 10% of
exam 1 scores and so this type of thing
comes up if you imagine suppose you're
doing suppose this is something more
interesting suppose this is like latency
so you've done some experiment where
like you've decreased latency for most
the distribution but the 10% longest
latency cases are actually
so you actually want to launch such a
change right because for 90% of people
you made the experience better but for
the worst ten you made it even worse you
know do you really want to launch such a
change so you have to think about that
so if one completely dominates the other
it's clear-cut if you have them crossing
then it's not so clear-cut okay you had
a question right so that's that's
related to the other problem which is
I'm supposed to stand here all the time
because this microphone only works and
this one doesn't yeah that's good idea
let's do it if we get in trouble it's
your fault all right all right yeah so
there's now a whiteboard over here and
this screen has the same thing as that
screen so that's two distributions right
so so one thing is like a lot of tests
would be based on a normality assumption
right so if you did like a two-sample
t-tests it's based on a normality
assumption so you know do you believe
the normality assumption well then you
should first look at some QQ plots and
see if that's legitimate if you don't
believe the normality assumption then
you have to use some sort of a
nonparametric test right so yeah if you
really want to do that but the question
is like the question of statistical
significance really comes to your sample
size right in this case arguably I have
the whole population so there's no
notion of statistical significance if I
say this is the whole population 37 and
40 right because statistical
significance is saying is your sample
large enough to justify generalization
to a population but if I say 37 and 40
is my population then I don't have any
generalization I'm just saying my
students did better on the first exam
than on the second exam ah we're going
to talk about them yeah they're they're
interesting and it's it's cute that that
it never complained about those three
and I'm going to talk about that in a
second because we kind of wish it would
have complained a little bit about those
three so we should talk about that okay
there was another question maybe okay
okay yeah we should talk about the other
three right right so there's the
question of yeah yeah
the question is exam one harder or
easier or did the students do better on
exam one an exam two if I'm just sort of
doing descriptive statistics maybe it's
the latter question I'm after but if I'm
at interest in inferential statistics I
might want to view my 37 as a sample
view my 40 as a sample and then and of
course then the right thing there is
that I'm really losing a lot of power if
I don't really associate the student
with the exam and treated as paired data
because really then it should be a
paired t-test for you know with three
observations dropped out so yeah so I
I'm not really doing this to talk so
much about the statistical inference on
whether or not we can generalize this to
a population and if you were doing that
there are tests that are appropriate I'm
doing this more as a descriptive
exercise to just sort of visualize the
37 versus the 40 students and you know
sort of you know in data mining right
often it's not so much generalization to
a larger population that you're after
it's understanding that all the data
that you have which might be effectively
the whole population but it is a good
point that the inference versus the
descriptive should not be too confused
and this is merely descriptive exercise
and we haven't established any
statistical significance hmm okay other
questions on this exercise so basically
we're looking at the two cumulative
distribution functions simultaneously
one more yeah it will just pop out a new
one so you want me to call legend and
like move it over to the left a little
bit it will just annoyingly put right so
yeah you start over again right but if
you're saving your commands then you
just it's just a matter of putting them
all back in again and trying a new
legend but I think you can see where
it's going it's going
what did I call it one twenty point six
so I think it's the upper-left corner
the question is like how big it is kind
of depends on your display and so that
can sometimes get messed up a little bit
but yeah one twenty point six looks like
the upper left corner so it's not
completely random but yeah you might
want to have to experiment any other
questions on this plotting exercise one
more in the back right so the question
is you know can we plot the difference
between the two distributions
independently and I'm going to do that
in a second but when you talk about
difference it's almost at the student
level and in that case I need to sort of
know which student took each exam right
well I mean you could plot differences
in percentiles but what I'm going to do
is next associate the students with the
exam and visualize that sort of in that
makes for a nice picture if I know which
didn't took each exam all right but yeah
you're right if you didn't want to plot
two c.d.s you could plot the difference
in two CDF's but it would be hard if you
had three right so it works well okay so
let's move on to the the next thing
which is okay I already answered this
question rice is based on the plot which
was harder you know which had lower
scores well the second exam had lower
scores because it was higher or as we
said probably better to say it's to the
left okay so so far you know exam one
and exam two data they were on the same
scale but they weren't paired meaning I
didn't know which student you know had
exam one score which didn't had exam 2
score I didn't have any association
between the two exam scores but a lot of
times we get data that's paired and when
does data that's paired come up well so
here's where I get in trouble so um data
is paired in the following situation
right basically if you think about the
chapter two idea of data which was this
matrix with you know observation one two
three four the different observations
and then we had different attributes
attribute a attribute B attribute C etc
so if I just take you know attribute D
if I just take any two columns right
take attribute a and attribute C the
that data is paired right because both
of those are for observation want both
of these are for observation - both of
these are for observation three so when
I have data like this and I'm comparing
two attributes for the same data I have
a paired relationship because I can tie
these together and I haven't done that
you know with the previous two data sets
right I had just 37 scores another forty
scores and I just compared them but this
third data that I have linked up here
exam scores and names actually does pair
the data together it actually tells me
which student took which exam and I can
show you exam scores and names right so
I don't have real names up here they're
sort of generic names student number
ones number two etc and then I know that
this first exam score 192 from exam 1
the first exam score want to use sum
from exam to actually go with the same
student so then how do you explain the
fact that one has 40 another has 37 well
right here this is what some of you
asked about this guy he didn't take the
second exam he's somehow dropped out of
the class and so same thing here and
same thing here right so the students
who took exam 2 is a proper subset of
the students who took exam 1 because I
have 3 missing values here in exam 2 so
that's paired data right because these
two numbers go together right they go
with student 1 the next two numbers go
together so what can I do with paired
data well of course the most obvious
basic thing is the scatter plot which I
think is you know my favorite plot right
the scatter plot can often reveal many
many interesting things and of course
the plot command in R is basically
producing a scatter plots points on the
x points on the y it makes a scatter
plot this is talked about in your book
on page 116 there's some examples in
your book this is interesting to talk
about in terms of large data set says
when the data has two or more numeric
attributes right we're plotting numeric
attributes you can make scatter plots of
all possible pairs and the function
pairs and R does that so some some
people I know the first thing they do
when they get a data set and when
everything is numeric especially they
type pairs they look at all the scatter
plots now if you're thinking about large
data in a lot of attributes you know P
choose two could be a large number and
you can't look at all possible pairs so
you know really if you have a lot of
attributes you might want to look at
certain pairs or random subsets of pairs
pairs with the highest correlation pairs
with the lowest correlation etc but you
know for just
small number of attributes sometimes
people use this pairs command and your
book calls this a scatter plot matrix
and they talked about it on page 116 if
you want to see that so that's sort of
nice for sort of visualizing all the
pairwise relationships between the
numeric attributes simultaneously so of
course the thing I want to do is to make
a scatter plot for my exam scores and
that's what I do here on the next the
next one now R by default is going to
scale your x and y for you according to
the minute of the max but because both
these exams are on the same scale I want
to scale x and y both the same so I'm
going to go 100 to 200 and I'll show you
how to specify that also some people
like to add the regression line I often
though just like to add the diagonal
line just so you can see who falls above
and below the diagonal and then we'll
see what we can see on the plot so let's
see here so I have the data read command
and actually let me I'll paste this on
at once and then tell you what's going
on so here's basically read the data and
now notice the interesting thing here is
what you don't see you don't see how to
recall false because in that data set I
have column name so what it's going to
do is going to take the first row write
the first row in the CSV file are these
three names it's going to use those as
the name so later on I can say data
dollar sign in exam one and that'll call
column one okay so that's interesting to
note then plot X comma Y now notice
actually data dollar sign exam 1 if you
notice the name in the CSV file is exam
space one R of course doesn't like
objects to be named with spaces in them
so it changes the space to a dot and
then exam dot two on the y-axis x limit
0 to 200 Y limit 0 to 200 PC H equal 19
right that's that's what I live by
that's the solid dot right not the
default open circle this one I like a
lot better label the the title X's exam
1 y is exam 2 and then a b-line I
haven't showed you this command before
but the function a b-line is good for
making sort of like in this case a
diagonal line you can call it a couple
different ways what I'm using is the
intercept slope right I get that right
intercept and slope so if I say a B line
0 1 that's the line width
intercept of zero slope one that's the
diagonal line you can also use it for
horizontal and vertical lines vertical
lines especially useful if you don't
want to say an infinite slope so you
just say H equal or be equal but here I
called it with a beeline C zero one
intercept zero slope one makes a nice
diagonal line and so this should give me
a nice scatterplot of my exam scores and
you can see here well it's better if
it's like a square display right so let
me try and leave it square you can see
here that what's going on all but two
people are below the diagonal which
means what means all the two people did
worse on exam one than they did on exam
two so this is when I say to see but the
data is even more powerful if you can
pair it because just to say all the
percentiles are lower everything like
that or whatever but you don't want a
case per case basis you've eliminated
the person-to-person variability and on
a case-by-case basis
consistently everyone did worse on exam
one than exam two with the exception of
these two people and speaking of
statistical inference which you guys
started talking started talking about
the sorry I probably say it backwards
half the time better on exam 1 exam 2
we're son exam worse on exam 2 the right
inference once you have this data would
be to use a paired t-test and just look
at the differences and do a one sample
t-test on the differences that has more
power than the independent sample t-test
for this case ok so then these two guys
are of course interesting because they
actually somehow did better on exam 2
these guys are sort of everyone you know
but you might want to know who's doing
really well and then these are sort of
the three guys that did really bad on
both exams but especially bad on exam
two and you know this is a nice
scatterplot I really see what's going on
in the data right away and so when you
have paired data this is useful the only
thing I sort of like to do beyond this
is to sort of you know label the points
in some way or maybe use some color
coding to reveal a third dimension and
that's really useful not well in terms
of color coding I won't show you that
but you can pass a co L argument and
give it a color vector the length
equivalent to the number of points and
you can make each one of different
colors so you could color you know the
boy is blue and the girl is red or the
senior is red and the jr. is green you
know whatever and just sort
see that at the same time I won't show
you that but what I will show you is how
to label the points and in our there's a
couple acute commands for labeling
points so there's my scatterplot text
and identify are very useful for
labeling points in the scatterplot so
text is the one that basically if you
just call text to the label all the
points or you can specify certain points
to label identify sort of a more
interactive point-and-click thing if
you're just playing around where
whatever point you click on it'll label
it for you okay but of course it's hard
to sort of automate that so it's
sometimes you use this just to figure
out where they are and then you use text
to label them and so let me illustrate
both of these for you so I want to use
the text command to label every one who
got less than 150 on the first exam and
then I'm going to use the identify
command to sort of pick out the two
people that did better on the second
exam in the first exam and I'm going to
use the first column as the labels
because if you remember what is this
thing called data data first column
that's the student names right and in
fact I can refer to this as data dollar
sign whatever it's called I think it's
called
I mean sheet data dollar sign student
with a capital S student that's the
first column right that's the student
names data dollar sign exam dot one
that's the first exam etc data dollars
i'm exam dot two is the second exam now
one thing that's real interesting here
right and i we talked about this little
already there are three people that
never took the second exam right and
those three people let me come back a
second here they don't show up right
they're not here and so it's interesting
that our never complained about that
missing data right it just the n/a is
just passed through there and it never
complained because the default behavior
and plot is n/a omit but you know that's
interesting and three out of 37 or three
out of forty rather not such a big deal
but if it was fifty percent of the data
you would like to know and the fact that
it doesn't complain is kind of annoying
so one thing you know people do sort of
as a first pass on data is they see just
how many missing values they have and it
might be curious you know where did
these three people are they three people
that did really well here maybe the
class is too easy so they dropped out of
the three
people did really bad maybe the class is
too hard so they dropped out are they
three random people would be interesting
right anyway we just sort of forgot
about those missing guys okay so let me
show you text and identify commands the
first thing I'm going to do is the text
and basically this is going to let me
see if I can explain this here first all
put it up here so this basically labels
that's kind of over written here
basically it labeled the three students
that got less than 150 right 150 is
there it labeled these how did it know
how to do that well let me put the
command back up there so text takes
basically three arguments right X Y and
then what the labels are and the labels
you have to literally say labels so
basically what I did here this shortcut
if you forget about everything in the
the square brackets here data dollar
sign exam one - an exam - labels if you
just called that you would basically
label every single point now that would
make the pot look pretty messy right so
what I did is I took a subset and this
is the same subset each time it's data
dollar sign exam one less than 150
dollar sings I won this one $50 singly
on one lesson 150 so this is not row
numbers but it's actually a logical
vector and if you just grab this part
here up sorry I just want to grab this
part here
won't let me will it okay I'll put the
square bracket myself just grab this
part here up down at our sign here I
just do this no square brackets there
it's just a logical vector meaning you
know which people got less than 150
which people didn't and in fact if you
look at dollar sign exam one it's just
going in order telling you you know this
person did not get less than 150 not not
true right so it's just a logical vector
so when you subset on that it will
basically pick out the rows where this
is true so you can easily just pass the
logical argument into the row into where
you would normally specify which rows
and that'll pick out the rows for you
and so I basically passed the same one
in each time and that selected those
cases for X those cases for Y in those
cases for the labels and so then that
has the effect of labeling everyone who
got less than 150 on my scatter plot and
as I showed you already your scatterplot
then looks like this right where you've
labeled everyone who got less than 150
and so you know you could write the
script and every time you know you give
the same exam it's always going to label
the student names for those people who
got left less than 150
so that's sort of the automated way to
label points on the graph then the
interactive way which doesn't really you
can't really write a script to do this
one but if you're just playing around
you can use this identify command which
takes sort of the same arguments as the
text command but it's more interactive
and once you do that it basically so you
see my window here sort of freezes okay
and what it wants me to do is go over to
my graphics window and basically now oh
sorry
basically it's a point-and-click thing
here right so you mouse over the point
and it actually depends where you put
them out so if you put it over here
it'll able to the left oh it doesn't
like my display if you put over here to
label to the left and if you put it up
here it'll label on the top right so and
then how do you make it stop you
right-click and you say stop now you can
still you know save this thing as a file
right those labels will be there but you
can't really write a script and this is
just sort of the point-and-click thing
or whatever point you point to and click
it'll put its label there and so that's
sort of fun you don't have to dig
through the data to see who's who you
just say I wonder who that point is you
click on it and you and it'll label it
so that's the text and identify and then
one other thing that I really like about
labeling points on the graph is the
following
and this is it's sort of a subtle point
but I think it's important um I when I
tell you that when you make a graph you
want it to be self explanatory so I know
this is exam one I know this is exam two
but the thing is whenever I see a graph
I always ask people like what are the
points right are the points you know in
this case I know now that they're
students as soon as you label one of
them as student number 30 you know that
all the points represent students but
often I see graphs and I say okay so are
these points URLs are these points
webpages are these points search engines
what are these points right I know what
the axes are but I don't know what the
what unit the point represents it's
possible you know if you didn't see
these labels here exam 1 exam 2 the
points could be professors
points could be universities the points
could be anything right you want to know
what the unit is and I think by labeling
at least one of them then you start to
know oh these units are students and you
actually know personally who a couple of
them are okay so that's another argument
for labeling points on the scatter plot
so let's see what else do I want to say
about scatter plots I think the last
thing I want to say about scatter plots
is the following okay
when you have a scatter plot um you
never really see this problem coming
until it hits you if both of these are
discreet or even if they're not discreet
you know if you they're floating points
but you've only sort of used you know so
much memory for them so at some point
you have some sort of at some point
everything is discreet right we know
that so if they're discreet and in fact
I think arguably does that exam scores
are right because I probably don't give
half a point so how you have a points
maybe I don't give quarter points so
let's assume these are rounded to the
nearest whole point so exam one is
discreet exam two is discreet well if
they're discreet that means that points
can hide on top of each other right well
any one of these points could actually
be two points now when is that a problem
well it's a problem when I started
getting a lot of data you know we talked
about data mining you know unless that's
you say how large is large well at some
point you know we're going to start
having problems and the point here which
we would have problems would be if I
have a whole bunch of this data what do
I have eventually I just have the whole
grid from zero to 200 and everything has
a point
and I have no sense of where there's a
lot of points or where there's a few
points right and that's really what I'm
trying to look at with Skype I'm trying
to see oh here's this relationship right
because there's a lot of points here
well if there's just it's just all
filled with points and I can't judge
through the density of the points in any
region I'm out of luck so what's the
solution with this discrete problem the
points lying on top of each other well
it's the simple sort of hacky solution
which is just to add a small amount of
noise and just jitter the points a
little bit now then you know the other
problem which is related to discreteness
but if you just have so many points that
you know then you just wind up with just
a huge black blob you still may need to
do some subsampling right and if you do
the sampling randomly then you can still
judge the relationship so these two are
a little bit related but if you have
small data and discreteness is the issue
the jittering will help if you just have
so much data that the scatter plot is
just the solid blob of ink then
might want to take a subsample but let
me show you this first case with the
jittering because this is course is
really easy to do an are suppose I just
want to add uniform noise to both x and
y uniform negative 0.5 to 0.5 well all I
would do is overwrite exam dollar sign
sorry dad a dollar sign exam 1 and s on
exam 2 with the same thing but our
uniform generates uniformly random
uniform random numbers distributed on 0
1 and this argument is 40 right there's
40 rows in this data so to each of them
I add 40 uniform random numbers and I
don't want to do it on 0 1 because that
would be adding a positive amount to
both dimensions so I subtract point 5 so
that'll be uniform on negative 0.5 to
positive point 5 and then I do
everything the same as before and so you
know this will overwrite the values of
exam 1 and exam 2 I mean you might want
to call it something different you know
just so you don't you know add noise to
your data and lose your data forever but
I'll just overwrite them and then go
back and let's see get this here and
that will make the graph and then oh
shoot
ok sorry don't overwrite the data so fix
fix it again ok sorry
try one more time don't overwrite the
data ok add the noise go back make the
graph don't get the data ok that'll make
the graph without overwriting the data
and then go and get the text and
identify commands and of course identify
doesn't do anything until I actually
click click quick okay stop okay so
basically it's the same graph plus noise
and if there had been discreteness
issues they would have gone away let me
illustrate this to you a little bit
better on the PowerPoint I'm going to
take here's my solution with noise let
me go back then and steal my other one
without the noise and line them up right
next to each other and just sort of show
you let's see oh wait that's not the one
I want I want the one with the labels
end
this one compared to this one there we
go this is so you can see sort of the
points to sort of dance around a little
bit right and so had you know had this
point been on top of another point once
I had the whoops once I had the noise
you know it moves around a little bit if
that's not enough you know if they're
still sort of smack on top of each other
you could add a little bit of noise but
you see how they sort of just move a
little bit now the thing I think that's
cute in this example when I did it is
here I have three labels and here I have
four labels right three labels and four
labels so the student number forty he
was not labeled before and now he is
what happened to him Yeah right before I
labeled everyone less than 150 so he did
not get below 150 right maybe he got
exactly 150 but as soon as I had noise
to him there's a 50% chance that he
falls below and so now he gets a label
so you know the noise is
non-deterministic so don't let that
bother you and you can save the seed if
you have to but you know like I said
it's sort of a hacky solution but it's
sort of uncovers points and so if you
have a lot of people stacked on top of
each other you can see and I one case
that comes up all the time as maybe I
can sort of illustrate this on this
board is that on the scatter plot you're
often looking for like linear
relationships like this right like
obviously there's a strong relationship
people you know positive linear
relationship right but what you might
happen you know if everything is
discrete you just see like this right
you know so let me here to do the case
of like nine points right so you just
see this but you don't realize that 1
million 1 million 1 million and like
these guys are just one point so you
don't realize that it is a really strong
relationship with all the data there
until you sort of jitter it a little bit
to uncover that so that's just the thing
to watch out for
why is good here so you guys look at my
back the whole time never knew that ok
so anyway
oh yeah yeah you can like do the bubble
plot and set the bubble to be the
density yet people do that too and the
bubble plot sometimes useful for showing
a third dimension also sometimes people
do that okay so let's see that takes
care of adding noise so I think where we
get the last thing I was going to talk
about was the box plots um let's see
should I get into the box plots today I
think I'll just save that sort of it's a
natural breaking point so I think I'll
save that for next time you guys have
any questions before I take off what
about those an a we haven't yeah about
it yeah can I set something like an
artificial score like a 94 dnase and
improv that yeah so I mean you could um
so the question is about the missing
values right so we had these three guys
that never took the score I mean all I
know about them is their exam 1 right
and so I could you know put a vertical
line
I could put a predictive point right if
you drew the least squares regression
line you can make a prediction for them
but that that's they're just going to be
right on the line because the best thing
you would predict is the means I'd like
to see where they yeah yeah I don't
artificial I would say yeah I TV cuz
that's out of the range so it wouldn't
right I my inclination would be in terms
of just displaying the data to just put
you know sort of tick marks down at the
bottom to indicate the missing values
and the thing to watch out for is you
know what if 90% of the data was missing
you know you to just ignore that as you
know a bad situation and you know I
think a lot of times when you deal with
real data the most interesting cases are
the cases where the data is missing
right you know there's something going
on often you know there's a lot of
practical issues with that I mean it's
like ok let's look at how good these
mutual funds are doing and we basically
are looking only at those who are still
in business transaction Hayter and all
those that have dropped out exactly
selected out of the magic is oh they
were right right well yeah those 10%
that survived exactly so it's like it
the students are going better and better
yeah because did the everyone else die
yeah yeah right I think they'd be
excellent example right if exam 2 is
higher than exam 1 it's a biased sample
it's the sample of people that stayed in
the class right so I've got rid of all
the people that were sort of on their
way out right and if I had exam 1 exam 2
exam 3 and people are dropping out
dropping out dropping out then each exam
could get better and better and better
right so yeah that's a very important
point that the missing missing values a
lot of the software just ignores them by
default but in the analysis sometimes
that's the most interesting signal is
why are people missing and to appreciate
the bias that you have because you're
only looking at the non missing data
other questions about anything before we
take off okay so we'll finish up these
notes on a Friday</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>