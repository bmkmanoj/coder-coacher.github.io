<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Bootstrapping Big Data... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Bootstrapping Big Data... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Bootstrapping Big Data...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/6b3-0jIrYZM" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yes so yeah so my name is Arielle
kleiner a PhD student at UC Berkeley I'm
going to talk about quantifying
uncertainty of estimates which hopefully
by now we're pretty convinced that this
is an important problem before I
continue let me say this is joint work
with a metallic r &amp;amp; pernah Sarkar also
at Berkeley as well as our advisor mike
jordan great so before I continue raise
your hand if you are familiar with the
bootstrap okay that's pretty good it's
almost like we're at nips so our setting
here is going to be our goal is pretty
simple but very general so we're going
to observe n data points for today
there'll be ID but that can be relaxed
we're forming an estimate theta and will
call the estimate based on n data points
theta hat n that can be something as
simple as a mean or as complicated as a
classifier and our goal here is given
this estimator theta and a set of n data
points we want to compute an assessment
see of the quality of theta hat n and
that assessment could be a confidence
interval confidence region a bias a risk
whatever you want right the point is
we're quantifying our uncertainty or
more generally assessing the quality of
our estimate and so the reason this is
important is for example if you know
we're training a classifier we train a
classifier based on five data points
right we should probably be less certain
that this is a good estimate than if we
train their classify based on a million
data points obviously we have that
intuition but it's very useful to be
able to quantify that for example if we
want to do hypothesis testing is one
actually a great example right so you
want to be able to do this in a great
deal of generality and so we want a
procedure which can take this theta and
quantify its quality assess its quality
and we want this procedure to be
accurate we want it to be automatic it
shouldn't depend on the internals of
theta right view we don't want to have
to do deep analysis of every new
estimator that
we want to treat and we want to be
scalable we want to scale up to large
amounts of data well because otherwise
we wouldn't be speaking in this workshop
so okay great so existing procedure
there if you look at you know
statisticians have been thinking about
the problem of assessing estimator
quality for some time and so there are
procedures to do this and in fact
statisticians have been known to use
computers and so they've developed
computational procedures even for doing
this the issue with existing procedures
is that in general they don't satisfy
all three of these criteria we have
procedures which are very accurate and
automatic but not so scalable more
scalable procedures which are not as
accurate and automatic and so I'm going
to talk today about work we've done on
creating a new procedure that is
satisfies all three more than existing
procedures do okay so how would we go
about creating such a procedure well if
we were God if we could do sort of the
ideal thing right we would simply to
quantify the quality of an estimator
theta we would run repeated experiments
right we would repeatedly observed data
independent data sets of size n for each
of them compute theta we get a
collection of Thetas and we would
quantify their the uncertainty in those
right we could compute their standard
error computer confidence region and
drawing this schematically right we have
n the independent data a number of
independent data sets each of size n for
each we compute theta hat take those
theta hats and you know assess there are
their variability more generally some
quality assessment the reason we can't
do this is we only observe one data set
of size n in practice right so that that
seems to leave us in a bit of a bind so
as I said statisticians have have given
some thoughts of this problem and so in
1979 Brad Efron at Stanford proposed the
bootstrap which many of you are familiar
with so I'm going to go pretty briskly
through this but the point is we just
basically do the simulate the ideal
process that I just went through on the
previous slide so basically we use the
observed data to simulate data sets of
size n by resampling with replacement
right we do this resampling process
repeatedly on each resample now we can
u theta hat now the fade ahead has a
star because it's computed on re samples
from the original observed data we take
the theta hats and compute our
uncertainty quantification and so
schematically that looks like this very
similar to the previous slide right
except that now we start with a single
observed data set without stars we
generate multiple simulated data sets of
size n compute the theta hats and
combine right this is obviously very
simple turns out to be very powerful
right because number one we're not using
any information about the internals of
theta right we just need you able to
compute the theta so functionally right
somebody could just give us a computer
program that computes theta and not tell
us exactly what's inside of that right
as in from the standpoint of
implementation turns out to be very
accurate in very general settings but
what about the scalability of what about
the computational issues let's talk
about that a little bit so the key fact
here is going to be that the expected
number of distinct points in a bootstrap
resample is about point 632 n so about
sixty three percent of the points and
the original observed data set will
appear at least once in a given
bootstrap resample and the reason for
that is again that weary sampling with
replacement right so some data points
will appear more than once in every
sample some will not appear at all the
second key point here is that the
resources required to compute the
estimator are generally going to scale
in the number of distinct points that
you feed to it right the reason for that
is that if you have replicated points in
in the data set you're feeding to an
estimator right you wouldn't actually
physically replicate them you just
attach weights to them right and most of
the estimators that we that we care
about right have this property that they
can operate on this kind of weighted
representation things like regressions
kernel methods em estimators in general
really and beyond so again right so when
running the bootstrap we would use this
weighted representation to avoid the
physical data replication the problem is
that if n is big and we're doing big
learning here right so and is expected
to be pretty big point 632 n is also
going to be very big right so concretely
I start with a terabyte of data then
each resample is going to
have size roughly six hundred thirty two
gigabytes and I have to do this repeated
computation of theta on data of size six
hundred thirty two gigabytes okay so
that's not so great because I'm going to
have to compute on a few hundred
resample and you might say okay well I'm
computing on these resample okay well
before I do that let me just write that
out and explicit pseudocode right so
let's say three hundred resembles for
each of them I resample about six
hundred gigabytes compute theta on each
resample combined sorry okay so okay so
we're doing this repeating computation
on large amounts of data you might say
okay I'm computing on each of these
three samples independently right why
why I shouldn't why not paralyzed this
writing the reason is that in each
within each iteration I have to touch a
different set of six hundred gigabytes
of data right so are you going to ship
six hundred gigabytes of data to each of
three hundred different compute nodes
that's not so great you could imagine
going through a great deal of pain
potentially to try to make this work but
we're actually going to give a procedure
that makes this trivially easy to power
or much much easier to paralyze okay
okay so just to summarize the bootstrap
has many virtues in particular it's
accurate for a wide range of theta it's
automatic as we've said but it has this
unfortunate need to repeatedly compute
on large sets of data and it's difficult
to paralyze that computation across
three samples okay so we want to go
overcome these disadvantages so how do
we do that well we go back to prior work
why not if we do that we go back to
prior work we'd come across something
called the be out of n bootstrap also
very simple very related to the
bootstrap and the idea here is that we
just reduce the size of our resembles
rather than resampling endpoints
resample be less than 10 points right
and just do what the bootstrap does so
resample us then points repeatedly with
replacements compute theta on each
resample combine them but we're not done
yet
and the reason is that we're computing
theta hat subbies here okay and b is
less than n so if b is less so an
estimator computer and unless data is
generally going to be more variable
right so this see that we're computing
in step three is based on theta hat fees
it's going to be too let's say if it's a
CI a confidence interval it's going to
be too big right so we need to rescale
it basically in the way this is done is
you need to analytically be scale or
analytically correct based on prior
knowledge of the convergence rate of
theta so what we've gained here so
actually it's worth saying this was
invented by statisticians to with
without computational issues in mind it
was it was created this procedure was
created to handle statistical edge cases
where the bootstrap is is known to fail
but it is more computationally
favourable clearly right we're computing
theta repeatedly only on much
potentially much smaller subsets of the
data but it turns out empirically that
the accuracy is quite sensitive to the
choice of beefy is too small the ativan
bootstrap will fail and also while still
fairly automatic now we have to do this
analytical rescaling based on
convergence rates and so we've given up
some of the automation I should say also
in the interest of thoroughness that
there's another procedure called
subsampling very related which the only
difference is that subsampling rather
than resampling BN point B points with
replacement it's sub-samples be points
without replacement okay and behaves
very similarly to the be out of n
bootstrap actually also tends to fail
for very large bees so in general has
somewhat worse behavior so we're really
going to be talking about when we talk
about prior compa prior alternatives to
the bootstrap will talk about the viet
event bootstrap okay so just to drive
this home these this issue about
brittleness of the accuracy of be out of
in bootstrap i'll show you some
empirical results this is going to be a
linear regression hundred dimensional
linear regression 20,000 equals twenty
thousand data points this is going to be
synthetic data which is necessary here
so that we know the ground truth right
we need to know the true c is for our
estimator in order to do the evaluation
you'll trust me that this is you know
we're
buying things from various data
generating distributions we're doing
these squares estimation are see our
quality assessment here is marginal
confidence intervals on the components
of the estimated parameter vector and
we're evaluating based on the confidence
interval sizes on the confidence see I
widths right so basically the bootstrap
outputs a confidence interval we
calculate its width compared that with
to the width of the true confidence
interval okay for the be out of n
bootstrap we're going to take B equals n
to the point 5 up to n to the point 9
and these are the results you'll see are
pretty robust to the choice of data
generating distribution we can do this
with estimating the specified models
similar results okay and here we go so
what you're seeing here is on the y-axis
is relative error of the output of the
procedure so again how far is the width
of the CI outputted by this bootstrap
from the true width of the CI right so
are we getting the appropriate
quantification of the uncertainty in our
estimator okay so lower is better the x
axis is time so as we run the procedure
as we process more and more resample in
the case of the bootstrap what happens
to the relative error right so the
bootstrap here is the blue line with
circles as we as we process born Murray
samples it does better and better right
converging to nice low relative error
the dotted line the other dotted lines
are all the be out of n bootstrap for
different values of B and so what we see
is that for the larger values of B 4 B
equals n to the point 7 and higher the
be at event bootstrap is behaving very
nicely it's converging to low relative
error faster than the bootstrap right
showing nice computational gains the
problem is it beats too low and to the
point 6 or lower right it's failing it's
not converging to low relative error
this is not good right number one we
want to make be as small as we possibly
can number two how do we know where this
this threshold is right how do we know
that n to the point six is that cut off
okay so what we'd like to do is at the
very least reduce this threshold well we
will also like to keep things more
automatic we want to eliminate the
rescaling that the be out of n bootstrap
has to do while maintaining the
scalability
okay so that's exactly what we do so
we're proposing a new procedure called
the bag of little boot straps or BL be
okay and what we're going to do is only
touch small amounts of data at a time
while maintaining much greater
robustness to the choice of be okay so
we start by repeatedly subsampling be
points be less than n points without
replacement from the original data set
so we have a number of these subsamples
for each subsample we then run the
bootstrap on that sub sample okay so we
take the subsample give it a subsample
weary sample n points with replacement
so we've gone from n down to be and then
we resample back up to n okay we do this
resampling repeatedly for each subsample
so for example we compute theta hat
given the resample we combine the theta
hats to get XE let's say a confidence
interval so now what we've done is for
the particular subsample that we're
processing right now we've obtained a CI
or standard error what have you okay and
now we have one of these quality
assessments for each subsample how do we
get a final single quality assessment
well we just average okay so if it's CI
we might average the boundaries if its
standard error we just average the
standard errors okay so i'll show you in
a couple of slides that this is more
accurate than alternatives like be like
the be out of n bootstrap it's i think
clearly by construction automatic right
like the bootstrap we're not using any
analytic rescaling we take the theta as
a black box right you just need to be
able to compute it how about the
scalability right so we are subsampling
be from endpoints but now we're
resampling endpoints so are we still
scalable the answer is yes and here's
why in one slide so here's the schematic
representation of that just to drive
this home right we start with a data set
of size n we take multiple different sub
samples for each subsample we run the
bootstrap on the subsample that's what
the rectangular box is that gives us a
key for the subsample multiple exceeds
we combine them by averaging okay
so let's talk about the scalability of
the lb so remember that the key thing is
the number of distinct points that we're
feeding T theta for each computation of
theta how many distinct points are being
exposed to it and the key thing here is
that each BLB subsample has only be
points be less than n points as a result
hbl be resemble also has at most be
distinct points ok and you can choose B
to be much less than N and an example
here is that if we start with n equals 1
million let's say say data point size of
a megabyte roughly and we're going to
take in this example be equal to n to
the point 6 so a million times a
megabyte is a terabyte right so just
like an earlier example now BLB
subsamples andrey samples are going to
have it most it a million to the point
six points so roughly 4,000 points so
Abby lb resample will have size at most
four gigabytes so we've gone from a
terabyte down to four gigabytes that's
pretty good how do we came here to the
bootstrap so that's compared to the full
data set right we started with the
terabyte in the full data set free
samples have four gigabytes a bootstrap
resample remember has size six hundred
thirty two gigabytes roughly an
expectation so we've gotten the to order
of magnitude reduction that's not too
bad I would say so let's summarize this
so like the bootstrap were accurate for
a wide range of theta so basically we
can show actually that you get
consistency and good convergence rates
favorable convergence rates under the
same conditions as the bootstrap where
as automatic as the bootstrap how do we
go beyond both the bootstrap and the be
out of n bootstrapper subsampling so
beyond the bootstrap right we're
explicitly controlling the amount of
data used to can use to compute the
estimator at each stage right so theta
only touches at most data of size b and
b over n can actually go to 0 as n goes
to infinity all right so generally going
to take be to be a slowly growing
function of n such as n to some power
some power less than 1 we're more robust
than to be out of in bootstrap and
subsampling to the choice of B I'll show
you p racal results on that in a second
we're generally faster than the
bootstrap even in a serial setting and
so we require less total computation and
more easily power
elizaville right because now let's take
our example we started with the terabyte
of data we're down to four gigabytes you
put four gigabytes on a single node
putting six hundred gigabytes in a
single node is a lot more problematic
okay so empirical results again y axis
is relative error so lower is better x
axis is time number of iterations of the
procedure number of resembles or in our
case sub-samples process so the
bootstrap is again the dotted blue line
converging to nice low relative error as
we process more resample BLB is all of
the solid lines for different values of
B and now you see that for all of the
values of be considered a 10 to the
point 5 up to n to the point 9 we are
converging to nice low relative error
much faster than the bootstrap okay now
these are results for this linear
regression setting we don't have that
much time so I'm not going to go more
deeply into the empirical results I
we've also done experiments as I said
these results are robust to the choice
of data generating distribution to
whether or not you're estimating a
misspecified model we've also done
results on logistic regression that
appears to be a harder problem but the
the message is still the same basically
we are still more robust significantly
more robust than the be out of n
bootstrap to the choice of B but of
course BLB is not magic and of course
there will always be a sufficiently
small value of B such that things break
the point is we push that significantly
farther down and we also eliminate the
the rescaling issue okay so and this
just puts the two plots side-by-side and
people have seen so far right so
remember before we had these for the
lower values of big things not
converging the elbe converges nicely
I'll finish since we're running out of
time by saying that while I'm not
discussing it here we have some nice
theoretical results showing that BLB
basically shares the bootstraps
favorable statistical properties
consistency fast convergence rates under
the same conditions used for analyzing
the bootstrap and so just to summarize
remember our original goal right was a
genetic general procedure for
quantifying or
estimator quality that was accurate
automatic and scalable right the
bootstrap is very generally accurate on
an automatic not very scalable other
prior works such as the be out of n
bootstrap is more scalable but suffers
on the accuracy and automation front and
so we're providing a new and I would say
more favorable combination of all three
for the large data setting thanks right
so in these experiments we were doing
100 resample spur subsample and the
number of some samples you need depends
on the value of B so for larger values
of B you need fewer so for like the
Eagles n to the point 9 you only need
one or two subsamples for beagles n to
the point five or point six you might
need I think 10 to 10 to 14 let's say 10
to 20 subsamples and it's also worth
saying that you can use fewer resample
depending on your quality assessment so
for estimating things like quantiles and
confidence intervals you need more
resembles if you're estimating something
like a standard error maybe 25recent
balls might suffice I mean the total
number of estimator computations is the
number of resample times the number of
sub Central's well you can run the reefs
ambles in parallel right so every read
every estimator computation here could
in principle be done in parallel and so
one of these guys might need several
hundreds a few hundred estimator
computations but now they're estimator
computations on say four gigabytes of
data rather than six hundred gigabytes
of data so
what can you speak a little bit more
loudly first of all yeah yeah so the
black line at the top here is beagles n
to the point 5 yeah and the green line
here is n to the point 6 I mean it's a
good question to be honest I don't have
a concrete answer to that it's worth
saying right that the jump between n to
the point 5 into the point 6 obviously
is not you know like linear in the
difference between point five and point
six right so you know I mean it's a
substantial reduction in the amount of
data you're looking at to go from n to
the point 6 2 n to the point 5 is one
answer it's also worth saying then in
these experiments right d equals 100 n
equals twenty thousand twenty thousand
to the point five is roughly 140 so if
you were to go really much lower in
terms of choice of B you'd be like
hitting up against the statistical
essentially like a hearts this is to a
barrier on the the reduction you can
have in the size of resampled and still
have good performance essentially so
yeah
in this case it is so so we haven't done
those experiments specifically but there
should be no particular reason you
should have any any particular problem
like that in fact you can use the
bootstrap to measure bias actually like
you can actually assess by us using the
bootstrap so i can say certainly from
the standpoint of theoretical analysis
it doesn't make a difference it's fine I
mean week so to be honest we didn't
start out with a particular application
in mind I think one nice thing about
these techniques is that they're so
generically applicable yeah I welcome
you all to create wonderful applications
of the bag of little bootstraps thank
you yes</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>