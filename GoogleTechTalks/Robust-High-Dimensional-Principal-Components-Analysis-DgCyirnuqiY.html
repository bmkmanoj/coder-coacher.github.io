<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Robust High Dimensional Principal Components Analysis | Coder Coacher - Coaching Coders</title><meta content="Robust High Dimensional Principal Components Analysis - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Robust High Dimensional Principal Components Analysis</b></h2><h5 class="post__date">2010-09-29</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/DgCyirnuqiY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">luckily my team McGill University and
then went back to John will talk about
robust high dimensional data and that on
shoe and reminds and that's a paper that
appeared and called this year and it's
kind of interesting I thought it would
be interesting for you guys it talks
about what you do when you want to do
PCA and you have contaminated data and
as we know appreciate extremely
sensitive they're going to single a
single bad so I'll talk about it in the
context of PCA but really what's lurking
here is a much more general problem and
there are two problems that this talk is
a special case of so the first problem
is called a driven stimulation so you
want to optimize a function expectation
of San Marco mega and the decision in a
way that you don't know and you have
samples of this of this problem and but
the concentration of the samples are
corrupted and maybe even children where
intercessory it's probably design
imal terry from you know she's movin on
Google perspective but that Haven and
dirt can be children when I have an
adversary that knows Robert and the
other problems more shimmering like is
is finding hidden structure so you have
a noisy measurement you have some
structure there and and there are many
examples like well like medical learning
and presencing and basically we want to
find the instructor so this talk is
about a very simple case which is a case
of PCA so here the function is right to
optimize
sort of varam so that excellence blows
omega and the even structures we look
for is a little dimensionally or not
manifold so this is real and if you want
to solve those two problems that
probably were just are started so
dimensional linear and function here is
smallness around function simple as it
gets
and that's really what this talk is
about and we'll focus on PCA because
it's tractable and our alpha Daisy but
the goal is to try and solve those
they're driven a stochastic engaging
problems that are important I think it's
Rob engineering and just figure out how
and if we can find the hidden structure
when we have contaminated data so let's
say just you know I mean I'm not sure
like everybody here is remember I
remember example is they say I should
probably answer is yes but just sort of
let me recall the details so you observe
high dimensional points and want to find
the best approximation which is a at
least square approximation and listen I
mean essentially every field of
engineering in shirin people are not the
one that you the most surprising but you
make yourself saying that is it then
expensively but also in communications
so this is just a picture here and we
have some noise and we assume that y
equals ax plus 3 so AIDS as in matrix
our signal X is X is the data that moves
over a and we add some noise and you can
assume whatever you like about the noise
perhaps so we have a signal and the
truth of it small mornings and you know
this is the picture that you're going to
see everybody everybody's happy you can
learn it and you can find you can find a
and that's
we're going to be the least
quarter-square air subspace
approximation finding subspace it has a
minimum error or currently the best
compression scheme for our data so
that's our dc-8 and just a few notes oh
well what you really do is we do that we
do SVD here and on sample covariance
matrix and that's going to be important
to remember I also reminded that the SVD
is not it's not a context problem so
we're solving a non-convex problem and
had looked at trying I'm using that fact
that were using least square error is
crucial here so everything will break
down if instead of please for Larry you
don't have something much a little bit
nicer so if instead you would have
something like that tails done and in a
more comfortable way at the end for high
values and things with right now so Shea
Collins the scope potential theory of a
generalization better and it's not
convex the result is a committee
approved so the sort of a greedy
approach to the best of my collection
and and basically the set up of flow
dimensions is not too interesting for us
so we'll move shortly to set of words
that dimensional is the same example
things are going to break down so so I
mean this is a very well-known result
even one outlier can make the output
arbitrarily skewed so I imagined it the
example before if you have one sample
here and then that would change
everything completely if you had it you
know somewhere over the moon then your
this square subspace approximation is
going to basically be destroyed
completely so that's one consequence and
the question you can ask yourself is
what can you do when you have a constant
fraction of
and that's one of the topic of this
stuff so this is what we were looking
into is we're looking into two things we
look at the high dimension regime so the
number of observations is essentially
the same in the dimensionality but we
can always embed the dimensionality to
be equal to the number of observations
so this is a regime of interest this is
what we call high dimensional and there
is a constant fraction of points that
are arms rather corrupted so side your
points are small to the points come from
a model but some points and this this
fraction is can be significant is
corrupted possibly by an adverse way
okay so what I'm not to do now I'm going
to introduce it this is album chocolate
mousse you start with some occupation
and then I sort of explain where things
get tricky in how many dimensions and
essentially the truth
the usual flows are going to fail and if
you want to suggest here jewels I'll try
to convince you that they fell - or
rewrite the paper correct in your view
and then I'll present an algorithm that
we call the HRP see a high dimensional
robust pca and sort of experiment with
how the proof works and about it so high
dimensional data I don't think I need to
convince you guys that I dimensional
they present it's important what I do
want to dimension here is an application
which is kind of which is kind of
interesting and that's application in
networks and them and use it oh aware
these are aware Network algorithms so
one of the problems that I think Mobius
theorem said before I'm really an
engineer this offends me so I'll funny
how I mean we are able to collect a
humongous amount of network data so
example the network think about an IP
network in many places and we get
huge string of data and we want to sort
of figure out
I mean what's really happening in the
network and so very useful for us to
just take this data and crunch it and
say things like you know this node it is
won't understand like the way this flow
works so if you can run flow equations
and you can get lots of data and then
basically once you figure out what
happens another application which is a
pretty cool and there you get linearity
by essentially by cubes of our laws and
another reason why our high dimensional
data is important to the kernel tree
that of course generates high
dimensional data so what we'll see here
with that traditional tools that do not
seem to scale up an example so here I'm
at a figure where we have no outliers
and here we do have outliers
now we think about it in the lower
dimensional regime so you know this is a
really easy problem right because here
are some things you can do you can just
discard those samples that have starts
running to it or that they have large an
obvious distance or you can look at the
ellipsoid that covers most of the data
and then expand it a little by little
and once it's it once you see that if
you expand it by little you don't get
much with them unexplained by much
because you do get much it's basically
if you cover this set or this it in this
case and discard everything else so if
you look at what's known as the minimum
volume ellipsoid or the litter that
contained in say 90% of your data then
we're told if you will be able to
discard so in order mention that the
problem is seems to be a
and there are a lot of good food
solution and they'll all work and others
are not really a problem but in
heightened dimension none of this is
going to work at all obscurity convinces
so after I do not need to have large
magnitudes they can have small
difference and in fact the volume of
small Centennial issue it is zero so you
know you don't going to change anything
there so on so our goal is to to get
game robustness to arbitrary graph data
and I'll show them all Mulla shortly one
measure to think about it is what's
known as the breakdown point so a
breakdown point is a concept in robust
test statistics and think about the case
where we have an estimator and you're
asking for salary is going to add to
sort of corrupt some samples how many
samples of this evil adversary has to
grow up so that he can arbitrary change
my estimator so for example in the case
of me so if there is a if you have a
single dimension around the Bible and
you want destiny and you take that
beautiful moon then its adversary wants
to change to an arbitrary value can just
take one sample and put it somewhere and
that's going to change the mean to the
atom sorry will we can make it basically
whatever he wants
if one instead we're going to take the
median and the regular point is half to
the point so to change the median to our
arbitrary points adversary's needs to
change add samples so the great kind of
point is a sort of an acceptable measure
in horvath statistics to to robustness
and what we're going to to try and look
at is that and I'm going to try and
bound there between the true PCs and the
PC that our algorithm is going to to
recover and the bond is going to depend
on two things the fraction of the
outlier so how many after as adversary
changed and properties of the
distribution of the signal and that sort
of that's like this is really what we're
going to do and so let me just be more
precise
from setup so we assume that there are
we have T authentic samples and the
samples reside in some RM and they are
and they have this German team also that
ever example is functional system home
plus no means so it is a matrix
representing their basically movement to
find a and X's is some random variable
the noise is assumed to be a normal
first or guardians birthday of this top
but this is not really important and a
is a matrix which is unknown and the
signal is just 0 mean with several parts
it's not any point was francitas you can
you can change the covariance and push
it into a so innocent identity so when
you have G samples and also a little fan
examples and then we have n minus T
outliers and those answers are gentle
abot rarely so they can be the you know
problems in the day said there can be
things that some adverse area push the
mass look in a network problem they can
be just errors we don't know where
they're coming from but there are
generated arbitrarily so we have n
samples overall and few of them are
authentic and n minus T are are not
static
we observed all the samples and so we
observe the automation is totals and
samples and and of course you don't know
which is which so we don't know which is
sample if I think in which sample is a
is arbitrary
now the regime that we're going to work
on is as a regime that the
dimensionality of the problem that's and
so everything here like resides in our m
and the number of samples is more or
less the same and much larger than the
dimension of symbol or if you like the
dimension of the matrix a which is what
we would like to dissolve and and what's
known as a signal so in you know a
signal s which is a romantic side of the
matrix a it assumes to increase us
careful with with the points or the
dimensions and our objective is to it
so this is ours the set up of interest
in a more precise way and of course
retrieving a is impossible so we're
going to settle for for something which
is like retrieve something that looks
like like a and now I'll shortly discuss
what's what the meaning of Sigma scales
are slow its illicit and I'm trying to
figure out where things that I get three
as before that kind of question but I
said that this is a really important
point if you miss that then I might as
well learn now for the rest of the stuff
so the second thing is very simple so
there is a genitive model deterrence T
point see samples the other samples are
generated somehow somehow and we want to
retrieve signal a so where things get
get treated so the first thing can get
tricky is and always explosion so we not
to mention the no planning to scale
status with no noise ninety two scales
that linearly with that dimensions in
particular the snr the signal-to-noise
ratio goes to zero and this is a first
are really annoying things about it so
just to explain on goes to zero if you
have an international round the Bible
Sega option then if you look at the at
the norm to norm then the two norm
essential equals the square root of n of
the dimension dimensionality with very
sharp concentrations this is a result
due to child round and I mean there are
many I mean this should the construction
is very sharp on the other hand is
signal so if you look at the norm ax and
then it's something like the Sigma Sigma
is just to remind you is Sigma is the
norm of a musical tape time through the
dimension so this scales with M this in
squirt the scales are much much slower
okay so what it means that the son
I may have much bigger magnitude than
outliers I mean the outliers are not big
and it also means that the noise is
going to drown the signal so the
direction of each sample is going to be
approximately orthogonal to the
direction of signal and all the samples
are going to be approximately orthogonal
to each other so let me just convince
you this is the images we had before so
you know this was our signal and this
was a low dimensional regime and in high
dimensions asleep the noise is very
large so the signal is really the bed
small and the noise is large but always
much larger than a single and and more
were if you have even if you have to
sample at sample that are closed in a
single space once you get the noise then
they're going to they can be very far
away from each other so now to make
things even more annoying every point
with more or less equal distance from
the origin and from all other points
because all the points reside on the
ball in a very high dimensional space in
a concentrate I mean not now if you
start playing with high dimensional
Japan's probabilistic George so all the
points are the same distance from each
other and from the religion and they're
all orthogonal to each other so they're
perpendicular to the single space and
what more or less fulfilling all to each
other so that's what happens once you
look at the regime of high dimensional
regime and I think that's a regime of
that there's a logical way to increase
dimensions so you have noise at these
areas in all dimensions should let's see
what we can do so let's try try to come
up with some natural approaches so we
can do a live one out for a more general
yourself sample and then basically cell
samples the data would you pca with do
another sub sample you pca we compare
what we get then then i mean once we
have a few corrupted points in every sub
samples and who knows what's going to
happen so once you do pca and you have
such equal points you know things are
going to end up badly for you if you
have really small samples then you might
not have enough data so it whistling oh
please
not seem to work and I think I said well
you know it's just they just do the
start things let's do a robust pca and
in reverse mistake basically we take the
parents matrix and we will bust if i it
so we saw her at the pc a proper to
respect to the worst case france metric
which is close to the true above trooper
grants matrix it is also not really a
good idea because well if you look at
the france matrix it's going to be a
huge matrix in high dimensions so it's
going to be a n square or an n square
matrix and and basically you know if I'm
traveling out enough observations do
anything use just because the matrix is
so is so large so start robust pca i do
not seem to be is probably not going to
work it's really such a going to get the
class of the noise so now you can say
well let's remove point to a very high
magnitude so let's just probably bad
point but as i said it's over then
always drums signal so we're just going
to throw away some point some of them
going to be authentic sound are number
two be authentic you have no idea it's
better to know which which ones are the
correct one so in this example the
outliers are going to have small managed
so this is something that you know that
virtual you can choose for you and so
there this is done seem like a really
good idea i'm so then you can say well
let's remove point with large on
distance and probably the same problem
so that virtually choose that I choose
those points and and basically it's not
going to work out and you can look at
other approaches I know distance this is
another distance measure and and again
that's Russell you can choose it in a
way that the noise drowns out the signal
and then you can look well that's a
little bit you know all sort of
estimation
estimator I live to ill-feeling and so
forth I'm not going to get into those
methods by the place I we haven't been
able to make them work in this version
so so that the pointed edge really those
approaches are work for the equals 5
equals 7 and that's it
not going to scale up to more than that
okay and then there is the issue of
direct ability so you want an algorithm
which is tractable so it should be
polynomial on the dimension and and one
approaches what dimension before the
medium of minimum volume ellipsoid so in
this approach you take a look site that
has a minimum volume and covers all the
data and it starts shrinking it and then
we throw away points that do not add I
mean look at the literature the contains
a 980 and that seem like a very very
general dimensions in high dimension and
zeroes because for any lips are that
you're going to take is an equals M so
if a number the Machine Aarthi was an
hour of data then every lips that covers
all later how that the volume of 0
because basically everything is flat
completely and then if you're going to
say let's try to - this still does
something and remove fracture on the
points you're going to get a commenter
old problem and you can do something
like judge pursue that's also not going
to work the projection person you're
just going to look at the best
projection to deference and then then
the second-best projector and so forth
that's not going to do that's not
guaranteed so basically we have not
aware of approaches the to work in this
regime and what I'm going to do not go
to present an approach the does work in
this region and digestive the approaches
it's really very simple
so that gate that you know we are aware
of the Charlotte who can go through all
the outliers for some of the evil points
I just want to cannot identify but will
keep them if they don't harm our
computation too much
so either with if a point harms the pca
by much i'm going to throw it away if
you doubt the timing by not jamming and
we don't care too much
so the explained may enjoy our so first
we assume that he is known so we assume
in dimensionality is known that's a big
assumption but about that will make a
why from a disco so we get the candy
direction from stampy
so we do PCA on the on all of the data
no problems then we do we project the
data to the little dimensional space we
use what's known as robust variance
estimator explain what it is it just it
just estimate the variance of the on the
low dimensional space and then we have
there are two are two things that can
happen well if the outliers are far from
the region so if the outliers comes with
the low to the burns we're going to
throw them away if they're if they're
close to the region so they jump up we
have much to the virus we're going to
keep them and say well they don't harm
us too much so we do the projection and
then we removal of points which are
strange okay so once it looks to have a
large Varmus and and what are the
properties that we want to obtain so I
wanted algorithm to be tractable
so this time same complexity as spca
essentially going to be robust to a fire
so one some performance guarantees which
will provide we want it to be
asymptotically optimal which means that
if the number of outliers is little
little of n so if you have very few
outliers then you want to have perfect
very maneuverable carbon and you want
your analyzable so let's just imagine a
set up so we have tiers of panting
samples in our m and they're generated
according to the knowledge is written
here so every sample is miss Dignam a
times I mean this is what it's time to
recover age at times X plus bonus no
he's assumed to be universal Gaussian
and we have outliers and we observe
everything together and we want to
recover a so let's me explain what they
mean by recover a Oh
so here's so here is a here in the
explanation in one dimensions maybe you
know this one here this is a real the
real matrix a and this will recover so
the angle looks like something
reasonable so if the angle is small then
we have
we feel that our recovery is good but in
two dimensions do you know who did it
how to compute your angles in two
dimensional spaces so there are ways to
do that but it's a little bit tricky so
what we can see there it is is basically
to look at how much violence is captured
so so you want device to be captured by
I mean as much long as it's possible to
be captured by your yourself space up
here if when you do the projection on
the on this curve on the other line
you not a lots of virus capture if
you'll take another line then it's
you're going to have much less than so
here you know we have a lot of violence
captured here in this example we have
just a bit Affairs captured so so
instead of looking at angles we'll look
at them at what's known as express
violence
so they express violence of herself
space that is spanned by a principal
component WR TWD with respect to the
true web space is this expression here
and this expression and basically it's a
number between 0 and 1 it is 1 then the
southeast is spanned by but the true but
the truth principal performance that 1wt
and if it's zero it is ripped open or in
the case of one dimension that is just
wiping with a thousand square of of the
angle so you know this is sort of our
our measurement of success we want this
to be as close as possible to one so
with the people the world sites please
some media mics if you have a meet at
the morning oh sorry
so this is just this is a very
reasonable I think the art stick so we
want to just look at the Express arms
and won't you find a the times its first
one has a split one as possible so what
I need to explain she what's it what's a
robust transaction either so what's so
what's our math for an estimator
you have a direction I'd say W and then
he ordered all the older points from 1
to n so you do the projection then you
order them according to the magnitude to
develop them to the value and for lunch
and and then basically you take on only
ejecta you can throw away the smallest
and the largest few I mean just to
compute the variance of the guidance are
in the middle so if somebody has a
contributes a lot a lot of the variance
you just throw it away that's going to
be error that's called the robust
variance estimator and the reason is
that the breakdown point of this very
simple estimator is you know it's going
to be not f1 it depends on the value of
how many points you take so it's
starting to die okay so let's let's just
see the example that we have that we
have here here we have some samples
we've projected them on the
one-dimensional thing will see the ended
outliers are in the middle so their
contribution to the virus is negligible
or it was it small so if the corrupted
points contribute a little just a little
bit to the virus then we're going to
describe authentic ones and then our our
estimator memorize that signature is
going to have to be biased toward small
smaller device due to smaller values but
that's still rather not a big deal
because you know it's not going to be
things that I'm going to be to go wrong
by much if on the other hand the arm
flies are you know in the country with a
lot of ions then we're going to discard
them so so basically we project and then
we take think about sunlight from the
55% of thousands a 95% out and computer
virus only of those are only those are
projected points so very simple idea the
brakes at the down point this estimator
is sort of same idea of like taking as
percentiles and it's going to depend on
how many points you decided to discard
that's a get clear I tell you this sort
of classical idea robusta estimation
it's a sort of sort of works we have a
pointer problem here okay so here are
just in picture that what happens to
their left arm our ancestor so here we
have outliers about sort of close and
what you wanted to do is we were going
to do the projection so if what happens
in the projection that the outliers are
you know somewhere up there then it's
not a big then that we're going to
discard them and they're not going to
harness if on the other hand we do the
projection and and in the direction that
we project they're in the middle of
things then no big deal I mean it's not
going to harm us in terms of the burns
estimator so we're going to under how
we're going to underestimate the
different weight but not very much
so the algorithm is there for asphalt
you perform PCA and then critical
performance in high dimensions so this
is easy because you have an end up
endpoints in M dimensions but you find
the video and show more about space
that's explained in warrens if there are
if there are bad friends left tonight on
the pc direction is the highest let me
record it and you refer the VCS and
you're happy
and then you randomly remove points
which is proportional to the fairness
along the principal component so you
compute the for every point how much it
contributes and you randomly throw away
all anyone randomly later then you
repeat until that you have you moved
enough points any none fool they
explained there and then basically you
output the last efforts wants to record
so the open is very simple I mean
obviously it amounts to performing
principal component projection
projecting on the spaces you I estimated
doing the robot for us estimate throwing
away a pointer a few points randomly I'm
proportion to the advance and repeats so
the other thing self is is kind of
something you can you can curl them to
know how many minutes in that work oh I
wonder because you mentioned through
nation lis the PCA can be corrupted
completely for you so you start actually
in it there in the direction that that's
very sorry that's right so so think
about like suppose there is one point
which is really out there in the moment
all right now nothing it changes to
everything so when you do the principal
components these points are going to I'm
in the direction when the first of the
bots will include this point in the
direction so this point will have a very
large magnitude and then when you're
going to irregularly remove a point in
proportion to express with very high
probability we're going to throw this
point away okay so the first you steps
you're absolutely right I mean you
expect to be mostly discarding adversary
points in the first few steps if of
course you have many outliers if your
data are essentially just outliers on
you know oh there's nothing I can do for
ya so there is a possibility for a true
data and and we have to control this
probability so the results that we can
get it's going to be in terms of the
fraction of outliers because our
algorithm inevitably
throw 90 points and you know that's
there's nothing we can do about it so I
love that little bit more precise here
just to make sure that were on the same
page so you perform dca on the empirical
covariance matrix yet the principal
components then you can peel too robust
far as estimator will show before and if
your prices that are large or large that
you've seen so far you update your
pieces then you randomly remove a point
which is proportional to the varnas
along the current pieces and you repeat
until enough points were removed and
then your output the last record
the principle components so that with
himself is very simple every step from
computational perspective you all that
you need to do it soon if you do PCA
which is you know easy
you need to computer backgrounds
estimator which is easy new to governor
sample them and throw away on one of
them so that Wilton is a polynomial and
it's it's you know it's random but it
works pretty well so what can go wrong
and so we can the rule appending points
and there you will remove the 30 points
but that said it's not a big field you
may ultimately not report something
which is not the best because you throw
away some points and you can also have
corrupt points that we are going to
remain in your in your in your dataset
so and they may contribute to other
important pieces eventual so the three
those are three saying things can go
wrong but I mean from a technical
perspective we can show that you can
control those errors and in if you
choose the front they are the parameter
of the algorithm carefully done so let's
just I'll show the guarantee I'm sure
that we're going to be over in 2002 the
truth so the result is no jobs going to
depend on the outliers
that's a lambda and the tails of a
signal so X the signal has some tails
and things are going to depend on that
here I just find that this is a so it's
a complicated expression but essentially
it measures how concentrated and the
signal is so this is sort of a nature of
concentration of signal and the general
demille the theorem of the main is all
that we have that the Express warrants
of of the output and IO makes a
probabilistic term here from this high
probability there Express mass is going
to be a lower bound by implication of
two terms the first term is a
the error that we are because we're
using a biased estimator so the robust
trans estimator is a biased planet it's
going to underestimate Department of
iron's and the second term is due to
random removal so we may not remove all
the outliers and there is a parameter
that measures how many outlets without
going through move and so what's
interesting about this result and again
I'll open to get into all the details in
a if if you remember of the number of
that example none of others is little o
of n so basically almost overwhelmingly
the samples are are authentic than
what's written here is one so your
express furnace is one and the other
interesting point is that the breakdown
is half so if that verse see once wants
to change and you arbitrarily affect the
eventual subspace if we choose it needs
to change and for the points that
million-dollar who will perform well if
you only turn is ten percent of the
points to arbitrary apparently change at
the subspace you needs to change the
points so Walter I proof idea and and
the proof is is a bit blank thing but
there are several key points so one key
points is the working of dimensionality
so so high dimensions instead of you
think about the bad thing it's really a
good thing the reason that it's a good
thing is that well we know how to this
is really weird so we don't have to have
to do how to do lo the flour grabber and
some situation like water when you have
many samples but here we have many
dimensions and we can do the same thing
with in same tricks also when we have
many many dimension another exactly what
we do here so we we use the number of
dimensions to get rid to sort of get rid
of the noise but that's the first idea
which is uh which is kind of interesting
and really the key point is an empirical
variance estimates a good fit for how
dimensional regime the second point is a
random removal point so so here is a I
mean we want to be in a win-win
situation so when we throw away points
at a point the data point well either it
set an outlier and that'll be happy
because of getting less outlier
all have to throw in a famine point but
then we happy because the author is
liberal guy of 20 points so we want to
be in a situation where the random
removal is if it's we throw it out or
we're happy if it's lower we don't run
out I will also have it because we have
a good solution and therefore a time
iteration and alliteration the algorithm
is going to find a good solution and
eventually we want to show that the
robot for his estimate does not screw us
too much it's going to stress a little
bit there's going to be the first term
here but it's not gonna stress by fight
so much by by under estimating the
violence okay so I'm not going to get
into the deals too much and I just want
to show you one thing which is very
useful here and after that the theorem
here and this is really the essence of
of the first step and that's something I
think it's worthwhile knowing so if you
have a general Aleutians function and
with respect to if you clear you know
then you have a very general very strong
concentration equality so every for
every function approval this is going to
deviate much from its I mean is upper
bound by by this term and this is a this
is dimension free so really this is
where the nationality picks in so when
we look at the covariance is going to be
the that log if they're going to
consider then the covert of the low
dimensional projection is going to be
very close to what it should be and in
order to do that we look at the
dimensionality
not the number of points so you have a
very high dimensionality and you sum up
things across high dimension and then
you can use those kind of results here
and as you
see you look for dimension free results
so basically that's a won't you save
save the day shall I add random removal
like a good thing so here's a certain as
simple symbols use maybe simplistic view
things well what can happen let's say
that a good event in any event where a
device of the offending points is upper
is that upper bound the virus of the
corrupted phones and here is some factor
that is one over case bigger than one so
so this is in terms of the finance this
is the virus of the pending points is
the virus of the corrupted point and
basically we say that the virus upon the
direction of the PC we want it to be
mostly due to the authentic samples so
if that's happened then then we're fine
I mean Kurata
the outliers did not the doctor just too
much so in every step then we do it
around them removal well two things can
happen
other the step is not good so basically
some corrupted points contribute a lot
to the virus and then in that case we
are we are going to eliminate points in
probability proportional to them to the
to the vines so with the probability
that this is a detailer we're going to
have minus out levels probability cut
over 100 from cap so there's a small
amount away from 0 probability to get
one hour or less so what what can happen
I don't we get 1 hour 1 hour or less and
then in the next step about you have one
less outlier to worry about or that we
are going to have on that we have a good
event meaning that most of the virus is
due to their authentic points and so
therefore we are just getting either and
are we happy or
and the output of the algorithm that
basically we will find at some iteration
and good so and I also I mean there are
ve said that without vs. major we need
to control for it and that's something
we need to worry about there is one
slight complication and that's a good
defense so we need you to know that
there is a good event with high
probability and that so then either said
either the expected number for at that
point if eventually it goes to zero or
that that we have they're essentially
indistinguishable from from the
authentic points and we can do that with
basically just looking at a number of
points of 2048 survived okay and then
I'm eventually in control further and so
I'll skip that so one thing that you can
ask is well in the second slider so it's
totally about caramel caramelizing so
what happens with criminalisation and
well you know we can do clear on pca and
everything that i mentioned so far i
will still work out fine so PCA which is
the source of pi - not everything that
would over time you can still compute
with pca you can do computer science
estimator and basically you can you know
I can give this talk again by just
talking about Karen PCA and so gets not
very surprised
okay so I want to conclude so so I mean
this thought that is fine research is
its first step to handling the case of
where observation and dimensions are
essentially the same and you have a lot
of outliers
that's a set up that at least in proms
I'm I've been talking work on this quite
is quite important and the key idea is
that you project to lock a nutshell
space there are some robust statistics
are very verified if it looks right
you're happy if it doesn't look right to
discard the points that do not match
what you think should be so the nice
models can be very general can be
example option local cave
it works very well in simulations and
and we had something which is pretty
close to the care of that we want to you
can generalize it to two other problems
that are graphical structures and you
have a probabilistic model and you want
to say well I have a my data German by
some some generative probabilistic model
and some outliers and I want to sort of
figure out what's going on so the
outliers are going to either either they
look the same order to look for a
different and you can do exactly the
same trick for for temporal for in this
case was written Omicron Bayes nets and
you can extend it we've been working on
extending it stochastic programming
which is a very second slide I will
start a sample data and do something
which is lower act based approach so so
this is sort of as far as I know this is
a new regime and to work on and sushi
Mesa is interesting because from a
statistical perspective the first point
thinks it's very interesting act as
allies and second point things makes it
hard so first point is solved by the
blessing of dimensionality the second
point is this is all really by having
gap I hide the bench I'm having on this
budget for ballistic rejection procedure
which is essential I mean to include you
can ask well why do you need to throw
things probabilistic
can't you just you know do robots -
estimate just always for always
high-step high-step I then well that's
the answer to that is twofold
but first you probably will probably
work what you're suggesting but if you
want to prove things I don't think it's
worse you really need to have this
condition
but what's crucial in the proof that you
need to have a good defense you need to
either get rid of all the iron or a
printer fold after that you can
distinguish and if you do it
automatically
then this should work in the English
simulation it works better at the same
well it's a mutual is very is such out
fires until it becomes very hard so
usually the way it works today very easy
to take the first view and then like
you're saying well nobody is going to
distinguish and I'm all set
so that's got that pollute thing next is
more details and if you want to I did
get into the middle of the proof I think
it's a introduced we talked about I want
to show that the crowd he would please
when dimension is high and the number of
observation is small so observations are
very expensive in a sense right have you
considered that maybe you know the whole
observation is contaminated so part of
it
or like one variable out of a thousand
is contaminated and the rest is not if
not them then you wouldn't want to
remove the whole observation just say
what you have there is no the one answer
is yes but it's hard and there isn't
it's harder that you don't really know
how to complete so suppose it I I do
just appreciate and tell you no sample
number fourteen feature number 72 screw
it somehow there is I mean what I'm
going to do you needs time are completed
so unless you do some self rinse you
have some self reinforcing your
procedure like bootstrapping this is I
mean I don't know how to do it in in in
the basic PCA with the exception of
as it's on its own issue so it's a
really I mean I see I understand where
you're coming from it's obviously very
important I don't know how to handle it
in any way better than booth starving if
you have to love dimensional mall you
know to come late of course and that's
would be what's wrong Oh they've been
forcing my unbelief yeah I was trying to
handle it as a missing data problem like
once you identify that with the missing
value they contaminated well you you
just remove it forget about it and then
you have incomplete data and try to deal
with right but even that is yeah yes and
it's very well almost clearly something
you know okay so in principle you can
you can take out all the points so we
take them one by one from you know if
you have an end points you throw
everything out eventually but you
guarantee that along the way one of the
solute I mean you will have a good
solution and if you have a certificate
to which solution is good at the
solution with a maximal variance okay so
so in principle you don't really need to
know the structure if you know either
you can stop earlier because at some
point you're not going to get anything
which are throwing away authentic points
and we're doing VCA that's right it was
the best Easter</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>