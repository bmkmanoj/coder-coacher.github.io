<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Building Large Systems at Google | Coder Coacher - Coaching Coders</title><meta content="Building Large Systems at Google - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Building Large Systems at Google</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/FvdyO1Xo2mg" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay good evening and thank you very
much to everyone for coming this evening
I feel well so i'm an engineering
director with google here in kirkland
and it's my pleasure tonight to
introduce sheba because to be talking in
this evening before we do I have
something to announce from somebody else
to make firstly the restrooms are in the
hall outside secondly we're going to be
videotaping this and putting it up on
google video which is one of several
great products written here in kirkland
so this will be on google video just the
presenters not the audience so if you
don't want your players to do it
whenever people know you don't worry
about that we encourage you to hang
around after the presentations over and
continue the discussion and myself
achievement a bunch of the other
engineers from google will be here but
in Lansing with three future events
coming up on jun 7th gen Fitzpatrick is
talking about user experience and Google
on jun 21st allen's furnace is talking
about a study of all Curt and Anjou 28th
winter and security of google so I'd
encourage you both to register for these
and also please forward to the invites
to go for this event to any friends or
coworkers you think might be interested
in attending any of those events so
that's it for the announcement the talk
tonight building our systems at Google
as it goes without saying really the
Google deals with huge amounts of data
and a huge number of users and so she
was going to talk tonight about
basically how this all works and how we
probably do what we do and how we can
scale Shiva is a jury director and the
Google distinguish on
erich arup on Google found in this
office here and has been directed at
that I've been here and he's be
responsible my name Google's advertising
products and the people search appliance
proud to work at Google he cooked down
to kick a beat startup which was
purchased by Napster here's a bachelor's
from UCLA and a PhD in computer sciences
happen so please join me in welcoming
she go thanks Peter so before I get
started I want a quick show of hands how
many of you have seen a taught by a
Google engineer the last five months
okay that's great it's not too many
because we do interview some slides and
dimension 117 slides so I'm going to
talk about billing not systems at Google
so it's really three different
components we have a bunch of Google
products that you've seen it's in search
in advertising in running email the link
talk about your other things and
typically all these products they tend
to be on top of a very large stack of
distributed systems and some
infrastructure via build up over time as
a bottom of the stack we have a
computing platform which is a whole
bunch of machines in a bunch of
different data centers which have been
designed and built up over time to work
with all these are involved
more about business and by the way if
you have any questions just stop
interrupting because I know what my
slides say I'll get a new board I just
talked to my slides to stop and talking
of a have your problem take the mission
that Google has had for the last seven
years is to organize the world's
information and make it universally
accessible as photostream make it useful
one question the people typically ask is
how much information is there the answer
is a lot so if we just look at just the
web and how it's been growing or the
last few years anyway when I started to
Google or Tigers back we had close to
maybe a billion documents and over the
last year it's going to be a lot larger
than that and if you just take away your
what there's even a lot more than that
in terms of your personal email your
files and databases and we've been
expanding to other areas too in terms of
video in terms of radio print etc this
one so in terms of products you've seen
a bunch of these things initially these
come in a few years done with the web
and our Charter was to say let's not do
anything but the web wait so let us
initial Charter and then over time we
kept expanding into mourning or media
formats in terms of images video and
into enterprises into scholarly
publication maps and recently the areas
like Chad and emboss already is from so
this is a list of all the Google
products that have been built up over
time if you just look at the list of
products you've launched from this
office right here last year and it's
quite a bit so there is a mix of
products in a bunch of different areas
there is a lot of work going on in
search in crawling and indexing so one
of the guys we've launched in the last
year is google sitemaps which is a
mechanism
which webmasters tell us what they have
on their site as well as get information
from us as to what we believe we know
about their site then there is a bunch
of client software for example is google
gadgets they launched about a couple of
weeks back and this is my google desktop
search who would pack is another one
I've launched about three months time
there's a lot of work going on here in
terms of google maps for example love
the rendering for the geographical
information of maps as well as for
specific countries is handing out of
your google video be they declined and
mostly the the front end was better
again we're trying to get more of those
areas talk google chat and a variety of
other things have been primarily built
from this office it is the past you have
so everything one of these parts that we
have is hard because there are more and
more users using these different pieces
of of applications and every one of them
has more data and everyone requires more
results better results your time so for
example if you pick any of our products
if you want to make sure that by endless
analyzing what users are doing we want
to be able to give them better is also
so every single one of these products
keeps expanding in terms of the amount a
of the process as well as the Hubble
users who actually one uses cesario so
the overall goal that we have for
systems infrastructure that is neither
the pieces of software of the stack that
is necessary to support all these
different products is on every few
months every day we want to keep
building out large scale as those higher
performance infrastructure support these
applications because everything is
growing and also we want to make sure
that they don't go bankrupt so we focus
on making sure that the price book per
performance is actually is getting
better over time and these abuse as well
we do not want to have every single
after
build up entire parcel stack so we want
to make sure the venture security bill
is scaling as well as supporting more
users and it's easy for the the folks
within the company to deploy more and
more provide for time so any questions
on fire if I can do into the episode
systems infrastructure ok so I'm going
to talk about three different parts over
here so just a reminder of this so
Google products and systems which are
going to talk about the hardware
computing platform later but in terms of
the instruction we had I'm going to talk
about Google file system which is a
mechanism it is really a large
distributed file system in which we
throw in lots amounts of data and then
we have a couple of computational models
as storage models already talking about
the song so Jean offense is is yet
another distributed file system crap let
me build up in the last few years so the
question is why do we want to build
another file system as opposed to be
using some existing file systems as it
turns out we have a lot of flexibility
in what we do we control our
applications we can throw an operating
system as well as the machines on which
we on these things so based on this
observation we looked at a whole class
of applications and realize that certain
of these things have large or rewrite
pilot as well as we need to have
reliability over many many missions and
you want to make sure that it works
across multiple data centers and also
the the actual units of files are
actually pretty large inside this one so
when you have a bunch of applications
that can store a huge amounts of
across lots of machines as well as in
most areas so that's written by example
in Google file system so the way in
which it kind of works is there's a
google there's a gfs master and the
master is what is what I a single
application doctor and the master
controls a whole bunch of these chunk
servers so chunk servers are essentially
configure them as as as machines which
have certain fractions of content that
is localized to that particular machine
and what the master does is it takes any
father we want to store it let's say you
wanna store it ever information takes a
file and breaks it up into chunks of
data and says a you machine number 55
you will be responsible for storing this
particular sub shop and the other
component the master is make sure that
it's replicated across multiple machines
so if a machine goes down and we know
the machines will go down then at least
the application can survive by talking
to another country machines very easily
so the master is actually manages the
the chunk 2 to the location mapping and
make sure that the the online systems
are scaling in terms of low dancing as
well as redirecting traffic to the
machine there are pleased loaded also
where machine is finishing goes down
then the other chunks also react to that
by by transferring data automatically
between the peerage himself
so in terms of the the gfs use in the
Google we have a lot of these gfs
servers and we have all these clusters
and they scared a very large amounts of
data so typically the clusters are have
a browser machine a few thousand
machines and the number of clients that
they talk to the master a pretty large
Felina growing with Armstrong and it can
be pretty impressed with the amount of
rewrite time to go through these systems
and every one of these things is is it
is part of our of say the anti-point a
search five point or any other problem
that we talked about in terms of being
the cord storage coupon which should be
stored all out here yeah you do one last
effort cluster so the question is is
there one master professor so it depends
so in in a bunch of cases you want to
make sure that there are a bunch of
multiple masters so that if a master
actually goes down you could reliably
hand it off to another mattress one but
you want to make sure that that there
are multiple masters that are really
massive the same time right because you
all want to have them controlling which
chunks are going to which machine and
making different kinds of decision so
you can imagine living a leader election
that decides which of all these
thousands of machines which ones do I
want to become the master urban league
there will be a master hand of crosses
on some of these things can be automated
some of these things are are no manual
this fall so for example machine goes on
it could be that you may have to restart
a buncha
Master machines so that they do take
over for these the actual task but you
also want to make sure them the Machine
goes down there's enough persistent
storage so that another magic comes
along and picks it up it would have
complete state and you do some things
that again further multiple businesses
at theaters are multiple instance of gfs
yes there are lots of gfs cells and
different applications could run
different gfs cells and there is
actually org work where there are
standard gfs locations where people
actually can store data into as one so
depending on the application you can
either choose to use an existing gfs
custard or makeup you and Jacob
structures form if you are the master
self-managing or you have another master
which man is the master so the question
is are there other or the Masters
self-managing the answer is you would
like them to be right and in the case of
there not because it could be bugs or if
it is something as a min build-up you
want to make sure that you have enough
why a reliability in terms of proving a
master from one set of machines to
another machine
so the question is what is the leader
election algorithm and there is a the
answer that I don't know too that
against further question but there are
lots of different options and there are
shoes on whether you want to have bigger
relation in the context of one data
center or multiple data centers and
depending on how much network punish me
expect to happen you may choose one
misses yeah and also you gotta really
look at how often do fishings go down
and a half into machines to the match
that I should go down is arguably would
actually say that the master could be in
a much more reliable machine than other
missions these options that that are
they are made on a free daily basis but
in theory adding between in the cluster
could become a master or always selected
machines are eligible so the question
was in any machine become a master again
it depends on on the application there
are certain classes of machines which
have a slightly more reliable so it
depends on on again the precise context
you may be willing to lose some data in
some cases again depending on how much
money you want to spend versus how much
reliability we want that's why looking
at the price book performance number on
application basis is very important for
example we don't want to lose any logs
data right in that case you may be
willing to msn a lot more money you have
much better master nationalities are
much better machines in those different
situation on the other hand it's my
personal
gfs cluster jnanam he respond but again
the application can make its own trade
off but you want to have canal libraries
option allottees and building blocks so
that when an application chooses to do
one misses the other they have em
flexibility it puts the maximum rate
what is the maximum range of chunks
hours per masterful the answers I don't
know so it depends again right so you
can actually look at the number of two
let's say that if a typical chunk size
is around 64 megabytes but that's Nomar
suggested number but you could arguably
had chunker's and add that process
chance there are larger than that so
anything the question to answer is how
much is a storage of mean I think a more
interesting question to ask is what is
the storage of a given a thousand
machines how much can you store how much
alive will begin again and how much
shall be a mite through together and
that depends again on how you want to
configure there will be some default
configuration but these are things that
you like to change this one is a guy
make sense yeah again the key is to make
sure that we have enough infrastructure
so that people can make different
choices and as an M flexibility and you
have to have the right bearing blocks on
the way so people can choose right
numbers so I'm going to move from gfs to
a a couple of other systems on top of
you so gfs has been around for a while
and it's a it's a very liable stored in
this anecdotes that we've lost as in 64
kilobytes or 64 megabytes of data ever
since gfs was deployed so now that
that's in line with storage mechanism we
want to start processing data and doing
interesting things with it so why are
they the the example applications which
runs on top of gfs and processes huge
amounts of data and does interesting
things with it is not an option I'll be
talking about big day goes well I gave
us a is more of a lookup service cuz in
a log structure let me store all my dear
or I'll get
those lights so mad production is is
it's a way by which you can have a lot
of very complicated aggregation kinds of
complication happening on large amounts
of data so imagine that we have a whole
bunch of web pages and what we would
like to do is we would like to count up
the number of words that occurred across
all the different web pages so what the
mad face would do is it would try to
gain take all the records so in this
case LD the pages will be imagining them
to be stolen gfs so what do you do is
you take every single page stream it
through by production and the mapping
face would would essentially yield key
value pairs of the form word to the URL
of the picture so it is actually it
translates or maps from one server one
view of the data to another view of the
data based on what you would like to
aggregate them so in this example the
the mapping pays for doing all the web
pages and create a key value pair of
keeping the word the valuing Purls great
production what it does is it takes a
whole bunch of Records which are of the
same key type and I'm trying to advocate
them by doing a bunch of things and then
producing the final word so everyone who
uses this model of map reduction would
write essentially to face everyone else
and people in the company who wanna
process a large amount of data up saying
a terabyte of data on a thousand
machines would take the radar and what
would make the mapping steps and
deduction steps and would run this on a
larger of machines and the unlined
library which is the map reduction
library or take your investigators for
example how do you how do you move kadar
from one machine
are you make sure that when a machine
actually dies but it will automatically
and either a the intermediate results
aren't even lost well actually what
non-production does so here's a
pictorial yoga so again you see that the
the central component of yours is gfs
forgiveness it so there has lots and
lots of data and all the boxes in there
are the the chunks over every one of the
chunks of Earth has say a bunch of as a
partition of the file that it would like
to do a beam at reduced row as you have
the application design one to analyze a
greater so what the what the application
would do is it'll take all the data and
then find the mapping step will produce
the whole bunch is key and value doubles
and then we have a shuffling step which
essentially aggregates all values of the
same in kita so over here we have that
all the K ones which aren't different
machines which are all getting there
around from gfs probably shuffle we sent
across to the reduction step which will
likely be another machine and then the
Russian step would contaminate all the
values within that he'll with Nikita and
finally after that reduction step is
done which could be for example a sum or
a filter or whatever you want to do then
will be run back into gfs for additional
processing so that's a high-level
overview of how map reduction works any
questions ahead so far okay
so why do people use my productions one
is it actually works across whole bunch
of machines it's an interesting
computational model it's a nice way to
partition any given task into a whole
bunch of these map and reduction steps
but can be sent across lots of machines
machines can die and the the system
which is specular automatically and it's
scalable you can just keep adding more
and more machine and also it's a nice
competition model than that it was
across a whole bunch of different
application it works across it works
across search workers are as it works
across video any product that you can
think of as some component that has some
point in its in its evolution whole
bunch of a productions or analysis that
is going on to make sure that we
understand what use is annoying or to
compute to be computer much of data that
will be useful back to the user
okay gonna switch to big tables any
questions about gfs or non-production so
far could you give a few more examples
of using MapReduce to solve particular
problems presumably you find that you
can turn a whole bunch of things into
MapReduce that you win the fudge could
right so the question was can I give a
few more examples of my productions
there are a bunch of applications one of
the simplest ones is safe a gram so what
you would like to do is let's say you
want a trauma units I'll grab you want
to take a whole bunch of data and then
you want to do a word count for that so
imagine a file in a whole bunch of
clients and you want to do a graph to
convey a whole bunch of keys and I'm
touching to you and you work on under
that so what you do in that case is you
would make each of the the files as
files in gfs and then the mapping step
will take the files and will map it into
say for example line numbers and then
the counting step would actually be done
reduction step for another example would
be sorted right so it takes something as
simple as Sodom and say that I want to
thought a terabyte of Vega so what I
would like to do is I want to aggregate
essentially all the all the the records
for the same key into the same machine
and let me do a aggregation on top of
that again that will be a natural thing
for a production and higher p is a prime
specific level we can think of the think
of an example may be calling pipeline
for indexing pipeline they are actually
there are a bunch of maybe 20 different
map reductions that run the one in every
single part of this pipeline that they
do various different things again it's a
matter of looking at data with a whole
bunch of Records and again
the keys and maybe counting it up
filtering it out and then you went back
to gfs that you have a second map
reduction will come along but again what
on this on top of Styria and then go
along in every state for the pipeline
macho is very helpful around it yeah so
much reduced acknowledges you describe
the son's life interesting way to
pacific computation the person i have is
having found any problems that are
marking fish was nothing now that you
need some different way of disability
reputation so the question is not
production enough map reduction and
solve a large class of problems very
nicely but there are cons UK svet is in
to the right things so what we do is we
every few months we have a new set of
problems it come along and people go and
work on building new computational
models on top of on da gfs and it could
not decide be not reductions so we
haven't talked about a bunch of things
that we done in-house there are now two
actions but it's a nice community to do
bunch of things very easily especially
if you've done any simple things like
the accountant or phil gramm and you
have too much that fits on it doesn't
fit into memory
reductions but there are other things
where you don't actually do better'n
counting of thresholding in which cases
we want to do something else but this
does not attend to be the ultimate panel
programming language is that is this
wrong but it works well across coffee
machines or segments with us
so we talk about gfs which is our final
story it's a log structured file system
we talk about you can store whatever one
stone to that map reduction is a
computation on top of gfs so another
thing that we have that works on gfs is
big aver so big table is is you can
think of it as a as a look of mechanism
so we have a lot of structured and
semester here at Google and what you
would like to do is you want to go and
quickly query what are different
attributes or for a different values
that responded given key pretty fast I
supposed to say screaming through gfs
files and and doing our grip on top of
that so it's a lookup table and
traditional people have thought
databases that fun so the problem we
have is that the scale is very large we
have lots of URLs and lots of not so
popular of example is look at maps which
is something we launched about a year
back and that has much of data and you
go on you want you want to have a
storage back wit e look up given a key
what the value of
people and the bottom is up and you
can't just throw it into and to pick
your favorite commercial areas because
the scale doesn't quite work it's it's
way too much we have right and in the
past few years to use commercial
solutions but then I work partly because
the too expensive or the gum have been
up reliability or they don't work across
a thousand machines or and pick your
areas of your favorites are the problems
and also the other key is on by
controlling the low-level storage you
can do a bunch of optimizations so that
you can control how fast something would
work with a certain kind of commutation
so that's why we sounded better in
BigTable or some humans here we go
nonsense just founded by large systems
so we do that so the key features is
that it's a map so just said given key
you want to get back a value as opposed
to gfs which is where you store all your
data so imagine how do you better it is
to give your hash map or should just map
and you want to scare picked off the
machines and you want to be fast so you
wanted to be as much often in memory as
possible and also it has to support lots
of reeds and lots and lots of guides and
also support issues so please all the
kinds of problems that a given solves
and also it's reliable you can take out
a bunch of machines you can add in our
country machines and the whole system is
magically works in by doing go dancing
by moving gear
by just reconfiguring itself so that you
can just keep adding more machines while
in the system serving and it doesn't
lose their very info so in terms of
billing yes the previous page said it
was a mapping key to that is it also
sorted by key the question is is it also
sorry Becky it's actually surprisingly
an intimidation be here as it turns on
right now it is there is a bunch of
sorting that's going on in the head
kiala so scanning is pretty efficient so
you want to give a range it says only
get back all the tuples of this within
one range to another then it would
support that focus if they haven't sort
of said oh that's going to be true a
year from convey via but I suspect it
will be because scans are pre important
to to do is fun so yes he talked a
little bit about whether 2010 two gfs a
big table wind up doing the same thing
for example coping with machine failures
or despicable rely of gfs for everything
like that their movies so in question is
how do a big table and gfs comparing
them the machine failures so gfs is a
storage fun and this actually is a dozen
I segue into my next slide it talks
about building blocks and one of the
things that I believe it does use is gfs
so all the storage happen in gfs and
there's an instruction on top done and
there are the components that are very
relevant as well like there's a few
blocks servers it works across multiple
machines and there is a scheduler that
make sure that machines can go pretty
well but I think gfs is very reliable on
song big table what does really well is
it's nicely into which you can make it
in store dare modify your data and keep
growing the velocity and do lookups
which is not something that is
so these are two different layers but I
think you might have some my question I
was it then I asked you to compare the
API for programmers the gfs vs big table
what sorts of operations and retina fix
it with our gfs yeah Jesus is very low
so you want to store a bunch of data
into it and what supports is it lets you
store huge amounts of data supports very
good read write bandwidth as long as it
scanning rather than 30 lookups and big
day was more about multiplying data and
wanting to store lots of data and adding
more machines as you would need to and
it all just magically works good so
again so getting that another class
every single component be good why
should we use some component that you
only built in the past or in the cases
when it doesn't work then we go and
reaper that particular moment so for
example we needed to have a lock service
and actually work reliably it caused
lots and lots of machines we felt that
and map reduction was just the
computation on top this clearly because
such a short piece of infrastructure we
certainly on a nature than any new
system rebuild good work not production
is a strong and also one at the same
point reusing aspect of gfs and
schedulers as possible so we don't go in
and implement every single party will
start every single running around
comes along so the question is are we
done so somebody asks as a little while
back we aren't even close to being done
because every few months the scale just
keeps increasing dramatically and
whatever we heard before doesn't
necessarily work yet because we do think
about what is going to happen a year
from now or two years from now whatever
we build system subban scale
unfortunately that unfortunately what
happens is the number of users keeps
going which is nice and a monitor just
keeps growing so we have to go on and
make sure that every single component
you have the stack will have to be read
on the scale it will extend so every few
months we have a new computational model
comes along or we have a new bunch of
weeks I need to be made with our storage
engine or for the way in which we do
have our pcs or between which we copy
data across from one machine to whole
bunch of machines so all this is is is
is ongoing and every so often every six
months engine comes along the prior area
that says that what if we do LA and the
blah will lead to either say hunt full
production in latency or doing something
or will lead to production in calculus
furniture because you can use any one
tenth of as many machines as we did in
the past and you see the vid some
tangible benefits because it's scaling
if it all scales so you have very good
darin algorithms to figure out that he
do X or Y or if he introduced a new
computational model or we change as part
of the staff then all these other things
will be impacted along the way is wrong
so the goal really is done is what I
said earlier the goes always create
something that's larger than it actually
is higher performance and make sure it
works in
in more ways so for example one prompt
so I'm dissolution of a bunch of
problems that are interesting open areas
that are always looking at so
replication and low latency access from
across the word that's a hard drone how
do you get a whole bunch of machines and
a bunch of data centers across the world
to reflect the latest version of what
use it it it's very hard so we keep
working on those kind of problems and
then what happens if a bunch of data
centers piranha what happens for
punching machines go down there is a
bunch of work that we have done so far
to make sure we cope with many of these
things but as the case keeps changing we
keep solving more of these kind of
problems or even something like resource
parents how do you have let's say you
have a lot of machines how do you want
and how you make sure that these
machines which are doing perhaps
multiple tasks how dare you be so sure
how do you make sure that serving which
is let's say that you're doing serving
on on such traffic that's clearly more
important than your ex compassion is
running on a few machines and copying
data crumbs so how do you make sure that
certain things are privatized higher
than others that's a no Conservation's
from leaky perform any questions before
I switch to Harvard yes
so the question is if you go and change
a fundamental library like mine
production or whatever how do you go and
change every other application that's
running out there so this gets him to
make a helpful will engineering works
affect more than I had planned to but
the nice part is that we don't have too
many legacy application that we need to
maintain or any few years so every time
the person who doesn't you but or does a
except a bunch of changes the
improvements are reflected in there are
applications the next time they build it
and push it as far as opposed to buying
about applied software is out of the
field of innovative the next target so
since everyone is building is billing
and pushing out by earnings they just
pick up changes underneath the basis
okay so you saying you don't you
implement going forward right so means
here so the problem is that any software
that is deployed out in the field it's
harder to change that's why you have
company that have problems with legacy
software in our case since we control
all the applications every so often
everyone better the next question of a
certain product is being push up for
example let's say the video Phantom is
pushing out a new version there after a
production then they put it and they
pick up the leaders of changes and the
bushra which means that the average time
the binary is out in the thousand
machines national very very low so
within the next like a few weeks it will
be X so it might be correct to say any
moment any machine when you have
multiple versions different versions
after just lightly there's a lot of
other things I haven't talked about that
I can't even talk back at this point
which are things we've done for what
time to make sure that version
versioning problems and their
consistency which is actually a whole
bunch of very hard problems I actually
being addressed under the coverage
somewhere so what happens if one server
pushes actually talks upon talks is
having kind of forgot and then you have
another server that is the point how
does that actually respond to that so a
bunch of challenges in that against
wrong any other questions ok so we've
talked about a steel products talked
about some of the the computer
internships that we have and down with
three piece of technology there are lots
of others and because they haven't
talked about yet but these are indicated
with the kinds of problems that we will
be like a salt but beneath that we have
a lot of other things that are going on
from a computing center so when you have
a lot of genes are you done them to be
cost efficient how do you maintain them
so that we have multiple datacenters
what is a bunch of these things die what
if the how much power you consume all
these are pretty interesting problems
should let me look at okay so from our
computing standpoint there are lots of
problems that are interesting so there
is this year on so design so let's
assume that we have this magical sort of
systems stacks at work on a budgeting
machines how do you design the actual
fungus of what about Richard what have
chips teaches how do you get the right
kind of desks to work for the setup and
power-efficient another important to our
graph this man is like networking is
hard because you have lots of these
machines again that need to talk to each
other you want to make sure there aren't
too many single points of failure or
they aren't let's say you have a bunch
of machine
you want to make sure that the switches
have enough bandwidth capacity to send
in our packets through so they are not
the bottling especially they are talking
about talking from one set of machines
or not sufficient so the classic
philosophy we've had is we want to build
a lot of these very low-end machines so
we saw a TV TV service so pick up if you
get some motherboards that are commodity
you pick up your fields are brown and
then this and play the bodies and the
assumption is I any company that sells
alto nivel harder they are necessarily
working that the quality of of failures
is perhaps lower but you seem to
compensate for the the types of either
they fail by running a lot of these
systems on talk to recover when machines
actually our desks actually done so the
observation was nice to just go for the
cheap things just build up a whole bunch
of interesting software on top of it so
that wherever they do done because we
know that they would like that you just
said record it in a nice seamless way
and and our products would just work
so a lot of you may have seen this
picture this was the initial google
fishing and you can see a bunch of they
go Sassy's over here which held all the
harder things evolve little bit since
then this was a 99 and again this is the
skids talk to the ultra cheap commodity
hardware that we rely on the use of my
own and in fact these were a fire hazard
to because he's worked out of our food
and they won't do it if you were cutting
be single you lines right a decent will
catch fire but they tow optimize your
pipe is having the corn hole with the
one over on top and then we this is just
hanging over when and then at some point
when the Santa Clara Fire Department
jected to this they moved to a more
traditional kind of data center board
which which actually had can see some
chassis use over here which encompass
the motherboard we the components so
yeah and the company handed have is a
model of of ten of these amazing every
single RSC ideas and just come alive
it's completely empty and then few days
later just it's all completely stock up
of machines and it's all connected up
and all just magically works it's kind
of cool so right now it's again in the
current design is slightly more
profession you can see there is like an
board cardboard over here and again it's
still a lot of these things are open
because of power consumption and coding
issues and again the same same circle
back
we have pc past other coats and we have
very low end ID eat hard drives and we
are next on these things and we put up
all these software systems on now these
things that do not know where the
machines are why later when they die
when they come back a lot and we
seamlessly move things around a tree
houses all kind of works so taking the
step back what is Google doing in the
hardware space s recently software
system space we are working we've been
working with chip manufacturers for a
while because the and someone we
realized the the price per performance
was actually getting better good time
over the last few generations and the
the prices of these things were actually
was getting variable x for us and you
get a cheaper components and was getting
his going the right direction but a
price low wattage has only one of our
does been consumed on power when a
performance basis was actually steady
and the problem is that if you want to
cool a regular motherboard with a bunch
of components it may take like 20 50
watts to to to power it up but then
cooling also takes another cojones pig
was and as the system give more and more
complicated you had these huge cooling
issues and empowered shoes and and I
think we do have a graph that show that
if we just keep going it on way then the
machines would not be the most
significant component of our advanced
but you know so we've been working chip
manufacturers for a while to make sure
that we have more power efficient chips
as well as controllers 30 important
devices and the data centers themselves
are very interesting so those
interesting is following
how much cooling data quiet and how much
how much time it takes to build up over
the whole bunch p data centers in song
so long work and is Judith a you use any
color of the solid anymore do your own
business so the question was do we use
co2 facilities in here ever do we have
our data center the answer is really a
combination about and thinking of what i
can say so at some point we had a
separate this being recorded that song
there's at some point we had a whole
bunch of days ideas are coming online
because people thought it was all money
to allah and over time are they went
back up because you couldn't find ways
in which they can monetize other
companies coming
they started evolving you do in demand
service providers and other wisdom or
less wrong so just to be safe I think
all companies it makes sense to have a
mix of oleo facilities if you'll
maintain as far as things that your own
car and then maintenance wrong oh my god
i found Robin exactly where we had
enough fishing fun so doing too much
things in the harbor area and working
with people who can supply us with with
things that will work well in our in our
context and also come as we talked about
we keep working on these these larger
and higher performance systems of myself
from a software send one to that every
three months we just change from out of
the stack and all the applications just
benefit from that and a choice so now
that we have a lot of machines and lots
of data and long time with that actually
you have between different machines and
different laterz interest what do you
and this is where we spend most of our
time you say wow we have all this here
was give me recompute more can be me
make query processing faster by doing a
bunch of user connects or can we offer a
new feature or product that actually
will be compelling to the user so it's
really a mix of inservice a full cycle
so we started parts and then the promise
work on talk of the bunch of interesting
systems tag and then you have the
hardware platform bonita and as the
tourists keep driving the improvements
the stack has less leave the computing
platform we can move less it only to go
to more more cool things from the
products and one so that's what we keep
going good so we had every single
servings we have we have a whole new
tradition keep coming up and in hardware
and networking systems algorithms
cramming languages statistics practice
on mechanical engineering and so it's
kind of a cool playing around because
what we just carry right combination of
hardware and software we can solve or
more comes with after you won't be feet
are you so this is the inner is like
that too just 222 de place in some
contexts in Jessamine the last year and
have just from this office right here we
launched our local maps and improv the
indexing and so on every single one of
these things uses every part of the
stack the gap in terms of
the Senate machines in terms of the
stories that we have in Jesus or the
analysis tool that we have from a
productions or the destroy the lookup
kind of service and pay cable offers
that's really what we try to do keep
betting more and more of these things
okay any more questions this is this is
the video upcoming talks in the next few
weeks any other questions about building
nonces in the Google yes someone who's
had a bit about resource allocating I
understand it if you've gotten an app
you can say hey you can only use 500
machines so at that level you can
constrain it but do different apps use
the same infrastructure say gfs or big
table or do they tend to use their own
copies in other words can you control
the interference between apps not only
machine a machine level when they use
some shared service or don't they so the
question is are these different pieces
of infrastructure they maintained by the
different themes on dating services and
any of their services you know right are
they services that multiple apps use or
busy chap habits on the copy of the
service so very different applications
have their own copy or they have just
one service a taken videos I think I
mean the answers really depends on the
on the three actual service some of
these are libraries so every application
can easily use whatever I functionality
to get out of that but things like
storage it again depends on our group or
per requiring basis you might imagine
that the ad system which is pretty darn
important to the company or the
statistic is very important company they
would not be very open to having my
favorite map reduction drawing
on every table right and that's because
it says it's scary but you can just
bring down all of that but I didn't when
I get there are a bunch of things that
aren't there are services for example
there's one crawler does it make sense
you have multiple colors so you have a
bunch of machine that this around and
will call a bunch of work because that's
an influence that we have overdressed
reliable so sundancer now some pieces of
infrastructure make sense as services
and certain just make sense to have your
own coffee tastes on on reliability or
the interference you concerned about but
so you've avoided situations in which an
app you an unimportant app can bog down
an important appt by interference I'm
shared service right so the question is
what about interference between one app
and another rap that's another open area
that we are excited by and I can't tell
you exactly where how much you honestly
in that area that it's an important area
the more we can reuse machines across
different services the better of phobia
p and it's a bunch of our problems so we
solve it in men and in ways at work for
now but may not necessarily work in
higher so which is why we keep doing
more of these things
you see this change
so the question was so far we have had a
lot of back in service interview control
and what happens if he saw shipping
software to a bunch of clients and how
does how do things with Oliver a lot of
the application your ship so far have a
very strong very light on on the end
users computer and they interact a lot
with the backend technologies that we
have so they thinking you should request
pack of us for storing some component or
may require some interaction that would
be necessary on all the users computer
and so Ollie the the clients we launched
our are light and thin on the user
facing site and have a lot of back in
interaction you got any on a string sir
so you didn't really say much about how
you manage software fault tolerance and
we're going to raise an inexpensive
figures all that there's a lot of
existing software out there to do that
sort of thing in our clusters and so
forth are you using some of the existing
software or doing something completely
proprietary or do you start with
something was out there and forth it
into something to write a career one so
the question was how much do we reuse
software that has been built up over
time forward reliability and scalability
pretty much all the things that we do
have them build up over time and only so
at some point we would have a few
solution that we wait right out and they
just didn't scale enough so in that that
situation if it's open doors you could
do a bunch of thank you that if it's not
any mayor is where we use components and
should realize that we the benefit of
having an existing stack is very
important so you gonna have to redo the
entire stack so if you have existing
fact you know that if you build some
component in top of it you could you say
some like a gfs or you could reuse some
lock scheduler or lock manager right and
blondie existing software is region
though they don't know us we have the
same kind of benefit and they have
really worked with the number of
machines over theirs and theirs or a
scale that we have until you point to
one like large scale system that works
across many problems of machines
reliable fault on way and processes so
much earlier lot of these sections area
there's not any good commercially
deployed solution that actually convert
G of the internal systems is shared
you build special software to leverage
technology
so the question was how do we make sure
that different offices share information
about how it build different
applications so we have a lot of body
centered at this point we have maybe 20
25 1660 okay I thought it's like running
your shape it's going everything that
it's very hard to keep targeting sense
so there's usually a bootstrapping
problem as an industry where when a team
starts in a new operation you have bunch
of learning that needs to happen how do
you use it three of these things that I
talked about and a lot and um knowledge
about how that actually happens but
there's no substitute for having done a
bunch of these things so typically what
happens is in any of the R&amp;amp;D centers
that we have there are some people moved
from say from mountain view or from from
here or view our cover ever to start
seeing at these places so there is some
initial knowledge to start groups with
and I know would not use have you have
enough expertise built up because you
know what are the leaders people are
going on a big table or violation going
on in systems that is rather than you
yeah curious the third the big table
indexes fan data center stuff so like
what are so the question is do the big
table self spanning multiple layers
interests I can't count on exactly how
it works cause multiple data centers but
it's a very hard problem and so that's
one of the things that will keep
changing as we keep going inside and you
can imagine how it works very hello
buddy has small amounts of data but if
you start thinking about a pedophile
trader thrown in your big table that has
to work across multiple datacenters and
it would have to be liable up to the
second in terms which are in terms of
coherency that's really hard probably
impossible in some cases so again kids
back to the list of applications and
perhaps can get away with copying some
data in a bag on process and mashes my
relationships these are high profit so
the application goes was a pick and
choose there is nothing that we have and
will build up things that I need to
build up into
you take two more questions yes you talk
a little bit QA what do you do to make
sure it's stable before it goes into
production okay the question is about
you and what do we do before in terms of
code going into production testing is a
huge component and engineers haves all
unit as regression tests and this gets a
typical engineering rising in the usual
price and as you might imagine every
company that was in the space has a has
a canary testing set up where they push
out a bunch of binary and white out and
see where actually works if it if it has
memory leaks if it doesn't do the right
things and you have lots of monitoring
button and once it actually kind of
works and you put it out to production
and there are glitches and this is
partly why it's interesting as well
because we are we are happier about
making quick progress by doing a bunch
of changes in pushing about in
production rather than waiting for a
month to push our changes because we as
long as we can fix it quickly enough air
in better shape actually used to be a
badge of honor people who brought down
google com and it's kind of nice to have
that magic
one last question yes yeah I just want
to get for redundancy like you know you
have any measures before the gaining
systems for actually not having a
permanent storage for your data I just
150m gfs some of the APIs classes go
down or something like over the other
chemical fantasy and you have a master's
in sulfide like you have a permanent
like a state story or something like
that to the question is about tape
storage mrs. disks so one of my spots
were jeff has as I said earlier is that
we had lost much theta from it at all
since the beginning of time for plastic
64 megabytes for your charm water that's
not too bad but as you might imagine
just for disaster recovery the company
has thrown through policies about what
they really want maintaining and what
they really wanna you want around and
what is the cost of that can I could be
done on an application basis but again
there the goal that we had is always to
build up the different components and
will work across a large you got your
applications and then the applications
running on how important that i can pick
and choose in terms of price in terms of
how expensive it is to maintain or you
won't thank you for coming and we hung
over here</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>