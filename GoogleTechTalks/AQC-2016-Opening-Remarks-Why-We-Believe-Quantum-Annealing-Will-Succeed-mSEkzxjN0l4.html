<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>AQC 2016 - Opening Remarks: Why We Believe Quantum Annealing Will Succeed | Coder Coacher - Coaching Coders</title><meta content="AQC 2016 - Opening Remarks: Why We Believe Quantum Annealing Will Succeed - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>AQC 2016 - Opening Remarks: Why We Believe Quantum Annealing Will Succeed</b></h2><h5 class="post__date">2016-10-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/mSEkzxjN0l4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good morning everybody let's start a
session this is a QC 2016 this is the
fifth of the series and we have 180
participants this time it's it's the
biggest one the field is expanding
exponentially so the first speaker will
be good morning everyone
on behalf of the Google team I would
like to welcome you all very happy and
glad you could make it and hopefully we
have a few interesting and fun days
together as mr. Morrison said we were
pretty astonished and I'm pleased to see
that there's quite a growth and maturity
coming to our field in the sense that we
had about a hundred submissions this
year and even though we went for the
first time for this conference
doubletrack we still had to reject about
half of the submitted talks and as
always there's a random element to it so
for those where we unfortunately had to
relegate it to the poster session please
don't be sad about it and inversely
please check out the poster session
because there's a lot of high quality
talks in there as well so getting to the
scientific part I wanted to kick off
this conference with some optimistic
notes and happened to busy the optimism
our team and I wanted to share with you
an argument why I think quantum
annealing is going to succeed and why I
believe in the next 1/2 years we are
going to gather conclusive evidence that
this is indeed the case and I have a
argument I want to walk his through
rather quickly so the the first point of
observation is this was our December
paper and all details will be presented
in the next talk by Vaziri densha
but essentially we saw that for a
crafted proof of concept problems that
had a very rugged energy landscape
characterized by tall and narrow energy
barriers quantum annealing did better
much better than its classical
counterpart thermal annealing and then
moreover since then we look closer and
found not surprisingly this is not a
rare phenomenon actually you can use a
tool I would like to refer you to the
talks by hammered cut scrubber and
jungjoo and team so Helmut developed a
measure we refer to as P of Q will not
explain what it is other than to say you
can use it as a litmus test to
characterise the landscape of an
optimization problem you're dealing with
and then if you find that in this
landscape in the ideal case can you have
tall and narrow barriers but not any
broad barriers then you find in
numerical simulation that simulated
quantum annealing in our case is
simulated by quantum Monte Carlo
succeeds much quicker than the classical
counterpart again but actually the
inverse is also true you can use this
litmus test to find many examples where
quantum annealing is not doing as well
as thermal annealing which leads to the
conclusion that what you really want to
do is you don't want to have quantum
annealing fight on its own the portfolio
of best classical solvers what you need
to do you need to combine the best of
both of thermal transitions as well as
tunneling transitions and we have
recently developed an algorithm because
the quantum parallel tempering algorithm
which achieves this and let me quickly
explain to your houses works I'm sure
most of you are familiar with
traditional classical parallel tempering
very similar to similar
to the kneeling the difference is that
you don't have one temperature
parameters that you die down from high
to low but rather you produce replicas
of your system and they live at
different temperatures each one follows
the metropolis dynamics and then you
have metropolis swaps between those
replicas and this elicits this diffusion
process where eventually you have the
best solutions winding up at sea cold
temperatures but by what I said earlier
you should expect that if you have a
large optimization problem you will have
all kinds of barriers you will have so
tall and narrow kind and broad barriers
so what you should expect that if you
just run it classically then there will
be groups of spins locked up behind tall
and hopefully narrow barriers and in
that case you would benefit from just
extending the scheme and running
annealing a little bit different than we
normally do it you would have again at
this classical column but then every
once in a while or after every outer
loop the quantum annealer comes in grabs
a replica initializes the annealing
process at a classical state then you
dial up the transversal field to a
certain gamma max you don't know exactly
necessarily what is this and dial it
back down take that replica and again if
there was group of spins locked up
behind it hall barriers there's some
probability you will find such pins on
the other side and you would readmit
this state back into the pool of
replicas again by metropolis rule so we
think that in this combination you get
the best of both worlds and we think or
expect that the question you should ask
now how would we know that such a scheme
has a chance of beating the best non
quantum enhanced optimization algorithms
and there's none list of reasons
actually why we think that's to be the
case for example again look at talks by
him
you will find that the classical
parallel tempering scheme alone it's
highly competitive with the winners of
the set competition for example there's
a paper out in the archives that shows
that it beats CC LS 2015 which was last
year's winner of the set competition we
also know is that if the connectivity
graph is sufficiently dense then methods
based on cluster finding of methods like
the HFS algorithm that exploits the
existence of sub graphs that are nearly
tree-structured those algorithms are
going to fail going to cease to be
effective and I would check out those
talks so therefore if we can I would
like to make the following prediction
that if you integrate quantum annealer
x' that feature sufficiently dense
connectivity graph that's important into
an parallel tempering scheme then you
can attain speed ups that are
practically relevant and if you're from
a Google perspective that's how we
envision eventually an optimization
service to work like I mean this what we
would like to offer our customers
eventually said Forks internal or
external 1/2 optimization problems they
send instances to a service and they're
sort of the service will return good
solutions to their optimization problem
and the nice thing about the scheme is
you can build an asynchronous service
architecture where you essentially have
a big server in the middle that hosts
the replicas at different temperatures
and you have running sort of the
baseline algorithm ok local stochastic
search realized by metropolis updates so
you have this replica is living there
different temperatures going through
there metropolis dynamics that's the
heart of the algorithm but then you have
Google we would call it a worker pool
you have sort of specialists who come in
and say oh I have time run
give me one of your replicas let me see
whether I can help you with an update
and there again if those problems live
on sufficiently dense graphs we will
find that the quantum annealer eliciting
tunneling transitions as described
before will will be able we expect that
to be able to give you good updates that
you would readmit back to the replica
pool and that this particular move we're
not using any classical technique would
take much longer time but in this scheme
see in the past we have always done this
bag of quantum annealing against
everything here it's more like a judo
move we say we understand much better
now what quantum annealing is good at
and let do the quantum resources do what
they're best at
again that is dealing with rugged energy
landscapes where you on a dense graph
where you try to explore the length
energy landscape and find a lower
minimum but you can mix and match this
with any type of classical update you
like if your problem admits for cluster
updates
use those if you see there are sub
graphs which you can update quickly by
classical methods Doozers so essentially
in this scheme anything you can do
classically to advantage you can use it
and the nice thing is also because the
the core of it's a replica server as a
classical object you can instrument it
and take measurements there and you can
for example look at replicas overlaps
and to be explained by her mode you can
look at time correlation functions and
this information can help you to guide
your the flow of replicas through the
system and you can even get rather fancy
putting machine learning on top to see
that sort of the flow of replicas is
optimal as sufficient as that kind of
service we hope to do sort of diligent
benchmarking work and shows that in such
an architecture having quantum annealing
there will be a valuable member of this
architecture helping with certain types
of updates
that in the let's say 500 nanoseconds or
1 microsecond said kneeling run would
take that you wouldn't find any
classical schemes that could do a
competitive move in that short time ok
so that is the reason why one browser
optimistic and now you may say okay fine
fine have introduced this architecture
but when you are faced with a true
np-hard problem you haven't changed the
complexity is still looks like
exponential time complexity or inversely
you might just be able to lower the
residual energy by a few percent so not
complexity series may say yeah not much
achieved but from a more economic
perspective I would say a lot is
achieved because even a few percent
improvement along important product
dimensions is very important and why is
this because the Internet has brought us
markets with near-perfect transparency
so this leads to this winner takes most
phenomenon that if you have a product
that's just slightly better than the
next product people will buy your
product not the next one over and
therefore we should realize that every
computation happens in a context and
that context may what starts out as an
initial small game could exponentially
amplify it so we should not dismiss the
ability to get single bit more as
interest rates you know rather than sort
of what has been gained in absolute
singer if you have increased interest
rate a little bit and then sing of
what's that going to do over a few years
and that may be somewhat odd remark but
this very phenomenon makes me think that
funds is the field of quantum biology
quite interesting and I think we will
see more Quentin biological effects for
that very reason if there's a bunch of
cats out there and we talked about the
dangerous road if one of them has a
little bit better or some of them have a
little bit better car detector that's
the only time
yep you will see around a few
generations later so I think it's
important to keep that phenomenon in
mind
so as this concludes my upbeat
assessment of why I think that quantum
annealing will be very worthwhile to
invest into I want to finish with one
last shameless plug and that is we have
taken the liberty to interpret the theme
of this conference a little bit broader
in the sense that quantum annealing is
really is a prototypical and in some
ways the first pre error corrected
quantum algorithm but there's more now
and in particular we are quite excited
by the possibility what you can do with
shallow quantum circuits and what pre
error corrected quantum algorithms you
can do there and I would like to point
you to a talk that I think will be very
interesting by a cellular bike so and
co-workers showing how to reach Quentin
supremacy with near-term buildable
shallow circuits yeah so if you forgive
me this plug yeah I will conclude with
wishing you all to have fun here's the
next day's that aqc 2016
I should ask you to come here for a
question to record so do you have any
questions or comments no questions
that's strange
please Hartmut i'd like to ask you
something
Hartmut I'd like to ask a question
that's part of the I well I didn't
understand in the tempering thing was
when you go to the middle server and you
find a configuration that's something
diagonal in the Z basis right and then
when you send that over to the quantum
annealer you're sending something which
is diagonal in the computational basis
right it's not outward yeah so then what
happened so then what we are what the
now you dial up the transversal field
and this will start sort of just spread
Oh so the way you send it in the
transverse field is zero and yeah I turn
it up exact okay god and to a certain
strength and we don't know which
strength is this so we will try a few
gamma max and and then miss that okay
thank you interesting aspect but I would
like to know what is the definition of
replica is very replica a notion related
to statistical physics system can you in
a for dummies explain what replica is in
sorry yeah I was under the impression
that everybody is quite familiar with
parallel tempering so replica simply
means that you have the problem on the
same graph so you're in d-wave language
age eyes and the ji J's would be
identical but the replicas can differ by
the exact spin configuration because
each one goes through probabilistic
metropolis updates at different
temperatures so the evolution will be
different but you're all each replica
lifts on the same identical graph was
the same values characterizing the
connectivity and the local fear
thank you once again to go on to me</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>