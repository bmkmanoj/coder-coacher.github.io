<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>GTAC 2016: How Flaky Tests in Continuous Integration | Coder Coacher - Coaching Coders</title><meta content="GTAC 2016: How Flaky Tests in Continuous Integration - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>GTAC 2016: How Flaky Tests in Continuous Integration</b></h2><h5 class="post__date">2016-12-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/CrzpkF1-VsA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">who last talk of the day we have a
presentation by John Miko and ativ mem
on going to talk about some research
they've done at Google on flaky tests
take it away okay my name is John mica
as he said and I run I'm a manager
managing internal testing continuous
integration at Google this talk is
mostly about some work that we
collaborated with Atif mem on he came in
worked in on sabbatical with us for
about six months and we sent him into
our data and he came back with some
interesting stuff and so here's the
stuff anyway if you want to introduce a
little bit yourself - sure I am attic
women I'm from the University of
Maryland in College Park and I spent the
first half of this year in John's group
doing doing my sabbatical yeah so here's
let me talk a little bit I want to set
up the problem a little bit of talk
about some of the stuff that he's found
from our data aught of thought I was a
really great managing his project
because basically I was too busy to help
him and I gave him terabytes of data and
said go find what you can find in there
and let me know what's interesting and
he did so it was cool but one of the
things we're going to focus on a little
bit is about flaky tests a lot of people
don't think it's an important problem I
hate even was talking to a grad student
who sent out a paper
math PhD thesis on flaky tests and was
told by peer reviewers that it's not an
important problem well I'm here to sort
of dispel that rumor and to say
flakiness is a problem for us we define
it as a test that passes and fails when
run on the same exact code so if you can
take the same code and run it twice
we're running n times and if it ever
observe a different result that we call
that a flaky test one of the most
startling observations which was just
sitting in our data waiting to be
discovered was that 84 percent of the
time when a test was passing and then it
starts failing 84 percent of the time
that indicates a flake and not something
that somebody broke that's huge and
that's and that's across all of our
tests Corp
almost 16 percent of all of our 3.5
million tests have some level of
flakiness they've been observed to flake
at least once
flaky failures frequently block and
delay releases for us pushing out to the
web people have to investigate to figure
out whether it's a real failure or not
or run it again or wait or whatever and
we spend somewhere between about 2 and
16 percent of our continuous integration
compute resources rerunning flaky tests
and this little box diagram that you see
at the bottom that's very busy it looks
like almost Christmas time maybe it is
almost Christmas time you can see here
that this little box is the test history
for a single test that's Leakey and
anybody any person looking at that can
say yeah that's a flaky test it probably
never broke for real with at least not
in the window that we're seeing here
maybe there's like three in a row right
up in the top row could be a real
failure I don't know probably more
likely a flake and so this is just a we
start monitoring this and this is how
much of our compute resource pool we
spend on rerunning flaky tests it might
not seem like a lot all it's only 5% if
you knew the size of our pool it's
pretty expensive
let's just put it that way and what what
are the things that cause tests to be
flaky well we've done so we haven't done
a good analysis yet this is an area we'd
like to do some more research in but
it's all the things you would expect you
know race conditions waiting for
resources multi-threaded testing
environment issues either it's the code
under test or it's the test case I mean
we really don't have a good break down
yet and we'd like to have good data
about the root causes of all the
flakiness that would help us sort of
characterize things better so we're
still that's an area we're still trying
to figure out from our data there we go
what we also see is that when we run
tests we run our 3.5 million tests we do
it continuously we're seeing about a 1.5
percent of the tests that we start
running end up resulting in a flaky
failure that's a pretty big number and
like I said it continually blocks our
releases and also
if you imagine in order to ship a
project like Gmail Gmail has like you
know thirty thousand tests that they
care about so if you have thirty
thousand tests and 1.5 percent of them
are going to flaky fail when you run
them are you ever going to be able to
ship well yeah you can but it's it's
kind of a little bit of a pain to do it
and despite we've made a large and
concerted effort to fix flakiness to fix
to go after the developers and say damn
it you should fix the test and make it
less flaky we tolerate a certain level
of flakiness across the board we still
observed that the insertion rate is
about the same as the fix rate as
developers write new code so we really
haven't been able to make much progress
in reducing the total number and really
CI systems all have to deal with a
certain level of flakiness in the tests
and we really want to try and minimize
the cost to developers and to releases
and to velocity of having these flaky
tests in our continuous integration
system because they're not what the
continuous integration system is all
about we're trying to find developers
actually breaking the stuff right so
again I've used the number a few times
we every time a developer at Google
submits a change we look at what he
changed and we do a reverse dependency
map through our build system and find
all the tests that depend transitively
on the thing that was changed and we
schedule them to run and we do that as
fast as we can and as frequently as we
can most tests have have two distinct
flag combinations that they run with on
average likes like opt-in fast filled so
some some optimized build and some debug
build we run them both and then we
record the pass/fail result of each and
every test in the database and we
uniquely identify those by the test that
was run the flags that were used and the
change that was submitted by some
developer and we have two years of test
history of all of these test results
pass/fail and every night we every time
there was a down transition of
transition from pass to fail we SiC our
D flaker on it and it runs the test ten
times and says did it ever observe a
pass if it did then that test is marked
flaky
we keep a database of all the flaky
tests and we're starting to do a few
things that are a little better than
that but that's something for the future
so and we're just really touching on the
very beginning of analyzing those
terabytes and terabytes of data but they
allow us to do a lot of good hypothesis
and to see if it's borne out by the data
is it that tests fail more frequently
when the systems are loaded is the test
I mean any kind of thing that you can
think of you can look at our data set
and you can answer it without having to
run any tests without having to do
anything much about it you can also get
the correlations between changes that
were submitted by developers and
failures that occurred right so you can
say oh this this you know that this see
I'll cause this test to break and so you
can start answering questions about well
what are the commonalities between
changes that cause test to break and
that's some of the research work that
octave will talk about in his half of
the talk it's really pretty cool what
he's discovered on some of the data so I
will talk a little bit about how our
continuous integration system works we
have our happy developer over there he
submits some code I'm gonna make Gmail
better today I'm picking on Gmail hope
nobody's here from Gmail so and he
submits a change that change goes in we
say hey what tests should be run on that
change what did he change and what tests
depend on the thing he changed let's
schedule all of those so we send into
our test scheduler we batch them up we
put them into we batch them into groups
we run on a massively parallel back-end
but it's like a two-stage system I don't
want to even go into the details it's
very complex but first we have to batch
up the tests that we're going to run
together we pick usually between four
hundred and a couple thousand tests that
we're going to run together and we run
them into the massively parallel
back-end and get the results so we build
we run the tests and we record the tests
back in the database along the way
there's a certain amount of
infrastructure failure when you're
running you know millions of tests every
now and again something goes Boop's and
a machine crashes or Borga fix a job or
whatever it is and the test doesn't get
completed and we reschedule those until
we get a result pass or fail so we
guarantee that every time as frequently
as we can we get a pause
negative pass or fail result for every
change that was submitted now quite we
skip over some of the changes when we
don't have enough resource to run them
all because we don't we basically run it
and end up running less than half of the
triggers the times when we think the
test should be run
but we always schedule it at the next
time when we have resources so I know
that's a little confusing probably a
little more information you needed but
we actually can't run every test at
every effecting change because there's
just too much work to do but we actually
give back to developers a result after
submission within supposed to be within
two hours is our SLO trying to give
results to a developer that they their
submission passed or failed all the
tests that were affected by it
we don't always meet that SLO I'll be
honest some people here from internal
could probably call me on that but we're
we're doing our best and generally it's
within three we also have a tool that
tracks as I said all the flaky tests the
names of the tests have been blacked out
to protect the innocent or the guilty as
the case may be here but we this
particular display aggregates all the
times and we keep track of the
stacktrace and all the information about
the tests that flaky failed and we try
and later on determine if a new thing
that we're seeing matches an old flaky
test it's the same test it's the same
stack trace then we assume it's a flake
but if it doesn't match this that the
same test on the same stack trace it
could be a new failure of a flaky test
which is a little bit different harder
to discover but we keep this database so
that we know we can get developers
looking at the tests that are the most
flaky we can tell we use it as a
indication in our UI hey this test
failed but we pretty sure it's a flake
you don't have to worry about it
so you don't have to investigate as
carefully those things and we're trying
to get a lot lot better Murat is Murat
here uh he must have left but but he
went one of the guys was here earlier in
the day Murata in England London he's
running the group that runs this
infrastructure that records all the
flaky tests and there they have some
great plans for the future of how
they're going to make this better and
using some of the techniques that we're
going to talk about or that autiful talk
about
his half of the test so without further
ado let me show you some of the
interesting things that he found in our
data I need to know hold it just the
green yes that's it it's pretty simple
right okay so as I mentioned I was on
sabbatical at tap and it's an amazing
group to be in if you are looking for a
sabbatical you should contact John it
was amazing for a couple of reasons one
is that the tap team is amazing so very
helpful and very friendly folks second
reason is I have access to all this data
right I have terabytes of data all the
test results for the last two years all
the code that was that corresponded with
all the results test case code alright
so I'm looking at all this I'm staring
at this data for six months and
analyzing it so that was good the
analysis and the results I think in my
opinion I think they have led us to
almost a breakthrough in how Google can
handle flakes in the future right we are
at a point where we could be looking at
test results or at offer of a single
test case and look at it and say oh
that's a flake we don't even need to
rerun it ten times right like how we do
right now and I think that's if we start
incorporating much of the things that we
found we should be able to do much
better than we are doing now as my
six-year-old daughter she's the one
who's artist on this pictures that
that's her interpretation of what we're
trying to do the scientists on the left
who looks a lot like a princess she is
saying that I am 97% confident that this
test is resolved this flaky and this
other person is saying how can you tell
without rerunning it because real tests
don't behave like that and John kind of
said this if you look at this test
result and those F's are the failures
and the green ones obviously are the
passes and the other ones in the ones
with the dashes and other things are
results that we could not obtain for
various reasons one of them that we
didn't have resources to run at every
change so if you look at this
old a test went from pass to fail failed
to pass so many times and real tests
just don't behave like that all right
it's not that developers break tests and
then they fix them after an hour after
two hours they break them again right
they remain pretty consistent so you
will find that there are lots of
failures and then most of the remaining
ones are passes so by looking at it you
can tell all right
so I want to thank my daughter for
drawing that so thanks Ruby
all right so what we want to do with
this result is we want to take our flaky
test infrastructure it's not that the
infrastructure is flaky the tests are
thinking we want to we want to improve
it all right and how are we going to
improve it we want to identify flaky
tests without rerunning them in three
ways one is we going to follow our
intuition and one of the intuitions that
John and I had was you can look at
simple signals of pass and fail patterns
right like how we just looked at those
reds and greens to indicate flakiness
right let's try to formalize that the
other one is let's try to find features
of flakes what makes a test flaky all
right are there signals that we can
derive from all this data that tell us
that oh yeah flaky tests behave in a
certain way all right they have certain
characteristics not the pass and fail
only but other things all right
and whatever first models have shown
that we are right a lot of time so we've
been able to classify 90% of the flakes
correctly then we have the opposite
problem if we can identify flakes we
should also be able to identify real
failures so if you give me a failure or
a transition from pass to fail I should
be able to look at it and say this is a
real failure or this is a flake so how
about we identify features of real
failures all right and then use that to
filter out flakes because if something
is highly correlated with real failures
any deviations are highly likely to be
flakes for example if I one of the
results that we saw was a file that has
been modified by more than 15 developers
in the weeks period right
97% likely to cause a failure all right
so that's that's a feature that we can
find which is highly correlated with the
real failure right we can use those kind
of things what do we do with that first
we want to model flakes and their
behavior so I'm going to show a couple
of slides where we when I'll show you
how I formally model the flake and its
results and then I'll show you some of
the results the I will only give you
like a preview of most results we have a
we have a couple of papers in the
pipeline that will describe more and you
can talk offline about whatever
questions you might have
all right so this is how I'm modeling
our test target on a failure a test
target is really a test case for all
practical purposes so there that's a
test target that's the name of the
target their top project some service
tests and here is its test history now
Ciel's in google our code commits all
right so you're committing code you're
going from the left-hand side
you keep committing code and you keep
running this test case and you get
passes and fails all right and I'm going
to start modeling this history in terms
of negative edges and positive edges so
simply put you went from a path to a
fail that's a negative edge fail to a
pass that's a positive edge all right so
they here we have three edges in the
history of this test target and I'm
going to model them as the start CL
which is a commit at which the edge
starts the NCL where that ended the
length of that which is like how many
commits were in between and whether it
was a positive or negative right so each
history I'm going to model it like this
I have one way of describing my test
history all right so I took more than a
million of these edges all right and I
looked at how tap which currently runs
reruns test targets ten times at night
to D flake I wanted to see which of
these edges were real and which ones
were not and it turned out that eighty
eighty four point five percent or 85
percent close to eighty five percent
were due to flakes with confidence we
can say that okay the remaining 15
percent
most likely did not include flakes right
but they may have now if you imagine
what this means
whenever you see a path to fail right
you send this to you it recognizes a
failure you send it to somebody to look
at this manually right so you have a
build cop or you have a developer who's
looking at this and 85% of times this
developer says oh I will flick throw it
away there are enormous resources that
are being wasted because of this right
and only 15% of time this is due to real
bugs okay so the takeaway message from
this slide is that even though a small
percentage of flakes at a given instant
a small percentage of overall results
are due reflects one point five to two
percent that tests on our flakes but if
you look at over a time period a flaky
test that can go pass to fail very
frequently will give you so many edges
that it becomes 85 percent and that's a
much better indicator of the impact of
flakes than just saying that there are
only 1.5 percent of our tests are flaky
all right okay okay so here's our
scientists again all right so let's
start looking at some of the features of
flakes how can we start to automatically
identify flakes right so there's a
hypothesis that flakes have a large
larger number of edges per time period
all right so I have two tests test 1 and
test 2 test 1 seems to go from Paul plus
to minus
many times like fail to pass and pass to
fail test 2 does not seem to do that and
over a 5 year period 5 hour period right
so let me start modeling some of this so
I'm going to take all my test cases
right and I'm going to partition them
based on number of edges that I have
seen in their entire history ok so for
example test targets that have 21 edges
ok they are in this vertical bar here
101 and 816 ok and then I'm going to see
off those test targets what percentage
were flaky and what person they were not
flaky right so the
test targets that have 21 edges 60% of
those were not flaky right but 40% were
okay as you move in this direction more
and more become flaky if a test target
has 816 edges
it's definitely flaky let's fill in the
gaps here and you can start seeing even
though it might not be very clear
because the color you see lot more blue
here all right and then the red starts
taking over here all right so as you
increase the number of edges you are
getting more and more flaky right so the
takeaway message from this is the test
targets with more edges in their history
are more likely to be flaky that's a
great signal to look at if you are
looking at if you're trying to filter
targets based on flakiness right because
measuring edges is easy right and
counting the number of edges over a time
period is straightforward now if you
notice there are some blues in here with
large number of edges and when I showed
this to some folks at tap their comment
was that most likely these are flakes
which were not detected by a tap all
right now let's start to quantify flakes
how can we say that something is more
flaky than less flaky right so what I do
is this is a result which is most likely
not flaky
right and that's a flaky target right
test how do we start to quantify this
let's start to compute a flakiness score
okay what we are going to do is we're
going to extract these vectors and I am
representing that simply by using a
regular expression there but what we are
saying is start looking for patterns
which are flake a failure at least one
right or more followed by anything which
is not a fast and then a number of
passes and then followed by anything and
then followed by a number of F's okay if
I can extract vectors out of this
history which fit those patterns right
and then I fit them into this formula
which I am NOT going to
close but it takes two parameters one is
the number of vectors that I found in
the history of the test target and how
what the length of those vectors were
right if I can do that I can compute a
function for each and every test case
once I compute this function I can then
correspond that with what Tab believes
our real failures are not flakes are not
because of their ten times they run
their test cases right so the top 404
according to my score were 100% agreed
by tap that yes they are definitely all
flakes okay nine hundred ninety one of
my top 1000 were flakes according to tap
and then the percentage drops as I go
down to say twenty five thousand so off
my top twenty five thousand twenty three
thousand nine hundred and thirty are
flakes so percentage is still pretty
good okay so I can actually start to
quantify flakiness and I can start to
rank all the test targets by their
flakiness score another thing I can do
is if I I can see if a test targets
flakiness is increasing or decreasing
right let's say somebody change the code
of the test case I can now start to say
that oh yeah over this period the
flakiness score went down now somebody
made a change flaking a score went up so
we can start figuring out why test
targets became flaky yeah okay let's
look at something else flakes are
unlikely to share their histories with
others you imagine this I have four test
cases here right test 1 and test 3 have
exactly the same pass and fail pattern
and test 1 test 2 and test for have the
same pattern over this five hour period
what does this mean it means that test
well all these test cases are highly
unlikely to be flaky right tests don't
become flaky in a pattern right if one
bug in the code might cause a bunch of
test targets to go from pass to fail and
if they are all sharing that history
over a long period they're highly
unlikely to
flaky let's explore that hypothesis and
let's let's see how what that means
formally now I'm going to extract my
edges vectors as I did before right and
I see that I'm marking those two with
the blue and they have identical
histories these four have identical
histories they are highly unlikely to be
flaky whereas these two that don't share
their history are highly likely to be
flaky right let's start plotting this
okay so this is a somewhat complicated
plot but I'm going to try to explain it
a little bit
both of these are log scales right
because as I found out in my sabbatical
everything in tap is some follow some
kind of a logarithmic or exponential
scale so log scales are very easy hard
to visualize but then they kind of
expand the data in the right way so here
I am showing you two targets because
this x-axis here is to write they two
targets share their history okay I'm
going to draw a plot here I draw a dot
there and the number of edges in the
history which is fast to fail are
roughly 20 because on the y-axis you
have roughly 20 there all right and
that's how we're going to read this very
little sharing okay because there are
only two targets that are sharing here
whereas if you look at something like
this here five thousand-plus targets
which are when I say targets I mean test
cases they share their history and the
edges in this history are - okay there's
a lot of sharing because what more than
5,000 test cases are sharing their
history okay and this is the only
history they have over the entire period
that I looked at this is the only
history they have on the y-axis which is
X is equal to zero there is no sharing
okay now if I fill in all these there
this data and I start coloring them by
flakiness as we know it to be from tap
you see a clean separation
I'm increasing my sharing of history and
most of these targets that I find are
green which means they're not flaky
right and if I go closer to flake a
sharing of one zero or one right there
are more flakiness and if I go in number
of edges above hundred then it's almost
all red which means test targets that
don't share history with anybody and
have large number of edges are almost
always flaky okay so if I could compute
this history and I could correlate all
the histories of all the test targets I
could use it as an amazing signal for
whether this target is flaky or not
right I keep saying target when it
should be really test case you guys
missed me up they use the term test our
test target for test cases all right so
that's our result here now to take away
messages degree of sharing is a signal
for flag detection yeah if you have
questions about this plot we can talk
offline but I want to get to the next
step right so now let's look at the
opposite problem
okay now the scientist is saying I'm 90%
confident that this is a real failure
not flaky okay
now we are not talking about flakes
anymore we are talking about real
failures okay how can you tell well
because the code and the test artifacts
are consistent with a real failure okay
let's look at an example we used
multiple sources of multiple signals one
is a relationship between the code that
was changed and the test case that was
run as a result of the change we were
going to look at the code modification
frequency we're going to look at the
source file type all right if you always
suspected that C++ was worse than Java
that's true Shiina changeless authors
who authored that commit right if you
suspected that Joe always broke the bill
yes that is how that happens
and files modified by multiple authors
in a short amount of time are more
likely to false breakages right let's
see some of the results so one is a
relationship between the code in the
test okay so as we all imagine our code
repositories we can imagine them to be a
tree or a dag right because they depend
on each other so I have a test I have a
source file that is sitting at the
bottom here and all these other files or
packages they require they depend on
this file for building so imagine this
is like a build tree or build graph
right so I somebody modified this file
and because of this modification most of
the packages that are above it need to
be rebuilt okay because they depend on
it directly or indirectly okay so this
is my source file that got changed
that's my test case or test target that
needs to be run as a result how far is
it in terms of number of edges what is
the distance between that and that okay
if you look at the minimum distance can
somebody count
what's the vs. shot what's the path
that's the shortest for right okay so
we're going to call that the min rank
which is the pet on that Google has used
in its documentation that's a min rank
between that node and this node which is
a shortest length of the shortest path
okay is there a relationship between
test tests that break and the code that
gets modified and the distance between
them okay so if you look at the min
ranks of code the files that were
modified and the test targets right if I
plot all of them in Google I get this
plot right now these are all the
minerals they start from one right and
they go all the way to 41 here right
that means a piece of code that was
touched was modified here and a test
target that was run they were at a
minimum number of edges was 41 okay
that's a pretty deep stack there but if
you look at this most of them tend to be
roughly in this area right so it's like
a wide kind of a graph and then it goes
up quite a bit right this is the total
population okay
but if I wanted to look at what happens
to only the targets that go in edges
they go from pass to fail or fail to
pass and I plot the probability most of
them tend to lie between two and nine
okay which means when you modify some
code test targets that are beyond 10 min
rank are highly unlikely to break okay
most of the closer ones which are like
less than or equal to 10 are more likely
to break okay now how can we use this
result we can use this result as test
target edges farther than min lying rule
10 are highly likely to be flaky right
if some test target broke somewhere like
distance 40 from your check or change
that's highly likely to be flick-right
because now we have a matter we have a
signal for real failures okay let's look
at number of times a file was in this in
commit so when I modify a file right I
can modify multiple times if I modify a
file multiple times I can start counting
over a period how many times it got
modified okay and then when you got
modified it might have caused a breakage
all right or it went from pass to fail
or fail to pass I'm going to measure the
probability of that happening as the
number of times the file gets monochrome
files get modified increases all right
so for example files that got modified
11 times right
they were 0.25 likely to cause a
breakage okay but as then this number
increases finds it
modified 41 times or 0.75 likely to
cause breakages so file modification
frequency is actually a good signal for
predicting which test targets might
break or not okay so this is failures
associate frequently modified code are
highly unlikely to be flaky okay all
right source files type we took all the
file types that Google had censored some
of the ones that we don't want you to
know about right and and then we saw
which ones contributed more to failures
now this plot is a bit funny because
these the black bars are actually all
passes and the light gray ones are all
failures right so HPP files they were
involved in lots of breakages right only
20% of time when an HPV file was
modified
was it not involved in raikage okay and
if you go all the way here config files
were never in a breakage okay where does
Java sit right here okay and where does
C++ sit right here okay so I'm not
saying anything else but but what about
the what about how the code is
distributed so we started we superimpose
the curve on top of this which tells you
that how many files did we see in the
Google codebase alright so this is a
little known secret but Java code seems
to be the most prevalent in Google right
I hope I didn't say this that vibe
wasn't supposed to say all right we
still use a lot of Python C++ all right
and if you look at the frequencies you
can tell that there are some files like
HPP there very few of them so they're
not going to make a huge difference on
our testing cost even though they might
be involved in lots of raikage's but we
should look be paying careful attention
to Java and C++ and those commonly
occurring file
right so failure is associated some kind
of fires example config files are highly
likely to be flaky because they almost
never fail right okay so that's one okay
authors this is the interesting one okay
now it may surprise you that there is no
user called ABC at Google right but when
this user committed code 182 times over
the history that I looked at 59 times
that this user broke the build all right
31.4% that's pretty high right whereas
user XYZ was much better okay now this
may be for all kinds of reasons maybe
user a bzees looking at very complex
code right we don't know so we're not
going to do anything about performance
or bonus or stock options and there are
lots of tools that make that modify the
code also for example product one
release is a tool that generates a bunch
of things and then it commits code right
and it commits in code 42 percent 42
times but it broke the build 39 times
right I don't know what kind of tool
these these are but 92.9%
okay so if I look at this user I can see
the failures associated with author
product one release are highly unlikely
to be flaky right if I see a breakage by
this author by a code that was modified
with this author I can say that's highly
unlikely flaky file modification by
multiple authors this was an astounding
result for us we feel that multiple
people should work on code is the same
okay but this result shows that I'm
going to tell you how to read this plot
but I'm going to show you what the big
result is the big result is when one
person is modifying code they are highly
unlikely to break it when two people
modify code at the same time roughly the
fraction of breakage goes down okay in
many cases
possible because they talk to each other
or they send emails and they maybe
review each other's code and that ends
up being better written code but as soon
as the number of developers grows beyond
two it goes up significantly all right
and then it kind of stays roughly there
but once you get into like 15 I
developers when I say users I mean
developers here then you are more most
likely to actually cause a breakage
right so if you have in your company
three or four people modifying codes the
same thing just ask them to stop okay
okay all right so that's these are all
the signals that we have there were some
signals that were not really related to
real failures for example number of
times a test target was affected was not
a strong signal so we have really good
results there but where do we want to be
we want to be at a point where we say
that stop running that test
it's a flake okay but you did not even
run it you run it even once right
without running the test case right but
this test has all the elements of a
flake right what elements does it have
it has these characteristics that we
know of that are associated with flakes
right we want to be there and we want to
be able to predict whether a test is
going to be flaky now if I told you that
I am going to run an Android test that
uses a user interface test harness right
uses a bunch of sleeps and wait commands
all right and it's multi-threaded you
will most likely say oh it's most likely
going to be a flake all right whereas if
I say I'm going to run a unit test which
is going to supply a bunch of parameters
it waits for nothing it's not
multi-threaded it's highly unlikely to
be a flake how would we get all these
characteristics we go through the test
cases we will go through its code we
analyze the code artifacts we look at
what kind of code is being tested the
execution environment and come up with a
holistic model
to be able to predict whether a test is
a flake or not right it or is it going
to be a flake or not and then maybe we
don't have to run it or waste resources
on it alright so I have one more quick
thing here to say we did an awesome
collaboration with thought if we learned
a lot he learned a lot we're publishing
papers it's really cool but we're still
looking to collaborate with other
companies with other researchers who are
working on the same kinds of problems
that we're working on with flakiness and
with predicting breakages we really want
to do we have lots of data we have tons
of data if you're interested in piling
into data come and talk to me but and we
really do have you know the largest test
pull of the largest codebase around
we've seen a lot of researchers in
academia they study open source projects
with tiny code bases and small numbers
of changes and it's really hard to draw
a good crisp correlations with our large
data set we can answer your questions
pretty quickly and we've been working on
trying to be better at publishing and
we're publishing a paper without that
we're publishing our data set there's
going to be several papers coming out of
the work that you just saw we also I've
run a journal review Club once a month
we pick an academic paper we review it
we talk about it we're always inviting
the paper authors to come and to talk
and most often they do and it's been a
good exchange so far it's completely
open to the public
we do sort of a Google Hangouts I'm
sorry but every month and we have this
paper that that we review if you're
interested there's a link here these
slides will be up online in case so you
can actually get the link from my little
link thing there other than that I think
we're kind of done if you have questions
or one more click oh yeah
and Annika did the wonderful
illustrations yes excellent thanks John
and a tip I have so much ammunition how
I'm able to use against developers haha
this is fantastic we can give you the
real names internally all right look out
ah so yeah we have a lot of time for
questions which is good wait a minute
how did it developer submit this what
this why not just to lead all the flaky
tests where who let that developer well
I mean if you talk to some of the teams
who have flaky tests first of all it's
impossible to get rid of it all if you
look at the 16% of tests that have ever
exhibited flakiness if we just deleted
them all I think some of our key apps
will probably stop working and some of
them have very low levels of flakiness
which are fairly tolerable and don't
often cause problems for the project and
ones that don't we have active
enforcement that moves them out of
blocking releases so we basically take
we ignore the results of tests that are
flaky beyond a certain point until they
get fixed and bugs get open for
developers to fix them and put them back
into the test pool so a certain level of
flakiness is just a fact it's a part of
life and we're willing to tolerate it if
it's low enough and if it's not we just
remove it from the blocking the releases
and then we do either delete the test or
make it better so I always find it
interesting that developers can spend an
entire week trying to get a different
color to print out of their bash script
but when it comes to flaky tests they
said just just a leading mm-hmm my own
opinions my own opinions not of those of
the speaker Hydra when we distinguish
between flaky tests and real bugs to do
concurrency which will also pass fail
non-deterministic louie I'll let you
take that one Oh I think the question
really is that the bug is possibly in
the concurrency a concurrent code and
based depending on the execution the
test that's testing it may pass
sometimes may fail sometimes and I think
the answer to that is I don't think we
distinguish we still call that a flaky
test yes and then we pass it to the bill
cop and then they in the debugging
process they have to figure it out but I
think from the signal that we have until
we start to categorize code as
concurrent or not until we use our
signal it's going to be very hard to
distinguish right right now it's really
impossible from the outside just
observing the pass fails to be able to
tell whether it's a bug in the code
under test or the test
questions in the room oh so let me read
this one and then I'll come over with
the mic as you're answering how much
history on a test needed to be able to
predict flakes what if test has changed
doesn't mean we need to gather more
history first yeah so I think in the
data that I showed you there were seven
plots where I used a week worth of
history and of course that most test
targets tend to get affected during a
week multiple times so you have a
reasonable set of data and if the test
case changes or the code changes then
you will see a change in flakiness and
we started to measure that in fact
Murad's project I think is doing that a
lot so that's that's probably true and
and at Google I mean we run all of our
3.5 million tests about every 20 minutes
so we get history pretty fast on new
tests so hi I'm Simone from blah blah
car I just wanted to know about false
negatives how do you know that tests
that are passing all the time are not
flaky or broken because they always pass
and can you use the data that you've
been collecting to correlate the code
that's changing a lot with flaky or bad
tests that are passing all the time
right so I like that question basically
what you're saying is and I've been
saying this all the time when we define
flakiness the way we will define
flakiness is it goes from pass to fail
right but at
might just pass all the time but inside
the code when it's executing it might be
executing different paths right and if
it's executing different paths
is that not flakiness right unless it's
designed to execute different paths so
that's been my definition of a flame and
in fact one of the interesting papers
journal papers we reviewed they actually
went and basically recorded all the
program state as it was executing and
even without observing a change in the
past failed they observed changes in the
code paths that were being taken like
the amount of code coverage and so forth
varied from run to run and yes you're
right that's in some definition of flaky
test we're looking where the light is
because we can see the pass/fail but
there's more under the covers that we
haven't really looked at yet it's harder
to get at that because it's not
reporting anything right and going to
your second part of your question if we
once we start to correlate features of
the code in the test that test code then
we might be able to go back to all the
tests that were also passing all the
time and see if they exhibit some of
those features and if they do then we
could flag them I get any other interim
questions hang on
I have a question for the you know you
continues integration testing especially
for the pre committee testing or
pre-merger testing right way we need a
quick turnaround for the test verdict
against the patch from developer because
everything is automated way and tests
pass then we you know carat plus one
plus two right if some case fail we were
minus one so then the patch will be
gated so at this stage there is no room
for flaky tests so I would like to know
the googles practice at you know
pre-integration stage how you identify
the oh definitely you have the mechanism
to identify the flaky tests but I am the
more interesting thing I would like to
know is how what happened then you know
because it's everything is automated if
you from my understanding if you are
confident that okay this is a flaky test
then this result will not we are not
block the patch right so we need some
mechanism there in CR system so I can
talk about a couple of things we've had
some recent advancements in there but
the basic practice at Google is most
teams block submission unless the pre
submit passes and they indicate which
projects they want to pre submit against
so that's part of it yes we do block
submissions just like you're talking
about and we do the same thing post in
the results of the code review there is
a way in our system to indicate that a
test is flaky developers opt in and they
say this test is flaky if they say that
we rerun if it fails we rerun it up to
three times and look for pass so then it
will deflate itself sort of as part of
the pre submit that's bad in the sense
that if it's a we allowed tests to run
up to 15 minutes so if you have this
test that has to get run three times
each time it takes 15 minutes to run
before it reports of failure you're
you're delaying the developer by 45
minutes to get that result
so we've just started an effort it's
just launched a couple
a few weeks ago to actually exclude
anything that's in our flaky test
database from running during pre submits
we just exclude it we don't run it
during pre submit at all if a developer
were to break that we would catch it
post submit but it doesn't block their
submission doesn't cause them a problem
so we actually keep statistics and we
exclude them from running on the pre
submit now we do have a very
fine-grained ability to control exactly
which tests run during our pre submits
and we run them all in parallel so we
get a result very quickly and we also
prioritize all of that work over
anything for post submit so if a
developer is waiting for a result we
give it our top priority in our back-end
infrastructure so it just goes out and
farms and runs quickly and comes back so
between those different efforts being
able to deflate on the fly being able to
exclude the tests from running and
paralyzing massively the the pre submits
we give redevelop errs results pretty
quickly on pre submit so he said most
developers are not so patient no they
are not that is true and and in fact our
90th percentile latency for pre submits
is under 45 minutes generally so you get
back a result pretty quickly and usually
we spawn them off when you send the code
out for review we spawn them off and
then we post back to the review thread
generally the reviews take longer than
45 minutes so we have a pretty good you
know track record of posting the result
before they need it the rerun is fully
automated he's asked me if the rerun is
fully automated when you say flaky yes
if you say this if you mark the test
this lake hylia rerun is completely
automated happens end-to-end up to three
times and then if only if it fails three
times in a row it'll report back to the
user no you failed you actually broke it
hello I'm Rahul Goodman from Oregon
State University so my question is
something slightly different in your
graph about the number of authors who
modified a file and how many of them
caused failure I saw a Goldilocks
location at two others I mean it's
better than one single author modifying
a file so what's happening there
actually you mean the breakages go down
yeah when you have more than one author
it only until 2 but then it goes up
again after that I mean the yeah the
breakages go up significantly as so you
have three authors and we I don't think
we explored that I think we have some we
guessed that when two authors are
working on the same file they most
likely are better at collaborating with
each other right because they can send
it and this is very specific to how
Google does stuff right you have a
single codebase if somebody is modifying
a file you can know when somebody
modified the file you can contact that
person everything is open so maybe they
talk to each other or they send email
they review each other's code so
breakages go down it's almost like
another review process so yeah there's
something like a pair programming or
something where or is it more like more
distant in time I think it's something
to explore but I wouldn't say directly
pair programming I don't think they are
sitting together but yeah I mean I think
it's more of a code review so far we've
observed the astonishing result but we
haven't dug in to figure out exactly why
and we'd like to know more yes I just
assumed it was the developers intern
came in and that's the second authority
to fix all the bugs
another question a good question is
olive here how can other companies
leverage this research I I'm happy to
talk about the research with anybody
else up to the extent that I can I am a
professor so I will do anything for a
free lunch believe me he wasn't free no
but but in all honesty you know we
really are publishing all this results
in papers we want to work with other
companies and collaborate to share our
ideas and our results I feel you know
contact thought they've contact contact
me happy to collaborate with you explain
what we did come and talk about it we
really want to get other companies to at
least if they can't share exactly the
code that we have at least share the
ideas and and we really want to be open
with our datasets and ideas the extent
that we can be like I'll read this one
and come on over with the mic but have
what priority do the team's place on
fixing flaky tests
on fire our men I'll take that one huh
so in terms of working practice at
Google
every team decides for themselves how
strictly to adhere to any process they
define what their processes are and they
just define their adherence levels I
will say large groups most of the large
groups at Google make that develop most
of our main apps pay very close
attention to flaky tests they're all
using the same work practice of removing
flaky tests from blocking their builds
and blocking their submissions and they
aggressively pursue developers to fix
flakiness and when your test is
quarantined due to flakiness your
choices are to fix it or to delete it I
mean it's pretty I'd say most of the
work practices are good but again
there's always exceptions there's thirty
thousand developers at Google it's
broken into small teams it's hard to say
that all teams are doing it but
generally the work practices in that
area are pretty good
hi I'm Manisha your Oh anytime from Red
Hat and so there was this slide
predicting that a test will be a flake
before running it so all the parameters
that I mentioned there they relate to
our UI test with selenium webdriver so
are the UI tests the only ones that you
are terming as flaky ones which we know
that are the most prone to or you've
seen the other ones some relating to the
API or the backend that have you know
given this trend of being flaky I think
so UI tests are not the only ones are
flaky most we've observed that most all
time tests that depend on external
resources like if they are making some
RPC calls or they're waiting for
something they tend to flake out more
okay and when it is during the day right
when the test is running those resources
tend to be used more also right and this
is one problem with the current approach
where we run tests ten times at night
right to kind of figure out if they were
flakes or not and those resources that
are waiting for they happen to be
available at that time because they
don't get used that much at night so if
we suddenly see that okay this test is
passing they you know we didn't do the
statistical correlation between sort of
test
type whether it's a unit test system
test partly because we don't have good
information about which of our test
targets are say web tests or we didn't
divide the space in that way I think
it'd be interesting to know what those
correlations are but we have observed
flakiness and absolutely every kind of
tests and absolutely all of our code
base at some level okay
is it more in web test than maybe it is
in unit tests that may be in less than I
think yes I think it's probably going to
turn out to be true although we haven't
studied it that the integration tests
and web tests tend to be more flaky than
smaller more isolated tests but it would
be great to show that from the data
there was there has been some work
although I don't know if I can talk
about it where they did they were able
to cobble and successfully into test
types and then say that these are less
likely to be flaky in these others ones
are more likely to be flaky but if you
think about test cases that run for a
long time right and require many many
more resources let's say this one test
depends on five services to be available
at the same time and it's going to run
for 15 minutes that's most likely more
likely to be flaky than a unit test that
just calls a bunch of methods right so I
think that result is lurking somewhere
in the data all right thanks
I have another question you mentioned
about the positive edge and negative it
so I just want to know how do you
measure the number of edges like if I'm
using Jenkins is my CI tool I see that
the number of bills pass/fail so how do
you measure the edges based on the
amount of data that you had is there a
tool or no I just wrote a bunch of Map
Reduce to go through the test history
and then whenever it went from pass to
fail came to pass just start collecting
all those but but we did it at the test
case level which means you'd have to get
out the X unit XML files from the
Jenkins invocation to go down to the
next level and say here are the 300
tests that I ran and this one here's the
history of that test our system
automatically breaks those down for us
so we can see each individual test what
its history individually and you would
have to start recording that data for
you to be able to analyze it in the same
way and find edges for
we'll test to say that test is flaky
this test is not flaky if you just look
at the Jenkins top level display you're
going to see the whole bill passed the
whole bill failed the whole bill passed
there isn't any edges there really right
you you would have to go digging into
the actual xunit xml files and extract
them Jenkins lets you do it now some of
the newer pipeline versions let you
extract the XML and and parse it right
out of the box you can do that I'm sure
there's also plugins that do it yeah we
could we need to parse the data then for
that you need the part of the exam oh
yeah it's there thank you I think we had
time for one more question back here um
hi I have a factor
I'm Abhijit from math folks so we also
have a lot of selenium which are web UI
base tests and we were having a lot of
flakiness out over 100% around 17 to 18
percent were flaky tests so we decided
to run a pilot where selenium tends to
be very a flaky if it runs one
particular test target a test file for a
long time
so we kind of divided it such that none
of the test targets take more than five
or six minutes to run and we found that
our flakiness reduced to 5% out of 17%
it was one very good outcome of that
pilot and another I had a question for
John so with your new system that went
alive few weeks ago you said that a
flaky test you do not run it for
pre-summit you run it with Poe summit so
flaky test to have importance they tend
to find regressions so if it's after
post summit and there is an actual
regression in that source code you are
kind of delaying that regression to be
identified at a later process then yes
we are but we found in practice that
developers would ignore the failure
signal from a test that they need to be
flaky or that was marked as flaky and in
fact our UI even will tell them it's
flaky so our UI for the test result says
your pre submit failed because of test
XYZ and it was flaky and users even if
they broke that flaky test they
universally would ignore that
check in anyway so so I mean it's it was
just an acknowledgment of that behavior
and saying look it's better to let them
submit and we actually now are also
starting to explore on the other side
Auto rollback for test failures so if a
test failure happens we roll back some
1cl automatically we're starting to
explore that in one of our groups so so
that's our remedy - oh they checked it
in and it was no good we'll roll it back
automatically for them and I used to
work at math works by the way
all right great that was an excellent
talk to end on and gives us a lot to
think about at the dinner tonight thanks
say TAFE and John anyone applause</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>