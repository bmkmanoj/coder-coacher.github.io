<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Amdahl's Law in the Multicore Era | Coder Coacher - Coaching Coders</title><meta content="Amdahl's Law in the Multicore Era - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Amdahl's Law in the Multicore Era</b></h2><h5 class="post__date">2009-02-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/KfgWmQpzD74" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'm just very pleased mark had a chance
to come and talk to us about this and as
you see this is work that's done with
Google's own and Mark Hills previous
doctor Michael Marty that's right all
right well I'm very happy to be here
even though I had to get up at 3:30 a.m.
Pacific time to be here please interrupt
me with questions and I'll try to repeat
your question and answer it because
that'll an interactive talk is much more
interesting than a one-way lecture all
right so this started in a bar actually
when I was talking to IBM's Thomas pusuk
and he says that everyone knows am dolls
law but quickly forgets it and the
reason we were having this discussion
was the next day I was going to be as
part of a debate with Yale Pat and so
this is myself in the middle and Yale
Pat and Joe Lambert I was the only
person without a gray beer so I had my
art student daughter correct that that
debate I was taking the extreme position
on parallelism and this one is a much
more balanced position and what we're
gonna do is we're gonna develop a
corollary to Amdahl's law which says
that we're gonna have a simple model of
multi-core hardware and it will
complement and all software model and
for fixed resources
it'll assume fixed resources for the
course and that core performance will
improve sub linearly with resources now
you'd say well this is so simple how can
any consequences possibly follow from
that well what we're gonna see is that a
lot of things follow from that we of
course need dramatic increases in
parallelism to use increasingly large
multi-core chips but and but it's worse
than you might intuitively think so if
you're 99% perfectly parallel and you
have 256 cores your speed ups limited to
72 and of course that's not counting
synchronization and communication so
we're gonna need a new Moore's Law where
we're going to try to double parallelism
every two years that's what we need what
we get we'll see
another interesting consequence of this
model is that when we get larger chips
with more transistors we're gonna want
to increase core performance this goes
against some of the conventional wisdom
that we should just deploy a whole bunch
of wimpy course there's going to be
pressure and advantages to doing
asymmetric designs where one or a few
cores are statically faster and there
are as incentives to do dynamic designs
where you can shift resources between
parallel and serial and so I'm going to
do in this talk is give you a little
background where where research is these
days remind you of Amdahl's law
developed this model and then apply it
to these three kinds of chips and at the
end I'm going to bring this full circle
and I'm gonna try to say even though
this talk is about multi-core chips that
it also has relevance to the warehouse
scale computers that Google tries to do
okay so just a quick review we have
transistors we have an integrated
circuit the number of transistors has
been doubling rapidly called Moore's law
we architects have been taking this
increasing transistor budget of also
faster transistors
we've been doubling core performance
every two years while we were incredibly
successful we became invisible to most
of computer science now that we're
failing you'll know our names again and
so we're moving to these multi-core
chips this is the Sun Niagra that has
eight cores each of which looks like a
for like four cores so it looks like a
24 or a thirty two way symmetric
multiprocessor on a chip and we're doing
this for power reasons to tolerate
memory latency such as was pioneered by
the great / Anya work that some of you
in the audience worked on to have
shorter wires and to divide and conquer
complexity so the future is going to be
more cores not faster course but all
that be somewhat faster but they're not
going to double in performance every two
years and so the real question is will
chip performance keep doubling somehow
and what we have seen is there's been
this virtuous cycle this these are
slides do to Jim'll eros where there's
been this increased processor
performance which
for larger more full-featured software I
would use the term bloated larger
development teams higher level languages
and abstractions slower programs but no
problem increased processor performance
keeps the virtuous cycle going and a lot
of money to be made okay what's going to
happen now we're not going to get
increased processor performance at least
as defined for the uniprocessor so
what's going to happen
game over next level federal parallelism
and multi-core chips alright so how has
the the architecture research community
prepared for this this is my community
so this shows a histogram of the premier
conference in architecture number of
papers that deal with multi processors
by year and you can see as we ramped up
to a multi-core we did this the lead up
to multi-core we went to nothing and
then we're starting to to go past that
unfortunately don't have the 2008 data
Luis Barroso is in charge of 2009 so I
think that will do very well but one of
the questions I want to ask is are we
going to overreact we're gonna under
react or we're going to act react just
right now just to pick on some other
communities what about PLD I so that of
course this begins a lot later
I think peel di is doing quite well
there modestly improving or increasing
the fraction of papers that deal with
with multi-core and but not overreacting
what about the systems community this is
s OS P in odd years and then SOS P and O
SDI more recently and you can see that
the system community is not paying
attention whatsoever now to be fair
little things like the World Wide Web
did happen during this interval which
could have caused some distraction okay
so on to the to the meat of the talk I'm
going to remind you first of Amdahl's
law
alright so Amdahl begins with a very
simple software assumption basically a
limit argument a fraction F of an
execution time is perfectly
parallelizable there is no problem with
scheduling communication synchronization
life is good on the other hand one minus
F is completely serial there's nothing
you can do about it you just have to
wait and therefore you can just model
the time on one core or one processor in
the old days as the serial fraction
divided by the rate of one core one and
the parallel fraction only running on
there on one core is at the same rate of
one I'll normalize the one if you have n
cores then what happens is you only
speed up the parallel part and the net
result is the speed-up equation which
I'm writing somewhat differently than
usual and you'll see why later so this
was coined in 1967 and anthos was trying
to stop the move to saying that
parallelism was all we needed he wanted
to argue still needed sequential
performance in 1967 because at the time
especially with system software you were
35% cereal and that tremendously limited
you and his law has applied through the
mini computer and PC eras so what about
the multi-core era that's the subject of
this talk all right so when we design
multi-core chips we have to confront all
the degrees of freedom that a single
core designer has to confront
instruction wake up select execution
unit blah blah blah ok but you have
tremendous number of additional degrees
of freedom how many cores how big each
shared caches how many banks memory
interface on ship interconnect etc ok
and so the purpose of this talk is to
not lose the forest for all the trees we
want to have a high level model to help
guide us a little bit and so remember
how simple Amdahl's software model is so
our hardware model is going to be
equally simple ok first of all we're
going to say that the chip hardware is
roughly partitioned into two things a
bunch of resources that we can use for
the course and that includes the level 1
cache and the rest and I'm going to
simply simplistically that the rest
doesn't change and will zero in on the
resources that apply to course
that's assumption number one assumption
number two is that that resource you
have for the cores is bounded you can
only spend n resources I'm not even
going to tell you what the unit event is
is an area power cost or some other
factor there's a good argument that it's
going to be power our picture is gonna
are gonna use area because Mike Marty
couldn't draw a power third assumption
is that Marco micro architects are
bright people and if you give them more
resources of this bounded resource they
can make the sequential performance
better okay we're going to assume a
simple base Core normalized to one unit
of resource a base core equivalent with
performance one okay and then you have
the option of having an enhanced core
where you spend more resources our
resources and you get performance get
garnered by the function performance of
our is this assumption true in the face
of things such as I tannic well the
these models are at the sort of
hand-wave level where you don't deal
with specific instruction set
architectures or even languages however
once you deal with those things you can
do much worse than what these models
project so these models are upper bounds
if you will
efficiency frontiers no guarantee you
get anywhere close by the way if you're
really interested in something
interesting there's a Wikipedia article
on Itanium shows sales projections of
Itanium as a function of year it's it is
that funny all right so what about this
performance are what does it look like
well it turns out if performance R grows
faster than linearly with r you should
always enhance the core because it helps
both the sequential in the parallel
phases therefore our equations are going
to assume that the performance grows
less fast and linearly
all right now to do graphs we have to
make some more specific assumptions and
I'm going to assume that the performance
grows as the square root of R so you can
have twice the performance for four
times the resource three times the
performance for nine times the resources
now why are we assuming this well
initially we started assuming this quite
frankly because we want a function that
had no coefficients because we had no
data for which to estimate coefficients
but the amazing thing is talking to
people in industry such as the dec alpha
people and Intel they often use the
square root law you know for initial
studies when they don't have the data so
there is some legitimacy and furthermore
the equations work for any function okay
so how you gonna speed up these enhanced
course not the subject to this talk all
right that's it for the lot pretty
simple and but amazingly you're gonna
see that there's a bunch of implement
eight implications that follow from it
so first of all symmetric multi-core
chips okay so each chip has a bound on
its resources you can spend n resources
on the course each core is going to have
our resources symmetric multi-core means
all core is identical therefore you have
n divided by our cores per chip using a
continuous approximation because n
divided by R times R equals n which is
an example of the high order math and
computer architecture
okay so pictorially if you have 16
resources to spend you can have 16 cores
of course you have to have the other
stuff not drawn you could have 4 4 BCE
cores and you could have 1/16 BCE core
alright so how does this affect
performance
ok the serial fraction 1 minus F uses 1
core at the rate performance R so the
cereal time is the fraction divided by
the the rate the parallel fraction uses
all the cores at this performance hour
rate and so you get this equation right
here and therefore the speed up with
respect to one base core is sort of a
richer version of Amdahl's law where
this performance r takes effect and what
you see is that the enhanced cores
potentially speed up both the serially
parallel parts well not really because
this parallel part over here has this R
in the numerator which is bad because
it's growing faster than this
performance R so there's going to be a
trade-off now if you're mathematicians
were done you can look at this equation
and all all implications can follow from
it now for the rest of us we're going to
plot some graphs and that's going to
require the square root assumption okay
so here's a graph and I'm gonna explain
the axis because there's a bunch I'm
gonna have many graphs like this okay so
the y-axis is speed up with respect to
one base core at the top of the slide we
see we're using six resources for 16
base course and then the x-axis has this
RB CES which is the amount of resources
we're going to spend on each core on our
symmetric multi-core so if we spend one
resource there's going to be 16 cores 2
resources per core 8 coarse etc ok and
finally now this first line shows if our
fraction parallel is 50% and we have 16
cores to spend you ought to spend them
on one core and get the square root of
16 as your speed up and so if you only
have 50% parallel multi-core is not the
optimal thing to do and I wonder if some
client workloads are in this situation
so obviously we need to increase
parallelism to make multi-core optimal
this is not shocking alright if your
parallelism is 90% which is not bad
multiple core becomes optimal it's
optimal to spend two resources on a core
have 8 cores and you get a speed-up of a
disappointing 6.7 and so you know you
have to have more parallelism for this
to really do amazing things ok and so if
your parallelism asymptotically
approaches a hundred percent of course
things are totally wonderful so even
though I'm going to talk later about how
we want to speed up cores let's not
forget the most important thing for
using multi-core is to get the fraction
parallel up and so many people should
definitely target that
and so you know way of thinking about
this is that we have this technologist
Moore's law where you double the number
of transistors per chip every two years
this is still happening you have the
micro architects Moore's law where you
double the performance per core every
two years this ran for a long time but
has stopped largely and we're gonna need
a third Moore's Law for multi-core we're
gonna architects can double the number
of cores per chip but what about somehow
we have to double the parallelism per
chip and maybe we can have some
architectural support and then the net
result might be to double the
performance per chip every two years and
so to me this is fascinating because for
the first time software has to be a
producer not consumer of performance
gains alright back to the hardware so
this is our graph with 16 base core
equivalents ok and Moore's Law is still
running and so we're gonna be able to
build chips that could have 6 256 base
core equivalents so the question is
should you have more course should you
enhance the course or both who thinks
more coarse enhance the course both ok
the answer of course like anything in
life is it depends okay so here's our
case this was 90 percent parallel we had
eight cores for a speed-up of six point
seven if we now go to 256 be CES for
ninety percent parallel you should have
nine course which is only one more than
eight you get a much bigger speed-up you
should spend all your work on enhancing
the course we're now using twenty eight
resources on each core obviously if you
have almost perfect speed-up
you should only spend it on more course
however there are plenty of regimes like
a middling level of parallelism I hate
to call 99% a middling level of
parallelism the optimal thing to do is
increase the base course to three
resources and instead of having 16 of
them have 85 of them okay so as Moore's
Law increases and we
often need to enhance the core designs
so we shouldn't stop figuring out how to
do that like there's a danger of doing
right now
um so ffs really matters I'll just put
these up real fast and you know at 99%
parallel that's really not enough to use
256 effectively and people like Berkeley
and Stanford are talking about 1k course
and they're you're even losing half your
performance with only 99.9 percent
parallel yes
well I'll give you so the question was
am dolls sought I'll rephrase your
question and all software model is
incredibly naive isn't it mark
that it only has a parallel phase and a
sequential phase when in fact you know
you can have Minos who hunts a lot
bottlenecks but you can have imbalances
due to locking that doesn't affect
everyone I'll address that a little bit
at the end but I think unbalanced I'm
not addressing that I'll tell you by the
way a way that Google can estimate F for
its workloads but we will need richer
models right and this is just a very
simple one okay um I wanna just want to
add one thing though that there's this
naive belief that if you're speed up on
C course is less than C that this is a
bad idea that you should never deploy
such a system okay and David wouldn't I
wrote this article a dozen years ago and
we define something called cost up which
is how much more does the C Kors or C
processors cost you relative to a
uniprocessor and even back then on the
SGI Power Challenge which was developed
in one of these buildings right here a
32 processor system the cost up was 8.6
meaning the 32 processor system cost 8
times what a uniprocessor cost how could
this be
well that's because a lot of costs went
into memory and disks and so on the
power challenge you were cost-effective
if you got a speed-up greater than 8.6
okay and going into these multicores you
know I believe you're going to be cost
effective at much lower speed ups
because the cost up the marginal cost to
your whole system of adding a few cores
on the chip is going to be quite low
you'll be cost effective as we as
engineers they're not going to be very
satisfied but that is something that's a
little bit more rosy looking all right
so how might all this evolve with
servers and clients and in the embedded
space well
gonna be largely good news for google so
in the 1970s we're in the Watergate
scandal there was a secret source named
deep throat mark felt who recently died
and he wanted to help people without
actually giving away himself away so
you'd only confirm information but never
provide information and he used to say
frequently follow the money follow the
money
well going forward I recommend follow
the parallelism you know we're
parallelism helps performance and we're
parallelism helps cost performance
computing will flow and servers
obviously can use vast parallelism with
tremendous engineering challenges yes
however on a client and embedded side it
is an open question but there are people
who are going around saying well
computer science advancements going to
end if we can't make multicores work on
a desktop and I think that's totally
false
you know it's guaranteed to work for
servers once we engineer it right and if
clients and embedded don't work well the
center of gravity of the computing
universe will move ok so that was
symmetric chips and some software things
what about asymmetric multi-core chips
also called heterogeneous okay symmetric
required all cores to be equal what
about having enhancing some but not all
the course okay so for Amdahl's simple
assumption of course it makes sense to
only enhance one core and leave the
others based course and what are the
techniques for doing that they're not
the subject to this talk so how does
this affect the hardware model well
we're still bounded by n resources for
the course we're going to spend our
resources on this one bigger core and
that leaves n minus R for the other
course they're all based so there's
going to be n minus other base course
and so for a chip of 16 here's our
symmetric one you get a 4 or 4 BCE
course and now for asymmetric you can
have one for BCE core and 12 1 BC of
course so is this chip over here going
to do better or worse than that one
actually it's not the pens it will
always do better unheard a small
assumption that it's not hard to develop
a software for an asymmetric joke ok so
how do we analyze this with this model
the serial fraction is the same it's the
serial time the serial fraction divided
by the rate on the parallel side you're
going to have one core going really fast
n- core is going at rate 1 or assuming
that load and balance is not a problem
for you brilliant software people and
you get a speed-up according to this
equation ok we go back to our square
root assumption first of all before I
look at the lines let me define the axis
it looks very similar but now instead of
applying these RBC ease to each of the
cores you're only applying it to one
core so if you only give it one it's
still symmetric but for example for
leaves one enhanced core with 4 and 250
to base course and so forth ok so how do
a symmetric and symmetric speedups
compare I already gave it away the
asymmetric ones are always better so
here's our symmetric one again and so
for example at 90 90% parallel we got a
speed-up of twenty six point seven and
for the asymmetric we more than double
that speed up to 65 yes
I'm assuming that yes we architects can
enhance that mega core and that you
software people can use that mega core
and part of the reason I'm assuming that
we architects can do that is I'm trying
to make the case to people like the
National Science Foundation that they
should still fund things who are trying
to enhance course there's a current
belief that well all we should do is
deploy little dinky cores and that will
solve the problem yep
the micro cores are just one unit the
small cores are all base courts for for
Amdahl's simple software assumption it
it makes sense to spend all the all the
enhancement on one core and leave all
the others base now I doubt that's
really what you want to do for real
workloads yes
right oh right so the so this is the
same as before in the sense that the the
equations use any analytic or empirical
function performance and the graphs are
going to use square roots so we're going
to assume that all these enhancements
are done and you're you're only getting
the square root back now it's still an
open question whether you can't even get
the square root back for say a chord
that's using you know 128 resources but
or okay
all right so how does speedups compare
so this was 6 26.7 and with the
asymmetric you more than double that it
also works when you're much more
parallel so if you're 99% parallel your
speed-up if you could do all this goes
from 80 to 166 doubling okay so
asymmetric offers greater speed-up
potential than symmetric and in the
paper you can see that as Moore's law
increases and the resources for the
whole chip the benefit of asymmetry
expands okay and so the flip side of
this is that when you go back to more
modest chips for weight chips
the benefit to asymmetry contracts and
that's one of the reasons why we haven't
seen too many asymmetric chips because
the potential benefit is small you know
the practical issues of dealing with are
not worth confronting but I think that
could easily change now one example of
an asymmetric chip when I gave this talk
IBM pointed out that you know cell is an
asymmetric chip that that may or may not
be a positive example all right so
there's all kinds of software issues for
a summit and asymmetry how do you
schedule how do you manage locality how
do you synchronize and what level do you
do this at you know the higher level you
go up the more you potentially know the
lower you do it at the
or you leverage if you get it if you can
do all this in the hypervisor all the
programs running on top the hypervisor
benefits and there's lots of issues and
we'll see we'll talk a little bit about
applying this to a warehouse scale
computer these issues are going to apply
at the warehouse scale level also okay
dynamic multi-core chips are the third
kind of chip children and computer
scientists like to have their cake and
eat it too
so what if you could have an all-in
based course all the resources in the
parallel phase and then you could
somehow harness a bunch R of the
resources together at no opportunity
cost in the cereal face so here's one
sort of picture of it you got all these
resources in parallel mode and then you
go to sequential mode you somehow gang
them together and they they act like a
big sequential processor and so forth
now this is one way to do it there's
another way to do it which i think is
actually likely the first way is
possible this one I think is likely so
assume that your limiting resource is
power ok
you could only spend so much power
because that's what you can bring in
that's what you can cool and you're no
longer so limited by area ok this assume
we're unconstrained by area then you can
build the following chip you can build a
chip where we power up the little guys
in parallel mode using our full power
budget of N and when we get to
sequential mode we power up a
potentially big sequential processor and
we use our whole power budget there and
so forth and this is pushed by grease OE
and some of his students now the
interesting thing is that both of these
models are both of these kinds of chips
fit the EM they'll model the same way
that in the parallel mode there's no
opportunity cost you get to spend all
base cores on the parallel mode and then
in a serial fraction you you get as many
as you can put together up to hour
that performance our which in the
equations is the arbitrary function in
the graphs is the square root okay so
what does this look like the equation
looks like this what do the graphs look
like well here's our asymmetric graph
once again and here was our 99% parallel
and we had a maximum speed-up here of
166 and you'll notice if you spent more
resources than that then the than the 41
it was not optimal okay in this dynamic
thing it's not going to happen there's
no downside to spending more resources
if you can so what we'll see instead is
that the curves always keep going up
okay and so potentially at 99% parallel
you could increase your speed up to 223
there's a slight problem we just don't
know how to do this but the model says
that it's possible so dynamic
dynamically adjusting the resources has
greater potential than asymmetric so I
should are I argue that should be
studied as well it has all the issues of
asymmetric only worse and I actually
believe that dynamic chips because of
the limit of power and error becoming
less precious are likely not everyone
agrees with that though all right so
let's do some caveats and wrap up and
first I'll begin with some comments on
warehouse scale computers so I recently
read a draft of Barroso and holes
warehouse scale computers which is this
synthesis lecture in computer
architecture I edit the series and these
guys are trying to explain to the world
a little bit of how you guys do business
and so I'm reading this I was trying to
think about how with what's happening
there apply how it interacts with this
Amdahl model well first of all within a
warehouse scale computer you have nodes
the nodes are going to be multi-core
whether you want it or not and so there
there could be some issues within that
chip you know do you want for your
workloads static and dynamic what
be the asymmetry what would be the
implications of that if some cores were
faster than others on the chip
we probably got to extend the model
because you're gonna have to be very
concerned with thrashing the last level
cache and the memory bandwidth if
everything on a given multi-core chip is
doing independent stuff you're not gonna
get anywhere near the course potential
but a more interesting question is what
happens if you pop up and say let's
think of what was our node what was our
was a core in our model so far let's
make that a node so let's let's pretend
that we're gonna do this for a warehouse
scale computer and we have these notes
and Google spreads its work among the
nodes a query fans out does some
parallel work and fan-in so what are the
implications to looking at this computer
system with the model well you still
have to adapt to some performance
function if the nodes of your warehouse
scale computer you want to buy a faster
node you know it's probably not going to
get performance that scales linearly
with the cost you're gonna pay some
premium maybe it's the square root so do
you want to do this do you want your
nodes to be symmetric or do you want
some nodes to be faster either
statically or dynamically what's the
best way to achieve this a symmetry what
about Amdahl's model you know we were
dismissing it before but does it apply
you know the bad news here if it's 99%
parallel your speed-up is limited so an
interesting question is what are the F
the fraction parallel for Google's
workloads because you could call this
the G well fortunately there's some
interesting work on the subject and it
instead of looking at the fraction
parallel it looks like the fraction
serial so kind of like sometimes it's
useful to look at miss ratio as one
minus the history show so let's let's
zero in on the fraction serial there's
this great paper by carp and flat which
is also a Wikipedia article which says
that you can with some quick math from
Amdahl's equation it's just simple
algebra you can compute the frat you can
to make the fraction serial by looking
at the time on P nodes divided by the
time on one node and then subtracting
one minus P and you can read the
denominator and this is nothing magic
it's just algebra and the interesting
thing to do then is to say we can now
empirically measure this fraction serial
for varying P now you say to yourself
mark what are you drinking here the
fraction parallel is not supposed to
change well yeah that's in Amdahl's
model right but for real software it
could change and one of the things you
can look at is what is the fraction
parallel at a given p and what happens
as you increase the number of nodes in
your wafer in your wafer warehouse scale
computer is it stable in which case
Amdahl's model is doing a good job does
it increase does it decrease if it
decreases actually that's kind of a
bizarre result it usually means some
super linear caching effects but I think
there's a lot of interesting questions
there for understanding your workloads
what is Google's fraction parallel
alright so here are our three equations
putting them all in on one place and we
see that you know the sequential sides
are all the same and then the parallel
sections differ by enhancing all the
cores enhancing one core or basically
paying no opportunity cost for enhancing
course all right
I'm an unusual academic instead of I
will actually admit that this work is
not perfect so you could make some
charges about the software model that
the serial and parallel fractions are
not totally serial not totally parallel
yes this could be extended to tree
algorithms and bounded parallel so you
can say that this notion that you do the
same thing when you have a larger
machine is absurd when you have a bigger
machine you actually solve a bigger
problem maybe in the same amount of time
that's true for weather prediction right
you need the prediction for the six
o'clock news bigger machine you get a
better prediction this is called week
scaling by
Gustavson I think prudent architectures
have to support strong scaling and
there's also an argument that at least
within the multi-core chip weak scaling
is hard because you can't just scale the
memory capacity and memory bandwidth
arbitrarily large
now what about synchronization
communication and scheduling you can
extend for this software challenges for
asymmetric and dynamic are worse
absolutely true and then finally my
favorite argument is future software
will be totally parallel see my work and
I'm skeptical even your MapReduce is not
completely parallel right you have to
fan things out you have to do some work
you have to fan it back alright so this
is easy criticisms for me to make
because these are really criticisms of
gene Amdahl who's now retired what about
our Hardware model so it's naive to
bound the cores by one resource
especially area well we're not really
doing area it can be any kind of mix of
things it's naive to ignore the off chip
bandwidth and the effects of last level
caching absolutely true ok why did we
ignore it then it's because we didn't
know how to characterize workload so we
require too many coefficients to do it
not that it wasn't important it's naive
to say that performance grows as a
square root of the resources well that's
only the graphs the equations can use
any function including an empirical
function and finally mark your you're
running things in two realms where we
have no idea how to do this well that's
exactly the point right we should this
shows the need for this if we want to
effectively use multi-core so we should
figure out how to do it so you can go to
our website and you can actually play
with some Java where you can put in your
own functions to see what the graphs
look like but more importantly computer
scientists and and our architects should
work together to try to make this third
Moore's law happen and double the
parallelism and you know ultimately you
know
we don't talk about digital computers
anymore Digital is implicit and the same
thing has to be true with parallel we're
only going to be successful if in a
couple decades we talk about programming
and computers and though that is
parallel programming and parallel
computers so I'd love to see where we go
up to a thousand base core equivalents
and we get a speed-up
near a thousand and we certainly can't
do this today but you know if we don't
do some work you know we want to try and
succeed or fail we don't want to fail by
not trying so in this talk I have
developed a corollary Amdahl's law which
is a simple hardware model that just
goes from some very basic assumptions
about fixed resources for cores and sub
linear gain with resources and from this
several interesting conclusions follow
and you know the second one is really it
was something that people were really
going away from they thought we don't
need more core performance and I think
we do need more core performance we just
have to figure out how to do it with
really efficient power a lot of our high
end cores use a lot of content
addressable memory and they're not
efficient in power I'm optimistic that
ideas are out there and I think we
really have to seriously consider these
asymmetric designs not with just one
bigger processor but with many depending
on the workloads and there'll be many
problems but the potential gap is so
large that I think they're worth
confronting thank you
yes
okay so there's there were a couple of
questions there and let me just rephrase
them one is the simple one is did you
look at other functions besides the
square root of R and we definitely
looked at other functions such as
different changing the value of the
radix of the yeah we looked at some of
that and for modest changes it doesn't
change the results too much a lot of
people but I didn't look at it a
thousand I don't want to say this we
looked at some variations of it it
didn't seem to qualitatively change
things too much but by the way I don't
think you should take these qualitative
results at all seriously right I mean
this these are this is just to
illustrate some some larger trends night
to the other point which let me rephrase
that as wait a second mark you know the
only thing you're trying to run on this
bigger processor was something that's
pretty serial how are you ever going to
get speed up on it that's a legitimate
question and I don't know the answer and
maybe we can't but
right I have no idea how to do this the
the idea was to show the need that it
should be done and you know off if there
is a there's a need for it to be done
then people will try pretty hard and
they may fail because it can't be done
but we hate to have them fail because we
didn't try yep right and so the so
you're the so the statement was you know
some of these processes we hit we know
how to build some big processors we know
how to build some small processors but
we're only used that's only today's
designs there were more possibilities in
particular on the big processors kind of
what happened was we were building
processors caring about performance only
and then power was considered sort of at
the end and then suddenly we've had this
pullback
to more modest course and I'm arguing
that we should revisit building those
higher performance processors when power
is a first-order initial consideration
and it's there may be solutions out
there
so so if the cost function is the
logarithm rather than the square root
then yeah the diminishing returns will
get serious pretty seriously so you can
go to our website and just plug in
logarithm and see what the graphs look
like
okay so the observation was that the
processor chips have swallowed up many
many kinds of chips except principally
not main memory I'm skeptical that main
memory will be absorbed except in those
instances where you want to live with
relatively small main memory our our
ideas always seem to want more main
memory and the values when you want a
lot of capacity of specializing the
semiconductor processes to logic on one
hand India Rama on the other hand seemed
pretty pretty substantial but I can
totally believe that we could get you
know mobile phones of the future that
have that have one chip the other thing
is there's many engineering games to be
played to improve the bandwidth between
a few memory chips and the processor
chip improving the latency is very
difficult but improving the bandwidth
there are there are many difficult
engineering techniques but sort of
normal difficult engineering
so the the the question is that a lot a
lot of architectural research has
treated like instruction level
parallelism is very different from
thread level parallelism well I was
basically using Amdahl's law which says
that there's you know there's thread
level parallelism on the one hand and I
guess the the the the fact that you can
actually enhance the performance of a
core is assuming that you're doing
something which is but has to be
partially exploiting low-level
instruction level parallelism whether we
want to combine the two in the future
that's well beyond the scope of this
simple model to answer yep
right so the question is instead of
talking about the cores on a multi-core
chip why not think about Google's issue
where in some sense you know what what
is a core and this talk corresponds to a
computer it is a node in a warehouse
scale computer well I have thought about
a little bit on airplane out and I think
the lot of the arguments that are made
in this talk will also apply to Google
situation that you will want to have
some enhanced notes now enhance nodes
will not be that Google enhances them
necessarily it means that Google will
pay you know a price premium to have
some higher performing nodes to get the
sequential performance when you're near
the bottleneck of your work or did I not
answer your question
right so that so you're saying okay so
one of the questions that Google wants
to answer is should you put more course
on a node or have more notes yeah that's
a question has to be answered you know
it's a hierarchical question beyond the
scope of this simple model and you know
the trade-offs are going to involve what
the cost-effective curves look like but
I think as you get large enough there's
going to be pressure to go for this
asymmetry and they'll be pushback from
software because obviously there there
are tremendous challenges I don't have
any magical answers at this time yep
right so so the question was these
results use AM dolls law where the
fraction parallel is fixed and you say
that the it's likely that the fraction
parallel will go down one way that'll
happen is John goof Stinson result where
he says basically when people get bigger
machines they compute for the same
amount of time on a bigger problem and
the the parallel side grows faster than
the sequential side you know that's true
we're in salad days and I think it
depends on the problem for architects I
got to believe it's not true for all
problems and so we have to architect
chips for the more pessimistic and bail
assumptions for Google that's a good
question for you to ask as you get to
more applications and more you know as
your workloads grow what does happen to
the fraction parallel I mean if you got
a fan in on a tree and fan out I don't
it's not going away
I haven't looked at that specifically of
course that cost up argument you can
treat cost in any dimension it doesn't
have to be dollars it could be power but
that the basic thinking that you what
you said I guess I didn't repeat the
repeat what you said Louise's question
was if you look at the cost of stuff
have you applied it to power I think the
basic sentiment definitely holds right
is that if you have all this stuff
sitting around burning power
it might be worth being locally
inefficient in the core to make sure
you're utilizing all this other stuff
which is burning a lot of static power
and to the extent which you can scale
back that static power which you're
arguing for then there's less pressure
and but it's definitely something I
think that should be a first order
consideration not something you think
about after the system is deployed</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>