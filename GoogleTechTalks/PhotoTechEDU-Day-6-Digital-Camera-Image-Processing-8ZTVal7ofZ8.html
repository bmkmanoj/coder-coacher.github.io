<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>PhotoTechEDU Day 6:  Digital Camera Image Processing... | Coder Coacher - Coaching Coders</title><meta content="PhotoTechEDU Day 6:  Digital Camera Image Processing... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>PhotoTechEDU Day 6:  Digital Camera Image Processing...</b></h2><h5 class="post__date">2007-10-08</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/8ZTVal7ofZ8" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay today we're going to talk about the
image processing pipelines and digital
cameras it's kind of a lot of the core
of the technology and digital
photography is how you process the
images and there's a whole lot of
material there so again today's lectures
is kind of a quick overview of that and
we'll have a lot of open topics we can
talk about at the end of stuff to maybe
get into later there's a number of books
here from which I've drawn ideas
inspiration data and figures and the one
upper left there by hunt is sort of the
Bible on color that tells you all about
how color works and why color matrix
Singh is necessary and strategies that
have been employed in different systems
like television and photography and so
forth over the years you've seen the one
in the middle several times before I
keep bringing that one up the one on the
right digital color imaging bye-bye
Sharma is it's got a really good chapter
on image processing pipelines from which
I've drawn a few figures and i'll be
talking about that one and then the one
on the bottom technical introduction to
digital video by charles poynton is a
good source of data how it's got all the
all the matrices you need to represent
different color space standards and
things like that so this book the Sharma
book has this chapter by ken pearl ski
and Kevin Kevin Spalding these are a
couple guys that I got to know a bit on
some of the standards committees and
they've done a lot of work to sort of
develop good methods for image
processing and to propagate those
through through standards and books and
writing and stuff and that's a lot of
good details in that book so one of the
one of the one of the figures are two of
the figures in that book are these
simplified pipeline diagrams that show
you what happens in a in a consumer
camera and in a professional camera the
difference between them is what's the
output basically so the consumer camera
outputs jpeg files and the professional
camera in this in this conception
outputs raw files so the question is
really how do you organize the different
stages and where do you do storage and
so on but in terms of what the
processing is to make a picture it's
more or less equivalent in the in the
two cases there's a lot of optional
steps and
the number of steps in a typical
pipeline is actually greater than what
you what you see here and the
professional cameras are likely to have
more steps than the consumer cameras and
so on so we'll get into the what some of
those other steps are I want to go back
and start at the sensor where we were
last week and talk about what what goes
on on chip before you get the data in
the high end there's a on chip amplifier
but not the A to D converter and in the
lower end the ATD converter is often
integrated on the same chip with the
sensor as well these amplifiers can have
a lot of different features for example
the correlated double sampling I talked
about which is a thing that that removes
certain kinds of noise reset noise ktc
noise which may be one of those or in a
CMOS amplifier that may be one per
column you can have multiple amplifiers
for example for the two sides of a big
chip or some of the new sensors in the
new Canon camera i think has eight tap
readout something like that so it's it's
like it's reading out the sensor as
eight sensors in parallel or on the
fobian chips you tend to have a channel
for each of three colors or you can have
more you know different ways of
organizing output taps and amplifiers
some of the chips have an optional or
it's not optional some of the chips have
and some don't have a programmable gain
associated with their amplifier some of
them are fixed gain so the the software
controlling the camera can increase the
gain or decrease the game if that
optimizes the performance for different
iso speed settings and so forth and in
many cases there's also an off-chip
amplifier between the sensor chip and
the ATD converter so next thing that
happens is you convert those analog
signals to digital some of the ADC chips
are sort of complicated analog front-end
chips that they call them have a thing
called a black clamp that that looks at
the leading pixels on each row which are
coming from a light shielded area on the
sensor and they take the level there as
a black reference and then they try to
try to get black in the right place my
experience that's that's not all that
common it's you do find it on a 2d
converter chips but it's not all that
common that it's actually used in
cameras because it has certain problems
in terms of the way the way it adapts it
can add some row noise and things like
these converters typically will go at
least 10 bits in the low end and 14 bits
or more in the high end they're
reasonably linear around 10 bit
linearity the extra bits are not usually
worth a whole lot but sometimes they are
and at the ad converter you sometimes
will immediately follow by a lookup
table and that can either be to to
linearize the response in a CMOS gypsy
moths chips are not as linear as C CDs
because the charge to voltage conversion
is being done on the capacitance of a
diode which is a nonlinear device so you
get a little bit of a compressive
response there and for some of the
subsequent steps you want the processing
to be pretty linear so you may have a
lookup table that linearize ha's the
response so that the digital numbers
you're working with are in a linear
relationship to the amount of light
instead of a slightly nonlinear
relationship sometimes you'll also have
a lookup table there for other purposes
for example to impose a non-linearity to
make the data compress better to fewer
bits or you may go straight into a
lossless or nearly lossless encoding to
store raw data at that point so here for
example is a is a case where you you
might put a gamma curve right on the raw
data coming out of a sensor if for
example you have a say a machine vision
camera of some sort that wants to
communicate back to your PC you over an
8-bit channel but it has a 12 or 14 bit
a 2d converter you can ask what's the
best way to squeeze that data down to
eight bits and there's lots of fancy
clever ways you can compress the
bandwidth down but if you really just
want eight samples 8-bit samples through
a lookup table what's the best lookup
table and the the thing that works
pretty well there is to make the
quantization steps at all parts of the
intensity range be in the same
proportion to the noise at that
intensity and so the the slope at the
bottom end of the curve here is
determined by the the read noise of the
sensor you want a lot of counts per
light intensity when the noise is pretty
small it's just the floor the noise
floor there
which is about 30 electrons in an
example sensor and at the high end where
you're up around say 58,000 electrons
the square root of that is the number of
electrons of shot noise around 240 so
it's eight times as much noise up at the
high end so you can use eight times as
big a quantization step and get a sort
of equal amount of degradation to your
data so here with with only eight bits
you can keep the quantization step size
about the same size as the noise which
means it doesn't really add very much to
the variance over the entire range so if
you if you need to cram 12 or 14 bit
data through an 8-bit pipe that's one
very easy way to do it it's okay so this
graph shows the I'm sorry I didn't label
it on the in fact I clipped the numbers
off the axes because they weren't right
on the horizontal axis here is the
number out of the aid to D converter
that's presumed approximately
proportional to light intensity and on
the vertical scale are the numbers out
of the lookup table which go from 0 to
255 so that's an 8-bit code so the point
is that between codes is not much light
down at the dark end and between codes
is is more light lower slope this way up
at the bright end so if this was a if
the you covered the other way if you do
the opposite of what I just said but
this is a compressive curve which means
that the slope is is reduced at high
intensity the slope being reduced means
that the increment horizontally and the
light domain is greater right so more
light change to get one code when it's
bright and less when it's dark because
the noise is less in the dark if there
was no read noise and you had just shot
noise this curve would be a square root
curve and then to reel in your eyes from
8-bit back to linear intensity you'd use
a squaring and that's pretty close to
what people use for gamma curves but you
can get a an actual optimum curve in
fact the optimum for the case of just
shot noise plus an independent read
noise is a square root curve that's bit
offset so that the the origin is is
moved up a bit you can you can find
parts and catalogs like yours from TI
they're talking about their did
still camera engine and they show in the
top of the figure the thing called a
analog front end which is any of various
other parts you can attach there that
does the correlated sampling what they
call automatic gain control an A to D
converter I'm not sure what they mean by
automatic gain control here typically
there's a programmable gain amplifier
under control of your software but
automatic tends to mean it automatically
adapts to signal level and I haven't
seen cameras that work that way it's
it's automatic at some point but it's
you know that that automatic control is
deferred to the software it's not in the
analog friendship the and then the
digital still camera engine chip there
has the image processing and image
compression and that's the bulk of what
we'll be talking about and in today's
pipeline stuff yeah question they showed
a CCD and correlated double sampling in
the on front end is crawling doll set
them down the CCD student we yes well
the CCD the the analog output can be
alternating between a measurement of
zero charge bucket and a charge bucket
with a signal and the 0ther is set by a
reset so that you can do the off-chip
cancellation of that reset noise this is
a this is all the charges are going
through a single path and you can do the
CD s off chip with the with the CMOS you
can't do it that way you have to do the
CD s in the column amplifiers because
the charge to voltage conversion was
done back there but actually you don't
have to depend there it's complicated
but typically the way it's done okay so
there's a number of steps that are
important to get the black level in the
right place on a sensor and this this
tends to mess up pictures if you don't
do it right so I talked about the black
clamp in the A to D converter but that
that doesn't give you an absolute black
reference very well because the A to D
converter doesn't know where the charge
0 was of the of the sensor unless unless
your CD s chip has a good absolute zero
in it in which case it might work very
often you have a step called dark frame
subtraction especially with CMOS sensors
because with CMOS each one of the pic
soul sensors itself has a transistor
with an offset or a drift of flicker
noise and so on that that moves its zero
around so you get these fixed pattern of
fixed patterns due to the transistor
offsets and if you do it you capture a
dark frame and an image frame and do the
difference between them those will
cancel out and then there's what I call
dark level drift adjustment if if
between capturing the dark frame and
capturing the light frame the
temperature of your sensor changed or
something because so you keep it cold
normally and every time you turn on the
camera to take a picture it's warming up
while you capture these two frames and
there's a couple degrees difference that
may be enough to to offset the two
images with respect to each other so you
can you can detect that easily by
looking at the dark shielded edge pixels
and make sure that after you do dark
frame subtraction those edges go to zero
if they don't go to zero on average you
sense that average and correct for it
and then the the what is often a fairly
large effect in images is the glare or
flare that even talked about optically
and that's a that's an offset due to all
the stray light in your in your camera
that's hitting the sensor that might be
one percent or so of the average
intensity of your image so you might
have another step that measures the
average intensity of the whole image and
subtract off one percent of that to try
to get the black parts of the image near
zero
another thing that's done pretty early
on the on the raw data is to adjust the
gains of the pixels to account for the
vignette of the lens or the angle the
angle dependent quantum efficiency roll
off of the sensor so you have sort of a
slowly varying gain across the image
that that's typically like a gain of one
in the middle and you boost the edges
that have fallen off you can represent
that that 2d varying gain as either a
polynomial in x and y up to some
moderate order fourth or sixth sort or
something like that or you can represent
it as a as a table typically a sparse
table over the image you could also
represent it as a gain for every pixel
and that would allow you
generally it's not ludicrous to store
everything you know and that's what they
do but well everything you know is
really just a little bit of metadata on
top of the image raw data so you get the
you get the data from the sensor that's
the bulk of it multiple megabytes and
then the metadata is just some some
strings about the camera settings or
maybe the light meter reading and focus
measurement if you have that and but
yeah generally the metadata will store
pretty much everything the camera knows
let's see so after after you do this
spatially varying gain correction in the
raw converter typically or in the
somewhere in the pipeline and the
consumer camera now you've got numbers
that are proportional to the light
intensity of the scene that you took a
picture of you've got black at number
zero so everything is is just
proportional and that's a that's a good
place to be then you can start to do the
real processing here's a here's a figure
out of a paper I found online there's a
reference to it in which they they
illustrate a very useful concept when
you're thinking about designing
pipelines for digital cameras you you
kind of want to model what the camera
does and then undo it that is like pop
the stack go through operations in
reverse order to undo what the camera
did so they've they show here an input
processor that's undoing what the
imaging array did and that's like
linearization and spatial gains and then
they show this color engine I'm doing
what the color filter array did so
that'd be like interpolation to try to
get back to an estimate of the color
image that was that was here before you
went through the filters and then they
show this aunty been getting well in
this case they've done the you know they
put the vignette and gain in association
with the lens of course you could do
this in different orders but
conceptually you want to think about
undoing what the stages of your camera
did so the they've got the camera
pipeline going right to left and then
the processing pipeline and
corresponding stages from left to right
they haven't shown all the stages all
the details here that you're going to
want to do but this gives you an idea of
a good way to think about how to undo
some of the the peculiar or non-ideal
effects of your your optics and your
sensors
so forth so another another important
step that you tend to want to do fairly
early in the pipeline is to figure out
what what color off of your sensor
represents white that is what what ratio
of red to green or ratio of well you
need you need at least a pair of ratios
like a red / green ratio and a blue /
green ratio is a one way to characterize
it what ratio is there correspond to the
colors that you're going to call neutral
in this image and it varies with what
you're taking a picture up because when
you're outdoors under you know bright
sunny sky that's about a 50 500 Kelvin
illuminant it's a different color than
when you're in the shade when you got
more like 6500 Calvin you got a blue or
light or if you're indoors under
incandescent light around 3,000 Kelvin
you have a very orange light but in all
of those conditions if you're taking a
picture of something you want your
photograph to show the color in a kind
of a natural way if you take a picture
of a white piece of paper you want that
to reproduce as white in the photograph
or pretty nearly so the camera has to
figure that out pretty early in the
processing it has to know what what
color that the sensor measured are we
going to call white for this particular
photograph so you can get that from an
automatic white balance calculation and
I mentioned here gray world assumption
plus plus and the the the idea there the
gray world assumption is you just you
look at all the pixels in your image and
you assume that white is the mean of all
those so now that you just sort of
distribute all your colors in such a way
that the mean color is neutral it's not
a it's not a great or wonderful way to
estimate white the you know the
illuminant color in an image but it
turns out it's it's very often as good
as the much fancier methods and so the
plus + there represents the fancier
methods which try to take this gray
world method and improve on it for
example you can detect large areas of
sky and large areas of green in the
foreground and things like that and try
to D wait those in the average and
thereby get a better estimate of where
white might be or you can take a manual
white balance setting if your if your
cameras been calibrated it's got
settings on it for say day light and
shade and incandescent and so on where
you've predetermined at the factory
if you take a picture of a white thing
under this illuminant here's the color
ratios you get so you just you set it
and then the processing can proceed
knowing what you've told it the
illuminant color is now at this point in
the processing pipeline you can apply
channel balancing gains that is you you
apply some gains to the sensor red green
and blue channel until the ratio of red
degree and blue to green and so on are
equal to one for the thing you're going
to call white question celebration is in
factory how does the picture know under
what like emissions education so that's
if yeah if you're calibrated in the
factory how does the picture know the
light conditions that's what I meant by
manual white balance setting here so
most the cameras have a setting where
you can declare what the illuminant is
at least the high-end cameras do so you
go into white balance you either set it
on auto or you pick sunlight or cloudy
or incandescent or whatever so if you've
declared what the illuminant is the
camera doesn't have to try to figure it
out it uses the calibration data to tell
it what those ratios are
right some cameras have a light sensor
to look at the lighting so if you if you
have like a little light meter a little
white ball on your camera that light
comes into from all directions it
basically just does an average of the
scene that you're looking at or of the
light that's falling on the camera it's
not very much different from averaging
the light in the picture that the sensor
captured and sometimes it could be much
worse because for example you're taking
a picture of a shady scene but you're in
the Sun it's going to be really messed
up or vice versa you're in the shade but
and getting a lot of shade light but the
subject across the street is in the Sun
so those sensors they can be useful but
they are not necessarily better than the
gray world assumption at giving you the
the right answer over here yeah why is
an 18-percent gray card eighteen percent
grade that that's a that's a very
interesting historical question and I
don't have the details of the answer
here there are papers I've read that
have gone into that in considerable
depth and I can't answer it okay so once
we figured out what what color were
going to call neutral we can apply the
channel balancing gains as I mentioned
so that now you have in your relatively
raw data still you have if you have an
RGB value where the three numbers are
equal that represents a neutral color
that's a convenience for some of the
stuff that comes later you don't
necessarily do that at this stage but
some pipelines do and then there's a
step of optionally clipping highlights
to white if you have applied gains such
that equal values means neutral then you
clip it everything to the same level
will clip to white a straight clip is
not the best way to control highlights
but if you if you don't do something
here then regions that are too bright in
the sensor starts to saturate are very
likely to have one channel saturating
before another and they're going to the
the apparent color if you process it
without any care is going to be way off
neutral and you don't really want your
bright highlights to come out pink
purple blue or whatever you want them to
come out white so you have to do
something about the overexposed
highlights yeah Jim
once again or animal yeah so we're way
past the A to D converter so we're doing
all this digitally in older cameras it's
can it's conceivable that you could do
these gains and clipping in the analog
domain I'm not aware of any that do it
that way but there probably were some in
the past yeah Francesca yeah so is the
white clipping done as part of the D
mosaicing and yes as it says here second
line from the bottom optionally include
highlight neutralization so good
question if you haven't clipped the
highlights already in fact when you when
you've got only one of red green and
blue at each pixel you can't really do
highlight clipping on the pixels
independently because you don't you
don't have all three components and so
you really want to ideally integrate
that with the D mosaicing somehow so
this is where you take where data that
only has one color measurement per pixel
and bloat it into three so it's a it's
an interpolation within each color plane
or it's a you know extrapolation of data
to to fill out the the RGB format that
you're trying to process toward there's
a lot of different complicated algorithm
choices here generally what they do I
this last week when I showed some
aliasing examples is that they they tend
a sacrifice resolution of chroma detail
in an attempt to get better luminance
resolution that means if you if you
assume that the chrominance the color
ratios if you like is only sort of
slowly varying then you can estimate it
over neighborhoods and if you've got it
correctly estimated in a neighborhood
you can use that to get better
resolution of luminance close close to
as much as you would have gotten with a
monochrome sensor if you've got the
color correctly estimated you you you
don't have enough information however to
actually get the color correctly
estimated and whenever there are sharp
chroma edges if if there's a sharp edge
and you assume it's varying slowly than
you will induce aliasing artifact
X into the luminance that you've
estimated so there's all kinds of tricky
things here about trying to estimate
where the edges are and respect them so
you don't corrupt them and so forth this
is a figure out of the the / all ski &amp;amp;
Spalding chapter again that illustrates
one simple example of an edge detecting
algorithm where they they go through
some logic on the neighborhood comparing
pixel differences to thresholds to try
to decide if there's an edge there and
depending on whether there is in which
direction it is they do different
averaging of pixels sorry so I'm not
going to get into the details of these
algorithms I just want to know there's a
lot of still active research in this
area trying to find out one of the best
ways to infer the missing values yes hi
is that RGB sensor
lock live in all of your
hey that's a loaded question but
absolutely question still get into that
here yeah it can survive I think so I
mean people are reasonably tolerant of
the the small kinds of artifacts that
you get off those cameras and if you do
have enough pixels on a strong enough
anti-aliasing filter the the images are
acceptable so compute and apply the
color matrix so now I mentioned this a
couple of times that there's sort of
among the the amateur aficionados of
digital imaging there's not really much
awareness of this concept of color
matrix thing but it's it's fundamental
to getting decent colors out of a sensor
is it you have to go through a three by
three transform to convert to a an
appropriate color space part of what
you're doing there is to map the white
point of your scene into the white point
of the output color space for example if
you use srgb that space is defined in
terms of a set of primaries on a
reproduction white point where they say
the reproduction what white point is d65
which which means approximately the
color of a 6500 Kelvin black body
they've defined it that way because
they're thinking about monitors and
reproducing pictures on monitors but the
the scene you took a picture of might
have been in 3,000 Kelvin or five
thousand Kelvin or whatever so you get
it you got to map the white point and we
talked about how to do that with
changing the gains but that that only
gets the white point mapped and then you
also have to map all the colors around
that and keep them in a decent
relationship and that's what this whole
matrix thing thing is about and there's
no there's no perfect color metrically
accurate way to do that because in in
colorimetry you can quantify the exact
color of what you took a picture of and
what you're reproducing but there's
nothing in colorimetry that tells you
what the relationships between those
should be if the reproduction white
point is different from your
color so you have to do some
psychophysical experimentation to find
out good mapping algorithms and those
usually get reduced to a three by three
color space transformation at some point
so you want to preserve relative colors
and preferred color rendering as so this
preference idea is not just getting the
colors mapped right but it's getting
them rendered in a way that you like and
that that means usually for most things
you want you want your photographic
image to have more color saturation than
your original scene you want it to
really pop many cases there's an
intermediate color space used for
example the LMS space which stands for
long medium and short referring to the
wavelength sensitivities of the cones in
the human visual system or the rim RGB
which is a reference input media metric
color space that Kodak developed for
this purpose so you in these
intermediate spaces if you just scale
the three primaries you get a pretty
good color white point adaptation so if
you transform into one of these spaces
do just a diagonal transform that is
just scale the three primaries to get
white where you want it and then
transform back to the space you want to
be in that's a reasonable way to do
white point adaptation yeah Lance right
whatever is made sometimes about being
able to reproduce color more accurately
if you have four or five primaries
is there any leverage and using two
different greens in a layer pattern to
try to get better color yeah it's the
questions let's point out that the Brian
wandel at Stanford has said you can get
a better color accuracy overall by using
more than just three primaries for
example four or five and could we use
could we get an advantage by using two
different greens and the in the bear
pattern to get more color accuracy and
yeah you can in fact that was what Sony
tried to do with their sensor that
replaced one of the greens with what
they called emerald it's a slightly
different spectral sensitivity curve
there's a lot of problems in doing that
like all the standard work on D
mosaicing algorithms doesn't apply
anymore if you're trying to get better
chroma information you're gonna have to
give up some luminance detail to do it
are you going to risk more kinds of
aliasing so yeah you can get better
color but it's not easy and it's not
often done at gym this is where this is
linear that I'm talking about a
completely linear transform it doesn't
necessarily have to be linear but it
almost always is because that's what's
understood so here's an example of a
matrix in operation if we if we had a
color specified in XYZ space which is
the CIA's sort of primary lingua franca
of color spaces that everything's
generally defined with respect to it and
we wanted to convert that color to sRGB
there the srgb color space comes with a
standardized definition of the matrix
that does that conversion this assumes
that the white point is already at the
correct location in XYZ space so there's
no aluminum adaptation possible in this
matrix it's just a fixed matrix I put a
box around the top row of that matrix
that's those are the weights that you
know when you do matrix multiply you
take a row of the matrix times a column
of the thing on the right and you do a
dot product which is the sum of pairwise
products so that means three point two
four times X minus one and a half y plus
half of Z minus half a Z essentially so
that that defines the red value and
srgb space srgb is a nonlinear space so
we've got the subscript linear on the
are here just to remind us or we're
we're not really an srgb yet we're just
looking at linear data the if you put in
a neutral value or say you want to get
out of neutral value you want to get out
r g and b equal to one you can invert
that matrix and find out what X Y Z
value going in would have given you that
and that I've shown it here and it's 0
point 95 on the x-point 9999 on the Y
and 108 on the Z that's a because the
third component of the Z is a little
bigger that's a kind of a bluish color
that's what d65 is it's kind of a bluish
white like on your monitor now the color
triangle I've drawn here in the XY space
and I've put the formula to remind you
that XY is just a projection of X Y Z in
which you've normalized by the sum of
the components to get a chromaticity
without respect to intensity the color
triangle there is the region where all
three of our G and B are non-negative
and so the the edge that I've pointed
out with the blue line there where it
says R equals zero that's the edge of
the triangle where the red component
goes to zero so there's some interesting
relationships you can notice between
these matrices and triangles and so on
so that that edge is defined by that
equation we're ready quill 0 equals that
linear combination of XY and Z so it's
easy to work out where that edge is and
the there's no explicit representation
here of what the primary colors are that
are used by this that's RGB space but if
you if you look at the picture you can
see that the red primary over here is
defined by the edges this is the the
blue equal 0 edge and this is the green
equal 0 edge so that the row of the
matrix that defines the red has nothing
to do with the red primary but the red
primary is defined by the other two rows
of the matrix so there are these kind of
dual relationships between matrices and
primaries and so forth so when you when
you look in a camera pipeline you tend
to have these structures that are all
ski &amp;amp; Spalding call the
matric approach this this is like
simple overall pipeline for taking
camera data and getting it into an
output color space so the the camera
data they show it going through a
non-linearity here which is typically
actually a linearization to make the
data linear then there's a matrix and
then there's a lookup table on the
output the matrix in the middle they've
shown factored into three parts and the
one I just showed you this one called
the XYZ d65 to sRGB matrix and then that
assumes that you've done something in
between that to do or before that to do
chromatic adaptation to get the to get
the color that you want to call white to
map to the d65 point in XYZ space and
then something before that to to take
account of the color sensitivities of
the camera itself these are all three by
three matrices and so they composed
together into a three by three matrix
and what the camera software typically
does is it it treats these things in a
factored way so you can you can sort of
reason about the different parts of it
and then it composes them all together
and it actually runs the data through a
single three by three matrix
transformation there's there are some
cases where a pipeline will have more
than one matrix transformation and do
some explicit operations in an
intermediate color space in between them
so after that matrix seeing their these
look-up tables where they've again
they've shown a single lookup table
factored into two parts there's a tone
scale lookup table and an srgb
non-linearity yeah questions all right
so whenever you matrix matters question
of resolution
and does anything for how much of that
movie CES is users in this technology so
there's a question of resolution you say
I assume you don't mean spatial
resolution here you mean like the bitter
numeric accuracy right and so these
calculations are generally done in like
16-bit space or some reasonably high bit
depth wide dynamic range space sometimes
floating point at the end you're going
to want to get back to an 8-bit number
but you want to keep all your
intermediate calculations with a few
extra bits both for kind of headroom and
overflow issues and noise issues and so
on and sometimes you may store or cache
intermediate results in a processing
pipeline so that subsequent adjustments
can be very fast so you need plenty of
dynamic range if the decision about how
bright to render your picture hasn't
been made yet okay here's a tone
rendering non-linearity or an s-curve
this is they showed again a lookup table
that had two different functions and
this is the first part of that function
and I emphasize this again and some of
the earlier lectures that this is a step
that's very often forgotten when when
engineers and amateurs look at digital
imaging because they haven't been
steeped in the lore of photography from
the last hundred years where or this was
an important function as part of what
photography is all about is mapping the
range of tones into something that makes
a nice-looking photograph you don't want
to just take linear data represented it
as srgb and stick it out and claim
you're done so in particular this the
top of the s-curve which takes a wide
range of bright highlights and rolls
them off this automatic mouse thing as a
painter that's that's pretty important
to get a wide range of bright areas in
an image to to roll off nicely just see
if I can cause my mouse to stay here
yeah and then in the middle of this
curve it's kind of its curved upwards
here you can see that's what we call an
expansive non-linearity it means the
more signal you have the more gain you
have it's kind of almost like a square
law thing but quite that steep but
that's a that indicates a gamma greater
than one meaning that you get some
contrast boost whatever the ratio of
intensities was in your scene you're
going to get a greater ratio than that
in your reproduction in that region and
then down at the bottom its lower slope
again and that's to kind of make a soft
landing into black and reduce the shadow
noise and again make a good looking
picture so these these characteristics
of the the toe of the curve the
so-called straight line portion which is
actually a it's actually not straight
but it's it's straight on a log-log
curve if it has a gamma to it and then
this rolling off on the shoulder these
are all important aspects of tone
rendering and typically they have
parameters that adjust them the you know
rock converter these parameters may be
brought out explicitly to the user to
play with with sliders or something or
in a consumer camera they're just
they're either fixed or they're they're
made to be automatically adaptive to the
histogram of the scene so you can detect
certain scene types and you can say this
looks like a high key shot that's a low
key shot or this is a portrait or
landscape or something like that and put
in different strategies for how to
render that to make a good photograph
yeah all the mark
yeah does srgb gama gamma there yes it
does and so that'll be next so that's
the second half of this mutt in this
picture is the srgb non-linearity and
let's see if it's coming up next no it's
not quite here yet we'll get to that in
a minute actually I don't even know if I
have a picture of it but I showed it in
a previous talk it srgb has its own
gamma that's not exactly a it's not a
pure power law type gamma but it's it's
a two-part piecewise power law kind of
thing this is just an example to show
that you may have more than one of these
matrix let kind of structures and
they're connected in the middle with an
intermediate RGB space and these guys at
Kodak developed a couple of spaces they
called rim and ROM for the reference
input media metric and reference output
media metric RGB color spaces so
reference input means it's a it's a
scene referred color space the intention
there is that the numbers in that space
represent the intensity ratios in the
scene that you took a picture of and
reference output means that the numbers
there represent the intensities of the
photographic reproduction that you're
making and so it's in getting from the
input referred space to the output
referred space that they do the
rendering and they chose the space in
such a way that it's a good place to do
that rendering it's both a good place to
do white point adaptation by scaling the
primaries as well as a good place to
apply tone rendering curves so you get
into this space you mess with your data
to get it the way you like it and then
you convert to the output color space
you want so that's two of those lookup
table matrix lookup table structures and
then the start my last slide but is this
the finish up step of the processing
pipeline is basically you you matrix it
to the final color space that you've
chosen for your output a lot of cameras
nowadays have a have a choice there that
you have to make you can use srgb or you
can use Adobe RGB or sometimes you can
use others some of them have other
choices where in addition to choosing
the output color space you're also
changing the kind of color rendering
preferences you can choose vivid or
muted or something like that
that it doesn't change the color space
you're going to but it changes the
matrix because you have different color
rendering preferences in a rock
converter that's the same thing you may
render it to sRGB for viewing on your
screen and then render it to Adobe RGB
to put it out to a file or you may
render it to some space that your your
computer's built-in color management
software can take care of rear ndering
to a different space depending on
whether you want it to your screen or
your file or whatever and these these
color spaces have not just different
primaries and different matrices
associated with them but also different
gamma nonlinearities for example on the
Macintosh the Apple RGB and color match
RGB and things like that have a gamma
1.8 and on the pc the popular color
spaces have a gamma 2.2 actually I've
forgotten what adobe RGB is it's one or
the other and then you save it to a file
sometimes uncompressed as a well reduced
to eight bits but otherwise uncompressed
or sometimes even 16 bits as a tiff file
or you compress it to a jpeg next week
is it next week that we're having a
shock talk about lossy compression I
think it is so there'll be some logical
sequence there if you go to JPEG there's
actually another color space conversion
involved where there's another three by
three matrix it takes RGB and matrixes
to what's called ycbcr so the why there
is the the luminance component it's not
exactly the same as the the color of
metric why because here it's a linear
combination of things that have already
been gamma compressed so it's it's a
linear transformation in a funny
nonlinear space but you get this thing
they call Y and then you get the the red
and blue chroma components the red and
blue chroma components are sometimes
subsampled and to get better compression
that way and there and the three
components are more nearly independent
of each other so they they encode they
compress independently reasonably well
so this notion of three by three linear
transformations is all over the
processing pipeline and the notion of
nonlinear look-up tables is as well but
there's a lot of other steps
sprinkled in at various places and
there's no kind of standard place to do
noise reduction you kind of do it
wherever you can usually that's a
nonlinear filter if you do a linear
filter like a smoothing operation you
can reduce noise but you give up too
much signal you lose a lot of sharpness
and nobody wants to do noise reduction
by having their images just come out
blurry so that's not too good but with
non linear filters you can do more
clever stuff there's a family of filters
called rank condition drank selection
filters which have interesting
nonlinearities I'll give you a couple
examples of that and next slide I think
there's a median filter which tends to
be a little bit harsh on photographic
images but some is sometimes used
there's de speckling which is sort of
the gentlest possible our CRS filter and
there's a bunch of things in between or
you can do a you can do a smoothing
operation where you limit the amount to
which any given pixel value is moved by
the smoothing so that in areas that are
only have small fluctuations the
fluctuations go away and when there's a
big edge the big is gets preserved and
Lance was smiling because he knows he's
the guy that taught me how to do this
through a technique called D
quantization as where I learned about
this and then I kind of applied it to
things where it's not really doing d
quantization but it's the same kind of
algorithm you want to make you want to
make an output that's consistent with
the input so if you have an estimate of
the noise in the input or the
quantization of the input you want the
output to be consistent with that in the
sense that no pixel value moves by more
than say the noise standard deviation
that works really quite well and there's
a lot of things like it called coring
accoring is another approach that does a
very similar thing and in general
there's this class of things called edge
preserving smoothers many different ways
of going about smoothing away noise
while preserving sharp edges and they
they work in varying degrees and if you
look at images from different
manufacturers and kind of noisy
situations and look at the shadow detail
and so on you get very different
feelings from different cameras based on
what they've what they've chosen to do
there sometimes they get very smooth and
pasty in the shadows and they still have
sharp edges
sometimes you get kind of a watercolor
painterly effect between sort of these
discrete brushstroke looking things the
other thing you can do there that helps
a lot and is pretty common in consumer
camera processing pipeline hardware is a
luma dependent chroma gain reduction so
that means in the dark areas of the
image desaturate the chrominance get rid
of the color noise when you're in the
shadow area where it's it's too noisy to
keep good color so these these kinds of
steps get sprinkled in wherever it's
convenient in the data pipeline here's
example I said I was going to show you
on the rank conditioned rank selection
filters so the idea is you take take
some neighborhood most typically just a
3 by 3 neighborhood and you sort the
nine pixel values in that neighborhood
you rank them and then you look at what
what rank is the center pixel and then
depending on what rank it is you select
some other rank out of your sorted list
as the value for that pixel so a median
filter isn't always characterized this
way but it's a it's one extreme version
of this you sort all the pixels and then
no matter what the rank of the one in
the center is you replace it with the
one of rank 5 so you always replace each
pixel by the median of its neighborhood
the D speckle filter is a is a thing
that in most cases doesn't change the
pixel value so if the if the rank of the
pixel in the center is anywhere from 2
through 8 out of the one through nine
you just leave it alone but if it's Rank
1 you replace it with the rank 2 value
and if its rank 9 you replace it by the
rank 8 value so that'll that'll take
things that are outliers and it will
move them this just enough that they're
no longer outliers that they're they're
no longer outside the range of their
neighborhood that's that's if you have
like random bit noise in your data it
makes a lot of these pixels that are too
big or too small for their neighborhood
this will pretty effectively knock them
out and it's not very smart but it if
the numbers of such things are fairly
small this is a pretty effective way to
reduce some tour you probably won't
notice them it can it can do damage to
the pixel
the the image structure in some cases
but it's it's usually not bad yeah so
are you doing it with r g and b
independently yes you can do that on the
raw data before d mosaicing with some
kind of a neighborhood and bear mosaic
space or you can do it after d mosaicing
and it may be less effective I don't
have that much experience with how to do
this with Bayer cameras actually okay
there's there's other optional steps
that can go in there the pipeline's
where the guy was i'm doing the effects
of the camera stages showed a lens
distortion correction that's what i
haven't talked about but you may want to
do a an operation that's not affecting
pixel values locally but it's actually
moving them around and distorting the
whole image so you can have a lens that
has distortion and you can undo it or
you can have a lens that doesn't have
distortion and you can distort the image
for some other purpose for example to to
link up multiple images that are
perfectly rectilinear into a panorama
because you're shooting at different
angles the the rectilinear projections
don't really line up so if you map each
of those to a cylindrical projection
then you can stitch them together and
make a panorama or you can warp each one
to a different same rectilinear
projection say so there's a lot of image
warping stuff that that goes on in a
pipeline somewhere especially if your
camera is supposed to do panoramas
there's the high dynamic range remapping
kinds of steps some of these are based
on Edwin lands reten ex model of
perception where you he has a particular
way of preserving local contrasts while
doing adaptation in a way that has the
right kind of long-range correlations
and so on that's one approach people
often use to designing these algorithms
works pretty well and if you've warped
your images to make a panorama you still
have to worry about exactly how to get
the edges lined up and blended so the
the the idea of slowly varying intensity
gain and a high dynamic range mapping
can be integrated with the kinds of
intensity adjustments you need to bring
the edges into agreement in a you know
panorama stitching and besides just
adjusting the the gains of the images to
make them line up you can also adjust
the colors on a slow continuously
varying process and that's part of lands
rednecks theory as well as how we how we
can spatially adapt to color so that
when you're looking at a sunny area here
in a shady area here you your I kind of
adjust to those so that if there's
something white hearing something white
there you don't really notice that that
one's yellow and that one's bluer they
both just look white so you can do that
in your images as well so that the white
objects and those different lighting
environments actually do render to the
same neutral color if that's what you
want to do here's an example of a high
dynamic range rendering problem again
from the the Pearl ski &amp;amp; Spalding
chapter they show the same photograph
rendered dark and rendered light and the
on the bottom we have the histogram the
the different light values of those
images and a couple different ranges
that they rendered this is one reason
why automatic rendering of images is so
hard if you have a computer program that
looks at this histogram and tries to
figure out how to render it to make a
pleasing photograph it doesn't know that
those are your favorite people are in
the foreground or your favorite
mountains in the background it doesn't
know what part of that picture you care
about so it can compress all that range
and give you a low contrast image but
that's not really a very useful result
so you really need these nonlinear local
operations to bring down the gain and
the bright mountain area and bring up
the gain in the foreground and try not
to introduce spatial artifacts and the
way the gain varies between them and
they've done that in the book I didn't
think it would be worth projecting that
image here but there's some nice color
plates to illustrate some of these
algorithms and if you want to see the
book come find me so there's a lot of
big topics in all this then we just had
you know broad brush treatment in an
hour but well there they are I think
we'll just leave it at that and go for
questions thanks
questions
okay shut up
yes
curves of kenosha
yeah is this essentially just playing
with curves and photoshop you ask me
well it's it's not you can't do it that
way there's no way so if all you have is
curves in photoshop you can reduce all
of this dynamic range sorry all of this
dynamic range to one low contrast image
but you can't get high contrast in the
foreground high contrast in the
background at the same time with one set
of curves if you could segment the image
and apply one set of curves in one place
one side of curves in another place then
you could do the what's what the high
dynamic range rendering does but then
you have to worry about the boundary
between those segments and that's what
the retina X algorithms tried to do is a
they try to bring up the gains where
it's dark take the gains down where it's
bright and make a transition between
them that doesn't give you visible
artifacts yeah question parts yes the
question is can you use sort of a
Fourier approach and frequency analyze
the image take out the low frequencies
and then crank up the gain on what's
left and yeah in fact you can and that
that approach was pretty well developed
actually in the 1960s by Al oppenheim
and Tom stockham in a paper they wrote
called something like you know something
about visual models and image processing
if sorry I don't remember the title
right off but they but they showed and
they actually developed this whole
theory of homomorphic signal processing
homomorphic image processing and what
they showed is that since the lighting
that very slowly is multiplying the fine
structure of your image you want to
separate a low frequency thing from a
high frequency thing when they're in a
multiplicative relationship you want to
convert to a logarithm log image space
so that the in that space the low
frequency and high frequency are
additive and remove some low frequencies
convert back so yeah filtering
techniques in a log domain is one way to
do this it's it's well studied well
developed has a nice theory behind it
it's not the best approach but it's a
good it's a good starting point yeah
everybody else yeah part of the problem
that's great
yeah yeah so high point out that if you
have to get the dynamic range at the
sensor before you have any hopes of
processing it this way and film may have
better dynamic range than digital I
think that's digital can have enough
dynamic range in a DSLR big pixels and
shot at a reasonable I so you've got
probably more dynamic range than you can
get with a lot of film you may have
maybe not quite as much as you get with
some color print films but close your
dynamic range will in the what when the
dynamic range is high the thing that
limits it is going to be usually the
lens flare the stray light in your
camera that pollutes your shadow regions
and when you're in that region that puts
a finite limit on how much dynamic range
the camera has to capture or can capture
whether it's film or sensor so the low
end cameras you're right you probably
don't get enough dynamic range to to
really need this kind of thing very much
but in the high-end most DSLRs get a
pretty good dynamic range probably about
as much as you're going to get with film
picture
go back 15 years there was much less
seemed like it was much less less less
blowing out highlights an average
amateur pictures 15 years ago yeah well
part of the problem is a lot of the
camera companies don't do an aggressive
job of rolling off the highlights when
they do the rendering and Nikon in
particular tends to have kind of a
clipped highlight look so you do get
these big white blown out highlight
areas and pictures and that's something
that film was particularly good at it
had a very you know long shoulder on the
curves so it's I think it's not so much
a question of total dynamic range as it
is a question of how people have chosen
to render the images yeah one channel
clips that we've got trouble so yeah and
so you got to be careful with the
exposure and film is better at the high
end at maintaining reasonable behavior
when it's over exposed whereas a sensor
once you hit clipping it's not much you
can do there but render it to white yeah
Lancers trying to get around that they
have dogs what kind of mother right you
can you can invent all kinds of schemes
for getting around it like Fugees two
photodiodes per pixel scheme where they
measure with a small pixel they try to
get a measurement of what's going on up
in those highlights where the main pixel
is saturated and there's other
techniques like that my impression they
don't work all that great we're going
into screensaver mode yeah I ever had
full about how do you try to understand
what's going on
I say that user yeah how do you
understand like reviews of consumer
cameras given us all this magic is going
on that's a good question i'm trying to
harm you with some knowledge so that
when you read about these things you can
interpret them better but it's hard the
guys that are doing the reviews don't
typically understand all this stuff and
so they they have their own mental
models of what's going on and sometimes
they say things that only make sense in
their own imaginary world okay thanks</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>