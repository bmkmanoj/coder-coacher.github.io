<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>A Dynamic Network Layer for Advanced Cloud Computing | Coder Coacher - Coaching Coders</title><meta content="A Dynamic Network Layer for Advanced Cloud Computing - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>A Dynamic Network Layer for Advanced Cloud Computing</b></h2><h5 class="post__date">2008-09-05</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/9Z6RworZrLQ" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">good afternoon so thanks all for coming
so first let me start off by just saying
a word or two about why the title on the
slide isn't the title that was
distributed when this talk was first
announced so my group has been involved
the last couple of years in something I
have been referring to his internet
scale overlay hosting and recently I've
been becoming interested in this cloud
computing model that's been evolving and
it's and have been trying to understand
the relationship between the overlay
hosting kind of model we've been
pursuing and the cloud computing and
it's becoming clear that the cloud
computing environment offers a great
context for the kind of advanced
networking capabilities that we've been
developing so while the this isn't
exactly the same talk I would have given
a month ago when this trip was planned
it does address a lot of the same core
issues and I hope that by putting this
it in this context it will make it
relate better to some of the things that
are going on here so let me just you
know start off with some basic
introductions cloud computing is has
emerged as a mechanism for supporting a
variety of different computing
applications but in particular for
delivering web-based services and the
power of it includes the ability to
rapidly deploy new services on a global
scale allowing even small organizations
to quickly develop and deploy new kinds
of applications and of course the
dynamic scaling offered by the cloud
computing environment enables the
supporting mechanisms to scale up and
down in response to use it man but the
range of the applications that can be
supported in typical cloud computing
environments is fairly constrained by
some of the limitations imposed by the
Internet the fundamental internet
service model that at least that's
available in the public Internet
has barely moved in about the last 20
years so we're essentially using the
same internet services that we were back
in the 1980s so what I'm going to be
talking about is the introduction of a
dynamic network layer within the cloud
computing infrastructure to extend the
range of applications in particular to
support real applications that involve a
significant real-time person-to-person
interaction capability and this dynamic
networking layer is completely
programmable so that it can be adapted
to the needs of particular applications
it's also completely optional so that
applications that are perfectly happy
with the ordinary networking
capabilities offered by the Internet can
simply use those directly without any
cost impact by the without having to
bury any of the costs of the dynamic
networking layer now some of the things
that are needed in order to support the
introduction of dynamic networking or a
listed here and I'll get into some more
details as we go along so first of all
the application developers will need the
ability to specify where processing
resources are located geographically
they'll need to be able to connect their
clusters of processing resources with
provision virtual links have the
mechanisms to support Internet scale
traffic volumes while getting router
like performance and the one of the keys
to enabling that is the integration of
flexible high-performance packet
processors into the infrastructure in
addition to the conventional kinds of
servers that are provided today so
here's just sort of a picture of the
kind of cloud computing infrastructure
we're talking about so you have a
collection of distributed data centers
on top of which
we can layer many a number of different
applications so I'm showing just one
application here now connected by
provision virtual links and at each data
center we may in fact have a cluster of
processing resources that may be
specialized by functions so we may have
a some lower level processors that are
implementing the core network forwarding
and core services that are provided to
the upper layers a set of application
processors that implement the main
application logic and potentially some
additional processors and interface to
users so before I get into some of the
specifics let me tell you a little bit
about a very small application study
that we've done to try to investigate
the kinds of capabilities that are most
suitable for this kind of dynamic
network infrastructure and help us kind
of explore the limits of what we can do
so this is a scalable online game
application and of course there are a
variety of ways you can distribute the
state associated with a large game we've
been focusing on highly interactive
games like first-person shooters which
leads you typically to distribute
applicate the application in a
particular way so the the servers around
the periphery here serve the players or
manage the avatars for the players that
are physically connected to them so that
we get responsive rapid response to a
user input and then the overlay network
that supports the servers provides game
aware network services and in particular
we use on the directional multicast to
provide the state update distribution
with region based filtering that is
regions in the virtual world so state
updates issued by the servers on behalf
of their players get distributed across
the multicast and at the egress the
servers will receive only updates for
regions of the virtual world that
they're subscribed to and just some
quick back of the envelope estimates to
sort of help calibrate you if each
dynamic object in the game generates a
twenty to thirty kilobits per second of
traffic then a game with say 10k objects
which is roughly ten k players would
involve maybe 2 300 megabits per second
of network capacity for all of the
traffic combined you wouldn't
necessarily have that on any single link
and so even if you did however you could
supports a 30 game sessions of this size
on a 10 gigabit link so you know while
this is you know for a game this size
this is a non-trivial amount of network
capacity it's not really at all large by
modern standards and of course there is
the potential to reduce the bandwidth
you substantially as you get close to
the edge of the network although at the
core you might come close to those kinds
of levels one of the key issues with
these kinds of applications is managing
the state update distribution again
there are variety of ways you can do
this we're not really particularly
focused on you know the getting the best
possible way of managing the game since
the objective here is really to explore
the networking the implications for the
network layer but let me just say a
little bit about the the general
approach we're taking so we divide the
the game world into static regions and
associate a multicast with each region
you can either do this using a
rectangular distribution here I'm
showing a hexagonal division of the game
world and again object updates would be
labeled with the region identifier where
the object is currently located and each
server will receive state updates
only those portions of the game world
that are visible from where it is to
make this work efficiently you need some
mechanism for compactly representing the
regions that are visible from any point
in the game world and so I'm not going
to go through this in any detail but
here is just sort of an indication of a
well in this diagram the yellow regions
are those that are visible from the
central point the light and dark blue
regions are invisible we can represent
this by encoding a tree and so with a
fairly compact bit string we can encode
the visibility region even for a fairly
extensive portion of the game world ok
again service can subscribe to the
regions as needed or you know we can
also think about extending the
underlying network service to actually
infer those subscriptions simply by
observing the traffic coming from the
servers if we observe the state updates
coming from servers we can implicitly
determine what other regions they should
be interested in based on this
visibility information now in either
case as soon as a server is subscribed
to a given region it's going to start
automatically receiving updates there's
no coordination that's necessarily
required with the sender but one
complicating factor is typically state
updates and games or Delta encoded so
you only transmit an update you only
transmit the parameters the change as
the game proceeds with without having
any explicit interaction with the sender
this creates an issue we can resolve
this in a couple of ways one is simply
to periodically send full state updates
at some lower rate another alternative
is to have the endpoints issue requests
for full updates as they're needed and
those can either be delivered back to
the sender
or potentially cached within the overlay
nodes themselves now we built an initial
prototype and I'm just going to tell you
a little bit about this to put some
things in perspective so this is
implemented within Washington
University's open network lab which is
built around network crisis group based
routers we have I XP 28 hundreds that
are implementing the routing knows these
are these ones that are shown in green
here on the user interface and so for
the particular application or the
particular configuration here we have
eight routers and 20 servers and for the
the starting point for this study we
just did kind of the simplest thing you
can imagine we represent the game world
with rectangular grid representing the
region's with all state updates
delivered by to all routers in the games
we're not tempting to do any traffic
filtering in the backbone of the network
we are filtering at the edges so the
servers only receive the updates they
need but we go ahead and distribute the
updates throughout to all of the routers
and to support this we add two plugins
to the to the routers a subscription
plug-in which just handles the
subscription requests coming from the
the servers and the subscription
information is very simple it's simply a
array of bit vectors that identifies the
ports that are associated that packet
should be forwarded on for each region
of the game world and then a multicast
forwarding a plug-in that runs on four
of the micro engines of the the I XP
2800 so here's just a little bit about
the the software that's mapped on to the
I XP 2800 for those who aren't aware
this the 2800 is a multi-core processor
with 32 RISC processor cores in the MUP
Intel context these are referred to as
micro engines and what this is showing
is the the main
packet processing pipeline and a mapping
of that pipeline onto micro engine so
that's what the parenthetical comments
are referring to so the the two major
components here are these two in the
center the the pars look up and copy
block which parses the packet header
performs both route lookups and general
packet classification using the the tcam
and the copying required for multicast
there is a queue manager that manages
about 16,000 queues for each of the
outgoing ports and then there are also
connections to the X scale management
processor that controls the operation of
the system as a whole now in addition in
the onl router we've added a plug-in
environment we basically set aside five
of the micro engines to implement
plugins that can be added by users and
so each plug-in has its as a dedicated
queue of packets being directed to it
but we can also share these queues in
various ways and there are a couple of
micro engines that just implement
general bookkeeping functions the
subscription plug-in runs on the first
micro engine and then we have for micro
engines that are implementing the
multicast forwarding actually
replicating the multicast packets as
state updates are received and these
operate off of a single shared queue and
after packets have been copied their
forwarded by the plugins to the queue
manager and the the forwarding table is
kept in external SRAM accessible to all
of the plugins and so change is made by
the subscription plug-in or immediately
available to the multicast forwarders so
just a couple of results some
performance results associated with this
so there are two metrics I'll mention
the first is Fannin fannin is the number
of objects for which a server receives
updates now
the if the game world is divided up to
coarsely we'll end up receiving state
updates for objects we don't really care
about they may be in regions where we
potentially have some interest but we
may not be able to actually see those
objects in the game world because the
region is so big and so we'd like to
keep the fan and as small as possible
because that's a significant as a
significant impact on the performance
that the server's can sustain the second
parameter I'll mention is the regions of
interest this is the number of regions
that servers must subscribe to we'd like
to keep this small in order to reduce
the subscription overhead and of course
the easiest way to keep that small is to
have large regions so these two things
trade off against one another and you
can see that in this chart so what this
is showing this is a fairly small
instance of the game we have three
players on each of twenty servers so a
total 60 players and all and as we
divide the game world up into smaller
and smaller regions the fan n drops from
a high of around 50 here for the maximum
fan n down to a little under 30 and
similarly for the average fan and we go
from about half that and this asymptote
represents the the fan and the
corresponds to the number of objects
that players are actually interested in
receiving the updates for so dividing
the game world any finer at that point
doesn't really do you any good and of
course we can see that the number of
regions of interest is growing roughly
linearly with the number of regions in
the game world as you would expect and
so the kind of sweet spot happens to be
at this place where the number of
regions matches roughly matches the
number of players and this is
characteristic of maps that have
relatively uniform spaces in the virtual
world as you have maps where the the
spaces in the virtual world vary widely
in size they'll this comp this
characteristic will be more complicated
than what you're saying here and this
kind of simple uniformed division into a
uniform grid is going to be less
suitable as well this slide is showing
or this chart is showing how the Fannin
and regions of interest scale as you
increase the number of players on a
server and so this is with a fixed map
with a fixed number of regions and so
but both the kinds of parameters are
increasing with the number of players
per server something that's important to
keep in mind though if you think about
scaling in a larger scale is that as we
increase the number of play what we're
doing here is we're fixing the number of
this isn't just increasing the number of
players per server it's also increasing
the total number of players in the game
and that's really a large part of why
the scaling is occurring the way it is
and one of the things that's important
in these virtual environments is as you
increase the scale of the UN crease the
number of players you also typically
want to increase the scale of the game
world as well if things get too crowded
games become difficult to play and
people die out too quickly and so you
typically scale these things together
rather than independently as indicated
here okay one of the reasons we're
interested in this in addition to just
using it as a vehicle to kind of explore
these how these kinds of overlay
techniques can be implemented is that
we're interested in the potential of
these kinds of virtual collaboration
environments for having an impact on
things like large-scale travel so some
of you are probably familiar with sons
mpk 20 project there's the potential
with applications like this to
significantly impact the amount of
travel and as we all become much more
aware of the environmental impact
associated with both physical travel and
the infrastructure need
to support physical travel finding
smarter ways to deal with that is
becoming more and more important we
simply can't afford to have everybody in
India and China having the same kind of
energy and environmental impact that
everyone in the u.s. sort of takes for
granted that simply doesn't appear to be
sustainable in the long term and so
using these kinds of advanced
applications to address that kind of
issue can have a big impact now there's
some interesting problems if you really
want to try to make virtual environments
like this work well and one of the most
important is actually not video but
audio so high quality environmentally
accurate audio can have a tremendous
impact on the perceived quality of the
virtual environment so imagine a virtual
environment that is being used to
support a large-scale conference where
you can walk around the lobby of a
conference center and you know meet and
talk to your friends and at the same
time here the six other conversations
going on around you and as you move
through the virtual space that has to
all happen in a natural way even though
the actual physical people may be it you
know hundreds of different locations
around the world and in order to make
that happen you need to bring together
the appropriate subset of those audio
inputs and bridge them for every single
participant in the in the conference so
that creates some interesting challenges
for at both the application level and
potentially at the network level because
some of this may get pushed down to the
network level in order to handle it most
efficiently would also like to be able
to do things like use video to capture
the expression of the individual and
perhaps use that to control the
expression on the user's avatar control
where they're looking and how they're
moving and of course at the network
level we need to provide the network
level support to ensure consistent
performance even as
people move around the game world or the
virtual environment and interact with
changing numbers of other people all
right well let me get back to the way we
might implement these kinds of
applications in a cloud computing
environment so this is just showing some
of the components of an application
cluster that might support this kind of
application in a large implementation
and so we're dividing the we have
multiple kinds of processing elements
that serve different roles in the
application so at the front end we may
have some access nodes in interface to
users and provide load balancing across
application servers the application
servers might implement the core
application logic so things like the
game physics computations and issuing
state updates to the network and the
backbone nodes that provide the core
data path services like the multicast
forwarding and subscription and over at
the right here an application controller
to oversee the overall operation of the
application coordinated over multiple
sites in combination with the
application controllers running in other
sites so we'll do things like manage
session creation create incrementally
create multicast trees and so forth okay
now to implement this kind of
application most effectively it's going
to be helpful to have processing
components that are well suited to
different tasks now conventional servers
are well suited to implement the kind of
sophisticated application logic that we
have in an application like the the
distributed gaming or virtual
environments application for some of the
lower low
processing things like network processor
blades that are designed for real-time
packet processing can provide
significantly better performance we
doing sort of side-by-side comparisons
of network processor packet processing
versus packet processing and user space
on conventional server blades we see you
know well over an order of magnitude
improvement and throughput and multiple
orders of magnitude improvement and
latency and those throughput and latency
characteristics are extremely important
for applications that are highly
interactive and involved in a direct
person-to-person communication now it's
not just network processor blades it may
be other types of processing subsystems
as well that can play a role here fpgas
and DSPs for audio and video processing
and potentially even graphics processing
units even though the GPUs are typically
we think of that as something that
happens only in the end device they can
also potentially play a role in certain
applications in the infrastructure now
having this kind of diverse collection
of processing resources available within
the cloud computing infrastructure is
reasonably straightforward to support if
you have a resource model where your
your basic what you think of is your
basic API focuses on allocating raw
resources to applications the you know
the glue that holds things together is
nothing more than the commodity a 10
gigabit ethernet switching layer that's
now coming into existence and I want to
point out that there's one can even in
the context of this kind of lower level
raw resource provisioning model
implement higher level API is on top of
that the only question is you know what
level of visibility do you provide to
that lower
level do you have one API or multiple
api's and certainly there's a variety of
you know there's a large space of
possibilities there okay so one of the
crucial things that's necessary to
enable applications like this to work
well within a cloud computing
infrastructure is the provision of
application isolation so you want to
isolate one application from another's
first of all so that they can't
interfere with the correct operation of
other applications but also so that they
can't interfere with the performance
that other applications experience now
certainly the easiest way to achieve
application isolation is physical
separation and in fact for applications
that are large enough scale to justify
at least one processing element in every
physical location physical separation is
a great alternative okay so it can work
well for a large fraction of the kinds
of applications that you typically want
to run in a cloud computing environment
so as long as each processing component
is used by one application we can
physically separate the applications
fairly easily at the network level we
can use VLANs to isolate the packet
traffic from one another that is one
application can't send packets into
another applications virtual network and
as long as we have a non-blocking
switching layer we effectively get
traffic isolation okay that's
essentially what non blocking switching
means if on the other hand we want to
share processing components there are
some various ways we can do this and
there's certainly times when sharing
processing components is useful if we're
using conventional servers then virtual
machines gives us an effective way of at
least getting the security kind of
isolation that we want there are limited
abilities to provide performance
isolation within / chill machines for
NPS we've actually put together a
configure will runtime framework where
we have programmable plugins at various
points in
the processing pipeline and I've been
able to demonstrate that you can you
know get a reasonable level of
flexibility using these kinds of limited
mechanisms now this does sharply
constrain the programming environment
making this most suitable for basically
simple fast path processing that you map
on to a shared network processor rather
than more complex processing it's also
important to recognize that if you're
going to share processing components the
there are implications for the
underlying network you need finer grain
or queuing if you're going to maintain
isolation between you know different
components that are reached through a
single network interface for example
okay coming out of a switch and so
having / VLAN cues with configurable
bandwidth becomes a required capability
so this all of these issues have some
implications for how data center
switching is carried out in this
environment so let me say a little bit
about that so here is sort of a no
cartoon of a folded Bennish network or
fat tree topology which is typical of
the topologies that are used for
enabling scaling across a wide range of
sizes so this kind of architecture using
commodity switching components enables
you to build data center supporting
thousands to tens of thousands of
servers and allows you to take advantage
of the commodity switch components that
are available down now in order to get
isolation of application clusters again
we can restrict the routing between
different application clusters simply by
using virtual LAN techniques that are
supported by the commodity switch
components and we need none blocking
switching over those switches in order
to get the
a traffic isolation even if we're
physically separating the applications
on different processing elements now
ideally this is done with / destination
queues and by / destination here I
actually mean no / end point over this
whole collection so if you've got a data
center with a hundred thousand
processing elements or you if you really
want isolation over that entire complex
you need a hundred thousand cues on each
link okay now that's obviously not
something you're going to see in
commodity switching platforms but that
is the kind of characteristic that it
takes to guarantee non-blocking
switching so what else can you do so if
you want to get the effect of non
blocking switching and other things as
well you need to add some additional
capabilities around the edges of this
network and so let me mention three in
particular so first of all load
balancing this kind of a interconnection
topology creates lots of different paths
between any point in the network and in
order to get non-blocking performance
you're going to have to take advantage
of those different paths that are
available to you you're going to either
do that by load balancing on flows or
aggregates of flows or you can do it by
load balancing on packets and I'll say a
little bit more about that in a minute
another issue is how to regulate the
traffic flows within an application
cluster so as to avoid causing
congestion within the switching Network
and finally for applications that
require multicast we need some mechanism
to support multicast forwarding within
the clusters and throughout we need to
ensure that processing elements can only
be used and straight for in a safe way
with conventional server blades if we
have control of the operating system
it's fairly easy to ensure this with
some of these lower level components
like
network processors it typically don't
have an operating system per se in order
to ensure safe use you need to be more
reliant on mechanisms that are available
at the switch layer okay so let's talk
briefly about this load balancing issues
so again here's kind of the the cartoon
of the the network topology and in order
to get effective non-blocking
performance across an entire data center
we need to distribute the load coming
from the processing elements down here
at the bottom across all of the top
level switches so the one of the
conceptually most straightforward things
to do is to distribute the traffic based
on flows or some aggregate of flows and
one of the things that makes this
attractive is that it gives you a very
direct way of maintaining packet water
for packets that belong to a single end
and flow but it leads to significantly
less than optimal balancing of the load
across these switches and so an obvious
question is how bad does it get well if
you want to do load balancing on an
incremental basis that is you know as a
new flow starts you want to pick a top
level switch through which to route that
flow in order to reach your destination
then if you have a topology that
requires this kind of a three-level
network you actually need to six times
over provisioning the switch man with in
order to guarantee that you can always
accommodate that next new flow ok now
you can reduce that to about 2x if you
allow yourself to dynamically rearrange
flows but rearrangement is can be costly
you can't count on being able to just
rearrange a few flows you may in fact
need to rearrange a very large fraction
of flows in order to accommodate a new
one and of course it can be disruptive
because when you rearrange flows you're
sending packets along different paths
which means you no longer are
maintaining the packet
during which was a reason for using
flow-based load-balancing in the first
place no now it actually has very little
to do with the buffer size it really let
me take that offline because it's it's
it's not a short explanation okay the
you get the 6 up even if it was circuit
switching even if you were doing circuit
switching you'd still have that 6x
speed-up requirement and the and the
last point here is you know the truth is
flows aren't static right flows don't
have a single bandwidth they're
constantly changing their bandwidth
usage and if you're going to you know
keep the load balance well you're going
to need to frequently rearranged as the
traffic changes so all of that is a
little problematical so that leads us to
the alternative of doing packet level
load balancing and in contrast packet
level load balancing is capable of
providing ideal load balancing all
across all the top level switches and
its robust to changes in the dynamic
traffic but it does require ricci qin
singh at the output okay that is you
need to maintain at the at the egress
you need to maintain rhe sequencing
buffers that can take the packets and
deliver them in the order that they
entered the network rather than the
order they came out however in the
context of a network where you have well
regulated traffic flows it's actually
fairly easy and inexpensive to implement
this kind of packet level ricci
cleansing and so I would argue that in
fact this is a more straightforward
problem to solve and it completely
avoids the the speed-up requirements
that are associated with doing flow
based or aggregate load balancing now
let me say a little bit about this
traffic regulation problem non-blocking
switches in general are in order to
deliver non-blocking performance they
require admissible traffic so in
particular that means that you can't
send more traffic to an output than it's
capable of receiving so processing
elements down here have a limited
bandwidth at this interface and if you
have you know 100 processing elements
that are all trying to send traffic
simultaneously to this guy you can you
have the potential for congestion here
which can leak over into other portions
of the network and interfere with
traffic going to other outputs that may
not be congested okay so we want to
avoid congesting those outgoing
interfaces two peas in order to enable
us to get non blocking behavior so the
key to doing this is to equip the
processing elements with virtual output
cues that have configurable forwarding
rates and then adjusting those
forwarding rates dynamically using
control algorithm that's periodically
distributing information about the
backlog is that different processing
elements have for various destinations
and dynamically adjust the queue rates
to in the first place ensure that we
don't get congestion at any of the
outputs and secondly while attempting to
optimize whatever our performance
objectives is it may be simply
throughput or it may be some were a
complex objective now it is possible
with this kind of an approach to get
strong performance guarantees if you
have some 2x speed up in the bandwidth
you can actually get fairly good
performance even in the absence of
significant speed up although to get you
know theoretically provable performance
guarantees you do need that 2x speed up
and in when this kind of technique is
applied in routers will typically have a
virtual output queue for every
destination in the system in a data
center with a hundred thousand
destinations
maintenance of a hundred thousand vo
cues and the distribution of backlog for
100,000 v oq s becomes problematical and
so for scalability reasons we may want
to limit the number of concurrently
active vo queues at the application
level that is you constrain the number
of destinations that each processing
engine is going to talk to at any one
time so as to be able to limit the
number of Bo queues for which you have
to maintain state okay multicast is
another crucial feature for a number of
applications ethernet-based multicast
turns out to be a fairly limited use in
this context because it relies on
broadcast within VLAN domains now you
could conceivably use VLANs on a per
multi cast session basis that is
basically set up a VLAN so that you
deliver the packets only exactly where
you want them to go but that is a fairly
limited approach and well it's limited
by the number of vlans you have at your
disposal and by the overhead required
for configuring vlans there's also no
straightforward way to regulate the
traffic in this context and so an
alternative which works quite well in
this situation is to push the handling
of multicast back to the processing
engines and use a distributed multipass
kind of mechanism where packets are
copied by the processing elements into
vo queues and then normal voq scheduling
and unicast switching mechanisms can be
used to deliver them to the destination
okay and to get the most scalable
multicast delivery we can do this with a
simple binary multicast tree so this is
being illustrated here I think of this
circle as denoting a processing element
where track the multicast is originating
and
we send a two desk this processing
element forwards the packet the two
destinations by placing copies and two
different vo cues this relay processing
engine forwards and the two more
similarly here and so we have four
copies being made having made what one
two three passes through the network in
general if we want to produce a fan-out
f multicast we're going to end up
sending a total of roughly two F packets
across the switch so this does imply
that multicast has basically a 2x
bandwidth penalty relative to unicast
but it gives you a simple and highly
scalable way to implement multicast and
kind of pushes it into the application
space rather than pushing it onto the
common switching infrastructure ok so
these edge functions I've been referring
to can either be implemented in
processing elements or in some kind of
fabric interface component the load
balancing and traffic regulation is a
fairly simple thing to do the processing
associated with it it's very simple the
amount of memory needed even for the
recency buffers is fairly modest on the
other hand this has important
implications for the security of
different applications and in particular
for the traffic isolation and so this is
a good choice to implement in a fabric
interface component that's under the
direct control of the the cloud
computing infrastructure multicast on
the other hand is a an applicant is a
network feature that's highly
application dependent in fact exactly
what we mean by multicast may change
significantly from one application to
another if we implement multicast using
this kind of distributed multipass
approach there's really no security
impact because all of the traffic
isolation and so forth is handled by
forwarding we're really forwarding just
unicast packets in the core it also if
we did attempt to push it down into a
an interface component this does require
potentially large amounts of per flow
state and buffering so that can be
expensive to push on to the common
infrastructure and so this is something
that's best implemented by PE s and on a
per application basis okay so let me
quickly tell you a little bit about a
prototype platform we've put together
over the last year or so for you know
these kinds of applications and this is
really focused more on the overlay
hosting environment we've been working
where you know rethinking this now in
the cloud computing context but i will
i'll just describe this system as is now
now we targeted this for application in
planetlab so this is a fairly small
scale system our objectives include
maintaining capability for standard
planetlab applications of somebody who
has developed an application on
planetlab can map it to our platform
without any changes and have things just
work this gives us an easy migration
path but also opens the door to boosting
application performance by then
restructuring the application to map the
high frequency parts of the application
onto a network processor resident fast
path so this enables a small number of
NP blades to be shared by multiple fast
paths which in the context of planetlab
where we're talking about relatively
small amounts of overall network
bandwidth is appropriate we do this by
having this configure will framework
with plugins at particular points in the
processing pipeline and each fast path
selects from one of several static code
options that are available in the as
plugins and we're planning to employ
five of these systems as part of the
genie prototyping initiative that's just
getting underway now so let me give you
give a quick review about planetlab in
case anybody isn't familiar with it
connect planetlab is kind of the
canonical overlay hosting service so it
enables multiple overlays to run on top
of a shared infrastructure in planetlab
applications just run as user space
processes within virtual machines and
it's been a very effective and important
research testbed but it's had limited
impact as a service deployment mechanism
because it simply doesn't have the the
kinds of resources and kind of
performance that's necessary to provide
effective application performance okay
and that's probably one of the largest
installations in the world then typical
installation is to and local the local
administrators get to determine how much
Network man with the planet lab nodes
get to use and I was stunned to learn
that it is not uncommon for network
administrators to limit that to 100
kilobits per second and it's very common
to limit it to maybe 10 megabits per
second so that kind of gives you a
calibration point for for typical
applications so what we've been what
we've done here is we've built a
platform that includes network processor
based components and conventional server
blades and so you can run an application
or run a portion of your application as
a fat in a fast path that runs on the
network processor blade while the
apportion running on a general-purpose
server environment implements exception
handling and control so the expectation
is that the the majority of packets
coming into the system will be processed
within the network processor and
forwarded on their way but occasionally
packets will be forwarded up to the
virtual machine for you know more
complex exception handling
our control operations and of course you
can have multiple slice and sharing a
network processor and you can have
applications that simply run in the
standard as standard planetlab apps the
components of the system include a line
card this is also a network processor
based line card with 10 1 gig interfaces
we have the referred to as GPS
general-purpose processing engines these
are just conventional server blades that
run the standard planetlab environments
with a couple of small extensions the
network processing engines are dual YX b
28 50s there is a control processor
which manages the operation of the
system and implements the standard
planetlab control mechanisms so in
particular it interacts with the plc and
princeton to pull down new sliced
definitions and based on those sliced
definitions it'll go ahead and
instantiate new v servers running on one
or more of the gps and finally the line
card provides all of the external IO and
has a queuing subsystem and filtering
mechanisms to deliver arriving packets
to the appropriate either npe or gpe and
so this is physically what the system
looks like so this is a six slot atca
chassis with the three of em sorry three
of the network processor blades to GP ES
and then the Switchblade down here at
the bottom the GPE just is one of two
different kinds of processing components
so application developers can choose to
run their application either entirely on
a conventional server blade work and use
a combination of the server blade and
the network processor blade okay
so just as a to get a quick
characterization of how the performance
of these two varies is showing the
throughput of the network processor up
here four packets of varying sizes and
the performance of the network server or
the conventional server blade these are
log scales on both axes and so the
network processor is keeping up with the
line rates for most payload lengths 40
bite payloads it's falling a little
short but it's still getting close to a
4 gigabits per second this is with
actually just one of the I XP 28 50s on
the blade the conventional server blade
on the other hand doing packet
forwarding in the end user space is
barely able to keep up with 50 kilobits
per second and so we have about an ATX
improvement in packet forwarding rate in
that case now at the high end of the
spectrum we still seeing a 10x
improvement even for maximum size
packets so just to sort of drive this
point home what the saying is one of
these is doing the work of 80 of these
okay in this kind of context so if
you're doing something that is low-level
packet processing you're a whole lot
better off you know pushing that down
onto a lower level mechanism like a
network processor than attempting to
handle it certainly as a user space
program and even in price performance
terms we see about a 15 X gain here
these network processors don't come as
cheap as these guys do either but even
taking the added cost into account you
still get a substantial game okay so let
me wrap up so our objective here with
this dynamic networking is to enable
rapid deployment of new network services
within a cloud computing infrastructure
so that you can really create the
services that you want to support your
application rather than settling
for the services that are provided by
the Internet as it exists today so we
can remove the constraints are imposed
by the internet and extend the range of
applications that we can handle and in
particular applications that require a
significant real-time person-to-person
kind of interaction component since
that's the those are the applications
that most stress the limitations of the
Internet today the technology needed to
build effective internet scale systems
is on hand we simply need to put them
together in appropriate ways the
commodity 10 gigabit ethernet switches
really are the essential glue that
allows all of these different components
to work together effectively it does
take some you know small amount of
additional edge functions in order to
get effective nonprofit non-blocking
switch performance and get the kind of
application isolation that's needed but
you know this really doesn't take very
much we think that having a model in
which diverse processing components can
be made available and drawn on by
application developers is helpful to get
the best price performance kinds of
characteristics and finally you know
there are lots of you know good problems
that need to be addressed in this
context I think the greatest
opportunities really rest in developing
new kinds of services both at the
network level and applications that can
exploit new kinds of network services to
good effect and so this is where I think
the the most exciting opportunities
exist as we get these kinds of
capabilities available within cloud
infrastructures there are also some you
know issues associated with how you
build the cloud infrastructures of
course I've touched on a few one that I
haven't touched on at all is the control
problem how do you configure and manage
this kind of a large distributed
infrastructure how does the the cloud
infrastructure in
with the users of the cloud
infrastructure that want to you know
create and dynamically modify their
computing infrastructure supporting
their application as it's running and
finally you know at the lower level how
do we make better use of the the kinds
of highly parallel processors and
configurable hardware components that
are now available to us in the most
effective way in these kinds of
environments and we have the ability to
you know really change the the core you
know what we used to think of as just
the network hardware you know at way in
ways we never could do you know 10 years
ago okay and now everything at this
level even at the very you know high
performance end of the spectrum is open
to modification if we can you know
simply you know decide what it is we
want to do with it and how we can best
put it to use so with that I'll wrap up
and I'm happy to take any questions yes
so the rec traffic distribution between
them you know how does that impact
networking
well I are you asking specifically about
this distributed gaming for sure well
yes so well let me you know maybe put
this in the context of the the gaming
application for a minute so in some
respects that gaming application is less
variable than what you're describing
because each of the objects that make up
the game are issuing state updates on a
regular basis say maybe 20 times a
second right and there you know fairly
predictable size so there's not a lot of
traffic variation on the other hand what
happens at any given server may change
as its players move around in the
virtual environment so as its players
move around they may that server may
need to start subscribing to regions of
the game world it wasn't subscribing to
before and may receive different amounts
of traffic going to it now in fact it's
partly for that reason that are an
initial version of this we haven't even
tried to optimize the use of the network
bandwidth we're simply transmitting
allstate updates to all of the routers
all the way around the edge and that
means that no matter how things move we
always have the state updates we need at
each router and so we can deliver them
to the server okay and so you know we
don't need to you know be very clever
about how we predict the use of network
bandwidth
but certainly if we were you know trying
to optimize in particular those
downstream feeds so that we you know say
prune to the multicast tree further up
with respect to you know certain of
those state updates then we would have
an issue because if you if you do that
pruning then at the point where you need
that network bandwidth you want to be
able to get it back and if you if you
haven't you know essentially pre
reserved it you're not going to you
can't count on getting it back and there
isn't really you know any there's no
magic bullet there right if you want
firm guarantees okay so that you can
provide kind of strong quality of
service and there isn't really an
alternative to reserving the capacity
one you know I would need this you know
shrug it off by between like point a and
point B so in those kind of scenarios
how would the not the cloud computing
algorithms for like distributing like
network resources change would they
would they kind of significantly change
or would they I mean is it is it so I
guess my final question is is is your
algorithm like very much dependent on
your application like traffic or is it
sort of so I don't want to give the
impression that we have a solution to
this problem we don't okay so yeah so as
soon as you said your algorithm i
inferred you were thinking we had this
problem solved we don't okay and clearly
it's an important issue right and you
know there's a there's a clear trade-off
I mean even even in this gaming
environment I'm the other thing that's
going to clearly affect the amount of
bandwidth you need is simply how many
players there are in the game session
right session with 10 players is going
to be very different from the session
with a thousand players right
and so you know the first thing you want
to be able to predict is the trajectory
of the number of players you're going to
have and you know I'm not sure how you
would go about doing that right look at
things yeah and it's going to it's going
to change from application application
other questions
so we had a question earlier about the
where does the 6x speed up come from
okay yeah so so if you just had a level
network this becomes what's known so in
the classical literature is a closed
Network and as a classical reserved we
love them by the way okay good so I'm
glad that's not an unfamiliar term to at
least some of you okay now three-stage
closed networks have this you know
well-known property if you want to make
it non blocking your middle stage has to
be twice as big as the edges the first
and third stage right and it's exactly
the same fundamental property that's
going on here but when you take your
three stage closed network and go to
five stages right that you know 2x speed
up increases to a 3x or I'm sorry for X
speed up and when you go from five
stages to seven stages it goes from 4 x
26 x okay and you know this goes all the
way back to nineteen fifty three and
sort of the basic circuit switching
characteristics of these these networks
and this network we're saying here is
essentially a closed network that's been
folded back over on itself okay so you
know in terms of its blocking
characteristics it behaves exactly the
same way and so that's where it comes
from but you know but that's the
briefest answer I can give I can
certainly go into more details there are
more complications that arise as well
because of the the variable size of
different flows okay and I'm largely
ignoring that I'm just sort of
pretending all flows are actually the
same size and if you when you actually
take into account variable size clothes
they can actually get worse than at 6x
so my question was regarding the fan-in
problem you mentioned that to prevent
Fannin from causing queues building and
propagating backwards there is an
algorithm or a distributed state
mechanism that the uninterested have you
get implemented that what does it look
like and what is the communication
paradigm for communicate back to the
source during attention you source
quench across the network yeah so so
we've implemented it you know in a
slightly different context but it's
exactly the same algorithm that's
required and so you know this involves
periodically sending backlog information
and in our case we simply take that
distributed backlog information we use
it to divide up the bandwidth in
proportion to the different backlog so
the simplest kind of throughput oriented
metric you can use for example if you've
got one input that's responsible for
fifty percent of the backlog of packets
going to a particular destination well
you give it fifty percent of the
bandwidth that's available at that
interface okay so that's kind of just a
very simple throughput optimizing kind
of mechanism you can use now given those
application those allocations then each
input is going to attempt to send as
much traffic as it can to each of the
outputs up to the limit it's been given
of course it's also got to respect the
the bandwidth limitations of its ingress
interface to the network and that can
potentially limit it as well and it's
because of the combination of that
output limit and those ingress
limitations that you actually need this
2x speed up if you want to guarantee
that you can provide strong worst-case
guarantees and in the context of router
is the kind of throughput guarantee
you're typically trying to achieve is
what's called work conservation right
that just means that you you know you
never have a link that's idle if there's
some packet going to that link so the
I'm assuming that the switches them you
there's some sort of pulling mechanism
that watches what the queue depth is and
then and sends it back to a centralized
collector how do you actually do it in a
distributed way same the same thing
applies how do you get those information
or the those data back in case the
fabric is experiencing session or just
relying on the the speed up to eliminate
congestion on the back yeah so you know
where this is actually a problem that
happens for example to us sure that
sometimes the state information we want
to send unrelated to us gets chewed off
we're already congested network
right right and so yeah I don't have a
separate answer for that you know you
could obviously I mean what we do is we
engineer things so that the amount of
state that we're distributing is a small
fraction of the overall state being sent
through the network so it's maybe you
know a few percent of the network
traffic is associated with these
distributed state updates now if you
wanted you know to make give yourself
sort of strong guarantees with respect
to that you would probably want to
assign that to a high performance or a
high priority traffic class so that you
could have a high you know assurance
that those packets would get through
even when there is congestion in other
traffic thank you so you mentioned per
packet load balancing yep and the end of
course of potential for out of order
package delivery have you done have you
actually done anything in that regard
where you've implemented per packet
round-robin across links and look oh
yeah and then what's your what if any
experience if you hadn't apologized for
the amount of the assumptions that go
into this right but have you had the
fight with TCP and out of order packet
delivery triggering yeah and that's
transmitted right right right no so I
mean you can you know if you're managing
the overall traffic flow through the
network properly the the the you can
engineer the the probability of outer
border packets to be smaller than the
probability that the wings of your plane
fall off okay I mean you can really
going to make this as small as you want
right you can't make it zero right okay
unless you I mean there are techniques
you can use to make it zero as well but
in this context with well regulated
traffic you can really get very strong
probabilistic guarantees that have
they're not at all dependent on traffic
okay so that's you know important factor
to understand about it is it dependent
on the relative simultaneity of
communicating state information across
the network nope
nope it's just dependent on you know I
mean there there is a probabilistic
limit but it's a probabilistic limit
imposed by the random choice is made by
the algorithm okay the algorithms making
certain random choices and that's what
really you know is the main source of
randomization or you know probabilistic
characteristics here okay and that sort
of offsets any you know negative effects
of the probabilistic traffic right okay
okay thanks very much</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>