<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Visualizing Data Using t-SNE | Coder Coacher - Coaching Coders</title><meta content="Visualizing Data Using t-SNE - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Visualizing Data Using t-SNE</b></h2><h5 class="post__date">2013-11-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/RJVL80Gg3lA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">I'd like to introduce Lauren's funder
Martin who's our speaker today he's
going to talk about a new visualization
technique that works better than any
other technique for laying out large
data sets in 2d Maps so you can see the
structure of them Larenz got his PhD in
computer vision from Tilburg as we say
in English in 2009
he did a postdoc at UCSD with Laurence
Saul in machine learning he's now an
assistant professor at Delft University
of Technology in 2008 he visited Toronto
for six months where we worked together
on developing T's neat I'm very proud of
one of my contributions which was to
think outside the box and say it would
work better if we made the probabilities
of a set of mutually exclusive events
add up to four and lounged I tells me it
works better if we make Matt up to
twelve okay
Lars thank you very much chef thanks for
for having me it's great to be here so
I'll talk today about the work I've been
doing with Jeff and out of continuing
since I visited in in Toronto on
visualizing data using Tizen II so the
setting we consider in this in this talk
is we're given a bunch of high
dimensional objects in this case the
book called x1 to xn and we want to get
a feel for what the structures are the
underlying structure of the date of this
data is we want to know like are there
certain clusters in there is there what
is all sort of the more local structure
of this of this data manifold that is
formed by the body high dimensional
inputs and so the question is well how
can we get a feel how can we get an
intuition for you know how does high
dimensional data is arranged in data
space and if you look at the information
visualization field they have a lot of
they've developed a lot of methods to do
data visualization you may be familiar
with parallel coordinates
maybe these radio graph layouts may be
things like tree maps or I'm sure you've
seen whirls before and if I look at
these ten days all share sort of one
common characteristic or well maybe two
you're all very colorful and nice and
they all only visualize a few variables
that
time and well--since also at Google
presumably you're often in situation
where you have many variables the
question is well how can we visualize
very high dimensional data right how can
we how can we deal with this sort of
very large number of variables how can
we get an insight into those and so a
simple idea to solve this problem would
be to try and build a map in which each
of the high dimensional objects is going
to represent it by a point in such a way
that similar objects are going to be
represented by nearby points and the
dissimilar objects are going to be
represented by distant points and these
points we're going to embed in some low
dimensional map of maybe two or three
dimensions and well if we managed to
properly construct the map then we can
lay out this two or three dimensional
map basically we can visualize it as a
scatter plot and this hopefully gives us
some insight in in the data so here's an
example of that where we have a bunch of
high dimensional data we try to embed it
in a two dimensional space and clearly
we didn't do a very good job because the
basically the distances between the
points in the low dimensional map are
very different from the high dimensional
map and so what we want to end up with
is more with something like this right
where the distances in this low
dimensional map reflect the similarities
in the original high dimensional data
and to do this we're going to minimize
some objective function that basically
measures the discrepancy between the
similarities in the original data and
the dissimilarities in the map and so
this is called dimension reduction or
embedding or multi-dimensional scaling
and a typical technique that you can use
to to to do this is principal component
analysis which basically finds a linear
projection of your higher dimensional
data in such a way that the variance of
the projected data is maximized and so
here's an example of a typical map that
you would get from principal component
analysis so this is a visualization of
5,000 images of handwritten digits from
the taken from the end
data sets and so what I did here is I
just took the original images which have
28 by 28 pixels and I laid them out
using PCA in two dimensions and so each
point here corresponds to a digit in the
data set and the color of the point
corresponds to the digit that is
represented in the in the image and so
what you see here is that PCA captures
some of the structure of this data for
instance the red points on the Left
they're a form sort of a cluster of
zeros and the orange points on the right
they form a cluster once and that it
happens to be the first principal
components so sort of the main source of
variation between digits is basically
between zeros and ones and that makes
sense if you think about them in terms
of pixel values right series and one say
very little overlapping pixels the
second principal component in this case
is on the top you basically get four
sevens and nines which are a bit similar
in terms of pixel values and on the
bottom you get three fives and eights
which are also a bit similar so that's
your sort of your second source of of
variation now this looks all very nice
but that's mainly because I put in the
colors if you were in the in the
unlabeled case where you didn't have
sort of proper labels for your data
you'd see something like this and you'd
not be able to make very much out of it
right maybe on the right bottom right
you would see a little bit of
high-density structure but it would
basically just be sort of one blob of
data so the question is can we do better
and in particular the question is SPCA
is that minimizing the right objective
function if you think about PCA then
basically what it's doing it's mainly
concerns concerns with preserving large
pairwise distances in the map it's
trying to maximize variance which is
sort of the same as trying to minimize a
squared error between distances in the
original data and distances in the map
and because you're looking at a squared
error you remain concerned with
preserving distances that are very large
right so all that's pca is doing is it
wants to make sure that stuff that's
dissimilar that ends up for
parts right like the zeros and ones and
the question is is that really what you
want in a visualization in particular
are those large pairwise distances in
the data are those things that are
reliable and I think the answer is no so
if you if you think about data in terms
of nonlinear manifolds such as the
switch role here then you you would see
that the in this case the Euclidean
distance between between two points on
these manifolds would not reflect very
well their similarity right this the
distance between these two points
suggests that the two data points are
similar whereas if you would consider
the entire structure of the data
manifolds it would actually be very far
apart right so the key idea is that that
PCI doesn't work so well for
visualization because it it preserves
large pairwise distances and those are
not reliable
what is reliable even on these type of
manifolds are very small ones right the
very small Euclidean distances distances
between points and their nearest
neighbors those are still pretty
accurate also on very curved data
manifolds like this so people realize
this in about 2000 and they started up
coming up with techniques that were
basically focusing on preserving this
local structure and preserving small
pairwise distances in the map and an
example of such a technique is ISO map
and basically what it does is it it
tries to it tries to estimate the
distances between points in the original
space via some geodesic distance right
so it basically tries to estimate the
distance over the manifold in this case
and then uses those distances as input
into PCA and it will give you something
like this and you can see that the that
the embedding is a bit better for
instance the cluster of ones which is
the orange cluster here is is fairly
well separated from the rest of the data
but even here if it would take out the
colors you wouldn't see so much
structure another example of the method
is called locally linear embedding and
it would produce this plot on the on the
NS data sets may the locally
locally linear embedding is very similar
to to tease me in the sense that it
really tries to preserve small pairwise
distances but it has a kind of funny
thing where what it really likes to do
is collapse all points into the origin
and so what you see here this is
basically the origin of the of the data
and then it uses a bunch of outliers to
satisfy a very simple covariance
constraint that it has on this embedding
and so typically what you see if you do
embeddings with locally linear embedding
is you see a whole bunch of points
collapse and then you see a few strings
flying out basically to satisfy this
covariance constraint that it has now we
basically build on the idea of locally
linear embedding and we try to come up
with a better technique in particular we
try to come up with a technique that
doesn't have this this problem of trying
to collapse all points onto a single
points and so this is the T distributed
sarcastic neighbor embedding technique
or T's nee abbreviated and here's sort
of how it works so in a high dimensional
space we're going to measure
similarities between points and we're
going to do that in such a way that
we're only going to look at sort of
local similarities so it's similarities
to nearby points so the red point here
is point X I so this is a point in the
high dimensional space and what we're
going to do is we're going to Center a
Gaussian over these points and now we're
going to measure the density of all the
other points under discussion so in this
case of point XJ so we're going to
measure the density of all other points
on our discussion and then we're
basically going to renormalize which is
the bottom part of the of the fraction
here and what this gives us is a set of
probabilities P IJ which basically
measured the similarity between pairs of
points I J you can think of this as a
probability distribution over pairs of
points where the probability of picking
a particular pair of points is
proportional to the similarity right if
two points are close together in the
original high dimensional space you're
going to have a large value for p IJ
if two points are dissimilar or far
apart in a high dimensional space you're
going to get a pIJ that is basically
infinitesimal all right so now we have
an idea of of what the similarity is the
local similarities of points in the high
dimensional space or and so now the
question is how we're going to use them
I mean in practice we we actually
compute the pIJ values a bit differently
so what we do in practice is were not
computing joints joint probabilities
over pairs of points where we're
actually going to compute a conditional
distribution so everything that has
changed here is basically the bottom
part of the fraction here we don't
normalize over all pairs of points but
the only over pairs of points that
involve basically point X I and the
reason we do this is because it allows
us to set a different bandwidth Sigma I
for each point and the way we're setting
the bandwidth is basically in such a way
that the conditional distribution has a
fixed perplexity so you can think of
this as basically scaling the bandwidth
of the Gaussian in such a way that a
fixed number of points fall in the modes
of this of this Gaussian the reason we
do this is because different parts of
the space may have sort of different
densities and this trick basically
allows us to adapt to those different
densities and then next what we do it's
sort of a hack where we say well the
joint probabilities rights are this
distribution over pairs of points P IJ
is just going to be the symmetry the
symmetry version of the conditionals
right so we're going to take the two
conditionals involved P of J given I and
P of I given J we're going to basically
average those and that's giving us the
final similarities in the high
dimensional space so now the question is
well what what are we going to do with
those well so what are we going to do is
now we're going to look at the low
dimensional space so this is a two or
three dimensional space right this will
be our final map and we're going to lay
out points in that in that map so we're
going to represent each high dimensional
objects by a point in this
dimensional map so again red square here
is is point I'm in the low dimensionals
case I will call my sub I and we're
basically going to do the same thing
here so again we're going to Center some
kernel over this point y sub I and we're
going to measure the density of all the
other points Y sub J under that
distribution and again we're going to
renormalize by dividing over all pairs
of points and so what this gives us is a
probability Q IJ which basically
measures the similarity of two points in
the low dimensional map right so now
what we want to do is what we want is we
want these probabilities q IJ to reflect
the similarities p IJ which we computed
in high dimensional space as well as
possible right if the Qi J's are
basically identical to the P IJ s then
apparently the structure of the map is
very similar to the structure of the
data in the original high dimensional
space and well the way we're going to
measure the basically the difference
between these P IJ values in high
dimensional space the q IJ values in the
low dimensional map is by
kullbackleibler divergence which is sort
of the standard measure of a sort of a
natural divergence natural distance
measure between probability
distributions and take the form that I
have here on the slides so it's a sum
over all pairs of points of P IJ times
log P IJ over Q I J and now what we're
going to do is what we want is we want
to lay out these points in the low
dimensional space in such a way that the
schoolbag library divergence is
minimized right in such a way that the q
IJ values are as similar as possible to
the p IJ values and in order to do that
we're basically going to do gradient
descent in the school bed live with
their versions which boils down to just
moving the points around in such a way
that the school bed like the divergence
becomes small now the only thing
well so why do we look at a Kubek liable
divergence why does this preserve local
structure well this basically has to do
with the asymmetry of the koaclarck the
divergence if you think about having two
similar high dimensional points right
two high dimensional points that are
close together these points will have a
large pIJ value right if you have a
large pIJ value in the collaborative
virgins
well then you better make sure that
these two points that this pair of
points also get gets a large skew IJ
value because otherwise you're going to
suffer a large penalty to go back like
the divergence right otherwise you're
going to get a large value P IJ times
the log of a large value of P IJ divided
by a small value of q IJ which is
basically going to blow up right so what
this cool big library of versions does
is it really tries to model large p IJ s
so similar high dimensional points by
large q IJ so points that are close
together in the two dimensional map it
doesn't work the other way around so if
you have two points that are very
dissimilar so that have small p IJ then
you don't really care about what the the
corresponding q IJ value is right so
we're really only looking at getting
sort of the local structure of the data
right now the only question I didn't
answer yet you may have noticed it in
when I was computing the Qi J's I wasn't
using a Gaussian kernel right I wasn't
using this thing e to the power of minus
1/2 times the squared Euclidean distance
between the points but I was using
something else what I was using was
basically this thing which is a student
T distribution with one degree of
freedom which is a distribution that's a
lot more heavy tilt than the Gaussian
distribution and so the question is well
why why do I use this student T
distribution and the reason is as
follows so let's suppose our data was
intrinsically high dimensional right so
for instance it was uniformly sampled
from like a 10 dimensional hyper cube
and we're going to protect this data
down to two dimensions right and so as a
result we can never preserve
all the pairwise distances accurately we
need to compromise somewhere and well so
as we saw what te was doing was his
truck was trying to model local
structure right it was trying to get
similar stuff close together in the map
and what that means is that as a result
of that that this similar points right
so dissimilar high dimensional data has
to be modeled too far apart in the map
now I will illustrate that with a very
simple example where I have three points
in two-dimensional space and the red
lines I'm going to call basically the
small distances right to the local
structure and the distance between the
two other the two points on the on the
corners of this triangle I'm going to
call that global structure right so a
large pairwise distance I'm going to
embed this data in 1d while preserving
local structure well I can do this
perfectly well right I can do this
embedding into into one dimension and
you can see that the the two small
pairwise distances are completely
accurately preserved but what you also
see happening is that the distance
between the two points that are far away
have half drone right these two points
are now further apart and they were in
the original data right before they were
basically square root two apart and now
they have a distance of two right and so
if you're embedding intrinsically high
dimensional data into low dimensional
maps this happens a lot right there are
a lot of pairs of dissimilar points and
these dissimilar points they basically
have to be modeled too far apart in a
map now by using this heavy-tailed
distribution I'm basically allowing that
to happen so if I have to if I have a
pair of points that have a pairwise
distance of let's say 10 let's say that
under the Gaussian that gives me a
density of 1 then to get the same
density under the student-t distribution
right so the density of 0.01 because of
the heavy tails of the student-t
distribution these points have to be 20
or 30-year parts right so for dissimilar
points these heavy tails q IJ is
basically allowed these similar points
that we model to far apart in the map
all right so let's let's run T's neon
the same M nice the data set so what you
see here is basically teasingly running
grading
since doing the learning doing the
minimization of the Kubik library
virgins and what you see appearing is
basically a lot of a lot more structure
than you had in the PCA arise on map
plots right what you can see here is
that the the ten clusters at ten
different digits that they're fairly
well fairly well separated in this in
this low dimensional map if you run it a
little longer you'll even get something
that looks like this right where you get
quite large gaps between the different
clusters you have to keep in mind that I
just ran this on just based on the pixel
values right and that the the labels of
the digits were not used in producing
the embedding the only time I use the
the labels of the of the digits is
basically to color the plots right so
but it's a completely unsupervised thing
and so apparently if you if you think
about digit space a lot of the digits
are actually very very well separated in
this digit space already which basically
which basically means that we can that
we can learn to separate them quite well
now if you plug back the original digit
images you would see something like this
and what this shows is basically that
also a lot of the local structure of the
data is preserved in the embedding for
instance here you can see the zeros
going from round zeros on the left to
more elongated ones on the right if you
zoom in on the top here you basically
see a lot of orientation a lot of
orientation differences between these
digits and what you can also see is for
instance on the bottom of the sevens you
can see a little cluster of so called
continental sevens which are the cells
that have a little horizontal crossbar
and they are separated from from the
rest of the sentence if you look closely
between the knives in the forest there
will be a few digits for which it's very
hard to make out whether they're
actually a nine or four and you see that
showing up very well in the in the
embedding here right so what this is
showing is that we didn't just preserve
sort of the global layout of the data
but that we also preserved a lot of
local structure now the way I use this
is when I'm doing I'm somebody who comes
from machine learning so
do a lot of feature extraction from data
in this case from images of of texture
that were basically made under
photographed under different few points
and so here for instance I was
interested in developing texture
descriptors that are invariant under
locally affine transformations and once
I've developed these features these
descriptors I can basically use Tizen e2
to evaluate whether these features
actually capture what I was interested
in and so I made at each knee plot based
on these features and then you see
basically that all the brick walls
cluster very closely together
irrespective of under what's under what
few point they were photographed right
so apparently the features that I
developed really captured what I was
after namely this locally aligned
invariance so here are some other
examples of that
for instance of I think this is some
type of humanly water and here's an
example of basically carpets right so
what you can see is that that each knee
can help you basically in developing and
evaluating new features it's going to
give you more information than just
measuring the classification error based
that you get based on based on some
feature representation because it's also
going to give you some insights into
what this feature representation is
capturing and then maybe also what it is
not capturing now it's interesting to
look in a bit more detail in into the
gradient of the school with like the
divergence that we're minimizing so the
gradient with respect to a point right
so basically how do we have to move a
single point y sub I in the map in order
to get a low a lower cool back like with
our versions takes this form if you look
at it closely you can basically see it a
consists of a spring between a pair of
points in this case between points F N
and C and the individualization and
disorder term is basically a term that
measures the amount of exertion or
compression of the spring so what you
see is basically a difference between p
IJ - similarity in a high dimensional
space and qi j similarity in a low
dimensional space right so if
if the cue IJ is perfectly modeled the
pIJ s then this term would basically be
zero right there would know there would
be no force in the spring and what this
sum is basically doing is it's taking
all forces that acts on the single point
and it's summing them up right so the
way to think of this is all points
exerted force in this case on on Point C
and what we what we're after is
computing the resultant force on that
point C right and that resultant force
basically tells us how we have to move
the point in order to get a lower orbit
like the divergence now one limitation
of this is is that we basically have to
consider all pairwise interactions
between points right if we have end
points then we basically have to look at
N squared interactions between points
and those we have to sum basically in
every gradient update and this is sort
of limiting if you're going to visualize
data sets that are larger than let's say
five to ten thousand objects and so what
I worked on is on an approximation to
these forces that basically operate in n
log N and as a result that skills are
much better to two large data sets and
the intuition for this approximation is
that if you have a bunch of points in
this case a B and C that are close
together and that exert a force on some
points in this case I that is relatively
far away then these three forces will be
pretty much the same right they will be
very similar and as a result what we
could do is we could basically take the
center of mass of these three points
compute the interaction between the
center of mass and the point that we're
interested in multiply it by the number
of points involved in this case three
and use that as an approximation for
these three forces and this is what is
called the barns Hut approximation it
originally comes from astronomy where
astronomers try to basically model the
interactions between stars they have
very large galaxies of stars so they
need this type of analog and network
so the way we implement this in practice
is in this case we did it fire poetry so
poetry is sort of a space partitioning
tree so the partitioning is on the map
here right so on the two-dimensional map
that we're constructing and basically
each node of the tree corresponds to a
cell so the root node corresponds to two
basically the entire map and then it has
four children each node has four
children which basically correspond to
the smaller the smaller the four
quadrants of basically the cell of the
root nodes or of the parent nodes and
what we're storing in each of these
nodes is restoring the number of points
that sits in a cell and we're storing us
the center of mass which is this little
blue circle that I plotted here and
we're basically building a four quad
tree which means that we're proceeding
until every cell contains only a single
data point right so a single point in
the map and we can use this quad tree to
quickly do the born approximation so
what we do is we're basically going to
do it that first search on this tree and
so every so in this case we're
interested in computing the interactions
with point F which is down there and
basically at every at every point in the
depth-first search we're going to ask
well is this cell is this far enough
away from point F and this is small
enough such that we can basically use
the cell as a summary for the
interactions well in this case in the
root case well we're basically looking
at the whole map so that's probably
going to be too big so this condition is
going to fail so we're going to do it
that first search and we get to this
cell and let's say in this case that our
condition says well you know this top
left cell that's small enough and far
away enough from point F so that we can
use it as a summary nodes
well then basically what we can do is we
can take the center of mass that we
stored in these nodes we can compute
your interaction with point F we
multiply it by the number of basically
the number of children the number of
points in this in this
which was three we also store that in
the quadtree in the in the country that
we built and this gives us basically a
summary of the interactions between
points a B and C on one side and F on
the other sides right and so then we can
prune basically everything that sits
under it and we basically are going to
continue our debts firstsearch right and
this gives you an analogue an algorithm
for basically evaluating the gradients
and as a result the skills up a lot
better to very large datasets so to show
you an example this is a visualization
of the full amis data set which contains
70,000 digits and this was made in about
10 minutes on a single core which with
the original N squared algorithm would
be completely unthinkable it would take
many many days to construct something
like this and you can see that also in
this map that the the structure of the
data is is preserved quite well right
that you still get sort of these ten
separate clauses if we plot back the
original digits again we get something
like this and if you zoom in there you
can still see that a lot of the local
structure is there so here you see a
little bit of orientation showing up you
see going from thin ones on the rights
to thicker ones on the on the left so
here's another example of a whole bunch
of zeros which are apparently quite
similar and here's a visualization of
takes a little bit of sixes going from
sort of more elongated sixes on the left
to more round ones on the on the right
as a second example this is a
visualization I did of the C for ten
datasets which is a data set of images
of 32 by 32 pixels that were crawled
from from Google Images and they
basically have ten different labels
which are listed on the on the top right
here and the representation I built here
was basically a bag of freezer words
representation using spatial pyramids
and so then this is the map that I guess
what you can see is that apparently
representation is not being a is not
able to capture sort of the the really
able to capture sort of the differences
between these ten different classes so
will be interesting to for instance try
all the deep learning stuff that Google
is doing get those features put them
into tasting and see if you get a better
map you shoot still even the back of
visual words if you plug back the
original images is able to capture quite
a bit of of structure you can see that
sort of in the plot here and if we if we
zoom in we see for instance that the see
for ten dataset contains a lot of
pictures of red cars which are all
grouped together here
it likes to group together planes on the
right and boats on the left because they
all are sort of a structure with strong
edges on a very uniform backgrounds
here's another example of apparently
there are a lot of animals in the in the
sea for ten datasets which are all sort
of grouped together and this was a nice
little cluster of people making images
of their dogs now to give you a google
examples it is a visualization I did
actually yesterday of the street view
house numbers data set so these are
about 600 thousand images that look that
look like this and the representation I
used here were basically histogram of
oriented gradient features and based on
those features I laid those out in a map
mistook I think about one and a half
hours on my on my laptop to construct
this map what you can see here is
actually a quiet good separation between
the different classes although some
classes will be split up into parts and
one thing that I think might be going on
is that they're sort of small
translations between the images that
basically cause a large effect in the in
the histogram of oriented gradient
features right because the edges are
basically moving and I think that's why
you get the the separation of the
classes here so the the biggest data
said I ran this on so far as the timid
dataset which is a a speech data set so
it contains about 1.1
million instances that are represented
so little speech segments that are
represented using M FCC features and so
the labels here are basically phone
labels for this D speech segments and so
this one takes about three and a half
hours to build on a on a single core now
there's one last map which I want to
show you so this map wasn't done by me
it was done by Joseph Turin at the
University of Montreal but I'm going to
show it because it's very nice and also
because it highlights a problem that
we're going to try and solve afterwards
so it's a word it's a map of words so
basically what what Joseph did is he
learned a feature representation for
these words basically from Wikipedia
it's type of word embedding model and he
use these features as input into Disney
and you see a lot of structure in this
model so for instance you see a lot of
spatial locations and it's actually
going from locations inside the US on
the top to locations outside of the US
on the bottom you get a nice tight
cluster of all your 12 months there's a
there's a cluster of names and actually
goes from first names on the top two
last names on the bottom so what you can
see is that you know these embedding
features apparently capture a lot of
these sort of fine-grained semantic
information here's a cluster of of words
that there are all basically related to
counting to numbers here's a very large
cluster of all sorts of adverbs there's
a nice little sports cluster now the
reason I'm showing this map is because
it also highlights a problem namely if
you are modeling if you're trying to
embed words if you're trying to
visualize words based on semantic
similarities in a single map you can
never do it quite right so let's suppose
you want to and you want to model the
words riverbank and bailout in a single
map well the word river as a semantic
similarity to Bank the word Bank has a
semantic similarity of the bailouts but
the words River and bailouts have
basically nothing in common right and so
in a single map you can never do it
right because if you're going to put
River
close to bank and bank closed the
bailouts River and bailouts by the
triangle inequality are going to be
close together so if you have data
that's basically of which the similarity
structure the underlying structure is
non-metric right it doesn't satisfy the
triangle inequality you can never do it
right and this can happen if you
visualize words based on Association
data but also if you were visualizing
authors based on co-authorship I could
have written a lot of papers with Geoff
Hinton Jeff Hinz and maybe has written a
lot of papers with Terry Sejnowski
but I never wrote a paper with terracing
offski so we cannot be visualized
correctly in a map so what we did is we
try to get around this problem by not
constructing a single map but by
constructing multiple maps so what we're
going to do is we're going to give each
words and embedding in each of the maps
and we're also going to assign a weight
to each word in each of the maps and now
all of a sudden we can we can correctly
model this river bank bailout example
because what we can do is in map one we
can put River and Bank close together
with high weights in this case weight
one for river and weights a half for
Bank and so the word bailout is also in
map one but it has weight zero right so
that's why I'm not visualizing it so
that basically puts River and Bank close
together and now I map to we're going to
put bailouts and bank close together
with high weights and we're going to
give River weight zero and that's in
that second map and as a result what we
get is we basically unraveled these two
different senses of the word Bank right
so the way we mathematically define it
is we're going to redefine this
similarities q IJ which were the
similarities between basically word I
and J under the model in this case the
multiple Maps model and so what the I
sub I super M is is basically the
embedding of word I in map a map m and
this pi here is going to be basically
the words of that sorry the weight of
word I in map M
and so what we're computing here is the
similarity between two words in map M
between two between words I and J we're
going to multiply this with the weights
for these words involved and we're
basically going to sum over all maps
right and this gift is basically a new
definition of of the similarities q IJ
under the model and now we're going to
do when we're learning is we're
basically going to take the same Kobach
lied with they're virgins and we're
going to minimize it again using
standard gradient descent
except we're not now just minimizing
with respect to the locations of the
words in a single map what we're doing
it with respect to the locations of the
words in all of the maps and with
respect to the weights of the words in
this match right so we're basically
taking all these parameters and we're
going to learn everything simultaneously
now we apply this on a data set of words
Association data that was gathered by
psychologists so there are about 5,000
words and basically the way they
gathered the data was they took human
subjects they would they would tell the
human subject of words and the subject
would have to respond with sort of the
first Associated word that that came to
minds so we repeated this a whole bunch
of times and basically what that gives
them is a probability given that I say
the word I what is the probability that
the human subject will produce the word
J right and so these similarities are
exactly the type of similarities that we
can use as input into G is nior multiple
Maps this knee in this case so we made a
visualization of of this data in in 40
Maps and so you see two of the maps here
and what you see in the top part is for
instance the word cheerleader which
apparently has a strong association with
words that have to do with sports right
like a basketball football tackle stuff
like that which is a you know a clear
association of the word cheerleader now
if we look in one of the other maps we
also see the word cheerleader but here
she goes with gorgeous and sexy and
beauty which is a completely different
type of Association of the words
cheerleader that
has nothing to do with sports right and
so the multiple maps model basically
automatically learned this as a second
example so here are two of the two of
the other maps so let's take as an
example the word monarchy which is here
modeled close to basically state form
selected more Percy and Republic and NRB
even and in one of the other maps we see
the association of the word monarchy
basically with the page entry
surrounding the monarchy right like king
and crown and royal and stuff like that
right so this sort of very subtle census
are actually picked up from the word
Association data in these visualizations
now on my website I actually have sort
of an interactive d3 tool that you can
use to play around with all 40 maps I
was going to show it but the laptop is a
bit too slow to to do it here but you
can check that out later if you're
interested so to conclude I hope that I
have shown you that by visualizing high
dimensional data in maps that this
basically may lead to sort of more
insights into your data and that TCE is
an effective and also a relatively
efficient way of of producing such maps
and I should mention that this thing has
already been applied in a lot of domains
for instance bioinformatics computer
security climate research cancer
research
and hopefully soon also in a lot of
problems that Google is working on now
if you're interested in trying this out
there's code in various languages on my
website in the MATLAB Python C++ and
even our I think so it should be
relatively easy to to try it out for
yourself and that's all I wanted to say
thank
yes very nice so when you're
running gradient descent and kind of it
utilizing each iteration the structures
seem to be pretty constant and then you
get a picture of the final result and
mm-hmm yes oh that was actually not the
same run so one of the things is that
basically all these all these methods
because they only look at basically the
discrepancy between pairwise distances
there for instance invariant under
rotations of the map right if I rotate
the map then the cost function is not
going to change because the pairwise
distances aren't changing right so the
the objective function is a non convex
function as a result you're going to get
slightly different results every time
you run is basically from what I've seen
they all are quite quite similar but you
have to be a bit careful in how you
interpret the maps right so you have to
take into account that it's really only
trying to get sort of local structure
right so if a cluster of ones and zeros
if that's a distance like this apart or
a distance like this that basically
doesn't mean anything right so beyond
sort of a certain distance you should
not assign meaning to the distance as
you see in the map right it's only sort
of the smaller distances that are that
are really telling you something at
present no but would it would be great
if you would make one
okay so open demo put a vacuum work
actually a work table and what a lot
stro reputation so that was on top
concept that the party see or education
under the word so yes yes that's a great
question so I have been I've been trying
to do that but so far I haven't been
able to get it to work right so it would
be really nice if you sort of would be
able to make a hierarchical
visualization we're sort of in the top
level you basically in a sense group to
gather groups of points or words right
and only lay out the sort of
similarities of these groups and then in
the lower levels basically work out
these clusters and then lay those out
and would be able to sort of connect it
all together so up till now I wasn't I
wasn't really able to to get this to
work and one of the main reasons was
computational until sort of earlier this
year we were only working with the N
squared algorithm also in producing for
instance the multiple maps plots and
it's got quite slow over time right
because you if you if you're sort of
building building multiple
visualizations simultaneously which are
all N squared and then all also interact
then it gets really hard but I am
planning on basically working on that
idea again now that we have sort of such
analog and approximation and and maybe
are able to do it a lot faster but it's
a very very interesting idea yeah yeah
misunderstanding like the display shows
those are all the images that were
trained on not it pulled out this find
the mapping no so it's it's a it's a
good question so the the maps that were
learning is we're basically doing a type
of non parametric luring right where we
are only learning how to
who represents images by points in a map
where we're not actually learning a
function from the original high
dimensional space to the map right so if
we now get sort of a new instance if
when you get a new dated image we don't
really know where to put it in the map
now I've been doing work I didn't
present it here on basically
parameterizing this function using a
deep network deep neural Nets we would
basically directly learn the function
and then you then you were able to also
do generalization to two held out data
and that works actually quite well but
this was before the time when there was
you know dropouts and and also very big
Google computers at least for me so yeah
but it is definitely something that's
possible except for in these plots that
I'm showing really the only thing we're
interested in is in in visualization
right so I wouldn't advise you to use
them to generalize or as part of a
classification system it's really for
inspection of your data yes it's like
picking the number of clusters in the
clustering technique
no it's quite hard so one of the things
I've been doing is you can basically in
the originals in the original input
similarities you can look for each words
what are sort of my five nearest
neighbors or my ten nearest neighbors
and now I'm going to measure the same
thing under the model right so based on
the cue IJ values and now I'm basically
going to cross validates over the number
of maps are basically measuring okay
what percentage of nearest neighbors
they don't have preserve in my model but
that's all really only a heck it's a
little weird for me to have an intuition
on the same map now so so each so the
way we implemented the model now at
least
each words has only one location in each
of the maps right and so in within a map
we cannot really split a word it's
something that we have tried at some
point but we at that time at least we
couldn't get it to couldn't get it to
work
right so the different senses of the of
the word sort of different different
types of associations I should say right
because not really senses those are sort
of separated over over maps
mm-hmm I'm not exactly sure that I did I
understand the question I mean if you if
if you pick a sense for a particular
word right then sort of in the in the
cluster around it they're going to be
all words that are so related to this
sense right that's basically what you
end up with right now you're around will
make sense okay maybe we should take
this offline all right yes yes it's it's
actually quite Roberts so basically what
the perplexity is doing is its
determining the size basically the
variance of the Gaussian distributions
that I use to measure similarities in
high dimensional space and what it's
doing you can think of this perplexity
as a type of effective number of nearest
neighbors right so if you set the
perplexity of 30 you could roughly say
that they're about 30 points that are
sitting in the mode of this Gaussian
right that's the way you set the
difference now in practice for at least
from my experiments it seems like it's
quite robust to this perplexity value
right then you typically for larger data
sets you want to set it a bit larger
right because you you because they're
basically just going to be sort of more
points close to you that are similar
right so you want to look at you will
basically want to interact with more
points but for sort of data sets of like
five or ten thousand points you would
typically be somewhere between 20 like
10 and 50 something like that but it's
quite robust students you will see that
if you make it very small that it will
basically start seeing more clusters
right because it's looking sort of at a
very small scale and at a very small
scale sort of these larger clusters they
basically split up right so that's
something that you
and see happening so the problem with
that is that in the high dimensional
space basically the tails of this
t-distribution they're going to blow up
right they're basically going to contain
all your probability mass so we have
tried it in the past but it didn't work
very well
so at present we haven't looked at that
so it's probably something I would do is
I would train some sort of let's say a
factor analysis model right before I
would do the pca so you could say well
if I take my original very high
dimensional data and I use a linear
model to project it down until let's say
100 or 200 dimensions then I'm not going
to lose so much structure well I might
lose structure but I wasn't going to be
able to visualize that in two dimensions
anyway right so I would probably do
something like training factor analysis
model do am over your missing variables
and then use the resultant
representations you get for your
your-your-your sparse feature vectors as
input into T so we'll do something like
that
mm-hm
so basically in high dimensional space
you are able to plug in any distance
measure that you have right so if you
know that you know cosine distance is
sort of a more appropriate distance for
your feature vectors then then Euclidean
distance you can just plug that in your
Gaussian kernel yes exactly
yeah so in so in the case that your
inputs are really sort of conditional
probabilities right that's sort of a
natural form of your data which happens
in the World Association case this data
will be non symmetric it will be it will
basically violate the triangle
inequality right it will be the
similarities will be intransitive
and so this multiple Maps model is able
to deal with that by basically if you
get this in transitive similarities by
basically splitting up sort of the
middle point into two senses and
modeling both parts of the relations
separately so in the word Association
data we don't use this Gaussian at all
right are we immediately get our inputs
which are basically this probabilities P
IJ right we immediately get those
because that's the natural form of the
data right so then we immediately try to
model those probabilities in the in the
map or in the maps okay Lawrence is
going to be around for the rest of today
and we have brandy so if anybody has
further questions or just said you can
find them on the web send email de
Emily's ordinary account and they will
be hang on</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>