<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Remote Profiling of Web Server Resources Using Mini-Flash Crowds | Coder Coacher - Coaching Coders</title><meta content="Remote Profiling of Web Server Resources Using Mini-Flash Crowds - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Remote Profiling of Web Server Resources Using Mini-Flash Crowds</b></h2><h5 class="post__date">2007-12-20</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/sOvMOlHzvG0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">we'll go ahead and begin I
want to let you know that we're going to
have a small luncheon afterwards and so
you're welcome to join us downstairs at
No Name we have a table arranged for 46
rate people it's we had some folks in
advance who said they wanted to attend
and so we'll do that so I would like to
take a moment to introduce our speaker
who's coming to us today to speak about
some of the characteristics of
networking kayla has a background in
traffic and monitoring and traffic we're
saying estimation and engineering i
think is the is the term you're using
given what's going on with us and how
we're planning to coordinate and arrange
data centers coming up this is a
particular interest so please join me in
welcoming Aditya I Kayla Thanks ok so
it's a pleasure to be here so the title
of my talk is remote profiling of web
servers using a technique we called mini
flash crowds and this is something we
have been working on for about an ear
and this is kind of a bit of a departure
for me in terms of my core interests my
core interest like Gregory had mentioned
are in traffic engineering and routing
this is more to do with servers and data
centers and with this basically started
out with the question of okay if you
look at a typical service today what is
the most constraining resource is it the
bandwidth or is it some component of the
application infrastructure how do you
tell this and more generally speaking
how do the different components of
servers impact the response time that
typical users see so we believe that
with this tool we have some initial
insights into some of these issues and
this is joint work with my student at UW
Madison pret up and with folks at CMU
AT&amp;amp;T and IBM okay so so I'm going to
start with basically defining what i
mean by provisioning and you know just
to clarify this term with other common
uses bye-bye provisioning I basically
mean configuring a web server for
example determining you know how much
hardware to provision kind of you know
how much network bandwidth to provide
for the server how much cpu processing
how to provision
the back in data processing subsystem
how much memory and also various
software configuration issues just
bought software to what server software
to use how to set the parameters for
example the number of threads cache size
query front end what have you so why is
determining this configuration or
determining the right way to provision
these servers important well optimally
you want to ensure that the the right
configuration for a web server is able
to provide reasonably good performance
to the expected clients under a broad
range of operating conditions right so
you want to make sure that when you know
things are likely loaded you know
everybody stays happy or you know under
certain situations where there is a
sudden interest in a particular stored
media file or there's a certain interest
in a particular object in a shopping
cart even then the clients continue to
stay happy and they get the expected
response times and so on now determining
this optimal configuration that works
under a broad range of operating
conditions is a black art it's not
really easy to do this and so what most
people do is they kind of heuristic alee
estimate that okay so this is roughly
how many clients I expect to come see my
server and based on these expectations
this is how much bandwidth I will
provide and this is kind of how much
back and data processing and and memory
I will provide and I just configure my
server this way and hope that it works
and those that have money the bigger
websites for example tend to over
provision these resources or sign up
with some third-party content
distribution networks now with these
techniques like your heuristic Lee
estimating or even over provisioning
server operators typically are left with
very little sense of how exactly their
server infrastructure will function when
subject to some kind of a sudden
overload right in particular how exactly
you know different resources or
components of the server behave when
subject to are subjected to a you know a
sudden increase in request volume so
this kind of certain increases can
happen due to things like flash crowds
where there's a sudden interest in a
particular piece of content or due to
attacks or even due to planned events
such as like a website
right and the the inability to provide
consistent performance under these
certain overload conditions or at least
to gracefully degrade performance under
these conditions could actually piss
some clients off and they may go to your
competitor that's not something that you
necessarily like right so you want to
continue providing reasonable service
under these conditions so it's not
without hope there are definitely some
techniques today to to kind of
understand the server performance under
these conditions so the most common
thing to do is to kind of try to
replicate the server setup in an offline
kind of a lab setting you know have a
bunch of clients over the land have them
make some requests monitor the behavior
of the server but the problem with this
approach is that it gives you a very
broad overview of the performance of the
server architecture under some kind of
request volume but it doesn't really
tell you ok so this is the constraints
that individual resources on the server
have or this is how they impact the
overall response time and the problem
also is that you know this is an offline
kind of a setup and it's almost
impossible to create all of the
dependencies that you have in a live
online setting in an offline word so so
our work is basically this this approach
called mini flashcards which is
essentially a a light weight distributed
profiling service which can be used to
obtain detailed information about the
performance of individual server
resources when subjected to a sudden
increase in load okay so so the key
salient features of our approach is
firstly that the server stays online
okay this is a live server we are
testing against a live server we use a
bunch of real clients to perform these
tests so are our clients are distributed
across the world and they make a bunch
of requests at this live server we can
employ this tool in a black box or a
white box setting what I mean by this is
a server can just come to us and tell us
oh I have this server can you test us
and tell us what's happening and give us
no other information about the server
will be able to give you some reasonable
inferences or the server operator could
be more cooperative and tell us things
about you know how data is organized at
the server you know some idea about what
they are using and that will help us
fine tune some of the observations that
we can make
and it is very non intrusive and we care
about this because we are testing
against the live server we don't want to
perturb its operation in any way in any
particular way and we really envision
our tool as kind of a third party
service that is deployed across a bunch
of wide area clients and you know in
addition to understanding the
constraints of individual server
components we believe that the
observations that you can get from such
a tool can be used in things like
determining okay how to shape my
requests when there is a sudden spurt in
certain kind of request how to schedule
my requests or to make decisions like
okay went to provision additional
resources dynamically and so on but the
surprising thing that we realized as we
went on is that this kind of a technique
is actually also useful to do debugging
of the configuration so we actually
found some surprising issues in the
configuration of some live servers which
helped the operators kind of fix some
things in their configuration when we
did these experiments okay so so he is
kind of a quick outline of my talk and
I'm going to start with describing kind
of the design goals of our approach and
go over the approach in some more detail
but I want to quickly stop here and make
sure that everybody understands the
problem setting so far okay all right so
so like I said our goal is to provide
very detailed observations about the
performance of individual server
resources and just to kind of more
formally state this what we want to say
is statements such as the following for
a given server a particular resource X
can handle at most n requests so if it's
not resources and requests and
simultaneous requests before clients
start to see a certain T millisecond
degradation and response time okay think
of it the other way what we are saying
is when n clients simultaneously maker
requests requests at a particular
resource the median client or the
average client starts to see a tea
millisecond degradation in response time
and we want to be able to say this for
different resources X at the server and
I get said earlier these are kind of the
requirements of our approach firstly we
want to be able to isolate different
resources you know we want to be able to
do this for different resources of the
server also software and hardware
and we want to be realistic we want to
use clients that are outside we don't
want to take the server offline and
another key requirement we have is that
the approach should be tunable for
example they could be a search engine
that cares about you know 50 or 100
millisecond increase in the time it took
to compute the answer to a query versus
a site that hosts stored media or like
Colonel is source which does not really
care about such a tiny increase in this
one so we want in both of these settings
to be able to give fairly accurate
answers and the key challenge in in
doing this is you know how exactly do
you isolate and study different
resources especially when you have a
server in a black box setting and if you
are even if you are able to do this how
exactly do you impose a certain amount
of load on that particular resource and
understand how that resource behaves and
this is challenging because you have a
bunch of clients that are distributed
all around internet it's hard to
coordinate their actions so I'm going to
describe to you how we address these
challenges but first I want to kind of
give you the setup we have of our
approach and then I drill down into the
details of how we address these channels
okay so our approach is mini flashcards
and you know we have a target server
that we want to run our test on we are
assuming it's a black boxer we know
nothing about it this is this test is
run by a bunch of nodes which are
controlled by a central node called the
coordinator the coordinator controls two
sets of nodes a bunch of clients and a
bunch of measures before starting the
mini flash code experiment the
coordinator does a bunch of
pre-processing I'll describe to you what
this pre-processing is and based on the
information collected during pre
processing the coordinator issues
commands to a small subset of the
clients and to all the measures
following which the clients and the
measurers make some requests at the
target after that the measurers report
what they saw when making these requests
to the coordinator and based on this
feedback the coordinator then decides
okay I am going to ask a slightly larger
number of clients and the measurers to
go make a request to the server and this
process continues until you make a
difference
so just reiterating the coordinator is
smart it determines how to schedule
clients what request these clients make
you know and in what order they need to
make the requests and it also determines
how to progress the experiment increase
the load or decrease the load and what
what influences to make and the rest of
the nodes are pretty down they basically
just listen to what the coordinator says
so so given the setup so our first
challenge was how exactly do we isolate
the different resources and study the
performance of individual resources and
these are the resources we are talking
about and the key kind of observation
which is kind of silly if you think
about it that we use is that you know
when a client makes a request for a
certain object at the server depending
on what object the client is requesting
some resources the server ends up
getting exercised a bit more than some
other resource right so this is exactly
the observation we use when you know in
a particular situation when the
coordinator asks each of its clients to
request a unique tiny Jeff like a note 2
by Jeff that has never been requested at
all at the server then we can assume
that all of these requests are going to
go to some storage subsystem which will
end up retrieving it and then serving
out to the client and because these are
all so tiny their bandwidth footprint is
going to be very tiny and you know no
other resource will get affected by any
large to any large extent similarly when
you when the when the coordinator says
okay all these clients go and request
the same large object at the server by
large object there could be a large
stored media file because the same
object is being requested multiple times
you can assume the caching kicks in and
once caching kicks in you know the only
resource that you're exercising is
essentially the access bandwidth of the
zone and similarly you know when we make
tiny queries by tiny queries I mean a
query whose response is tiny again you
know we are isolating the impact that we
make on the bandwidth and we are kind of
mainly restricting ourselves to the
back-end data processing and finally
when we have all these clients make head
request simultaneously we are basically
not touching any of the resources just
kind of probing the performance of the
bay
HTTP provisioning at the server so next
time so to do this or in a black box
kind of resetting what the coordinator
in our setting does is before the start
of the experiment in a pre-processing
stage the coordinator simply crawls the
target side and obtains a bunch of
objects and queries and kind of
classifies them into two categories
either small based on the size of the
object or the size of the response or
large based on some threshold okay okay
so that's kind of you know one part of
the problem which is kind of trying to
isolate different resources and study
the impact on resources but then once we
were able to do this we the next
question is how exactly do we impose a
certain amount of load on a particular
resource now one thing you may say is ok
if you know if all the clients made for
example you know n clients were to make
certain requests and imposes certain
amount of load the coordinator can just
say ok all of you guys go and request
this at the server but if they did that
depending on Layton sees between the
coordinator and these clients and the
latency is between the clients and the
server the request may arrive at very
different times of the server right and
so we no longer have any tight control
on the load we impose on the server so
this also kind of gives us a natural
indication to what the solution to
achieving tight control over the load is
we simply look at the latencies between
the coordinator and different clients
and tip just schedule the clients in the
descending order of the sum of the
Layton sees so the farthest client your
schedule first and the closest client
you schedule us in such a manner that
all client requests arrive at exactly
the same time at the server again once
you do that you are able to impose a
certain you know you know kind of you
you with high confidence you are able to
impose a certain amount of load on the
server ok now now the the thing to
remember about this is that we are
assuming that ok by having all clients
require make requests such that they
arrive at the same time you know the
server is is the load impose on the
server is basically a function of the
requests made by the class
but this to some extent also depends on
other requests that the server is
serving due to other clients that are
not part of our experiment right but but
the thing is that the synchronization we
achieve the the way we are able to
schedule the requests it's so tight that
basically all a request arrived at a
bunch at the server and the moment the
server starts to process the first
request the others are right behind it
so we can be fairly confident that the
load on the server is mostly because of
our requests okay so so once we have we
are able to impose a certain amount of
load the next question that comes to
mind is how do you increase the load or
decrease the load to observe a certain
degradation in response time and when do
you conclude that there is a particular
limitation at a resource and the key
thing that we realize here is that
because of centralized synchronization
and how we are able to schedule requests
to arrive at the server at the same time
we can also have these measurer guys
requests make requests such that their
requests arrive at the same time at the
server and then we can track the
response behavior observed by the
measurers to understand what is going on
at the server right so what the
coordinator does is you know request the
measurers to track to make requests to
the server at the time when a certain
end clients are making requests to the
server and then see observes the median
increase in response time at the
measurers meaning that fifty percent of
the measurers saw at least blah
millisecond increase in response times
and you know that is a that that is a
function of the current request volume
equals the first synchronization and
once that happens if the median increase
in in response time is above a certain
threshold T then you conclude that okay
have made some impact on a particular
resource and then you say okay perhaps
it is some kind of a resource limitation
here this needs further exploration if
there is no such if this inequality is
not satisfied meaning you haven't cross
the threshold T you increase the load by
having additional friends make the
request so that's basically it that's
our the simple many flash crowd approach
so basically the what we do is in the
pre profiling stage
for the measurement starts the
coordinator like I said crawls the
target and classifies objects into these
different kinds measures are tts between
itself and the clients and the target to
do the scheduling and then also measures
the bass response time for different
object types to use to understand when
the response time has increased and once
this happens the core mini flash code
approach itself is divided into a few
different stages where within each stage
all clients request a particular kind of
object like a unique small small object
or a same large object or same small
query and so on these stages are divided
into epochs where within each epoch we
have a certain number of clients make a
request at the server we start with five
clients and the set of clients that make
a request we call it a crowd okay and
you know be scheduled measurers to
monitor the performance of the server
when clients are making this request if
there is an increase in response time as
I mentioned earlier we we don't
immediately conclude that something went
wrong we repeat the experiment at the
same load a couple of different times
for accuracy if not we progress to the
next epoch by increase in the number of
clients that make the request so we kind
of gradually increase the load until
we're absolutely sure that we hate the
dash and when we are able to conclude
with high accuracy that something
happened the size of the crowd at at
that particular time is indicative of
the limitation of the of that resource
when we saw something happen okay
so I'm going to move on to some kind of
lab based experiments that test various
aspects of this approach and I am going
to describe to you some experiments that
we did with live sites to understand how
valuable some operators thought this
approach was so so the first test we did
was to understand how effective are you
know synchronization was in the presence
of wide area delays we set up a toy
server in the University of Wisconsin
which was basically running a bare-bones
HTTP returning a blank page our clients
were running on planet lab and they
basically sent a simple request to the
server and this graph over here
basically plots the arrival times of the
clients as observed at the server okay
so the x axis is the clients that we
scheduled we scheduled about 45 of them
and the y axis is the relative arrival
time of the clients and what we see is
about you know 43 or 40 40 of the 45
clients all arrived within 30
milliseconds of each other and these
clients are one planet lab which is kind
of distributed throughout the US and
some in Europe okay now this may not
seem very I mean this is kind of
reasonable that we are able to get
requests to arrive within such a small
interval of time but we actually did
much larger scale experiments where we
scheduled up to 350 requests at a
commercial production site and we were
able to have all of them arrive within
half a second of each other at the
server so this allows the stream force
very tight control over the load yeah
what meet the Kwan's on the plan a lot
why can't you just put some pieces box
it directly connect to the server is
this stress testing so that's one of the
things with that question please so the
question was you know why do you need
the clients to be on the wide area and
you know why exactly why didn't you just
have the clients be on some local area
network and make so the challenge with
doing that is we won't be able to test
the limitations imposed by the bandwidth
the access bandwidth to the wide area
network so even if you if you you know
you will only be able to test the
limitations imposed by the local
bandwidth let's make sense so we want to
understand how differently how different
resources are kind of dependent on each
other and what limitations they each of
them impose and in a local setting we
won't be able to delineate the effect of
the bandwidth yeah just say some point
in time which is up to like a 100-day
nanosecond awkward you start asking for
this oh no that the problem is if you
did that so the question was why can't
you have all the clients have a
synchronized clock and say okay at this
time make make a request now the delays
between the clients and the target are
all different right some of them are
very close to the target others are 100
milliseconds away so if you did that you
so the internet is you know between 50 I
understand yeah I absolute and if we did
what you are saying we wouldn't get even
the 500 millisecond spread of requests
it things could be arbitrarily delayed
enough that you know actually we did it
that was our initial approach of having
everybody request things at the same
time but we absolutely had no control
over what load we imposed on the server
so I mean although this this seems like
okay it's you know they're arriving
within half a second of each other
that's all of the 350 requests so 1 2 3
50 arrived within half a second but
ninety percent of those really arrived
within 60 200 millisecond so that's kind
of what we are looking and that's hard
to achieve without some scheduling
algorithm
are there other questions okay so so the
second experiment we did was to
understand you know you know we were
looking at the median response times of
day of the of the measured resum you
want to understand how well they are
tracking the server's behavior and so
what we did was that in a toy server
that we set up in the University we
implemented some kind of a response time
function which is basically a function
of the queue length when a particular
request arrives and we try to find
different few different curves and
basically what this graph shows is you
know this is the response time function
that we implemented the solid line and
this is as reported by the median
increase in response time of the
measures and it kind of fairly closely
tracks what the ideal measurement is to
some extent so again I mean we did a
bunch of experiences is just a snapshot
but you know it seems like this approach
of waiting for about fifty percent of
the clients to show a certain
degradation seems to be fairly effective
at traffic at tracking the service
behavior so so that's kind of obvious to
some extent but we wanted to also
questioned our basic assumptions of you
know different request types impacting
specific resources and how exactly how
accurately I be able to isolate the
different resources so again we set up a
toy server with a bunch of objects on it
and you know this was on a local land
because we now understand the
synchronization effects very well and so
in the first experiment we had a bunch
of clients run this mini flash code
experiment all requesting the same large
object and I you know as the number of
clients increase the response time
increased but what we saw was that the
network usage as reported by something
like k top also started to increase and
we saw that no other resource for
example the disk didn't show any reads
if there was no see increase in CP
utilization whatever so this kind of
confirms that the same large object
effects effects the network usage and
similarly when you know clients request
a unique small object that has never
been requested before the number of disk
reads increases so we're impacting the
disk more than anything else within them
within the server setup again this is
also kind of obvious but we wanted to
check it for ourselves
now the interesting thing happens when
we are talking about the the impact we
are able to make on the data processing
subsystem and to test this we set up a
server with my sequel database the
server has a query front end we tested
with two different query front ends fast
CGI and mongrel and this is the server
simply what it does is when a client
makes a request it retrieves a certain
number of entries from a database
computes the mean and standard deviation
of the entries and returns it back to
the back to the point so so what we saw
was kind of a very stark so all clients
were making the same query here and
there was a very stark difference in the
behavior we saw due to the two front
ends query front ends so with fast CGI
we saw that the the response time kept
creeping up and it started to show a
performance degradation but you know the
alongside the cpu utilization also
started to grow up the memory use it
also start to grow up but there was
really nothing that we could see of any
effect at all when we were using the
other front end it turns out what was
happening was it with fast CGI it was
kind of a broken implementation where
each time there was a request fast CGI
would for a new process and the child
would inherit the parents memory image
and the memory consumption kept growing
higher and higher as we made more and
more requests so so to some extent what
this tells us is that at least in this
particular case where we make small
queries it's not really clear that we
are impacting a hardware resource it may
be that sometimes we observe some of the
effects of the software front ends for
that for that resource and we cannot
really delineate the two things so again
we did another experiment where we had
clients require make different queries
to remove the effect of caching and we
use the more efficient of the two front
ends and in this case we did see that
you know as the number of requests
increase the cpu utilization increase
and there was no impact on other
resources within the system okay so in
some cases this approach is able to tell
us something about
the the hardware resources and in other
cases depending on what front end is
used we may be actually inferring
something about the software front end
but we cannot really tell the two things
apart always ok so the that's kind of a
broad overview of kind of what we can
infer and how effective these things are
but I quickly want to go over some
limitations before describing some of
the experiments we did so the first
limitation is that we don't always infer
limitations of hardware resources so
when we schedule a certain number of
clients and we see an increase in
response time that may happen because
you know you know for example in the
bandwidth case when an end client makes
a bandwidth intensive request that
request kind of eats up bandwidth from
the N minus one other request causing
all of them to show degradation response
time so in that case you can say the
load on the resource increased but in
other cases the enth client client
request may just be queued behind the N
minus one other requests and in that
case realistically what you're seeing is
the queuing delay for the further client
and when the client actually ends up
getting serviced by the resource it
takes a fixed amount of time meaning the
resource is not getting overloaded by
any extent in that case the the delay
actually depends on the queuing all the
scheduling algorithm you know how many
q's there are and all of this time so we
cannot really tell whether the delay we
are seeing is because of some scheduling
and queuing going on versus whether it
is actually a resource limitation that
we are observing play up as a an
increase in response time and the other
thing which is kind of we can get from
our from our tests on the query back-end
is that our observations are not
necessarily fine-grained enough meaning
we are not always able to pinpoint
individual resources and software they
are kind of limited to at the
granularity of a subsystem like the
storage subsystem and the query
subsystem which includes the hardware
and the software front end and perhaps
the access of system to some extent
and this is this is a limitation that's
imposed by our black box nature if the
if the operators to some extent
cooperated with us and gave us some
other information about request that
helps us separate out these constraints
then you know it may be able to get much
finer grained influences from that okay
so so what I want to describe now is a
bunch of experiments that we did in kind
of white box and black box settings in
the white box setting we recruited
actually a top 50 site as reported by
quantcast which is Webre web ranking
service was interested in this work so
they came and talked to us and said can
you run this on our side and you know a
couple of other universities were also
interested in this so so this top 50
ranked side this is in a black box a
white box kind of a setting we had
access to the logs at the server so we
could observe how our requests arrived
at the server we know what resources
they touched how much background traffic
there was and so on and we also had
operator feedback which is very valuable
for us to understand how effective and
valuable our approach was to them and we
tested both the production and non
production versions of the server these
hosts identical content but their
configurations are vastly different and
the production version of the server is
actually I think it gets about a billion
requests a day or whatever some really
high number but you know the non
production version almost saw no traffic
during the time we were doing our
experiments we we had about 75 clients
making multiple requests let me go to
the measurement observations we made
from running our experiment on the
production and non production service
starting with the non production server
so so what we see is so these numbers
here basically indicate the number of
clients that we scheduled at which the
median response time reported by the
measures saw a 250 second millisec 250
millisecond a gradation so for the non
production version when we made about 40
head requests there was a 250
milliseconds irritation in response time
the median response time for the large
object k
nothing happened we didn't see any
degradation at all and for the small
query case the the degradation happened
around 90 requests and you know the
operators were not surprised by this
they said you know we have very high
bandwidth so it's not surprising that
you saw nothing our small query the
particular query that we were using they
said that it go it went through multiple
stages and one of the servers in one of
the status was a known contention point
in their infrastructure so it kind of
makes sense that we are reflecting that
but they were very surprised by the fact
that head requests ended up showing this
kind of a of pathological behavior and
they actually think this is some kind of
a configuration issue and they were
still debugging it at the time I
prepared these lights they don't know
what is going on the production version
of the side we saw nothing we were
hoping to see something but really you
know we upped the number of requests we
made from each client 25 in parallel we
went all the way up to 375 requests and
we really didn't see any degradation in
response time and again this was vetted
by the operators feedback that look our
production server is in a large data
center where we have you know multiple
servers multiple multiprocessor servers
all working in tandem to service
requests so clearly we're not going to
see anything with this kind of setup so
again the the operator so very
interested they said okay they gave us a
few other queries and said can you try
this we want to see how this part of our
infrastructure works well in the
interest of time I'll skip those results
we ran our experiments on a couple of
other University servers both located in
the US the first University saw very
light traffic during our experiments
about three or four requests per second
it's a basic Apache v2 running behind a
1 gigabit per second link the second
university saw maybe four to five times
higher traffic during our experiments
again it's some kind of an oldish son v
240 server again behind a 1 gigabit per
second area ok so again we had access to
logs and operate a feedback so again
this was actually very distance kind of
tells us to some extent you know
university operators are not you know
then obviously they don't care about
having the configuration that works well
under a lot of different circumstances
but still there was some broken things
so in the first University we ran a
bunch of experiments this is just one
experiment that I'm showing here when
request when we made about 100 or 130
simultaneous requests all three stages
started to show a degradation in the
spawn Stein okay in particular what we
observed was that the moment the number
of requests crossed 100 the response
time was you know about 150 milliseconds
or so worse the reason it didn't show up
here is that we were using a 2 50
millisecond threshold now this is kind
of surprising that it should be so
consistent across all all the different
all the different resource types right
hundred exactly this kind of thing
should happen and after thinking about
it a little bit we figured out that this
is some kind of a software configuration
parameter issue right probably some
limitation on the maximum number of
threads at any point in time which
theory limits how many requests we can
make and kind of whatever requests you
make beyond that end up getting queued
and the server operators also kind of
confirmed as they said our our setup is
kind of five years old we haven't really
bothered to change anything since we put
things together in two thousand and this
kind of goes back to the comment I made
earlier that we cannot really say
whether the degradation we observe is
because of some resource limitations or
because of some server-side scheduling
issues okay but the fact that things are
so consistent points to the fact that
there is some software configuration
parameter that's broken in the case of
the second University bandwidth was
really good I mean we didn't see any
effect even when we went up to 150
clients for the small query stage about
30 client 30 simultaneous requests was
enough to have the server keel over and
this actually the again this brought
back some fond memories to the
University operators they said Oh legacy
is a query infrastructure we just
realized was not caching responses
and which kind of explains the the
degradation in performance and for the
head request again about hundred and ten
requests were enough to show degradation
response time and actually this was
interesting to them they said that the
difference between head and large object
was useful for them to debug a recent
anomaly that they had observed where
there was a popular video hosted on the
University side and a lot of users were
requesting it and you know when when
this was happening there was another
user was trying to make a large download
and getting frustrated not seeing the
the download finish at all and they were
wondering whether it was because of some
bandwidth limitation or because of the
server software limitation and the fact
that you know the bandwidth was fairly
good convinced them that this is
actually going to be some kind of a
request processing bottleneck at the
server so it kind of helped them
understand what was going on in that
case okay so were there any questions
until this point okay so so those were
the white box kind of a setup where we
had the feedback of operators and they
said this was interesting and they
realize things that they probably didn't
care about and thought didn't matter but
in addition we also ran a approach on a
bunch of servers in the wild realize
that I said it's a non-intrusive
lightweight approach so it gives us the
ability to run it on servers without
perturbing their function in any way in
particular we kind of picked servers in
different rank ranges in in this
quantcast meaning servers that were
ranked 1 through thousand service at the
9000 through 10,000 and we picked a few
in each of those categories and our
expectation was that the more popular
ones would be more well proficient than
the less popular ones and we wanted to
observe similarities and differences and
we ran these tests over 1.5 month period
mostly due to during off-peak ours to
kind of limit the impact of background
traffic ideally would want to run
multiple times and kind of make sure you
get a good idea and again our goal was
to understand resource provisioning
trends and kind of validate our approach
a little bit more
so this is the base stage which is the
head requests ran a run against the
different service so the x-axis here is
the different rank ranges for servers in
1 to 1000 range and so on and the y-axis
here shows the fraction of servers that
showed a degradation 100 millisecond
degradation response time when say up to
20 requests were made when up to 30
requests were made and so on and you
kind of see an expected trend as you go
to lesser and lesser popular servers you
start to see a larger fraction of
service showing a degredation in
response time which is kind of what you
would expect in particular for the the
least popular set of sites you know
about a forty five percent of the sides
could not handle any more than 60
simultaneous head requests before there
was a performance degradation but
actually to some extent what we found
surprising was that in in the bet in the
most popular category they were about
you know ten percent of sites about
which which could not handle any more
than 40 requests before there was 100
millisecond degradation responsive I
kind of surprised said that we expected
them to be hosted at large data centers
and we didn't expect this to happen so
this is for the next stage where all
clients make the same small query at
those servers and again the trend is the
same but the fractions are much higher
right so on the previous slide you know
the maximum was about like 45 in this
case for the least popular side we had
about 75% of the server's show
degradation response time you know kind
of the query processing portion of the
service doesn't seem to be as well
provision as the rest of the setup but
again the thing that we were a bit
surprised ways again there are about
twenty percent of the servers in the
most popular category which could not
handle more than 40 simultaneous queries
which I mean we do not know what is
going on because he is a black box type
of tests but we didn't really expect to
see this kind of a trend when we were
mentioning these sites
and this is the bandwidth test where
we're all clients request the same large
object thing that's a bit surprising is
you know in the first two ranked ranges
the results are fairly similar across
small query and large object kind of you
see the same number of servers showing
it degradation in response time but in
the last two categories you know there's
no increase in the total fraction it's
kind of flattens out over here probably
indicating that the most popular service
provision all their resources really
well but the less popular ones tend to
pay a lot more attention to their
bandwidth than to to their application
infrastructure itself maybe to some
extent reflects the thinking that if I
have good bandwidth that ensures
automatically good response times to
some extent just me know it's not really
true ok so again we also did some tests
on startups but I mean the basic idea
that we you know we obtain these
startups from popular technical media
and we were hoping that these would be
hosted at last data centers and we
wouldn't again see any degrading
degradation in response times which is
largely true about you know whatever
fifty-eight percent and forty-four
percent in the two cases head and small
query didn't show any degradation
response times but we did see that about
twenty-four percent and thirty-three
percent in these two classes could not
handle any more than 20 requests so
they're really not prepared for a
success disaster kind of situation but
all of a sudden they become popular and
there are a bunch of requests at this
level okay so so I want to spend a
couple of minutes talking about some
extensions of our approach this is kind
of like a V 0 of our approaching we're
still working on improving this approach
the first section extension that I want
to talk about is this notion of how this
approach can inform server operators of
the vulnerability of their service to
distribute it in isle of service attacks
so there are kind of you know multiple
types of attacks their network level
floods where there are a bunch of
requested overwhelm the bandwidth or
there are application level attacks
which open the application
infrastructure and in the at least in
the research community different
solutions have been proposed
for these two classes of attacks and the
effectiveness of these solutions kind of
depends on the volume of the attack the
higher the volume typically the more
successful the approaches a twatting
attack so our observations basically can
can kind of tell you what is the most
constraining resource which solution
really make sense to protect your server
and really is that solution going to be
effective because if your server cannot
handle any more than 20 requests and it
starts to see keel over then no solution
can really protect you and you are
better off kind of figuring something
out at your end in terms of provisioning
it better so the other kind of
application of this thing is this notion
of a staggered mini flash governor and
the key thing here is that if you go
back to our synchronization approach we
kind of enforce this very tight
synchronization where we have all
requests arrive at the same time and
this reflects kind of a pathological
flash crowd where there's a whole lot of
spurt of requests but there may be other
kind of low volume flash crowds where
you know request arrive in a more
staggered manner and the server may be
doing okay in these low volume flash
cards but it may actually keel over in
the in these more synchronized flash
crowds to some extent but the thing is
you know because of our of the
centralized scheduling what we realized
is that it's possible for us to schedule
requests to arrive at the server in any
arbitrary predefined order right so it
allows us to to test a variety of
requests profiles at the server and
understand the constraints of the server
on two different situations okay that
actually brings me to the end of my talk
so so hopefully I mean I was able to
convince you that this is a useful tool
to understand the limitations imposed by
the configuration of a server and this
is actually it's currently running as a
service on planetlab and if you'd like
us to test any of your servers you know
please get in touch with us and we
actually force you a lot of applications
to this in terms of debugging figuring
out how to design your request front
ends and so on okay that's the end of my
dog
so thank you thank you everyone is going
to get the standard warning we have
about all talks and that is this is
being recorded for the net and so no
questions please it'll reveal any
proprietary information but since I have
the microphone here and I have a number
of questions I'll take advantage of the
opportunity to ask you one first and
that is when you're doing this you've
got this this notion of this external
stimulus with your with your set of
machines that are testing but how is it
it's about cause and effect how do you
presume pre-existing load and are you
presuming that it's constant yeah so so
on so we don't know what the P existing
node is right ideally what we should
have done is to test our target server
on at different times of the day and
kind of get the best of the results
because that reflects the the least
loaded time of the server but one thing
we noticed in when we look at the logs
when we were experimenting against the
the University service was that even
though there was background traffic the
baby scheduled or requests was such that
all all of our requests arrived in a
bunch and they kind of you know okay
were able to beat out other requests and
impose a particular amount of load on
the server at the time we scheduled it
so there is some effect of background
traffic but it's not so predominant that
it severely affects the observations
that we can make all right so then
leaves me to the next question and that
is that if you had instead of
approaching this as a black box if you
were able to do this as a white box
approach what would you change about the
way that you're running these
experiments and if there are any
questions please go ahead and use the
microphone in the center that way we
won't have to repeat the question um
yeah that's actually we didn't really
give too much of a thought thought to
that I mean it's kind of if it were if
you were running things in a black box
in a white box kind of a setting then
you can think of it as some kind of a
root cause analysis type of a situation
so we observe something we don't know
what is going on we kind of figure out
some requests that help us
tell what may be going on make those
requests and kind of draw a flowchart of
oh it's not this it's not this it should
be this right we haven't really given
much thought to how we would do it in a
white box setting and what kind of
algorithmic way we would use to explore
the space of possibilities of
constraints I don't have an answer to
that but the way we were thinking of a
white box kind of setting is less
automatic than that fairview the
operator would tell us what to run and
what he expects it to be and we kind of
tell him whether that matches what he
expects if not what possibly could have
gone wrong right so that's the level to
which our understanding of the white box
approaches and it's a great question we
really have to think about how we would
do it in a much more automatic manner so
one question how is it compared to the
commercially available side testing
companies out there like used to be
mercury HP now and then some other
people in the market yeah that's a good
so so perhaps you were thinking about
things like keynote also right there's
no next time yeah so so the thing is you
can imagine keynote employing something
like this to improve the observations
that they provide as far as I know
keynote kind of says okay we will have
these these clients in different cities
make requests to the index dot HTML data
server in kind of see what response time
they see and use that as a feedback we
inform the service design but they don't
again as far as I know they don't give
any further details of really what could
have what may have gone wrong at the
server when when there was a certain
flood of requests whether it was a
limitation due to some application
infrastructure configuration or
bandwidth or whatever else that said
there is nothing unique to our approach
that prevents someone like keynote from
applying it in their setting all they
really need is this scheduler and kind
of saying okay rather than make generic
request to the base page will make the
specific requests to specific resources
so anyone else all right well please
thank me or please join me in thanking I
did you I cal</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>