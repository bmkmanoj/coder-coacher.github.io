<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The Next Generation of Neural Networks | Coder Coacher - Coaching Coders</title><meta content="The Next Generation of Neural Networks - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The Next Generation of Neural Networks</b></h2><h5 class="post__date">2007-12-04</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/AyzOUbkUf3M" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's almost fun introducing people who
need no introduction uh but for those of
you who don't know Jeff and his work he
pretty much created he helped create the
field of machine learning as it now
exists and was on the cutting edge back
when it was the bleeding edge of
statistical machine learning and neural
nets when they first made their
resurgence for the first time in our
lifetime and has been a constant force
pushing it um pushing the analysis and
the field away from just sort of the
touchy-feely let's tweak something till
it thinks and towards getting building
systems that we can understand and that
actually do useful things that make our
lives better so you if you read the talk
mounts meant you've seen all of his many
accomplishments and members of various
royal societies etc so I won't list
those I think instead of taking up more
of this time I'm just gonna hand the
microphone over to Jeff why don't you
I'll go so the main aim of neural
network research is to UM make computers
recognize patterns better by emulating
the way the brain does it we know the
brain learns to extract many layers of
features from the sensory data we don't
know how it does it so it's a sort of
joint enterprise of science and
engineering the first generation of
neural network I can give you a two
minute history of neural networks the
first generation were things like
perceptrons where you had hand coded
features they didn't adapt so you might
put an image the pixels of an image here
have some hand coded features and you'd
learn the weights to decision units and
if you wanted funding you make decision
units like that these were fundamentally
limited in what they could do as was
pointed out in 1969 and so people stop
doing them then sometime later people
figured out how to change the weights of
the feature detectors as well as the
weights of the decision units so what
you would do is take an image here you
go forwards through a fee forward neural
network you will compare the answer the
network
Eve with the correct answer you take
some measure of that discrepancy and you
send it backwards through the net and as
you go backwards through the net you
compute the derivatives for all of the
connection strengths here both those
ones and those ones and those ones of
the discrepancy between the correct
answer and what you got and you change
all these weights to get closer to the
direct answer that's back propagation
it's just the chain rule
it works for non linear units so
potentially these can learn very
powerful things and it was a huge
disappointment I can say that now
because I got something better um
basically we thought when we got this
that we can I learn anything and we'll
get lots of lesser features object
recognition speech recognition all be
easy there's some problems it it worked
for some things yeah Lacan can make it
work for more or less anything but in
the hands of other people it has these
limitations and something else came
along so there was a temporary
digression called kernel methods where
what you do is you do perceptrons in a
clever away you take each training
example and you turn a training example
into a feature basically the feature is
how similar are you to this training
example and then you have a clever
optimization algorithm that decides to
throw away some of those features and
also decides how to weight the ones it
keeps but when you're finished you've
just got these fixed features produced
according to a fixed recipe that didn't
learn and some weights on these features
to make your decision so it's just a
perceptron there's a lot of clever
matter how you optimize it but it's just
a perceptron and what happened was
people forgot all of Minsky and papas
criticisms about persepolis not be able
to do much also it worked better than
back propagation quite a few things
which was deeply embarrassing but it
says a lot more about how bad back
propagation was and about how good
support vector machines are
so if you ask what's wrong with
backpropagation it requires labeled data
and some of you here may know it's
easier to get data than labels if you
have it as a model of the brain you've
got about that many parameters and you
live for about that many seconds
actually twice as many which is
important to some of us there's not
enough information in labels to
constrain that many parameters you need
10 to the 5 bits or bytes per second
there's only one place you're going to
get that and that's the sensory input so
the brain must be building a model of
the sensory input not of these labels
the labels don't have enough information
what so the learning time didn't scale
well you couldn't learn lots of layers
the whole point of backpropagation was
to learn lots of layers and if you gave
it like ten layers to learn it would
just take forever and then there's some
neural things I won't talk about so if
you want to overcome these limitations
we want to keep the efficiency of a
gradient method for updating the
parameters but instead of trying to
learn the probability of a label given
an image where you need the labels we're
just going to try and learn the
probability of an image that is we're
going to try and build a generative
model that if you ran it will produce
stuff that looks like the sensory data
and otherwise we're going to try and
learn to do computer graphics and once
we can do that then computer vision is
just going to be inferring how the
computer graphics produce this image so
what kind of a model could the brain be
using for that the building blocks I'm
going to use are a bit like neurons they
intended to be a bit like neurons
they're these binary stochastic neurons
they get some input they give an output
that's either a 1 or a 0 so it's easier
to communicate and it's probabilistic so
this is the probability of giving a 1 as
a function of the total input you get
which is your external input plus what
you get from other neurons times the
weights on the connections and we're
going to hook those up into a little
module that I'll call a restricted
Boltzmann machine this little module
here has a layer of pixels and a layer
of feature detectors so it looks like
it's never going to learn lots and lots
of layers of feature detectors it looks
like we thrown out the baby with the
bathwater and we're now just restricted
to learning one layer of features but
we'll fix that later we're going to have
a very restricted connectivity and
name where this is going to be a
bipartite graph the visible units for
now don't connect to each other and the
hidden units don't connect to each other
the advantage of that is if I tell you
the state of the pixels these become
independent and so you can update them
independently and in parallel so given
some pixels and given the you know the
weights on the connections you can
update all these units in parallel and
so you've got your feature activations
very simply there's no lateral
interaction set these networks are
governed by an energy function and the
energy function determines the
probability of the network adopting
particular States just like in a
physical system these stochastic units
will kind of rattle around and they'll
tend to enter low energy states and
avoid high energy states the weights
determine the energy is linearly the
probabilities are an exponential
function of the energies so the
probabilities the log probabilities are
a linear function of the weights and
that makes learning easy there's a very
simple algorithm that terrace and Oscar
me invented in about 1982 in a general
Network you can run it but it's very
very slow in this restricted Boltzmann
machine it's much more efficient and I'm
just going to show you what the maximum
likelihood learning algorithm looks like
that is suppose you said take one of
your parameter on your connection how do
I change that parameter so that when I
run this machine in generative mode in
computer graphics mode it's more likely
to generate stuff like the stuff I've
observed and so here's what you should
do
you should take a data vector an image
then you should put it here on the
visible units and then you should let
the visible units via their current
weights activate the feature detectors
so you provide input to each feature
detector and you now make a stochastic
decision about whether the feature
vectors should turn on lots of positive
input it almost certainly turns on lots
of negative input it almost certainly
turns off then given the binary state of
the feature detectors we now reconstruct
the pixels from the feature detectors
and we just keep going like that and if
we run this chain
a long time this is called a Markov
chain and this process is called
alternating Gibbs sampling if we go
backwards and forwards for a long time
we'll get fantasies from the model this
is the kind of stuff the model would
like to produce these are the things
that the model shows you when it's in
its low energy states given its current
parameters so that's the sort of stuff
it believes in this is the data and
obviously you want to say to it believe
in the data not your own fantasies and
so we'd like to change the parameters
the weights on the connections so as to
make this more likely and that less
likely and the way to do that is to say
measure how often a pixel I and a
feature detector Jeon together when I'm
showing you the data vector V and then
measure how often they're on together
when the model is just fantasizing and
raise the weights by how often they're
on together when it's seen data and
lower the weights by how often they're
on together when it's fantasizing and
what that will do is it'll make it
happier with the data lower energy and
less happy with its fantasies and so
it'll grantees will gradually move
towards the data if it's fantasies are
just like the data then these
correlations the probability of pixel
line feature detector J being on
together in the fantasies will be just
the same as in the data and so it will
stop learning so it's a very simple
local learning rule that a neuron could
implement because it just involves
knowing the activity of a neuron and the
other neuron it connects to and that
would do maximum likelihood learning but
it's slow you have to settle for like a
hundred steps so I figured out how to
make this algorithm go a hundred
thousand times faster the way you do it
is instead of running for a hundred
steps you just run for one step so now
you go up you come down and you go up
again and you take this difference in
statistics and that's quite efficient to
do it took me 17 years to figure this
out and in that time computers got a
thousand times faster
so the change in the weight now is the
difference is a learning rate times the
difference between statistics measured
with the data and statistics measured
with reconstructions of the data that's
not doing maximum likelihood learning
but it works well anyway so I'm going to
show you a little example we're going to
take a little image where we're going to
have handwritten digits this is just a
toy example we're going to put random
weights on the connections then we're
going to activate the binary feature
detectors give them the input they're
getting from the pixels then we're going
to reconstruct the image and initially
we get a lousy reconstruction this will
be very different from the data because
they're random weights and then we're
going to activate the feature textures
again and we're going to increment the
connections on the data and we're going
to decrement the connections on the
reconstructions and that is now going to
learn nice weights for us as I'll show
you nice connection strengths that will
make this be a very good model of
handwritten two's it's important to run
the algorithm where you take the data
and on the data you increment connection
strings and on your this is really a
sort of screwed up version of the data
that's been infected by the prejudices
of the model so the models kind of
interprets the data in terms of its
features
then it reconstructs something it would
rather see than the data now you could
try running your learning algorithm
where you take the data you interpret it
you imagine the data is what you would
like to see and then you learn on that
that's the algorithm George Bush runs
and it doesn't work very well so after
you've been doing some learning on this
for not very long I'm nice showing you
25,000 connection strengths each of
these is one of the features take this
guy that's a feature and the the
intensity here shows you the strength of
the connection to the pixels so this
feature really wants to have these
pixels off and it really wants to have
these pixels on and it doesn't care much
right the other ones mid gray means zero
and you can see the features are fairly
local and these features and I very good
at reconstructing twos it was trained on
twos so if I show you show it some twos
it never saw before
and get it to reconstruct them you can
see it reconstructs them pretty well
the funny pixels here which aren't quite
right is because I'm using Vista um so
you can see the reconstruction is very
like the data and the it's not quite
identical but it's a very good
reconstruction for a wide variety of
twos and these are ones it didn't see
during training okay now what I'm going
to do that's not that surprising if you
just copied the pixels and copied them
back you get the same thing right so
that would work very well but now I'm
going to show it something you didn't
train on and what you have to imagine is
that Iraq is made of threes but George
Bush thinks is made of twos okay so
here's the real data and this is what
George Bush sees that's actually
inconsistent with my previous joke
because it assumes he runs this learning
algorithm sorry about that okay so you
see that it perverts the data into what
it would like to believe which is like
what it's trained on okay that was just
a toy example now what we're going to do
is train a layer of features like that
in the way I just showed you will get
these features that are good at
reconstructing the data at least for the
kind of data it's trained on and then
we're going to take the activations of
those features and we're going to make
those data and train another layer okay
and then we're going to keep doing that
and for reasons that are slightly
complicated and I will partially explain
this works extremely well you get more
and more abstract features as you go up
and once you've gone up through about
three layers
you've got very nice abstract features
that are very good then for doing things
like classification but all these
features were learned without ever
knowing the labels it can be proved that
every time we add another layer we get a
better model of the training data or to
be more precise we we improve a lower
bound on how good a model we go to the
training data
so here's a quick explanation what's
going on when we learn the weights in
this little restricted Boltzmann machine
those weights define the probability of
given a vector here we constructing a
particular vector there so that's the
probability of a visible vector given a
hidden vector they also define this
whole Markov chain if you went backwards
in four words many times so if you went
backwards and forwards many times and
then look to see what you get here
you'll get some probability distribution
over hidden vectors and the weights are
defining that and so you can think of
the weights as defining both a mapping
from these vectors of activity over the
hidden units to the pixels two images
that's this term and the same weights
define a prior over these patterns of
hidden activities when you learn the
next level Boltzmann machine up you're
going to say let's keep this keep this
mapping and let's learn a better model
of the posterior that we got here when
we use this mapping and you keep
replacing the post the implicit
posterior defined by these weights by a
better one which is the P of V given H
defined by the next Boltzmann machine
and so what you're really doing is
dividing this task into two tasks one is
find me a distribution that's a little
bit simpler than the data distribution
don't go the whole way to try and find a
full model just find me something a bit
simpler than the data distribution
there's going to be easier for a
Boltzmann machine to model that's very
nonparametric and then find me a
parametric mapping from that slightly
simpler distribution to the data
distribution so I call this creeping
parameterization what you're really
doing is it's like taking the shell off
an onion you've got this distribution
you want to model let's take off one
shell which is this and get a very
similar distribution that's a bit easier
to model and some parameters that tell
us how to turn this one to this one and
then let's go and solve the problem of
modeling this distribution so that's
what's going on when you learn these
multiple layers after you've learned say
three layers you have a model it's a bit
surprising
this is the last restricted Boltzmann
machine we learned so here we have this
sort of model that says to generate from
the model go backwards or forwards but
because we just
kept the POV given H for the previous
models this is the directed model where
you sort of go chunk chunk generate so
the right way to generate from this
combined model when you've learned three
layers of features is to take the top
two layers and go backwards and forwards
for a long time it's fortunate you don't
actually need to generate from it I'm
just telling you how you would if you
did we want this for perception so
really you just need to do perceptual
imprints which is chunk chunk chunk is
very fast but to generate you'd have to
go back enforce for a long time and then
once you've decided on a pattern here
you go just go chunk chunk that's very
directed and easy so I'm now going to
learn a particular model of some
handwritten digits but all the digit
classes now so we're going to put
slightly bigger images of handwritten
digits from a very standard data set
where we know how well other methods do
in fact it's a data set on which support
vector machines beat backpropagation
which was bad news for back propagation
but we're going to reverse that in a
minute we're going to learn 500 features
now instead of 50 once we've learned
those we're going to take the data map
it through these weights which adjust
these weights in the opposite direction
and get some feature vectors we're going
to treat those as data and learn this
guy then we're going to take these
feature vectors we're going to tack on
10 labeled units so now we needed the
labels but I'll get rid of that later
and so we get a 510 dimensional vector
here and we're going to learn a joint
density model of the labels and the
features we're not trying to get from
the features to the labels we're trying
to say why do these two things go
together so we're learning a joint model
of both not a discriminative model when
we've completed this learning what we're
going to end up with is the top level
here is a Boltzmann machine and so it
has an energy function and you can think
of that as a landscape when the weights
are all small here or close to zero then
the energy landscape is very flat all
the different configurations here are
more or less equally good as it learns
is going to carve ravines in this energy
landscape if you think of it as a 510
dimensional energy landscape these
ravines are going to have the property
that in the floor of the ravine there's
about 10 degrees of freedom and those
the ways in which your digit can vary
and still be a good instance of that
digit like a two with a bigger loop or a
longer tail up the sides of the ravine
there's like 490 directions and those
are the ways in which if you varied the
image it wouldn't be such a good tour
anymore but the nice thing is it's going
to learn long narrow ravines so that one
too can be very different from another -
and yet connected by this ravine the
ravines captured the manifold so it
could wander from one to another in a
way that it won't wander from a 2 to a 3
even though the 3 might be more similar
in pixels for the two okay I want to
show you this generative model actually
generating before I do that I want to
own up we did a little bit of fine
tuning which actually took longer than
the original learning where you after
you've done that greedy layer-by-layer
learning you do a bit of fine tuning
where you put in images you do a forward
pass bottom-up with binary States and
when you do this forward pass you adjust
the connection slightly so that what you
get in one layer will be better at
reconstructing what caused it in the
layer below then you do a few iterations
at the top level Boltzmann machine you
go back as it falls a few times to get
the learning signal there and then you
do a down pass and during the down pass
you adjust the connections going upwards
so they're better at reconstructing what
caused the activity in that layer so
during the dam pass you know what caused
activity because you caused it and you
try and recover those causes that
fine-tuning helps but it'll work without
it so now I'm going to attempt to show
you a movie
that's not very nice
okay there's that Network here's we're
going to put images here's 500 features
500 features two thousand features and
the ten labels first of all we'll get it
to do some perception so I'm going to
give it an image and tell it to run
forwards oops sorry I didn't mean that
and that that and you'll see these are
stochastic they keep changing but it's
very sure that it's a for C those are
the identities of these neurons it knows
that's a four and it has no doubt about
it even though it's feature detectors
are fluctuating a bit if I give it a
five hopeful it'll think it's a five
yeah doesn't have any doubt so now let's
be mean to it because that's a lot more
fun I'm going to give it that so it said
so four six eight four hey hey hey hey
eight four he can't make up his mind
brother's a four and age and that's
pretty reasonable in those circumstances
it will actually for that one say eight
a bit more often than anything else so
we plastered is getting that right but
it's very unsure whether it's an eight
or four and just occasionally things
that can be other things like a two but
it basically thinks for ID I can make it
run faster so you can it's basically
four eight and occasional six I could
give it something like this and it
thinks basically one or seven and
occasionally or four because I
programmed this myself I want to point
out that it's very reasonable for this
this is my baby and it's very reasonable
for each a thing that might before cuz
look you can see the four in there okay
okay now that was just doing perception
but the very same model does generation
so what I can do is I can fix a
top-level unit and all I've done is I've
fixed the state of one neuron there's a
million connections there because that's
two thousand five hundred I just fixed
this one neuron but when I fixed that
state then the weights the two thousand
weights coming out are there two these
neurons here what they'll do is they'll
lower the energy of the ravine for twos
and they'll raise the energy of the
ravine for all the other guys so now
we've got this landscape in which you've
got all these ravines but the two ravine
has been lowered and if we put it around
and point it will eventually stumble
into the two ravine and then it'll stay
that I'm wander around so let's see if
we can do that so what's really going on
here is I'm just going backwards and
forwards up here ignore that for now I'm
going backwards and forwards here and
letting it gradually settle until it's
into a state that this network is happy
with so that's his brain stage and that
doesn't mean much to you if you look at
that you don't really know what it means
so what we're going to do is as it's
settling we're going to play out the
generative model here we're going to do
computer graphics to see what that would
have generated and so what you got here
is that's what's going on in its brain
and this is what's going on in its mind
so you can see what this is thinking and
I'm serious about that that is I know it
sounds crazy when I say to you I'm
seeing a pink elephant what I mean is
I've got a brain state such that if
there were a pink elephant out there
this would be perception that's how
mental states work they're funny because
they're hypothetical not because I made
a spooky stuff so I use this language
where the terms refer to things in the
world because I'm saying what would have
to be in the world for this brain state
to be perception now if I got a
generative model I can take the take the
brain state and say well what would have
to be in the world for that to be
perception well that so that's what it's
thinking that's his mental state right
there so you've got brain States and
mental states and most psychologists
won't show you both
let's go a bit faster and it still
hasn't settled into the true ravine and
now it's about in the two ravine and now
it's just wandering around in that two
ravine and this is what is thinking it
knows about all sorts of different twos
and it's very good that it does because
that means it can recognize weird twos
let's give it another one it hasn't got
into the eight ravine properly yet it'll
jump out into other ravines who's not
really there but by back now it will be
in the eight ravine and it'll show you
all the sorts of different eights it
believes in if you run it long enough if
you ran it for an hour now it would
probably just stay in the eighth ravine
showing all sorts of different aids okay
let's do one more because I like it so
much
again it's not really in the five room
properly yet but there was a six by
about now it's in the five ravine and
it'll show you all sorts of weird Phi's
ones with out top some occasional sixes
and then it ends up with a pretty weird
one but that's definitely a five and
it's very good that it knows that that's
definitely a five because it allows you
to recognize things like that okay
that's it for the demo
how do I get relaxed
okay so here's some examples of things
it can recognize these are all ones that
got right and you can see it recognizes
a wide variety of twos it recognizes
that this is a one despite that and it
recognizes this is a seven because of
that if you try writing a program by
hand that'll do that you'll find it's
kind of tricky if you'd never thought of
these examples in advance if you compare
it with support vector machines now what
we're doing here is we're taking a pure
machine learning task we're not giving
you any prior knowledge about pixels
being next to other pixels we're not
giving it extra transformations of the
data so this is without it's a pure
machine learning test without any extra
help if you give extra help you can make
all the methods a lot better but a
support vector machine done by DeCoste
and she'll cope from very good got one
point four percent the best you can do
with standard back propagation is about
one point six percent this gets one
point two five percent and significance
here is about a difference of point one
so this is significantly better than
that K nearest neighbor gets three point
three percent now I fine-tune that to be
good at generations so I could show you
it generating using this sort of up/down
algorithm but we can also use back
propagation for fine tuning and now that
I've got this way of finding features
from the sensory data I can say things
like nobody in their right mind would
ever suggest that you would use a local
search technique like back propagation
to search some huge long linear space by
starting with small random weights it'll
get stuck in local optima and that is
indeed true what we're going to do is
we're going to search this huge
nonlinear space of possible features by
finding features in the sensory data and
then finding features in the
combinations of features we found in the
sensory data and keep doing that and
we'll design our features like that so
we didn't need labels we just needed
sensory data once we designed all our
features we can then use back
propagation to slightly fine-tune them
to make the category boundaries be in
the right place so a pure version of
that would be to say let's learn the
same net but without any labels okay so
we do all the pre-training like this
after we pre trained now what we're
going to do is we're going
attach ten label units to the top and
we're going to use back propagation to
fine-tune these and the fine tuning is
hardly going to change the weights at
all but it's going to make the
discrimination performance a lot better
so this is going to be discourage of
fine tuning and that gets one point one
five percent errors and all the code for
doing the pre-training and the fine
tuning is on my webpage if you want to
try it now given that we now know how to
get features from data we can now train
things we never used to be able to train
with back propagation if you take a net
like this where we're going to put in a
digit and we're going to try and get out
the same digit but we're going to put
like eight layers of nonlinearities in
between if you start with small random
weights and you back propagate you get
small small times small and by the time
you get back here you get small to the
power of eight and you don't get any
gradient if you put in big random
weights you'll get a gradient but you'll
have decided in advance where you're
going to be in the search space what
we're going to do is learn this
Boltzmann machine here after we've
learned that we're going to map the data
to get activity patterns then learn this
Boltzmann machine then we're going to
learn this Boltzmann machine then we're
going to learn this Boltzmann machine
but with linear hidden units and then
what we're going to do is put the
transposed weights here because this is
good at reconstructing that so this
should be good and so on and we're going
to use that as a starting point and then
we do back propagation from there and
it'll slightly change all of these
weights and it'll make this work really
well and so now what it's done is it's
communicated this 28 by 28 image via
this bottleneck of 30 units but using a
highly nonlinear transformation to
compress it if you make everything
linear here you leave out all these
layers and make everything linear this
is PCA principal components which is a
standard way to compress things if you
put in all these nonlinear layers it's
much better than PCA so this is all done
without labels now you just give it the
digits you don't tell it which is which
these are examples of the real digits
just one example of each class these are
the reconstructions from those thirty
activities in the hidden layer and you
can see that actually better than the
date
this is a dangerous line of thought um
PCA does this and you can see it's kind
of hopeless compared with this map at
least that's what you meant to see now
we could apply this to document vectors
I don't find documents as interesting as
digits but I know some people are
interested in them you could take a
document vector and you could take the
cants of the 2,000 most common words and
there's a big database like this of
800,000 documents and so we took 400,000
sorry yeah I know I see people smiling
big 800,000 I'm an academic okay um we
then train up and you're all net like
this where these are now pass on units
for those of you know machine learning
we can use any units in the exponential
family where the log probability is
linear in the parameters so we train up
this to get some features we train up
this to get some features and then we
train up this to you get just two linear
features that seems a little excessive
and obviously when we reconstruct we're
not going to get quite the right cans
but you'll get a you'll get canceler
much closer to the right cans than the
base rates so if down here you have a
high count for iraq and cheney and
torture up here you'll get high counts
for similar things so we can turn a
document into a point in a two
dimensional space and of course once we
got a point in two dimensional space we
can plot it in 2d and for this database
someone had gone through by hand more
less by hand and labeled all the
documents we didn't use the labels okay
but now when we plot the point in 2d we
can color the point by the class of the
document so if you do the standard
technique which is latent semantic
analysis which is just a version of PCA
and you lay out these documents in 2d
that's what you get and you can see the
green ones are in a slightly different
place from these blue ones but it's a
bit of a mess if you use our method it
does a little bit better you get that
and so now if you look at these
documents your business documents right
if you look at these documents here you
can see there's lots of different kinds
of documents about accounts and earnings
presumably there's an Enron clustering
here somewhere
and it would be very nice to know which
other companies are in this Enron class
okay but there's something more
interesting you can do that's just for
visualization but now I'm going to show
you how to solve the following problem
suppose I give you a document so this
isn't like what I call Google search
where you use a few keywords and you
find what you want this is I give you a
document and I ask you to find similar
documents to the one I gave you
okay documents with similar semantic
content so I'm using a document as a
query what we're going to do is we're
going to take our big database of
documents a whole million of them and
we're going to train up this network and
it's going to convert these documents
into 30 numbers I'm going to use
logistic units here that is numbers that
range between 1 and 0 and we're going to
Train it as Boltzmann machines then
we're going to back propagate and we'll
get intermediate values here that can be
lots of information and then we're going
to start adding noise here and we're
going to add lots and lots of noise now
if I had lots and lots of noise too
something that has an output between 0 &amp;amp;
1 there's only one way it can transmit a
lot of information it's got to make the
total input that comes from below be
either very big and positive in which
case it'll give a 1 well very big and
negative in which case it'll give a 0
and in both those cases it will resist
the noise if he uses any intermediate
value the outcome will be determined by
the noise so it won't transmit
information so it won't be very good at
getting the right answers so the noise
is something like Gaussian it's not it
scary its Gaussian noise and we
gradually increase the standard
deviation and it's noise in the input to
the unit and we gradually increase this
and we use a funny kind of noise that I
didn't want to get into that makes it
easy to use conjugate gradient descent
and what will happen is these will turn
into binary units so we now have a way
of converting the word-count vector a
document into a 30 bit binary vector
and now we can do what I call
supermarket search so suppose you want
to find things that are like a can of
sardines what you do is you go to your
local supermarket and you say to the
cashier where do you keep the sardines
and you go to where the sardines are and
then you just look around and there's
all the things similar to sardines
because the supermarket arrange things
sensibly now it doesn't quite work
because you don't find the anchovies as
I discovered when I came to North
America I couldn't find the entries they
weren't anywhere near the sardines and
the tuna that's because they're near the
pizza toppings but that's just because
it's a three-dimensional supermarket if
it was a thirty dimensional supermarket
they could be close to the pizza
toppings and close to the sardines so
what we're going to do is we're going to
take a document and using our learned
Network we're going to hash it to this
thirty bit code but this is a hash code
that was learned it's not some random
little thing it was learned with lots of
machine learning so it has the property
that similar documents mapped to similar
codes so now we can use hashing for
doing approximate matches
everybody knows hashing is nice and fast
and everybody is it can't do approximate
matches but with machine learning you
can have both so we take our document we
hash it to a code and in this memory
space at each point in the memory space
we put a pointer to the document that
has that code and you're engineer so if
two documents have the same code you can
figure out what to do so now with the
query document we just go there and now
we just look around like in the
supermarket and the nearby similar
documents will have nearby codes and so
all you need to do to find a similar
document is flip a bit and do a memory
access okay that's two machine
instructions so if you were to have a
database to say ten billion documents
and I give you one and say give me a
hundred thousand documents similar to
this one for my other search technique
I'm going to use it can only cope with
100 thousand you're going to have to do
a hundred thousand times you're gonna
have to flip a bit and do a memory
access so that's only two hundred
thousand machine instructions only two
machine instructions per document is
completely
dependent of the size of your database
okay because you've laid things out like
in a supermarket you've got a document
supermarket now in 30d so if you compare
it with well we've actually only tried
it because we're academics on 20 bit
codes and a million documents and it
works just fine but nothing could
possibly go wrong when you scale it up
um it's actually quite accurate well
that is if you compare it with a sort of
gold standard method it's about the same
accuracy and when you now take your
shortlist that you found in this very
fast way and you give those guys in the
shortlist to the gold standard method it
works better than the gold standard
method alone it's much better than
locality-sensitive hashing both in terms
of speed we use the code that's on the
web for that and it's about 50 times
faster and in terms of accuracy
locality-sensitive hashing will always
be less good than this because it's just
a hack for doing this and the Kelty
sensitive hashing works on the count
vector if you work on the count vector
you will never understand the similarity
between the document that says gonzales
quits and the document that says both of
its resins they're very similar but not
in the word count vector but if you've
compressed it down to some semantic
features they're very similar documents
so the summary is that I showed you how
to use this simple little Boltzmann
machine with the bipartite connections
to learn a layer of features then I
showed you that if you take those
features you can learn more features and
as you go up this hierarchy you get more
and more complicated features they're
going to be better and better for doing
classification this produces good
generative models that are good at
reconstructing data and producing data
like the data you saw if you fine tune
with this sort of up/down algorithm
which has this funny name if you want
good discriminative models what you do
is then you fine-tune with
backpropagation but the good news is you
don't need labels for all your training
data you can learn all these features on
a very big data set and then with just a
few million labels or even a few hundred
labels
you come back propagate to fine tune it
for discrimination and that'll work much
better than for example using any
machine learning method that just uses
the labeled data it's a huge way you can
use the unlabeled data very effectively
and I shown you that it can also be used
for explicit dimensionality reduction
where you go down to a bottleneck and
that you can do search for similar
things very fast and of course we'd like
to apply it to images but for images you
have a problem which is in documents
a word is very indicative of what the
documents about in an image
what's indicative of what the image is
about is a recognized object and so what
we're trying to do now is make it
recognize objects so that were then we
can get the objects in the image and
then apply this semantic hashing
technique but we haven't done that yet I
see I've managed to talk very fast so I
can show you a little bit about how
we're going to do the image recognition
suppose you want to do generative model
which would allow you a graphics model
to take a type of an object and produce
an image of that object so I say Square
and I say what it's poses it's
positional orientation then we might
have a top-down model that from this and
this predicts where the parts might be
and if it's a kind of sloppy model it'll
say this edge ought to be round about
there and they said you ought to be
round about there and if we pick
randomly from these distributions we'll
get a square where the edges don't meet
up now one way we could solve that is to
generate very accurately here we could
say I'm going to generate each piece
just right but that requires high
bandwidth and lots of work we're going
to generate sloppily we're going to
generate a redundant set of pieces and
then we're going to know how the pieces
fit together we're going to know a
corner must be collinear with an edge
and the edges here must be collinear
with corners and now bilateral
interactions here using something called
a marker from the field we can get it to
set align to that and so now our
generative process is at each level the
level above says where the major pieces
should be roughly and a level that knows
about how these pieces go together like
how eyes and noses and mouths go
together says okay the nose should be
exactly above the middle of the mouth
and the eye should be in exactly the
same height the level above doesn't need
to spell
about that that's known locally so how
are we going to learn that well we're
going to introduce lateral interactions
between the visible units that's fine
the real crucial thing in these nets is
you don't have lateral interactions
between the hidden units so we can learn
that and the way we learn that is we put
an image in here we activate the
features then with the features fixed
providing constant top-down input we run
these lateral interactions to let this
network settle down and we replace the
binary variables by real value variables
so we're doing something called mean
field we let it settle down to something
is happier with a reconstruction it
doesn't need to get all the way to
equilibrium it just needs to get a bit
better than this and then we apply our
normal learning algorithm to these
correlations in these correlations like
this but we can also learn the lateral
interactions by saying take the
correlations in the data minus the
correlations in the reconstructions and
that'll learn all these lateral
interactions so now what we're going to
do is we're going to learn a network
with 400 input units for a 20 by 20
patch of an image this is just
preliminary work when we learn the first
network these aren't connected then when
we use these feature activities to learn
the second level Boltzmann machine we
connect these together and we learn
these and these then when we learn the
top Boltzmann machine we connect these
together and we learn these weights and
these weights when we're finished we can
generate from the model and so as a
control what we're going to do is we're
going to learn this model on patches of
natural images which have notoriously
hard things to model because anything
could happen in a patch in natural image
so it's a very hard thing to build a
density model of we're going to learn it
without lateral connections and we get
my model that's very like many other
models when you generate from it what
you get is clouds
so here's natural image patches and they
have the property that there's not much
going on and then there's a sudden
outbreak of structure like here so if
you apply a linear filter to these
things the linear filter will usually
produce a zero and occasionally Bri
is a huge approach if you apply linear
filter to these things it'll produce
some kind of Gaussian distribution these
have exactly the same Fourier spectrum
as these what they don't have is this
sort of heavy-tailed distribution where
there's not much happening and then a
lot happening and long-range structure
so now what happens if we put in the
lateral interactions and do the learning
again if we put the lateral interactions
in they can say things like if you have
a piece of edge here and you'd like a
piece of edge somewhere around here put
it here where it lines up so that'll
make much longer range interactions and
so now when we generate from the model
with lateral interactions we get that
and you can see that these are much more
like real image patches they pass many
of the statistical tests for being real
image patches they've got this kind of
much longer range structure they they've
got sort of collinear things and things
at right angles and all sorts of nice
structure in them which we didn't have
before and so we're getting this is
probably the best modeler is of natural
image patches if you ask anybody else
who models them show me samples from
your generative model they say oh well
we tried them they look terrible so we
now publish those this is I think the
first model that generates nice samples
from the model you has a models may be
comparable what we'd like to do now is
make more layers and we'd also like to
have attention that as you go up you
focus on parts of the image and what I
want to do is get something you give it
an image you go up it's focusing on
parts and it gives you a figure at the
top it gives you what you see which is
you look at an image you see a face and
then you look again you see the eye then
you look again you see a group of four
people and those are the things that
come out and those are going to be like
the words that need to go into an image
retrieval system you're going to have
this is going to run for a long time
learning and then it's going to run for
quite a long time on each image but
that's all offline okay I'm done
look like we've got time for questions
if you have questions can you if you
have questions can you please hit the
mic in the middle so that the folks out
of their offices can hear okay
hi um so you were saying that this
method doesn't require labels I was just
wondering if it would actually help if
you have labels for at least some of
your training data oh yes um labels help
the main thing is to show that you can
do a lot without them and therefore you
can get much more leverage from a few
labels okay so for example in the
semantic hashing idea you could as
you're learning those thirty dimensional
codes you could say if two things are
from the same class and the codes are
far apart introduce small force pulling
them together and we've got a paper on
that in aerostats last year and that
will improve the sort of clustering of
things in the same class but the point
is you can do without knowing the
classes as well hi so people have built
autoencoders for for a long time before
and they use regular sigmoid units and
use backprop to train them but they
never worked very well correct it would
if we actually had multiple layers of
these over these sigmoid units and then
use and train them in the same fashion
as you're doing one layer at a time
would it work as well as rbms or or not
okay that's a very good question so it's
a bit confusing this deep thing with
multiple layers trained with rbms I
called a multi layer or two encoder but
you can also have a very small auto
encoder with one hidden layer that's
nonlinear and trained that up and the
RBM is just like that so you could train
these little auto-encoders and stack
them together and then train the whole
thing with back prop that's what the
question was and that will work much
better than the old way of training
autoencoders but not quite as well as
this so yoshua bengio has a paper where
he compared doing auto encourages with
doing restricted Boltzmann machines and
restrictive Boltzmann machines work
better especially for things like
cluttered backgrounds
I've got a question which I can't ask
because I'm holding a microphone so this
morning we were talking about the
application about news which were the
problem with news is that everything
changes from day to day do you have any
intuition this is one of those unfair
what do you think would happen do you
have any intuition on how hard it would
be to adapt a deep network like this
once your input distribution changes or
as it continues to change okay so one
good thing about this learning is
everything scales linearly with the
amount of training data
there's no quadratic optimization
anywhere that's going to screw you for
big databases the other thing is because
it's basically stochastic online
learning if your distribution changes
slightly you can track that very easily
you don't have to start again so if it's
the case that the news tomorrow has
quite a lot in common with the news over
the last few months and few years and
you just need to change your model a bit
rather than start again then this is
very good for going to be good for
tracking and it's not going to be as
much work as learning at all in the
first place and in fact once you've got
all these laser features basically
changing the interactions between
high-level features will get you lots of
mileage without much work so ever
another question about the so about the
supermarket search you were saying you
just flip a bit in your hash code so
what I'm wondering is you know one thing
that I'm not sure about is like if you
flip one of these bits you might not
necessarily get something there I mean I
don't know that you're going to find
something there then also maybe is is
there some way of finding better bits to
flip and like how do you decide which
ones so of course if you make the number
of addresses be about the same as the
number of documents the average density
is one right okay and you if there's
nothing there you flip some more bits
sure so yes you'll get some misses but
that's just a sort of constant we can
look at actually how how evenly spread
over addresses it is and typically most
of the addresses won't be used and a
typical address will be used like three
or four times so it's not as uniform as
we'd like but that could all be improved
and we've only done this once we just
trained this network once on one data
set and that's all the research we've
done so far really um if we get a tiny
bit of money from someone we could make
this whole thing work much better so one
thing that is special about digits is
that they evolved in a way to make them
discriminative yes so you would hope you
it's not that surprising that in an
unsupervised way you can attract
features that are discriminative I was
wondering what happens with completely
other applications where so clearly when
you do an supervise you might throw away
some very indicative features yeah so
basically there's two kinds of learning
there's discriminative learning where
you take your input and your whole aim
in life is to predict the label and then
there's generative learning where you
take your input and your whole aim in
life is to understand what's going on in
this input you want to build a model
that explains why you got these inputs
and not other inputs now if you do that
generative approach you need a big
computer and you're going to explain all
sorts of stuff that's completely
irrelevant to the task you're interested
in so you're going to waste lots of
computation on the other hand you're not
going to need as much training data
because each image is going to contain
lots of stuff and you can start building
your features without yet using any
information in the labels so if you've
got a very small computer what you
should do is discriminative learning so
you don't waste any effort if you've got
a big computer do generative learning
you'll waste lots of the cycles but
you'll make better use of the limited
major label data that's my claim
I just ever questioned what happened to
regularization what kind of
regularization is implicit in all of
your stages
okay so we're using a little bit of
weight decay and the way we set the
weight decay was just we fiddle about
for a bit to see what worked on the on a
validation set the usual method and if
you don't use any weight decay it works
if you use weight decay works a bit
better and it's not crucial how much you
use so we are using some weight decay
here but it's not a big deal and like I
say all of the code is in MATLAB on my
web page there's a pointer on my road
so you can go and look at all those
things in all the little fudgy three
years right but but the Boltzmann
machine is fundamentally sort of
entropic regularization and then your
your little pieces of tuning with
weighty care from the other family so
you blending the pose some know the
Boltzmann machine it's true there's a
lot of regularization comes on from the
fact that the hidden units are binary
stochastic so they can't transmit much
information that does lots of
regularization for you compared with a
normal auto encoder but in addition we
say don't make the weights too big and
one reason that is not just
regularization it's it makes the Markov
chain mix faster if you don't make the
weights too big
Thanks okay so in your example of digits
you actually tell them till the
algorithm that they are ten classes yes
so I wonder how will points that impact
we do not give this a number correct hey
um so yeah okay so what you can do is
you can take this auto encoder that goes
down to thirty real numbers and not tell
it how many classes right just give it
the images get these thirty real numbers
then you can take this 30 real numbers
and apply a dimensionality reduction
technique that samurais and I have
developed and the latest version of that
you can lay them out in 2d and you will
get eleven classes and it did that
without ever knowing any labels you'll
get just these eleven clusters which is
close to ten it often thinks that the
Continental sevens are a separate
cluster so you're saying this is how you
have try and as hot happen oh I might
even have it in this talk somewhere I
might not though it's on mine is oh
there you go
that's pure unsupervised on the digits
now in this case these are twos and
these are twos in 30d is got the
clusters when you force it down to 2d it
wants to keep the twos next to each
other but it will also wants these these
are the spiky twos and these are the
sevens and it wants those close and
these are the loopy twos and these
the threes and in once those close but
it also wants the threes close to the
eights and so in 2d there just isn't
enough space to make ten clusters but
look it made eleven there and if I don't
cheat and do this in black and white you
can still see there's sort of roughly
eleven clusters so this was pure
unsupervised it found that structure in
the data so when psychologists tell you
you impose categories on this data they
aren't really there in the world it's
rubbish I mean they really they're so
the magic number 30 is it if I choose
enter a number you will be fine also if
you choose a smaller number you might
not preserve enough information to be
able to keep the classes and if you
choose a bigger number then PCA will do
better so your comparison with PCA won't
be as good how does the performance of
the digit justification vary according
to the number of layers you are using
okay obviously using the number of
layers I showed you is one of the best
numbers to use um if you use less layers
it works a bit worse if you use more
layers it works about the same I've now
got a I've got a very good Dutch student
who has the property he doesn't believe
a word I say
and we will know he's using like 40
cluster machines and he's going to get
the answer to this but so far I'm right
that using less less isn't as good and
he hasn't got two more layers yet he's
actually made with the same number of
layers you can make it work better and
we'll see if he makes you work better
with more less so just it gets related
questions so it's clear how to evaluate
this model say if you have some labelled
data and it you can try to see if you
predict similarly but if you trade
generative this Boltzmann machines with
like especially pairwise interactions in
the same levels and so on if I gave you
another set can you say how good
generatively it is and is it easy okay
how do you evaluate that kind of part so
the problem with these bolts machines is
is a partition function and what you'd
love to do is take your data set hold
out some examples
train your generative model on the
training set and then say what is the
log probability of these held out
exotics and that will be the sort of
gold standard and that's very hard to do
you know the log probability up to a
constant but you don't know the constant
so people in my group and I working very
hard at a method for interpolating
between Boltzmann machines that allows
you to use a Boltzmann machine with zero
weights which is a pretty dumb model and
then gradually change the weights
towards the Boltzmann machine that you
eventually learned and you can get the
ratio of the partition functions of all
these Boltzmann machines
so in the end you can get the partition
function you can get a pretty good
estimate this is called it's a version
of annealed important sampling called
bridging and we think we're going to be
able to get pretty accurate estimates of
the position function now by running for
like you know a hundred hours you do
this after you've learned just to show
how good you are but the other thing you
can do is you can generate from the
model and you can see that the stuff it
generates looks good and you can then
take the stuff you generated from the
model and you can apply statistical
tests to that and statistical test to
the real data and statistical test to
the other guys data the other guys
generated data if you choose the right
statistical test you can make the other
guys data look terrible okay um I think
we're out of time now I'd like to thank
Jeff again and</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>