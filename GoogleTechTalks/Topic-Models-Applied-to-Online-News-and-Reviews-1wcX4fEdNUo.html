<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Topic Models Applied to Online News and Reviews | Coder Coacher - Coaching Coders</title><meta content="Topic Models Applied to Online News and Reviews - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Topic Models Applied to Online News and Reviews</b></h2><h5 class="post__date">2010-08-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/1wcX4fEdNUo" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">okay hi my name is Jimmy Lynn I work
here at Google and I'm happy to
introduce professor Alice oh she's an
assistant professor of computer science
at the Korea advanced Institute of
Science and Technology professor OHS
group does research in natural language
processing machine learning and
human-computer interaction and today
she'll be talking about the work that
her group is doing on topic models
related to online news and reviews okay
I'm very glad to be here and thank you
Jimmy for hosting this talk so today I'm
going to be talking about topic models
and how we apply that to online reviews
and online news and in doing so I'm
going to be talking about topic models a
little bit just to make sure that
everybody knows what topic models are so
it'll be a brief introduction to it and
then I'll go into the details of our
work these are sort of to me talks the
topics art but the second and the third
items are related but they're not quite
the same and these are from two papers
that my students and I recently
submitted to wisdom next year so keep my
fingers crossed okay so let's just dive
into the main problem that we're going
to be talking about and recently Google
Books has announced this right I'm sure
many of you read about this somewhere
that Google Books has counted at least
130 million books out there ever written
and I'm sure they're more so what we can
see and I'm probably preaching to the
choir here is that there is a lot of
text data out there to be to be
understood right so if you have a
hundred and thirty million books we can
ask this question
it's a very simple question when you
just look at it what are the books about
but it's a very challenging problem
right so if you know anything about text
processing you'll agree with me that if
we have a hundred and thirty million
books and we're trying to figure out
what actually is in those books that's a
very difficult question topic models is
one answer one approach to getting at
that answer okay
so the plate diagram up there is what
you normally see when we talk you when
we talk about topic models so I just put
it up there we'll get back to the plate
diagram in a little bit but topic models
the main purpose of them is to
understand sorry to understand and
uncover the underlying semantic
structure of text of your corpus okay
so let's look at an example of what a
topic model could do for you right so
this is an article from the New York
Times a couple days ago a few days ago
and the title is economic slowdown
catches up with NASCAR and as you can
see from the headlines it's talking
about the NASCAR car racing and it's
also talking about the economic
recession and what topic models do for
you is it kind of discovers it uncovers
what the latent topics are in an article
or in a corpus okay so you can see in
these three colors here green orange and
little I guess purplish pink color the
three topics then with three of the main
topics you can see from the article so
the green one is about NASCAR races and
you can see sort of throughout the
document I've highlighted the words that
are about that topic
okay so NASCAR races track Raceway cars
etc and then the same thing with orange
is about economic recession so you would
see or like sales
costs so on and the purple is the
general sports so the enjoy shine
hopping models and the oven the a
topping model would have every art every
document is made up of multiple topics
and the Tod words in the document are
generated from those multiple topics
okay so LD a the latent dirichlet
allocation is one of the simplest topic
models and it's very widely used so and
it's a generative model which means that
it tries to mimic what the writing
process is right so it tries to generate
a document given the topics okay so
let's see how that works so bear with me
if you are experts on LD a I'm sure I'm
sure some of you are so here again we
start with the three topics the NASCAR
races economic recession and in the
general sports topic and when you have
those topics and and notice the topics
are made up of words and I'm just
showing you a subset of the words that
have high probabilities in that topic
but actually the topics are multinomial
is over the entire vocabulary so the
NASCAR race topic it has it gives high
probabilities to those words but there
are other words in that topic and they
have small probabilities okay so when
you have these multinomial is over top
over words when you want to say
generates three documents from these
three topics what you would do is kind
of produce or kind of guess these topic
distributions of the documents that
you're trying to generate so for example
the the middle the one in the middle
the writer is thinking you know I'm
going to write mostly about the general
sports topic and then I'll talk a little
bit about maybe some of the other topics
okay that's what the big purple bar
means okay so when you have those topic
distributions distributions then what
you can do is Alps going backwards okay
so from
from say that the bottom topic
distribution you can generate the words
according to that distribution okay so
since we have a lot of the green you
would see many green words popping up
there okay and then the same thing you
would do for the other documents so
that's the generative process of an Lda
okay so let's look at it from the play
diagram perspective so here up on the
left is the general is the widely used
play diagram for this and what you can
see here is the the thighs okay
next to the beta there are the topics
and those which are the multinomial is
over the vocabulary and then and then up
there up to the right corner over there
are the Thetas which are the topic
distributions okay and then from those
topic distributions if you want to
generate one document you would generate
sort of the the set of rectangles I have
over there which are the words or which
are the topics of the words that you're
going to write in your document okay so
so in this example I'm looking at the
first topic distribution and I've sort
of picked out the topics that I want to
write about and then if when you have
those topics then you can look up the
multinomial topics over here to generate
to actually come up with the words
according so for example the first word
you're going to write is an orange so
you're going to come here and look at
the orange topic and say okay these
words have high probabilities in this
topic so I'm going to pick one one of
those words okay so so that's what you
get in the actual the the rectangle down
below where you actually have the words
in your document okay and that's how the
document is generated okay but in
reality what what you have is that you
only observe the words so these are the
documents in your corpus okay so we're
this is one document in your corpus and
all others like the
topic distributions the topics
themselves their latent we don't we
don't know them but the the purpose of
fitting the model then is to come up
with those topic distributions and the
topics okay so they all those others
must be discovered by the model okay so
that's what you do when you you know fit
an Lda okay so what doesn't output of an
Lda look like they look something like
this there are some other outputs that
Lda gives you but one of the major
outputs of Lda
is these multinomial over words which
are which are the topics so the NASCAR
topic for example has those words with
high probabilities what I put at the
very bottom row is to let you know that
actually every topic has every word in
the vocabulary it's just that some like
the money word here in the NASCAR topic
has very little probability in actuality
is probably much lower than that okay so
that's what the topics look like so if
we go back to the the question I posed
earlier if you have a hundred and thirty
million books and you want to answer the
question what are the books about then
you can imagine you you can sort of feed
these books into an Lda to discover
topics right so if you represent one
book as one document and you run it over
130 million of them you can discover the
underlying topics the underlying
semantic structure of your corpus let's
let's look at a smaller problem since
it's very hard to run Lda on a hundred
and thirty million documents but if we
have news articles and we have about two
hundred thousand of them over the last
twelve months then we can ask the
question what are the what are the news
articles about and this is something
that we can try to solve and one
difference here is that time is a very
important dimension here right because
news is inherently
Quen chill and temporal and you want to
know what happened when and how long did
it last and so on okay so we need
something that considers time okay so we
propose what we call topic chains which
is has the main purpose of uncovering
the underlying semantic structure of a
seat of a sequential corpus of noose
okay and this is a work that my student
did mostly and so if you have any
detailed questions about it you can send
him an email although I'll try to answer
most of them by the way if you have any
questions feel free to raise your hand
and ask okay so let's look at what what
we have what kind of tools that we have
available to us now so if you look at
the New York Times if you kind of scroll
down to the bottom half of the page this
is what you get you get a quick news at
a glance type of a thing
right so you can look at the New York
Times the front page of that website and
kind of figure out what's been going on
in the last couple of days okay in terms
of Technology or the world business arts
and so on and this is a very good view
and I love I love to look at it but this
as you can imagine takes a lot of
intelligence and a lot of work right so
this is a product of you know
intelligent New York Times editors out
there who are trying who are putting
this together and plus it doesn't have
the dimension of time because this is a
snapshot of the news right so here at
Google somebody has made this really
interesting tool it's called Google News
Timeline it's still in the Google labs
so you may not may or may not know but
this is where you can look at the
sequential issues and events right so
right now it's showing the monthly view
so you can
what was the most important news in
March of 2009 and so on and you can
search too so if you search for a
certain keyword then you would get
articles that are about that keyword
right and you can look at the weekly
view and the yearly view as well I think
and the daily of course so this is
pretty cool but I think that um here we
still have some questions that are
unresolved for example if you have an
article say there's an important article
in March of 2009 are there similar
articles that follow that are talking
about the same thing in April 2009
perhaps a couple or few months later
right and if there are similar articles
talking about the same topic over a long
period of time how long is that period
of time how long did that topic last
right and if it's a long lasting topic
then is it part of a general sort of
professional topic like the US economy
or was it part of a long-running sort of
event or issue such as the h1 and one
issue right or was it part of a very
short temporary topic such as the death
of Michael Jackson okay so we would like
to know those things when we look at the
articles but at least what we saw in the
previous slide didn't really show that
and if it's if it's a general sort of
long-running topic like the h1n1 for
example the topic is soft kind kind of
evolves through the nine months or how
many however many months that it lasts
okay first it was talking about the
outbreak perhaps maybe it was talking
about travel restrictions and then
vaccinations
that's schools and so on okay so we
would like to see how the same topic
evolves through time so what we propose
is something like this this is part of
our result is that you can look at
several months of news and kind of look
at the topics and how they're clustered
together in what we call topic chains so
you here you see that there's topic
chain about labor unions education the
war in Afghanistan the swine flu and so
on and then you would see some events
like there was a terror in Hong Kong or
something like that and the death of
Michael Jackson
so we produce something like this where
you can see the general perpetual topics
you can see the long-running it topics
and then you can see sort of the
temporary events that happen so so this
is what we call the topic chains and
this is the plan that we had to do
something like that right so what we did
is we took a bunch of articles over a
bunch of months and we divided the
corpus into time slices and we just
chose the time slice of ten days each
okay and for each time slice you would
have a bunch of articles right and we
can find the topics using just the
simple Lda
and when you have the topics from the
LDA then you can try to match them up to
see which topics are similar okay and
when you have the similar topics you can
sort of link them up into topic chains
and once you yeah yeah so I'll talk I'll
talk about similarity metrics in a in
the next slide I think or a couple of
slides and then once we have the topic
chains we can identify which are the
long top eggs which are the short topics
and within the long topics we can sort
of see what the topic evolution looks
like okay so let me talk now about sort
of each of those steps except for the
first one because that one is trivial so
we worked with the corpus of nine months
of
news in Korea so we took the websites of
three major newspapers and collected
documents articles from all of them the
corpus looks like that hundred and
thirty thousand articles one hundred and
forty thousand unique words named
entities and we we chose fifty as the
top the number of topics per each time
slice for a total of fourteen hundred
topics and let me just show you the
results of the LDA finding topics using
Lda
and there's since there's 1400 of them i
can't really show you them all but I'm
just showing you four examples of how
the topics turned out pretty good
the first one you can see is about
sports the second one is about business
and then about smartphones and
Technology and the last thing last topic
is about academia so when we have those
topics we can construct topic chains
like this where you look for similar
topics within certain window size and we
also I'll show you an experiment that we
did with increasing or decreasing the
window size and what happens there but
what it means is do you look at only the
time slice before or how many times
license do you go back to find the
similar topics okay so here comes the
answer to that question measuring
similarity this was kind of an important
issue because the major thing about
topic chains is that we're finding
similar topics right so remember the
topics look like those which are
multinomial over words and so you can
imagine various ways to measure
similarity and in within the topic
modeling research community people have
used most of these metrics most notably
they usually use KL divergence or cosine
similar cosine cosine similarity and
and I've kind of categorized the six or
six or 5:06 similarity metrics by how
each metric looks at the topics okay so
the first thing you can look at or I
said that a topic is a multinomial over
the vocabulary right so if you have two
probability distributions and you want
to measure the distance then KL
divergence is the answer right or j/s
divergence which is the symmetric
version of KL divergence okay or you can
look at a topic as a vector where each
dimension is a probability of the word
in the topic so if you take that view
then you can use cosine similarity
because to measure the distance between
two vectors right or you can use
Kendall's tau if you look at a topic as
a list of ranked words a ranked list of
words so if if we just ignore the
probabilities but just look at you know
NASCAR is the first rank and so on then
we can use Kendall's tau or DCG which is
used I guess a lot in information
retrieval and then lastly if you look at
only the subset of words that have top
high probabilities then we can look at
the intersection and unions of sets
which we can measure with jacquards
coefficient so we wanted to test these
metrics to see which would be the most
or the best performing similarity metric
okay so what we did is this we computed
the log likelihood of data of the corpus
given the topics that Lda found what
that means is if if you have low if you
have a small negative log likelihood of
data given the topics then that means
your topics are explaining your corpus
very well okay so the higher the value
there's sort of a mismatch between your
topics and and your corpus okay so it's
kind of like perplexity to Oh
so what we did is we took an original
set of 50 topics that Lda found for each
for one time slice and replaced five of
those topics with similar topics that
are found by each of the metrics each of
the six metrics okay so for example you
know if KL divergence says among these
50 topics and then another set of 50
topics in the next time slice these five
are the most similar pairs then replace
the topics from the second time slice
and kind of put them in the first time
slice so you would have the 45 of the
original topics plus five new ones that
KL says is most similar or most similar
okay so so then when we compute the log
likelihood of the modified topics or the
log likelihood of the data given the
modified set of topics then we can see
which of the similarity metrics found
the most similar topics okay so as I
said before KL divergence and cosine
similarity are most often used
similarity metrics and we found that Jas
divergence actually performs a little
bit better and the the asterisks next to
the metrics mean that there's a
significant difference statistically
significant difference between that
metric and Jas divergence and Jaccard
coefficient performs pretty well too but
we didn't use that because you have to
have this per parameter there's a
parameter that we had to set and we
thought that that's probably not as
general as just using Jas divergence
with no parameter so that's what we
chose to use as our similarity metric
for constructing the topic chains and
let's now talk about the size of the
window the size of the sliding window so
if we can take the Mar off assumption
and just look back one time slice to
find the similar topics but then you
would
find that that case of sort of the long
arrow over there where a topic was kind
of an important issue for a while and
then it kind of disappears for a few
weeks and then comes back again so we're
doing it we don't want to miss that
similarity chain there so this is what
we did as their experiment to see how
the sliding window size effects the
resulting topic chains so up at the very
top is a set of topic chains found when
we use the sliding window of size one
looking back just one time cycle and
then at the bottom is the sliding window
of size six so it's kind of an obvious
result but you can see when you're
looking back only one time slice then
the topic chains are kind of fragmented
and they're kind of dispersed all over
the place but as you increase the window
size then the topic chains over there
that had sort of a little gap at the
middle in the middle and then continued
a few weeks later they kind of merged
together right so the topic chains
become larger longer and you would find
these pretty large topic chains at the
bottom what's interesting is if you look
at the middle one where there are two
major ones and then they come together
at the size of five those topic chains
are about technology and business so one
of them is about the technology itself
sort of manufacturing research
development type of topics and then the
other one is sort of the business side
of the technology okay so you can see
that they kind of merged together at the
window size of five and it's kind of
hard to interpret that in terms of what
it means for the user right so if the
user wants those to be kind of
separately separate then that's probably
what we should do but if you want them
to be sort of in this same topic chain
then you might want to go with the
larger a larger window size but in
general as you increase
the size of the of the sliding window
the topic changed tend to become more
abstract so at the end you would have
something that's similar to like the
sections in your newspaper right so
business life and you know culture and
you know the world news and so on right
whereas sort of in the middle you would
have sort of more concrete topics okay
so that's what that shows let's look a
little bit closer at the chains
themselves what what they mean so if we
look at the long chains for example the
swine-flu chain right you want to know
more than just that there was this big
chain of swine flu and we can see that
you know in 2009 it was kind of a big
issue for most of the year we want to
know how that topic actually changed
okay so as I talked about before first
it was talking about the outbreak and
then vaccinations and so on right so we
call those focus shifts so within a
topic chain we can look at how the focus
shifts in in the chain and I apologize
for the small font it's it's really hard
to see but this topic chain is about
automobile industry okay so I'll just
read you topic number one has the top
words automobiles Vietnam Kia Motors
vehicle and sales and topic number three
which is right below that is develop
technology automobile investment and
Industry okay and then the other topics
are pretty similar to that so you really
can't tell what is going on just by
looking at the topics themselves okay so
what we wanted to do is look at words
that change the most between two similar
topics so what happened between this
topic and the next topic and if we look
at the words that are not common but our
most
different among the two topics then you
can sort of figure out what what's been
going on and on top of that we looked at
just the named entities named entities
are things like names of organizations
names of people sort of specific things
like that because a lot of the news a
lot of the events that happen in the
news are about specific people or
organizations and so on okay so coming
going from number one to number three
again when we do that the excuse me the
named entities that we find are green
solar Japan energy and what's laws and
carbon yeah so so that that tells you
that there was something going on in the
in the second so those are the words
that changed that increase the most in
probability from topic number one to
topic number three and we if we just go
back and look at the headlines you see
that there was there were a few
headlines that are talking about
Japanese car makers like Toyota coming
up with solar powered cars okay so so if
we look at this sort of close close-up
view of the topic chains and the named
entities that change then we can have a
much deeper understanding of the of the
evolution of the topics okay now let's
look at the short chains and this is
pretty interesting here every line is a
topic and the left column is the date so
zero p07 means the first ten days of
July in 2009 there was there was a
missile launch there was a discussion
over the North Korea missile launch and
then the next line talks about the death
of Michael Jackson and then some milk
scandal and then some heightened a topic
about heightened security at the end of
the year and then some romance over you
know entertainment people
in April if two men has Arbor Day so
talk talking about trees and stuff like
that and then the last topic is kind of
interesting
Obama Republicans Jeju Island is an
island in Korea that's used for resorts
and playing golf and you see golf and
Tiger Woods but I don't know if Obama
went there to play golf or not with with
Tiger Woods maybe but what we can
interpret that is that Lda found a topic
that's kind of not about a single topic
and Lda often does that if you've ever
run Lda or any other topic models you'll
find many of some of the results some of
the topics that you find are not really
coherent so anyway these are short topic
chains which means they're like two or
three topics or even one two or three
topics and they represent mostly
temporal events temporal issues or they
could be about incoherent topics and you
can kind of see how if it's a coherent
topic then it would eat more easily find
similar topics in the next time window
right okay so that's actually the end of
the topic change part of the talk how am
i doing time okay
I'll go quickly over the next topic so
we proposed topic chains which is a
framework based on very simple Lda to
understand what's going on in the news
corpus okay now let me switch gears and
talk about sentiments and aspects and
reviews so the the model is called
aspect sentiment unification model and
its main purpose is to uncover the
structure of aspects and sentiments in a
review okay and this is another student
of mine who worked on this mostly and
the problem is this if you go to Amazon
this is a review of a digital camera
it's a very long review it's like it's
like conference paper
almost
and this is actually not the end there's
more but it's a very you know detailed
review he talks about or this user talks
about a lot of good things and bad
things about this camera and we want to
do something like this right Amazon does
sort of aspect or attribute based
sentiment analysis of the reviews so in
addition to the general how many how
many stars did this camera get it also
gives you how many picked ours for the
picture quality and so on the way Amazon
does it I don't know exactly how they do
it but I noticed that so this is a
camera with lots and lots of reviews
like 300 reviews or so and then there's
it's the same canon digital camera
certain other models which have very few
reviews and for those we actually don't
have these attributes so it looks like
there's some some manual work and some
automated way of looking at what the
attributes are and we call those
attributes aspects and there are things
like this this thing is small and it's
light starts up and turns off fast the
low-light performance is best and so
these are actual sentences from the
reviews and the sentiment is is
something like this the the words
highlighted in pink are the ones that
carry sentiment for this for each
sentence okay so let's look a little bit
closely at what these sentiment words
are okay some of them are general sort
of effective words that express emotion
like love I love this I'm satisfied I'm
dissatisfied I'm disappointed okay and
then some of the other ones are general
sentiment words like best
excellent bad they're they evaluate the
quality of something but they're just
general if something is best then it's
best no matter if it's a coffee maker or
a chair right and then there are aspects
specific evaluative words and this is a
little bit more fine-grained than
domain-specific evaluative words so let
me show you what I mean
in the camera domain okay if you say
this camera is small it's probably a
good thing the LCD is small is probably
a bad thing right if you're in the
restaurant domain the beer was cold is
good Pizza was cold is bad and the wine
list is long is good and the wait is
long is bad so beyond the domain right
we need to go sort of down to each
aspects of the review and say whether
the sentiment word there expresses
positive or negative sentiment okay so
this is the problem that we're trying to
solve okay we're trying to discover the
aspects automatically as well as the
sentiment and the words that carry the
sentiment so to do that we made two
models one is called sentence Lda the
other is called aspect sentiment
unification model and we worked with two
types of corpora the first one was
Amazon reviews and we took seven product
categories including digital cameras
coffee makers
I think heaters and things like that
they're just pretty different electronic
products oops
and and we also looked at the Yelp
restaurant reviews over four cities and
320 restaurants and on average each
review had 12 sentences and our
observation starts again with the same
set of sentences what we noticed here is
that for many of the sentences in the
reviews one sentence describes only one
aspect okay and this is different from
the general Lda assumption which is that
each word in the corpus each word in the
document represents or is generated from
one topic or if you apply it to aspects
one aspect okay so we wanted to make
this sentence Lda if you notice the only
difference is the box around the W
circle okay so what that means is the
words and n is the number of words in
your document so each word is
generated but Z which are the topics are
over m which is the number of sentences
okay so we're saying there are only M
aspects in the document which is the
number of sentences in that review okay
and each sentence has one topic or one
aspect okay so that's the basic
difference between LD n a and slda and
what we found is that when we run slda
over our our data so this is oh this
yeah the results from both the Amazon
reviews and the last one actually is
from the restaurant review so remember
we we ran slda over all sort of seven
categories of electronics reviews and we
get these aspects that are similar to
what we saw earlier in the Amazon
attribute categories okay so the
portability quality of photo and ease of
use those are the three in the camera
product and then if you look at the
laptop reviews the first one is about
software and OS and then the second one
is about hardware and so on and some and
what we found when we compared the
results of Lda
versus slda is that LD slda was finding
more product specific aspects okay for
example the last one liquors category
liquors topic or aspect is was not found
by LD 8 instead the words like beer and
wine and martini was actually one of the
top words - they were kind of spread out
over different topics like wine was
maybe with the Italian food aspect and
so on so I think it's important to
notice that LC slda because of that one
difference in the assumption it makes
finds better product specific aspects
details of the reviews okay
we then tuck slda and
standed it to form a joint model over
aspects and sentiments okay so the right
side of the model which has the gamma
the PI and the S so s is the sentiment
and you can see that word is now
generated from a pair of sentiment and
aspect okay so with this joint model
then if we run it over the corpus
without using the labels any labels of
the of the corpus of the documents we
can automatically discover the aspects
and the sentiments okay but we just use
seed words we took tourney's paradigm
words they're kind of generic paradigm
sort of sentiment words that a lot of
people use like good and nice and bad
and nasty and then so that was one set
of seed words were used we also
augmented the paradigm words a little
bit with other sort of general sentiment
words that we found from the corpus okay
so and what we do with these with these
sentiment words that are a little bit
different from other prior work in this
joint modeling of sentiment and aspect
is that we build the seed words right
into the model by playing with the
priors of the LDA okay so setting
asymmetric priors and initializing gift
sampling which is an inference algorithm
to to kind of play with the seed words
okay and I'll let me explain that a
little bit better here although I didn't
even talk about gift sampling so so if
you want to explain that we can talk
later after the talk so beta is the
prior for the racially distribution over
the Phi's okay
and what that means is do we start with
a uniform distribution of betas which
means that every distribution is equally
likely if we play with the betas and
asymmetric priors then
we're saying some of the distribution
distributions are more likely than
others okay so so what we do what we did
with slda is we just use the uniform
priors what we do with the beta is here
is we set zero beta to zero for any
negative sentiment seed words in the
other in the opposite in the positive
sentiments okay that means and when we
do vice-versa for the negative for for
the positive sentiment seed words okay
that means if you have a positive seed
word like good then it's not going to be
assigned a nonzero probability in a
negative sentiment negative aspect
sentiment okay
and also we start gives sampling we
initialize that give sampling by setting
the positive seed words to have positive
sentiment and the negative seed words to
have negative sentiment so that's
supposed to randomly assigning sentiment
which is what we usually do for give
sampling okay so the combination of
those two make the seed words kind of
right into the model without without
fidgeting anymore with the words
themselves okay so so these are senti
aspects discovered by awesome as we call
the model and these so every multinomial
now is so every word in the multinomial
is generated either by the sentiment or
by the aspect or actually jointly by the
pair sentiment and aspect okay
so interesting results here like the
meat senti aspects here negative
positive and meat negative the meat
aspect was not found by slda so what
this tells you is that for some senti
aspects if there's strong sort of
sentiment correlated with that aspect
then it comes out better with the
awesome model than it does for just slda
without sentiments built in so so in
original
slda what happens is the meet aspect is
kind of scattered around again in you
know in pizza and burgers steak right
those those aspects have meet words in
them but because we've forced kind of
the sentiment to play a bigger role and
finding the aspects we see aspects like
that and an interesting case that we see
with payment is that we only get a
negative aspect for payment we don't get
a positive cent asmik for payment it's
the same thing with parking - what that
tells us just an interesting bit is that
people complain about payments not not
being able to use the credit card or
they complain about parking situations
but if they have some satisfaction with
it they don't really write it in the
reviews okay and the yummy aspect is
kind of funny - the last word is funny
right so so that aspect is something
that Lda doesn't really find okay so let
me go on so what we can do with these
topics
okay the words and the topics is then we
can try to figure out which are the
sentiment words and which are the aspect
words right so if we have the to meet
senti aspect words we can look at the
words that appear common across the two
sentiments like meat and I don't know
what else sauce I think and we say those
are the common sort of aspect words for
the meat category okay whereas things
like crispy or bland are the sentiment
carrying words for that aspect okay so
those are the aspects specific sentiment
words and so what we do we is we align
the top of the senti aspects with the
similar aspect again and then we look at
the positive aspects the negative
aspects and we look at the common words
and the words that have a lot of
difference in them to figure out things
like this the screen aspect the words
the common words are like screen glossy
LCD and then the sentiment words are
bright clear those are the positive
words and like reflect glare Macbook
obviously doesn't have a good screen so
um or apparently I don't I don't agree
with that personally but anyway so
that's what we can do with those topics
okay let's look at some other results of
this so here is a result that shows you
that sentiment classification per
sentence is done pretty well so we for
so these are two reviews the first one
is about a coffee maker and then the
second one is about restaurant and you
can see in green those are the positive
sentiment sentences so that's what the
model found as the positive sentences
and the ones in pink are the one the
model found them to be negative and of
course I'm going to show you good
examples but most of the examples are
pretty good okay so another set of
results we can do we can look at is how
well are the aspects assigned to the
sentences all right so these are four
different reviews where the same aspect
was found the aspect sent the senti
aspect of parking and the negative
sentiment and you can see parking is
only validated for three hours and so on
so those are and these came out pretty
pretty well right here's another example
some some of these things like very
convenient how the model fine found that
to be coffeemaker easy
it could be some I don't know it's PI
just because convenient is up there as
one of the top probability words for
that senti aspect some of the other
shorter sentences this model has trouble
with because there's not enough clue
okay but and and and I do like to show
some of the bad examples as well so the
second one it took us several uses to
understand how much coffee to use that's
obviously not a positive sentiment but
the model classified it as that but you
know one out of five is not bad right
okay so those are the senti aspect to
scientist sentences that we can see oh
and the last one
I put in there to show you that our
assumption that one sentence carries one
aspect may not always be true right so
the last sentence is talking about how
nice it looks and how easy it is to use
and you can kind of say you know are
they the same thing or not I don't think
they're really the same thing
nice-lookin is probably it should be
another aspect sort of the design aspect
of the product and then the ease of use
is the usability of the product right so
we do see a lot of sentences in our
corpus that do not validate our
assumption that of one sentence equals
one aspect or even one cent is one one
sentiment but so that's future work for
us right - - to deal with sentences like
that okay
so all of these that I've the results
that I've showed you because of the way
topic models you know they produce these
topics it's really hard to evaluate them
there's really no good way to
quantitatively evaluate the aspect so we
can't ask users to go through 20,000
reviews and find all the aspects and
kind of compare them against our results
right or the sentiment so a sentiment
would be a little bit easier to do so
what we do with sentiment actually is we
quantitatively measured how sentiment
classification is done against other
generative models that jointly model
sentiment and aspect together and so so
this I have to tell you though so these
are model as well as these other models
jst and TSM are the two models that
we're comparing against they're all not
designed for sentiment classification
per se so they're all trying to discover
aspects and sentiments together and come
up with these sentiment words and so on
they're not you know models to do
classification and neither is ours but
we put this experiment in there to show
at least that sentiments are found well
okay so let me explain the different
things awesome
the
is our model with the regular paradigm
words I think there's like a dozen of
the paradigm words there I showed you
awesome plus is the paradigm plus words
the Augmented list of words and then jst
plus and TSM plus we also they also use
seed words so we use the same set of
paradigm plus words and we implemented
those two models and ran classification
over our our own corpus to see how they
how well they perform so you can see the
red line which is awesome plus performs
the best the next one is the blue with
so awesome without the paradigm + words
and then the other models don't perform
as well on on the classification task so
so just to tell you these models are
pretty similar to ours
they both don't have the one sentence
one aspect assumption built in they
don't use the seed words right into the
model they do something else with the
seed words so those are sort of the main
differences I would say between those
two models in our model okay so let me
just wrap up I think time is probably up
to I just talked about a cell slda
and awesome which are the two models
extensions of the basic Lda to discover
sentiment and aspect together and we
discovered that the specific aspects
that we found were pretty well aligned
with the details of the reviews that
people actually write and we can by
looking at the topics and the words
within the topics we can learn aspects
specific sentiment words and lastly we
just tested with sentiment
classification and and found that it
performs pretty well okay so just to
wrap up now really topic chains and
awesome are the two things that I've
talked about and they both work with Lda
right so they're on different domains
the one is on the news domain and then
the
other one is in the reviews domain where
we're trying to do the same thing we're
trying to uncover what is latent what is
the hidden semantic structure within the
news corpus and within the reviews
corpus so if you're interested in our
work for their discussions with me and
my students please send us email or you
can look at our website to see what the
latest things are going on okay thank
you questions
could you comment on what you discovered
when you ran these two books like you
were mentioning the 130 million books we
haven't done that ever done that yeah
well I should ask Google Books to do
that for me I don't I can't imagine what
the results would be one thing about Lda
though is that in topic models in
general is that they are very
computationally expensive as you can
imagine right if as the document size
grows large the number of a capital
unique vocabulary grows large and you're
doing gibbs sampling over all your
vocabulary at each iteration and you
have to do thousands of iterations to
converge so the inference part is
difficult and we would like to maybe use
google and you know use distributed
computing and all that to to figure out
how to do that
so the aspects you found usually it
seems not the ones defined by users like
the camera is there any way you can
specify okay I like to find those
aspects specified by users yeah that's a
good question so we are we are thinking
about it we haven't done anything I
don't know how to do it sort of like the
sentiment seed words you can have maybe
seed words for aspects too to say I want
to find these aspects good question and
a good idea for an extension of this
mark okay
thank you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>