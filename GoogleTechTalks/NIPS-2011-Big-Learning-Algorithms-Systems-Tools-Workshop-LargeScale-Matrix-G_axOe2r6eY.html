<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Large-Scale Matrix... | Coder Coacher - Coaching Coders</title><meta content="NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Large-Scale Matrix... - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>NIPS 2011 Big Learning - Algorithms, Systems, &amp; Tools Workshop: Large-Scale Matrix...</b></h2><h5 class="post__date">2012-02-13</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/G_axOe2r6eY" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">Amy so this started off as a MapReduce
algorithm and most of the stuff I'm
going to tell you is actually about
MapReduce and how you can do a
stochastic gradient in all versions of
MapReduce but the experiment i'm going
to show you another MapReduce for the
reasons I've II just saw it in the top
right so you can do it much faster if
you implement an MPI and I think young
is one of my co-authors also implemented
on spark and it seemed to work very well
there so this is jon bovi speedo haas
and llanes from IBM and christine and
palace my pit students will right there
ok so what we're going to talk about
we're going talk about majors
factorization of this gradient descent
or su casa gradient descent so here's a
a simple example of what we're trying to
do so here's the matrix suppose it for
some reason we do not absorb the entire
matrix so we just observed some points
so now this is really bad because of the
quality of this Beamer but actually if
you if you look at it my screen you can
kind of make out that it shows your face
so probably you can't so what we want to
do is we want to take this partially
absorbed image and we construct the
orange ok so how we do this is we assume
that we can approximate this original
image to go to a good approximation with
some low rank matrix and if you have
just some points in this image they
might be good enough to extra
reconstruct low point matrix so if you
do this on this image on this sample
image that I show you here this is what
you get alright so you can see it's a
face but you can also see it's kind of
noisy right so you probably want to do
some are better so one thing you would
probably do in this setting is you want
to regularize so I did not recognize
that's where you can see all these
spikes and you probably also want to
make use of the fact that if you have an
image the neighborhood pines are likely
to have similar values and there's been
a paper than it nips conference on how
you do that for images I do not want to
talk about images I want to talk about
the more general problem right so here
is an fx problem its most of you I guess
have seen so consider your matrix with
users and movies right so you have sweet
users Alice Bob and Charlie and three
movies ever tall the matrix and up and
I've seen some ratings I'd one means bad
and five min scores ok so we're trying
to do is you're trying to predict the
missing wings in this matrix it's
basically the same thing than what would
I just showed you by is not an image and
you
like it's a it's a real value matrix so
how could we do this you could do a
factorization so if the simplest
possible case i ranked one factorization
what I do there is associated with each
user and is each movie just one number
but you can think of this number for
example as the degree of to which the
movie is an action movie or the degree
of two which uses like actually weeks
right so sometimes there is this
intuitive connection between the numbers
and something in the river but they may
not be in general so we associate these
numbers such that they are pretty good
at approximately the original values
that you see right then you can do this
for example by taking the part of these
two numbers and try to make it as close
as possible to the original values okay
so once you did this you can actually
use this to predict the missing entries
and this is they see it all there is to
these kind of confusion problems so now
to make this work better to get a better
estimation of the missing entry suit you
add stuff to this loss function i showed
you here so something you would for
example add is like user movie biases
some users like movies generally better
than other users like movies or some
movies are just better than other movies
and you would try to add these kind of
places to be more you also add
regularization and you're probably also
at time because the taste of people
change all the time okay and this
basically what you see here is roughly
the model that was used one of the
models of us used to thin the netflix
temperature one of the pieces okay so
let's generalize about this and let's
talk about the problem that i'm going to
talk about how to solve right so it's a
generalized matrix factorization and you
can use it for these kind of
applications i just showed you but also
other applications like text indexing
face recognition or topic models so
while we what the problem looks like
this as follows you're getting some
matrix the blue box here the matrix V
and you're going to some entries in
these matrix right for example the
yellow entry here right so this could be
in the case of the netflix competition
this will be the observed ratings all
right but if this is it for example that
document world matrix this would be the
votes of creating documents and nonzero
entries and we're going to assume that
the number of entries like you actually
see you are is much smaller than the
entire matrix okay then what you want to
do is you want to fit some parameters
some for the bros some for the columns
and put
movies for the users such that they're
pretty good at estimating the entries
that you see okay and this might be done
in the simplest case is no product but
you can also use imaginado six so we are
going to use a loss function that we
associate with each element of the
matrix right for example with this IJ
element you associate loss function i J
it takes in this features here from the
rope and the features from the column
and tell you how good you all right so
this loss function would include
something prediction our regularization
or video Information Act date and time
whatever you can think of okay and then
you put in some constraints for example
I want all these numbers here could be
non-negative okay so the problem that we
try to solve is given such a statement
try to find the best way to CC owh that
minimizes sum of all the losses in the
entire matrix of all these local offices
okay so this is a problem that we try to
solve all right so here's some instances
the movie recommendation I've told you
so the day is that you get if you
participate in the in the competition
was like I for a thousand users and
twenty thousand movies but on netflix
has more than 12 million users and
they've much more than twenty cells
movies and if you project up the numbers
that they publish this would probably
correspond to 2.4 billion ratings and 36
k by 2 theta 9 gigabyte model or more
depends on what the mango factorizations
and similar there are the applications
where these kind of techniques have been
used for back stat recommendation about
the paper by Microsoft or 4 News
penetration by Google items and if you
look at these problems you can see that
they actually pick right so they have
big data this is not so it's not
terribly big right so it's not petabytes
of data the bigger problem is these kind
of things is that when you want to
compute these factorizations actually
the computations that you have to do all
going to be expensive so I argue that
you actually have to go and process
these things so completely sings in a
distributed fashion because otherwise
symptom of CPUs we have oh just
insufficient to do this in a reasonable
month time ok so how do we do this I
want to talk about su casa gradient
descent very briefly like because I know
the nose most of you know it so what's
the classic indecent
some function this is not no function on
two values and this is the optimum of
the function in the contour lines your
online so it takes two values so what's
the casing pretty decent you pick a
random starting point you have for
example this one here now the red line
here is a gradient if you do gradient
descent you would now walk in the
opposite direction of the gradient if
you just took a sec gradient descent you
just estimate this baking for example
with this blue line and then you walk
along the blue line can you do this over
and over over again and then you get
this cooking right and you can see it
converges to the right point and you can
describe this by its different equations
to Casa different situations would take
this and you can prove that they
actually as entirely notes this is
making decent okay so how does this look
for matrix factorization so this is the
last if you remember it's just the sum
of all the absorbed entries we have and
if you look at the gradient of this loss
because the greatness is linear operator
it's just the sum of the local agents
over all the entries so now how can you
come up with a quick estimate of what
the gradient is where we just take one
random entry evaluate its loss and then
scaled up by the number of values you
have okay so it's a very very cheap and
very very crude approximation about this
great handles and now you can put this
into an organism which picks a random
entry evaluates the gradient updates the
parameters and repeat it sounds very
stupid but if you actually try this in
practice it works pride quite well but
here can see three different algorithms
the black one is a bfgs the green one is
alternating these squares and the red
one here is su G which is dumb bust of
the awkward if you want and what this
shows on the x-axis you is how often did
a scan the data doesn't show time how
often is a scan the data and on the
y-axis how good am I ok you can see that
in this metric actually performs very
well right so if you go to plot this by
time i'm going to show you later we
actually see that ALS is much much
slower and running in each of these
epochs it has to do much more
computation so for example it scales up
cubic with the rank of the factorization
there's an STD its linear until you have
a hiring factorization that will be big
big big difference in one time so this
is easy to paralyze this is easy to
paralyze is not so easy right but it's
the best awkward so what we try to do is
try to make this power or distribute it
okay so how do you disturb us to do so
this time the problem is that the steps
that you do in SD depend on each other
right so what you do here at some point
NTM you value of the gradient
approximation of the green at this point
and then you make a small step to get
theta n plus 1 by the now if you want to
have theta n plus 2 the next step you
need Peter M plus 1 is an input so
everything these steps depend so what
people did is it just ignore this right
and try to try to work around it right
so here's two things that have been
proposed recently one is parameter
mixing which means is ok where I'm going
to do is I'm just not dealing with these
dependencies and dividing my data into K
subsets I spread it to K nodes each node
1 HD for a while and then everyone's
result at the end and this is my this is
going to be my my solution and this is
this book this works for convex problems
but this problem is actually not convex
so it may it may still work so we tried
it and this is what you get like the
boomerang so it doesn't work right
because II the averaging of these local
solutions does not give you a good
global solution okay so the next thing
you can do is to make this better
something called iterative parameter
mixing to do the same thing you spit the
data on cocaine notes you run for a wire
not until convergence then you have it's
a power meters broadcast them again to
the node continue running and so on and
by this you actually get convergence but
it's no I don't know if you can see this
it's this menu like the blue line and so
by doing these kind of averaging change
you're losing lot of the power of the
density has a lot of the performance
that CD has okay so what we thought is
chemical can be committed result average
so here's how we do it it's actually
very very very simple so if you look at
the the ICT updates at YouTube right so
what are you essentially doing you're
evaluating a local ossify the grain of a
local dysfunction and the only thing you
have to do is read the current row
vector of all parameters liquid column
parameters compute this gradient and
then update these two things okay so now
if you look at two successive steps what
could happen is that the next step is
here wait so then you're in trouble
because you're reading something that
you updated here so you cannot relate
these two steps same thing if you're
next next step would be here but they
depend on each other
but in most of the cases two steps will
be completely independent of each other
so this step oops on a completely
different subset of the parameter space
under step right and just by making this
observation you can actually easily come
up with a parallel su dia organism so
what you do is before you do is step you
lock the row and column we do your step
and then you can do this this 8 10 20
sweats and Paulo and it will look
beautiful like you tried this right so
for full polarization this is a very
nice structure because there's no
contention basically for these resources
okay so skip this so you've seen that if
you if you pick something different rows
and columns the things are independent
right so how can be users to come up
with a distributor organism so we take
the input matrix and suppose we have
three nodes you split it like this so we
split the matrix into the three by three
blocks maybe split the W parameters into
three blocks the H parameters into three
blocks and then what we do is on the
first note we put this entire row of
blocks you on the second node we put
this entire world blocks and then
certain of this integral of blocks okay
and let me doing the following thing we
select a subset of the blocks for
example this diagonal and then we run
LCD on each of the subsets and if you
look at this on each of these blocks
we're actually broken again on different
parts of the parameter space which means
i can actually run this impeller without
communicating at all like on these three
notes so once we're done with the block
you just pick another set of blocks like
these ones and we do the same thing
again okay I'm use over and over and
over so this is an instance of optical
stratified su G which is a more general
organism but this is how you would use
it for these kind of factorizations and
is probably correct and i want to show
you how roughly how it works at like the
intuition behind it so suppose this is
our matrix here right so it's it's a two
by two a block the two by two and we're
dividing into two different pieces like
the main diagonal and the sub diagonal
block of something happens okay so this
is I hope you can see this this is a
function one optimized on this entire
matrix this green function right this is
our starting point this is why we want
to go okay if you only look at the red
part of the matrix the optimum is here
right it's far away if you only look at
the blue matrix the optimum is here
right so we want to go to the global
so what I'll give is Macy does is it
picks rats and then walk slow vet so
what it does here it optimizes the wrong
function optimize the wrong function
okay then again it decides red red or
blue pics wet again folks along the red
function and then we're double again
don't fix blue now I bought a box along
the gradient on this blue function and
you repeat this repeat this until at
some point of time you can push what's
basically happening that as a solution
you can think of the red and the blue
solution pulling the parameter to itself
and at the point where boys put at the
same strengths this is your solution
okay and if you do this with so
castigating you send it's basically the
same thing right and I think this makes
should give you some intuition of why
this by the sink smoke you just make
sure their posts get equal time pony all
right so let me show you some
experiments on this so this is on the
netflix data so when i show you here so
i want two hundred epochs to run scans
over the matrix whose STD is done with
sequential one and then it's a
disability but i just told you one node
eight sweats to notate sweats for night
sweats so what you can see here is that
you get a big speed improvement in time
x is one time here right so let's zoom a
little bit in right you see what's
happening so this is STD right the black
line here the red line is if we run the
stratification trick i do the diagonals
on eight sweats on one mode okay the
green one is on two nodes with h let's
each and the blue one is on four nodes
these wet suits and you can see that we
can get weekly gets beat up like they're
going to run stuff faster so how much
faster RV so this so how much faster be
so this is the blue line here which you
probably can't see which goes in between
you is if you would just have run it in
parallel on one note without doing this
kind of blocking in the diagonals just
one little pair of the lock rows and
columns so it actually looks better if
you do which is one note then doing this
kind of blocking this verification
okay why is this so if you add to notes
then then the two notes will be faster
get for you professor again right in
general if you have bigger problems in
the net fix problems you're not going to
get away this one not anything okay so
why is this so if you look at this this
version of the plot so now again I show
you how much effort progress we made in
each scan of the matrix and this is the
loss but you can see that there's a
difference between the standard SUV
organism and our overs right but you can
also see that this difference doesn't
change much if I go from one to four
loads and so on so paid once all right
but then you can scale up by adding more
movies so currently there's basically a
car set of paper distribution this is
probably true for most of the organisms
we think that there's there are some
ways to actually make these grave
clothes right so that you don't pick
this constant okay so here's another
example on the ket data it properly
looks the same my let me to show you a
bigger examples so this is a 10 million
by 1 million matrix that be synthetic
you generated it has 1,000,000,000
nonzero entries its rank and we try to
come up with a rank 50 factorization of
it and again I show you three different
water reasons so the black one here is a
distributed version of ALS 18 notes
right and you can see there's trouble
conversion right so this is logs gave
you like it has total conversion it
starts coming into Long Valley in bonds
as well so as is not the organism you
probably want to use these kind of
problems so the green line here is if I
just use it on one note with parallel
sweats and this matrix is just so big
that it still fits into memory for nodes
it's by roughly Saudi bigger plans okay
but if you go and distribute it you can
be much much much less let me try this
with matrices up to half a terabyte of
size or two bigger clusters and it seems
to look very okay to summarize I talk
about major a factorization you can use
it for different problems by customizing
these Lots functions that you put in
there all right so for example you can
use it for images fec nip nips and you
have large instances in practice why'd
you kind of millions of rosemary columns
billions of entries so the disability is
sarcastically indecent approach i showed
you is very simple and its most of time
so you can implement this just a few
lines of code you don't average instead
of averaging you make sure but note that
people broke on different parts of
parameter space at all times and it says
fully distribute data the model is fully
distributed the processing is fully
distributed and its competitive to the
all terms of organisms that we compare
it to okay so in the future we are
trying to improve the stratification to
get this gap that I showed you a little
bit lower but we also want to apply this
to other models because I think the same
is a basic idea it's too large of
different sheeran organisms thank you
yeah yeah yeah yeah you do it in a way
that minimizes communication
Oh
yes but the way we do this we would be
viewed randomized the rows and columns
before use this blocking to make this
very unlikely the randomization looks
pretty well you can try to do something
cuter to make sure that the blocks of
our key balanced but we didn't do that
so for LS what what happens is that if
you run a couple of steps and you look
at these distributions that it gets it
starts getting one matrix very low on
the other one very small one very big
and the other one very small and as
trouble what I would have to do is
increase one and decrease the other at
the same time but I can't because it
updates one matrix optimize one matrix
then optimize the other matrix optimize
the first matrix and so on and this is a
reason one of the reasons why get stuck
it makes two big moves initially
good</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>