<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>WITCH: A New Approach to Web Spam Detection | Coder Coacher - Coaching Coders</title><meta content="WITCH: A New Approach to Web Spam Detection - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>WITCH: A New Approach to Web Spam Detection</b></h2><h5 class="post__date">2007-12-15</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/UAO73M7NlHk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">and then they'll be pointing to my site
right I mean I'm paying 21 dollars for a
click to sell home equity loans that's -
what - uh domains I can register two
domains with that maybe even three with
certain domains
it looks like seven dollars is $7.00 for
a domain now I don't even know how much
it is per domain so sort of the new
trick is up here's how I you know make
my site look good I'm some you know I'm
some you know not so so nice site on the
web I sit there somewhere in the web and
I have you know some content or I'm
selling something and no one links to me
I'm just sitting there bored and lonely
what do I do well I mean first I create
some pages I add in some extra nodes in
this graph then I link to myself so I'm
now going to make all these pages and
these new friends that I've created and
make a link to me they're all going to
link to my page so now so that looked
very popular this may look a little
fishy to a search engine so say okay
well how would I have them linked to
each other as well so all these sites
now link to each other it looks like
this is big complicated network which is
surrounded by me they're all linking to
me I look how popular I'm Amaker I'm a
really really good web page now this
still looks a little fishy so maybe what
I do is I also say okay we know what
I'll just link having link to some other
normal pages as well I'll create some
other pages have them link to link to
some good pages as well you know right
now if you know if you're a indexed or
you didn't see the evil faces this might
look like a normal web graph and you
look might look like a very popular page
so this is basically what web spam is um
I think that you don't actually have to
create your own domains now I think
there's actually businesses which do
this for you and they have pages which
look good and and which have links you
know you basically pay them to put links
in different places but this is well
this is basically the way it started
people would just generate new domains
or they would take other domains and and
and start putting pages that look like
good pages
I actually I just got one of these I
think this if I can do this easily yeah
oh is this gonna or is it yeah this is
um this is a spam page you guys probably
saw spam I think a few years ago you saw
a lot of web spam I haven't seen it why
I was a super hard to find you know one
links to web spam anymore you can't like
the Google searches don't come up with
web spam anymore I'll even typed in spam
examples you could even find them turns
out one way to get your site to drop and
PageRank is to link to a spam page
because it looks very very fishy
so I guess if you look basically there's
all these just these random you know
these random you know here if you want
to go to hotels and all it says is we
show the best folks websites hotels and
just has all these random links to other
sites and flights and again best
election websites and flights and
holidays all these all the all these
pages are exactly the same except for
the word that the word in bold here so
we'll have to get back to the where is
it oh so it work well it's good so so
that in terms of a learning problem this
isn't typical I mean you actually have
adversary generated you have a graph
with adversarial generated nodes
webpages that were created for the sake
of fooling you I mean most learning most
machine learning problems are one where
you know there's some kind of using
there's some random process generating
your data you know trying to figure out
you know maybe there's maybe it's one
prosecuting generating one type of data
and one other prostate another nobody
data like maybe you're trying classify
you know state or machine learning
problem may be classifying cancers or
classifying people's faces and images
that's a standard machine learning
problem you know but you sort of soon
this is kind of this is generated by
some natural process like for example
you know people's genes creating their
the weight of their faces look and you
know the environment this is not natural
process this is the process created for
the sake of fooling you the point is you
didn't well the indexer to actually be
able to figure out what which pages are
spam so I mean it's inching I mean you
know you have this hyperlink wrap as
well but it's not clear what the hyper
got hyper hyper link graph means I mean
what is what does a link mean I mean I
have a link and I know it link and it
goes somewhere but but what does it mean
was it created for the sake of you know
this page actually thought this other
page was good or did this link mean that
you know that it wants to boost the rank
of this page or is this link just meant
to you know this this this particular
host is just used as a spam host but
it's just linking to good you know it
could be looking to good hosts just for
the sake of making it look like it's not
a bad host I mean it's not clear what's
going on you have a lot of confusion now
on this problem right so um from a from
a machine learning perspective this is
sort of what what the problem looks like
you know you have you have a big graph
and this I think of this is like you
know millions and millions and millions
of pages and then you have a few that
you know that are bad and a few more
that you know that are good in fact in
general I think I think roughly ten
percent of pages are bad so there should
be like a 10 a 10 to 1 ratio here you
know you have all these hosts in the
goal and you have no idea and of course
you can't label them you know by human
can't you know too expensive to label
all these nodes but you know you want to
say okay I have a very sparse collection
of good knows of even smaller collection
of bad nodes now I forgot what would
these guys in the middle you know are
they spammer or they not spin out how
can I trust the links coming from their
pages so um it yeah so it turns out that
um that Turner there's one observation
here that is very very important I don't
know if you this um you guys very
working web spam you probably note this
is one fact about web spam which is
basically consistent across the board
can anyone guess has to do with the
links
but turns out you almost never see a
good page linking to a bad page it
almost never happens so basically if you
see a host that you're sure is good
linking to another host that you don't
know you can almost be certain that that
host is good I mean it's it's not
certain but you have a really really
strong guarantee that's paying you know
this page is good you've looked at it it
looks like good page it's literally like
cnn.com today come it's not going to
link to a spam page that's a huge
advantage that sometimes make the
problem very easy in some sense because
you know you know this you know it
basically you can be pretty sure that
that that that that nodes that basically
only have links that go - too bad pages
and and or links and only only have
links that come from bad pages are going
to be are going to be a hole and sorry I
have to confuse links it what was that
oh so I'm walking by sorry if it's ones
question um you know that when when it
when it when a good page links somewhere
you can almost be sure that the other
page is good that's that's that's a very
very nice fact that makes it makes the
problem much easier so um basically what
I'm saying here is it you know you have
a good node like this you're never going
to see it linking to a bad node it's
never going to happen
that's very very helpful for you um so
if you saw um if you saw a note in here
linking to a bad node that would all
tell you a lot as well it's very likely
cannot be good node so um I'm gonna talk
about some sort of you know methods have
been around for a while for detecting
web spam
there's sort of two roughly two
categories one is graph base neither one
is content base you want one you took
advantage of the hyperlink graph you try
to use the hyperlink graph as much as
possible any other methods you try to
use the content of the pages so the the
graph based methods are sort of very
similar to sort of PageRank you guys
probably mostly goes what PageRank is or
have a rough forget what PageRank is um
trust rank is basically this it's
roughly the same thing it uses sort of a
some different sort of it's sort of
parameters are different than the
PageRank algorithm
typically with PageRank what you're
doing is you're you're you have a these
these uh these are sort of your ranks or
your ranking numbers or how are your
scores for a gift for the page at some
vector of scores T the capital T is a
transition matrix so it's like it's
basically just the matrix generated by
the links so if you know the IJ entry is
is 1 over the number of outgoing links
from the
I if there's a link from I to J you put
one over the number of outgoing number
but it's normalized you normalize within
about going links so it should be a
probability matrix and basically to
compute PageRank you sort of just you
should have hit hit this vector T enough
times with with this matrix capital T
and then of course you're supposed to
normally supposed to sort of add kind of
like a constant factor of alpha every
time to make sure that everybody gets a
little bit of a little a little bit of
trust or a little bit of PageRank and
then after that it's propagates
propagates from there I don't to go to
which details with this but this is
basically roughly the PageRank algorithm
but with trust rank what you do is you
set the initial distribution to be
basically you set zeros for pages that
you know aren't that you don't know
anything about and you set one over m
for the pages that you are sure are good
so you have page that you're sure that
are good and you make the initial
distribution one over m where m is the
number of good pages that you know about
so effectively you have your initial
distribution start with only good pages
and see all the bad pages get zeros so
um in fact yeah for the sucker you don't
even using the bad nodes if you have
some bad nodes you don't you don't
actually take advantage of them here
there's actually methods for taking
advantage of that it's like a name ask
for that after the talk and I could I
could shed some light on that
so basically in the end that the trust
scores are going to what's going to come
out of the algorithm at the end you're
going to hit T in a little T with this
matrix capital T it's gonna be something
like a station in sputia of this Markov
chain that's a whole nother field I
don't want to get into that now but so
um trust rank basically you can actually
think of it as sort of a random walk
where basically you're starting in the
graph at a good node and you just start
walking so here we're Skrim started that
that node right there and you're gonna
start walking around the graph and
basically if you run this for a number
of times where you're going to end up
and if you end up at you know a given
node a lot of times it means it's
probably not spam and this is sort of a
good heuristic actually for checking
some expand because if you sort of if
you could get from from this node from
from a good node then you know probably
this guy link to go down he probably
linked to a good know he probably knew
good node and then at the end the last
guy should have been a good note um
nodes like this guy right here he that's
a bad example actually sounds like this
guy right here probably not not good
nodes right because he's only linked you
from bad nodes and he only links inside
the great you basically all you know the
only incoming links he has are from are
from bad
so this is the other class of methods
are called content based content based
methods and basically you just look at
the page you know you try to analyze you
know have I seen this page before in the
web or how many words does it have how
many links does it have can I compress
the page is it really really easy to
compress maybe there's really no
information on the page and you
basically try to train the classifier
you know maybe just a linear classifier
or some kind of you know usually take
whatever off-the-shelf fancy machine
learning algorithm you have and just try
to train it on some features on the page
you know take take the page and just you
know boil down to some important
features that should help you find what
you know figure out if it's spam or not
and you know this is some standard
machine learning approach just tranq
crania classifier on some features
standard trick here you're not using
anything really using a hyperlink graph
maybe you'll use the hyperlinks to
generate some other features maybe
you'll say like you know how many feet
how many outgoing links you have that'll
be a feature but roughly this is
basically not you're not going to taking
advantage of the structure of the graph
it's apology of the graph so um what the
algorithm and I worked on is called
which we called it which we sort of
chose which sort of at midnight on the
submission deadline because I didn't we
couldn't think of anything better but it
basically you know I did the letters
work out correctly if you right which so
we want to take advantage of both
content and hyperlinks so we wanted to
have web spam identification through
both content and hyper makes want to use
both the link information and the
content of the pages and both of them
are valuable to some extent but we
wanted to use both of them so the kind
of key ingredients of this algorithm I'm
going to just give you a sort of an
overview right now is that we sort of
used a substandard support vector
machine framework this is kind of the
kernel method you know funny linear
crossfire support vector machines are
good because they're efficient I don't
know you may not know about SVM it's not
I'm not going to go into a Cheeto about
the the sort of mathematics but you know
it's sort of the efficient method you're
basically finding a hyperplane in some
kind of high dimensional space to
separate your data it reduces to some
some optimization problem it always
reduced to an optimization problem which
generally can be solved efficiently so
the we we had to add these slack
variables which is sort of I'm going to
try to try to describe that later what
would what is a slack variable named was
this this is a this is a very very
important feature of the algorithm
we use what's called Sorum work along
semi directed graph regularization we
graph regularization is sort of a new
trick that people have been using in the
and the machine learning world if you
happen to have some data points and you
also have some relationships in the data
point so you have you have two different
types of inputs you have both both the
but you know what features for a data
point let you know what the contents of
a page but you're also given some kind
of relationship structure like a graph
structure on your data you can take
advantage of the graph in the
regularization procedure now reg repeat
if you don't know recognization is it's
not just a such a big deal I'll give you
kind of a rough idea of what it is in a
second but graph organization lets you
kind of train a train a classifier train
some kind of a decision function which
should have should very sort of smoothly
along the graph it shouldn't change if
you if node a links to node B then there
you know there shouldn't be that much
difference between of that predicted
values between the two nodes so from the
last thing that I was sort of just
writing a final slide about and I don't
have much if you guys should asked with
this later if you're interested this is
that the the optimization procedure was
basically some sort of Newton like
optimization I don't know if you care
about that if you care about it asking
about it afterwards they'll probably put
a gloss over the optimization trick this
is this is very important of course for
for large scale like for what you know
people at Google do this is very
important you know the optimization
because we were all working on sort of
data sets that you know fit within one
computer and and if you're working to
gauge that really doesn't it you know
has to fit a like a I know distributed
harddrive you know it you you know
that's these kinds of questions are you
know how you had you optimize functions
that's that's a big question
so standard SVM framework is there a
laser pointer it doesn't matter if you
can find one so standard are same your
your you standard the support vector
machine what you do is you basically
minimize this cost function so you're
finding some some vector W so X X I is
your data point you have you know you
have L data points here and X is X I and
your data point and you minimize
basically how well your vector W
classified say Y is your label this is
going to be the its +1 if the if the
page with spam and -1 X pages not spam
you want wxi to match the sine of Y I so
this number is basically going to be
it's going to be 0 this this sub plus
means it's hinge loss don't watch me to
this function this is basically saying
how well this
this terms is how well did W classify
and how well did W classify X it just
just hinge function if you got you got
it right if you were your way off it
starts to increase and in machine
learning you learn about these things
it's not it's not too crucial the point
is this says how well did you classify
the data point this term says how how
big was the vector W so when you
minimize this function you want to say I
want to I want to minimize the error I
made in my training set plus some
regularization term and the
regularization term is basically saying
how complicated is your classifier you
don't want to have a complicated
classifier you want your class you want
you want your decision function to be
sort of simple you want to kind of be
smooth this is sort of a general theme
in learning when you're when you're
going to you have to control complexity
so that you don't over fit basically an
overfitting control if you make sure
that your classifier is smooth that
ensures that you want you want to fit
your data so the graph regular is SVM is
basically like like the standard SVM but
you add in a second term um second term
is this this graph regularization term
what you're doing here is you're you're
you're saying I want to I want to fit my
data well I want to have a simple
classifier but I also don't want my
function to very much along the graph
here is my set of edges no good okay um
he is my set of edges and I'm going to
look at all pairs of pairs of nodes I
and J or two nodes and what make sure
that my classification function wxi is
not very far away from w xj so that's
clear so what's happening here is this
is the predicted value of node i this is
the predicted value of node j and well
if i penalize for the squared difference
this is maybe like an agent weight some
kind of an edge weight if I penalize for
the difference squared that just ensures
that that I this is saying that I want
my classifier to not change if a links
to be I don't want wxi or you know if I
links to J I don't want wxi to be very
far away from WX James I'm going to
penalize for the difference in for those
you don't know about you know this
regularization this is the is the key
idea this of this part of the algorithm
the last thing is going to be the slack
variable so it turns out that you can't
you can't get good performance if you
just train on standard linear classifier
you need to actually say I'm not only
going to learn the the spammy city of a
given node by by train
linear classifier W times the features
which is a dot product in some weight
vector in the features that's what W is
it's the classifier I'm actually have to
learn some Zi for every note so Zi is
some value some protective value for
every node I had to actually throw that
into my my algorithm so my final output
when I predict the spam isset e for a
node for a node I that the final output
is gonna be W dot X I which is like hmm
what are the features telling me and Zi
says okay what do I just think about
this node let's say basin maybe where it
lies in the graph so you're learning w
DX I plus Zi that's the actual final
output that's what you're predicting as
the value on each node and again you
want you're going to regularize for the
wneud regularize disease three port your
regular is both of these things um and
in the end of the day you also graph
regularize the these predicted values
across the graph so I'm going to
regularize the the predicted value on
the graph this is the predictive value
for node J this is pretty good value for
node I and I sum this up over all IJ in
the graph this is this is graph
regularization it's the port vector
machine with graph regularization um so
yeah the terms that these are the terms
they're the new additional terms so it
turns out that this um I was using this
squared difference so I'm predicting I'm
trying to predict whether I know all
these things are spam nodes or not and
I'm and I want it to be smooth along the
graph the graph fertilisation says that
if you know I don't think that spam
nodes should link to non span those but
it turns out that if you just use
standard graph regularization it doesn't
actually take into account the
directionality right I said the
directionality is very very important
it's a very very crucial factor in how
you how you how you how you learn these
things so standard undirected graph
organization like I had written before
my head over here this is just the
difference squared difference squared
there's no directionality right that's
just if they're different
i I penalize what I should have is
something that tells me actually more
well what happens if you know if if if a
bad links to good that's okay that sort
of seems natural right I said that it's
Nina they might do that to try to fool
the try to fold the indexer but good
links to bad that's actually that's
that's a very bad thing right I really
don't want that so an alternative is
directed regularization where I I do
something which is kind of like a a
hinge squared you can call it what's
happening is that if if it's not it's
not going to be si my
this is the if sa is a pretty good spam
value of note ASB is pretty good spam
value build B and a links to be it's
going to be like it's not gonna be this
thing's squared it's me like the minimum
of this thing in zero squared so you
sort of drop if this thing is negative
you you don't you you you should say
it's zero
I don't know forget it maybe to be
positive maybe I got the direction wrong
but you see they're saying like if if a
is greater than B then you then you
penalize for the different square but if
B is greater than a you don't penalize
that's fine if if the if the spam
linking to non-spam that's not that's
okay spam leaking done on spam it's not
a problem
we don't P we don't apena lies for that
so intuitively you would think that this
would be much better because it does
have this directionality right the
directionality is very very important in
the problem turn that this was actually
false we we discovered here's a graph of
performance or algorithm I'm not going
to talk about sort of the algorithm and
general performance and metrics and
stuff in a little bit but high means
good and low means bad we're measuring
AUC which is area in the area under the
ROC curve doesn't matter if you don't
that is basically how well did you rank
the pages how old was your ranking of
spam is city turns out that on the on
the left side is directed regularization
and on the right side is undirected
regularization this is what I was
initially suggesting and I'm saying
shouldn't shouldn't be correct well then
I said let's let's try to change it and
use directed regular regularization
turns out that direct realization is
much worse than a director undirected
regularization but a little bit of
undirected regulation and mostly
directed organization is actually the
best thing to do you can see I'm
actually mixing the two here so I'm
actually considering one of them if I
consider you know I can think of a trade
off and I use either one or use the
other where I just use kind of a mix of
the two I can say you know alpha one
regulation plus 1 minus self another of
another regularization that's what this
alpha parameter is saying let's let's
say you use a trade-off between the two
turns out that this is a very very minor
trade if I just use a little bit of
undirected characterization that's
that's the optimal thing that's that's
the ideal the ideal from do this is
really surprising we actually didn't
figure this out into right at the end
and this seems to be very very very very
crucial part of the of the of the
albergue very reason why we have gotten
this good performance is because of this
fact right here so it turns out if you
just sort of semi direct the
regularization looks like this it's not
quite
full quadratic it's not quite a you know
quadratic on one side and then flat on
the other it's this kind of weird sort
of lopsided quadratic where you you
don't penalize a lot over here and you
penalize a lot of your penis penalize a
little bit over here but not very much
sorry penalize a lot over here and not
very much over here so this is I think I
used alpha equals two point to narrow I
think that often was alpha equals two
point one so it should actually a little
bit a little bit lower it should be that
bout down there so we actually recur
through this why didn't this you know
why didn't our intuition that you should
penalize for spam linking in one
direction you know just family kin one
direction why do you wanna penalize a
little bit the other direction what's
going on here but we spell out looking
at this and it turns out there seems to
be a lot of bad cases um the classic bad
case is this one you have a spam node
your shirt's spam it links to some node
and that node that you don't know that
the value of this the questionmark node
links to good nodes so spam no links to
unknown node links to three good nodes
so what do you do in this case well if
you're using undirected regularization
if you're if you're just trying to
basically pull the this issue a the spam
score only in the direction of sort of
them
so what's well I get confuse her all the
time so if he is said to be spam that's
perfectly consistent because spam links
to spam and spam also links the noms fan
there's no problem with that right if he
is not spam
well spam links to not spam that's
perfectly consistent and not spam all
links not spin that's perfectly
consistent you're not you're not
penalizing anything there so in fact
this guy can have a value of anything
right and in certain sunson there's a
lot of uncertainties where you just have
no idea what to do if you just use a
simple one directed regular
regularization this guy his value is
basically free to move and I think his
movie value just gets set to zero
because you don't have you don't have
any idea what you know you just give him
basically a default value so a lot of
guys were just getting you know a lot of
sort of via in certain cases we're just
getting sent to zero um and that's
that's bad you sort of want to say well
in this case look he's linking two three
good nodes and he's linked to from one
bad node I should probably take
publishing you know if it was the
opposite way there's three bad nodes and
one good note I should probably think
he's bad but this way I think I should
think he's good right
something very crucial here I mean I
actually do want to use the links to
good guys in the links from bad guys a
little bit I should use them a little
bit for spur for the sort of the the
cases where I can't make a decision
right this is this is the the harder
this is or the hardest case and here I
think the algorithm what's going to do
is say this guy's good it's gonna
predict he's good or he's gonna give him
a more of a goodness score than a bad
news score because he has more is more
outgoing friends that are good than
incoming friends that are bad yeah so so
I I just wrote this slide up like sort
of hat half-assed like when I was when I
was finishing it so I thought maybe
people would be interested in this the
the optimization was sort of sort of
Newton style Newton step style basically
you two we haven't tried this on large
data sets we will enjoy this and small
data sets but it is actually a pretty
fast method you have to do a nude method
the Newton optimization method you have
to do
Newton steps it's required or invert a
Hessian but you don't have to invert the
Hessian it has seen is this big matrix
remember you have tons of variables here
so you don't want to never pass in it
turns out you can you can play games to
get the Newton step done efficiently you
can use linear conjugate gradient to do
that and that requires every time you
want to do one Newton step if you do
roughly fifty or hundred passes over the
data which is you have to look at your
entire web graph and your entire feature
set for each for all the pages 50 times
for every single step and you need about
ten steps so you have to over your date
about 500 times do this and I mean we
don't I in terms of scale bill that we
don't know how if those numbers scale
correctly as you as you move up well
that's actually still kind of an open
question for us and that is a very
important open question so are we of
course you know we were inching how well
this thing works this is sort of an
innocent right now an inter algorithmic
comparison I told you that if you add
different features first a standard
trick for classification is SVM support
vector machine just just trade a linear
classifier just try to see if you can
you know put your put your your data
points in high dimensional space and
just sort of find some kind of a
separation plane between your data
points standard SVM trick if you do that
you get the blue curve this is by the
way how much data you get and this is
the performance in terms of AUC is how
well you rank the data
so the blue curve is just training SVM
simple you know first-year graduate
school trick or maybe even senior in
college or if you I don't I don't know
how if they keeps these things and
undergrad okay
now take that and I'll throw in graph
regularization now try to make final
lunar classifier but make but also
penalize it for a friend up firfer for
predicting you know across the graphic
puts it of predicting wrongly for
changing along the graph very rapidly
you want to be smooth along the graph so
the graph organization sort of it'll
wish I had a white board I can show you
where that is it oh I do have white
board you have graph organization comes
in to play when you're funding linear
classifier oh did I and you have like to
dig data clouds like one here and one
here and you know that this guy is good
smiley face and this guy is bad and if
you didn't let's say you have like some
graph this guy's pointing to all these
other nodes in here we have some big
complicated graph and some graph over
here same thing um oh did I do this
right oh no sorry should be yeah it
should be like sad face here a happy
face up here pirate
now if you just sort of train a
classifier without without using kind of
this unlabeled graph data that you have
you would probably do this right or did
I get it wrong maybe it's like that
right you just try to separate these
these two d-street 80 points vide you
sort of don't see these you're not using
the graph at all right you just have two
data points except we're like separated
like this but if used graph organization
you also say that the classifier
shouldn't cut through the graph too much
so actually would choose like this right
which shows the right fit right if I saw
two data clouds I have one data point
for cloud this guy's go to this guy's
bad I probably think that the rest of
these guys are also bad that's sort of
the trick with graphic so if you throw
in graph organization you do get a
slight performance particularly when you
went when the number of labeled nodes is
very very small which makes sense when
you have fewer and fewer labelled nodes
you um you you do a lot better the big
boost seems to come from slack variables
you have those slack variables in you
get a huge improvement in performance
and that seems to be the following it
seems like you can't really you know we
only have like 200 features and the
features were basically like based on
the content of the page the links thing
coming lengths outgoing links the
features don't tell you enough because
people can simply the spammers can just
copy the pages from other pages and they
can just take pages that look good and
make their own page out of them so
features don't really tell you enough to
make it to make a decision
these slack variables say well for for
each for each node I can also learn a
specific score specific spammy City
score independent of the features and
that that that that's freedom allows the
algo to do a lot better now of course by
doing that you may over fit your
solutions we have to regularize those
those those uh slack parts those slack
variables as well there's a trade-off
here you know you you give you algorithm
or freedom but at the same time it might
over fit but it's still nonetheless that
those slack variables are very very
important in a procedure you get a huge
boost performance from slack variables
now if you just use slack variables and
forget about features ignore features
you get the red curve and if you use
everything together slack variables
features graphitization everything you
get the black curve so that's the one
that's that one that we're kind of I'm
promoting here is the the the the we're
calling which is that top curve now it
seems like the features don't help very
much lower on and they're like this
shows that they hurt but I don't believe
those numbers you know that sort of 1%
of the data wasn't really enough data to
check that I think you basically do at
the same right we're at the bottom but
features give you a pretty pretty a
pretty decent boost not a major boost
the slight vary
seemed to do a lot more so um we took
part in a web spam challenge this web
spam challenge this summer they had two
tracks they they had a track for where
they gave you like the actual like
websites themselves that had tell us
that these are spam or not then they had
to track 2 which is just they give you
some some hidden data set where you
didn't if you didn't get to see the
actual you didn't get to see what the
actual pages look like you just got some
features for the pages and it was just a
graph no one's Rachel with pages work so
we took partners and it turns out we
actually won the second track of this
which is we took part in we we we tried
again on the track one data set we
actually would have won that if we took
part in it I'm always in it to brag but
of course you know in hindsight of
course it's much easier to beat people
once you've seen what the kind of the
benchmark is but but we were actually
pretty pretty hefty margin away from the
front from the leader so on this
particular competition we actually did
very well and you can actually look at
these are sort of I just showed you
graphs of all these different sort of
very variations um which is that is the
bottom one the the challenge winner got
point nine five six a you see on on the
on the yeah on that down the track one
data set we got point nine six three and
you can see we also do pretty well with
with very little data which i think is
pretty cool ten percent data you you you
you still do quite well compared to
different methods and that was also big
thing is that the data set was actually
pretty well labeled i mean the number of
actual labeled nodes in in the way a
full web graph is gonna be much smaller
than you know what we had we had you
know ten percent I think ten percent of
those were labeled which is not
realistic so I think that's basically
all I have to say that went quicker than
I expected it to but feel free to ask
questions and thanks to Alexander for
putting those animations together
they were they were kind of there's a
variety there was um there were some
content so basically they they took I
think it's some kind of a dictum he has
him like SVD of the the words in the
page and kind of SVD representation of
the words in the page they used also
some kind of link based features they
check to see what was like the length of
the words and the links I think they may
have also done like taking the words in
and the anchor tags the anchor what they
call the what what's between the a href
what's in between there its quotes
called anchor the anchors it's just they
did some kind of some kind of a low
dimensional feature ization of the add
the anchor tags they also said you know
how many how many words are in each page
it wasn't wasn't very very complicated
the features were relatively were
relatively simple they had also some
friction frequency vectors you know what
those are I could check I mean III been
if he wants to I look at that but today
there was a variety of different
features and they had a variety of
different things they they check you
know like how many links for coming in
and thinks rowing out it's a bit too
freely available day to say so
the grappling showed it seemed that the
percentages features only matter and
these they're equal
he said living cottage I make sense
pumpkin even if the Davis little bit off
they're still so we go back so you're
saying that features are useless down
here I mean we have two hundred features
and down here it's like you have like
two hundred labeled data points so you
have as many data points as features and
that's kind of like situation where like
it's very hard to really learn what's
going on in terms of the feature
representation um still up here you have
the entire graph so if I still tell you
that you know I've 10,000 nodes in a
graph but I can say what a hundred them
are 100 of them are you can still
propagate pretty well the information
from the graph the graph is still very
very rich right the graph tells you very
very tel you a lot um
the features I mean is it it features
are sort of uses I think unless you have
a lot of features once you have a lot of
data for good features and they don't
seem to help until basically you gotta
film they we're still improving as we go
as we get up here um that's the best
answer I can give you I don't really
know I mean I think this company has a
Cheryl nature of the problem you know I
mean the features are generally the page
contents are being created to look like
they're good pages unless some human
actually looks them and says this is
obviously trash this is obviously
nonsense so so um yeah I mean I think
that basically the kind of conclusion
here's the graph tells you everything
right the graph is so much more valuable
than the features um the features give
you a big boost right at the end there
but you know it's not clear that's
really know yeah well the blue pages
almost no link why I check this it does
happen actually but it's like us yeah
well unfortunately thousand that's all
about these blue pages yeah we didn't
good question actually I mean we
actually check this because I looked at
the labeled set and I think it was like
there may have been in terms of I looked
at all the links between the labeled
notes I think there may have been
something like 5,000 links and this is
all um get on the roughly on the order
5,000 links from from from bad pages to
good pages and maybe like a hundred
links from good pages too bad pages or
maybe 300 links from good pages to that
pages so I was like a you know factor of
20 difference between the you know the
direction of the of the links um and I
think honestly my sense is if you ever
link to a bad page in this our room
would basically say you're spam it
because that's just such powerful
powerful information you can't ignore
that information you can't say well
maybe he made a mistake you know me that
information tells you so much if
something's to a bad page it just told
you this person does not look
trustworthy right some cases where the
pages that many many good properties of
good page still look good
yeah I mean I mean I haven't really I
mean actually look at every single you
know page and see what the contents of
pages in the whether you know this guy
just made a mistake or not um I don't I
don't know world whether we were making
a lot of errors
I don't know if maybe we maybe that was
just you know maybe where we didn't we
were doing well because we were costing
a lot of pages is being bad maybe maybe
your algorithm list was just too
pessimistic I mean it's very possible
and I haven't looked closely enough to
see is this page you know did we do
where we're making our mistakes was it
because you know these good pages just
made mistakes and accidentally you know
did this link exchange link to a bad
page I wasn't we didn't check that I
mean is a way to identify pages that
their properties kind of porosity
wanting to get good medical care links
from good pages
then but they're out link seems to be
suspicious so in a way they're not
trustworthy but they are a good maybe
good con I yeah I mean I I don't know I
haven't looked at this is it that you're
asking an ancient question I mean I
haven't looked at the statistics we
shweet that's important to check but
doesn't happened very much that that
good page is linked to one page that you
don't know and then one page or links to
bad pages and that never happens why
would why would a you know why would
good pages link to a page it's like your
bad pages I mean sort of seems it seems
it's just encounter tutor for me these
things should happen very very rarely
that's that's the hope realistically
leasing should bear rarely happen
basically assumptions that I'm making it
those things shouldn't happen very often
and the hope is it I don't mean because
I'm I'm really am using the direction of
the links very heavily in the algorithm
so I don't have an answer your question
you can sure what should I bet a future
if it does happen really good
for those kinds of situation area
adversary that's trying to make a good
pitch
explaining it what's the spikes planet
again so if you had a labels like labels
and then if you could label it I see I
see I see cuz it could be like you know
you could be like me because in sums you
could be like the indexer of the bad
page link that's been baby like I got
you know a government agency Nexus fan
pages writing out a bad pagers where's
point state don't go to the site right I
mean you're writing that would put a
huge advantage that should help
performance right you would imagine that
if you could you could say don't trust
that link that link doesn't you know but
I mean how would you learn that how
would you find out what links that's
another whole nother learning problem
right how do you find out what what are
the links that were intended to be
that's your big that's true that's true
that's true yeah yeah I know that's a
good question like maybe you could do
like another layer like first you train
and see what links should be trusted you
know you find a waiting on the links and
then try to use that basically maybe try
to find the graph weights and end it in
you know learn learn their weights in
the graphs and then do another and then
basically do something like this just
try to train a classifier figuring out
which pages are spam that's actually
good idea and it's actually it's decent
idea and we did actually we didn't
actually take advantage of them
we didn't play games the weights too
much the weights for the the edge
weights are basically like log of the
number of links from this page to that
page or log of that relation that hosts
to that house or log of 1 plus a number
of links from that house to that host
you know we didn't play too many games
with with with with with the weights and
we probably should have I mean I you
know the trust rank algorithm they need
to have a probability distribution on
the on the outgoing weights so like some
of your after your outgoing weight
should should be equal to one and we
didn't we didn't do that at all I think
it we finally didn't work as well when
they tried it once and it didn't work as
well so
well a lot of work
ready
yeah I wonder I mean we actually enjoy
it you're asking was it was that your
question
III don't know because because this
directed graph is I was actually going
to point this out that it's almost seems
like if you know if you use a little bit
of sort of if you if you're penalizing a
little bit for the links that go
backwards and PETA lysing law for wings
to go forwards it's almost like saying
you know with probability alpha you you
go you follow a positive link of
probability one - I'll be follow a
backwards link that seems kind of
natural I mean it seems like that might
be that might and I don't understand the
random walk literature one ought to be
able to say if that's what that's that
well that's what we're doing I know that
our algorithm is very giving from that
because when you train and algorithm i
graph regularization with features it's
not clear if you can't you can't simply
say that training a regular eyes SVM
graph organizes ham it's some kind of
random walk because you have these
features that are there it's not just
you don't have just the nodes you have
features as well seems like you would
take around a walk look at the features
make a decision then it take walks to
take a you know I don't know how to how
to interpret in terms of lazy random
walks so
a lot a lot yeah
yeah if you just use slack variables it
would just be like trust rank I mean it
wouldn't be exactly but it would be
maybe along the same lines as your
method you know if a student
presentations majorly based on their
listen I'm not sure sir he doesn't use I
don't know if trusting uses my interests
was they don't use the bad notes they
only use the good nodes so goodness
propagates forward they don't have so
yeah they like GB I think you heard of
GPU the good the bad and ugly guy that
Yahoo basically that you should like we
should do like a trust rank which like
how much like trust propagates out of
the good nodes and then you should and
you transferring which is how much to
propagate backwards and and I know that
we beat them but nothing that what I
mean I think we tried that and we did
better than that but but but that says
roughly sort of similar idea I mean we
might be doing something similar to that
with some sort of you know you take
advantage of features as well I mean
your your point is well-taken it's a
very good point um you can talk
afterward this one as well about that so
yeah in common stance
especially on
very pretty common fund
say
as several thousand
I see I see just a few 75
I see sir saying that Yanni the point is
you should definitely not include
comment links inside of your algorithm
yeah I imagine it that's a good point
actually like you know that seems like a
obviously the big problem comics famines
diligence is so pervasive right now I
haven't even played around with that
kind of data so I don't know and other
questions</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>