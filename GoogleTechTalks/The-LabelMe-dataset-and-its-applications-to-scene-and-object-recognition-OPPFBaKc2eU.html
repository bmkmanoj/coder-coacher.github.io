<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>The LabelMe dataset and its applications to scene and object recognition | Coder Coacher - Coaching Coders</title><meta content="The LabelMe dataset and its applications to scene and object recognition - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>The LabelMe dataset and its applications to scene and object recognition</b></h2><h5 class="post__date">2008-08-16</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/OPPFBaKc2eU" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">it's a great pleasure to introduce and
welcome bill Freeman great thanks
it's a real pleasure to be here thank
you for asking me so when I was a kid
you go to a movie theater there would
always be a short before the movie like
you see Bambi and then you see the main
thing you know that's really fun we
don't really do that anymore so I
thought it would maybe bring that back
and have a little short before the main
talk this is because I was preparing
this talk a night and I saw this I love
to show that to them so I think I will
so this is a short before the main thing
and this is creating and exploring a
large photo realistic virtual space with
Joseph's Civic used to be a postdoc with
me and a grad student mind building
anikanova and Antonio trauma who was a
faculty colleague Shah of Iran who works
at Adobe and me so here's the idea you
want to start from a picture and just be
able to drive just go anywhere from that
picture and what we're gonna do is use
the world's other pictures to help us do
that so we want to make a little photo
realistic virtual space it doesn't
really exist anywhere but we're gonna go
look at the world's images and see
suppose you started there and you wanted
to go left what's a good picture to put
there and let's let's blend it in
properly and seamlessly and just put it
there and then we can make a sort of
large photorealistic virtual space you
can just start from a photo and just
drive go forward back or turn around and
explore this large virtual space so well
first of all why would you want to do
this well it might be a nice hook for
photo sharing websites you could start
from your picture and drive off and see
where it takes you go to somebody else's
picture and it might be useful for these
large virtual worlds like Second Life
it's just a sort of infinitely large
photo realistic virtual world and maybe
for computer games maybe for movies and
ads so I let's see okay so here's what
we did so I got to say I got a lower
expectation expectations a little bit
the results are not going to be
perfectly seamless there some you can
see artifacts but here's the approach we
took we we grabbed 66 million images off
of Flickr and if you just do them
naively mminton to crazy images but so
we grouped them somatically and we
wanted to stay within theme so we we
built a classifier to find classify
outdoor scenes or to classify bar scenes
or to classify these different
categories and then once we were within
the category then it worked pretty well
to let us drive around in this virtual
world so how are we going to match up
images there's you know you're not going
to be able to find a picture which
starting here would give you a
photorealistic match to these things so
we're going to lighten up a little bit
and let it have a match in terms of
orientation structure so we use this so
called gist representation I'll talk
about it in the second talk as well
where you basically break the image into
blocks and look at oriented energy at
different spatial scales and different
orientations across these block put that
into a long vector and that's your
representation for the image and that's
what you're going to match on and by the
way that's sort of a consistent theme in
computer vision now everyone's
discovered that eye matching image
intensities that's not good matching
orientations ah that's good so there's a
lot of nice flexibility that gives you
and as I mentioned we picked these serve
semantically consistent themes and we
built a classifier to serve pre-process
these six million to pull out one or two
hundred thousand within each scene so we
have a skyline stimulus it's a theme
landscape theme Street theme and if you
stick within if you only grab pictures
within that theme you can do pretty well
if you if you start mixing themes then
you kind of tend to go off in a hyper
space and then we we have some basic
motions that we allow our joystick
operator to have you can go forward you
can rotate you can go left or right and
you know you kind of do what you to make
this happen you sort of do what you
would expect to move the camera you go
and look for pictures
that match this one in that part of the
frame of that picture so let's see
camera and again we match not directly
on pixel intensities but uncommon eight
actually a combination of color
intensities and this orientation
structure structure and then you have to
play games to make it look good you
can't just plug it in you have to wiggle
it around to find the best fit then you
have to use dynamic programming to pick
the best cut and then you have to use
Poisson blending to put it all together
to again to try to minimize the
artifacts I'm not quite clear what all
your backgrounds are it's plus I'm
blending a term that people know anyway
it's a really cute idea the idea that
the graphics folks have come up with
which is to say if you want to paste two
pictures together seamlessly if you just
put the overlapped at pixel intensities
you're going to see the cut between them
you can't really hide that well but if
you just cut and paste the gradient
values and then integrate everything up
together you don't you don't include the
gradients for that cut and and and you
do a great job of hiding the the scene
between images so so we use that
approach and then okay so rotation is
just a slight twist on that you instead
of looking for a match there you look
for a match that if it's a rotated image
that would be there and and then zoom is
kind of the obvious thing you you pick a
find an image which is going to match
there do the same thing so so let me
show you some still images first and
then I'll show you the video so if you
just start from this image and do the
operations I described you can make this
little infinite panorama and well just
to point out the artifacts this doesn't
look like good one to me but the other
ones look pretty reasonable and here's
where the here's behind the scenes these
are all the different pictures
were actually used to make this panorama
and and then again we had to use dynamic
programming to find a scene that that
broke the fewest gradients as it went
from top to bottom in the image and then
the same story with with with zooming
and so you can go on the street and just
start driving forward just keep on
driving and it keeps on finding pictures
with Fitch there and here's the the
seams those they put them in so okay so
here again instill before I go the video
here's some panoramas that we made from
this little virtual world that doesn't
exist anywhere but maybe it'd be useful
to have in your computer for second life
or something
okay play the video and as I said you
know there are some artifacts I'm not
really happy with it but it's it's a fun
start and I I think there's no reason
you just can't do it better
so so let's now if you see the animated
version version of what I've been
telling you about we start from a street
scene and just keep on driving forward
and it keeps on you know finding image
does it fit in there and putting them in
and and and you know where you go now
let's let's rotate
and you know you can tune this depending
on its all it really depends on the set
of images you're searching over to blend
in and then finally it's just to
translation again so this one these are
these are all and I guess in the city
category and you know skylines can just
kind of keep on going forever
so you can also place the images you
know sort of make a little explore this
virtual world by putting the images all
where there would be here's some more
driving this one is the more I look at
it the more I'm bothered by the
artifacts but anyway okay so let's let's
explore famous sights you can start from
this picture and then just go off to the
right what's there off to the right from
the Hollywood sign well you know a lot
of interesting things
oh yes here's a
so here from there this is yeah we end
up at a famous site and you might wonder
you know what
what is off to the right from the XP
screensaver okay another thing you can
do um you can make an image taxi you say
well here's what I'm starting from here
is the image I'd like to get to could
you drive me from here to there please
and and so it you know again does
dynamic road when you and finds the best
set of forwards rights and and and pans
to get you from one image to the other
and again this type of thing might be
useful for a photo sharing site okay so
that was the short before the main
feature are there questions about that
ready right and but that that's the
manual oh sorry the repression was are
you manually selecting these directions
that you go in and yes but I mean that's
the part that you want to have be
interactive so you want to let the
person do that except I should say for
the image taxi we weren't manually
selecting it that was the user selected
the start and the stop and then the
program found the taxi path that would
take you from one to the other
okay you can go out and get popcorn and
here's the next the main feature so
let's see so my lab does a number of
things related to image search in
collaboration with and I worked closely
with Antonio Tarawa who used to be in my
lab but now he's a professor at MIT with
his own lab but he and Rob Ferguson I
worked on our collection of finding 80
million images and what can we do with
them but I understood that Rob talked
about that when he came to Google
recently so I'm talking about a
different related sort of web search for
images project that we called label me
and then a use of label me so and I
should say this is the these two pieces
are part of the thesis work of student
of mine who just graduated Brian Russell
and then also the co advisor and really
person had a lot of intellectual input
into it was Antonio Tarawa so I want to
credit them I'm behind both of them in
the author list on this work so and what
I want to talk about two pieces one is
label me which is this online labeled
image set of labeled images and then the
second is an application of that labeled
training set to use it for object
recognition
so so label me is an online database of
labeled images so what's a labeled image
you get an image and you draw a segment
around some object and you label the
segment and that's one polygon label for
one image obviously for object
recognition you'd like to have a big
data set of those and we wanted to let
the world help us do that so we made a
web-based annotation tool and anyone in
the world can go and label images for us
and they do then that's nice
so you know talk about coals to
Newcastle the purpose of the slide is to
say the importance of having large data
sets for machine learning but I think
you guys understand that you people
so for language you know a million
samples is good for character
recognition data sets on the order of
you know 10,000 samples for object
recognition the data sets are much
smaller would you really like to make
them much bigger and you want the okay
what would the deal computer vision
labeled training set it would have you
know an infinite number of object
classes say thirty thousand it would
have many many examples of those and
many objects per scene and many examples
of each object in many different scene
categories so to to try to strive toward
these goals we made this online tool and
we supply the images like these and
these and then the users go online and
and label rectangles label polygons and
assign names to them so here's a
screenshot and here's a sort of typical
labeling for tree and a car and a wheel
and windshield and so forth
and if you google label me you'll find
it and I assume that you're very careful
about Google becoming a an
english-language word so I should say if
you google brand search engine the label
me you will find in so these are
statistics are old but the right orders
of magnitude we have about two hundred
thousand images available for labeling
and about two hundred thousand object
annotations obviously many images have
more than one annotation and at least 41
thousand images with at least one
labelled object and also we have sort of
seen categories as well so if you if you
go to the web page and you can play with
it if you type in kitchen you'll it'll
give you less examples of kitchen scenes
and then labelled versions of them or
streets same thing and here's some
labelings of individual objects here's
balcony and then examples of the image
crop that it came from and the
silhouette showing what part of that
image crop was labeled as balcony here's
chair again the image crop with the
image of the chair and then the
silhouette that the user defined as
having this label chair so here's a
little all at once view of many of the
different categories we have this is a
registered average of the images within
each category you can see we have
orientation annotations on cars
different orientations faces and and
indexed by orientation of the face
so just some statistics the this is the
number of pollen a number of instances
that are labeled in the dataset as a
function of what for the different rank
for the topmost and and going down to
the thousandth most frequently labeled
object category so person is is at the
top and then car head tree window and so
forth and you can see the distribution
on this log log scale of how many
different are two categories we have
labeled yeah can be labeled
oh it's labeled each image is labeled by
more than one person can be labeled by
more than one person typically they
don't label the same polygon so when
when the image comes up it's got the
progress so far on it displayed and then
the new user can can modify those edits
or add add more and then just some
statistics so this tells what percentage
of the image is labeled for the ones
that are labeled and you know most of
the image have a small fraction of the
image labeled less than ten percent
although some have every single pixel
labeled and the number of objects per
image most just have one object label
per image but it can go as high as you
know higher than 15 and here's our
progress as a function of time we went
online
three years ago in the summer and this
is a snapshot from about well from a
while ago it's it's a bit beyond that
now I haven't updated this slide and
then here's our derivative the number of
new descriptions appearing as a function
of time so this is sort of as you'd
expect it the the common words and
common things were were described first
and then now we only have occasional new
categories coming online and so that you
know there a lot of issues like what are
you doing letting the whole public label
your data set for you isn't it going to
cause a lot of noise and problem
and so I just want to talk about that a
little bit so first let's talk about the
quality of the labeling that we get from
this totally volunteer set of people and
also I should I should say it just to
sort of contrast it from like LaVon ons
approaches it's fun but it's not like
the most fun game you've ever done just
to label these things you know it's
really I don't know I haven't really
done much of it I I'm not sure you know
what kind of person really enjoys doing
it but there are people who do it for us
so regarding the quality of the labeling
one kind of rough measure for what the
quality of a labeling is is how many
line segments there are in the polygon
to describe something so in general the
more segments there are the the more
care was taken to describe that outline
so we can we gathered statistics on how
many segments were in each polygon and
and then here's a picture repertory up
so here's the the mean and then the the
25 percentile in 75 percentile polygon
counts for these different categories
and so you can see I mean I don't know
what you're taking messages when you
look at it when I look at this I think
well it's pretty good you know on
average people are doing pretty
reasonable silhouettes and you can look
at many of these and figure out what it
is for at least 25 percent of the time
people are really doing seriously
serious seriously careful careful
labeling for us and then even the bad
ones I you know they they could be
useful and then here's some of the
extreme examples you know the people who
really took care so so this is you know
these are very elaborate and then we
also have extreme examples the other way
people who did not take so much care and
then we do get a certain amount of you
know let's just kick the tire on a
system see what happens and testing and
other testing and you know
just creative testing so but we can find
these things and weed them out so it
doesn't take much effort to just quickly
verify what people have done and we do
that before we put it into the data so
another concern is is you know we made
this choice to not provide a menu of
object categories for people to choose
but rather to let them enter a text
string for what the object is they
labeled and so you win and lose with
that you
we don't bias their descriptions towards
some predetermined set of objects we
think ought to be in these images so
that's good on the other hand you have a
certain variability in the object
category descriptions which you may not
want and so to address that we use word
net to help us determine what multiple
labelings might actually be the same
superior category for a given word so is
everyone familiar with word net I'll
take Sam to be the proxy for everyone
cuz he nodded his head so that's okay
right so so you can starting from a
given noun you can go up and down in the
hierarchy of like if you put in a child
you can find out that it's an instance
of a person which is an instance of you
know a living thing and so forth and go
so up and down the tree and so we can
use this to take people's multiple
different labelings and assign them to
the same node in a tree and we do that
to help to help address the the
variability of people's labels so so if
you so using word annette then we can
get a label that we come up with like
person and find all these different
labelings that people made the that
should be in the same category as person
and similarly with car we can go and and
find out that taxi in some sense is an
instance of a car and put it in the same
category for car sorry yes you still can
right yes yes we can search for both
both ways we can search for the
individual labels that people had and
also we can search for these categories
so here's examples of the different
categories using word net tool food
plant animal and then the retrieval so
we can get within that category based on
the individual labels of people so under
tool we can pull out tool box knife
scissors and corkscrew and so forth so
this is helpful to address the many
different words that some people put on
their labelings
and then I have some sighs he's a little
bit outdated comparing our data set with
others it's really certainly at the time
these snapshots were made you know it
sort of beats the others this the sort
of collaborative effort by the by the
web beats the individual databases that
people have made so in terms of number
of categories label Mia's here we and a
number of objects per image number
categories is this access number of
objects versus this one and we have this
range and others have arranged with far
fewer object categories and again this
is because we took advantage of all the
different categories of people labeled
in our images and then in terms of the
number of labeled instances and the
percentage of the image that they occupy
we're at this region of the scattergram
C and again for a number of object
categories a number of instances of each
category we're on this blue line which
is in the right direction relative to
these other data sets and similarly with
number of control points yeah please
pick a number k and take the K okay so I
spot-checking we tried estimate for
various values of K and try and match
that hmm so of course the problem is
that these are really late means liquor
for the tags for example yeah right
right so it's right right I guess it's
some sort of the way I would see what
you're saying is that you know these are
all these statuses pretty much if you
put in the word for something you're
going to get a picture of that thing and
then whereas Flickr just creams us in
terms of the size of its data set but
it's very inaccurate in terms of what
you'll the precision what you'll pull
out yeah and so you want to take Flickr
and put on this chart somehow yeah okay
that's a good thought
let's see Kevin Murphy keeps pointers to
image labeled image databases and you
might want to look at that if you're
interested in using some of these so so
that's kind of the data set itself and
then now let's go and do data mining on
our data set so one thing that's of
interest is a lot of the segments that
people label are overlapping and what
does that mean well it could mean one
thing is behind the other it could mean
one thing is a part of the other and so
you can use these overlap segments to
pull out what are parts of other objects
and so we come up with an overlap index
for what fraction of a polygon is
complete
the within another polygon and if you
find many instances of one label always
being inside of another label you might
decide oh that's a part of that other
thing so we can pull this out from the
labelings that people have made for us
and build object part hierarchies just
from their labelings even though we
didn't explicitly ask them for these so
you might imagine what are going to be
the parts of a car from side car from
the rear or a sky you know sky had parts
but it does so car side has parts that
we in effect discover from the labelings
that people have done for us
wheel car door car window and tire okay
car rear has taillight wheel license
plate car window and mirror as its parts
and the sky has the Sun as rainbow has
the moon has clouds as bird as parts of
the sky so that kind of falls out at the
labelings that people do and and so
here's another display of that mountain
and its parts
building and its parts including air
conditioner and a spider patio awning
text marquee pillar balcony a person has
kind of an obvious set of parts a face
head hand hair eye mouth nose neck and
this guy I mentioned here's a here cars
with their parts indicated and people
with their parts indicated you can also
pull out depth ordering okay so here's a
little puzzle suppose you have two
polygons and the intersect how do you
figure out which ones in front of the
other
any ideas yeah so you count the number
of segments that each one has in the
intersection when it's inside the other
one and it you're exactly right I didn't
hear who said it thankthank you didn't
see you said it the one with the more
polygons is the one that's in front and
so we it's a heuristic that almost
always works and and we can use that to
to again even though people didn't
explicitly do it we can pull out a depth
ordering from these labelled images so
here's here's an image that was labeled
and here's an ordering of the objects
that were labeled in depth and similarly
for this and you know it works pretty
well and let's see so okay alright
here's this one is a failure example of
that heuristic but in general it works
it's pretty reliable yes
right well actually no I mean I think
the service sumption underlying this is
that the people put down polygons at a
sort of fixed rate as a function of the
variability of the boundary and if you
make that assumption then this heuristic
would hold I mean well the answer is yes
the people's habits do enter into this
and it's it's a little bit a product of
the way the world works a little bit of
product way of the way humans work when
they label the world but I mean I think
I think you can make a mathematical
assumption about how many how frequently
they change straight lines as a function
of the object boundary and and you would
arrive at this heuristic for how you
should label what's in front listen back
well people vary is what they do I mean
you saw from those different 75 in 25%
points on the number of polygons people
have a real spectrum of the way they
label I mean I think the bottom line is
yes people's habits are folded into this
but but you can try to pull out things
that statistically do the right thing
oh then another thing you can pull out
of this is where do objects appear in
images so now this of course relies on
the fact that our images are taken by a
photographer they're not
we don't just toss a camera up in the
air and click at a certain random
instant and because these were
photographed by people who put the
ground plane in a particular place
usually and Center things on the objects
of looking at you get this fairly
reliable distribution of positions of
objects within the frame and you can use
that for identifying objects in in some
new image these are priors on where
things should appear so that's the piece
I want to say about this labeled dataset
and of course it's free for everyone and
you might enjoy labeling peace try it
out are the questions about before I
move to the last part
okay yeah number of classes it looks
like once you pass the two hundred five
hundred classes we only had very few
that's yeah that's true I think that's
true yeah I I I think that's sure so how
much you use this label dataset well you
could build an object detector that
would take these you know look at AB
local bounding box that bounds the label
of something and build a an appearance
based detector for those objects that's
a very natural thing to do we've done
that a much more interesting thing to do
is to exploit the regularity of scenes
to help to help identify objects as well
and so what I'm gonna do now so tell you
the the sort of research theme of my
colleague Antonio turrialba who's really
gotten a computer vision world psyched
about this approach and kinda here's one
point of view of describing it is that
nature is kind to us I mean what men
nature's kind to us in many ways at a
pixel level nature is kind to us when
you look at a pixel it's not just a
random collection of dots it's it's as a
loud structure and images look very
different from some you know from just
random Gaussian noise and that's nice
and people have looked at that and used
that for a long time
another way that Nature has been nice to
us is in terms of the scene itself the
whole picture itself if you look at a
scene it's not just a random collection
of objects and and pixel clumps in the
world it has a structure at the at the
whole picture level itself as well and
that structure you could call context
and that we as humans use it all the
time in recognizing objects and where
the field is now beginning to make use
of that to recognize
by computer as well so you know if you
imagine walking down a sidewalk you can
sort of picture what objects you're
going to see and if you all wrote them
down I bet you would come up with a list
that was very similar from one to the
other what you expect to see as you're
walking down a sidewalk so so that kind
of regularity can help us recognize
objects because we know what is likely
to be there so again we have those
priors just from the label me data set
and Antonio has done previous work at
trying to estimate where objects are
likely to be based on the entire picture
here's another sort of a demonstration
of the fact that you yourselves use
context when you recognize objects okay
let's have a little poll all right what
object is that okay what object is that
excellent now what is so cool about this
and this is just a very clever demo that
Antonio made those are the same pixel
values they're just rotated one from the
other and so what does that mean so you
weren't using your local feature
detectors to figure out that that was a
car or a person you were looking at the
whole picture and you were saying okay
building roads something vertical there
probably a person you know and then you
are so yeah and then a horizontal over
there well as probably car so so you
were using all this other information to
do your object detection on this thing
here because the local evidence wouldn't
give it to you it's the same as the
local evidence for that other object and
so that's kind of the new one of the new
directions that computer vision is
taking sort of led by Antonio is to take
advantage of all this information here
as we try to estimate what's there and
that's what we're gonna do with this
label me dataset here's another example
I don't know have any of you seen this
before okay okay all right your house
anyway so here's our colleague Rob
Fergus and you know you can all look at
this and you make up a story about
what's going on even though the visual
evidence is really rather
impoverished pretty low res that's
because you're so smart but you're
actually too smart because you got it
wrong in this case it's cleverly cooked
up to not satisfy those regularities
that you assumed when you interpret
objects and so you know this is it's
it's a different interpretation than
what you were making you know he
programs trash it's got these beer cans
but that just shows you how much you're
taking advantage of those regularities
which usually are correct when you
interpret the visual world so that's
what we want to do here for the last 10
minutes of this talk here's the and and
again I you know I'm presenting this
work but I just got to give credit to
Brian Russell and on Tonio Tarawa
because this is really their baby taking
image sorry you take an image and you
you go into the labeling database and
find the other times you saw something
that looked kind of like that like the
whole image so again this is taking
advantage of the world being nice to us
not just at a local patch level but at
the entire picture level I think there's
a similar scene in there somewhere and I
want to find examples of those similar
scenes and so you go pull them out and
find them now we've got a labeled data
set of similar scenes with objects
labeled their positions labeled and
they're all similar to this one and and
so then we're going to use that to pull
out what the objects in the scene are so
we're going to take advantage of we also
have a kind of bottom-up you know patch
based detector in here as well but we
explicitly take advantage of this
contextual information that the humans
are so good at using so just to kind of
go through at the top level what the
pieces are so you might ask how do you
find similar scenes you know pixel
values there they might not work you
might not find something with the same
color blue and so forth so again we're
going to go to this direction that
people in computer vision have found so
useful which is to look at their
orientation structure of the thing and
so we're going to take this can convert
it into a 150 dimensional object that
measure describes orientation as
functional position within this thing
and look for other images which have
similar orientation sort of texture
restructure and now turns that doesn't
always work so so instead of finding
just the one we're going to take these
guys and cluster them because we pull
out not just the desired cluster which
matches this one we've also pull out
some outliers so we're gonna take our
retrieval set and cluster them based on
their labeling because now once we're in
this world we've got all this extra
information we got the labels polygons
so we're gonna cluster them based on
their labeling and they're labelled
values and then and then with this set
and this original image we're going to
find the cluster and the object values
which best explain our input image so
this is a way to take advantage of again
both local appearance appearance based
matching and also this seen context so
let me just go through this in more
detail so again to do the matching we
want to rely on this sort of
squinty-eyed view of the world the sort
of texture as a function of position
view which is this gist and so you
measure the energy strengths the
contrast strength at eight different
orientations and at four different
spatial scales and at sixteen different
positions and string it all into a big
vector and that's your low D
representation of this image now this
kind of tries to find this sweet spot
it's high dimensional enough to explain
a lot of different images and to be
somewhat discriminative about one scene
versus another yet at the same time it's
low dimensional enough that you can hope
to find good matches in there and hope
to generalize well whereas pixel
intensities don't really satisfy that so
let's say this is our input image here's
what we pull back pull out I don't know
if you I don't know how well you can see
these but you know a lot of these are
pretty good matches just and in terms of
things with the same objects around
there these are Street scenes and okay
this one has a person and this one
there's no well there's people there but
you can get a good good clue as to what
you should see in this image by looking
all its friends that were pulled out of
the lailanie dataset and again all these
ones now have labels on them so it's
going to be help us to interpret that
input image here's another example sort
of office scene and here's what it pulls
out now it's not always right here's two
plates on a kitchen table but you know
but many of these are correct and and so
you can you can ask the question how
well is this representation at telling
you what the category of the picture is
that you're looking at and so people
have done this this is a little subfield
and computer vision not called object
recognition but called scene recognition
what's the what's the general scene that
you're looking at so this one would be
conference room scene out there would be
you know patio senior something so so
here's a list of sixteen candidate
scenes and percent correct and here's
different representations used in a sort
of nearest neighbor matching I think
that was it from a labeled training set
how well does the representation do what
helped that led letting you find let me
find the correct category of a scene
just by matching a labeled training set
and the gist does as well as as as any
so we'll use that and then as I
mentioned they're not all good so here's
another office scene this is like this
is a student oficina your offices are
cleaner than this but um anyway here's
what it pulls out and well you notice
there's you know so some of them are
good let's see these are the good ones
circled and then the good meanings good
means basically other examples of the
same general scene and then there's some
bad ones and bad means these just really
don't have the same objects in them at
all even though their orientation
structure it's a function of position
happen to match pretty well the
orientation structure as a function of
position of the other one yes
look at this I don't think right well
that's yeah I mean that's that's kind of
the that's the part of this that I find
cute actually is that that there is all
this additional regularity in the world
that we don't really notice that that
lets you take advantage of the fact then
in our database for one reason or
another that a lot of photos of things
with these kind of say mop objects in
the same kind of places and I agree that
there's going to be a lot more
variability than you would expect from a
you know European Street scenes or
things like that but context is powerful
and we use it all the time and I think
and that's as we're trying to trying to
tap into and I think you're I get made
preface your point is that it's not
going to be as reliable as context say
on a building from one window to the
next and that that's true yeah yeah
right so the important thing is to know
how well you know to know what the the
variances are to know how well your how
powerful your inference is or not so so
this isn't the only thing we use here to
identify the objects we're also going to
use a local appearance based detector as
well okay so let me go through this
quickly the the idea is in our retrieval
set we get some that are good some are
not good so to try to make this somewhat
robust let's cluster the ones that we
have come back and again we're going to
cluster based on the labels of the
objects that
come back and and look for consistent
sets of those
so we built a probabilistic graphical
model to try to describe these
relationships there's a scene variable
that influences what objects are labeled
in the scene and where what those
positions of those labeled of the
bounding boxes of those objects are and
so given any retrieval set we try to fit
this model to it and we have a prior on
the number of different categories that
were going to come up with and we use
this to try to to break our retrieval
set into some small number of categories
sort of a simpler way to do this you
could just take the labels of the
retreat objects and their bounding boxes
and try to do k-means clustering on it
for example but this is a somewhat more
sophisticated approach to trying to get
a similar end and so let's look at the
clustering we get then again here's the
full retrieval set for this image and by
doing inference in this graphical model
based on the the labels and the
positions of the retrieval set labelings
we find that the things which have the
same kind of squinty-eyed appearance as
this one fall into five different
clusters based on what their labelings
are and what their positions of those
labelled objects are here's cluster one
which again it's clustered based on the
similarity of the objects within this
cluster it has a lot of monitoring this
is kind of this is the right cluster for
this one it has the kind of monitors
office scene
here's another cluster that came back
that looked kind of like this one in
terms of very very loose visual
appearance but has a whole different set
of objects so this was assigned to a
different cluster this is kind of
close-ups of people and that was
assigned to its own cluster and then
similarly so then let's see and and so
so here's kind of what you pull back
you'll get a number of different
clusters then how many times each
cluster shows up in the retrieval set
you get those images and each image has
a set of objects and a set of
probabilities for each one of those
objects that it shows up in an image
from that cluster so if someone told you
which cluster this image was really in
this this would be your prior
probability and what objects ought to be
there so now we have a much simpler
problem with just finding which cluster
is this and then also tells you where
those objects would be so you've pulled
out a whole lot from this kind of from
two things from the fact we have a label
data set and then also from the sort
these kind of regularities of the world
that that we really do have a lot of
pictures that are kind of of the same
kind of thing is that in our data set
and so they have similar objects and
somewhat similar positions and and again
this is the same trying to tap into the
same thing that let you all decide that
that blob at one orientation in one
position was a car and a different
orientation in position was a person so
the same stuff we're trying to take
advantage of so just another picture of
the the same thing with a different
input image here's another image and the
different clusters that we put the
retrieval set into for this one so again
this I think would be the correct
cluster you would say for this image it
has similar objects similar positions
and here are some other clusters this
this one image was assigned to its own
cluster yeah
that case when you're on retrieving it's
also going to be bigger cluster of cars
even if see well I think that yeah right
well so so this Kansas two entity so
we're talking about here one is the kind
of scene and one is the object within a
scene either one of them being rare is
gonna hurt you definitely III think I
mean after we talk about later I think
the prior statistics are sort of taken
care of
naturally by this graphical model but we
can talk about that afterwards so we
let's see did this okay so alright so
and we also get the position so so
here's the procedure get a new image in
and you want to label all the objects in
it you go find this just representation
find a set of similar images to it from
the data set and then we cluster them
find their objects now we have a prior
on where the objects are going to be for
the each of these different clusters and
what the objects are now we have to go
finally and integrate that with we do
indeed have a local appearance detector
that we run on the image as well so we
have two sources of information as to
what objects are going to be here one is
just kind of search around the patch and
find if it looks like a keyboard and the
other is ask what kind of scene we're in
and might that have a keyboard and where
would it be
and so we combine those two in a final
graphical model here and these are our
observed variables this is the objects
in our that we're labeled by people in
our retrieval set their positions and
these are the sort of an SVM local
appearance detector outputs and then the
hidden variables we're trying to explain
are the whether or not these objects are
there and what kind of scene we're
looking at
and we use Gibbs sampling to do
inference based on the observations of
the shaded variables and let's see well
first of all let me show you something
cute which is to say that we get more
power from the context part of it than
from the local appearance detector part
of it so you can plot you can unplug
each component separately and make a
detector that only uses this local
appearance measure for objects again
based on training based on our outlines
for the objects and you can also unplug
that and plug in this context part which
tells you what objects ought to be there
and look at the compare the areas under
the ROC for each of the different
methods and for for most of the objects
you have a higher area under the ROC
curve if you just use the this kind of
scene context than if you actually use
the local detector it doesn't say you
should throw away the local detector but
it just says don't forget this other
part yeah pretty much that's what this
does so we do we take a bounding box and
we go step by step through the whole
image and I they must be of different
scales as well and find a good match so
that's pretty similar to what you're
describing
oh just raw pixels yeah okay you're
right there's yeah
I see and sort of also what you're
saying is in a in addition or instead of
these features that you might have which
happened to be a little just features on
a local scale why not just use pixel
values as a feature as well that's sort
of we I mean yeah I mean there other
things fold in there then you're going
to lose if your monitors has a different
color than the other one that range that
but but yeah let me close up soon so put
all together do the inference what do
you get well you get reasonable
detection so you find these bounding box
locations here's the input images and
here are the detected objects and where
they were expected to be so let's see
how is it doing it's doing this is just
the bounding box so it's not got the
it's not a political description it's a
bounding box of where this thing might
be in many cases to get it to get you're
right here's a funny one this is a
signal which I'm pretty sure this is the
signal and that's the window so it
actually missed but because it has this
prior on where signals are in Street
scenes that look like this you know it
did it it knew there should be a signal
there a traffic light and it so that's
obviously coming from the sort of
top-down context inference as opposed to
the bottom-up inference think of this
one I don't know if this is gonna be too
small to go through this is from the
nips paper itself which was sort of
nicer than the previous ones that are
from the thesis so let's see top are the
input images and then B are the
detection using again that's both this
top-down this bottom-up method so
building cars and so forth are labeled
if you unplug the context and just rely
on our local detectors alone this is
this is the labeling you get for those
images and there are a lot of false
detection zhh which the sort of context
helps suppress then this is the same for
another building and then here's a whole
row just
bloopers so here's the sky I think
there's a I'm sorry I can't read this
here but this is probably like keyboard
or something anyway you know we gonna
Rob sometimes but I think I want to stop
I guess say there's this is sort of a
direction that the field is heading in
and really it's been pioneered by
antonio de alba my lab has done works
often in collaboration with him
Alyosha F Proust and Derrick home that
seem you have also embraced this
approach and are using it and it's a
direction that people are going in you
can just by the fact that there's this
notion that the world's images are not
that disparate asset and they're really
all similar one to another you can use
for all sorts of things so Alyosha Frost
has used it to make a very cute image
filler in ER you said I want to get rid
of this building and put something there
go find me an image it looks kind of
like that and then put whatever belongs
there in there and it's against relying
on that same regularity from I've seen
context and it does a good job so so
that's what I want to go through first
this is leaving me data set which is a
nice label training set you can use for
a number of computers and applications
and then one application that we put it
to which is to take advantage of
regularities of the entire picture
itself to help figure out what objects
are there and where they should be so
thank you
it's not exactly I would characterize it
so we did have these two sources of to
time streams one was you called bottom
up just looking at local image
appearance and the features we use were
these little small versions of us just
so basically orientation and within a
little box the other one I wouldn't call
just low frequency I would call it I
mean it's it's from regularities in the
labelings
by people of images that look similar
anyway but I suppose you could call the
low frequency I don't quite think about
it that way but
right so we use the the ladder that you
described because we we take this
retrieval set which matches the
appearance and we cluster cluster the
labelings and that gives us a prior on
what objects to expect let's see so okay
so just a sorry just taken to it you
have a retrieval set and it's labeled
and we want to use it to to inform our
detectors about context and what is your
proposal for doing that I see okay
right but actually we we use we use
human labeling on clustering we the the
retrieval set is clustered based on
similarities of the labeled objects in
them so in some sense we do that but
we're using a very sophisticated
detector we're using people and and
their heat map is is outlining this
objects themselves and that's what we
used to cluster those retrieval sight
images right yeah if we if we pick a bad
retrieval set we're sunk but once we've
got it then everything is labeled and we
use those labels</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>