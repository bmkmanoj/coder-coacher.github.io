<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Programming Line-Rate Routers | Coder Coacher - Coaching Coders</title><meta content="Programming Line-Rate Routers - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Programming Line-Rate Routers</b></h2><h5 class="post__date">2016-10-12</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/df2_72wjEdw" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">yeah thanks Jeff so I'm going to be
speaking about work I've been doing over
the past two years or so on making high
speed language switches programmable
this is John Guare the number of
collaborators from several different
institutions with listed here so to
start off this is what I consider that
of the traditional view of networking as
probably taught in an undergrad class
it's almost this dogma of having fixed
our simple switches and programmable are
smart endpoints so you see this
reflected in most applications today
whether it's web browsing or video
conferencing the smarts are and the
clients in the servers and the switches
themselves are just relegated the first
order with just the task of forwarding
packets now over time this viewer
started showing signs of age perhaps the
simplest symptom problem is that switch
features had typed a fairly long design
cycles in hardware so the fastest
switches today are built out of
dedicated silicon which means even if
you want to introduce what in my view is
a fairly benign change like a new
protocol format you need to wait two to
three years to see that in the next
generation of the switch and at the same
time operators especially within
datacenters seem to need much more out
of their switches for a variety of
reasons
and finally as a researcher it's deeply
frustrating that if you come up with a
new switch algorithm you can basically
not deploy it in a production network
today because there's no way of changing
a switch to run a new algorithm so I
believe the solution to these problems
is to have a programmable switch where
you can change the functionality on the
fly instead of waiting for the next
generation of switch hardware so how
switches over time stacked up against
this curve so the early switches from
the early days of the opponent through
the mid nineties not little listed some
of these here they were built out of
mini computers or server machines so
they were essentially servers that were
branded as routers because they had some
special problems powering software
this alone is sufficient because the
forwarding rates of the time were under
a megabit per second who's also
eminently programmable if you wanted a
new switch you just took out the
forwarding software replaced it with
something here then you got a new switch
now since the mid-90s the situation has
changed partly because as the internet
exploded these fees exploded as well so
this chart tracks aggregated single-chip
switching capacity since the late
nineties and on the red line here I'm
plotting the performance of the fastest
Hardware switches at those points in
time so this is what I mean by line
right through the rest of the talk so I
use that term to denote the highest
speed supported by a particular wire
standard like even at the same time
they've been several attempts at the
software switches that use some
programmable substrate but hopefully the
implication here it is clear that
software switches regardless of their
substrate end up losing 10 200 X and
performance so efficient for much of my
work is whether we can have the best of
both words can we have the performance
of the fastest function switches that
are built out of dedicated silicon today
but at the same time make them much more
programmable than the fixed functional
nature of these devices today and I'm
talking about program ability that far
exceeds open-floor Software Defined
Networking so open flow allows you to
program the control plane of the switch
by moving it out of the switch and on to
a server it has little to say about
programming the data plane behavior of
the switch within the switching is the
same time we can see that this program
really is a purpose-built
for the domain of switching so we are
not looking at a turing-complete
software switch that allows you to do
arbitrary kinds of packet rustle and
that's a compromise that I knew is
reasonable because that's what you give
up for retaining this high performance
so over the past 3 years or so several
chips have emerged around this paradigm
and the most recent of these these chips
have performance comparable to the
fastest
fixed-function chips today they're all
the languages statistic for emerging to
program these chips so with that context
let me describe the internals of a
switch so that I can then describe my
own contributions within the context of
a switch internal okay so this which has
a pipeline this pipeline starts off with
a pastor the pastor takes bites coming
off the wire turns it into sort of a bag
of header fields
this bag is passed through the pipeline
on some kind of a bus
the pipeline is counted as a sequence of
match action tables so the match just
filters or packets of interest such as
TCP or UDP packets the action carries
out things like packet filled
manipulations such as decrementing the
TTL field the packets then end up in the
scheduler where they just sit around
till an output link is free to schedule
a packet there's a similar egress
pipeline and then packets are sent out
so within this I've talked about three
concrete pieces of work first one have
to the first two have to do with the
pipelines the first third of the talk I
talk about a machine model that
formalizes the computational
capabilities of the ingress and egress
pipelines so what does it mean for these
switches to be programmable what does
that instruction set look like and what
are the design considerations when
designing these instruction sets the
second third I'll talk about a high
level programming model for these switch
pipelines and this is in the same spirit
that you don't want to program your x86
chip in assembly you ideally want a high
level programming model for the x86 chip
and finally I'll talk about how to
program the switch scheduler which is
the part of the switch then decides
which packet to send out next when an
output link is free by the way this
spring yarn you actually fire it yeah in
the bottom ok so let's look at the
machine model so as I said a switch is a
pipeline of my action tables the max
just serves to filter out packets of
interest the actual packet processing
in the action so let's just focus on
that these action units might have
access to some local state this could be
as simple as a counter that counts
packets going through the data plane of
a switch
it could be more involved like we'll see
the stock when packets go through one
stage of the pipeline the action units
operate on different slices of the
packet and parallel and transform them
sort of in lockstep synchronously and
then send them off to the next stage so
in steady state you can assume packets
going through this pipeline one after
the other
animation is a little slow but the more
important thing here is the typical
requirement from one of these pipeline
stages is the ability to process a
packet every nanosecond right so you're
looking at a clock rate of about a
gigahertz for typical switching
pipelines today okay so this combination
of the state plus the combination of the
state plus the action unit that operates
on the state we call it an atom and we
use this to denote the smallest unit of
atomic packet processing that the
hardware exposes is an instruction to
the compiler and the program so what
does an atom look like here's an example
atom here we take a piece of state X and
either add or multiply a constant based
on a choice control variable that sets
the output remarks so this is an example
atom and you can imagine coming up with
different kinds of atoms and specifying
how many stages you have in the pipeline
the pipeline depth and how many happens
you have within each stage the pipeline
width and with these pieces of
information you've essentially encoded
the switches instruction center so
that's the machine model for the switch
now let's look at some design
differences between atoms that do and do
not manipulate state within the data
plane of the switch yeah
the definition of a finite state machine
is a combinational partner some state is
that this is mostly like an arbitrary
function on it and update the state but
in practice in order to meet the timing
requirements of passing a packet every
nanosecond the amount of computation
that you can carry out is limited to
what you can finish within a nanosecond
and some specific operations in mind
when yes we'll get to those okay so
let's distinguish between atoms that do
and do not manipulate state because I
think it's useful to understand this to
understand how these atoms need to be
designed so let's say we have this
statement operation that takes a pair of
packet fields subtract Sept 3rd and
writes it into a foot now how do you do
this in hardware you can imagine a
two-stage pipeline which will render but
the first state if this pipeline takes
the fields f1 and f2 and writes it into
a temporary field that you've told the
parser to the serve some space for the
second stage of the pipeline subtract
Sept 3 from temp and writes it into F 4
now what I did here was to take an
expression involving 3 packet fields and
break it down into a sequence of
pairwise operations on pairs of packet
fields but this is more generally true
you can take a more complicated
expression it sort of Whittle it down
into these pairwise operations on pairs
of packet feeds now as a switch designer
this makes life very simple when you're
trying to design these stateless
instructions you can basically provision
for all binary operations on a pair of
packet fields and then use the compiler
to do exactly what I just did the
consequences is straightforward to
pipeline these dateless operations
because each packet is completely
independent of the next so let's see if
we can apply the same trick to a
stateful operation so here's the
simplest example I could think of
account
and here's an attempt at pipelining it
so you have a piece of state X that is
read in one atom into a temporary field
this the second atom increments the
temporary the third one writes it back
now would this approach work let's say
you have two packets red and green
arriving back-to-back
clock cycles zero and one so red just
shows up now after clock cycle one edge
temp is picked up zero Green has just
entered after clock cycle two green stem
picks up zero which is the old value of
x you can probably see where this is
going
as Green makes its way through the
pipeline it gets incremented to one and
it ends up writing one back into X so
after two packets have gone through the
pipeline X is now still one when in fact
it should be doing so the problem with
this is you're taking the stateful
operation and you're trying to pipeline
it into a sequence of stateful atomic
instructions and that doesn't quite work
instead you need to provide an atomic
instruction that implements X plus plus
so that the next packet arriving and
that item sees the updated value of
State and this is more generally true if
you have like an abstract operation
where you take a piece of state s apply
some functionality on s and write it
back into s you need to actually provide
an atomic instruction that corresponds
directly to this operation because
there's no easy way to take this
operation and pipeline it into a
sequence of smaller atomic instructions
and as a result the stateful operations
end up looking the stateful operations
and hence the stateful instructions end
up looking far more complicated than
garden-variety x86 or arm instructions
as we will see in the stock ok so that
is the machine model of the switch and
hopefully gives you a sense for the
kinds of program bility that are enabled
at the hardware level but how do we
exploit this what
good model for programming the switch
pipeline so our abstraction for that is
something we call a packet transaction
which is a block of imperative code that
captures an algorithms logic and we use
the term transaction here to refer to
the fact that this block is atomic and
isolated from other instances of this
block formally the semantics of a
transaction are very straightforward a
packet comes in you execute a sequence
of statements within the transactions
block you execute the sequence
sequentially you end up updating some
packet fields and some state in the
process and only after you finish this
packet you move on to the next packet so
the programmers view is that you're
processing one target after another
serially with no overlap and processing
across packets so here's an example this
is an algorithm that samples every tenth
packet by counting from 0 through 9 and
sampling at the tenth packet so let's
say you get p1 at the end of p1 count is
incremented to 1 but it's not sampled
with p2 you have a similar story but
notice that we're processing p2 only
after p1 is completed and we keep going
serially until we get d-10 which then is
actually sampled because Sounders at 9
so the sir gives you the solution of
processing packets serially on almost an
extremely fast single core processor
without having to worry about underlying
hardware details like pipelining and
what atoms are actually available in the
hardware so we applied this abstraction
more generally to a whole host of
networking algorithms that span things
like measurement conditions control load
balancing and so on and I've listed out
the number of lines of code when
expressing these as packet transactions
and the number of lines of code is
actually not too different from how many
lines you would write if you were to
express these algorithms or pseudocode
or export from software switch so
to give you some sense for the fact that
these transactions are a fairly
expressive abstraction for the kinds of
algorithms we care about okay
by contrast if you were writing one of
these algorithms and before which is the
status quo for switches today you would
have to deal with concurrency head-on
you wouldn't actually have to say what
happens in every pipeline stage and much
of the work of the compiler for packet
transactions is to automate this process
as we will see okay so how do we compile
these transactions the programmer
supplies a transaction written in an
imperative language that we called
Domino this is similar to C or Python
except we forbid loops of any kind
because there is no clear way to
synthesize a loop with an unbounded
iteration count and still guarantee line
rate performance we feed this to a
compiler that does two main things it
extracts out these code fragments that
we call code lines and if we run those
coordinates atomically you can guarantee
the semantics of the transaction that I
just described and the second step of
the compiler will take these coordinates
and map them okay
map them to underlying Adams provided in
the hardware now it might turn out that
the space of computations that the Adams
capture because of that digital
circuitry is insufficient to actually
implement the coordinates in which case
we reject the code and then the
programmers that it can't be mapped now
this is a rather different trade-off
from programming on a software switch on
a software switch all for that you throw
will be compiled and the performance
will depend on how complex the code is
on a line rate switch all code that
compiles will run at line rate but there
will be code that is rejected by the
compiler because conservatively it
doesn't know how to actually map it to
the hardware run and
okay so let's go over the compiler in a
little detail so the first step is to go
from this serial view of the code that
the programmer supplies to this pipeline
coordinate representation that is closer
to the hardware so this is the same
package sample code except it's being
pre processed in a few ways to make it
more amenable to compiler analysis so we
start with one node for each instruction
in the transaction we add dependencies
between these nodes to reflect
dependencies within a packet so for
instance while processing the current
packet packet dot gold is written in the
first node and then read in the second
node so there's a forward arrow in that
direction
but the more interesting thing is
dependencies across packets and this is
captured truth state maintained on the
switch for instance counting and we
represent these dependencies by pairing
up every read and write to a piece of
State so the intuition for this is you
want the right to a piece of State for
this packet to complete before you read
that state for the next packet otherwise
you don't have the correct value you
also want the read to a piece of State
to finish before you modify and
subsequently write the state for any
given back so that's the illusion
between those two arrows to write
address so this point you look for
strongly connected components in this
graph and the strongly connected
components are the coordinates that you
are looking for so the compiler sort of
correctness theorem is that if one of
these strongly connected components
which is also a coordinate is run
atomically or can be run atomically by
the underlying hardware then you can run
that transaction with the semantics that
we have at line rate okay so now that we
have the strongly connected conference
we fuse together all nodes within a
component and only retain nodes across
components and this gives
a dag with our coordinates so we've
taken out all edges within a component
and we can now sort of just schedule
this dagger into a pipeline by just
doing a topological sort and at this
point we have this coordinate pipeline
that we are looking progress if you take
the code as you can see it right now if
there were anywhere to run any of these
code lengths all of these code lengths
atomically on the given hardware then
you can run this algorithm so the second
step of the compiler checks that so it
takes these four legs looks at the
underlying Hardware substrate and maps
the code let's one by one to Adams
provided by the underlying hardware so
we do a rather simple heuristic for this
now we just assign each code lengths
independently - so an atom there is no
packing of coordinates into atoms and we
also assume the substrate is homogeneous
there's a single atom type that is tiled
through the whole fabric and then the
process if you run out of either
pipeline depth or width you just reject
the code but assuming you've gotten past
the process and you do have enough atoms
you should till haven't checked if the
coordinate can actually be implemented
by the circuitry within the atom so I'll
give you an example of what I mean by
checking this process so let's say you
had this atom which is the same one from
earlier where you add or multiply a
constant into a state variable X if you
were supplied the code let X equal to X
plus 1 you can map it to this atom
simply by setting constant to 1 and
choice to add if on the other hand you
supplied the code let X equal to x times
X there's no way to map it to a single
instance of this atom because there's no
setting of constant and choice for which
these two are equal right arms
concretely we use program synthesis to
solve this problem where we trying to
synthesize the right configuration
settings for the atom that
said implement a specification which is
the coordinate but popping up one level
this really is the crux of the compiler
it tells us if the algorithm that was
supplied by the programmer can or cannot
run at line rate so it can be rejected
for one of two reasons either you ran
out of pipeline depth or width or this
part of the compiler wasn't able to map
your coordinate any of the computations
available within the other okay so
there's obviously one immediate use case
for a compiler which is you know once
you've built a programmable switch you
take programs and compile it down to the
switch but I think more interesting use
case is when you're designing these
programmable switches in the first place
so can you use it as a tool to run these
thought experiments where you say if I
had these instructions what algorithms
for I support and can you iteratively
refine the instructions in the process
so here's a blueprint of how that would
work you supply an algorithm written in
our imperative language if we call
Domino we supply a specification of the
item that tells you what the
input-output behavior is we specify the
pipeline geometry which is the F in
terms of the number of stages of the
pipeline and the width in terms of the
number atoms per stage we feed all of
these to a compiler which tells you if
the algorithm compiles or not ok so
invariably when you start out the
algorithm doesn't compile because the
item that you're trying out is too
simple and then you iterate by either
increasing the number of atoms in terms
of depth or width or just changing the
circuitry within the atom to capture
more computations
either way you iterate until eventually
the algorithm does compile at this point
you move on to another algorithm and you
sort of repeat the process till you're
satisfied that the item covers as many
algorithms that you
Yeah right okay so let me show you a
demo of this in action because I think
that will explain this a little better
so we need to start out with some
algorithm and this should also give you
a sense for how we program and Domino so
this is an algorithm that we call a
learning filter that learns pairs of
source port destination put in the data
plane as packets go by and it uses a
bloom filter underneath to do this so it
maintains okay so again these three
pieces of state state filter one filter
two and filter three that come
correspond to three ways in a bloom
filter so that's the state that is
persisted in the switch across packets
this line here is the signature of the
packet transaction it just takes a
packet as an argument and implicitly all
the states that is available globally so
the body of the transaction does the
following things it computes three
indices into these three different
arrays using three independent hash
functions because it's a bloom filter it
checks membership at all those locations
and because we're trying to learn new
pairs of ports in the data plane you set
the membership prints at those locations
as well okay so this is the algorithm we
need an atom to go along with this
because we need some atom pipeline is
our bootstrap this process so the
simplest atom that I could think of is
something called the readwrite atom okay
this atom allows you to either read the
latest value of a piece of state in the
data plane and write it into a packet
field on every packet it also allows you
to write either a constant or a packet
field into a piece of state X right so
that's the functionality of that's
provided by this adder so let's see if
we can compile the learn filter
algorithm to a pipeline made up of these
three writers
so when we run the compiler we supply
the algorithm a file format that
describes that describes the
input-output behavior off the readwrite
atom and a pipeline depth in width we
start out with apartment two because
that seems like the natural place to
start so this takes a while and just
tells you that it cannot find American
because the pipeline is too small and
you need a larger pipeline for this
piece of code to compile so let's
increase the pipeline to about 10 in
both dimensions and now when you run
this it takes a little longer and it
actually finds a mapping and tells you
what the pipeline configuration looks
like so it tells you in each pipeline
stage water sort of the active atoms
which implement this algorithm ok so
let's move on to a slightly more
complicated algorithm this is something
that I call a heavy eater detector this
is a fairly well-known algorithm for
detecting flows with large traffic
volumes in the data plane and underneath
it uses this problem stick data
structure called accountant sketch that
counts flow volumes the state here is
similar to the bloom filter of state
there are three arrays representing
three rows in a conference catch you
again hashed into those three locations
so that's similar as well this is the
piece of logic that detects a heavy
hitter if all three locations exceed a
particular threshold and because in
counting packets in the data plane we
increment those locations within the
countenance cage right so again
hopefully these two algorithms give you
a sense of how to program in time it's
not very different from how you would
write algorithms for a network simulator
software switch okay now let's see if
the compiler can compile the
heavy-hitters algorithm to the pipeline
of the same readwrite atoms from before
so it takes a while and this time it
fails to find a mapping because it says
the Adam isn't expressive enough and if
you look at the Aviators algorithm this
should make sense because the stateful
manipulation within okay
the stateful manipulation within the
heavy-hitter detector is this ability to
read a particular location in an array
add one and write it back while the
readwrite atom only allows you to write
a completely new constant or a new
packet field into a piece of state it
doesn't allow you to use the old value
of state add something and write it back
so this makes sense so can we move on to
a slightly more complicated item so that
heavy-hitters can actually run at
library so the next most complicated
item that I could think of is something
called the read and write an item that
takes a piece of state X and add either
a constant or a packet field to X and
writes it back into ax right now by
setting the constant to 1 you would
imagine that the heavy literal detector
would be able to run on a pipeline of
these atoms so let's see if the compiler
tells us that so we run the same command
line as before but with read a drive-on
takes a while and it finds a mapping and
again prints out the pipeline
configuration ok so hopefully this
should give you a sense of how you can
use this compiler to sort of create a
new atom and then check your work
and see if an algorithm actually
compiles to this item that you came up
with so we in fact went through this
whole process and designed an entire
hierarchy of such happens the first two
items are the same atoms that I showed
on the demo the interesting
the hierarchy is that every item on this
list can express all the computations
that its predecessors can and the reason
we went with this approach is because we
were selves looking for this one atom
that could subsume all computations that
we could then sort of declare as the
atom that you should be implementing on
a programmable switch you could imagine
a different approach where you know
different sets of atoms provide
different kinds of stateful update
functionality and you have a switch
substrate it's made up of multiple kinds
of atoms okay so now that we have the
atoms let's actually go through what a
network operator would do when compiling
these packet transactions to switch is
made up of these atoms so these are the
same algorithms from before when we take
each of these algorithms and compile it
to a pipeline of each of those atoms so
sort of the full Cartesian product and
here I have listed the most expressive
stateful atom required from the
underlying hardware in order for the
algorithm to actually run at line rate
again the first two entries here should
be clear from the demo the bloom filter
just means read write for text and send
the heavier detector needs you to read
add and write back but the more
interesting thing here is the caudal
algorithm that doesn't matter because
entirely it has the square root
operation on a piece of state that is
more than any of our algorithms can
provide now you could fix this by having
a lookup table that approximate square
root but the broader point still remains
that no matter what atoms you come up
with there will be some algorithms that
are protected this cannot be run at line
read because the space of computations
is finite within an atom
how many atom instances did we need so
this is the pipeline depth and weight
for each of these algorithms and if you
put them put them together around 100
atom instances suffice now how much do
these atom instances cost in hardware
we wrote these atoms and circuits in
merry log and synthesized them to a
recent transistor library and they all
meet timing at a gigahertz with
considerable slack so it's actually not
too hard to build this out as Hardware
circuits furthermore the area overhead
related to a baseline switching chip is
quite modest so here are some concrete
area numbers okay so as you can see that
they're actually pretty small even for
the most complicated of these atoms and
the area's increase as you go down the
hierarchy right and that's to be
expected because the atoms get more
complex the takeaway here is it less
than about one percent additional area
of 400 atom instances now obviously this
number depends on the number of atom
instances and obviously at some point
you might pack so much functionality
into an atom that it starts it starts
why lighting timing it on gigahertz at
which point you might have to come up
with multiple different kinds of atoms
each of which meets timing in a
gigahertz but provides with different
such a functionality but the point I'm
trying to impress upon you here is that
it's actually quite a small amount of
additional area and it's actually
feasible to do many of these stateful
computations and a clock rate of a
giggle so what the wires area for
connecting these atoms yeah so we have a
slightly more nuanced analysis that
includes the cost of the maxis that take
outputs coming from one stage and going
to the next stage that adds another
percent or so again this these the cost
of the maxis itself keeps going down
because that's just combinational
circuit see what's connecting them might
be the wires in some sense is cost that
you need to pay even for a fixed
function switch because you need to
shuttle the packet headers from wall
stage to the next
so that's not something that you
additionally add for
unless you headed to add another stage
just to make another batter yes if you
need to add additional stages yes that
is additional cost if you wanted to
wider the number of your headers because
the program will switch anyone here like
do 100 headers then you would have to
add this but for most of these
algorithms we don't need too many
headers in fact the ones that stress the
number of headers like the then you are
bringing in the wires because it might
be some because you had the winds as
well that they are you need to
communicate within the month on stage
you say it's programming with the width
width in obviously it has to pay those
many wires within the width of a
pipeline state but you know I'm going to
have hundred atoms within the pipeline
stage you go to have hundreds readout or
like say ten stages and ten atoms the
cottage okay so that concludes my
discussion of the ingress and egress
pipelines now the last part of the talk
and talk about the switch scheduler
which is the part of the switch that
transmits packets whenever a link is
ready to schedule something and so far
if you look at switches the schedulers
on them are fairly limited these are
things like
round-robin priority is traffic shaping
and you can change the coefficients of
these algorithms at a few conflict
settings but can't change the underlying
algorithmic logic itself so why is this
why has programs on scheduling been so
hard I contend that it's not for a lack
of algorithms we have enough scheduling
algorithms but there's little consensus
on the right abstractions that underlie
all these scheduling algorithms this is
in contrast to other aspects of the
switch pipeline that have become
programmable such as fast graphs for
parsing and much action tables already
and this lack of an abstraction has a
real consequence especially because the
schedule there are fairly tight timing
constraints of one in here and DQ every
nanosecond so you can't just get rid of
any abstractions on together and destroy
a CPU in the fast path it just doesn't
cut it for the kinds of performance
numbers we're targeting so in summary
what you read is an expressive
abstraction that can actually one at
line rate we're maligned right I mean a
1 gigahertz processor ok
so together this abstraction let's start
at the beginning what does the scheduler
do it decides on two things the order in
which packets are sent from work
conserving algorithms like first-come
first-serve and priorities the time at
which packets are sent for non work
conserving algorithms so these aren't
mutually exclusive you could put them
together to come up with more exotic
combinations but any programmable
scheduler needs to make each provide
flexibility on these two items so here's
a strawman
programmable scheduler that in some ways
reflects the way scheduling is done on
fixed function switches today so packets
come in they're classified into one of a
number of first-in-first-out cues of
per-flow queues and then there's some
programmable logic that tells you this
flow goes next or hey it's time for this
flow to send according to its token
bucket now I should send this flow so
this is pretty similar to how even fixed
function schedulers are implemented
today the scheduling logic runs on the
DQ side it sort of goes around robbing
across
and then arbitrate within cused within
each book so can we just extend that and
make that piece of hardware logic
programming the terms of the problem
with this approach is that there's
actually very little time on the BQ side
so if you're going to be D queueing a
packet for a hundred three or four
hundred d-link you have less than a few
nanoseconds like about five clock cycles
at the lowest packet size to DQ packets
before you have to DQ the next packet
now this limited time is a problem
because these scheduling algorithms
aren't pieces of computation that can be
easily pipeline because they have
considerable amount of state and the
presence of state complicates pipelining
like I outlined earlier now the net
result of all this is if you were to do
program will scheduling on the DQ side
then you're really limited to those
fight clock cycles and there's only so
much program data you can pack into
fight clock cycles right so the question
that we're trying to go after is can you
move the programmability to the end to
your side so that you potentially have
all the time in the world from when the
packet was transmitted at the end horse
when it's received at the switch
schedule so our abstraction for
scheduling does exactly that the sort of
new insight that we have here is that in
many schedulers the relative order of
buffered packets does not change once
packets are actually in the system so
what do I mean by that when a packet
comes in you can determine its rightful
place in the scheduling order as it's in
queued and you can just leave it there
secure in the knowledge that you don't
have to move around packets related to
other packets that are already in the
mother so a natural primitive for this
is something called a push in first out
queue where packets are pushed into an
arbitrary position based on the ramp but
because from the head
well this round could either reflect the
scheduling order or scheduling time and
this is a
a priority queue we use the term Python
because it captures both the work
conserving and non work conserving use
cases while a priority queue is
typically just used to implement strict
priority work conserving scheduling so
this is a mock-up of how it works let's
say you got a packet with rank 8 it gets
dropped between 7 &amp;amp; 9 and as you can see
you don't have to move around the
remaining packets related to each other
you have to move them one index to make
room for the new packet but 7 5 and two
don't get reordered related to each
other so how do you build a program will
scheduler on top of a Python really the
only thing that you can program in the
scheduler is the way the ranks
themselves have computed the part of
pushing a packet in to a particular
location based on the rank and you know
generally just keeping the array sorted
by rank is not programmable in any sense
it's just fixed and what do I mean by
programming the rank computation you can
write out any sequence of steps that
eventually sets a packet at rank field
so this is something I just made up
where you take a packets flow look it up
in a lookup table called T and for
whatever reason add T dot length to it
and that becomes the packets run now
this is really the key modularity in the
design
there's the fixed logic with just the
right enforcement and there's the
programmable logic that actually does
rank computations and note that we've
now moved all the programmable logic
onto the NT aside and you know we are
giving up something it's only satisfied
algorithms for which you can actually
determine a packet scheduling order on
being your side but this gas is broad
enough and by moving it to the MQ side
we have a much larger time budget to
play around so concretely where do these
two paths go within a switch the pipe
was replaced the standard queues why the
rank
computation runs on the English pipeline
and we can repurpose the packet
transactions machinery to actually
around the back time computation itself
so here are a couple of examples so the
well not fair queuing algorithm is one
so we use the virtual start time fair
queuing implementation here where for
every packet we track its virtual start
time related to an idealized fluid fair
queuing system and that becomes the
packets ramp in the by phone as a non
work conserving algorithm we have token
bucket shaping which maintains account
of tokens that are incrementing at a
certain rate and cap to a certain birth
size we set the package send time to
either the current time or whatever time
in the future it would have made up the
shortfall in tokens according to the
cloven pockets and this send time the
absolute want hoc send time as opposed
to the scheduling order becomes the
packets around in the Python C so these
are both examples of ran computations
running on the same switch itself but we
can also have the right computation run
elsewhere in the network so let's take
the switch pipeline and compress it down
onto the switch and add an additional
inversed okay and we can use this to
implement traditional mining process so
for every packet we track explore and we
said the packets ranked to the remaining
flow size in an RPC kind of system where
you send some bytes but you still have a
few more bites left within an RPC okay
so all of these examples illustrate what
you can do with a single by phone and
common to them is this property that
packets that are already in the system
don't need to be reordered related to
each other now they're practically
important schedulers that violate this
property and the simplest of them is
hierarchical schedulers and the
canonical hierarchical scheduler is had
a little packet fair queuing
so HP fu divides link a pass
between two classes in some ratio and
then recursively within flows within
each class right and here are some
example ways it can be shown that if you
try and implement HP of Q you can't do
it with a single priority you are a PI
for because it requires you to reorder
packets that are already buffered with
new packets from but also turns out we
can implement these schedulers using a
hierarchy of fibers which is more than
one by foe and this hierarchy is
actually quite natural it's somewhat
isomorphic to the HP fq3 itself so the
root of the hierarchy is a PI for that
schedules across to child by force red
and blue so this is a change in
semantics now instead of the scheduling
of packets this scheduling across other
Piper's son recursively and each of the
child Piper's can use the cross packets
just like the phone now on the end you
side let's say a one showed up just the
class 4a is entering to the pipe our
food why the packet for eight goes into
PI for red right
and as you can see at the root level red
and blue alternate because the ratios
are equal at the leaf level a is ahead
of all packets from B because air has a
much higher weight than B and the
leaflet so that's on the NQ side on the
DQ side you first start D queuing from
the root pi foot and that pulls out B
now B is not yet a packet but it's a
pointer to another pi/4 so you chase
that pointer go to pi for blue pull out
x1 and then transmit that so the DQ
process now involves traversing a chain
of these pointers before you can be
queue a packet so that should have
hopefully given you a sense that PI foes
are expressive but are they actually
feasible in hardware to answer this we
set out to meet some performance targets
that are typical for a shared memory
switch so the important thing here is
the buffer is shared across all ports
and so is the scheduling logic so any
area numbers that we report to the
scheduler don't need to be
multiply it as number of course so
what's the naive solution to this if you
had 60,000 packets that you wanted to
support a PI 4 over the really naive
solution is to have ones array with
60,000 packets and when a packet comes
in you compare it in parallel to all
60,000 elements and use these
comparisons to determine exactly where
the packet goes in shifty array
accordingly and drop the packet there
now as you can imagine having these
60,000 parallel comparisons is a
non-starter
we need something more scalable yeah
yeah you need something more scalable
for this to actually work in hardware so
we exploit the observation that most
practical schedulers can do across flows
implicitly using the observation that
ranks increase monotonically with enough
flow so how do we use that hardware
design has two parts there's a flow
scheduler that schedules across the head
packets of all flows and then there's a
rhinestone that stores packets from the
second-ranked downwards now on the NQ
side let's say you got ranked six from
flow see that just gets stuck in the
back of the ranked store if d4 shows up
there's no entry in the rhyme store it
goes all the way to the floor scheduler
on the DQ side if you were to transmit a
packet right now we'd send out a zero
then you would go into the ranch store
pull out a to compare it in parallel to
all elements in the flow State in an
array and drop it in the right location
now as you can see we're still doing
this parallel comparison followed by
insert right only now the element a2
needs to only be compared against all
held elements of on flows which is about
thousand elements as opposed to the
naive design where you have to compare
it against all 60,000 packets in the
buffer so this is what makes this work
so we looked at the hardware feasibility
of the design and we first note that the
rank store is just a bank of first
switch designers commonly used this to
buffer payloads so we add it in our area
calculations when we don't you know
actually implement the design ourselves
so the flow scheduler is a new part so
we wrote this as a very large circuit
and were able to synthesize it to a
recent transistor library and show that
it means time in your gigahertz so the
limit of this design right now is about
2048 flows and fails timing at about
4096 yeah that's for a 10 gigabit switch
if you went up to a hundred give it
switcher for the mute switch would you
end up having to handle fewer flows or
no so this is for a pipeline that has a
1 billion packet per second aggregate
capacity it's not it's not dependent on
whether that is divided up as 64 10g
ports or 600 people all right so you
might have to go to a small switch radix
as you go up in an actual wine
reiterative yes sweet oh yeah because
it's provisioned for a given packet
processing rate okay so if the packet
processing rate went from one gear it's
to some higher number you presume we
also have more trouble meet the target
so that's exactly right but typically
people don't clock their pipelines more
than a gigahertz
they have multiple parallel pipelines
and we don't yet handle the case of
multiple i/o and by police have to use
classification the entry goes 5 points
if you don't know the harder problem is
actually we have this into one pattern
where you have three packets being
include from multiple pipelines into an
output port which really reach requires
you to deal with multiple NGOs per clock
cycle as of course we just wanna take
your clock cycle so the flow
classification itself at some level can
be run in parallel in each of the
pipeline's it's more this encashed kind
of timer okay so as an example of bury
our head this takes up about 7 mm
squared in this 16
Lord and in return you get a five-level
programmable hierarchical scheduler and
this is about four percent relative to a
baseline switching chip so to conclude
I'd like to distill a blueprint for
programmable switches that least I
learned from both of these projects and
hopefully by now you've convinced that
high-performance networking needs
specialized hardware where you purpose
build your silicon for the purpose of
switching but the specialization is at
odds with program really on the one hand
you want to specialize enough to get
really really high performance out of
your hardware and a limited budget of
transistors but if you specialize too
much and go too far with this you risk
the hardware becoming obsolete the next
year when your requirements inevitably
change so my view of resolving this
tension is to tailor these programming
abstractions to rather restricted
classes of switch functions so the
restriction is what allows you to retain
high performance because you're not
trying to do everything that a during
complete CPU can do and hopefully you
pick important enough classes of switch
functions so that your abstractions are
future-proof in the sense that they
cover current and future algorithms
within that class so the start really
was two examples of the surprised so
stay put header processing and
scheduling and in both of these cases
the abstractions have tailor-made with
these specific classes of switch
functionality so it's far from a
turing-complete CPU you can't use the
scheduler to I don't know miss
multiplication for instance we're
applying a similar approach to designing
high-performance abstractions and
parameters for measurement that will
appear at Hartnett's
software test bonding these projects and
papers are available at these links ah
thank you so much for your time so
before we take questions I plug we have
some time this afternoon any Kappa tubes
anybody wants to meet and also I think
there's only a few of this point for
lunch so if you would like to join us
instead in a coma
lunch lunch is on Google it's free so we
have time for a few questions including
anybody who's left on the phone there so
you're talking about the scheduler yes
yeah so you want to support at the level
of the entire switch for a single by
comments which you want to support one
in queue and one video every clock cycle
but for a specific port even at the
highest speed of 100 megabits per second
you want to support a DQ only once every
five clock cycles because you cannot run
more than that on 100 d'Alene while on
the entir side you could potentially get
an into every clock cycle even on a
single hundred G output board because
you can have this into one anything
class types another stronsay T but there
is the choice the the computations that
can be paralyzed across all your
pipeline each each one of the packets is
arriving once per clock cycle gives
grant computation you can be done
separately so you have plenty more
cycles per packet so that's an
interesting question so right now we all
of this discussion is about a single
pipeline tradition if you have a multi
pipeline switch and if you did not need
to coordinate between state on different
pipelines then you could do them in
parallel typically many of these
scheduling algorithms need you to
maintain state that has to be shared or
suppose you have synchronized that state
yes and that's not easy to do so the
multi pipeline case certainly requires
more work both in the side of the pipe
oh and on the run computation side but
yeah if you can divide up state in a way
that you know each set of package means
its own access to some local state
that's me
synchronize with other pipelines it
should be still doable yeah so how can
we translate the same it's in this to
the Nick yeah so I think that's a really
good question
the problem is also a little easier with
the make because you have 40 G 100 G as
opposed to 640 few elements per second
yeah so I think in fact if you if you
use one of these programmable Nick's
such as the ones metronym cells or I
think even max has some of these at this
point I think the most straightforward
way to go would be to take a pi/4 and
just implement it on like an FPGA
attached to like say a mana Marx knee
yeah we haven't done that ourselves but
I think that's absolutely an interesting
direction because that's actually much
easier to do because you don't have to
build it up like silicon you can
actually implement it on an FPGA or even
a multi-core substrate they ask
questions okay
you</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>