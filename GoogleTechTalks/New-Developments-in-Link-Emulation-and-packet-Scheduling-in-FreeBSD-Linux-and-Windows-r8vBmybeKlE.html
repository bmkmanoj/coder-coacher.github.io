<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>New Developments in Link Emulation and packet Scheduling in FreeBSD, Linux, and Windows | Coder Coacher - Coaching Coders</title><meta content="New Developments in Link Emulation and packet Scheduling in FreeBSD, Linux, and Windows - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>New Developments in Link Emulation and packet Scheduling in FreeBSD, Linux, and Windows</b></h2><h5 class="post__date">2010-04-06</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/r8vBmybeKlE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">which welcome Luigi Riza here from the
University of Pisa we've worked together
on the previously project for over 10
years and Luigi's contributions are many
in that area involving a networking IPFW
dummynet all kinds of contributions some
of which he'll tell you about today and
without any long introduction I'd like
to just hand it over to Luigi and look
forward to hearing what he stopped as
well thanks Mario
so I'm going to present a couple of
networking projects we have been working
on at the University of Pisa in in Italy
the first one is something you might
have already heard about is the
Dominator mu later which is a package
that has been widely used in the past
and on which we announcement recently
the second one is related an algorithm
for fast packet scheduling which is some
importance now that networks have become
extremely fast and computers have become
also streaming fast and so some of this
solution used to provide service
guarantees are not completely usable
anymore
so now to move to the first part of the
talk why do we need a Malaysian why we
want a Malaysia Malaysian is standard
tool in using protocol and application
testing because it makes your life easy
when you want to set up an experiment
when you want to reproduce results of
your experiment and also it can give
much more realistic results than pure
simulation as can be done with an s2 and
s3 in order simulated and only model the
network part of the system usually when
when you run an experiment on
application the application behaves as
the combination of its component the
application itself the operating system
kernel device driver etc etcetera so you
really want to account for all these
specific effects when you run your your
test there are many many existing
options for emulation one of them is
Dominator and all the other Linux based
solution like nice net the combination
of TC and
another tool called net pot which is
based on unclick so isn't Vicki's motor
router project which is mostly developed
and worked working on loneliness
although there is a user space part as I
said dominate is pretty old I started
developing in 97 as a side project when
I was doing some research on TCP
congestion control it was included at
the time in FreeBSD and then it was
inherited by OSX in recent years as part
of European project one lab we did some
extension 2d 2d emulator most of the
goal of this project was to extend the
ability to emulate the behavior of
wireless networks but I took the chance
also to make the tool more usable and
port it to other operating system as
many other similar systems dominate the
intercepts packets in various points of
the protocol stack you can pick the one
you want either your work in a layer 2
or layer trio or the bridging level and
so on packet through a classifier that
decides what to do with with traffic and
then puts the traffic you select through
an object called a pipe which models the
behavior of a communication link when
the packets come out of the pipe after
some delay possibly not even coming out
because they are dropped by the queue
the packets are to the protocol stack or
they can go back into the classifier so
you can build some complex topologies
just by looping through the emulator
multiple times eventually you get out of
the emulator and and you get your
traffic as if we had traversed a network
with with the feature that you configure
the user interface is really simple
basically you have just one common that
you use to set all the things that you
need configure the classifier configure
the pipes read back the statistics and
so on and so here you you see an example
where you can define
a couple of pipes number one and number
two which model the two directions of a
communication link
they don't don't the same
characteristics for instance this one on
one direction we have a 256 kilobits per
second and a little bit of delay the
other one is four megabit link so this
could be a model of DSL line and then
you configure the so that some tough
certain traffic is sent to pipe number
one some molar traffic is sent to pipe
number two and then the flow of packets
is like in this picture packets coming
in from the network go to the classifier
the one selector for pipe one go into
pipe one and then they get out and are
sent to your your application and the
same happens in the other direction
packets that are targeted for the output
interface are passed to the classifier
then sent to the appropriate pipe and
then they go out to the proper
destination the main application for
this type of emulators are at least the
three that we show here one thing that
you can have is just a link emulator for
protocol application testing and this is
sometimes just used inside your your
computer your workstation or laptop etc
possibly even on the loopback interface
to test how your system behaves if
instead of ten hundred megabit link or
Gigabit link you have a DSL line or some
some form of delay in your communication
oh you can build very easily transparent
bridges with a couple of network
interfaces and so all the traffic from
that goes to you to your bridge is
subject to a Malaysian according to the
rules that you configure in the
classifier and so you you can have one
one box that implements the emulation in
your network ten and get advance the
desired result on the entire sub network
that you connect through this bridge
another application of dominate in
particular and emulator in general is a
shaping many after I developed this tool
many people started using it not for
running experiment but to sell bandwidth
to customers just according to much day
they pay if you're not an ISP you might
still want to reserve bandwidth for
certain application and and limit
bandwidth for some other applications so
they don't get the entire link that
connect your network to the external
world and going through the solute if
you want to use the emulator or a tool
like an emulator in a large test pad or
at an ISP you need certain features that
are not needed in a standard in a
stand-alone setting you need to scale to
thousands of emulator links or you need
extra features to do the classification
and the multiplexing of traffic very
quickly you cannot afford scaling number
of emulated link because that would be
the project I was mentioning before was
done in the context of extending the
features of planetlab planetlab is an
initiative for developing global testbed
sponsor initially by Intel Berkeley and
connect Mellon and there are many
organization involved in in this project
each of them contributes a few nodes to
the system and users have account on the
whole system where they can run a fully
distributed application client lab is
supposed to use the actual network as it
is as a playground for your experiment
but sometimes you want to have more
reproducible results or the network the
connectivity that planetlab don't sever
is very good much better that than what
you would ever from for instance
customer residential customers cetera so
you really want to squeeze the bandwidth
or apply some modification to the
features of the link and that's why we
develop a version of planetlab that runs
within plant lab nodes and in order to
make life easier for researchers
simplified even more the user interface
I showed before that computer emulation
you need to set up a few things not not
many but you need to stop the pipes and
you need to set up the classifier etc
implant lab we have a much simpler user
interface well you just issue one
comment which is called net config and
then you set the one line you said at
the same time the bandwidth of the
inbound and outbound links and that type
of traffic you wants to you want to act
on and whether you expect to run a
client application on your node so an
application that opens the socket and
established connections to the outside
world
or a server application one that accepts
incoming connection and with this
interface and the picture just shows the
architecture how did the signal the
signaling and the configuration works on
a planet like Nome with the the system
each planetlab
user can define independent emulated
links reconfigure them on the fly while
running an experiment because this is a
common that you can run from a shell or
running system from within your
application etc and so you have a very
very flexible tool and easy to use and
to put on an existing application so
let's see a little bit of the internals
of the minute how these the objects that
make up the emulator are implemented and
watched features they provide the basic
object is called a pipe and models only
the basic features of the link and
actually the this is was the main
strategy in our design of the minute
keep it as simple as possible and only
model things that in a way matched
physical reality we didn't want to
implement models for delay losses jitter
and so on because first of all we are
lazy second thing we can develop a model
but it's not clear that this model
covers all possible situations so we
need a huge number of different you know
probabilistic models of these features
and the other thing is that in many
cases some features like
losses reordering and so on are not
feature of the link but there are
official of the traffic that goes into a
link and causes these effects so our
idea was we just simulate the links and
the pipes and the queues and then let
the user generate the effect of using or
misusing these these links in the way
that the application likes to use them
so basically the coming back to the link
we can configure the size of the to
either invites or in packets we can
configure the bandwidth of the the pipe
and we can configure the additional
delay which models the physical distance
twenty points of the of the link that is
model we have a little bit of
configuration on the queue management
policy at the time when this tool was
developed things like red orgy red of
variation activity to management
algorithms were very popular now I think
they're not very very important given
the advance in packet scheduling we
avoid as I said non-deterministic
behavior for the reasons that before we
do have some features that are non non
deterministic because they are too
useful to be left outside like for
instance we have a simple random packet
drop features where we can program that
a small percentage or a percentage of
packets is dropped also we have a random
rule match option in the classifier so
the the outcome of classification is not
deterministic but only acts with a
certain probability and we use them to
run simple experiment on how an
application would for instance respond
to persistent congestion or on or to
model some kind of just by passing
through these probabilistic match
feature by passing packets to different
pipe with different delays you don't
have to use them if you don't like the
models and you're free to generate
traffic so that causes the effects you
want
so the the classifier is just used to
send the traffic to one of many pipes
that you can define and we use a VFW
which is the standard firewall in
FreeBSD it has a large number of option
and be done I've done a lot of work in
the past to make it more efficient more
more flexible and also to have to add
some features that make it suitable and
scalable to the using dam in it which
means that we want to be able to handle
thousands of pipe Stein's
thousands of rules through one one thing
for instance we act added to the
classifiers was the ability to have
multiple passes through the classifier
one once the packet comes out of the
pipe if you want to do more complex
things than just sending traffic to a
very simple pipe then we need to split
the pipe in its components we and pipe
is made of at least three things one is
a juice or peripheral cues if you want
to do some some work related to packet
scheduling and then it has a scheduler
which in the simplest incarnation is
just a FIFO scheduler and then we have a
link with certain bandwidth and delay
but we could think of adding more
features to this link model so the the
reason of this plate is that in this way
we can configure independently or all
the the features of each of the
components and compose all these pieces
together in ways that are not not
possible if we have a monolithic pipe so
the first thing we did was to split the
queue from the rest of the of the pipe
and we created an abstraction called a
flow set which is a absorption to model
flow Q it is a basically a data
structure which we configure by defining
flow masks which is something we use to
partition traffic in into multiple flows
according to
certain and configuration of the
protocol IP address and source and
destination addresses and protocol
source and destination port then a
scheduler which means that we can attach
packets coming from different flow sets
to the same scheduler and then we can
run a scheduling algorithm on those
queues and then they are scheduling
parameters attached to these flow sets
like weights or priorities or shares of
the link that should be given to each of
the flow configuration of the flows that
is again very very simple very simple
once you understand the the model and
here we have an example where we
configure basically two flows as one is
the keyword is called pew for backward
compatibility reasons I didn't come up
with a better word at the time I
implemented the future and so we can
convey we here configure one flow set
which is number one which is attached to
scheduler number five and that it has a
weight of 10
no no mask here which means that all the
packets that are sent to this cloth that
will be put in the same queue and this
queue is attached to a scheduler and
will be presented to the scheduler with
a parameter the weight with the value of
10 typically the way to define zom at
what share of the link the flow will get
the other floss at number 2 is
configured as a touch to a same
scheduler it has a mask which we takes a
few bits of the destination IP and the
weight is 1 which means that when when
packet reached this flow set their
partition in multiple queues according
to the final age bits of the destination
IP like in this example and they are
sent to the same schedulers as before
but with a weight of 1 so when all these
queue will reach the scheduler
presumably the the queue coming from
flows at 1 will get a bigger share of
the bandwidth then then the other
again we need a couple of rules because
we have just defined the objects here
but we need we need classifier rules to
send packets to to these flow sets and
here are the rules
simple as before instead of using the
pipe keyword we used a cue keyword and
then we select traffic coming from my PC
and send send it to cue number one with
a bigger weight and traffic coming from
other machines on my subnet and sent to
flow set two which have which generates
per IP queues with different and smaller
weights regarding links the basic
feature links are bandwidth delay we
have a uniform random blog feature which
I mentioned before we are doing is not a
feature of the link but is implemented
through random probabilistic packet
matching like using a configuration like
this I can tell a classifier to send
packet to pipe one with the probability
0.3 and send packet to the pipe two with
a 100 percent probability of course
subject to the two matching of the other
parameters and then as you see pipe
pipes can have different delays and so
when traffic go to the through these
rules part of it goes to one PI part of
it goes to the other and the result is
that packets are reordered about mac
layer hover as we have many wireless
links now and those are not as simple as
the model that we have been dominat
where each bit takes a constant amount
of significant protocol layer overhead
like preambles contentions etc we have
two mechanisms to implement this one is
using scheduler because the Mac itself
is in fact a scheduler and one is used
the feature I'm going to show now which
is called profiles with profiles what we
do is model the extra airtime for packet
transmission and we use an empirical
distribution of the extra hair time
which in injecting the emulator and then
at runtime for each part
the emulator computes the actual
transmission time of the packet and then
adds a random value taken from this
distribution and depending on how we set
this these tables and this empirical
function we can model simple things like
just the the framing of the packets or
we can model more complex things
although not in a very precise way more
complex things could be contention on on
the channel or even link layer
retransmission or eventual losses after
a number of our transmissions in terms
of schedulers the original dominate only
had five for scheduler then we added the
ability to run specific scheduling
algorithm WUF squared the few plus which
is a proportional share scheduler which
with very good the scalability and
service guarantees features then just
because we needed to emulate to make a
better emulation of the make layer and
we are also doing some research on
packet scheduling we added the ability
to have configurable schedulers in the
system
and so now we have to define an API
using which we can load runtime in the
kernel different scheduling algorithms
and then we can configure the pipes to
use one or the other type of schedulers
at the moment we have this list of
schedulers and other accounting five for
deficit round robin priority and so on
schedulers
differ in the type of guarantees they
provide and also in the runtime
complexity so some situation you want a
simple solution some other situation you
want more complex solution and you're
willing to pay the extra computation
time also as I said mac layer a mac
layer is also a scheduler and so we are
developing and it's almost finished in
802 11 be scheduled and an order will
come after this first prototype
schedulers the masks to a same as flow
set so when whenever
a flow that sends packet to a scheduler
the scheduler does an additional
partitioning of traffic and can create
on the fly multiple instances of a pipe
I'm actually multiple instances of a
scheduler and then of the pipe attached
to the scheduler which means that for
instance an ISP can can create
independent schedulers for thousands of
clients by justifying where run rule
like this one at the bottom where it
defines the scheduler it defines the DD
the type of scheduler and defines a mask
and magically well automatically each
each customer will get its own scheduler
its own pipe which is not conflicting
with the user the traffic used which
other customers are using so the the
fact that we have a well-defined and
very simple scheduler API which takes
care of almost everything you you need
in passing traffic to the kernel like
assembling packets into doing the
classification etc etcetera
make it's very simple to create new
scheduling or test new scheduling
algorithms and you don't have to worry
about you know classification getting a
traffic locking building YouTube
allocating memory etcetera etcetera and
then the scheduler themselves become
very very simple as you can see from the
line count of the various implementation
that we have in the kernel most of them
are all of them are below 1000 lines of
code and most of the at least half of
the lines are comments or copyrights and
so on the overall structure of the this
revised version of dummynet which links
together pipes flows at the queues and
so on is this so we have flows at
objects which dynamically create queue
according to the masker given in the
flow set and then we have scheduler
objects
which dynamically create scheduler
instances here again according to the
mask given to the scheduler and then the
the packets from multiple flow set can
go to one of the instance of the
scheduler which is created dynamically
like in this configuration
testing testing code is very important
especially with kernel code were not
completely trivial to reproduce the
operating condition and some of the
scheduling algorithms in particular that
we are dealing with a bit tricky because
they there are corner cases that must be
tested in some cases if you make a
mistake in doing certain computation the
the theoretical guarantees of the
algorithm O's do not hold them anymore
so we really need to be sure that before
inserting the schedule in the kernel we
we know that implementation is is
correct and so we build some support to
run these schedulers in user space so we
using a system like this well we use the
same exact code that we run in the
kernel but linked to a user space
application which is driven by a packet
generator and the packet generator
itself can create multiple flows with
the features that we like in terms of
different addresses the packet length
weights and so on and there is a
controller which drives the packet
generators and pulls traffic from from
the scheduler and the rules of the
control is to give us a way to control
the operating point of the scheduler
whether we want the scheduler to be run
in a near-empty
situation or with two O's almost full
etcetera so we can try to exercise all
the code paths within the scheduler
the other nice thing of this setup is
that we can run performance testing of
the scheduling self without all the
extra javert of pocket either packet
degeneration or packet reception from
the interface going through the routing
layer locking and so on so we can really
compare schedulers performance in this
way
which is something that would be very
difficult by by running an an experiment
in the kernel with the entire packet
processing chain here what what you see
here is an example of how you can run an
experiment by specifying the algorithm
type the low and high threshold of of
the number of packets in the scheduler
and the composition of flow sets that
you want to generate with you can
generate many flow sets specifying
specifying the the weight and the number
of flows belonging to each of the of the
flow sets so when you use an emulator
you always wonder how accurate and it is
well it reproduces the condition the
operation of the actual link and there
are at least three main factor that
influence the accuracy of evaluation
most of the emulator run off a timer in
the kernel and the accuracy of the timer
as generally in the order of one
millisecond but you you can push the
this value down to very high frequency
so very high values we very low
resolution we have tried running the
freebsd kernel with the hertz
set up to 50,000 it does work there is a
quite a bit of overburden in the system
doing that but it does work and and you
see in the in the experiment that the
accuracy of the of the system improves a
lot by doing that there is another
reason for that influences accuracy
which is the interference of computing
traffic eventually the emulator will
receipt Rafic from physical link and
will send prophets to a physical link so
there is no point in having an emulator
which is accurate to the microsecond
when competing you could have two pipes
ascending expecting to send a packet at
the same time on the output link because
the packet will compete for the physical
bandwidth on the link and so one of them
might be delayed by as much as one
maximum packet size or more if you have
multiple pipes competing for the same
link so at gigabit speeds these these
values in your
120 microseconds or more if you're
running as it's very common
if you're running a link at hundred
megabit per second so I mean doesn't
make a lot of sense to bring the timer
accuracy down in the 20 microseconds
range when you have an another factor
which is much much worse here and the
third thing is the operating system
interference most of the system where
the minute runs are not real-time system
and many kernel activities can take this
lock the CPU for a month the period of
time and if you just run measurements by
using some very CPU intensive kernel
activities like heavy disk IO having
Network IO etc and you run a simple
experiment like pinging a remote node
suggest to that the traffic you will see
that the ping response time pariah lot
they are typically in the order 50 70
microseconds but very very easily they
go up in the 100 microseconds range and
so this is just another factor that hits
on the accuracy of the emulator so
overall if you run the emulator in
carefully controlled condition making
sure that you don't put much load on on
the system or that the you you bring
down the accuracy of the time etc
etcetera
it's really reasonable feasible to have
a hundred microseconds accuracy on
modern hardware which is a good result
in practice when when you run an
experiment in user space the even the
process scheduler itself will give you a
much bigger jitter on your application
in terms of performance we have detailed
analysis and the upcoming issue of
computer communication review basically
the main factor limiting performance is
the per packet processing time and this
is coming from three component one is a
classifying of course another one is the
scheduling cost because when you run
multiple pipes at the same time you have
to schedule the work in D in the
emulator itself and then there are
emulation
so much what you need to do to move
packets through the emulator from Q's to
the scheduler to the delay lines and so
on
d we have made a detailed analysis of
these three components and the
classifier cost as a constant part and
then one which is proportional to the
number of rules and in the entry level
PC Agua we get x in the order between
400 and 1,000 nanoseconds with up to 20
classify rules which is a decent size
for for a set of rules for an an
emulation system you shouldn't need more
them because the classification can be
done basically in a logarithmic number
of the number of rules is typically
logarithmic in the number of flows that
you need to handle scheduling cost
server I a lot but all the algorithms
that we have runtime from order order
one or two order of log n in a number of
flows and emulation costs moving packets
to to the the emulator and so on as a
cost which is logarithmic in the in the
number of pipes so we have measured
times of up to 1.5 microseconds with
thousand independent fly pipes so
overall we can estimate between two and
three microseconds per packet on
entry-level PC hardware so there is the
emulator goes reasonably fast for most
application one one would like to to try
this porting so we recently as I said we
are putting the code to Linux and
Windows so now all major operating
systems the ability to run emulation
directly natively on the kernel and
application you can test your own
application without needing needing an
external box when we started doing this
work the code was only for FreeBSD n OS
X and our decision was to was to make as
little changes as possible to the source
code except for others so we basically
built compatibility layers
replicating the content of the FreeBSD
headers and then we built a glue layer
which implemented the FreeBSD API is on
top of the existing API on the other
operating systems and most of the
difference in turn epochal
representation locking packet filtering
nooks etcetera etcetera but most system
have see me a subsystem or hooks with a
similar feature so all that most of the
trouble was finding out the prayers of
subsystem that were matching matching in
the different operating system in
particular for packet representation
most of the time we don't have the
hovered of copying the entire packet the
freebsd representation uses and buffs
and as many other representation and
buffs adjust a container from metadata
and the pointer to the actual packet
data and when we get packets online
hooks they're represented by SK buffs
when there when we get packets on
windows they're represented by n these
pockets and our strategy was the same in
all the cases whenever we get two
packets from foreign operating system
for and for D laminate source code we
create pseudo n buff descriptor which is
initialized with metadata from the
original representation and with a
pointer to the original data then all
the processing goes on on this pseudo
and bath and then when the packet is
released back to to the original kernel
we destroyed his extra data structure
and similar approach is used for order
order API is like locking like packet
filter improves the the perhaps the most
challenging thing was that in some
system like Linux api's are changing
very frequently and and given that our
module is not part of the of the kernel
is an external model we need the same
set of sources to support you know all
post old version of Linux from 2.4 to
all variation of to point
six and then the code becomes a really
spaghetti to adapt to the biased version
of the same API so you can get more
information at this address
in terms of availability in FreeBSD was
therefore since 98
OS X probably 2006 we completed the
Linux end up open wrt version in 2009
and windows version was completed a
couple of months ago and these are the
students who worked on the on the honda
project on various parts of the project
in addition to myself and then we are
moving to the second topic which is
order one packet scheduling at high data
rates and the first thing is why do we
care about packet scheduling a few years
ago people might think okay we can solve
most scheduling problems but by just
over provisioning our links fact II is
now links have become fast but also CPUs
have become very fast and so it's very
easy for a process on workstation or
workstation on a network to completely
saturate one of the links and then our
provisioning is not feasible and the
goal of the scheduling is to arbitrate
access to common resources and give each
of the customers of these resources
certain service guarantees and
guarantees relation in the use of these
resources so again our provisioning is
in my opinion is not feasible anymore in
many cases and that's why we need the
real schedulers and links are very fast
too and so we need that the scheduling
algorithm keeps up with the speed of
traffic and so it but it must be very
fast itself now in terms of setting
setting the problem what our service
guarantees what we care about well very
often at least this is the model that we
follow we consider an ideal system which
is called a fluid system where the
communication links or the shared
resources is infinitely divisible among
customers and each customer gets a
fraction
of the available resources which is
proportional to a parameter which is
called weight and so the formal
definitions that this customer should
receive a fraction fee I of divided by
the sum of weights of all customers that
are active on the link at that time they
are called backlog clients in the fluid
system because the the link is
infinitely divisible each the system
serves or flows is more tenuously and so
you have perfect sharing according to
these weight parameters in the real
system which we call packet system this
perfect sharing is not possible because
at the minimum you have to serve one
packet at a time so you really need to
define a good sequence good order of
serving customers that approaches as
much as possible these perfect sharing
of flows and the packet system has
constraints in addition to serving one
packet at a time one problem it has is
it is that often is not profitable and
also it must design online cannot
foresee the future and so avoid sending
a packet because another one will be
coming which has a higher weight or
budem would end up its transmission
earlier in the fluid system what we do
is normally compare in terms of defining
service guarantees we compare the
behavior of the fluid the fluid system
and the behavior of the packet system
and the way we do this comparison is to
compute the difference in the amount of
service this is an equation equations
always good in on papers now this
equation what's the meaning of this
equation this equation is tells us that
our performance matrix is the maximum
value overall customer k and over all
possible time intervals delta T of the
amount of service received by the actual
amount of service received by a customer
which is WK over that delta T and the
theoretical amount of service that that
customer would receive on the fluid
which is fee k times w which is the
total amount of work done in the system
in the best possible packet system and
there exists such a system which is
called W F squared Q these deviation in
terms of service is equal to one maximum
segment size which means that in the
worst case a customer will never be
behind in terms of service more than one
packet or head in terms of service more
than one packet being either had with
respect to the fluid system it's not a
good thing because it leads to Barcenas
and application generally don't do not
like Barcenas as well as humans too
don't like parson si I drink one bottle
water a day but I don't like to drink 30
bottles of water the beginning of the
month and nothing for the rest of the
month so now this design that captures
very well this idea of fairness in the
distribution of traffic and it's very
easy to compute the amount of service
received in it's easy not in term well
we have a well-defined the formula to
compute the amount of service received
in the fluid system so that's a good
reference also we have a normal packet
system which can give these optimal
performance index which is one MSS and
so we also have a reference of what is
feasible in practice and so we we can
design scheduling algorithms and see how
well they behave compared to this
optimal optimal system in fact one would
wonder why do we need to design
scheduling algorithms when we have this
optimal system and the answer is easy
because this optimal system there is a
theoretical results that says that it
has a run time complexity of at least
log n in the number of flows and there
is a result by student of mine 2004 who
came up with an actual algorithm which
mad we matches this lower bound so log n
might be expensive when the number of
flows become very
or the speed the packet rate becomes
very high and so we need to break break
this bar here and breaking this bar here
implies the relaxed guarantees because
if theory if the theory is correct and
the theorem is correct we cannot have a
faster algorithm which gives these
optimal performance index now when we
relax the guarantees we can relax
guarantees in various way and depending
on alpha we have from this ideal this
optimal value we have better or worse
schedulers the state-of-the-art term
includes the categories listed here we
have priority based schedulers which are
really fast but give no guarantees to
anyone except the the king the flow with
the highest priority we have round robin
or deficit round robin schedules or
variation of them which have order one
runtime but pool guarantees basically
the amount of service that is given to
the deviation between the ideal service
and the actual service can be as many as
as large as n times the optimal service
which is a bad thing because again we
can have a large bar sternness there are
there is a family of schedulers which
are called time stamp based scheduler
which internally model the behavior of
the of the fluid system and try to track
it and WF squared q the optimal
scheduler is one member of this family
which gives optimal service guarantees
in logon time and there are approximated
variants of these schedulers which have
Auto one run time and their guarantees
are reasonable
they're just constant time largest and
the autonomic guarantees but so far this
is these schedulers the run time which
was several time slower than round robin
which is the best you know proportional
the best proportional scheduler that we
have a
our result is a new algorithm which is
called qfq which is give the same
guarantees of wfq plus variation of time
stamp based scheduler only more or less
five times the maximum packet size and
it's truly constant the other the other
algorithms that I mentioned before are
actually dependent on some other
dimensions which are not the number of
flows but are some other parameters of
the algorithm so in the end their code
needs to iterate over some data
structure to perform operation how our
algorithm says no loops inside just
because it is based on some data
structure which enforce some ordering
properties that involved in the
scheduling decisions and the
instructions that we use are very simple
basically and or masking and find first
set there is only one multiplication in
the in queue or the queue operation and
in terms of run time again on my desktop
machine which is an entry level
workstation the algorithm runs in Q and
the Q pair which are the basic operation
for sending and pulling a packet to from
a scheduler runs in hundred and ten on a
second per packet which compares to 555
nanoseconds for then deficit round robin
and over 400 nano second for KPS which
is one of I think the fastest competitor
in terms of order one scheduling
algorithms I think the result is
important because it makes vacuuming
feasible in software at gigabit per
second rate or feasible also in
inexpensive hardware like switches an
overview of the algorithm I will not
enter into the details because some of
them are very technical and boring but
the idea is that it tries to track the
behavior of a fluid system and for each
flow and for each packet tries to
compute when the packet would start
service and finish service in the in the
fluid
system and this is something that all
timestamp phase schedulers do so each
packet is tagged with a visual start
time visual finish time and the system
also tracks a virtual system time which
is the trucks the evolution of time in
the fluid system now the idea is that we
would like to schedule packets in the
same order as in the fluid system so we
would send packet by ordered by finish
time by vo to finish time the problem is
that sometimes when we have to make a
scheduling decision we don't have the
the next packet that the fluid system
would our finish has not arrived yet
so we cannot really make the same
decision we have a narrow interval in
this process and the other thing is that
in order to avoid too much Barcenas we
should avoid serving packets that have
not yet started service service in the
in the fluid system so basically our
scheduling decision is mostly based on
sorting but it's a constraint formal
sorting because he has to account also
for these visual start times and the
sorting step is what implies a log and
complexity lower bound in the complexity
of the algorithm and as in many cases
with sorting you can try to reduce the
complexity by you sorting on bounded
universe
so you basically approximate values and
so that you have only a finite number of
distinct values so you can use some kind
of bucket sort algorithm that is a
constant time sorting algorithm but when
you do this our approximation we you
have to be very careful not to introduce
additional deviation in the behavior of
the scheduler from the ideal ideal
schedule so quickly glancing at the data
structure of dearth of qfq
the yellow boxes are flows here and the
idea is that when we need to in queue a
packet the packet belongs to a flow we
partition the flows in the in the number
of in a number of groups and each group
has a different round instance and the
rounding is proportional in a way to the
to this parameter the maximum packet
length divided by the weight of the flow
so the heavier is the floor and the the
smaller is the sorry the bigger is the
rounding that you get anyways the de
partition is done in such a way that
groups we flows with similar features
end up in the same group within each
group we by virtue of the features of
the algorithms we can only have a finite
number of different finish time values
and also start time values so basically
we have a finite number of slots where a
flow can end up in each group so within
each group with a no sorting by using
bucket sort a bucket sort algorithm and
for selection purposes what other
algorithms - with other algorithms which
use a similar principle they scan D and
the list of groups looking for the the
flows with a minimum finish time and
that are so-called eligible so they we
look for the flows that have already
started service in the in the fluid
system in our algorithm we organize
groups in a number of sets for different
sets which are represented by these bit
marks so that the index of the group the
ordering of indexes in the group is this
reflects the ordering of finish time in
the group themselves so when we need to
find the group with the smallest finish
time we only actually need to find the
bit
in this set which has the smallest index
and this is easily found with a single
machine instruction which is a fine
first set so the tricky part of the
algorithm is in managing these sets so
we don't need to you know iterate over
the groups to manage the sets and and
organize them in a way that preserved
this disorder link and without entering
into many details but I mean the
algorithm is once you follow the older
proof the algorithm becomes very very
simple it just when you when you in
queue a packet and we will see it here
when you in your pocket you just need to
find the group it belongs to which is
basically a static decision insert using
bug head sort the packet and the flow
within one of the buckets in this group
and then update these four bitmaps that
represents the set that we care about
one of this mid map is the one from
which we do the selection of the next
candidate for transmission and in the in
queue in the dequeue process we just
look at this particular bitmap we find
the first bit set which is the group
which contains the flow that should be
served first then we go to this data
structure which has a finite number of
buckets we again using a final first set
we find the first packet which has flaws
in it and we pick the first flaw in the
in the queue and that's all after doing
that we need to put the flow back in the
into the proper position usually within
the group and do some management of
these four bitmaps which is easily done
using a few logic operation basically
masking to move a number of groups from
one bitmap to the other one so now we
move to the performance evaluation of
our algorithms in a paper that it's on
the webpage that we mentioned in a
moment
there are proofs that show that the
service guarantees for our system are
expressed by this formula and
the end result of this formula once you
put in the constant etc is that a WF Ida
performed the performance index is
within five maximum segment segment size
compared to one which is the optimal
system there is an equivalent in terms
of delay which is pressed by by this
formula but we don't care much about it
in terms of actual performance running
time how fast it is our first P of Q is
compared to the other system for this we
use the same scheme that I showed before
talking about dominant we implemented
the natural qfq scheduler a natural
implementation and we have actual
implementation on many other schedulers
including competing algorithms and we
tested with the system mentioned before
with the variety of configuration in
terms of number of flows and packet
sizes and scheduler state of sin terms
of occupation and performance data are
here this graph shows the scalability of
the system what we compared here is the
baseline which is this graph called none
which includes a system where we run the
same system as before without any
scheduler so it's just your word of
generating the traffic and driving the
the controller and independently of the
number of flows more or less
independently of the number of flows the
system with no scheduler consumes in our
test around 60 nanoseconds or so when we
add a simple FIFO scheduler even just
two in the Q and the Q takes a little
bit of time and we have this second
curve deficit round robin which is as I
said one of the fastest proportional
share scheduler that we have as this
curve and then you have Q it just above
here so about twice as slow
then the deficit round robin the next
competitor is KPS is up here in the 400
to 500 nanosecond range for a pair and
just for a comparison this is the WUF
square q+ which has a run time which is
logarithmic and in effect we can see
this from the graph because the x axis
is logarithmic in the number of flows
from this experiment we see a few things
which can easily easily easily be
explained as you see most of the
algorithms are flat up to 256 or perhaps
even 1k flows when we go up we start
seeing - caching effects because even
even though these algorithms have
ordered one runtime they touch memory
the amount of memory they attach is
proportional to the number of flow so
when we starting start running out of
cash the runtime increases and that
runtime increases for all the algorithms
qfq as a peculiar behavior with only one
flow one flaw in a scheduler is almost
always the possible case for a position
of the scheduler because all the time
you take a flow take it out of the data
structure and you put it back in the
same place after making a lot of useless
checks to see if there is some other
flow ready for for service so we we
could have optimized the code to look
good on on this point but then we would
have had worst performance in the other
points and in the end if you have a
scheduler with only one flow you don't
care too much about the wrong time so
have we these are real numbers from a
real implementation and some of these
phenomena you wouldn't see from
simulation or from theoretical purely
theoretical analysis the previous graph
I showed the performance only with the
flow of the same type but we can run the
same experiment with a mix of flows of
different features in terms of weights
and and of course we have different
results and this destabilize the
result of the experiment with the
different combination of flows and
different algorithms as we see the
numbers were I the standard deviation
are really small in our experiment just
because we moved most sources of noise
from from the experiment there's nothing
particular to say other than here we can
see also the the growth of the runtimes
in in W F square Q Plus which is
logarithmic at within small number of
flows even this algorithm is acceptable
as the number of flow Scrolls probably
we really want some of the fast
algorithm to to run conclusion we have a
time stamp based scheduler which has
near optimal service guarantees through
order one run times which is comparable
to round-robin and this is our short
code is not just a radical result is
already available as part of dominate so
you have it on many platforms now and
the code is available at this link this
joint worked by the way with the fabric
Elkhorn student of mine and polyvalent
who was a former student of mine and we
are planning in addition to the dominant
implementation we are planning to put
this scheduler also within a clique
module just because clique is widely
used as a solution for doing all sort of
networking devices routers switches
emulation systems and so on and future
worker will look at doing more detailed
performance analysis to see how we can
optimize this system even more
especially I think we are thinking we
are actually running the scheduler on we
were T platforms which are you know
cheap access points system which have
only a hundred mega CPU not much memory
not a lot of memory bandwidth we would
like to see how fast the system works
there and how can we optimize it for
this platform also we want to see how
much we can gain in hardware
implementation because there are many
parts of this algorithm that can
actually run in parallel
so if we if we can isolate this part and
build a hardware implementation perhaps
on at FPGA or in the femur of some
network card that would be an
interesting test to make so links are
covered on the talk you have a link to
my dominate page and qfq one for
everything else let's go
that's all questions welcome so matica
born was a sum of code students within
the FreeBSD project last year this year
I applied as my department not as a you
know one of the standard organization
and fortunately application was not
accepted frankly I don't know the reason
but we can discuss this offline but no I
have been trying to encourage my
students to apply to a Summer of Code in
the past and in our five of them had
worked on several FreeBSD projects some
of them related some of them were not
related and naturally now that you ask
there is some other activity we have
been doing also within context of some
approaches this scheduling I couldn't
cover it in the salt which was already
to dance but I'm happy to discuss it
offline if people are interested yeah
yeah okay
the theoretical so okay sure the
question was how much the did is whether
we measure the actual deviation of the
scheduler from the optimal deviation
the answer is we did and there are some
experiments where we showed the the
average deviation we don't have I don't
remember at least I'm sure we have the
data I just don't remember the result
what is the actual deviation that we
measure in our experiment we have to
think that many cases we care about
worst case because you know you never
know how an attacker could drive your
network and and try to force you to run
in one of those worst cases so it would
be great if we would be able to
replicate those worst cases with a
particular traffic pattern and actually
this one of the goal of having an
environment for testing that we can run
in userland with with the controller
that drives the scheduler we haven't
worked on actually trying to figure out
what are the worst case patterns for for
the scheduler
all the classifiers in Linux or other
Malaysian solutions okay so the question
was how does IPFW and dominant compares
with the other solution that exists in
Linux so we need to go back to the first
yeah to this slide so nice net is a not
actively developed I think it's not none
of them is a standard part of Linux
except perhaps from TC nice that is not
actively developed in smashing figure in
terms of features TC is mostly as I
understand traffic shaper it doesn't it
doesn't have support for instance to
emulate delays and so you need an
external package to do all the delay
emulation and at least the feedback that
I got from other people who are used to
website different solutions that this
IPFW and dominate is a lot easier to use
because it's really it's really as
simple as this it's really simple as
issuing a few comments in do in all the
other cases you have to familiarize with
the structure of TC and now the
classifier works and how to add the
additional emulation package after after
it in the other solution that part net
part is a project which builds an
emulator using a click configuration
click is a system where you can
basically assemble modules in a graph
connect them and then it has a very
efficient device driver and it has very
efficient packet processing path and so
in terms of comparison with dominate
the net part approach is flexible
because you can increase what either
build or use one of the existing click
models to do all sort of emulation that
that you like you can do some kind of
scheduling scheduling but there are
there isn't actually actually a good
range of schedulers in in click the
performance of net path solution is
better and I think for given the same
Ardoin that part goes twice as fast as
dominant in terms of peak performance
but this is only true if you use it as a
dedicated system because most of the
overhead in dominate doesn't come from
the processing of the EPI IPFW and I
mean it comes from the network stack you
Usenet part in a standard Linux box you
have all this over back in and basically
the performance in is the same so I mean
I'm clearly biased in in saying that a
minute and PFW are a lot easier to use
but this is the message that I love got
from a lot of people using this order
solution as well</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>