<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Jeremy O'Brien: &quot;Quantum Technologies&quot; | Coder Coacher - Coaching Coders</title><meta content="Jeremy O'Brien: &quot;Quantum Technologies&quot; - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Jeremy O'Brien: &quot;Quantum Technologies&quot;</b></h2><h5 class="post__date">2014-06-17</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/7wCBkAQYBZA" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">jeremy is the director of the center for
quantum photonics in Bristol which is
sort of ream it's pretty fair to say one
of the if not the world's most important
place for doing photonic implementations
for quantum computing
Jeremy originally is from Australia and
we got his PhD and also spent many years
as a senior researcher on cost related
topics and confined electrons and
organic structures superconducting
devices and of course photonics and
folks may know that the UK just
announced I think it's half a billion
pound the UK pound dollars okay so half
a million half a billion dollars on a
new quantum computing initiative and
some of those funds are probably going
to the University of Bristol to first as
a work you guys are going to hear about
and that of course allow us to scale up
initial prototypes in interesting ways
and that will briefs I think additional
life into the overall field and also
will create broader interest around
quantum photonic devices
so Jeremy very interested to hear what
you are going to do right thanks very
much happen so firstly thanks very much
for the opportunity to to be here and
tell you a little bit about what we're
doing in Bristol in in photonic quantum
technologies generally I'm going to
focus on on quantum computing because I
think that's probably what you're most
interested in but I'll touch on these
other areas on the way through before I
get going I need to acknowledge that
there's quite a large number of people
at Bristol and beyond that have done the
work that I'm going to describe and I'll
try to highlight who they are as I go
along so
before I get going I just want to put up
a quick advertisement for this Center
for doctoral training in quantum
engineering which which Google apart of
whether you like it or not thanks very
much to Hartman for giving us quite a
bit of advice and supporting this
enterprise
the ambition here is to Train of order
fifty to a hundred graduate students in
quantum engineering over the coming
eight or nine years and the focus is
very much on the engineering so I think
it's all in the title it's about
training a new generation of of
engineers who understand the quantum
physics but who are very much focused on
delivering the technology so I guess the
the message for you is if you want to
hope you know if you want to host some
very smart young people with this sort
of mindset who will have some some
pretty substantial initial training then
please sir please get in touch and I
think my colleague Mark Thompson who's
the director of this Center will
probably be sending heart that I am a
request for projects pretty soon so just
bear that in mind okay so in in the
usual way
at Bristol where we're interested in
these quantum technologies in
communication secure and other types of
functionality that you get by harnessing
quantum mechanics in sensors which which
reached the ultimate precision limits
dictated by the laws of quantum physics
we're interested in simulations in
particular you know analog simulations
that are distinct from a full-scale
quantum computer as well as the science
that underpins that area and what I'm
going to mostly talk about is quantum
computation I'll mention some of these
these highlights that I've shown here
where we've we've made a demonstration
of this sort of handheld device to bank
ATM machine for example of secure key
growing we've measured the concentration
of a blood protein using entangled
photons in a device like this which
combines those integral photons with a
microfluidic device and we've done a
simulation of a helium hydride molecule
again using one of these integrated
devices and as I say I'll touch on those
three things a little bit as I go along
but mostly I want to tell you about
our efforts towards a full-scale
photonic quantum computer this is really
my advertisement for a photonic approach
to these things so I think it's it's
fairly clear that there's no there's no
really feasible alternative than photons
for communicating quantum information
over any distance optical
interferometers like this that operate
in the classical regime are arguably the
most powerful precision measurement
tools we already have and so enhancing
their precision using photonics is a
very natural way and I think it's fair
to say that it's in historical fact that
photonics is has led the way in
exploring the foundations of quantum
physics and then more recently in
exploring the fundamentals of quantum
information science through violation of
Melonie qualities through entanglement
of multiple systems and teleportation
and so forth and so the argument would
be that if you're doing analog type
simulations that photonics is a very
appealing way to do that where you you
know you have much smaller scale systems
and what I'm going to make an argument
for is is for photonics for you know a
full-scale programmable digital quantum
computer and so I'll start just with a
with a reasonably brief background on
photonic quantum computing and then tell
you about our particular approach to
doing that and try to highlight some of
the check you know that there's some of
the current challenges and the future
challenges to all of this so just
briefly this is say this is an article
that I wrote with several colleagues a
few years back on physical approaches so
the platforms for quantum computing and
of course we couldn't get away with
publishing this paper without the
editors insisting that we produced a
league table of all of the different
approaches and we were pretty reluctant
to do that and we were we were made to
do that so you know compare it trying to
compare all these different approaches
and I guess the reason that we were
reluctant is that this table is not
really the not necessarily the
meaningful thing to do and the reason
that there is this sort of tie
is because building a quantum computer
is you know has a sort of list of
requirements that are all that are
pretty much contradictory to one another
namely you need these physical systems
that have very very low noise which
means that they're very beautifully
isolated from their environment and yet
they interact extremely strongly with
one another if you want to if you want
to implement logic gates for example
they interact very strongly with your
you know your preparation and readout
apparatus so there's this sort of
contradictory requirements and what you
find is that invariably things go work
you know things things look good on
several of those requirements and not so
good on on others so that nevertheless
you know even back then it was pretty
clear to us that actually what was what
was important you know was the ability
to operate these things in a
fault-tolerant way and and and actually
you know realizing architectures for
these some for any of these approaches
and that that would be a sort of more
meaningful comparison and I guess what
I'd argue now is that we're moving even
even further ahead and I think the
arguments now in my mind are about
manufacturability so can we really make
large-scale devices and this is a
cartoon of a of a kind of a large-scale
photonic quantum computer where unless
you're a you know photonic specialist
and most of it will be fairly
meaningless but the point that I'd like
to make at this stage is simply that all
of the elements there are ones that are
familiar to photonics engineers they've
been used in telecommunications and
other applications for years or decades
that those same photonics engineers
would be a bit daunted at the scale of
the whole thing but certainly the
components that are going into it are
familiar to them and what what you don't
see in there is sort of you know any you
know physics breakthroughs required you
don't see any sort of exotic system some
so you know atomic scale fabrication or
milli Kelvin temperatures or ultra high
vacuum or anything like that so it's a
it's a it's a fairly friendly
environment and the actual fabrication
of these components is is done using the
same techniques that are used in in
fabricating micro electronics so that's
the promise of scalability in terms of
being able to manufacture their
components and then I think
a big challenge in terms of assembly and
manufacture and I'll come back to sort
of give you some details on that now no
just this sort of introduction if you
like to the photonic approach to to
quantum computing I am I I was that I am
a iro kickoff meeting at in Maryland
last week and I uh I introduced the
introduction by uh telling them all that
they might have forgotten what photonic
quantum computing is since I ARP I keep
this out of their programs several years
ago which made them chuckle but I I hope
that they can also see the the promise
of this now and obviously they wouldn't
be funding as if if they didn't okay so
the you know encoding in the
polarization like this is some it's very
appealing my my colleague at Bristol
John rarity likes to say that the
lifetime of these qubits is at least the
age of the universe because the
microwave background radiation is
polarized now I guess that's a fair
statement you'd need a fairly big
working space to take full advantage of
that
but intrinsically low noises is the
point here so effectively a zero
temperature system intrinsically which
is very exciting
we know a lot about the polarization of
light and everything we know about the
polarization of light for example is
directly transferable to the
polarization of single photons and in
fact we can manipulate the polarization
of photons in a similar way it's often
said that you know we use the same
off-the-shelf components to do this and
and that's the advantage we do use the
same off-the-shelf components but I
would argue that you manipulate a spin
with the same off-the-shelf components
the difference here is that you have a
very very powerful means to calibrate
these things very precisely and that is
simply to send a bright laser beam
through your through your device with
exactly the same properties except its
intensity and then you can calibrate
your your quantum systems your photonic
systems very precisely there's plenty of
other degrees of freedom that you could
use so here you can see how you go from
a polarization encoded qubit via a
polarizing beam splitter to a path in
coded qubit where you now have a
superposition of a photon in this path
or that path and in fact it's this path
encoding that I'm going to talk about
pretty well exclusively from here on in
and of course this there's always a
conservation of trouble in life and in
particular in in quantum computing and
the trouble comes for photonic
approaches in the form of how do we
answer this this question so the flip
side of these photons that don't
interact with their environment is that
they don't interact with one another
they don't interact with anything very
readily that's the challenge and this is
the proof by cartoon and at this point
I'd like to do a survey and ask if you'd
raise your hand if you've done this
experiment okay so that's that's for
people which is which is a that's an
equal record that was just just set I
think if Microsoft last week it's it's
amazing how few people have done this
experiment or are willing to admit that
they've done this experiment right and I
saw a few reluctant hands raised there
the reason is because it's a pretty
boring experiment you know what's going
to happen all those of you who haven't
done it already know what's going to
happen nothing happens right you you you
you sort of should be a bit surprised by
that if you think of it in terms of you
know beams of particles flying at the
speed of light into one another but it
does really does really tell you the
issue here in Australia where I come
from this is what we call a backyard
experiment because you go home from the
lab at night and you do it in the
backyard in the UK no one has a backyard
and there's a lot of talk around you
know the state of science in the UK and
I think this is that this is the real
issue of course here in sunny California
you guys have have plenty of space and
backyards and so on and that's that's
you know that's that's the reason for
success I'm sure okay so that's the
proof by cartoon this is the sort of
slightly more sophisticated version of
it imagine you wanted to realize a
controlled-not gate so the quantum
analog of an xor gate whether whether
logical operation on the two qubits is
shown there if we had a path encoded
target qubit where a photon in this top
rail represented a zero and in this
bottom rail represented a one we could
arrange an interferometer with a 50-50
beam splitter here and a 50-50 beam
splitter here which both of those
devices
transmitted half the light that comes in
and reflect half the light that comes in
and so if we send a single photon in
this zero mode it goes into a
superposition of being in the top in the
zero mode and in the one mode it then
interferes with itself in such a way
that it comes out here with certainty so
there's constructive interference ready
to come out here and destructive
interference for it to come out here and
similarly a single photon coming in this
one input here goes into the -
superposition inside the interferometer
and then interferes with itself to come
out the ones day so that's nothing more
than classical interference of waves
described at the single photon level and
it's also nothing more than just the
identity operation it doesn't do
anything so far it just Maps a 0 to a 0
and a 1 to a 1 and by linearity of
quantum mechanics a superposition of
those two states at the same
superposition which is precisely what
you'd like to happen if you're in the
top half of this table so if you have
your if you have your control similarly
encoded then very loosely speaking you
want nothing to happen to your control
and if the controls in the zero state
you want nothing to happen to your
target as I've just described well if
your controls in the one state you want
a bit flip operation and the way you'd
imagine doing that potentially is to
introduce a PI phase shift or 1/2
wavelength change in optical path in one
arm of this interferometer let's say the
top arm conditional on there being a
single photon in the control one mode
and so that's that's job done that's how
you realize a control not gate on on two
photon qubits the problem of course is
that to realize this operation here
you'd have met you you'd imagine using
some nonlinear optical material so a
material whose refractive index depends
on the intensity of light in that
material and what you'd be requiring is
that the intensity of this one photon
here would change the refractive index
such that this other photon would see an
effective pi phase shift and it turns
out that for conventional nonlinear
materials you'd need something like 10
to the 9 meters of that material for the
intensity of one photon to impart a pi
phase shift on the other photon now I
would argue that's not completely
impractical you might imagine a spool of
optical fiber that was 10 to the
meters long that would say something
about the clock rate of your computer
and unfortunately the transparency of
any such material is not is not so high
that you'd have very much probability of
either of those photons coming out at
the end and so this was the dead end for
for optical quantum computing at least
as I'm I'm proposing it here and the
understanding was you'd need some sort
of exotic system here like an atom
cavity system that would mediate an
effective interaction between photons
and and at that stage you know this was
regarded as you know an interesting
photonics was interesting in the context
of communication and an interesting
proving ground for quantum technologies
but not an ultimately scalable approach
until Canole the flammond Millburn came
along and showed that the intuition that
I've just sketched in some laborious
detail on the previous slide is wrong
and that in fact you can implement that
precise controlled-not operation on two
photonic qubits using only a linear
optical network so nothing fancy in
there just mirrors and beam splitters
and so on together with auxiliary or
ancilla photons and photon detection so
this this cartoon represents a
controlled-not gate in which you send in
your your control and target photonic
qubits together with some other photons
that don't encode any information going
in and when you detect a single photon
here and a single photon here out will
come your control and target photons
with with the appropriate operation
applied to them now this was a surprise
because of the intuition that I
described and in fact it was a surprise
to the authors so many can ill and Ray
Laflamme set out to prove the intuition
from the previous slide and as as
sometimes happens rather beautifully in
science they discovered something far
more interesting which is that that
intuition is wrong and the opposite is
true you can do it and they went on to
to present a a recipe for full-scale
optical quantum computing using only the
resources that you see on the slide here
single photons linear optical networks
and single photon detectors
and I think at the time so at that time
I was working on solid state approaches
to quantum computing in particular
phosphorus and silicon approaches and I
think it was pretty fair to say that
this was received with okay well you
know that's mathematically proven to be
possible but who really believes that
you're going to make a computer out of
photons flying around at the speed of
light and actually when you look at that
the details the number of these auxilary
photons that you that you'd actually
need
okay it was polynomial and you get an
exponential advantage so you know if
you're a theoretical computer scientist
that's fine its job done if you're the
guy who's got to build the stuff and
that polynomial is huge then that's
pretty worrying and it was huge and I'd
say the in the intervening you know 15
years or so the the situation has
changed and in the first phase it
changed you to theoretical developments
particularly in the realm of measurement
based or cluster state quantum computing
which reduced that overhead by many
orders of magnitude and then in more
recent times I would argue that all of
the components that you need have been
demonstrated in isolation and in
conjunction with one another in small
scale and there's a promise of actually
being able to scale these things up and
so the summary situation is if you want
to pursue a photonic quantum computer
there's going to be a price to pay and
you're going to need many more photons
than then you would need other physical
systems but my argument is that's a
small price to pay
relative to the gain in scalability and
manufacturability which I'll talk about
now I won't talk more about this
klm scheme and I should emphasize that I
don't think anyone really expects to
make a quantum computer pursuing this
type of klm approach circuit model
approach but it's useful from a
pedagogical perspective the reason that
people wouldn't pursue this approach is
simply because the fault tolerance
thresholds that we know of for
topological cluster state quantum
computing is so good and there's not an
expectation that they'll be matched in
the gate model okay so I'll just going
to briefly explain to you what's going
on in this in this picture what what
makes it work and then I'm going to
start moving fairly quickly through
there through the technical stuff
so what's going on inside there is
quantum interference so if you have a
50-50 beam splitter as we encountered
before and we send a single photon into
each input as indicated here such that
they arrive at the beam splitter
at the same time and we ask ourselves
what's the probability for one photon to
come out the top and one photon to come
out the right and if we just naively
looked at that picture classically we
would get the probability of 1/2 because
there's two ways for it to happen and we
just use our usual probability theory
but of course this is an inherently
quantum mechanical system and we should
better apply the quantum approach to
that which involves summing
indistinguishable probability amplitudes
which are complex numbers and can
therefore interfere in ways that their
classical probability theorem theory
counterparts cannot we take a mod
squared to get sensible probabilities
between 0 &amp;amp; 1 at the end and what
happens here is precisely that sort of
interference because we get this phase
shift on reflection and these two
amplitudes completely cancel one another
and so for me this is this is maybe the
simplest uniquely quantum mechanical
phenomenon to to to understand I have
hopefully explained it to you in just a
couple of lines it's also about as close
as you can get to doing quantum
mechanics with your hands in that you
can get into a lab and for example you
can change the arrival time of this
photon with respect to this photon using
a micrometer to introduce a delay for
example and what you'll see is data like
this it's data that I'm showing you as a
quarter of a century old nevertheless
graduate students around the world still
celebrate when they see data like this
because it's it's very hard to generate
and it's hard to generate typically
because in writing things down like this
I'm implicitly saying that these
probability amplitudes are
indistinguishable from one another in
principle so no measurement allowed by
the laws of physics could distinguish
those two amplitudes and as soon as
that's as soon as that statement is not
true then this effect goes away so the
reason that I've laid at the point is
that this is all that's different in
terms of single photons than everything
we know about bright light and it's this
phenomenon that drives
drives a quantum computer basically now
you'd like to know where where the
photons go and they're going into a
coherent superposition of both being in
this path and both being in this path
and if this beam splitter is not 50/50
then this this dip that we see here at
zero delay time doesn't go all the way
to zero but you still have a you still
have a probability for that to work so
for example here I have a 1/3 beam
splitter inside what is a linear optical
controlled-not gate and so again we've
got our control and target modes and
again we've got that interferometer that
we saw before and now our quantum
interference as I've just described
between a photon in the control one mode
and the target photon imparts that pi
phase shift that we required you can see
directly that this gate doesn't work all
the time a photon could come out this
top load or the bottom mode could get to
here or to here it works precisely when
one photon comes out in the control and
one photon comes out in the target this
is how my colleague Jeff pride and I
first realized such a gate in Andrew
White's lab using these you know using
some tricks on you know making
interferometers stable and so forth
you'll note that in this circuit there's
a control and target photon going in and
I just told you that it works when I
detect a photon here in a detector
photon here that's not very useful right
in sight of a computer because detection
of those photons typically involves
their destruction and so it's hard to
embed that into a circuit and so a gate
just like the original klm gate was some
was implemented in them
she gave taguchi's lab in a
collaboration that we had going for for
many years where you you you I mean you
can't probably do the translation
directly but it's this the control and
target photons coming in and these
auxilary photons which you then detect
and then your control and target photons
are then free propagating afterwards
again this is this is not necessarily
the way you'd do it the point I'd like
to make here is simply that if you saw
that that circuit in the lab or or these
ones then they would all look pretty
well the same and they would look like a
forest of optical elements mirrors beam
splitter
beamsplitters and so unbolted to a 1-ton
vibration isolation table and this this
skate here might consume
you know several square feet of table
space so if that's your transistor
you've got a very big computer at the
end of the day and in fact if you know
if you wanted to make a sensor you know
deployment is challenging if it's bolted
to a 1-ton vibration isolation table and
actually it takes really clever people
like real aqua multi-and tom he's in
Agartha you know six to eighteen months
to get these things working and you know
graduate students don't keep working for
that long that you could imagine making
circuits you know many times are more
complicated than this and that's what
we've been working on at bristol over
the last eight years or so the first
efforts were in we're in fiber which I
won't talk about I think they're
important in terms of communication so
making those same logic gates all in
fiber but I think in terms of ultimate
scaling you replace a forest of optical
elements with a bowl of spaghetti of
optical fiber so the approach that we
have been pursuing pursuing is this you
know this cartoon here of photons in
wave guides on chip and I very much
enjoyed my um my good friend and
colleague Thaddeus Ladd who was then at
Stanford interrupting me when I first
showed this picture at a workshop in
Tokyo to say Jeremy those wave guides
they're very lossy you know there's
light scattering out all over the place
there I'm a little worried about that
and I said think thank you 30s for that
question and view the low loss version
of the the artist's impression of the
wave guides which he appreciated the
idea is a simple one and that is to
basically fabricate square optical
fibres on chip so in this case in silica
on silicon where you have a slightly
higher refractive index core of silicon
with with a of silica with a slightly
lower cladding there any guide light via
total internal reflection just as in a
fiber and if you make the dimensions
right
you can support just a single transverse
mode there then you you make make your
beam spitters by bringing two of these
wave guides into proximity with one
another such that the evanescent field
from each waveguide couples to the other
and by controlling the length
that coupling region you can control
what the equivalent reflectivity is
could also do it by the spacing between
we use the length which is a more
reliable way to do it
so using this approach we've implemented
that exact same scene update that I
showed you before so control and target
modes here's that interferometer formed
by the two 50-50 beam splitters quantum
interference at this central one and the
important point of this was that we saw
that quantum interference that I
described to you at the start with a dip
where the visibility of that dip was a
hundred percent to within very small
error bars and that's important because
that would be a fundamental limit to the
performance of the operation of these
devices we combined several of these
gates into a compiled version of Shaw's
factoring algorithm for factoring 15 and
for those of you who don't know you
should understand compiled not as
everyone else would understand compiled
but as a euphemism for already knowing
the answer when you construct the
circuit for the algorithm and I don't
know how this abusive language
propagated but that's that's what it is
so you inverted commas around compile
when you ever hear about a compiled
Shor's algorithm we've been able to
implement one qubit operations with very
high fidelity using these these
resistive phase shifters on on the chip
where you locally heat the waveguide
underneath and thereby change its
refractive index we've implemented one
qubit operations with 99.99 eight
fidelity I like to put that eight on the
end there it's not quite five nines but
it's sir it's pretty close and this
technology is great in every way it's
very robust reliable repeatable etc it's
slow and and that's a challenge that
I'll come back to we've also explored
just as an aside sort of models outside
of conventional quantum computing so
here's a device where we implement a
quantum walk and in this device you have
21 wave guides in this central region
here instead of just two wave guides
that are innocently coupled you have
twenty-one wave guides that are
innocently coupled with one another and
you can implement some very interesting
quantum walks there and it's possible
that you might be able to directly do
some
simulations of important physical
systems using that another application
of that that I'll just mention in brief
because maybe you've heard of this and
you're interested this boson sampling
problem has got quite a lot of some
interest recently and the idea is that
if you take a unitary operation for
modes for optical modes let's say you
have n modes and you send route n
photons of autoroute n photons into
those modes then calculating where those
photons come out what the probability
distribution is for those photons is
intractable on a conventional computer
and I guess the reason that people are
pretty excited by this is that once you
get of order a hundred modes and ten
photons
that's about the limits of what what my
laptop could calculate and you don't
have to go very much further before it's
just simply not not-not-not calculator
ball I won't talk about applications or
lack thereof of that of that thing I
think it's pretty clearly an interesting
thing to do to say you know I've got a
device that will will outperform a
classical computer and that's not not so
far away of course once you cross that
threshold where you can't do you can't
use a conventional computer to tell you
what the output should be how on earth
do you verify the output of it and our
approach here is to reprogram let let's
say this unitary is reprogrammable which
I'll talk about in a little bit
reprogram it to implement the same
unitary as that quantum walk would
implement and that is straightforwardly
calculate of all what what the output
should be and in fact what you see in
those sorts of quantum walks is if you
have three photons you see this sort of
clouding so this is the where the
photons come out with twenty one modes
so on each axis what you know each
photon in you see this sum is clouding
behavior here in contrast to the
classical behavior where you see no such
thing and in you I can't really display
four or five photon data very easily but
if you look at the the five photon data
here then you can see a clear contrast
here so that's a that's a sort of
verification at a you know an
experimental verification technique I
think which is quite interesting
but what I'd really like to talk to you
about is the problems and how we've
addressed them so this is the this is
the chip that was used for the for the
factoring algorithm and you can see by
inspection a few few problems here one
is scaling so factors 15 and I I just
admitted that it doesn't really factor
15 at all so how do we scale that up to
do something useful you can see it
doesn't have any knobs or wires on it so
it's not reprogrammable to do other
things other than what it was fabricated
to do and actually it's still too big
right if that's a couple of transistors
then you still got a very very huge
computer at the end of it so just
briefly how we've addressed those things
I want to just quickly highlight that
they're relevant in these other
scenarios so I mentioned this at the
start this is a system that we've that
we've now that this this one here that
we've now prototyped and patented
jointly with Nokia and to do that
polarization control uses exactly the
same waveguide architectures that I've
described of course they're the
challenges to make it for a few cents
and fit into a very small bit of
existing chip ideally but but the
challenges of that are the same in the
sense that you need to make those
components very small and very highly
functional and again I showed you this
measuring of blood protein concentration
with entangled photons and again a
similar challenge if you want to deploy
those sensors where you've got these
microfluidic channels and wave guides
you need to you need to miniaturize them
in a similar way and in fact for anyone
who's still interested in science and I
certainly am there are reasons for
pursuing these things to explore you
know the very foundations of quantum
mechanics for example and this is an
experiment where we've I think we've
shed some light on the wave particle
duality conundrum that underpins all of
quantum physics in quantum technologies
anyway but this is a this is a technical
talk about addressing these issues and
we've addressed them in the following
ways so scaling you know I'm a I'm a
simple-minded experimental physicist by
training so I don't have grand ambitions
of making exponential improvements in
anything but if I can make enough factor
of to improvement I might be able to
turn the Impractical into the practical
and this is an example of that I'd say
so here's how you might implement
controlled unitary operations and as
you'll no doubt pretty well aware if
you're if you're in this field all of
all a quantum computer really does is to
control unitary operations a lot of very
big controlled unitary operations and so
if we could do those more efficiently
that would make life a lot easier
the idea is trivial almost here's a
control qubit you could imagine as many
as you like and here are some target
qubits and you simply take your target
qubits and based on whether that control
is in the zero-one you switch them into
the red mode which bypasses the unitary
or the blue mode which uh experiences
the unitary so I hope you can just see
immediately there that this does indeed
do the controlled unitary operation
where that unitary could be a black box
that I gave you where I didn't even tell
you what that what the unitary was and
that's that's quite a saving over the
usual decompositions that you would do
to realize the controlled version of a
particular unitary and it's it's
applicable to any system where you have
access to these four levels here in a
controllable way and it's precisely that
that circumvents are no-go theorem that
you may be familiar with which suggests
that you couldn't do this and you can't
do this if you just stay in the qubit
world um we've used that in a you know
you know Shor's factoring out of them
factoring 21 the bigger numbers not in
the news the news is that we've done a
sequence of logic gates that we've got a
probability distribution at the end
that's our non uniform which is means
it's distinct from noise so that's
interesting we've used it in a you know
what I think is a more exciting simple
application and that is to implement the
phase estimation algorithm which
underpin Shor's algorithm and a lot of
other important algorithms and in this
case we genuinely didn't know the phase
before we before we were in the olden so
it's a small-scale algorithm but it
really calculates something and I don't
really have time to talk about the
details of this but we've also performed
well we developed a new algorithm for
quantum simulation for quantum chemistry
together with Alan aspera physics group
at Harvard
and we've implemented that on a chip and
that the headline I guess is that
instead of doing this sort of Trotter
ization that you may be familiar with
where you have a very very long coherent
operation with with many many gate
sequences the task is now simply to
prepare States calculate expectation
values of poly operators on those qubits
your Hamiltonian is described as a as a
sum of you know simple products of those
poly operators and you can therefore
efficiently calculate from those
expectation values the energy of that
state and then you can use an a you can
use a classical feedback loop to then
simply variational e modify your input
state and find the ground state I think
that's pretty exciting the the question
marks are over whether the measurement
side of things can be made for pull and
so the distinction here is that the
output is expectation values of qubits
which are you know it's not the usual
sort of digital output anyway on with
this one with this story so obviously
reconfigurability well you need lots of
wires there's a bunch of wires going
onto a chip those wires connect to these
eight phase shifters here so you can see
that I've now got these mark sender
interferometers but I've got a phase
shift in the middle and afterwards that
allows me to send a single photon in
this input for example and prepare any
one qubit pure state at the output in
principle here similarly here you've got
the reverse over here and you've got one
of these controlled-not gates in the
middle and so by setting those those
phase shifters to a thousand random
values and then looking at the
probability distributions at the output
you can see how robust and reliable this
technology is because the fidelity is
very nicely peaked right new one you can
generate Bell stage you can generate a
sort of continuum of entangled States
and perform a continuum of Bell State
type measurements and you can write sigh
inside the block sphere if you trace
over one of the qubits so you can
prepare an arbitrary one qubit mixtape
and at this point we should have a
raging debate over whether size really
the appropriate
symbol to draw inside the block sphere I
think I'm willing to accept that ro
would be far more appropriate but I
would also argue that size are much more
beautiful character to draw in there if
you'd like to draw ro in inside the
block sphere then please go to this web
address lot login to the device that
I've that I've shown you control those
phase shifters directly yourself and
draw this draw there a symbol row with
it in all seriousness this is available
to anyone with web access to log in and
start programming and using this this
small scale device and if you have
serious idea if you have any idea to do
with it including serious ones and you
don't like the the GUI that has you
dialing up the the phase shifts you know
with you with your mouse or whatever
then let us know and you know you can
plug directly into the Python script
that runs it I mean it's sort of
targeted at school children and so forth
ok miniaturization there's a promising
one that's architectural and that is to
replace those those 2x2 beam splitters
within my end beams that is directly
using a so-called multimode interference
device and n-by-n beams that is a you
know very important operation and doing
them directly instead of composing them
into a whole lot of 2x2 themes that is
this is a great real estate saving on a
chip and even greater real estate saving
on the chip is to go from this silica
devices to silicon devices so in all of
these silicon devices that I've shown
you the size of the device is largely
dictated by the refractive index
contrast between the core and the
cladding which determines how tightly
the light is confined in those wave
guides and that in turn tells you how
fast you can go around the corner so
just as if you take an optical fiber and
you bend it enough the light comes out
so too if you go around the corner too
fast with these wave guides the light
will spill out and the minimum Bend
radius in all the devices that I've
shown you so far is of order 10
millimeters and that's why we have these
relatively large chips in these silicon
devices here that minimum Bend radius is
one micron and so the component density
increase is then a million fold in going
from these silica devices to silicon
devices which is a much bigger step in
fact than going from the bench
up to the first chip devices silicone is
appealing for all sorts of reasons this
is this is just just an aside that I
like so here's your silicon waveguide
your single photons are indeed bright
light propagating along here is some
micro ring resonators that are coupled
to that waveguide with a bragg grating
wrapped around the inside of it so that
the so that light is then emitted
vertically and this could be a solution
to chip stacking with photonic
interconnects or photonic vias between
them whether that's you know in the
classical or quantum world and I think
that's a really important architectural
issue is you know how we get light into
an orthogonal direction but what I want
to spend a few minutes talking about now
is the rest of the story so all I've
talked about really so far is light
photons flying around in the waveguides
on a chip but we of course have to get
photons in and out of there typically
using fibers we have to generate those
photons typically using a process called
spontaneous parametric down-conversion
whereby we send a bright laser beam into
a nonlinear crystal such that with very
low probability one of those photons in
the laser beams spontaneously splits
into two daughter photons conserving
momentum and energy now that's a very
nice approximation to two photons and if
you detect one of the photons you know
that the other ones there with certainty
because they're born in pairs but it's
totally useless in terms of making a
scalable quantum computer because it's
spontaneous and so you have no control
over when that event happens or whether
that event happens or not and I'll come
back to explaining a solution to that in
a moment
we of course detect the photons using
either semiconductor or superconducting
single photon detectors do some sort of
pre-processing and then ideally we
feedback what-what we learn on on to the
system itself and so needless to say in
the work that I've shown you so far the
whole quantum optics lab hasn't shrunk
to the scale of a chip because of what
all of this surrounding paraphernalia
still fills it and what we'd like to do
is head towards this where we have all
of those components integrated so you
know those nonlinear sources integrated
it's a single photon detectors fast
Reuters and so forth and here's a
particular
of what we'd like to do with that and
that is a mechanism or an architecture
for making those non-deterministic and
therefore useless single photon sources
into deterministic single photon sources
that will do the job for full-scale
quantum computing the idea is a very
simple one and that is that let's say I
have this source here I send my bright
laser beam in I send a pulse in and
there's a low probability let's say 10%
of producing a pair of photons in that
given laser pulse and any other laser
pulse if I have any of those sources all
pumped with laser pulses in parallel
then the probability of none of them
producing a pair of photons is
negligible right for any decent decent
end and then my task is simply to take
one of the seven or 12 or whatever it is
per pulse of sources that produced a
pair of photons and detect one of the
pair which tells me with certainty that
the other one of the pair is there and
then based on that use this in by one
switch to switch that photon into this
output and so you've turned then at
least in cartoon form a you know a 1 or
10 gigahertz train of laser pulses into
a 1 or 10 gigahertz train of single
photons with very high probability now
this looks like a brute force
engineering solution to the problem and
I totally agree that it is I also think
it's a very very promising one and it's
promising because we have very nice
control over the photons we can generate
beautiful photons in this process
dispersion engineering and phase
matching engineering allows us to
produce very nice photons that interfere
with one another and is suitable for
these purposes and then furthermore all
of those things are sort of mass
manufacturable and and scalable that's
that that's the argument that's not to
say that there aren't plenty of
engineering challenges in there but once
you've got once you've got you know a
handful of these things then scaling up
to hundreds and thousands and so on
which should be relatively
straightforward the final point is that
once you've solved all the problems of
this you've in fact solved all the
problems for everything that you need
for a full-scale quantum computer
because that's that contains all the
elements that you need
for the aficionados in the audience this
is your menu where you choose one of
these approaches for the sauces you
choose one of these approaches for the
detectives and so forth
I won't go into that in any sort of
detail but the point is that there are
solutions out there that have been
demonstrated I'll talk about some of the
things that we've done and some of the
things that other people have done is a
ring resonators source of photons in in
silicon waveguides we've looked at
lithium tantalate and also charcoal John
I'd plenty of people around the world
have done all sorts of demonstrations of
generating photons in nonlinear wave
guides on chip the fast switches for
that that end by one Ruta that I showed
you before
well I've shown you these thermal phase
shifters which are which it too slow and
in the telecommunications well lithium
niobate modulators that operate at 40
gigahertz have been them have been in in
use for for a long time and we've used
that same sort of technology to rapidly
manipulate paths and polarization of
single photons in terms of detectors we
really like these superconducting
detectors which operate at at around 3 K
in a closed cycle system and that's the
only thing that's that's unfriendly
about them they have very low dark
counts very low timing jitter in very
high efficiency this is just a step
towards that vision of multiplexing
where now I've got two sources on a chip
with one another here and we see quantum
interference between photons generated
in those two sources in this this beam
splitter on the chip again with with
unit visibility or unit fidelity to
within very small error bars and I think
that's a it's a very key step towards
this you know getting getting you know
hundreds of these sources running in
parallel this is some work by other
groups showing those superconducting
detectors grown directly on gallium
arsenide and silicon waveguides in the
key point is very high efficiency
because you can mode match directly to
those detectives this is some work from
Caltech showing them very long low loss
delay lines which could be a very very
useful addition to the
Vox you'll note that in the pictures
that I've shown you got a bright laser
beam coming in one side and single
photon detectors sitting at the other
side so you need a lot of attenuation of
that laser beam you know a hundred DB of
attenuation and again there's promising
results out there for that and so back
to this cartoon that I started with all
of these elements have now been
demonstrated to the sort of performance
levels that are required in isolation in
some combinations and now the task is to
you know maintain the you know maintain
those performance levels as you
integrate the whole thing and
manufacture the whole thing to give you
a flavor of how the computation proceeds
here's the physical photons in
waveguides and what you do is you
attempt to fuse them into a cluster
state and because you don't have these
deterministic interactions that fusion
doesn't work with unit probability and
so you end up with a giant cluster state
that has a lot of holes in it but you're
above a percolation threshold which is
which is a phase transition so you know
with certainty that in some given volume
you've got you know in an entangled
string of qubits in each direction and
then you essentially renormalize and say
alright that block is my qubit and then
i renormalize into a rouse and off
lattice and from there for words it's
exactly the same form of fault-tolerant
topological cluster state quantum
computing as as anyone who's pursuing a
gate model this is looking at so the
price that you pay photonic lis is that
you've got this this additional step
here and then thereafter it's the same
now the the points the final points to
make well firstly show you how you might
imagine doing that on this you know in a
smaller cartoon so here's a bunch of
multiplex sources ring resonators you
see four of them you should imagine four
hundred of them and here's eight of them
only for them are depicted but you can
see there where the other four would go
you run that into a linear optical
network here and you take your
unentangled single photons and you
entangle them into this star cluster
state here the success or otherwise of
that circuit is is is given to you by
the detection pattern that you get at
the output here and then you multiplex
this whole thing until you you're
generating these
with near unit probability and then
everything's ballistic thereafter you
simply attempt to fuse those things into
the cluster state you do that
imperfectly but you're above a
percolation threshold through that
cluster and then you proceed as per here
and you get these you know the usual
very nice fault tolerance thresholds
probably that there's this a couple of
things to say and I'll start with the
most important thing the physical depth
so you imagine this computer then looks
like a bank of single photon sources on
one side and a bank of single photon
detectors on the other and in between
them is a is a big slice of this cluster
state with you know the dimensions on
the other in the other direction is
determined by the physical size of your
computer the length of the computation
that you do determine by how long you
run how long you run this computation
for and the point is that the depth the
physical depth between the sources and
detectives so the number of optical
elements that they go through is fixed
as you scale up your computer so the
bigger you make your computer the depth
of that thing remains fixed that's very
exciting for me because if I told you
that you know that depth grew even you
know linearly you'd probably be a bit
worried right because the bigger you
make your computer the more loss you're
going to have them you know the more
elements that you have to go through and
that's simply a function of the other
kind of you know the the local
operations that you need to generate
cluster States so that's that's very
appealing the other thing is that the
error model for this this system is not
yet known but there's good reasons to
suspect that it might be quite a bit
more benign than the normal error models
that we that we are used to and that is
because of this effective zero
temperature that I described before so
there's no intrinsic coupling to a
thermal bath so maybe those Pauli errors
that we worry so much about usually are
not applicable there's a prospect to
turn a lot of errors into loss why would
you want to do that well it turns out
that loss is the best thing that can go
wrong with your quantum computer you can
lose in you can lose 50 percent of of
your qubits and you can still do
full-scale quantum
as long as everything else works
perfectly so if you so if you can
convert everything into loss then you
have these incredibly favorable
thresholds and how might you do that
well let's say you you have a
polarization encoded cube head and it's
depolarized somehow well you put it
through a polarizer you've turned that
depolarization into loss let's say
you've got some temporal jitter in your
photon while you put it through a
spectral filter and you've turned that
that that timing jitter into loss or if
you have fast enough detectors you can
indeed do that that filtering temporally
so there's great promise I think for
turning these things into loss the only
thing that I can think of that can't be
turned into loss in that way which
proves nothing I should add that just
might say something about my brain
rather than about reality
is dark counts on those detectors now
you might believe that dark counters are
more but you know it's a more benign
error that you could deal with more
directly and more efficiently than other
sorts of errors and in fact it might be
that you could get it to this sort of
you get dark counts to the ten to the
minus twenty level where you wouldn't
encode against them at all
and I think that's that's that's
everything that I that I want to say
about about that approach I think
there's one more thing but it's escaped
me just at the minute I've probably said
enough in any case and then of course
you know this is the vision for you know
stolen directly from IBM of of
conventional computer chips in in the
future where you replace copper with
photonic wires and so with a simple bit
of Photoshop and relabeling you just
then have your photonic quantum computer
and all of the Associated micro
electronics sitting right alongside and
that's that's deliberately a little bit
facetious but not completely facetious
and so I guess just I'll just finish by
saying that you know this is this is a
way of seeing the the photonic quantum
computing Snack got all these components
down here I'd say that there's been
plenty of work done down here plenty of
work done up up here and that the tasks
over the coming years is really to worry
about things in this in this middle
region in
sensitive architectures and there's
plenty of plenty of interesting and
exciting stuff to be done all right
thank you so I would like to ask you
about two different things in one
regarding that detection efficiency what
are the practical limitations out the
existing detectors especially if you
need to do photon number resolution also
the other question is regarding this the
generation of the single photon sources
I would like to see what the scale up is
with the that kind of brute force method
that you mentioned in in in the sense of
generating a large number of single
photons at the same time you know so
making sure that it's you have enough
you know time resolution between
generation of those single photon
sources choice okay so um on the
detectors
there's reports of you know 95%
efficiency with these superconductors
and there's expectation there may be
even better already in this expectation
that that will get significantly better
it's already well within the the
threshold for this you know loss only
regime that you'd like to work in I'm
reasonably convinced that our task is to
do sufficiently compelling
demonstrations with those
superconducting detectors that you know
a semiconductor company is sufficiently
inspired to then make semiconductor
detectors work to the performance levels
required ie put the tens or hundreds of
minions into the problem that would be
needed the reason they're not doing that
now is that there's no market for single
photon detectors right quantum computing
doesn't even rate a mention this you
know a lot of biologists doing
microscopy and so on this you know
there's not a big market for these
things so I think the detector thing
again there's plenty of you know
important and interesting engineering to
be done but it's it's it's definitely a
solvable problem would say and then the
second question was about the about this
sort of the source
so the the multiplex sources and this is
brute-force approach I I guess the
promise of it is all about
manufacturability in the sense that you
know if you if you if you're making
these these wave guides using you know a
conventional process if not exactly the
same process that people in the
semiconductor industry are using to make
the you know to make their optical
interconnects and make the you know that
photonic layer that I showed you right
at the end then you know once you've got
a few of these things working together
then scaling up should be should be
reasonably straightforward
I've just shown you some you know sort
of hot-off-the-press results of two of
those sources interfering with one
another and I'm pretty optimistic in
fact I've advocated to the guys back in
Bristol why don't you just quickly make
a circuit where you have a hundred of
these sources in parallel but you only
wire up three of them the first two and
the last one and then show me
interference between all three of those
sources and I'll be willing to believe
that you know with high probability all
other sources in between well because
you know I've used this you know this
reliable manufacturing process to
generate them they don't like the idea I
think they want to wire up all hundred
of them does that answer your question
they don't quite get the point you set
on these cluster vegetable a kelp one
debilitation there's a good death dozen
so the physical death doesn't grow and
the actual circuit depth so they're
computation that you're performing is
just how long you run things for how
long you run the computation for see you
you continue on one side you jet you're
generating photons at 10 or 100
gigahertz let's say and then you're
entangling into the slice of the cluster
state and the clusters the actual you
know
conceptual cluster state that you're
doing the computation on is the physical
dimensions this way that determines that
and then in this dimension is how long
you run the computation for so it's a
physical depth so the number of elements
that you have to go through and the
point is that because that's constant
that's just a clear fixed threshold that
you have to get get to no matter how big
you want to build build your computer in
the other two dimensions which
terms the width of your computation so
you'd be a bit worried if I said you
know oh it's you know that the the
physical depth grows you know
quadratically with with this size of the
computation then suddenly you know as
the computer as the computer gets bigger
the you know I you know the the
probability of actually exceeding a
threshold we you know decreases that be
set clear well I don't wait to get to
still why the basically why the physical
depth is fixed yeah it's it's simply
because you you know to generate that
cluster state just requires these some
these local kind of interactions so
maybe this is a this is a slightly
clearer picture or maybe it's not and
and here you imagine them you know you
you're you're entangling more and more
of these photons on the right hand side
and it looks to be backwards this
picture actually so yeah so you you
you're entangling more and more photons
onto the right hand side and then you're
measuring them out of the cluster on the
left hand side so those ones you should
just ignore the great ones they measured
out and so on and so to actually
generate that cluster state as you make
the cluster state bigger in two
dimensions doesn't require any greater
depth of two in terms of operation and
then once you've got the cluster state
all you're doing is doing measurements
on a sheet at a time and feeding the
output on to the next one so you
basically are measuring the cluster
state asset is be exactly streaming the
class exactly yeah so you don't you
never have to you never have to see the
whole cost to state in existence if
you're just you know
so I I guess none of the components
scare me what what what's most
challenging where there's most work to
be done is in that multiplex that
sources us as I say and I you know the
state of the art is you know interfering
a couple of sources but the point is
that you know doing that in a you know
using scalar scalable technology to make
them holds promise so that um that
worries me a bit but I think the thing
that really worries me is the kind of
assembly and manufacturer off the wafer
so likely this you know billion cubic
device doesn't fit on a single wafer my
back of the envelope calculation
suggests that you know you might need
something in you know that this that
this thing here well this this thing
might be sort of you know but this kind
of dimension in cross-section so you've
got you know a bank of single photon
detectors here running through and
detectives at the other end so making of
making a photonic brick if you like that
that doesn't scare me
it excites me but it's that you know
it's not been done before
and it's beyond what you know what the
semiconductor industry might do and
assembling that thing optically
electrically and mechanically it
represents outstanding challenges but I
guess the point is that all the bits and
pieces themselves we can stamp out from
on silicon wafers and I think it would
be exciting to understand just what you
could do on on a single wafer you know
like how biggest scale could you get to
and could you take some some problem and
and really perform that on there you
know so I think they've been in qubit
devices that say in 24-bit factoring
machine so you know basically a
reprogram of all digital quantum
computer but I think there are great
prospects for smaller scale devices that
attack particular problems very
efficiently and where there's a sort of
a fit to the app a fit of the algorithm
to the hardware if you like at the
hardware of photonics
and I think this this some boson
sampling whilst isn't you know it isn't
clear that the there is or ever will be
a an application for it it's a very nice
example of fit to the hardware right
because that thing does work
deterministically you know you've got
non interacting bosons just launched
photons into this thing and ask the
photons to do what they naturally want
to do you know do what do what you do
best just fire through that thing and
you've got an example of things now I
think all you know circuit model of
quantum computing is a bit the other way
around where you know we we conceptually
understand how a digital computer works
and we map that onto a quantum computer
and say alright well you you B qubits
and you know we're going to have logic
gates and so on
I think something from the bottom up
would be would be pretty exciting where
you you target the problem from the
bottom
yeah yeah so this is as I say from from
from here forwards is pretty standard
you know so you just have your rails and
off lattice and you know you get these
very nice thresholds and you know if
there's a tweak or two an improvement
that comes along then we could you know
we could generate a different lattice
and so on the other thing to say is that
you know like in this picture
this is not going to be the ultimate
solution because I've conceptually
divided this up into generating single
photons with near determinism into
generating these star clusters with near
determinism and then letting this thing
evolve well I would bet a reasonable
amount of money that that's not going to
be the most efficient way of doing
things it's a way for us to understand
it now but what why artificially draw a
line down here and draw a line down here
you might as well just say all right I'm
going to start with a bunch of things
here and generate that thing again that
and that that could produce you know
orders of magnitude savings by doing
things like that and that's the sort of
theoretical work that needs to be done
now in mind</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>