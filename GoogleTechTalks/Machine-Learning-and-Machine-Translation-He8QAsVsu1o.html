<!DOCTYPE html><html lang="en"><head><script async src="https://www.googletagmanager.com/gtag/js?id=UA-114897551-4"></script><script>window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'UA-114897551-4');
</script><script type="text/javascript" src="//platform-api.sharethis.com/js/sharethis.js#property=5ac2443d1fff98001395ab6c&amp;product=sticky-share-buttons" async="async"></script><title>Machine Learning and Machine Translation | Coder Coacher - Coaching Coders</title><meta content="Machine Learning and Machine Translation - All technical stuff in one place" name="description"><meta name="keywords" content="education, coding, programming, technology, nodejs, mongodb, software, computer science, engineering, teaching, coaching, coder, learning, java, kotlin, machine learning, AI, ML, tech talks, angular, javascript, js, typescript"><meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" href="/css/font.css"><link rel="stylesheet" href="/css/bootstrap.css"><link rel="stylesheet" href="/css/style.css"><link rel="stylesheet" href="/css/coder-coacher.css"></head><body><div class="container-fluid"><h1 class="site-title"><a href="/">Coder Coacher</a></h1><hr><h4 class="site-subtitle text-right">Coaching Coders</h4></div><div id="amzn-assoc-ad-99d6751e-2392-4004-ad16-73aa8385d9d0"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=99d6751e-2392-4004-ad16-73aa8385d9d0"></script><div class="post__breadcrumb"><div class="container"><ol class="breadcrumb"><li><a href="/">Coder Coacher</a></li><li><a href="/GoogleTechTalks/">GoogleTechTalks</a></li><li class="active">â¤µ</li></ol></div></div><h2 class="post__title"><b>Machine Learning and Machine Translation</b></h2><h5 class="post__date">2008-10-24</h5><div class="container"><div class="video-responsive"><iframe width="560" height="315" src="https://www.youtube.com/embed/He8QAsVsu1o" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe></div><div class="post__text">you happy to postdoc if you know skipper
improve it the language group mission
translation group yes the mighty empire
will be talking about some interesting
books from this year's lips as well as
HDL a DM coverage yeah okay so can you
hear me hum it's the motto it's
recording okay okay so mishchenko sit on
Joe Blanton Swick that I've been doing
at University of Edinburgh with my
collaborators Myles and Trevor and okay
so what are we talking about today so
obviously I'll start with some
introduction and I'm gonna motivate the
work we've been doing to apply ideas
from machine learning to machine
translation and to two bits of work in
particular the first using
discriminative modeling techniques and
the second using generative Bayesian
modeling techniques and I'll I'll I'll
motivate why we might be interested in
both okay so so this talk is going to be
quite sort of empty heavy and machine
learning heavy at the same time so if if
I assume too much please stop me
okay so just start with some motivation
so I see the current approach to machine
translation all mostly fitting
statistical machine translation fitting
within a dominant paradigm in which we
we have a pipeline of processes that we
go through and we've we've sort of come
up come up with this over time I sort of
hand engineering heuristic techniques
trying things and when they work keeping
them when they don't not doing them so
the current sort of approaches we start
with a parallel corpus we train a
cascade of word alignment models
something like the IBM word word
alignment models we learn word
translation models but then we we throw
away those translation models and just
keep the alignments
then we have heuristics for taking those
alignments adding extra points combining
them until we end up with it with a
phrase table or a grammar in the
synchronous grammar sense okay so then
we have more heuristics for scoring
those phrases we throw the lot into a
minimum error rate training regime and
at the end we have a translation whirl
and although this works and gives us the
state-of-the-art for for machine
translation it doesn't really give us a
well-defined model of translation in a
probabilistic sense or in a machine
learning to mean learning sense and
pipelines like this can make life
difficult when we want to extend them if
we change something the results can be
unpredictable if we make the word
alignments better often it makes no
impact at all on the TN translations so
we'd like something more well-defined
you know in a more thing since so I find
argue that something more like this
might be appropriate which we think of
translation as grammar induction or if
you're talking about phrase based models
phrase table induction and then
structured prediction so where we have
where we train a model to do the
translation task so what I'll talk about
today is these two these two ideas so
first I'll talk about
discriminative training a model so given
a grammar and a parallel corpus using
discriminative training to train a
translation model this is a difficult
machine learning task so it falls under
the heading of structured prediction
structured classification and that's
something that's that's quite popular at
the moment in machine learning but our
current models are limited to quite
simple structural tasks like post
tagging parsing with the correspondence
between the structure and the and the
underlying data is is well known and not
very ambiguous in translation the the
relationships
very ambiguous we don't know what the
process is to go from a source sentence
to a target sentence so we have that we
have the input in the output but we have
very little idea about what happens in
between and so so we really need some
sort of sense of latent variables and so
for this I use a probabilistic model
okay so that's that's the screen lead
modeling and after that I'll talk about
some some initial work I've been doing
using Bayesian models to do the grammar
induction tasks and so my feeling is
that word alignments are somewhat
incidental to the translation process
and what what we actually want to do is
not alignment but grammar induction and
I'll talk about that now talk about how
how we could apply some nonparametric
Bayes in techniques to doing that okay
so we'll start with the discriminative
modeling so all the work I'll talk about
today is based on synchronous grammar
models of translation it all essentially
applies without too much difficulty to a
phrase based finite state based models
let's use some synchronous camera models
because they have nice polynomial
algorithms and they also have nice
correspondences with monolingual parsing
there's a huge body of work in in
parsing that we can draw upon to to
inspire machine translation and also
there's a plenty of state-of-the-art
results using synchronous camera so we
know that it's possible to get good
result so this is just an example
grammar under on the left there as
simple as simple as CFG and I'll just
run through a derivation using that
grammar so that you can understand what
what I'm talking about so at the top is
a grammar the bottom I'll run through a
derivation and I'll just highlight the
rules I'm using as I do that so first we
start with a start symbol using the the
start symbol rule using the the rule for
rewriting an X sorry really an S as an X
we can replace that s we can then use
the lexical
translation rule to to translate guilty
he we can then capture the look park
instruction in French using a gapped
rule and finally when we insert the verb
we get the full translation so this is
this is what I will refer to as a
derivation in the synchronous grammar
and it's these derivations which operon
simply be concerned with modeling and be
talking about okay so so we want to we
want to model the probability of a
derivation the conditional probability
and so here the derivation is d the
source sentence is f may be for foreign
and the target sent in today may be for
english and so here we use a log linear
model and so we have out on the top some
over over feature the feature function H
on the derivations and we normalized by
Z which is the the partition function
and that's the that's the sum over the
the the probabilities for all all the
derivations okay so given that we can
state the probability of a derivation
we'd like the probability of a
translation here you given the source F
and we get that by marginalizing out the
derivations so summing over all the
derivations because the derivations are
ambiguous so multiple derivations can
can give the same translation so so we
can get the marginal probability of the
translation by here marginalizing and
this step although not done in current
translation system we need to think
about this if we want to have large
sparse feature sets with millions of
features and so it's something we need
to take into account and that's and we
want to have large pass features it
so the most interesting thing about this
modeling and the whole reason we might
be interested in it is so we can have
interesting features so like other
structured prediction tasks the first
thing we have to do is assume that the
feature feature function capital H set
decomposes with the derivation so that
we can use dynamic programming so we
assume that the the features decompose
with the rules that we apply and we
assume that those feature functions are
a function of the entire source sentence
so for every every rule application we
can we can use features from any part of
the source any part of the rule that we
are applying and finally this function Q
that I'll get to in a moment which will
give us a non-local features so the
first two the source and the rule can
can give us a large selection of
powerful features we can we can model
syntax from the sort sort of sentence we
could have multiple syntax annotations
dependency trees and we could we could
incorporate features over all of these
things okay but the second the second
part there the function Q we're going to
use to give us non-local features on the
translation and the reason we want
non-local features is because we know
that language models help machine
translation so we need to be able to fit
a language model in somewhere so the
function Q here if those of you that are
familiar with the David Chang's jairo
system will recognize it this is just
the function he defines for the language
model and I've got a a small German
example there so this function gives us
the boundary words of a cell in the
derivation and if we have a language
model of order in then it'll give us n
minus 1 words on either side of that
that cell so if we got a bigram language
model as in this example the function Q
gives us one word on either side of the
the cell that we're looking at so in
this example we can see that at the
first step we can translate each word in
the source as a single word the second
step we can give mind two together as in
everything and which
bans the source word one two three and
when we go for the final step spanning
one to four we elide the middle of the
string and we only need to store the
boundary word so those of you that have
looked at synchronous grammar
translation this will be very familiar
to so incorporating this into our
feature functions we can now have we can
now have a language model as a feature
but we can also have any features which
combine this target context with the
source so we can look at the source
syntax and we can combine that with the
lexical context and all sorts of other
things that we might dream up like
lexicalized reordering or or many
options okay so so far it's all well and
good we've got a nice description of a
model with interesting features but the
problem with doing this is that by
incorporating the non-local features the
dynamic programs become intractable so
there's still the still polynomial but
they're a very high order polynomial in
the length of the sentence and thus
we're going to have to approximate
somehow so here I'm showing this in the
equation by saying that we're going to
approximate it with the partition
function with Zed tilde and highlighted
in red there we're going to say that
rather than summing over all the all the
derivations we're just going to sum over
a subset and we need we need to
calculate the partition function in
order to train the model and to
calculate the feature expectations that
we need to calculate the gradient okay
so the interesting thing now becomes
what what subset of derivation should we
choose and a first obvious approach to
doing this might be to use just the
standard beam search techniques that we
have in machine translation to produce a
packed chart and use that the space
represented by that cut chart to define
the partition function so I'll call that
the first approach and they call that
the beam approximation a problem with an
approximation like that and it's a
problem that other approach is like
using in best lists suffer from is that
doing that we're only getting a very
compact point estimate around the
maximum of the function we're interested
in but if we want to estimate the
function we'd really like a bit more
from the tail of the distribution so
we'd like a more scattered set of points
so that we get a better feel for the
shape and don't make silly silly
assumptions by just seeing the peak of
the distribution so a suggested
alternative is to sample derivations and
so what I'm proposing is that we we
first train a model without the
non-local features which I call P minus
LM and we can do this exactly it's a
cubic s cubic complexity and then we can
draw samples from that model and we can
pack those into a into a pack chart
representation and use that to
approximate Zed and the hope here is
that although well most of our samples
are booked on a peak of the distribution
we'll also see some samples from the
details and maybe this will give us a
less biased our sample less bias
approximation so a diagram diagrammatic
representation of what I'm talking about
so in the top left we have a German
example a sample derivation so this is
what we've sampled from our our - LM
model on the right we have a packed
chart representation of that so we take
the sample we create a pack chart
representation where the it's a similar
to the represent representation I used
previously with the language model
context and the spans of the cells
represented there so we sample a second
derivation and again we we add that to
the the pack chart and there's quite a
bit of overlap so we only need to add a
couple of extra extra chart cells and so
now we have an efficient representation
of those two derivations and then we can
see that we actually in that in that
packed chart we have a third derivation
that we never added so by doing this
sampling procedure we're building a
space a much larger space of derivations
and we're actually adding and
do we know what happens is something
doing here because I mean in the way you
you they're not adjusting the parameters
the training trainees model so when you
remove the part that has to do with
information provided by the language
model I it's not clear to me where the
distribution is going to be it's kind of
broader or narrower because it's sort of
you it there's gonna change so it's yeah
well I get that but the question is what
kind of changes the changes favorable
unfavorable I think this sort of this
situation that Oh bite sampling from the
without language model maybe I hit this
things near the tail and then I sense
them with the language model but there's
your training then you're taking
advantage of the language model it's not
clear to me that when you remove the
light not only to make things some
things I've probably also make some
other things more problem yeah so it's
not clear to me that by removing
language you want what you get with
sample without language model later so
now the tray so what we get is we can't
sample with the language model so it's
just the best we can do so well yeah
although we suffer the same problem with
the beam search because we need
parameters to beam search with so again
we need some starting point it's I agree
it's definitely an approximation so it's
really whether the empirical results
support it is there something a toy
problem where you can actually have a
precise answer to this question sort of
synthetic gigabit well they give us some
intuition is the answer would depend on
the toy problem so there would be very
problems for which you know it depends
on how important the language model is
compared to the the underlying
distribution the more the divergence
then the more it's going to matter
that's a different distribution that
you're sampling from the if you wanted
to do this exactly you'd use a
metropolis Hastings algorithm and you'd
have an acceptance step and I've tried
that but it's really slow yeah so this
is trying to strike a balance between
fast and exact sir there's sort of a
bunch of different ideas like this
hopping around not only machine
translation within other areas where
intractable in France in structure
prediction will arrive and everybody's
doing things and I have no confidence
that any of those things are good bad or
indifferent and I don't know no one
seems to understand what the map the
line map is ya know it's very true
there's not a theoretical analysis so it
is reliant upon empirically whether it
gives us good performance basically very
the sampling density to get a better
idea of whether you're getting the right
direction along so that that's what
something like I'm in trouble of say
stings stick would give you it would
allow you to to try and sample more from
well we would lay the sample from the
correct distribution rather than the
right obviously just sampling tends to
infinity your cutter basically does get
attention you're kind of converging on
the exact model
we're not no because we're not sampling
from the correct distribution we're
sampling from the model without the
language model so it's very true that
it's not it's not the exact same
distribution yeah and there are ways we
could buy it to be the correct
distribution but it imposes a heavy
computational cost so I mean it's two
ways to think of it one is to think of
it as trying to approximate a proper
sampling procedure and another is to try
and add a bit of noise to beam search
like approach to try and make it more
stable so depends which angle you're
coming from but I agree that I think we
could do a lot more analysis of trying
to learn what a good ways to do this but
it'll becomes difficult with them
dealing with very large problems is
there something in the back forest
construction that makes you get a new
generation because I think that it's
possible to construct the forest in a
way that it just has these stuff you put
in yeah you could choose to do that and
you would end up so if you thought of it
conceptually is using an infinite
language model order so that you've
never alighted the strings then then you
would not get anything that you hadn't
inserted but as soon as you start
alighting the the strings you will be
able to get other derivation didn't it
this is the sausage in the speeds right
sausage yeah okay okay so so we've
talked about approximating the partition
function and we need that for training
and this is our training objective it's
a it's a relatively normal looking log
linear model objective where we have a
likelihood in the prior term I haven't
talked about the prior today but there's
a more about that in a minute
Eamonn LP paper so the procedure we go
through is to first train this model
without the language model use that to
drive either the beam search or the
sampler we create these pet chart
representations and then we train the
model with the language model features
using these these packet representations
and standard dynamic programming
algorithms ie the the inside-outside
algorithm
okay so that's the description of them
all so the experiments we've done so
what I've done is taken the Ida BLT
chinese-english data which is a smallish
data set it's got about 40,000 training
examples I'm using the Jairo rule
extraction heuristic to give the
underlying grammar and because that is a
heuristic there's no generative model
there it doesn't guarantee that the act
it doesn't guarantee that it covers the
training corpus so there's source and
target sentences in the training corpus
that we can't produce from that grammar
and so we lose 2,000 sentences for
training so I have those test and
development set and so one one notable
thing about this some this corpus is it
has very large number of references 16
one one reference was from the original
translated corpus and the other 15
reference to the paraphrase this creates
some some funny effects in the with blue
and the brevity penalty that I'll talk
about in the results and the basic model
I'll talk about here just has a very
simple representation is one feature on
each rule a word penalty in a language
model and really with this very simple
model we're just trying to learn or
trying to work out whether the model
will learn a sensible translation model
given a very simple representation and
so when we when we evaluate this model
it's on the development data if we look
at now to say okay we first look at the
model without the language model and the
model with we can see that there's quite
a quite a difference this is encouraging
it implies that we've managed to
leverage the information from the
language model and now our
approximations are working in some sense
to estimate that language model feature
weight and the word penalty in there the
other features if we compare the two to
approximations there's there's little
difference between them so the results
are quite similar
however the if we take the sampling
model it's actually producing charts
which are roughly half the size of the
beam search so
it's much more computationally efficient
so with a much smaller set of set of
derivations it's producing a better
estimate so I'll use that I'll use that
approximation in the next experiments so
if I put them to it including a language
model can you estimate so comparing
those two solutions right so you have a
chart being searched chart parse forests
and they have a sample based something
base far as forest can you do some
statistics under those two comparing
those two to see what what's happening
you can do so I mean this is given the
broad high-level statistic of 50 minute
smaller you could actually count the
number of derivations and look at the
space that's in there I'm yeah so I have
the answer is I haven't done those
statistics but you could sure it'd be
tractable to also look at the actual
lexical diversity in the charts as well
so the aim here is that the sampling
chart will have a lot more diversity in
it but I'm just assuming that from the
result and the smaller chart okay
now if we move to the the test set and
compared with the standard so the the
MIRR train model and this is a stamp
David Chang standard Jairo model with
all the features it normally has such as
Alexa codes as model one smoothing and
such things and I include both the NIST
and IBM interpretations of the Bertie
penalty in blue so we see that when we
look at the NIST brevity penalty results
the mert model comes out looking quite
quite good
so the NIST miss brevity penalty takes
the shortest reference in the set of
references because we have sixteen
references and they were paraphrased we
basically get an even distribution over
short and long so the short is it's
quite short so that that model is
producing very short output when we look
at the IBM metric which takes the
closest we see that there's as a closer
relationship between the result and the
final result is just using the single
reference from the original corpus and
that's a scenario that that the
probabilistic model was trained under
because it was just trained on single
references it has no sense of brevity
penalties it just tries to produce
translations looking like the corpus and
on that under that scenario it comes out
quite comparable it's very difficult to
compare models like this they're doing
very different things they're optimizing
different objectives so my argument here
is not that one model is better than the
other it's just that we can be confident
that our models in the same ballpark
it's producing state-of-the-art result
you'd expect the IBM to come up then
yeah you could run up with all but you
know as I said I'm not arguing that one
is better than the other because I don't
think that comparable we really need to
look at a human evaluation or some
searching but our models not it's not
optimizing blue at all so it's we're
quite happy that it's coming out with
reasonable police calls okay so so
that's the basic model and we've shown
that we can we can get state-of-the-art
results so now we'd like to add some
interesting features and see if we can
still estimator a good model
so the an obvious starting point is to
add supervised syntax from a source
syntax tree now there's been a lot of
work incorporating syntax into into
translation models and it's usually done
by inserting the syntax into the grammar
of a synchronous grammar based model so
here we're taking a different approach
in just treating the syntax as
conditioning conditioning context so
we're separating the search space which
is defined by the grammar from the
parameterization and this has advantages
this means that we don't have to worry
where translation units don't correspond
to syntactic constituents we don't have
to have heuristic ways of dealing with
that in that case the features just
aren't present so we're going to extract
rules which correspond to syntactic
fragments and this amounts to about four
million features and so in total we'll
have about seven million features in
them all so an example of what this
looks like
please forgive my anglicized chinese
path tree so a simple derivation the
derivations representing the top right
there there's two steps in the
derivation first we translate the the
noun phrase for a currency exchange
office and then the question part of the
sentence and so in the first step we
extract a syntactic feature which says
that for that translation pair the
Chinese source was covered by a noun
phrase and so they
the translation pair can now be
conditioned on that that property that
there was a noun phrase present in the
second step we can have the feature
which says for the question rule the the
variable exit we're incorporating
corresponds to a noun phrase and the
whole lot corresponds to a question so
this allows the model to learn a bias
for corresponding to the past tree if it
thinks that the past tree provides
interesting information it can learn to
use rules which are correspond to
constituents so when we throw those
features into the model we get slight
improvements but nothing dramatic so
this this might seem quite reasonable
that we give some additional features
and we see see some improvements for me
actually this is quite quite positive
because I was expecting more than the
model would go down so the features
mostly are all noise because forming and
syntactic features very few of them are
going to hold any information at all so
all we're really doing is throwing a
huge amount of noise with a little bit
of signal in there somewhere and the
fact that the malls actually found a
little bit of that signal is quite is
quite encouraging to me yes there is a
have to occur twice yep so Singleton's
are removed yeah otherwise you'd get a
great deal
okay so you get a great deal of
preachers yeah but she's a cop
additional issue you were you are some
irregularities yes yes so there is some
stuff going on with the regularization
it's basically a minimum divergence
style model where we are
so again when we're going from the model
without the language model to model with
I train it essentially as a minimum
divergence model from the original model
because when we do that approximation of
Zed we lose a huge number of the
features because we're not considering
all the derivations now and their
weights would go to zero if we didn't do
something and that that was quite
effective and it's it's it sort of makes
sense that when we add the translation
model we we expect things to get better
but we don't expect the model to change
dramatically it should still the weights
on the translation rule should still
stay reasonably within the same range so
that's um so I presented the discredit
model so now I'll move on to some work
with a generative Bayesian model so what
we'd like to do here is so as I said in
that previous model we were using the
Jairo grammar but really we'd like to be
able to induce the synchronous grammars
without having to rely upon word
alignments such things so that's our end
aim there hasn't been a great deal of
work on inducing synchronous grammars
current approaches in translation are
based on heuristics such as the jairo
heuristic or other ones using sometimes
using supervised parse trees they have
problems say they produce massive
grammars hundreds of millions of rules
even for medium-sized corpora most of
which are just noise and they don't as I
talked about previously we have this
reach ability problem where they don't
actually guarantee that they'll model
the data they were extracted from so so
we'd like to we'd like to treat this as
a as a grammar induction process and
induce synchronous grammars and so we're
going to try some some Bayesian
techniques inspired by
ideas in monolingual parsing so to come
back to that example so in this in this
section on the generative models I'm
going to restrict what I'm talking to
talking about to itg models and they're
models where synchronous grammars where
where we either have purely lexicalized
emission rules or we have binary binary
productions so maybe we'd hope to be
able to learn ITD's like this where we
have we can learn the correspondences in
translation but some sort of labeling on
the Constituent within the derivation
and maybe we can learn that a certain
certain labels for our constituents have
a higher probability of reordering or
occurring after or near in combination
with other labels so this is the sort of
thing that ideally we might like to
learn and so in these models we have
three types of three type three types of
rules we have monotone productions that
is a parent produces two children and
the order on the source and the target
are the same
reordering productions which shown with
a diamond there where the again a parent
produces two children but the order on
the target is reversed and then the
third type of rule is just the emissions
so there are the leaves where we produce
the actual phrasal translations so what
we would like to do is take some of the
work in monolingual parsing and here the
model of an Percy Liang
applying hierarchical diversity
processes to modeling a pcfg and extend
that to work with a synchronous grammar
so extend it to two dimensions and
hopefully we can use the stick breaking
prior construction from personally
Aang's work to have a prior over these
constituents and hopefully learn an
interesting set of constituents remodel
at the same time as learning the the
translation correspondence
so now I'll start talking about these
these Bayesian models so this two main
building blocks that that were using
this small the first is the stick
breaking construction and that's our
prior over the number of constituent and
so what it does it gives us a a sparse
prior distribution over an infinite
number of constituents and the way we do
that so we represent this with a stick
so in the middle there we have this idea
of a stick and it has unit length so
it's links 1 and we break off bits as we
as we produce the the prior and we
expect a ssin of the lengths of these
broken sticks is decreasing so sometimes
we might break up a bigger one than the
previous but the expectation over time
is that they'll decrease and that's
parameterised by a hyper parameter alpha
and this is produced using a piece of
distribution so we drew draw these pie
pie - PI prime if you're like parameters
from a beta distribution we then break
them off from our unit link stick and
scale by the remaining length and you
can see that that will give us a
decreasing or an expectation of a
decreasing length and we can keep doing
that to infinity so that's what we used
to give us a prior over there the labels
using that prior those those pi values
we get within so what we need in a in a
grammar base add NS CFG is a
distribution over pairs of labels
because you know model we have this
source sorry my source the parent
category producing a pair of children
labels so we need a distribution over
pairs and we do that by taking the cross
product of the PI distribution so we
take the infinite PI distribution take
the cross product to give us a
distribution over pairs an infinite
distribution of appears and we use that
as the base distribution of a dershlit
process ok so it's next like it's a
little bit heavy
but I'll walk through it slowly and try
and give the intuition for the model and
I'll follow this up with a diagram - to
try and help so on the Left I'm going to
describe the generation process from
this grammar and on the right I'm going
to highlight the priors which are being
being used and what I want to do is
highlight the hierarchical relationship
between the priors so to give an
intuition of how they're sharing the
same distributions so at the top we
start by drawing a start symbol and
we'll call it a Zed one so we'll label
the the nodes in the treatment as did we
first draw a start symbol okay sorry
first I'll say something about the
notation so that the bar here indicates
independence
so given Phi s then these ED draws are
independent from the multinomial so
we're drawing Zi from a multinomial
parameterised by bias in the in the
first instance then on the right we can
see that highlighted in red there on the
second line bias itself is distributed
according to address a process so that's
a sparse prior over this infinite number
of categories and the categories come
from the top-level stick breaking Pro
criar so we have a two-level prior
distribution going on here we have first
the direct process and then it itself
draws upon the infinite set of
categories defined by the mystic
breaking Parral and that top-level
distribution is how we link all these
things together okay so once we have a
start symbol we then need to draw a type
of our rule so again I said we had three
types of rules
monotone reordering and omission so the
first thing we need to do a draw as a
type the type is somewhat simpler it's
prior distribution is just a finite
directly a distribution because we only
have three types there's no infinite
things going on here so so we draw we
draw Tite from a sparse distribution
over over those three times
so given that we may have drawn a
monotone production and here we see so
the type that the pie of subs er this is
the distribution of the possible types
for the penny for Zi the terminal Zi
that comes from the single set there's a
single bearish way of a wah
super Y so that means that you don't you
have single prior to you thinking about
each of those
so the rule type distribution just all
drawn from the same right
no no no super pro satisfies it so
there's there's a for each different one
for each for each subscript yes but they
all drawn they are all no they all die
strong from the same that dice factory
they all have the same type of parameter
okay
so monotone productions here in this
somewhat cumbersome notation we have Z 1
and Z 2 having the same order on the
source and target we draw that from a
multinomial and again we draw its prior
from a drill a process
this time however because we're drawing
pairs of constituents the base
distribution of the dershlit process is
that cross product that I talked about
so that gives us a base distribution
over pairs of constituents but then at
the top level it's linked to the same
stick breaking prior that the start
constituent distribution was so they're
both the stick breaking prior is
allowing us to draw for these
distributions they're all drawing on the
same set of constituents so they're all
sharing the same inventory of
constituents and so for the reordering
rule is it's essentially equivalent to
the monotone production except that we
have a separate again we have a separate
multinomial a separate delay process but
the same top level stick breaking prior
and finally for the emission
distribution where we again draw from
address a process so he can think of the
infinite process being every possible
translation pair that we could that we
could have so every source and target
phrase and so we have an infinite
distribution over those and its base
distribution is some prior p0 that I'll
talk about in a moment but we can we can
parameterize that prior distribution
over phrase pairs so that's the basic
statement of the model in a somewhat
complex form
the Alpha Y sorry the superscript is
just it's not a lot of power sorry
so the elf all the alphas are hyper
parameters sorry so they're all hand
handset parameters so yeah we have we
have the hyper parameters alpha s we
have top-level alpha alpha s alpha Y
alpha m alpha they're all hyper
parameters for tuning the price
that's a broken
ooh sorry
yeah battery from the flat okay
so I'll run through a an instance of
generating with this this model so as I
mentioned previously first we draw a
start symbol from our inventory of
symbols and we'll give this the label 1
we then draw a type for that symbol and
the box indicates that we've drawn the
type of a monotone production so from
the the multinomial therefore the model
for the rule types we then we then draw
for the monotone production two children
from the that distribution have the
prior distribution using the cross
product of of constituents and then for
each child we draw again a rule type in
this one we draw on a mission and we
draw a pair of source and target phrases
in this case just question marks and we
continue through this process so we draw
a reordering reordering rule type we
draw it's two children and we keep keep
going through this process during their
missions so there we draw the the
translations for the current year
here I've accounted for the reordering
that's happening at the the constituent
label too and you can see we might see
that the model is reusing categories
because it has a prior to to bias it
towards reusing categories that's
already seen over introducing new
categories and so we keep generating
until we have a complete translation
that's the basic process that we we're
modeling
so when I talked about the emission
distributions I said that there was a we
have the ability to introduce a prior p0
and so I'm just going to talk about
three different options that we might
try for that obvious one is just to have
a uniform prior over phrase phrase pairs
the second one is to draw on the IBM
translation models and we can use IBM
model one to give us a prior over phrase
pairs and the third one is to use the
heuristic relative frequency estimator
that's most commonly used in translation
to give us the prior distribution so
model one should be reasonably familiar
to many of you the only interesting
thing here is that we actually need a
joint distribution so we have a language
model in that prior as well we just use
a simple unigram language model that
gives us a joint probability for all
possible phrase pairs and the thing to
note about model one is it has a bias
towards short phrase pairs so we expect
that the longer the phrase pair the
lower its probability is going to be so
the relative frequency estimator it
simply taken a word aligned corpus and a
phrase extraction heuristic just count
the number of times phrases are
extracted and that's what I symbolize
here in that equation with C being a
counting function here again we're using
we want to Joint Distribution so we're
normalizing by the total number of
phrases we extracted rather than using a
conditional model which is more popular
in machine translation and the thing
about this estimator is it's biased for
longer phrase pairs so this is a common
property of current phrase based or
synchronous grammar translation systems
actually have a strong bias towards
using long phrase pairs
so we need we need some way of training
this model and that can be difficult so
by having all these distributions and
taking the Bayesian approach of
integrating out the parameters what
we've done is induced dependencies
between all the distributions so there's
no independence so there's no efficient
dynamic program that we can use to to
estimate the model so an exact solution
to solving such a model is to use
something like cubes sampling and this
is popular it's quite slow and has some
take some hands you need to get to work
what I'll do instead is use a popular
approximation called the variational
mean field approach and I'm not going to
describe this in detail it's quite it's
reasonably complicated but the intuition
is is quite quite straightforward so
I'll just describe that and so what
we're going to do in our proximation we
want to estimate the probability
distribution P but we can't do that
because of all these dependencies so
what we're going to say is that we know
of another distribution Q which does
decompose between the parameters theta
and Z is the the trees for our the
derivation trees for our parallel corpus
so we know of another distribution that
does decompose and then what we can do
is try and minimize the divergence
between the model we want P and the
model that we can actually estimate Q
and hope we're hoping that that will
give us a good approximation of the of
the model we try to learn obviously
there's a lot more to it than that
Bitola o course over that for the moment
so what this looks like it essentially
looks like am training for anyone that
is familiar with with am training for
maximum likelihood and I'll try and show
you why it looks it looks like that even
though it may look quite different this
equation at the top so the process is
that we go through a process of we have
what we do we define rule weights for
all our for all all the rules in our
grammar and this is just an example rule
rest emotion equation at the top which
I'll describe
so we have rule weights and we go
through the process just like a.m. of
calculating expectations on our parallel
corpus and then our es tomato rule
weights and weary estimate using this
function so this is just for the
reordering distribution we have similar
equations for the monotone and for the
emission distributions are all
essentially similar so now if I just
highlight in red there we have the
counting there or the expectations
that's basically what you're doing again
so inside there is basically a normal en
update step and all we've done is added
some extra this dye gamma function and a
term from the prior which looks very
much like additive smoothing or
something like that so basically this is
if you have a if you have a
implementation of an iam algorithm
turning it into variational algorithm is
incredibly simple you just add the
diagram of function and the terms from
the prior there's a third step but
besides expectations and Resta mayshen
that that's not that doesn't have an
equivalent in the a.m model and that is
we need to re s tomates top-level stick
breaking prior and there's a there's a
somewhat complicated non convex function
that you need to optimize to do that no
won't we're going go into that here but
it's it's computationally it's not
expensive because over a small number of
parameters so we can do that just like
in the IAM algorithm we can use the
inside-outside algorithm to calculate
the the expectations for the trees so so
i've described the model and well that's
what the process is sorry so that's what
the variational training is doing it's
it's learning the Q distribution so we
can we can derive those model update
equations by making that assumption of
decomposition and then going through
some steps so we get we can you can
bound the likelihood the posterior are
using Jensen's inequality and in the end
you get out those nice am like equations
so given the the model I've described
I'm going to talk about some experiments
we're still this is still quite early so
our experiments are still somewhat
exploratory but first I'll describe a
synthetic experiment trying to recover a
simple thing synthetic grammar using the
model to see if it behaves like we'd
like we'd hope so we have a very simple
grammar we have a single production
which produces sorry a single binary
production s which produces A's and B's
we have a reordering rule there with the
B's so we have two monotone s rules and
one reordering and then for the Oh B and
C they each can emit a source and target
symbol from those sets so what we do is
we randomly generate 10,000 10,000
examples using that grammar and then
train our model and we also train a
maximum likelihood model using the M
over them and see what happens so here
in blue is the Bayesian hierarchical a
process model and in red is the maximum
likelihood model and on the left is the
binary production distribution and on
the right is the emission distributions
so if we just look at the left to start
with so remember there is one binary
production rule s production category s
and we can see that the from the blue
bar there that the Bayesian models
allocated that to category two and it
flirt a single constituent where as a
maximum likelihood has split that over
over to two constituents two and five so
split the s over to constituent on the
right we have the emission distribution
and we can see that again the higher
code if you like process has nicely
allocated three categories for the three
different for the three different
emission constituents whereas a maximum
likelihood models got somewhat confused
it's actually combined two categories
together into one inter label one there
and split another one over three and
four so the reason this is happening is
because the maximum
my favorite model has no prior to use a
small number of constituents so it has
nothing forcing it to use a small number
so it uses all of them so it's used the
five there that it was given the
hierarchical really process model does
have a prior to use as few as it can and
therefore it seeks to minimize the
number and comes up with the right
grammar these results of course because
it's a non convex optimization are
dependent on the initialization point
and if you initialize the maximum
likelihood model with the correct
solution it will stay there but if you
don't it will probably not find the
correct solution the hierarchy will
directly process because of the priors
tends to always find the correct
solution although I couldn't guarantee
that it did every time I tried it okay
so moving on to a real translation task
I'm going to reuse the experiments I
described earlier from the iw SOT data
have basically the same setup as those
previous experiments so something I've
not talked about so far is that the the
downside of this nice variational
training approximation is that obviously
we cannot represent an infinite number
of constituents so we have to
approximate that somehow and that's
called the truncation of the
distribution so we have to choose an
upper limit on our approximation so
that's it's not like in the maximum
likelihood model where we're actually
allocating that number of constituents
we're just setting an upper limit on the
approximation the model can still choose
to to use less so we need to translate
that number of constituents and in these
initial experiments we haven't gone very
high so I'll do one experiment where we
just have a single category and then we
just look at what the emission
distribution is doing and then an
experiment where I'll use five
constituent labels we also need to
truncate the phrase translation pairs
and in these experiments I'm just using
a standard phrase table from the
translation system Moses this is not a
limitation of the model this is just how
I did these initial experiments
obviously we would want to remove that
in a in future experiment
and again we have this problem of
unreachable 'ti and we discard about
7,000 of the training sentences which we
can't model with that grandma so a quick
comment about some of these hyper
parameters so there's quite a number of
five parameters and I'll just look at
one of them which is of interest is the
rule type distribution so a major
motivation for having these priors is
that if we train a maximum likelihood
model with with phrases it'll find
what's called a degenerate solution so
the maximum likelihood solution is to
place all the weight on the longest
possible phrases and this is not a good
solution and if you have no limit on the
length of phrases it'll always allocate
all the probability on full sentence
translations surely array would you
would like to have your prior to favor
aggressively favored short phrases yeah
I'm sorry I'm saying that the maximum
likelihood suffers from the desired
solution once we introduce the DP so
that's why we're going to so the way so
we can the distribution which has most
effect on this is the rule type
distribution so if you imagine the the
longer the phrases we use the the lower
the depth of the tree that will result
the less internal nodes will have and
the the deeper the tree the short of the
phrases on the leaves so if the rule
type distribution is biased towards
emitting phrases we'll get will get
shallow trees if it's biased towards
binary productions we'll get deeper
trees so what we see when we read
various parameter alpha Y which is the
the the parameter on the rule type
jewish-led process is that we actually
want very high values so this is not spa
it's the opposite it's suggesting that
we're trying to flatten the distribution
and that's because the model itself is
trying to learn very shallow trees in
place all the weight it's trying to find
a sparse distribution in which it really
wants to admit phrases and by forcing it
to spread some of the probability mass
to the binary productions we get deeper
trees have is that the models actually
you get two funny hierarchical grammar
right where you don't have that which is
that sort of divergent if you do this if
you put a lot of the probability on on
rules versus term versus terminals Danny
you you might have as a sort of I mean a
classically you could push some of the
degeneracy into the grandma yeah you get
this branching process with most of
probabilities to keep keep going rather
than making terminals but the young
likelihood talking was likely it's
fighting that so the two are fighting
each other so the so the and this is
basically a hyper parameter so it's it's
we're setting this by hand to try and
find a balance on on development data so
this is the parameter for the trend the
rule type distribution every
distribution has a parameter and I tuned
tuned them all on the development data
by hand you can infer the parameters you
can have you can place a prior on the
hyper parameters to automatically infer
those which is another step again in the
in the in the modeling but here I've
just done hand tuning of hyper
parameters there is one for the rule
type distribution I just use a single
parameter for both for both productions
reordering a monotone look you could
have separate ones but I just use a
single one there's a parameter on a
stick breaking prior and alpha and
there's a parameter on they start rule
distribution
and the emission distribution does I
mention that yeah this there's a number
of hyperbolas some of them are more
important than others the rule type ones
one that's really quite important well
these models seem to if you look at the
literature people do seem to find very
much so it's often very large or very
small you get you get something like 10
to the 10 to the negative 10 or
something or maybe not that high but 10
to the negative 5 or 10 to the 5 it does
seem to be that way yeah I mean you
could say that we're it's so high that
we're just saying find a flat
distribution but then it does actually
go up and down so there's something
going on there it could also be related
to this year size of the corpus could be
talking about forty thousand sentences
and there's a relationship there in the
scale of the the parameters that we're
talking about but yes I'm not exactly
sure why we find such large numbers so
yeah the interesting result there is
that there is that bias in the model and
that we can counter it with the
real-time distribution so for some blue
scores using this model the top level is
just a p.m. train maximum likelihood
model and then we have three Bayesian
models using the different emission
parameters and this is just with a
single category so there's no there's no
stick breaking prior going on here
because we just have a single category
and we see that as we use the just using
the Bayesian approach we get an increase
in in blue score all this is without a
language model so we're trying to learn
the translation model just strictly in
the in the grammar and as we had the
prior such a model one relative
frequency we see increases again with
the the the two prize-giving similar so
slightly bias towards the relative
frequency there yeah so as I'm saying
here that the maximum likelihood so our
model is overfitting it's finding and
Fraser's so the ability to add
interesting priors like this I think is
quite a nice property of these sort of
models of the fact that we can add this
prior information into the distributions
and things like the relative frequency
estimator it's not a proper generative
model in any sense but we can sort of
still use it within a nicely behaved
model in this way as we add some more
categories what the maximum likelihood
model over if it's even more could we
give it more parameters more degrees of
freedom and so it's performance goes
down whereas the Bayesian model
essentially stays the same it's gone up
very slightly so we're not seeing a
great deal of change in performance by
by adding the categories obviously this
is very few categories five but I
explained that there are some
computational problems with going higher
so the benefits tiny I think it's a
point one or point two but we do see
that the model doesn't over fit like the
maximum likelihood model so that the
prize itself were so after to
characterize well what's actually going
on with these five categories there's
very few so we're not going to see any
sort of complex syntax going on but we
do things like that the first thing the
model will do when we give it more than
one category is allocate a calorie to
the start symbol so that's the one that
likes to allocate first and after that
it likes to allocate a set as a symbol
for the start of a sentence so we see
that a symbol comes to emit all things
like the question words like what where
there's lots of questions in this corpus
so we get a we get a symbol that models
the start of the the sentence and the
rest and we see the similar thing for
the end but that's a basically the
extent of the the structure we see and
if we are really hopeful occasionally we
see a sort of reordering category but
not not consistently so
so to summarize what we sort of found
isn't just initial experiment so my
feelings are that I mean what we're
talking about here is an extremely
simple model of constituency and if we
do really want to count capture
interesting syntactic reorderings or
something like that in an unsupervised
way we're going to have to do more heavy
modeling we're gonna have to look at
context and things like that
and in terms of the efficiency of this
model doing this with a single category
scales really well we could train on
millions of sentences using a cluster
without too much trouble
adding the more categories slow things
down slows things down very quickly so
that's why with the variational
approximation you have to represent as
much of the the number of categories
that you've chosen which is a bit of a
limitation so to summarize what I've
what I presented today so I presented
the Bayes model so I presented a I work
with the screen to a probabilistic model
and knowing that we can use that to
incorporate these sparse features in
much the same way that we've done that
in other fields in NLP in parsing and
and sequence modeling and such and in
same benefits and I've described a
nonparametric Bayesian model and shown
some initial experiments with that so
what we're going from here so with the
discriminative framework a prime concern
is scaling it is a heavy model and we'd
like to be able to train for more data
and we'd like the ability to optimize
more interesting loss functions than
likelihood and natural natural options
error expected loss using a because it's
a probabilistic model so I think it's
like expected blue and on the on the
Bayesian model as I said front for our
initial experiments I'm moving more
towards a sampling approach to train us
to make these models and an approach
that might let us induce the categories
as we use them rather than having to
represent them all
from so I'm currently experimenting with
ways to two ways to do that and whether
we'll be able to do that more
efficiently and and of course we also
want to remove any dependency on those
initializing alignment through the
phrase table okay so thank you see
presentation
so when some talk to Percy and the
Berkley guys they kind of gotten gone
cold on the hierarchical usually process
very difficult to get the prior to do
exactly once
so they tended to you know in basic so
the attempted to go is a more empirical
methods which do rule splitting and
merging yeah which was sort of an
old-fashioned idea that didn't have a
nice nice math behind it but seems to
more scalable for them get better
results yes so you have something pretty
well so the fact that they're trying
this means that they don't believe that
so I'm no sort of
Bayesian Zilla I don't believe in it for
its own sake and if you look at the
results is there's not very many
positive results using Bayesian model
comparing to a map estimate you normally
see basically the same result so I'm
interested in the Bayesian models when
the the modeling actually gives you
something that you can't do as easily in
without that and with the constituents
as you say you can you can use States
building approaches like that I think
the most promising thing with the
Bayesian model is being able to explore
the space especially if the translation
pairs rather than the constituents
without every numerating it
so through samplers and such such we
don't need to use alignments in touch
and we can explore the space of grammar
rules by a sampler or some procedure so
I think we you actually I think the
Bayesian models are interesting when you
actually do have a something which is
nonparametric when your model is small
and and you have you expect a small
number of categories or a small number
of term laws I think there's much less
argument for using it but my hope is
that when you have a model which does
have very large or infinite parameter
spaces such as the translation
equivalences that we'll be able to see
some benefits so that's that's what I'm
hoping to get element
but I don't have any life towards using
it for its own sake and especially in
the they approach that the they were
doing it Berkeley it was a they were
looking at a small number of splits of
of categories and not inducing terminals
and they had fixed trees so I think
using simple models you'd get very good
result</div></div><div class="container-fluid bottom-ad"><div id="amzn-assoc-ad-6a809dda-347a-4187-8a86-91faf94575da"></div><script async src="//z-na.amazon-adsystem.com/widgets/onejs?MarketPlace=US&amp;adInstanceId=6a809dda-347a-4187-8a86-91faf94575da"></script></div><div class="text-center">We are a participant in the Amazon Services LLC Associates Program, an affiliate advertising program designed to provide a means for us to earn fees by linking to Amazon.com and affiliated sites.</div><script>(function(w, d){
    var b = d.getElementsByTagName('body')[0];
    var s = d.createElement("script"); s.async = true;
    var v = !("IntersectionObserver" in w) ? "8.6.0" : "10.4.2";
    s.src = "https://cdnjs.cloudflare.com/ajax/libs/vanilla-lazyload/" + v + "/lazyload.min.js";
    w.lazyLoadOptions = {};
    b.appendChild(s);
}(window, document));</script></body></html>